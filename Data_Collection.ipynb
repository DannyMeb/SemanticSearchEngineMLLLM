{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Install required packages\n",
    "This command installs the feedparser (a library for parsing RSS and Atom feeds) and beautifulsoup4 (a library for parsing HTML and XML documents) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install feedparser\n",
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "These libraries are essential for making HTTP requests, parsing HTML content, and handling file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Function to Fetch Articles from Crossref API\n",
    "This function fetches articles from the Crossref API based on the specified query and total number of articles. It extracts relevant information such as title, authors, published date, citation count, and abstract from the fetched articles, stores the data in a list, and saves it to a JSON file. After fetching articles for each query, a manifest file is generated that maps each query to its corresponding JSON filename. This manifest provides a clear and easily accessible reference for later evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global counter for unique IDs\n",
    "global_article_id = 0\n",
    "\n",
    "def fetch_crossref_articles1(query, total=300, data_folder_path='data', max_retries=5):\n",
    "    global global_article_id  # Refer to the global variable for article IDs\n",
    "\n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    rows_per_request = 300\n",
    "    num_requests = total // rows_per_request + (1 if total % rows_per_request > 0 else 0)\n",
    "    \n",
    "    articles_data = []\n",
    "    article_ids = []  # Store IDs of fetched articles\n",
    "    retries = 0\n",
    "\n",
    "    while not articles_data and retries < max_retries:\n",
    "        for i in range(num_requests):\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"rows\": rows_per_request,\n",
    "                \"offset\": i * rows_per_request,\n",
    "                # Filter to include both journal articles and conference papers\n",
    "                \"filter\": \"type:journal-article,type:proceedings-article,from-pub-date:2023-01-01\",\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                articles = data['message']['items']\n",
    "                for article in articles:\n",
    "                    global_article_id += 1\n",
    "\n",
    "                    title = article.get('title', ['No Title'])[0]\n",
    "                    authors = ', '.join([f\"{author.get('given', '')} {author.get('family', '')}\" for author in article.get('author', [])])\n",
    "                    link = article.get('URL', 'No URL')\n",
    "                    published = article.get('published-print') or article.get('published-online')\n",
    "                    published_date = 'No Date'\n",
    "                    if published:\n",
    "                        date_parts = published.get('date-parts', [[0]])[0]\n",
    "                        published_date = '-'.join(str(part) for part in date_parts)\n",
    "                    citation_count = article.get('is-referenced-by-count', 0)\n",
    "                    abstract_html = article.get('abstract', '')\n",
    "                    abstract = 'No Abstract or Keywords available'\n",
    "                    if abstract_html:\n",
    "                        soup = BeautifulSoup(abstract_html, 'html.parser')\n",
    "                        abstract = soup.get_text()\n",
    "                    keywords = article.get('keywords', [])\n",
    "                    keywords_str = ', '.join(keywords)\n",
    "                    articles_data.append({\n",
    "                        \"id\": global_article_id,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"published\": published_date,\n",
    "                        \"citations\": citation_count,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"keywords\": keywords_str,\n",
    "                        \"link\": link\n",
    "                    })\n",
    "                    article_ids.append(global_article_id)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for query: {query}, retrying... ({retries+1}/{max_retries})\")\n",
    "            time.sleep(1)  # Respectful delay between retries\n",
    "\n",
    "        retries += 1  # Increment retries count after each attempt\n",
    "\n",
    "    # Save the articles data and return article IDs as well\n",
    "    if articles_data:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(data_folder_path, exist_ok=True)\n",
    "        # Define the file path for storing the data\n",
    "        file_name = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Save the articles data to the JSON file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Total number of articles fetched and stored for '{query}': {len(articles_data)}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        return article_ids\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    global global_article_id  # Refer to the global variable for article IDs\n",
    "\n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    rows_per_request = 50\n",
    "    num_requests = total // rows_per_request + (1 if total % rows_per_request > 0 else 0)\n",
    "    \n",
    "    articles_data = []\n",
    "    article_ids = []  # Store IDs of fetched articles\n",
    "    retries = 0\n",
    "    articles_fetched = 0  # Track the total number of articles fetched\n",
    "\n",
    "    while articles_fetched < total and retries < max_retries:\n",
    "        for i in range(num_requests):\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"rows\": min(rows_per_request, total - articles_fetched),  # Adjust rows for the last request\n",
    "                \"offset\": i * rows_per_request,\n",
    "                # Filter to include both journal articles and conference papers\n",
    "                \"filter\": \"type:journal-article,type:proceedings-article,from-pub-date:2023-01-01\",\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                articles = data['message']['items']\n",
    "                for article in articles:\n",
    "                    global_article_id += 1\n",
    "                    articles_fetched += 1  # Increment the count of fetched articles\n",
    "\n",
    "                    title = article.get('title', ['No Title'])[0]\n",
    "                    authors = ', '.join([f\"{author.get('given', '')} {author.get('family', '')}\" for author in article.get('author', [])])\n",
    "                    link = article.get('URL', 'No URL')\n",
    "                    published = article.get('published-print') or article.get('published-online')\n",
    "                    published_date = 'No Date'\n",
    "                    if published:\n",
    "                        date_parts = published.get('date-parts', [[0]])[0]\n",
    "                        published_date = '-'.join(str(part) for part in date_parts)\n",
    "                    citation_count = article.get('is-referenced-by-count', 0)\n",
    "                    abstract_html = article.get('abstract', '')\n",
    "                    abstract = 'No Abstract or Keywords available'\n",
    "                    if abstract_html:\n",
    "                        soup = BeautifulSoup(abstract_html, 'html.parser')\n",
    "                        abstract = soup.get_text()\n",
    "                    keywords = article.get('keywords', [])\n",
    "                    keywords_str = ', '.join(keywords)\n",
    "                    articles_data.append({\n",
    "                        \"id\": global_article_id,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"published\": published_date,\n",
    "                        \"citations\": citation_count,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"keywords\": keywords_str,\n",
    "                        \"link\": link\n",
    "                    })\n",
    "                    article_ids.append(global_article_id)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for query: {query}, retrying... ({retries+1}/{max_retries})\")\n",
    "            time.sleep(1)  # Respectful delay between retries\n",
    "\n",
    "        retries += 1  # Increment retries count after each attempt\n",
    "\n",
    "    # Save the articles data and return article IDs as well\n",
    "    if articles_data:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(data_folder_path, exist_ok=True)\n",
    "        # Define the file path for storing the data\n",
    "        file_name = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Save the articles data to the JSON file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Total number of articles fetched and stored for '{query}': {len(articles_data)}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        return article_ids\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_manifest_and_ground_truth(queries, data_folder_path='data'):\n",
    "    manifest = {}\n",
    "    ground_truth = {}\n",
    "    for query in queries:\n",
    "        article_ids = fetch_crossref_articles(query=query, total=10, data_folder_path=data_folder_path)\n",
    "        if article_ids:\n",
    "            manifest[query] = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "            ground_truth[query] = article_ids\n",
    "\n",
    "    # Save the manifest\n",
    "    manifest_path = os.path.join(data_folder_path, 'query_file_manifest.json')\n",
    "    with open(manifest_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(manifest, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Save the ground truth\n",
    "    ground_truth_path = os.path.join(data_folder_path, 'ground_truth.json')\n",
    "    with open(ground_truth_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(ground_truth, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Manifest and ground truth generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Specific Queries and Fetch Articles for Each Query\n",
    "These are the specific queries related to core machine learning (ML) and large language models (LLMs) that we want to fetch articles for. This loop iterates through each query in the queries list and calls the fetch_crossref_articles function to fetch articles for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles loaded: 175\n",
      "Total number of articles fetched and stored for 'PageRank': 10\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Expectation-Maximization': 10\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Principal Component Analysis': 10\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 185\u001b[0m\n\u001b[0;32m      2\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPageRank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpectation-Maximization\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI techniques in financial forecasting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m ]\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of articles loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m \u001b[43mgenerate_manifest_and_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_folder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(global_article_id)\n",
      "Cell \u001b[1;32mIn[4], line 170\u001b[0m, in \u001b[0;36mgenerate_manifest_and_ground_truth\u001b[1;34m(queries, data_folder_path)\u001b[0m\n\u001b[0;32m    168\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m--> 170\u001b[0m     article_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_crossref_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_folder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_folder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m article_ids:\n\u001b[0;32m    172\u001b[0m         manifest[query] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 105\u001b[0m, in \u001b[0;36mfetch_crossref_articles\u001b[1;34m(query, total, data_folder_path, max_retries)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_requests):\n\u001b[0;32m     98\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mmin\u001b[39m(rows_per_request, total \u001b[38;5;241m-\u001b[39m articles_fetched),  \u001b[38;5;66;03m# Adjust rows for the last request\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype:journal-article,type:proceedings-article,from-pub-date:2023-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    104\u001b[0m     }\n\u001b[1;32m--> 105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    107\u001b[0m         data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\http\\client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\http\\client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\http\\client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\Master\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Specific queries related to core ML and LLMs\n",
    "queries = [\n",
    "    \"PageRank\",\n",
    "    \"Expectation-Maximization\",\n",
    "    \"Principal Component Analysis\",\n",
    "    \"Convolutional Neural Networks\",\n",
    "    \"Recurrent Neural Networks\",\n",
    "    \"Long Short-Term Memory\",\n",
    "    \"Generative Adversarial Networks\",\n",
    "    \"Few-shot Learning\",\n",
    "    \"Self-supervised Learning\",\n",
    "    \"BERT model applications\",\n",
    "    \"GPT-3 and its implications\",\n",
    "    \"AI for Climate Change\",\n",
    "    \"Quantum Machine Learning\",\n",
    "    \"Deep Learning in Edge Devices\",\n",
    "    \"AI in Digital Health\",\n",
    "    \"Ethical AI Practices\",\n",
    "    \"Federated Learning applications\",\n",
    "    \"Large language models\",\n",
    "    \"Computer Vision essentials\",\n",
    "    \"Statistical and Probabilistic Inference\",\n",
    "    \"Convolutional Neural Networks\",\n",
    "    \"Recurrent Neural Networks\",\n",
    "    \"Natural Language Processing Fundamentals\",\n",
    "    \"Support Vector Machines\",\n",
    "    \"Attention Mechanisms in Neural Networks\",\n",
    "    \"Generative Adversarial Networks\",\n",
    "    \"Techniques in Reinforcement Learning\",\n",
    "    \"Healthcare Diagnostics using Machine Learning\",\n",
    "    \"Autonomous Vehicle Navigation Systems\",\n",
    "    \"Predictive Analytics in Retail\",\n",
    "    \"Applications of Graph Neural Networks\",\n",
    "    \"Transformers in NLP\"\n",
    "    \"Foundations of machine learning\",\n",
    "    \"Applications of deep learning in healthcare\",\n",
    "    \"Training large language models for efficiency\",\n",
    "    \"Advancements in neural network architectures\",\n",
    "    \"Transformer models for natural language understanding\",\n",
    "    \"Reinforcement learning strategies in gaming\",\n",
    "    \"Techniques in supervised learning for regression\",\n",
    "    \"Clustering algorithms in unsupervised learning\",\n",
    "    \"Trends in natural language processing for 2023\",\n",
    "    \"Predictive modeling with linear regression\",\n",
    "    \"Binary classification using logistic regression\",\n",
    "    \"Decision tree complexity and pruning techniques\",\n",
    "    \"Ensemble methods: Beyond random forests\",\n",
    "    \"Optimization techniques for support vector machines\",\n",
    "    \"k-Nearest Neighbors algorithm and its efficiency\",\n",
    "    \"Naive Bayes for text classification\",\n",
    "    \"K-Means clustering for image segmentation\",\n",
    "    \"Applications of hierarchical clustering in genomics\",\n",
    "    \"Association rules mining with the Apriori algorithm\",\n",
    "    \"Google PageRank algorithm explained\",\n",
    "    \"Expectation-Maximization for latent variable models\",\n",
    "    \"Principal Component Analysis in dimension reduction\",\n",
    "    \"Deep convolutional neural networks for object recognition\",\n",
    "    \"Temporal sequence processing with recurrent neural networks\",\n",
    "    \"Using LSTM networks for time series forecasting\",\n",
    "    \"Generative adversarial networks for creative AI\",\n",
    "    \"Semantic analysis with word embeddings\",\n",
    "    \"Modeling with Gaussian mixture models\",\n",
    "    \"Applications of hidden Markov models in bioinformatics\",\n",
    "    \"Markov Chain Monte Carlo methods in Bayesian analysis\",\n",
    "    \"Strategy optimization with Monte Carlo Tree Search\",\n",
    "    \"Game tree pruning using Alpha-Beta technique\",\n",
    "    \"Real-time Q-Learning applications\",\n",
    "    \"Deep Q-Networks and their implementation\",\n",
    "    \"Policy gradient methods for robotics\",\n",
    "    \"Applying value iteration in dynamic environments\",\n",
    "    \"Structure learning in Bayesian networks\",\n",
    "    \"Markov decision processes in decision analysis\",\n",
    "    \"Gaussian processes for non-linear regression\",\n",
    "    \"Computer vision techniques for autonomous driving\",\n",
    "    \"Deep reinforcement learning in artificial intelligence\",\n",
    "    \"Semantic segmentation with fully convolutional networks\",\n",
    "    \"Robotics: Combining AI and engineering\",\n",
    "    \"Batch normalization in deep learning\",\n",
    "    \"Dropout regularization for preventing overfitting\",\n",
    "    \"Activation functions in neural network training\",\n",
    "    \"Initialization techniques for neural networks\",\n",
    "    \"Optimization algorithms for training deep models\",\n",
    "    \"Loss functions for machine learning models\",\n",
    "    \"Dealing with overfitting and underfitting in machine learning\",\n",
    "    \"Cross-validation methods for model assessment\",\n",
    "    \"Hyperparameter tuning in neural networks\",\n",
    "    \"Feature engineering for improved model performance\",\n",
    "    \"Techniques in dimensionality reduction\",\n",
    "    \"Ensemble learning strategies for better predictions\",\n",
    "    \"Bagging and boosting in ensemble methods\",\n",
    "    \"Evaluating machine learning models using ROC curves\",\n",
    "    \"Bias-variance tradeoff in model training\",\n",
    "    \"Data augmentation techniques in deep learning\",\n",
    "    \"Early stopping as a regularization technique\",\n",
    "    \"Explainable AI for transparent decision-making\",\n",
    "    \"Federated learning for privacy-preserving AI\",\n",
    "    \"Self-supervised learning from unlabelled data\",\n",
    "    \"Transfer learning for adapting pre-trained models\",\n",
    "    \"Meta-learning: Learning to learn effectively\",\n",
    "    \"Multi-task learning and its challenges\",\n",
    "    \"Singular Value Decomposition in machine learning\",\n",
    "    \"Topic modeling with Latent Dirichlet Allocation\",\n",
    "    \"Energy-based models for structured predictions\",\n",
    "    \"Evolutionary algorithms in neural architecture search\",\n",
    "    \"Transformer architecture for language models\",\n",
    "    \"BERT model for improving text understanding\",\n",
    "    \"Generative pre-trained transformer applications\",\n",
    "    \"Use of ALBERT model in resource-constrained environments\",\n",
    "    \"XLNet: Rethinking pretraining in NLP\",\n",
    "    \"RoBERTa: A robustly optimized BERT pretraining approach\",\n",
    "    \"T5 model and its impact on NLP tasks\",\n",
    "    \"DistilBERT: Distilling knowledge in transformers\",\n",
    "    \"Global vectors for word representation\",\n",
    "    \"FastText for efficient text classification\",\n",
    "    \"Doc2Vec for document embedding\",\n",
    "    \"Attention mechanisms in deep learning\",\n",
    "    \"Innovations in transformer encoder designs\",\n",
    "    \"Decoding strategies in transformer networks\",\n",
    "    \"Self-attention mechanisms and their benefits\",\n",
    "    \"Multi-head attention for better representation learning\",\n",
    "    \"Bidirectional attention for context awareness\",\n",
    "    \"Understanding and application of bertology\",\n",
    "    \"Fine-tuning techniques for pre-trained models\",\n",
    "    \"Tokenization strategies in text processing\",\n",
    "    \"Combining pre-training and fine-tuning for NLP\",\n",
    "    \"Sequence classification with neural networks\",\n",
    "    \"Labeling sequences in NLP\",\n",
    "    \"Generating text with neural networks\",\n",
    "    \"Developing question answering systems with AI\",\n",
    "    \"Automated text summarization techniques\",\n",
    "    \"Machine translation improvements in 2023\",\n",
    "    \"Semantic similarity measures in texts\",\n",
    "    \"Named entity recognition with deep learning\",\n",
    "    \"Sentiment analysis in social media monitoring\",\n",
    "    \"Classifying texts in multiple categories\",\n",
    "    \"Techniques in part-of-speech tagging\",\n",
    "    \"Dependency parsing in natural language processing\",\n",
    "    \"Parsing constituents in language processing\",\n",
    "    \"Challenges in real-time machine translation\",\n",
    "    \"Image classification at scale\",\n",
    "    \"Object detection methods in crowded scenes\",\n",
    "    \"Advanced techniques in semantic segmentation\",\n",
    "    \"Instance segmentation in medical imaging\",\n",
    "    \"Human pose estimation with deep learning\",\n",
    "    \"Technologies in face recognition systems\",\n",
    "    \"Creative applications of generative models\",\n",
    "    \"Key tasks in computer vision\",\n",
    "    \"AI-driven image generation techniques\",\n",
    "    \"Restoring old photographs with AI\",\n",
    "    \"Artistic style transfer with deep neural networks\",\n",
    "    \"Super-resolution via deep learning\",\n",
    "    \"Content-based image retrieval systems\",\n",
    "    \"Visual question answering capabilities\",\n",
    "    \"AI applications in robotics for healthcare\",\n",
    "    \"Path planning algorithms for autonomous vehicles\",\n",
    "    \"Localization techniques in mobile robotics\",\n",
    "    \"Simultaneous Localization and Mapping (SLAM) technology\",\n",
    "    \"Perception systems in robotic applications\",\n",
    "    \"Deep learning for advanced robotics control\",\n",
    "    \"Model-based reinforcement learning in robotics\",\n",
    "    \"Inverse reinforcement learning for behavior prediction\",\n",
    "    \"Strategies for off-policy reinforcement learning\",\n",
    "    \"On-policy vs off-policy learning in RL\",\n",
    "    \"Exploration vs exploitation in reinforcement learning\",\n",
    "    \"Temporal difference learning and its applications\",\n",
    "    \"Actor-critic methods for efficient policy learning\",\n",
    "    \"Proximal Policy Optimization in complex environments\",\n",
    "    \"Deep deterministic policy gradient techniques\",\n",
    "    \"Twin delayed deep deterministic policy gradients\",\n",
    "    \"Asynchronous methods in actor-critic learning\",\n",
    "    \"Soft Actor-Critic for high-dimensional control tasks\",\n",
    "    \"Meta-reinforcement learning for adaptive agents\",\n",
    "    \"Evolution strategies for optimization in AI\",\n",
    "    \"Genetic algorithms for feature selection\",\n",
    "    \"Ant colony optimization in path-finding\",\n",
    "    \"Particle swarm optimization for network training\",\n",
    "    \"AI-driven risk assessment methods\",\n",
    "    \"AI techniques in financial forecasting\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total number of articles loaded: {len(queries)}\")\n",
    "generate_manifest_and_ground_truth(queries, data_folder_path='data')\n",
    "print(global_article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compiling a centralized record of all article titles and their IDs \n",
    "Here, I consolidate titles and IDs of articles from multiple JSON files into a single, sorted JSON file named Master_record.json, excluding entries from a manifest file and any pre-existing master record file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article titles with corresponding IDs have been saved.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles_and_ids(data_folder_path):\n",
    "    # Initialize an empty dictionary to store titles and IDs\n",
    "    article_data = {}\n",
    "\n",
    "    # Iterate through all files in the data folder\n",
    "    for file_name in os.listdir(data_folder_path):\n",
    "        # Skip the manifest file and Master_record.json\n",
    "        if file_name in ['query_file_manifest.json','ground_truth.json', 'Master_record.json']:\n",
    "            continue\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            articles_data = json.load(file)\n",
    "\n",
    "        # Extract titles and IDs from each article\n",
    "        for article in articles_data:\n",
    "            article_id = article['id']\n",
    "            article_title = article['title']\n",
    "            # Store the title with corresponding ID\n",
    "            article_data[article_id] = article_title\n",
    "\n",
    "    return article_data\n",
    "\n",
    "# Specify the data folder path\n",
    "data_folder_path = 'data'\n",
    "\n",
    "# Extract titles and IDs from all JSON files in the data folder\n",
    "article_data = extract_titles_and_ids(data_folder_path)\n",
    "\n",
    "# Sort the article data by ID\n",
    "sorted_article_data = dict(sorted(article_data.items()))\n",
    "\n",
    "# Write the sorted article data to a JSON file\n",
    "output_file_path = os.path.join(data_folder_path, 'Master_record.json')\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(sorted_article_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Article titles with corresponding IDs have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
