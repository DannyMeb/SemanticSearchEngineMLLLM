{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Install required packages\n",
    "This command installs the feedparser (a library for parsing RSS and Atom feeds) and beautifulsoup4 (a library for parsing HTML and XML documents) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1786067004.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install feedparser\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser\n",
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "These libraries are essential for making HTTP requests, parsing HTML content, and handling file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Function to Fetch Articles from Crossref API\n",
    "This function fetches articles from the Crossref API based on the specified query and total number of articles. It extracts relevant information such as title, authors, published date, citation count, and abstract from the fetched articles, stores the data in a list, and saves it to a JSON file. After fetching articles for each query, a manifest file is generated that maps each query to its corresponding JSON filename. This manifest provides a clear and easily accessible reference for later evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize a global counter for unique IDs\n",
    "global_article_id = 0\n",
    "\n",
    "def fetch_crossref_articles(query=\"machine learning\", total=10, data_folder_path='data', max_retries=5):\n",
    "    global global_article_id  # Refer to the global variable for article IDs\n",
    "\n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    rows_per_request = 100\n",
    "    num_requests = total // rows_per_request + (1 if total % rows_per_request > 0 else 0)\n",
    "    \n",
    "    articles_data = []\n",
    "    retries = 0\n",
    "\n",
    "    while not articles_data and retries < max_retries:\n",
    "        for i in range(num_requests):\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"rows\": rows_per_request,\n",
    "                \"offset\": i * rows_per_request,\n",
    "                \"filter\": \"from-pub-date:2017-01-01\",\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                articles = data['message']['items']\n",
    "                for article in articles:\n",
    "                    global_article_id += 1\n",
    "\n",
    "                    title = article.get('title', ['No Title'])[0]\n",
    "                    authors = ', '.join([f\"{author.get('given', '')} {author.get('family', '')}\" for author in article.get('author', [])])\n",
    "                    link = article.get('URL', 'No URL')\n",
    "                    published = article.get('published-print') or article.get('published-online')\n",
    "                    published_date = 'No Date'\n",
    "                    if published:\n",
    "                        date_parts = published.get('date-parts', [[0]])[0]\n",
    "                        published_date = '-'.join(str(part) for part in date_parts)\n",
    "                    citation_count = article.get('is-referenced-by-count', 0)\n",
    "                    abstract_html = article.get('abstract', '')\n",
    "                    abstract = 'No Abstract or Keywords available'\n",
    "                    if abstract_html:\n",
    "                        soup = BeautifulSoup(abstract_html, 'html.parser')\n",
    "                        abstract = soup.get_text()\n",
    "\n",
    "                    articles_data.append({\n",
    "                        \"id\": global_article_id,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"published\": published_date,\n",
    "                        \"citations\": citation_count,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"link\": link\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for query: {query}, retrying... ({retries+1}/{max_retries})\")\n",
    "            time.sleep(1)  # Respectful delay between retries\n",
    "\n",
    "        retries += 1  # Increment retries count after each attempt\n",
    "\n",
    "    if not articles_data:\n",
    "        print(f\"Unable to fetch articles after {max_retries} retries.\")\n",
    "\n",
    "    # Proceed with saving the articles if any are fetched\n",
    "    if articles_data:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(data_folder_path, exist_ok=True)\n",
    "        # Define the file path for storing the data\n",
    "        file_name = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Save the articles data to the JSON file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Total number of articles fetched and stored for '{query}': {len(articles_data)}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate a manifest mapping queries to filenames\n",
    "def generate_manifest(queries, data_folder_path='data'):\n",
    "    manifest = {}\n",
    "    for query in queries:\n",
    "        file_name = fetch_crossref_articles(query=query, total=10, data_folder_path=data_folder_path)\n",
    "        manifest[query] = file_name\n",
    "\n",
    "    manifest_path = os.path.join(data_folder_path, 'query_file_manifest.json')\n",
    "    with open(manifest_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(manifest, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Manifest generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Specific Queries and Fetch Articles for Each Query\n",
    "These are the specific queries related to core machine learning (ML) and large language models (LLMs) that we want to fetch articles for. This loop iterates through each query in the queries list and calls the fetch_crossref_articles function to fetch articles for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles loaded: 250\n",
      "24770\n"
     ]
    }
   ],
   "source": [
    "# Specific queries related to core ML and LLMs\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"large language models\",\n",
    "    \"neural networks\",\n",
    "    \"transformer models\",\n",
    "    \"reinforcement learning\",\n",
    "    \"supervised learning\",\n",
    "    \"unsupervised learning\",\n",
    "    \"natural language processing\"\n",
    "\n",
    "    \n",
    "    \"Linear Regression\",\n",
    "    \"Logistic Regression\",\n",
    "    \"Decision Trees\",\n",
    "    \"Random Forest\",\n",
    "    \"Support Vector Machines (SVM)\",\n",
    "    \"k-Nearest Neighbors (k-NN)\",\n",
    "    \"Naive Bayes Classifier\",\n",
    "    \"K-Means Clustering\",\n",
    "    \"Hierarchical Clustering\",\n",
    "    \"Apriori Algorithm\",\n",
    "    \"PageRank\",\n",
    "    \"Expectation-Maximization (EM)\",\n",
    "    \"Principal Component Analysis (PCA)\",\n",
    "    \"Convolutional Neural Networks (CNN)\",\n",
    "    \"Recurrent Neural Networks (RNN)\",\n",
    "    \"Long Short-Term Memory (LSTM)\",\n",
    "    \"Generative Adversarial Networks (GANs)\",\n",
    "    \"Word Embeddings\",\n",
    "    \"Gaussian Mixture Models (GMM)\",\n",
    "    \"Hidden Markov Models (HMM)\",\n",
    "    \"Transformer Models\",\n",
    "    \"Markov Chain Monte Carlo (MCMC)\",\n",
    "    \"Monte Carlo Tree Search (MCTS)\",\n",
    "    \"Alpha-Beta Pruning\",\n",
    "    \"Q-Learning\",\n",
    "    \"Deep Q-Networks (DQN)\",\n",
    "    \"Policy Gradient Methods\",\n",
    "    \"Value Iteration\",\n",
    "    \"Bayesian Networks\",\n",
    "    \"Markov Decision Processes (MDP)\",\n",
    "    \"Gaussian Processes\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Natural Language Processing (NLP)\",\n",
    "    \"Computer Vision (CV)\",\n",
    "    \"Robotics\",\n",
    "    \"Deep Learning\",\n",
    "    \"Machine Learning\",\n",
    "    \"Neural Networks\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Semi-Supervised Learning\",\n",
    "    \"Transfer Learning\",\n",
    "    \"Adversarial Learning\",\n",
    "    \"Gradient Descent\",\n",
    "    \"Backpropagation\",\n",
    "    \"Stochastic Gradient Descent (SGD)\",\n",
    "    \"Batch Normalization\",\n",
    "    \"Dropout Regularization\",\n",
    "    \"Activation Functions\",\n",
    "    \"Weight Initialization\",\n",
    "    \"Optimization Algorithms\",\n",
    "    \"Loss Functions\",\n",
    "    \"Overfitting and Underfitting\",\n",
    "    \"Cross-Validation\",\n",
    "    \"Hyperparameter Tuning\",\n",
    "    \"Feature Engineering\",\n",
    "    \"Dimensionality Reduction\",\n",
    "    \"Ensemble Learning\",\n",
    "    \"Bagging and Boosting\",\n",
    "    \"Model Evaluation Metrics\",\n",
    "    \"Confusion Matrix\",\n",
    "    \"Precision and Recall\",\n",
    "    \"F1 Score\",\n",
    "    \"Receiver Operating Characteristic (ROC) Curve\",\n",
    "    \"Area Under the Curve (AUC)\",\n",
    "    \"Bias-Variance Tradeoff\",\n",
    "    \"Data Augmentation\",\n",
    "    \"Early Stopping\",\n",
    "    \"Model Interpretability\",\n",
    "    \"Gradient Boosting Machines (GBM)\",\n",
    "    \"XGBoost\",\n",
    "    \"LightGBM\",\n",
    "    \"CatBoost\",\n",
    "    \"AutoML\",\n",
    "    \"Explainable AI (XAI)\",\n",
    "    \"Federated Learning\",\n",
    "    \"Self-Supervised Learning\",\n",
    "    \"One-Shot Learning\",\n",
    "    \"Zero-Shot Learning\",\n",
    "    \"Transfer Learning\",\n",
    "    \"Meta-Learning\",\n",
    "    \"Multi-Task Learning\",\n",
    "    \"Singular Value Decomposition (SVD)\",\n",
    "    \"Latent Dirichlet Allocation (LDA)\",\n",
    "    \"Boltzmann Machines\",\n",
    "    \"Neuroevolution\",\n",
    "    \"Neuro-Linguistic Programming (NLP)\",\n",
    "    \"Transformer Architecture\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers)\",\n",
    "    \"GPT (Generative Pre-trained Transformer)\",\n",
    "    \"BERTweet\",\n",
    "    \"ALBERT (A Lite BERT)\",\n",
    "    \"XLNet\",\n",
    "    \"RoBERTa\",\n",
    "    \"T5 (Text-To-Text Transfer Transformer)\",\n",
    "    \"DistilBERT\",\n",
    "    \"Word2Vec\",\n",
    "    \"GloVe (Global Vectors for Word Representation)\",\n",
    "    \"FastText\",\n",
    "    \"Doc2Vec\",\n",
    "    \"Attention Mechanism\",\n",
    "    \"Transformer Encoder\",\n",
    "    \"Transformer Decoder\",\n",
    "    \"Attention Is All You Need\",\n",
    "    \"Self-Attention\",\n",
    "    \"Multi-Head Attention\",\n",
    "    \"Bidirectional Attention\",\n",
    "    \"Bertology\",\n",
    "    \"Fine-Tuning\",\n",
    "    \"Tokenization\",\n",
    "    \"Pre-training and Fine-tuning\",\n",
    "    \"Sequence Classification\",\n",
    "    \"Sequence Labeling\",\n",
    "    \"Text Generation\",\n",
    "    \"Question Answering\",\n",
    "    \"Summarization\",\n",
    "    \"Translation\",\n",
    "    \"Semantic Similarity\",\n",
    "    \"Named Entity Recognition (NER)\",\n",
    "    \"Sentiment Analysis\",\n",
    "    \"Text Classification\",\n",
    "    \"Part-of-Speech Tagging (POS Tagging)\",\n",
    "    \"Dependency Parsing\",\n",
    "    \"Constituency Parsing\",\n",
    "    \"Machine Translation\",\n",
    "    \"Image Classification\",\n",
    "    \"Object Detection\",\n",
    "    \"Semantic Segmentation\",\n",
    "    \"Instance Segmentation\",\n",
    "    \"Pose Estimation\",\n",
    "    \"Face Recognition\",\n",
    "    \"Generative Models\",\n",
    "    \"Computer Vision Tasks\",\n",
    "    \"Image Generation\",\n",
    "    \"Image Restoration\",\n",
    "    \"Style Transfer\",\n",
    "    \"Super-Resolution\",\n",
    "    \"Content-Based Image Retrieval (CBIR)\",\n",
    "    \"Visual Question Answering (VQA)\",\n",
    "    \"Robotics\",\n",
    "    \"Path Planning\",\n",
    "    \"Localization\",\n",
    "    \"Simultaneous Localization and Mapping (SLAM)\",\n",
    "    \"Robot Perception\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Deep Reinforcement Learning\",\n",
    "    \"Model-Free Reinforcement Learning\",\n",
    "    \"Model-Based Reinforcement Learning\",\n",
    "    \"Value Iteration\",\n",
    "    \"Policy Iteration\",\n",
    "    \"Exploration-Exploitation Tradeoff\",\n",
    "    \"Temporal Difference Learning\",\n",
    "    \"Deep Q-Learning\",\n",
    "    \"Policy Gradient Methods\",\n",
    "    \"Actor-Critic Methods\",\n",
    "    \"Proximal Policy Optimization (PPO)\",\n",
    "    \"Deep Deterministic Policy Gradient (DDPG)\",\n",
    "    \"Twin Delayed DDPG (TD3)\",\n",
    "    \"Asynchronous Advantage Actor-Critic (A3C)\",\n",
    "    \"Soft Actor-Critic (SAC)\",\n",
    "    \"Monte Carlo Methods\",\n",
    "    \"Temporal Difference Learning (TD)\",\n",
    "    \"SARSA (State-Action-Reward-State-Action)\",\n",
    "    \"Q-Learning\",\n",
    "    \"Deep Q-Networks (DQN)\",\n",
    "    \"Policy Gradient Methods\",\n",
    "    \"Proximal Policy Optimization (PPO)\",\n",
    "    \"Trust Region Policy Optimization (TRPO)\",\n",
    "    \"Deep Deterministic Policy Gradient (DDPG)\",\n",
    "    \"Twin Delayed DDPG (TD3)\",\n",
    "    \"Asynchronous Advantage Actor-Critic (A3C)\",\n",
    "    \"Soft Actor-Critic (SAC)\",\n",
    "    \"Meta-Learning\",\n",
    "    \"Model-Agnostic Meta-Learning (MAML)\",\n",
    "    \"Learning to Learn\",\n",
    "    \"Few-Shot Learning\",\n",
    "    \"Zero-Shot Learning\",\n",
    "    \"One-Shot Learning\",\n",
    "    \"Multi-Task Learning\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Model-Based Reinforcement Learning\",\n",
    "    \"Model-Free Reinforcement Learning\",\n",
    "    \"Deep Reinforcement Learning\",\n",
    "    \"Imitation Learning\",\n",
    "    \"Inverse Reinforcement Learning\",\n",
    "    \"Off-Policy Learning\",\n",
    "    \"On-Policy Learning\",\n",
    "    \"Exploration-Exploitation Tradeoff\",\n",
    "    \"Temporal Difference Learning\",\n",
    "    \"Actor-Critic Methods\",\n",
    "    \"Policy Gradient Methods\",\n",
    "    \"Evolution Strategies\",\n",
    "    \"Genetic Algorithms\",\n",
    "    \"Coevolution\",\n",
    "    \"NEAT (NeuroEvolution of Augmenting Topologies)\",\n",
    "    \"NSGA-II (Non-dominated Sorting Genetic Algorithm II)\",\n",
    "    \"CMA-ES (Covariance Matrix Adaptation Evolution Strategy)\",\n",
    "    \"Particle Swarm Optimization (PSO)\",\n",
    "    \"Ant Colony Optimization (ACO)\",\n",
    "    \"CV object recognition systems\",\n",
    "    \"Robotics path planning techniques\",\n",
    "    \"Deep learning in facial recognition\",\n",
    "    \"Learning Algorithms for regression modeling\",\n",
    "    \"Optimization algorithms for deep learning models\",\n",
    "    \"LSTMs for sequence prediction\",\n",
    "    \"Transformer models for sentiment analysis\",\n",
    "    \"GANs for video generation\",\n",
    "    \"RNNs for text generation\",\n",
    "    \"Probabilistic Models for risk assessment\",\n",
    "    \"Gaussian Models for data clustering\",\n",
    "    \"Bayesian Models for decision making\",\n",
    "    \"Deep Learning in autonomous systems\",\n",
    "    \"Neural Networks for image classification\",\n",
    "    \"Machine Learning algorithms for fraud detection\",\n",
    "    \"NLP sentiment analysis frameworks\",\n",
    "    \"CV object detection algorithms\",\n",
    "    \"Robotics motion planning algorithms\",\n",
    "    \"Deep learning in object detection\",\n",
    "    \"Learning Algorithms for pattern recognition\",\n",
    "    \"Optimization algorithms for neural network training\",\n",
    "    \"LSTMs for natural language understanding\",\n",
    "    \"Transformer models for machine translation\",\n",
    "    \"GANs for style transfer\",\n",
    "    \"RNNs for language modeling\",\n",
    "    \"Probabilistic Models for statistical inference\",\n",
    "    \"Gaussian Models for image compression\",\n",
    "    \"Bayesian Models for spam detection\",\n",
    "    \"Deep Learning in financial forecasting\",\n",
    "    \"Neural Networks for fraud detection\",\n",
    "    \"Machine Learning algorithms for recommendation systems\",\n",
    "    \"NLP sentiment analysis frameworks\",\n",
    "    \"CV object recognition systems\",\n",
    "    \"Robotics motion control algorithms\",\n",
    "    \"Deep learning in medical imaging\",\n",
    "    \"Learning Algorithms for classification tasks\",\n",
    "    \"Optimization algorithms for machine learning optimization\",\n",
    "    \"LSTMs for sentiment classification\",\n",
    "    \"Transformer models for language understanding\",\n",
    "    \"GANs for image synthesis\",\n",
    "    \"RNNs for sentiment classification\",\n",
    "    \"Probabilistic Models for uncertainty estimation\",\n",
    "    \"Gaussian Models for regression analysis\",\n",
    "    \"Bayesian Models for decision support\",\n",
    "    \"Deep Learning in healthcare\",\n",
    "    \"Neural Networks in finance\",\n",
    "    \"Machine Learning for fraud detection\",\n",
    "    \"NLP sentiment analysis\",\n",
    "    \"Robotics motion planning\",\n",
    "    \"Deep learning for image recognition\"]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total number of articles loaded: {len(queries)}\")\n",
    "\n",
    "# generate_manifest(queries, data_folder_path='data')\n",
    "print(global_article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
