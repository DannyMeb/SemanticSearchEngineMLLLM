{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Install required packages\n",
    "This command installs the feedparser (a library for parsing RSS and Atom feeds) and beautifulsoup4 (a library for parsing HTML and XML documents) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install feedparser\n",
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "These libraries are essential for making HTTP requests, parsing HTML content, and handling file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Function to Fetch Articles from Crossref API\n",
    "This function fetches articles from the Crossref API based on the specified query and total number of articles. It extracts relevant information such as title, authors, published date, citation count, and abstract from the fetched articles, stores the data in a list, and saves it to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global counter for unique IDs\n",
    "global_article_id = 0\n",
    "\n",
    "def fetch_crossref_articles(query=\"machine learning\", total=10):\n",
    "    global global_article_id  # Refer to the global variable for article IDs\n",
    "\n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    rows_per_request = 100\n",
    "    num_requests = total // rows_per_request + (1 if total % rows_per_request > 0 else 0)\n",
    "    \n",
    "    articles_data = []\n",
    "\n",
    "    for i in range(num_requests):\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"rows\": rows_per_request,\n",
    "            \"offset\": i * rows_per_request,\n",
    "            \"filter\": \"from-pub-date:2017-01-01\",\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data['message']['items']\n",
    "            for article in articles:\n",
    "                # Increment the global ID counter for each new article\n",
    "                global_article_id += 1\n",
    "\n",
    "                title = article.get('title', ['No Title'])[0]\n",
    "                authors = ', '.join([f\"{author.get('given', '')} {author.get('family', '')}\" for author in article.get('author', [])])\n",
    "                link = article.get('URL', 'No URL')\n",
    "                \n",
    "                published = article.get('published-print') or article.get('published-online')\n",
    "                published_date = 'No Date'\n",
    "                if published:\n",
    "                    date_parts = published.get('date-parts', [[0]])[0]\n",
    "                    published_date = '-'.join(str(part) for part in date_parts)\n",
    "                \n",
    "                citation_count = article.get('is-referenced-by-count', 0)\n",
    "                \n",
    "                abstract_html = article.get('abstract', '')\n",
    "                abstract = 'No Abstract or Keywords available'\n",
    "                if abstract_html:\n",
    "                    soup = BeautifulSoup(abstract_html, 'html.parser')\n",
    "                    abstract = soup.get_text()\n",
    "\n",
    "                articles_data.append({\n",
    "                    \"id\": global_article_id,\n",
    "                    \"title\": title,\n",
    "                    \"authors\": authors,\n",
    "                    \"published\": published_date,\n",
    "                    \"citations\": citation_count,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"link\": link\n",
    "                })\n",
    "                \n",
    "        else:\n",
    "            print(\"Failed to fetch data for query:\", query)\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Define the directory and file path for storing the data\n",
    "    data_folder_path = os.path.join(os.getcwd(), 'data')\n",
    "    os.makedirs(data_folder_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    file_path = os.path.join(data_folder_path, f'articles_data_{query.replace(\" \", \"_\")}.json')\n",
    "\n",
    "    # Save the articles data to the JSON file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(articles_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Total number of articles fetched and stored for '{query}': {len(articles_data)}\")\n",
    "    print(\"=\" * 80 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Specific Queries and Fetch Articles for Each Query\n",
    "These are the specific queries related to core machine learning (ML) and large language models (LLMs) that we want to fetch articles for. This loop iterates through each query in the queries list and calls the fetch_crossref_articles function to fetch articles for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles for: machine learning\n",
      "Total number of articles fetched and stored for 'machine learning': 5000\n",
      "================================================================================\n",
      "Fetching articles for: deep learning\n",
      "Total number of articles fetched and stored for 'deep learning': 5000\n",
      "================================================================================\n",
      "Fetching articles for: large language models\n",
      "Total number of articles fetched and stored for 'large language models': 5000\n",
      "================================================================================\n",
      "Fetching articles for: neural networks\n",
      "Total number of articles fetched and stored for 'neural networks': 5000\n",
      "================================================================================\n",
      "Fetching articles for: transformer models\n",
      "Total number of articles fetched and stored for 'transformer models': 5000\n",
      "================================================================================\n",
      "Fetching articles for: reinforcement learning\n",
      "Total number of articles fetched and stored for 'reinforcement learning': 5000\n",
      "================================================================================\n",
      "Fetching articles for: supervised learning\n",
      "Total number of articles fetched and stored for 'supervised learning': 5000\n",
      "================================================================================\n",
      "Fetching articles for: unsupervised learning\n",
      "Total number of articles fetched and stored for 'unsupervised learning': 5000\n",
      "================================================================================\n",
      "Fetching articles for: natural language processing\n",
      "Total number of articles fetched and stored for 'natural language processing': 5000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Specific queries related to core ML and LLMs\n",
    "queries = [\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"large language models\",\n",
    "    \"neural networks\",\n",
    "    \"transformer models\",\n",
    "    \"reinforcement learning\",\n",
    "    \"supervised learning\",\n",
    "    \"unsupervised learning\",\n",
    "    \"natural language processing\"\n",
    "]\n",
    "\n",
    "# Iterate through the list of specific queries and fetch articles for each topic\n",
    "for query in queries:\n",
    "    print(f\"Fetching articles for: {query}\")\n",
    "    fetch_crossref_articles(query=query, total=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
