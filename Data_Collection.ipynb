{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Install required packages\n",
    "This command installs the feedparser (a library for parsing RSS and Atom feeds) and beautifulsoup4 (a library for parsing HTML and XML documents) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install feedparser\n",
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries\n",
    "These libraries are essential for making HTTP requests, parsing HTML content, and handling file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Function to Fetch Articles from Crossref API\n",
    "This function fetches articles from the Crossref API based on the specified query and total number of articles. It extracts relevant information such as title, authors, published date, citation count, and abstract from the fetched articles, stores the data in a list, and saves it to a JSON file. After fetching articles for each query, a manifest file is generated that maps each query to its corresponding JSON filename. This manifest provides a clear and easily accessible reference for later evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global counter for unique IDs\n",
    "global_article_id = 0\n",
    "\n",
    "def fetch_crossref_articles(query, total=10, data_folder_path='data', max_retries=5):\n",
    "    global global_article_id  # Refer to the global variable for article IDs\n",
    "\n",
    "    base_url = \"https://api.crossref.org/works\"\n",
    "    rows_per_request = 100\n",
    "    num_requests = total // rows_per_request + (1 if total % rows_per_request > 0 else 0)\n",
    "    \n",
    "    articles_data = []\n",
    "    article_ids = []  # Store IDs of fetched articles\n",
    "    retries = 0\n",
    "\n",
    "    while not articles_data and retries < max_retries:\n",
    "        for i in range(num_requests):\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"rows\": rows_per_request,\n",
    "                \"offset\": i * rows_per_request,\n",
    "                \"filter\": \"from-pub-date:2023-01-01\",\n",
    "            }\n",
    "            response = requests.get(base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                articles = data['message']['items']\n",
    "                for article in articles:\n",
    "                    global_article_id += 1\n",
    "\n",
    "                    title = article.get('title', ['No Title'])[0]\n",
    "                    authors = ', '.join([f\"{author.get('given', '')} {author.get('family', '')}\" for author in article.get('author', [])])\n",
    "                    link = article.get('URL', 'No URL')\n",
    "                    published = article.get('published-print') or article.get('published-online')\n",
    "                    published_date = 'No Date'\n",
    "                    if published:\n",
    "                        date_parts = published.get('date-parts', [[0]])[0]\n",
    "                        published_date = '-'.join(str(part) for part in date_parts)\n",
    "                    citation_count = article.get('is-referenced-by-count', 0)\n",
    "                    abstract_html = article.get('abstract', '')\n",
    "                    abstract = 'No Abstract or Keywords available'\n",
    "                    if abstract_html:\n",
    "                        soup = BeautifulSoup(abstract_html, 'html.parser')\n",
    "                        abstract = soup.get_text()\n",
    "                    keywords = article.get('keywords', [])\n",
    "                    keywords_str = ', '.join(keywords)\n",
    "                    articles_data.append({\n",
    "                        \"id\": global_article_id,\n",
    "                        \"title\": title,\n",
    "                        \"authors\": authors,\n",
    "                        \"published\": published_date,\n",
    "                        \"citations\": citation_count,\n",
    "                        \"abstract\": abstract,\n",
    "                        \"keywords\": keywords_str,\n",
    "                        \"link\": link\n",
    "                    })\n",
    "                    article_ids.append(global_article_id)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for query: {query}, retrying... ({retries+1}/{max_retries})\")\n",
    "            time.sleep(1)  # Respectful delay between retries\n",
    "\n",
    "        retries += 1  # Increment retries count after each attempt\n",
    "\n",
    "    # Save the articles data and return article IDs as well\n",
    "    if articles_data:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(data_folder_path, exist_ok=True)\n",
    "        # Define the file path for storing the data\n",
    "        file_name = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Save the articles data to the JSON file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Total number of articles fetched and stored for '{query}': {len(articles_data)}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        return article_ids\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_manifest_and_ground_truth(queries, data_folder_path='data'):\n",
    "    manifest = {}\n",
    "    ground_truth = {}\n",
    "    for query in queries:\n",
    "        article_ids = fetch_crossref_articles(query=query, total=10, data_folder_path=data_folder_path)\n",
    "        if article_ids:\n",
    "            manifest[query] = f'articles_data_{query.replace(\" \", \"_\")}.json'\n",
    "            ground_truth[query] = article_ids\n",
    "\n",
    "    # Save the manifest\n",
    "    manifest_path = os.path.join(data_folder_path, 'query_file_manifest.json')\n",
    "    with open(manifest_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(manifest, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Save the ground truth\n",
    "    ground_truth_path = os.path.join(data_folder_path, 'ground_truth.json')\n",
    "    with open(ground_truth_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(ground_truth, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Manifest and ground truth generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Specific Queries and Fetch Articles for Each Query\n",
    "These are the specific queries related to core machine learning (ML) and large language models (LLMs) that we want to fetch articles for. This loop iterates through each query in the queries list and calls the fetch_crossref_articles function to fetch articles for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles loaded: 25\n",
      "Total number of articles fetched and stored for 'Few-shot Learning': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Self-supervised Learning': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'BERT model applications': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'GPT-3 and its implications': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'AI for Climate Change': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Quantum Machine Learning': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Deep Learning in Edge Devices': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'AI in Digital Health': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Ethical AI Practices': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Federated Learning applications': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Large language models': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Computer Vision essentials': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Statistical and Probabilistic Inference': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Convolutional Neural Networks': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Recurrent Neural Networks': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Natural Language Processing Fundamentals': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Support Vector Machines': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Attention Mechanisms in Neural Networks': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Generative Adversarial Networks': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Techniques in Reinforcement Learning': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Healthcare Diagnostics using Machine Learning': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Autonomous Vehicle Navigation Systems': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Predictive Analytics in Retail': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Applications of Graph Neural Networks': 100\n",
      "================================================================================\n",
      "Total number of articles fetched and stored for 'Transformers in NLP': 100\n",
      "================================================================================\n",
      "Manifest and ground truth generated successfully.\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Specific queries related to core ML and LLMs\n",
    "queries = [\n",
    "    \"Few-shot Learning\",\n",
    "    \"Self-supervised Learning\",\n",
    "    \"BERT model applications\",\n",
    "    \"GPT-3 and its implications\",\n",
    "    \"AI for Climate Change\",\n",
    "    \"Quantum Machine Learning\",\n",
    "    \"Deep Learning in Edge Devices\",\n",
    "    \"AI in Digital Health\",\n",
    "    \"Ethical AI Practices\",\n",
    "    \"Federated Learning applications\",\n",
    "    \"Large language models\",\n",
    "    \"Computer Vision essentials\",\n",
    "    \"Statistical and Probabilistic Inference\",\n",
    "    \"Convolutional Neural Networks\",\n",
    "    \"Recurrent Neural Networks\",\n",
    "    \"Natural Language Processing Fundamentals\",\n",
    "    \"Support Vector Machines\",\n",
    "    \"Attention Mechanisms in Neural Networks\",\n",
    "    \"Generative Adversarial Networks\",\n",
    "    \"Techniques in Reinforcement Learning\",\n",
    "    \"Healthcare Diagnostics using Machine Learning\",\n",
    "    \"Autonomous Vehicle Navigation Systems\",\n",
    "    \"Predictive Analytics in Retail\",\n",
    "    \"Applications of Graph Neural Networks\",\n",
    "    \"Transformers in NLP\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total number of articles loaded: {len(queries)}\")\n",
    "generate_manifest_and_ground_truth(queries, data_folder_path='data')\n",
    "print(global_article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compiling a centralized record of all article titles and their IDs \n",
    "Here, I consolidate titles and IDs of articles from multiple JSON files into a single, sorted JSON file named Master_record.json, excluding entries from a manifest file and any pre-existing master record file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article titles with corresponding IDs have been saved.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles_and_ids(data_folder_path):\n",
    "    # Initialize an empty dictionary to store titles and IDs\n",
    "    article_data = {}\n",
    "\n",
    "    # Iterate through all files in the data folder\n",
    "    for file_name in os.listdir(data_folder_path):\n",
    "        # Skip the manifest file and Master_record.json\n",
    "        if file_name in ['query_file_manifest.json','ground_truth.json', 'Master_record.json']:\n",
    "            continue\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(data_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            articles_data = json.load(file)\n",
    "\n",
    "        # Extract titles and IDs from each article\n",
    "        for article in articles_data:\n",
    "            article_id = article['id']\n",
    "            article_title = article['title']\n",
    "            # Store the title with corresponding ID\n",
    "            article_data[article_id] = article_title\n",
    "\n",
    "    return article_data\n",
    "\n",
    "# Specify the data folder path\n",
    "data_folder_path = 'data'\n",
    "\n",
    "# Extract titles and IDs from all JSON files in the data folder\n",
    "article_data = extract_titles_and_ids(data_folder_path)\n",
    "\n",
    "# Sort the article data by ID\n",
    "sorted_article_data = dict(sorted(article_data.items()))\n",
    "\n",
    "# Write the sorted article data to a JSON file\n",
    "output_file_path = os.path.join(data_folder_path, 'Master_record.json')\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(sorted_article_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Article titles with corresponding IDs have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
