[
    {
        "id": 17971,
        "title": "Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we describe Q-learning, which is one of the most popular methods in reinforcement learning. Q-learning is a type of temporal difference learning.  We discuss other TD algorithms, such as SARSA, and connections to biological learning through dopamine. Q-learning is also one of the most common frameworks for deep reinforcement learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.ss11hp"
    },
    {
        "id": 17972,
        "title": "Model-Free Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-5"
    },
    {
        "id": 17973,
        "title": "Crystallization Process Design by Model-Free Deep Reinforcement Learning",
        "authors": "Georgi Tancev",
        "published": "No Date",
        "citations": 0,
        "abstract": "Chemical process design is the search for an optimal manufacturing\nprotocol to perform chemical operations. For transient processes such as\ncrystallization, the optimal conditions can change over time, requiring\na dynamic strategy. Model-free deep reinforcement learning is an\napproach that can be used to identify the best sequence of states with\nrespect to a predefined reward function. In this work, proximal policy\noptimization is applied in a simulated environment to identify\noperational strategies that are optimal with respect to the desired\nparticle properties in unseeded batch cooling crystallization processes\nof paracetamol in ethanol. For this purpose, the corresponding Markov\ndecision process is formulated, and it is shown that the method is\npromising for the development of novel routes that allow the tuning of\nparticle size (623 μm) and provide high yields (96%) within a defined\nperiod of time (12 h).",
        "link": "http://dx.doi.org/10.36227/techrxiv.170792884.44909118/v2"
    },
    {
        "id": 17974,
        "title": "Crystallization Process Design by Model-Free Deep Reinforcement Learning",
        "authors": "Georgi Tancev",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170792884.44909118/v1"
    },
    {
        "id": 17975,
        "title": "Detection of Man-in-the-Middle Attacks in Model-Free Reinforcement Learning",
        "authors": "Rishi Rani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a Bellman Deviation algorithm for the detection of man-in-the-middle (MITM) attacks occurring when an agent controls a Markov Decision Process (MDP) system using  model-free reinforcement learning. We show an intuitive necessary and sufficient ``informational advantage\" condition  for  the proposed algorithm to guarantee the detection of attacks  with high probability, while  also avoiding false alarms.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22357600.v1"
    },
    {
        "id": 17976,
        "title": "Detection of Man-in-the-Middle Attacks in Model-Free Reinforcement Learning",
        "authors": "Rishi Rani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a Bellman Deviation algorithm for the detection of man-in-the-middle (MITM) attacks occurring when an agent controls a Markov Decision Process (MDP) system using  model-free reinforcement learning. We show an intuitive necessary and sufficient ``informational advantage\" condition  for  the proposed algorithm to guarantee the detection of attacks  with high probability, while  also avoiding false alarms.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22357600"
    },
    {
        "id": 17977,
        "title": "Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards",
        "authors": "Paul Krueger, Thomas Griffiths",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1191-0"
    },
    {
        "id": 17978,
        "title": "Model-Free Deep Reinforcement Learning—Algorithms and Applications",
        "authors": "Fabian Otto",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_10"
    },
    {
        "id": 17979,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review1"
    },
    {
        "id": 17980,
        "title": "Mfrlmo: Model-Free Reinforcement Learning for Multi-Objective Optimization of Apache Spark",
        "authors": "Muhammed  Maruf öztürk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358735"
    },
    {
        "id": 17981,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/review1"
    },
    {
        "id": 17982,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/review1"
    },
    {
        "id": 17983,
        "title": "Expert Initialized Hybrid Model-Based and Model-Free Reinforcement Learning",
        "authors": "Jeppe Langaa, Christoffer Sloth",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178306"
    },
    {
        "id": 17984,
        "title": "Mf^2: Model-Free Reinforcement Learning for Modeling-Free Building Hvac Control in a Nearly Zero-Energy Building",
        "authors": "Man Wang, Borong Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4459796"
    },
    {
        "id": 17985,
        "title": "Do model-based and model-free reinforcement learning correspond to goal-directed and habitual actions, respectively? A systematic review.",
        "authors": "Xuan Yee",
        "published": "No Date",
        "citations": 0,
        "abstract": "The idea that model-based reinforcement learning (RL) corresponds to goal-directed actions and model-free RL corresponds to habitual actions is deeply entrenched in psychology. To test this hypothesis, we will first review and evaluate the evidence base from behavioral and neural studies. Behaviorally, positive correlations between behavioral parameters that index model-based/model-free RL and goal-directed/habitual actions would support the hypothesis. Neurally, overlapping neural substrates that underlie the model-based/goal-directed constructs and between the model-free/habitual constructs, as well as a dissociation between the neural substrates underlying both groups of constructs, would support the hypothesis. We will then discuss alternative classes of computational theories beyond the model-based/model-free framework that purport to describe goal-directed and habitual behaviour, and compare these theories against the model-based/model-free framework as well as against each other. Some of the alternative theories covered in this review include dichotomy-based frameworks (e.g., value-based vs. value-free), hierarchical frameworks (e.g., action sequences or active inference), and biological frameworks (e.g., actor-critic models). We then outline potential approaches to synthesize the findings and future avenues of research. Overall, we find that model-based RL maps onto certain facets of goal-directed actions but not necessarily onto other facets. On the other hand, there is much evidence suggesting that model-free RL does not track habitual actions.",
        "link": "http://dx.doi.org/10.31234/osf.io/qm3gp"
    },
    {
        "id": 17986,
        "title": "Recovering Robustness in Model-Free Reinforcement Learning",
        "authors": "Harish K Venkataraman, Peter J. Seiler",
        "published": "2019-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8815368"
    },
    {
        "id": 17987,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review2"
    },
    {
        "id": 17988,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/review2"
    },
    {
        "id": 17989,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/review2"
    },
    {
        "id": 17990,
        "title": "Model-Based or Model-Free, a Review of Approaches in Reinforcement Learning",
        "authors": "Qingyan Huang",
        "published": "2020-8",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds49703.2020.00051"
    },
    {
        "id": 17991,
        "title": "Optimal State Estimation Using Model-Free Reinforcement Learning",
        "authors": "Haoran Ma, Ying Yang, Dingguo Liang",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728004"
    },
    {
        "id": 17992,
        "title": "Imitation as a model-free process in human reinforcement learning",
        "authors": "Anis Najar, Emmanuelle Bonnet, Bahador Bahrami, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "While there is not doubt that social signals affect human reinforcement learning, there is still no consensus about their exact computational implementation. To address this issue, we compared three hypotheses about the algorithmic implementation of imitation in human reinforcement learning. A first hypothesis, decision biasing, postulates that imitation consists in transiently biasing the learner’s action selection without affecting her value function. According to the second hypothesis, model-based imitation, the learner infers the demonstrator’s value function through inverse reinforcement learning and uses it for action selection. Finally, according to the third hypothesis, value shaping, demonstrator’s actions directly affect the learner’s value function. We tested these three psychologically plausible hypotheses in two separate experiments (N = 24 and N = 44) featuring a new variant of a social reinforcement learning task, where we manipulated the quantity and the quality of the demonstrator’s choices. We show through model comparison that value shaping is favored, which provides a new perspective on how imitation is integrated into human reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/797407"
    },
    {
        "id": 17993,
        "title": "EEG-based classification of learning strategies : Model-based and model-free reinforcement learning",
        "authors": "Dongjae Kim, Charles Weston, Sang Wan Lee",
        "published": "2018-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iww-bci.2018.8311522"
    },
    {
        "id": 17994,
        "title": "Model-free Predictive Optimal Iterative Learning Control using Reinforcement Learning",
        "authors": "Yueqing Zhang, Bing Chu, Zhan Shu",
        "published": "2022-6-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867561"
    },
    {
        "id": 17995,
        "title": "Value-free reinforcement learning: Policy optimization as a minimal model of operant behavior",
        "authors": "Daniel Bennett, Yael Niv, Angela Langdon",
        "published": "No Date",
        "citations": 4,
        "abstract": "Reinforcement learning is a powerful framework for modelling the cognitive and neural substrates of learning and decision making. Contemporary research in cognitive neuroscience and neuroeconomics typically uses value-based reinforcement-learning models, which assume that decision-makers choose by comparing learned values for different actions. However, another possibility is suggested by a simpler family of models, called policy-gradient reinforcement learning. Policy-gradient models learn by optimizing a behavioral policy directly, without the intermediate step of value-learning. Here we review recent behavioral and neural findings that are more parsimoniously explained by policy-gradient models than by value-based models. We conclude that, despite the ubiquity of `value' in reinforcement-learning models of decision making, policy-gradient models provide a lightweight and compelling alternative model of operant behavior.",
        "link": "http://dx.doi.org/10.31234/osf.io/ew58m"
    },
    {
        "id": 17996,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/decision1"
    },
    {
        "id": 17997,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/decision1"
    },
    {
        "id": 17998,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/decision1"
    },
    {
        "id": 17999,
        "title": "Model-free Reinforcement Learning for Demand Response in PV-rich Distribution Systems",
        "authors": "Ibrahim Alsaleh",
        "published": "2022-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sasg57022.2022.10200928"
    },
    {
        "id": 18000,
        "title": "On Distributed Model-Free Reinforcement Learning Control with Stability Guarantee",
        "authors": "Sayak Mukherjee, Thanh Long Vu",
        "published": "2021-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9482972"
    },
    {
        "id": 18001,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100065"
    },
    {
        "id": 18002,
        "title": "Model Free DEAP Controller Learned by Reinforcement Learning DDPG Algorithm",
        "authors": "Jakub Bernat, Dawid Apanasiewicz",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/argencon49523.2020.9505344"
    },
    {
        "id": 18003,
        "title": "Model-Free Approaches",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_4"
    },
    {
        "id": 18004,
        "title": "Model-Free Reinforcement Learning Algorithms: A Survey",
        "authors": "Sinan Çalışır, Meltem Kurt Pehlivanoğlu",
        "published": "2019-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu.2019.8806389"
    },
    {
        "id": 18005,
        "title": "Policy Learning with Constraints in Model-free Reinforcement Learning: A Survey",
        "authors": "Yongshuai Liu, Avishai Halev, Xin Liu",
        "published": "2021-8",
        "citations": 32,
        "abstract": "Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/614"
    },
    {
        "id": 18006,
        "title": "Model-Free Cooperative Optimal Output Regulation for Linear Discrete-Time Multi-Agent Systems Using Reinforcement Learning",
        "authors": "Beining Wu, Wei Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, an off-policy model-free algorithm is presented for solving the cooperative optimal output regulation problem for linear discrete-time multi-agent systems. First, an adaptive distributed observer is designed for each follower to estimate the leader's information. Then, a distributed feedback-feedforward controller is developed for each follower to solve the cooperative optimal output regulation problem utilizing the follower's state information and the adaptive distributed observer. Based on reinforcement learning method, an adaptive algorithm is presented to find the optimal feedback gains via online data collecting from system trajectory. By designing a Sylvester map, the solution to the regulator equations is calculated via data collected from the optimal feedback gain design steps, and the feedforward control gain is found. Finally, an off-policy model-free algorithm is proposed to design the distributed feedback-feedforward controller for each follower to solve the cooperative optimal output regulation problem. A numerical example is given to verify the effectiveness of this proposed approach.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2797557/v1"
    },
    {
        "id": 18007,
        "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving",
        "authors": "Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka",
        "published": "2019-10",
        "citations": 145,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917306"
    },
    {
        "id": 18008,
        "title": "Model-Free Indirect RL: Temporal Difference",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_4"
    },
    {
        "id": 18009,
        "title": "Model-Free Indirect RL: Monte Carlo",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_3"
    },
    {
        "id": 18010,
        "title": "Combined model-free and model-sensitive reinforcement learning in non-human primates",
        "authors": "Bruno Miranda, W. M. Nishantha Malalasekera, Timothy E Behrens, Peter Dayan, Steven W. Kennerley",
        "published": "No Date",
        "citations": 1,
        "abstract": "Contemporary reinforcement learning (RL) theory suggests that potential choices can be evaluated by strategies that may or may not be sensitive to the computational structure of tasks. A paradigmatic model-free (MF) strategy simply repeats actions that have been rewarded in the past; by contrast, model-sensitive (MS) strategies exploit richer information associated with knowledge of task dynamics. MF and MS strategies should typically be combined, because they have complementary statistical and computational strengths; however, this tradeoff between MF/MS RL has mostly only been demonstrated in humans, often with only modest numbers of trials. We trained rhesus monkeys to perform a two-stage decision task designed to elicit and discriminate the use of MF and MS methods. A descriptive analysis of choice behaviour revealed directly that the structure of the task (of MS importance) and the reward history (of MF and MS importance) significantly influenced both choice and response vigour. A detailed, trial-by-trial computational analysis confirmed that choices were made according to a combination of strategies, with a dominant influence of a particular form of model sensitivity that persisted over weeks of testing. The residuals from this model necessitated development of a new combined RL model which incorporates a particular credit assignment weighting procedure. Finally, response vigor exhibited a subtly different collection of MF and MS influences. These results provide new illumination onto RL behavioural processes in non-human primates.",
        "link": "http://dx.doi.org/10.1101/836007"
    },
    {
        "id": 18011,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_5"
    },
    {
        "id": 18012,
        "title": "Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning",
        "authors": "Laura Smith, Ilya Kostrikov, Sergey Levine",
        "published": "2023-7-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.056"
    },
    {
        "id": 18013,
        "title": "Model Free Human‐Robot Interaction Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch4"
    },
    {
        "id": 18014,
        "title": "Block-Decentralized Model-Free Reinforcement Learning Control of Two Time-Scale Networks",
        "authors": "Sayak Mukherjee, Aranya Chakrabortty, He Bai",
        "published": "2019-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8815077"
    },
    {
        "id": 18015,
        "title": "Model-Free Reinforcement-Learning-Based Control Methodology for Power Electronic Converters",
        "authors": "Dajr Alfred, Dariusz Czarkowski, Jiaxin Teng",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/greentech48523.2021.00024"
    },
    {
        "id": 18016,
        "title": "Model-Free Adaptive Control Approach Using Integral Reinforcement Learning",
        "authors": "Mohammed Abouheaf, Wail Gueaieb",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rose.2019.8790432"
    },
    {
        "id": 18017,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100065-1"
    },
    {
        "id": 18018,
        "title": "FORK: A FORward-looKing Actor for Model-Free Reinforcement Learning",
        "authors": "Honghao Wei, Lei Ying",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683288"
    },
    {
        "id": 18019,
        "title": "Secure Linear Quadratic Regulator Using Sparse Model-Free Reinforcement Learning",
        "authors": "Bahare Kiumarsi, Tamer Basar",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9028861"
    },
    {
        "id": 18020,
        "title": "Can model-free reinforcement learning operate over information stored in working-memory?",
        "authors": "Carolina Feher da Silva, Yuan-Wei Yao, Todd A. Hare",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractModel-free learning creates stimulus-response associations. But what constitutes a stimulus? Are there limits to types of stimuli a model-free or habitual system can operate over? Most experiments on reward learning in humans and animals have used discrete sensory stimuli, but there is no algorithmic reason that model-free learning should be restricted to external stimuli, and recent theories have suggested that model-free processes may operate over highly abstract concepts and goals. Our study aimed to determine whether model-free learning processes can operate over environmental states defined by information held in working memory. Specifically, we tested whether or not humans can learn explicit temporal patterns of individually uninformative cues in a model-free manner. We compared the data from human participants in a reward learning paradigm using (1) a simultaneous symbol presentation condition or (2) a sequential symbol presentation condition, wherein the same visual stimuli were presented simultaneously or as a temporal sequence that required working memory. We found a significant effect of reward on human behavior in the sequential presentation condition, indicating that model-free learning can operate on information stored in working memory. Further analyses, however, revealed that the behavior of the participants contradicts the basic assumptions of our hypotheses, and it is possible that the observed effect of reward was generated by model-based rather than model-free learning. Thus it is not possible to draw any conclusions from out study regarding model-free learning of temporal sequences held in working memory. We conclude instead that careful thought should be given about how to best explain two-stage tasks to participants.",
        "link": "http://dx.doi.org/10.1101/107698"
    },
    {
        "id": 18021,
        "title": "Model Free Reinforcement Learning to Determine  Pricing Policy for Car Parking Lots",
        "authors": "Sowmya Karri, Meera Dhabu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4169592"
    },
    {
        "id": 18022,
        "title": "Model-Free μ Synthesis via Adversarial Reinforcement Learning",
        "authors": "Darioush Keivan, Aaron Havens, Peter Seiler, Geir Dullerud, Bin Hu",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867674"
    },
    {
        "id": 18023,
        "title": "Robot arm simulation based on model-free reinforcement learning",
        "authors": "Kun Li, Ke Wang",
        "published": "2021-6-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica52286.2021.9498063"
    },
    {
        "id": 18024,
        "title": "Deep Reinforcement Learning for Autonomous Model-Free Navigation with Partial Observability",
        "authors": "Daniel Tapia, Juan Parras, Santiago Zazo",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco.2019.8902933"
    },
    {
        "id": 18025,
        "title": "On Robust Model-Free Reduced-Dimensional Reinforcement Learning Control for Singularly Perturbed Systems",
        "authors": "Sayak Mukherjee, He Bai, Aranya Chakrabortty",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147523"
    },
    {
        "id": 18026,
        "title": "A Study of Model Based and Model Free Offline Reinforcement Learning",
        "authors": "Indu Shukla, Haley R. Dozier, Althea C. Henslee",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci58124.2022.00061"
    },
    {
        "id": 18027,
        "title": "Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we introduce dynamic programming, which is a cornerstone of model-based reinforcement learning. We demonstrate dynamic programming for policy iteration and value iteration, leading to the quality function and Q-learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.6fs4s9"
    },
    {
        "id": 18028,
        "title": "A Novel Model-Free Actor-Critic Reinforcement Learning Approach for Dynamic Target Tracking",
        "authors": "Amr Elhussein, Md Suruz Miah",
        "published": "2020-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mic50194.2020.9209618"
    },
    {
        "id": 18029,
        "title": "Optimal Scheduled Control Operation of Battery Energy Storage System using Model-Free Reinforcement Learning",
        "authors": "Alaa Selim",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec54162.2022.10033035"
    },
    {
        "id": 18030,
        "title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning",
        "authors": "Jacob Rafati, David C. Noelle",
        "published": "2019-7-17",
        "citations": 22,
        "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. We present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on a variant of the rooms environment.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.330110009"
    },
    {
        "id": 18031,
        "title": "How to Use Your Model: Model-Based Reinforcement Learning with Model-Free Policy Optimization",
        "authors": "Kun Dong, Yongle Luo, Yuxin Wang, Yu Liu, Chengeng Qu, Qiang Zhang, Erkang Cheng, Zhiyong Sun, Song Bo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552867"
    },
    {
        "id": 18032,
        "title": "Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models",
        "authors": "Zhongyu Li, Jun Zeng, Akshay Thirugnanam, Koushil Sreenath",
        "published": "2022-6-27",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2022.xviii.033"
    },
    {
        "id": 18033,
        "title": "Model-Free Attitude Control of Quadcopter using Disturbance Observer and Integral Reinforcement Learning",
        "authors": "Hanna Lee, Youdan Kim",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-2396"
    },
    {
        "id": 18034,
        "title": "MODEL-FREE ONLINE REINFORCEMENT LEARNING OF A ROBOTIC MANIPULATOR",
        "authors": "Jerry Sweaﬀord Jr., Farbod Fahimi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2316/j.2019.201-2931"
    },
    {
        "id": 18035,
        "title": "Model-free Reinforcement Learning for Stochastic Stackelberg Security Games",
        "authors": "Rajesh K Mishra, Deepanshu Vasal, Sriram Vishwanath",
        "published": "2020-12-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9303846"
    },
    {
        "id": 18036,
        "title": "Intelligent Security Aware Routing: Using Model-Free Reinforcement Learning",
        "authors": "Anand Mudgerikar, Elisa Bertino",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccn58024.2023.10230195"
    },
    {
        "id": 18037,
        "title": "Model-free Control Design Using Policy Gradient Reinforcement Learning in LPV Framework",
        "authors": "Yajie Bao, Javad Mohammadpour Velni",
        "published": "2021-6-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc54610.2021.9655004"
    },
    {
        "id": 18038,
        "title": "Model-Free Decentralized Reinforcement Learning Control of Distributed Energy Resources",
        "authors": "Sayak Mukherjee, He Bai, Aranya Chakrabortty",
        "published": "2020-8-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281968"
    },
    {
        "id": 18039,
        "title": "Autonomous Response Agent for Cyber Physical System Attacks: A Model-Free Deep Reinforcement Learning Approach (Drl-Irs)",
        "authors": "May  Said Bashendy, Ashraf Tantawy, Abdelkarim Erradi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4716080"
    },
    {
        "id": 18040,
        "title": "Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization",
        "authors": "Matteo Turchetta, Andreas Krause, Sebastian Trimpe",
        "published": "2020-5",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9197000"
    },
    {
        "id": 18041,
        "title": "Active Hypothesis Testing in Unknown Environments Using Recurrent Neural Networks and Model Free Reinforcement Learning",
        "authors": "George Stamatelis, Nicholas Kalouptsidis",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289731"
    },
    {
        "id": 18042,
        "title": "Model-free Reinforcement Learning for Non-stationary Mean Field Games",
        "authors": "Rajesh K Mishra, Deepanshu Vasal, Sriram Vishwanath",
        "published": "2020-12-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9304340"
    },
    {
        "id": 18043,
        "title": "Multi-Agent Pattern Formation: a Distributed Model-Free Deep Reinforcement Learning Approach",
        "authors": "Elhadji Amadou Oury Diallo, Toshiharu Sugawara",
        "published": "2020-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207657"
    },
    {
        "id": 18044,
        "title": "Designing a Model-Free Reinforcement Learning Controller for a Flexible-Link Manipulator",
        "authors": "Mona Raoufi, Hadi Delavari",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrom54204.2021.9663501"
    },
    {
        "id": 18045,
        "title": "Model-free mean-field reinforcement learning: Mean-field MDP and mean-field Q-learning",
        "authors": "René Carmona, Mathieu Laurière, Zongjun Tan",
        "published": "2023-12-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1214/23-aap1949"
    },
    {
        "id": 18046,
        "title": "Data-Driven Model-Free Tracking Reinforcement Learning Control with VRFT-based Adaptive Actor-Critic",
        "authors": " Radac,  Precup",
        "published": "2019-4-30",
        "citations": 39,
        "abstract": "This paper proposes a neural network (NN)-based control scheme in an Adaptive Actor-Critic (AAC) learning framework designed for output reference model tracking, as a representative deep-learning application. The control learning scheme is model-free with respect to the process model. AAC designs usually require an initial controller to start the learning process; however, systematic guidelines for choosing the initial controller are not offered in the literature, especially in a model-free manner. Virtual Reference Feedback Tuning (VRFT) is proposed for obtaining an initially stabilizing NN nonlinear state-feedback controller, designed from input-state-output data collected from the process in open-loop setting. The solution offers systematic design guidelines for initial controller design. The resulting suboptimal state-feedback controller is next improved under the AAC learning framework by online adaptation of a critic NN and a controller NN. The mixed VRFT-AAC approach is validated on a multi-input multi-output nonlinear constrained coupled vertical two-tank system. Discussions on the control system behavior are offered together with comparisons with similar approaches.",
        "link": "http://dx.doi.org/10.3390/app9091807"
    },
    {
        "id": 18047,
        "title": "Model-Free Reinforcement Learning for Fully Cooperative Multi-Agent Graphical Games",
        "authors": "Qichao Zhang, Dongbin Zhao, Frank L. Lewis",
        "published": "2018-7",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489477"
    },
    {
        "id": 18048,
        "title": "Secure Planning Against Stealthy Attacks via Model-Free Reinforcement Learning",
        "authors": "Alper Kamil Bozkurt, Yu Wang, Miroslav Pajic",
        "published": "2021-5-30",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9560940"
    },
    {
        "id": 18049,
        "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning",
        "authors": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine",
        "published": "2018-5",
        "citations": 312,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8463189"
    },
    {
        "id": 18050,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-4"
    },
    {
        "id": 18051,
        "title": "Model-Free Data-Driven Predictive Control Using Reinforcement Learning",
        "authors": "Shambhuraj Sawant, Dirk Reinhardt, Arash Bahari Kordabad, Sebastien Gros",
        "published": "2023-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383431"
    },
    {
        "id": 18052,
        "title": "Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees",
        "authors": "Daqian Shao, Marta Kwiatkowska",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of the policy satisfying such specifications. Several experiments on various tabular MDP environments across different LTL tasks demonstrate the improved sample efficiency and optimal policy convergence.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/465"
    },
    {
        "id": 18053,
        "title": "Model-free Nearly Optimal Control of Constrained-Input Nonlinear Systems Based on Synchronous Reinforcement Learning",
        "authors": "Han Zhao, Lei Guo",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902515"
    },
    {
        "id": 18054,
        "title": "Comparing neural architectures for demand response through model-free reinforcement learning for heat pump control",
        "authors": "Christophe Patyn, Frederik Ruelens, Geert Deconinck",
        "published": "2018-6",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/energycon.2018.8398836"
    },
    {
        "id": 18055,
        "title": "Model-free Reinforcement Learning based Multi-stage Smart Noise Jamming",
        "authors": "Yuanhang Wang, Tianxian Zhang, Longxiao Xu, Tuanwei Tian, Lingjiang Kong, Xiaobo Yang",
        "published": "2019-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar.2019.8835788"
    },
    {
        "id": 18056,
        "title": "Stress enhances model-free reinforcement learning only after negative outcome",
        "authors": "Heyeon Park, Daeyeol Lee, Jeanyung Chey",
        "published": "2017-7-19",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1371/journal.pone.0180588"
    },
    {
        "id": 18057,
        "title": "Author response for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": " Jaime Liew,  Tuhfe Göçmen,  Wai Hou Lio,  Gunner Chr. Larsen",
        "published": "2023-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/response1"
    },
    {
        "id": 18058,
        "title": "Author response for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": " Jaime Liew,  Tuhfe Göçmen,  Wai Hou Lio,  Gunner Chr. Larsen",
        "published": "2023-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/response1"
    },
    {
        "id": 18059,
        "title": "Transition Based Discount Factor for Model Free Algorithms in Reinforcement Learning",
        "authors": "Abhinav Sharma, Ruchir Gupta, K. Lakshmanan, Atul Gupta",
        "published": "2021-7-2",
        "citations": 2,
        "abstract": "Reinforcement Learning (RL) enables an agent to learn control policies for achieving its long-term goals. One key parameter of RL algorithms is a discount factor that scales down future cost in the state’s current value estimate. This study introduces and analyses a transition-based discount factor in two model-free reinforcement learning algorithms: Q-learning and SARSA, and shows their convergence using the theory of stochastic approximation for finite state and action spaces. This causes an asymmetric discounting, favouring some transitions over others, which allows (1) faster convergence than constant discount factor variant of these algorithms, which is demonstrated by experiments on the Taxi domain and MountainCar environments; (2) provides better control over the RL agents to learn risk-averse or risk-taking policy, as demonstrated in a Cliff Walking experiment.",
        "link": "http://dx.doi.org/10.3390/sym13071197"
    },
    {
        "id": 18060,
        "title": "A model-free mapless navigation method for mobile robot using reinforcement learning",
        "authors": "Lv Qiang, Duo Nanxun, Lin Huican, Wei Heng",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2018.8407713"
    },
    {
        "id": 18061,
        "title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios",
        "authors": "Ugo Lecerf, Christelle Yemdji-Tchassi, Sebastien Aubert, Pietro Michiardi",
        "published": "2022-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529399.3529432"
    },
    {
        "id": 18062,
        "title": "Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning",
        "authors": "Muhammad Burhan Hafez, Cornelius Weber, Matthias Kerzel, Stefan Wermter",
        "published": "2019-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852254"
    },
    {
        "id": 18063,
        "title": "A Scalable Model-Free Deep Reinforcement Learning-Based Perimeter Metering Control Method for Multi-Region Urban Networks",
        "authors": "Dongqin Zhou, Vikash Gayah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4160361"
    },
    {
        "id": 18064,
        "title": "Model-free optimization of power/efficiency tradeoffs in quantum thermal machines using reinforcement learning",
        "authors": "Paolo A Erdman, Frank Noé",
        "published": "2023-8-1",
        "citations": 2,
        "abstract": "Abstract\nA quantum thermal machine is an open quantum system that enables the conversion between heat and work at the micro or nano-scale. Optimally controlling such out-of-equilibrium systems is a crucial yet challenging task with applications to quantum technologies and devices. We introduce a general model-free framework based on reinforcement learning to identify out-of-equilibrium thermodynamic cycles that are Pareto optimal tradeoffs between power and efficiency for quantum heat engines and refrigerators. The method does not require any knowledge of the quantum thermal machine, nor of the system model, nor of the quantum state. Instead, it only observes the heat fluxes, so it is both applicable to simulations and experimental devices. We test our method on a model of an experimentally realistic refrigerator based on a superconducting qubit, and on a heat engine based on a quantum harmonic oscillator. In both cases, we identify the Pareto-front representing optimal power-efficiency tradeoffs, and the corresponding cycles. Such solutions outperform previous proposals made in the literature, such as optimized Otto cycles, reducing quantum friction.",
        "link": "http://dx.doi.org/10.1093/pnasnexus/pgad248"
    },
    {
        "id": 18065,
        "title": "Correction to: Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations",
        "authors": "Ariyan Bighashdel, Pavol Jancura, Gijs Dubbelman",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-022-06298-2"
    },
    {
        "id": 18066,
        "title": "On Model-Free Reinforcement Learning of Reduced-Order Optimal Control for Singularly Perturbed Systems",
        "authors": "Sayak Mukherjee, He Bai, Aranya Chakrabortty",
        "published": "2018-12",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619022"
    },
    {
        "id": 18067,
        "title": "Model Free Safe Control for Reinforcement Learning in a Clustered Dynamic Environment",
        "authors": "Guiliang Zheng, Minhao Yang, Yuxuan Wu",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmra56206.2022.10145738"
    },
    {
        "id": 18068,
        "title": "Applicability Study of Model-Free Reinforcement Learning Towards an Automated Design Space Exploration Framework",
        "authors": "Patrick Hoffmann, Kirill Gorelik, Valentin Ivanov",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371864"
    },
    {
        "id": 18069,
        "title": "Missile Evasion Maneuver Generation with Model-free Deep Reinforcement Learning",
        "authors": "Muhammed Murat Özbek, Emre Koyuncu",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rast57548.2023.10197852"
    },
    {
        "id": 18070,
        "title": "Model-Free Ultra Reliable Low Latency Communication (URLLC): A Deep Reinforcement Learning Framework",
        "authors": "Ali Taleb Zadeh Kasgari, Walid Saad",
        "published": "2019-5",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761721"
    }
]