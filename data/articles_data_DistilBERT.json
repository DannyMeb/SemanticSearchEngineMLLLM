[
    {
        "id": 9484,
        "title": "A Distilbert-Based Hierarchical Text Classification",
        "authors": "Quang Tran",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4596434"
    },
    {
        "id": 9485,
        "title": "Sentence Transformers and DistilBERT for Arabic Word Sense Induction",
        "authors": "Rakia Saidi, Fethi Jarray",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011891700003393"
    },
    {
        "id": 9486,
        "title": "DETECTING DEPRESSION IN TWEETS USING DISTILBERT",
        "authors": "U. Yasaswini, Y. Sasidhar, P. Siva Sai, P. Eswar, V. Swathi",
        "published": "2021-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21276/ijircst.2021.9.4.8"
    },
    {
        "id": 9487,
        "title": "Sentiment Analysis of Amazon Customer Reviews Using Bi-Lstm, Bi-Gru, and Distilbert for the Purpose of Opinion Miningsentiment Analysis of Amazon Customer Reviews Using Bi-Lstm, Bi-Gru, and Distilbert for the Purpose of Opinion Mining",
        "authors": "Sahand Tebyani, Kian Jazayeri, Seyedeh Aridis Ahadi, Erbuğ Çelebi, Farzad Jahandar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4619168"
    },
    {
        "id": 9488,
        "title": "DistilBERT: A Novel Approach to Detect Text Generated by Large Language Models (LLM)",
        "authors": "BV Pranay Kumar, MD Shaheer Ahmed, Manchala Sadanandam",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLarge language models (LLMs) have emerged as powerful tools for generating human-quality text, raising concerns about their potential for misuse in academic settings. This paper investigates the use of DistilBERT, a distilled version of BERT, for detecting LLM-generated text. We evaluate its performance on two publicly available datasets, LLM-Detect AI Generated Text and DAIGT-V3 Train Dataset, achieving an average accuracy of around 94%. Our findings suggest that DistilBERT is a promising tool for safeguarding academic integrity in the era of LLMs.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3909387/v1"
    },
    {
        "id": 9489,
        "title": "PTUK-HULAT at ArAIEval Shared Task Fine-tuned Distilbert to Predict Disinformative Tweets",
        "authors": "Areej Jaber, Paloma Martinez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.50"
    },
    {
        "id": 9490,
        "title": "Gandalf: Harnessing DistilBERT Transformer and BiLSTM for Precise Website Content Classification and Blocking",
        "authors": "Yajat Malhotra, Krish Chatterjie, Raggav Subramani, Aju Dennisan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573966"
    },
    {
        "id": 9491,
        "title": "Comparative Analysis of State-of-the-Art Q\\&amp;amp;A Models: BERT, RoBERTa, DistilBERT, and ALBERT on SQuAD v2 Dataset",
        "authors": "Cem ÖZKURT",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the rapidly evolving landscape of natural language processing (NLP) and artificial intelligence, recent years have witnessed significant advancements, particularly in text-based question-answering (QA) systems. The Stanford Question Answering Dataset (SQuAD v2) has emerged as a prominent benchmark, offering diverse language understanding challenges. This study conducts a thorough examination of cutting-edge QA models—BERT, DistilBERT, RoBERTa, and ALBERT—each featuring distinct architectures, focusing on their training and performance on SQuAD v2.The analysis aims to uncover the unique strengths of each model, providing insights into their capabilities and exploring the impact of different training techniques on their performance. The primary objective is to enhance our understanding of text-based QA systems' evolution and their effectiveness in real-world scenarios. The results of this comparative study are poised to influence the utilization and development of these models in both industry and research.The investigation meticulously evaluates BERT, ALBERT, RoBERTa, and DistilBERT QA models using the SQuAD v2 dataset, emphasizing instances of accurate responses and identifying areas where completeness may be lacking. This nuanced exploration contributes to the ongoing discourse on the advancement of text-based question-answering systems, shedding light on the strengths and limitations of each QA model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3956898/v1"
    },
    {
        "id": 9492,
        "title": "A Multi-Category Content Classification Approach Using DistilBERT Transformer and BiLSTM",
        "authors": "Akanksha Dhar, Ishita Chauhan",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccis60361.2023.10425205"
    },
    {
        "id": 9493,
        "title": "Analyzing Sentiment Towards a Product using DistilBERT and LSTM",
        "authors": "Vishal Pramanik, Maisha Maliha",
        "published": "2022-11-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccis56430.2022.10037634"
    },
    {
        "id": 9494,
        "title": "DistilBERT and RoBERTa Models for Identification of Fake News",
        "authors": "A. Kitanovski, M. Toshevska, G. Mirceva",
        "published": "2023-5-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mipro57284.2023.10159740"
    },
    {
        "id": 9495,
        "title": "LegalDB: Long DistilBERT for Legal Document Classification",
        "authors": "Purbid Bambroo, Aditi Awasthi",
        "published": "2021-2-19",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaect49130.2021.9392558"
    },
    {
        "id": 9496,
        "title": "Analyzing the Performance of Sentiment Analysis using BERT, DistilBERT, and RoBERTa",
        "authors": "Archa Joshy, Sumod Sundar",
        "published": "2022-12-16",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iprecon55716.2022.10059542"
    },
    {
        "id": 9497,
        "title": "Effective Malicious PowerShell Scripts Detection Using DistilBERT",
        "authors": "Ahmed Yasser Merzouk Benselloua, Said Abdesslem Messadi, Alaa Eddine Belfedhal",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/amcai59331.2023.10431513"
    },
    {
        "id": 9498,
        "title": "Detecting Depression in Tweets Using DistilBERT",
        "authors": "U. Yasaswini ., Y. Sasidhar ., P. Siva Sai ., P. Eswar ., V. Swathi .",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3902663"
    },
    {
        "id": 9499,
        "title": "A Method For Answer Selection Using DistilBERT And Important Words",
        "authors": "Jamshid Mozafari, Afsaneh Fatemi, Parham Moradi",
        "published": "2020-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwr49608.2020.9122302"
    },
    {
        "id": 9500,
        "title": "Sculpting DistilBERT: Enhancing Efficiency in Resource-Constrained Scenarios",
        "authors": "V. Prema, V. Elavazhahan",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smart59791.2023.10428568"
    },
    {
        "id": 9501,
        "title": "Sentiment Analysis using DistilBERT",
        "authors": "Song Yi Ng, Kian Ming Lim, Chin Poo Lee, Jit Yan Lim",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icspc59664.2023.10420272"
    },
    {
        "id": 9502,
        "title": "Detection of Web-Attack using DistilBERT, RNN, and LSTM",
        "authors": "Biodoumoye George Bokolo, Lei Chen, Qingzhong Liu",
        "published": "2023-5-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isdfs58141.2023.10131822"
    },
    {
        "id": 9503,
        "title": "Named Entity Recognition for Drone Forensic Using BERT and DistilBERT",
        "authors": "Swardiantara Silalahi, Tohari Ahmad, Hudan Studiawan",
        "published": "2022-7-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icodsa55874.2022.9862916"
    },
    {
        "id": 9504,
        "title": "Detecting Potential Depressed Users in Twitter Using a Fine-tuned DistilBERT Model",
        "authors": "Miguel Antonio Adarlo, Marlene De Leon",
        "published": "2022",
        "citations": 0,
        "abstract": "With the spread of Major Depressive Disorder, otherwise known simply as depression, around the world, various efforts have been made to combat it and to potentially reach out to those suffering from it. Part of those efforts includes the use of technology, such as machine learning models, to screen a potential person for depression through various means, including social media narratives, such as tweets from Twitter. Hence, this study aims to evaluate how well a pre-trained DistilBERT, a transformer model for natural language processing that was fine-tuned on a set of tweets coming from depressed and non-depressed users, can detect potential users in Twitter as having depression. Two models were built using the same procedure of preprocessing, splitting, tokenizing, training, fine-tuning, and optimizing. Both the Base Model (trained on CLPsych 2015 Dataset) and the Mixed Model (trained on the CLPsych 2015 Dataset and a half of the dataset of scraped tweets) could detect potential users in Twitter for depression more than half of the time by demonstrating an Area under the Receiver Operating Curve (AUC) score of 65% and 63%, respectively, when evaluated using the test dataset. These models performed comparably in identifying potential depressed users in Twitter given that there was no significant difference in their AUC scores when subjected to a z-test at 95% confidence interval and 0.05 level of significance (p = 0.21). These results suggest DistilBERT, when fine-tuned, may be used to detect potential users in Twitter for depression.",
        "link": "http://dx.doi.org/10.54941/ahfe1001458"
    },
    {
        "id": 9505,
        "title": "Personality Detection on Reddit Using DistilBERT",
        "authors": "Alif Rahmat Julianda, Warih Maharani",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "Personality is a unique set of motivations, feelings, and behaviors humans possess. Personality detection on social media is a research topic commonly conducted in computer science. Personality models often used for personality detection research are the Big Five Indicator (BFI) and Myers-Briggs Type Indicator (MBTI) models. Unlike the BFI, which classifies personalities based on an individual’s traits, the MBTI model classifies personalities based on the type of the individual. So, MBTI performs better in several scenarios than the Big Five model. Many studies use machine learning to detect personality on social media, such as Logistic Regression, Naïve Bayes, and Support Vector Machine. With the recent popularity of Deep Learning, we can use language models such as DistilBERT to classify personality on social media. Because of DistilBERT’s ability to process large sentences and the ability for parallelization thanks to the transformer architecture. Therefore, the proposed research will detect MBTI personality on Reddit using DistilBERT. The evaluation shows that removing stopwords on the data preprocessing stage can reduce the model’s performance, and with class imbalance handling, DistilBERT performs worse than without class imbalance handling. Also, as a comparison, DistilBERT outperforms other machine learning classifiers such as Naïve Bayes, SVM, and Logistic Regression in accuracy, precision, recall, and f1-score. ",
        "link": "http://dx.doi.org/10.29207/resti.v7i5.5236"
    },
    {
        "id": 9506,
        "title": "Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts",
        "authors": "Macarious Abadeer",
        "published": "2020",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.clinicalnlp-1.18"
    },
    {
        "id": 9507,
        "title": "Roman Urdu Sentiment Analysis Using Pre-trained DistilBERT and XLNet",
        "authors": "Nikhar Azhar, Seemab Latif",
        "published": "2022-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wids-psu54548.2022.00027"
    },
    {
        "id": 9508,
        "title": "Towards smarter hiring: resume parsing and ranking with YOLOv5 and DistilBERT",
        "authors": "Shakti Kinger, Divija Kinger, Shivam Thakkar, Devashish Bhake",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18778-9"
    },
    {
        "id": 9509,
        "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection",
        "authors": "Rafel Palliser-Sans, Albert Rial-Farràs",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.semeval-1.131"
    },
    {
        "id": 9510,
        "title": "Deep Multimodal Architecture for Detection of Long Parameter List and Switch Statements using DistilBERT",
        "authors": "Anushka Bhave, Roopak Sinha",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/scam55253.2022.00018"
    },
    {
        "id": 9511,
        "title": "DeLT Net: Unveiling Sponsor Segments in YouTube Videos with DistilBert, LSTM, and DeiT fusion models",
        "authors": "Shreyas Dongre, Shrushti Mehta",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10308354"
    },
    {
        "id": 9512,
        "title": "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA",
        "authors": "Ieva Staliūnaitė, Ignacio Iacobacci",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.573"
    },
    {
        "id": 9513,
        "title": "Construction of Domain-Specific DistilBERT Model by Using Fine-Tuning",
        "authors": "Jing Bai, Rui Cao, Wen Ma, Hiroyuki Shinnou",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai51410.2020.00051"
    },
    {
        "id": 9514,
        "title": "The Utilization of DistilBERT Features Space for Topics Suggestion in the Context of Learning Content",
        "authors": "Kenny Jingga, Hidayaturrahman Hidayaturrahman",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwaiip58158.2023.10462787"
    },
    {
        "id": 9515,
        "title": "A Hotspot-Aware Personalized News Recommendation Mechanism Based on DistilBERT-TC-MA",
        "authors": "Qian He, Ke Wang",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "Aiming at the problems of existing news recommendation methods, such as inadequate exploration of the semantic information of news, neglecting potential hotspot features of news, and challenging the balance between user preferences and hotspot features, a hotspot-aware personalized news recommendation model (DistilBERT-TC-MA) is suggested, which integrates the distilled version of BERT (DistilBERT), text convolutional neural network (TextCNN), and multilayer attention (MA). First, it takes full advantage of DistilBERT, TextCNN, and self-attention mechanism to achieve news encoding. Following this, representations of trending news are dynamically aggregated using the attention mechanism, while user preferences are mined utilizing user click history. Finally, in order to successfully accomplish the click prediction of candidate news, the hotspot features, user preferences, and candidate news are ultimately combined using a click predictor. The experimental results of the suggested DistilBERT-TC-MA model on MIND dataset are better than several other advanced methods.",
        "link": "http://dx.doi.org/10.4018/ijdst.339565"
    },
    {
        "id": 9516,
        "title": "INTERFACES SIMILARITY ANALYSIS FOR PROGRESSIVE WEB APPS AND WEB-APPLICATIONS BASED ON DISTILBERT TRANSFORMER",
        "authors": "H.A. YEHOSHYNA,  , S.M. VORONOY, O.I. POLIKAROVSKYKH, R.O. GOKHMAN,  ,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "An approach to automated testing of components of Progressive Web Applications interfaces by determining their relevance to elements of the corresponding web versions of applications is proposed. An analysis of modern trends and existing categories in the field of Web Mining was carried out. It is shown that the predominant trend in the analysis of the interface structures of modern web applications is the use of Deep Learning technologies. Features and functioning of the latest Transformers neural network architecture are considered. The choice of the Transformers type model to determine the correspondence between the site structure and the PWA application interface is justified. It is shown that in the comparison of fragments of the interfaces of the web service and the PWA application, some elements have more impact (weight) than others. It is proposed to use the mechanism of multidimensional \"self-attention\" to take into account this feature of the content. It is shown that the analysis of correspondence of interfaces is a task of binary classification. Features of transformers of the Bidirectional Encoder Representations (BERT) type are viewed. Pretrained BERT model can be configured with only one additional output layer to create modern and powerful models for a wide range of problems. It is proposed to use transfer learning, namely the DistilBERT model and its fine tuning using the DistilBertForSequenceClassification class. For the basic architecture of DistillBert (embedding and encoder layers), the weights of the English-language model \"distilbert-base-uncased-finetuned-sst-2-english\" were used. The model was optimized using a modification of the Adam stochastic gradient descent method. It is also suggested to use a low learning rate to avoid \"forgetting\". The features of data preprocessing using DistilBertTokenizer are shown. The architecture of the model was designed and its research was done based on data set of CSS properties, which provide styling and layout of interface elements.",
        "link": "http://dx.doi.org/10.31474/1996-1588-2023-1-36-51-60"
    },
    {
        "id": 9517,
        "title": "Studi Empiris Model BERT dan DistilBERT Analisis Sentimen pada Pemilihan Presiden Indonesia",
        "authors": " Mahira Putri, Taufik Edy Sutanto, Suma Inna",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "Peningkatan jumlah pengguna media sosial di Indonesia sejak tahun 2014 menyebabkan data yang dihasilkan semakin besar dan kompleks, sehingga komputasi yang  diperlukan untuk mengolahnya juga semakin besar. Untuk melakukan komputasi pada data yang besar diperlukan model yang kompatibel, efektif, dan efisien. Penelitian ini adalah kajian numerik dari dua model terbaik Deep Learning saat paper ini ditulis, yaitu BERT dan DistilBERT pada kasus analisis sentimen menggunakan ratusan ribu tweet terkait pemilihan presiden Indonesia tahun 2014 dan 2019. Analisis yang dilakukan meliputi waktu eksekusi dan konsumsi memori. Pada model dengan nilai hyperparameter optimal, tercatat bahwa DistilBERT melakukan proses pelatihan dan prediksi 84% lebih cepat dengan penggunaan memori GPU 79% lebih efisien dengan nilai akurasi tidak terpaut jauh, yaitu 0.89 dan 0.85 untuk BERT dan DistilBERT. Hasil kajian ini dapat digunakan untuk memperkirakan besarnya sumberdaya komputasi atau biaya yang dibutuhkan ketika menggunakan model BERT atau DistilBERT pada data yang besar.",
        "link": "http://dx.doi.org/10.33022/ijcs.v12i5.3445"
    },
    {
        "id": 9518,
        "title": "A Population-based Plagiarism Detection using DistilBERT-Generated Word Embedding",
        "authors": "Yuqin JING, Ying LIU",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140868"
    },
    {
        "id": 9519,
        "title": "Leveraging Transfer learning techniques- BERT, RoBERTa, ALBERT and DistilBERT for Fake Review Detection",
        "authors": "Priyanka Gupta, Shriya Gandhi, Bharathi Raja Chakravarthi",
        "published": "2021-12-13",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3503162.3503169"
    },
    {
        "id": 9520,
        "title": "BERTino: an Italian DistilBERT model",
        "authors": "Matteo Muffo, Enrico Bertino",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.aaccademia.8748"
    },
    {
        "id": 9521,
        "title": "Automated Scoring of English Essays in CEFR Levels using LSTM and DistilBERT Embeddings",
        "authors": "Nabelanita Utami, Fariska Zakhralativa Ruskanda",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390038"
    },
    {
        "id": 9522,
        "title": "Membandingkan Nilai Akurasi BERT dan DistilBERT pada Dataset Twitter",
        "authors": "Faisal Fajri, Bambang Tutuko, Sukemi Sukemi",
        "published": "2022-12-31",
        "citations": 0,
        "abstract": "The growth of digital media has been incredibly fast, which has made consuming information a challenging task. Social media processing aided by Machine Learning has been very helpful in the digital era. Sentiment analysis is a fundamental task in Natural Language Processing (NLP). Based on the increasing number of social media users, the amount of data stored in social media platforms is also growing rapidly. As a result, many researchers are conducting studies that utilize social media data. Opinion mining (OM) or Sentiment Analysis (SA) is one of the methods used to analyze information contained in text from social media. Until now, several other studies have attempted to predict Data Mining (DM) using remarkable data mining techniques. The objective of this research is to compare the accuracy values of BERT and DistilBERT. DistilBERT is a technique derived from BERT that provides speed and maximizes classification. The research findings indicate that the use of DistilBERT method resulted in an accuracy value of 97%, precision of 99%, recall of 99%, and f1-score of 99%, which is higher compared to BERT that yielded an accuracy value of 87%, precision of 91%, recall of 91%, and f1-score of 89%.",
        "link": "http://dx.doi.org/10.19109/jusifo.v8i2.13885"
    },
    {
        "id": 9523,
        "title": "Are You Depressed? Analyze User Utterances to Detect Depressive Emotions Using DistilBERT",
        "authors": "Jaedong Oh, Mirae Kim, Hyejin Park, Hayoung Oh",
        "published": "2023-5-19",
        "citations": 2,
        "abstract": "This paper introduces the Are u Depressed (AuD) model, which aims to detect depressive emotional intensity and classify detailed depressive symptoms expressed in user utterances. The study includes the creation of a BWS dataset using a tool for the Best-Worst Scaling annotation task and a DSM-5 dataset containing nine types of depression annotations based on major depressive disorder (MDD) episodes in the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). The proposed model employs the DistilBERT model for both tasks and demonstrates superior performance compared to other machine learning and deep learning models. We suggest using our model for real-time depressive emotion detection tasks that demand speed and accuracy. Overall, the AuD model significantly advances the accurate detection of depressive emotions in user utterances.",
        "link": "http://dx.doi.org/10.3390/app13106223"
    },
    {
        "id": 9524,
        "title": "An Empirical Comparison of DistilBERT, Longformer and Logistic Regression for Predictive Coding",
        "authors": "Fusheng Wei, Jingchao Yang, Qiang Mao, Han Qin, Adam Dabrowski",
        "published": "2022-12-17",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020486"
    },
    {
        "id": 9525,
        "title": "Named Entity Recognition using Knowledge Graph Embeddings and DistilBERT",
        "authors": "Shreyansh Mehta, Mansi Radke, Sagar Sunkle",
        "published": "2021-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3508230.3508252"
    },
    {
        "id": 9526,
        "title": "SM-DBERT: A Novel Symptom-based Technique for Chronic Disease Classification using DISTILBERT",
        "authors": "Et al. Swati Saigaonkar",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "Machine learning and deep learning models when applied on EHR systems are considerably augmenting the prediction tasks performed on medical data. Humongous amount of information lies in the free form clinical texts. But there exist challenges associated with such kinds of unstructured data. Transformers based models like Bidirectional Encoder Representations from Transformers (BERT) has revolutionized the work. DISTILBERT, a lighter version of BERT, is even promising as the time required is reduced to nearly one-third without losing the performance. In this research work, we present SM-DBERT, Symptom-based Modified DistilBERT architecture designed for Chronic Diseases. The foundation of SM-DBERT is symptomatology, as an optimal model should prioritize symptoms as they are the key indicators. The existing DISTILBERT architecture has been modified by introducing additional layers and extra embeddings of external knowledge and presented along with input ids and attention masks. These extra knowledge helps the model to learn more relevant information.  SM-DBERT has demonstrated notable improvement in the results. The accuracy obtained with this novel approach is 0.98 as against the basic DISTILBERT model.",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i9.9275"
    },
    {
        "id": 9527,
        "title": "Thai Tokenizer Invariant Classification Based on Bi-LSTM and DistilBERT Encoders",
        "authors": "Noppadol Kongsumran, Suphakant Phimoltares, Sasipa Panthuwadeethorn",
        "published": "2022-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecti-con54298.2022.9795573"
    },
    {
        "id": 9528,
        "title": "Exploring Transformers models for Emotion Recognition: a comparision of BERT, DistilBERT, RoBERTa, XLNET and ELECTRA",
        "authors": "Diogo Cortiz",
        "published": "2022-8-26",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3562007.3562051"
    },
    {
        "id": 9529,
        "title": "Combining compact news representations generated using DistilBERT and topological features to classify fake news",
        "authors": "Carlos Abel Córdova Sáenz, Marcelo Dias, Karin Becker",
        "published": "2020-10-20",
        "citations": 0,
        "abstract": "Fake news (FN) have affected people’s lives in unimaginable ways. The automatic classification of FN is a vital tool to prevent their dissemination and support fact-checking. Related work has shown that FN spread faster, deeper, and more broadly than the truth on social media. Besides, deep learning has produced state-of-the-art solutions in this field, mainly based on textual attributes. In this paper, we propose initial experiments to combine compact representations of the textual news properties generated using DistilBERT, with topological metrics extracted from the social propagation network. Using a dataset related to politics and five distinct classification algorithms, our results are encouraging. Regarding the textual attributes, we reached results comparable to state-of-the-art solutions using only the news title and contents, which is useful for FN early detection. The topological attributes were not as effective, but the promising results encourage the investigation of alternative architectures for their combination",
        "link": "http://dx.doi.org/10.5753/kdmile.2020.11978"
    },
    {
        "id": 9530,
        "title": "UoR at SemEval-2021 Task 7: Utilizing Pre-trained DistilBERT Model and Multi-scale CNN for Humor Detection",
        "authors": "Zehao Liu, Carl Haines, Huizhi Liang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.semeval-1.166"
    },
    {
        "id": 9531,
        "title": "Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of DistilBERT",
        "authors": "Jaimeen Ahn, Hwaran Lee, Jinhwa Kim, Alice Oh",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gebnlp-1.27"
    },
    {
        "id": 9532,
        "title": "A Transformer Based Model for Bangla Fake News Detection Using DistilBERT",
        "authors": "Md. Nazmul Abdal, Most. Humayera Kabir Oshie, Md. Azizul Haque, Salman Rahman",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sti59863.2023.10465127"
    },
    {
        "id": 9533,
        "title": "Análise de Performance dos Modelos Gerais de Aprendizado de Máquina Pré-Treinados: BERT vs DistilBERT",
        "authors": "Rafael Silva Barbon, Ademar Takeo Akabane",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "Modelos de aprendizado de máquina (AM) vêm sendo amplamente utilizados devido à elevada quantidade de dados produzidos diariamente. Dentre eles, destaca-se os modelos pré-treinados devido a sua eficácia, porém estes normalmente demandam um elevado custo computacional na execução de sua tarefa. A fim de contornar esse problema, técnicas de compressão de redes neurais vem sendo aplicadas para produzir modelos pré-treinados menores sem comprometer a acurácia. Com isso, neste trabalho foram utilizados dois diferentes modelos pré-treinados de AM: BERT e DistilBERT na classificação de texto. Os resultados apontam que modelos menores apresentam bons resultados quando comparados com seus equivalentes maiores.",
        "link": "http://dx.doi.org/10.5753/sbrc_estendido.2022.223391"
    },
    {
        "id": 9534,
        "title": "Analisis DistilBERT  dengan Support Vector Machine (SVM) untuk Klasifikasi Ujaran Kebencian pada Sosial Media Twitter",
        "authors": "Naufal Azmi Verdikha, Reza Habid, Asslia Johar Latipah",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Hate speech is a significant issue in content management on social media platforms. Effective classification of hate speech plays a crucial role in maintaining a safe social media environment, combating discrimination, and protecting users. This study evaluates a hate speech classification model using SVM with linear and polynomial kernels. The dataset used consists of labeled Indonesian-language tweets. The importance of developing an effective classification model to address hate speech has led to the utilization of DistilBERT as a feature extraction method. However, DistilBERT has high-dimensional features, necessitating dimensionality reduction to reduce model complexity. Therefore, in this study, the PCA dimensionality reduction method is implemented with various scenarios of dimensionality, namely 10, 20, 30, 40, and 50. Evaluation is performed using F1-Score, and the entire study is evaluated using 10-fold cross-validation. The evaluation results indicate that in the scenario with a linear kernel, the model achieves the highest F1-Score of 0.75 in the 50-dimensional scenario. Meanwhile, in the scenario with a polynomial kernel, the model achieves the highest F1-Score of 0.7857 in the 50-dimensional scenario. These findings demonstrate that the use of a polynomial kernel with 50 dimensions yields the best performance in classifying hate speech.",
        "link": "http://dx.doi.org/10.47002/metik.v7i2.583"
    },
    {
        "id": 9535,
        "title": "Comparative Analyses of Bert, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition",
        "authors": "Acheampong Francisca Adoma, Nunoo-Mensah Henry, Wenyu Chen",
        "published": "2020-12-18",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip51612.2020.9317379"
    },
    {
        "id": 9536,
        "title": "McRock at SemEval-2022 Task 4: Patronizing and Condescending Language Detection using Multi-Channel CNN, Hybrid LSTM, DistilBERT and XLNet",
        "authors": "Marco Siino, Marco Cascia, Ilenia Tinnirello",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.55"
    },
    {
        "id": 9537,
        "title": "DistilBERT-based Text Classification for Automated Diagnosis of Mental Health Conditions",
        "authors": " Diwakar, Deepa Raj",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-9621-6_6"
    },
    {
        "id": 9538,
        "title": "Assessing the combination of DistilBERT news representations and difusion topological features to classify fake news",
        "authors": "Carlos Abel Córdova Sáenz, Marcelo Dias, Karin Becker",
        "published": "2021-8-5",
        "citations": 1,
        "abstract": "Fake news (FN) have affected people’s lives in unimaginable ways. The automatic classification of FN is a vital tool to prevent their dissemination and support fact-checking. Related work has shown that FN spread faster, deeper, and more broadly than truthful news on social media. Deep learning has produced state-of-the-art solutions in this field, mainly based on textual attributes. In this paper, we propose to combine compact representations of the textual news properties generated using DistilBERT, with topological metrics extracted from their propagation network in social media. Using a dataset related to politics and distinct learning algorithms, we extensively assessed the components of the proposed solution. Regarding the textual attributes, we reached results comparable to stateof-the-art solutions using only the news title and contents, which is useful for FN early detection. We assessed the influential topological metrics, and the effect of their combination with the news textual features. We also explored the use of ensembles. Our results were very promising, revealing the potential of the features proposed and the adoption of ensembles.",
        "link": "http://dx.doi.org/10.5753/jidm.2021.1895"
    },
    {
        "id": 9539,
        "title": "Automatic Extractive Summarization using GAN Boosted by DistilBERT Word Embedding and Transductive Learning",
        "authors": "Dongliang Li, Youyou Li, Zhigang ZHANG",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141107"
    },
    {
        "id": 9540,
        "title": "Improving Crisis Events Detection Using DistilBERT with Hunger Games Search Algorithm",
        "authors": "Hadeer Adel, Abdelghani Dahou, Alhassan Mabrouk, Mohamed Abd Elaziz, Mohammed Kayed, Ibrahim Mahmoud El-Henawy, Samah Alshathri, Abdelmgeid Amin Ali",
        "published": "2022-1-30",
        "citations": 24,
        "abstract": "This paper presents an alternative event detection model based on the integration between the DistilBERT and a new meta-heuristic technique named the Hunger Games Search (HGS). The DistilBERT aims to extract features from the text dataset, while a binary version of HGS is developed as a feature selection (FS) approach, which aims to remove the irrelevant features from those extracted. To assess the developed model, a set of experiments are conducted using a set of real-world datasets. In addition, we compared the binary HGS with a set of well-known FS algorithms, as well as the state-of-the-art event detection models. The comparison results show that the proposed model is superior to other methods in terms of performance measures.",
        "link": "http://dx.doi.org/10.3390/math10030447"
    },
    {
        "id": 9541,
        "title": "Online News Sentiment Classification Using DistilBERT",
        "authors": "Samuel Kofi Akpatsa, Hang Lei, Xiaoyu Li, Victor-Hillary Kofi Setornyo Obeng, Ezekiel Mensah Martey, Prince Clement Addo, Duncan Dodzi Fiawoo",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/jqc.2022.026658"
    },
    {
        "id": 9542,
        "title": "Natural language processing analysis applied to COVID-19 open-text opinions using a distilBERT model for sentiment categorization",
        "authors": "Mario Jojoa, Parvin Eftekhar, Behdin Nowrouzi-Kia, Begonya Garcia-Zapirain",
        "published": "2022-11-21",
        "citations": 3,
        "abstract": "AbstractCOVID-19 is a disease that affects the quality of life in all aspects. However, the government policy applied in 2020 impacted the lifestyle of the whole world. In this sense, the study of sentiments of people in different countries is a very important task to face future challenges related to lockdown caused by a virus. To contribute to this objective, we have proposed a natural language processing model with the aim to detect positive and negative feelings in open-text answers obtained from a survey in pandemic times. We have proposed a distilBERT transformer model to carry out this task. We have used three approaches to perform a comparison, obtaining for our best model the following average metrics: Accuracy: 0.823, Precision: 0.826, Recall: 0.793 and F1 Score: 0.803. ",
        "link": "http://dx.doi.org/10.1007/s00146-022-01594-w"
    },
    {
        "id": 9543,
        "title": "A Web Attack Detection Method Based on DistilBERT and Feature Fusion for Power Micro-Application Server",
        "authors": "Li Nige, Chen Lu, Zhao Lei, Teng Zhenning, Wang Zhiqiang, Shao Yiyang, Gui Xiaolin",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeege58828.2023.00010"
    },
    {
        "id": 9544,
        "title": "Audio DistilBERT: A Distilled Audio BERT for Speech Representation Learning",
        "authors": "Fan Yu, Jiawei Guo, Wei Xi, Zhao Yang, Rui Jiang, Chao Zhang",
        "published": "2021-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533328"
    },
    {
        "id": 9545,
        "title": "Performance Analysis of Federated Learning Algorithms for Multilingual Protest News Detection Using Pre-Trained DistilBERT and BERT",
        "authors": "Pascal Riedel, Manfred Reichert, Reinhold Von Schwerin, Alexander Hafner, Daniel Schaudt, Gaurav Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3334910"
    },
    {
        "id": 9546,
        "title": "Leveraging DistilBERT for Summarizing Arabic Text: An Extractive Dual-Stage Approach",
        "authors": "Abdullah Alshanqiti, Abdallah Namoun, Aeshah Alsughayyir, Aisha Mousa Mashraqi, Abdul Rehman Gilal, Sami Saad Albouq",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3113256"
    },
    {
        "id": 9547,
        "title": "Predicting Bug-Fixing Time: DistilBERT Versus Google BERT",
        "authors": "Pasquale Ardimento",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-21388-5_46"
    },
    {
        "id": 9548,
        "title": "A Feasible and Explainable Network Traffic Classifier Utilizing DistilBERT",
        "authors": "Chang-Yui Shin, Jee-Tae Park, Ui-Jun Baek, Myung-Sup Kim",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3293105"
    },
    {
        "id": 9549,
        "title": "The DistilBERT Model: A Promising Approach to Improve Machine Reading Comprehension Models",
        "authors": "Sahana V, Nagamani H Shahapure, Rekha PM, Nethravathi B, Pratiksha Khandelwal, Abhinav Anand, Pranjal Agrawal, Vedant Srivastava",
        "published": "2023-9-20",
        "citations": 1,
        "abstract": "Machine Reading Comprehension (MRC) is a challenging task in the field of Natural Language Processing (NLP), where a machine is required to read a given text passage and answer a set of questions based on it. This paper provides an overview of recent advances in MRC and highlights some of the key challenges and future directions of this research area. It also evaluates the performance of several baseline models on the dataset, evaluates the challenges that the dataset poses for existing MRC models, and introduces the DistilBERT model to improve the accuracy of the answer extraction process. The supervised paradigm for training machine reading and comprehension models represents a practical path forward for creating comprehensive natural language understanding systems. To enhance the DistilBERT basic model's functionality, we have experimented with a variety of question heads that differ in the number of layers, activation function, and general structure. DistilBERT is a model for question-resolution tasks that is successful and delivers state-of-the-art performance while requiring less computational resources than large models like BERT, according to the presented technique. We could enhance the model's functionality and obtain a better understanding of how the model functions by investigating other question head architectures. These findings could serve as a foundation for future study on how to make question-and-answer systems and other tasks connected to the processing of natural languages.  ",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i8.7957"
    },
    {
        "id": 9550,
        "title": "Flood Severity Assessment Using DistilBERT and NER",
        "authors": "S. N. Gokul Raj, P. Chitra, A. K. Silesh, R. Lingeshwaran",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-0047-3_34"
    },
    {
        "id": 9551,
        "title": "Analyzing DistilBERT for Sentiment Classification of Banking Financial News",
        "authors": "Varun Dogra, Aman Singh, Sahil Verma,  Kavita, N. Z. Jhanjhi, M. N. Talib",
        "published": "2021",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-3153-5_53"
    },
    {
        "id": 9552,
        "title": "Sh-DistilBERT: New Transfer Learning Model for Arabic Sentiment Analysis and Aspect Category Detection",
        "authors": "Hasna Chouikhi, Fethi Jarray",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-41774-0_22"
    },
    {
        "id": 9553,
        "title": "Emotion-Sentence-DistilBERT: A Sentence-BERT-Based Distillation Model for Text Emotion Classification",
        "authors": "Haoyu Wang, Xin Kang, Fuji Ren",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7943-9_27"
    },
    {
        "id": 9554,
        "title": "Multi-task sentiment classification model based on DistilBert and multi-scale CNN",
        "authors": "Guanghao Xiong, Ke Yan",
        "published": "2021-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc-picom-cbdcom-cyberscitech52372.2021.00117"
    },
    {
        "id": 9555,
        "title": "Prediction of Personality Type with Myers–Briggs Type Indicator Using DistilBERT",
        "authors": "Suresh Kumar Grandh, K. Adi Narayana Reddy, D. Durga Prasad, L. Lakshmi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-6702-5_52"
    },
    {
        "id": 9556,
        "title": "An Investigation of Structures Responsible for Gender Bias in BERT and DistilBERT",
        "authors": "Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Christophe Gravier",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-30047-9_20"
    },
    {
        "id": 9557,
        "title": "Towards Transfer Learning Techniques—BERT, DistilBERT, BERTimbau, and DistilBERTimbau for Automatic Text Classification from Different Languages: A Case Study",
        "authors": "Rafael Silva Barbon, Ademar Takeo Akabane",
        "published": "2022-10-26",
        "citations": 7,
        "abstract": "The Internet of Things is a paradigm that interconnects several smart devices through the internet to provide ubiquitous services to users. This paradigm and Web 2.0 platforms generate countless amounts of textual data. Thus, a significant challenge in this context is automatically performing text classification. State-of-the-art outcomes have recently been obtained by employing language models trained from scratch on corpora made up from news online to handle text classification better. A language model that we can highlight is BERT (Bidirectional Encoder Representations from Transformers) and also DistilBERT is a pre-trained smaller general-purpose language representation model. In this context, through a case study, we propose performing the text classification task with two previously mentioned models for two languages (English and Brazilian Portuguese) in different datasets. The results show that DistilBERT’s training time for English and Brazilian Portuguese was about 45% faster than its larger counterpart, it was also 40% smaller, and preserves about 96% of language comprehension skills for balanced datasets.",
        "link": "http://dx.doi.org/10.3390/s22218184"
    },
    {
        "id": 9558,
        "title": "Comparison Between SVM and DistilBERT for Multi-label Text Classification of Scientific Papers Aligned with Sustainable Development Goals",
        "authors": "Roberto Carlos Morales-Hernández, David Becerra-Alonso, Eduardo Romero Vivas, Joaquín Gutiérrez",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-19496-2_5"
    },
    {
        "id": 9559,
        "title": "A Sybil Detection Method in OSN Based on DistilBERT and Double-SN-LSTM for Text Analysis",
        "authors": "Xiaojie Xu, Jian Dong, Zhengyu Liu, Jin Yang, Bin Wang, Zhaoyuan Wang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-90022-9_4"
    }
]