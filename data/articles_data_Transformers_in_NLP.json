[
    {
        "id": 2401,
        "title": "Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments",
        "authors": "Danial Kamali, Parisa Kordjamshidi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.genbench-1.10"
    },
    {
        "id": 2402,
        "title": "Transformers Go for the LOLs: Generating (Humourous) Titles from Scientific Abstracts End-to-End",
        "authors": "Yanran Chen, Steffen Eger",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eval4nlp-1.6"
    },
    {
        "id": 2403,
        "title": "Transformers in the Real World: A Survey on NLP Applications",
        "authors": "Narendra Patwardhan, Stefano Marrone, Carlo Sansone",
        "published": "2023-4-17",
        "citations": 13,
        "abstract": "The field of Natural Language Processing (NLP) has undergone a significant transformation with the introduction of Transformers. From the first introduction of this technology in 2017, the use of transformers has become widespread and has had a profound impact on the field of NLP. In this survey, we review the open-access and real-world applications of transformers in NLP, specifically focusing on those where text is the primary modality. Our goal is to provide a comprehensive overview of the current state-of-the-art in the use of transformers in NLP, highlight their strengths and limitations, and identify future directions for research. In this way, we aim to provide valuable insights for both researchers and practitioners in the field of NLP. In addition, we provide a detailed analysis of the various challenges faced in the implementation of transformers in real-world applications, including computational efficiency, interpretability, and ethical considerations. Moreover, we highlight the impact of transformers on the NLP community, including their influence on research and the development of new NLP models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14040242"
    },
    {
        "id": 2404,
        "title": "The NLP Task Effectiveness of Long-Range Transformers",
        "authors": "Guanghui Qin, Yukun Feng, Benjamin Van Durme",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.273"
    },
    {
        "id": 2405,
        "title": "Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT)",
        "authors": "Robert Gale, Alexandra Salem, Gerasimos Fergadiotis, Steven Bedrick",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.18"
    },
    {
        "id": 2406,
        "title": "Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM",
        "authors": "√ñyk√º Berfin Mercan, Sena Nur Cavsak, Aysu Deliahmetoglu, Senem Tanberk",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asyu58738.2023.10296563"
    },
    {
        "id": 2407,
        "title": "ResearchTeam_HCN at SemEval-2023 Task 6: A knowledge enhanced transformers based legal NLP system",
        "authors": "Dhanachandra Ningthoujam, Pinal Patel, Rajkamal Kareddula, Ramanand Vangipuram",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.173"
    },
    {
        "id": 2408,
        "title": "NLP Transformers: Enhanced Text Summarization and Language Understanding",
        "authors": "Yunus Emre I≈ûIKDEMƒ∞R",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "As the amount of the available information continues to grow, finding the relevant information has become increasingly challenging. As a solution, text summarization has emerged as a vital method for extracting essential information from lengthy documents. There are various techniques available for filtering documents and extracting the pertinent information. In this study, a comparative analysis is conducted to evaluate traditional approaches and state-of-the-art methods on the BBC News and CNN/DailyMail datasets. This study offers valuable insights for researchers to advance their research and helps practitioners in selecting the most suitable techniques for their specific use cases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.31796/ogummf.1303569"
    },
    {
        "id": 2409,
        "title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers",
        "authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.40"
    },
    {
        "id": 2410,
        "title": "NCUEE-NLP at WASSA 2023 Shared Task 1: Empathy and Emotion Prediction Using Sentiment-Enhanced RoBERTa Transformers",
        "authors": "Tzu-Mi Lin, Jung-Ying Chang, Lung-Hao Lee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.49"
    },
    {
        "id": 2411,
        "title": "Establishing an Optimal Online Phishing Detection Method: Evaluating Topological NLP Transformers on Text Message Data",
        "authors": "Helen Milner,  , Michael Baron,  ",
        "published": "2023-7-12",
        "citations": 1,
        "abstract": "This research establishes an optimal classification model for online SMS spam detection by utilizing topological sentence transformer methodologies. The study is a response to the increasing sophisticated and disruptive activities of malicious actors.We present a viable lightweight integration of pre-trained NLP repository models with sklearn functionality. The study design mirrors the spaCy pipeline component architecture in a downstream sklearn pipeline implementation and introduces a user-extensible spam SMS solution. We leverage large-text data models from HuggingFace (RoBERTa-base) via spaCy and apply linguistic NLP transformer methods to short-sentence NLP datasets. We compare the F1-scores of models and iteratively retest models using a standard sklearn pipeline architecture. Applying spaCy transformer modelling achieves an optimal F1-score of 0.938, a result comparable to existing research output from contemporary BERT/SBERT/‚Äòblack box‚Äô predictive models. This research introduces a lightweight, user-interpretable, standardized, predictive SMS spam detection model that utilizes semantically similar paraphrase/sentence transformer methodologies and generates optimal F1-scores for an SMS dataset. Significant F1-scores are also generated for a Twitter evaluation set, indicating potential real-world suitability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47852/bonviewjdsis32021131"
    },
    {
        "id": 2412,
        "title": "NCUEE-NLP at SemEval-2023 Task 7: Ensemble Biomedical LinkBERT Transformers in Multi-evidence Natural Language Inference for Clinical Trial Data",
        "authors": "Chao-Yi Chen, Kao-Yuan Tien, Yuan-Hao Cheng, Lung-Hao Lee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.107"
    },
    {
        "id": 2413,
        "title": "Unleashing the Power of NLP and Transformers: A Game-Changer in Medical Research and Clinical Practice and a revolution of Medical Text Analysis.",
        "authors": "Soufyane Ayanouz, Anouar abdelhakim Boudhir, Mohamed Ben Ahmed",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607720.3607787"
    },
    {
        "id": 2414,
        "title": "iSAI-NLP 2023 Committee",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354614"
    },
    {
        "id": 2415,
        "title": "iSAI-NLP 2023 Tentative Program",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354669"
    },
    {
        "id": 2416,
        "title": "iSAI-NLP 2023 Copyright Page",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354677"
    },
    {
        "id": 2417,
        "title": "iSAI-NLP 2023 Cover Page",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354898"
    },
    {
        "id": 2418,
        "title": "iSAI-NLP 2023 Message from General Chair",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354579"
    },
    {
        "id": 2419,
        "title": "iSAI-NLP 2023 Message from Program Chairs",
        "authors": "",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354885"
    },
    {
        "id": 2420,
        "title": "What aspects of NLP models and brain datasets affect brain-NLP alignment?",
        "authors": "SUBBA REDDY OOTA, Mariya Toneva",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1273-0"
    },
    {
        "id": 2421,
        "title": "ANALIZA OPRAVDANOSTI INVESTICIONIH TRO≈†KOVA NOVIH EFIKASNIH ENERGETSKIH TRANSFROMATORA TOKOM PERIODA NJIHOVE EKSPLOATACIJE",
        "authors": "Petar Markoviƒá,  , Ana Pastor, Milan ƒÜeloviƒá, Jelena ƒÜeloviƒá, Miroslav Bosanƒçiƒá,  ,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "Due to the goal of increasing energy efficiency in the power system, engineering solutions are required to reduce power losses. Although the efficiency of power transformers is high, considering that they are one of the most expensive components of the power system and are designed to operate for many years, increasing the efficiency and cost-effectiveness of transformers is necessary. This paper provides a theoretical overview of the types and causes of power losses in power transformers, as well as a review of engineering solutions for their reduction. The analysis focuses on high-voltage power transformers rated at 110 kV, with an overview of energy savings over the average life of a power transformer designed to traditional and modern requirements. The conclusions drawn in this paper can be useful for defining power loss requirements in transformers and for calculating the cost-effectiveness of replacing old, less efficient power transformers with new ones.",
        "keywords": "",
        "link": "http://dx.doi.org/10.46793/cigre36.0241m"
    },
    {
        "id": 2422,
        "title": "NLP+Vis: NLP Meets Visualization",
        "authors": "Shafiq Joty, Enamul Hoque, Jesse Vig",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-tutorial.1"
    },
    {
        "id": 2423,
        "title": "Umeed: VR Game Using NLP Models and Latent Semantic Analysis for Conversation Therapy for People with Speech Disorders",
        "authors": "",
        "published": "2023-8-19",
        "citations": 0,
        "abstract": "UmeedVR aims to create a conversational therapy VR game using natural language processing for patients with Speech Disorders like Autism or Aphasia. This study developed 5 psychological task sets and 3 environments via Maya and Unity. The Topic-Modeling AI, employing 25 live participants' recordings and 980+ TwineAI datasets, generated initial VR grading with a coherence score averaging 6.98 themes in 5-minute conversations across scenarios, forming a foundation for enhancements. Employing latent semantic analysis (gensimcorpus Python) and Term-Frequency-Inverse Document-Frequency (TF-IDF), grammatical errors and user-specific improvements were addressed. Results were visualized via audio-visual plots, highlighting conversation topics based on occurrence and interpretability. UMEED enhances cognitive and intuitive skills, elevating average topics from 6.98 to 13.56 in a 5- minute conversation with a 143.12 coherence score. LSA achieved 98.39% accuracy, topic modeling 100%. Significantly, real-time grammatical correction integration in the game was realized.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131408"
    },
    {
        "id": 2424,
        "title": "üöÄ NLP Colloquium",
        "authors": "Ayana Niwa, Sho Yokoi, Junya Takayama, Itsumi Saito",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.31.300"
    },
    {
        "id": 2425,
        "title": "News Signals: An NLP Library for Text and Time Series",
        "authors": "Chris Hokamp, Demian Ghalandari, Parsa Ghaffari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.21"
    },
    {
        "id": 2426,
        "title": "Do not Trust the Experts: How the Lack of Standard Complicates NLP for Historical Irish",
        "authors": "Oksana Dereza, Theodorus Fransen, John P. Mccrae",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.insights-1.10"
    },
    {
        "id": 2427,
        "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP",
        "authors": "Yoshitomo Matsubara",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.18"
    },
    {
        "id": 2428,
        "title": "The State of the Art of Natural Language Processing‚ÄîA Systematic Automated Review of NLP Literature Using NLP Techniques",
        "authors": "Jan Sawicki, Maria Ganzha, Marcin Paprzycki",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "ABSTRACT\nNowadays, natural language processing (NLP) is one of the most popular areas of, broadly understood, artificial intelligence. Therefore, every day, new research contributions are posted, for instance, to the arXiv repository. Hence, it is rather difficult to capture the current ‚Äústate of the field‚Äù and thus, to enter it. This brought the id-art NLP techniques to analyse the NLP-focused literature. As a result, (1) meta-level knowledge, concerning the current state of NLP has been captured, and (2) a guide to use of basic NLP tools is provided. It should be noted that all the tools and the dataset described in this contribution are publicly available. Furthermore, the originality of this review lies in its full automation. This allows easy reproducibility and continuation and updating of this research in the future as new researches emerge in the field of NLP.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/dint_a_00213"
    },
    {
        "id": 2429,
        "title": "A Validation of the Effectiveness of NLP Self Growth Program",
        "authors": "Jeongbeom Kang, Songhee Han",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22143/hss21.14.2.193"
    },
    {
        "id": 2430,
        "title": "Strengthening Relationships Between Indigenous Communities, Documentary Linguists, and Computational Linguists in the Era of NLP-Assisted Language Revitalization",
        "authors": "Darren Flavelle, Jordan Lachler",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.c3nlp-1.4"
    },
    {
        "id": 2431,
        "title": "Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP",
        "authors": "Anya Belz, Craig Thomson, Ehud Reiter",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.insights-1.1"
    },
    {
        "id": 2432,
        "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers",
        "authors": "Matthew Dutson, Yin Li, Mohit Gupta",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01551"
    },
    {
        "id": 2433,
        "title": "High-frequency transformers optimized design for power  electronic transformers",
        "authors": "Yijie Liu",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "Because of its small size, high frequency transformers are widely used to maximize energy transfer. However, the leakage inductance and distributed capacitance of high frequency transformer can not only cause resonance, but also lead to transient changes of voltage and current in high frequency, which can lead to voltage spike, so that the switch tube is damaged. For transformers with the same output power, high-frequency transformers are much smaller and have lower calorific value than low-frequency transformers. Therefore, at present, many consumer electronics and network product power adapters are switching power supplies, and the internal high-frequency transformer is the most important component of switching power supplies. The basic principle is to turn the input alternating current into DC first, and then turn it into high frequency through a transistor or FET, etc., through a high-frequency transformer to change voltage, and then rectify the output again, plus other control parts, and stabilize the output DC voltage. In this thesis, we choose a more rational and cost effective winding structure, choose a more appropriate core material based on the comparison of different core materials, research on the insulation and cooling properties of transformer so as to improve the insulation properties of the transformer, make it safer and more efficient. The study has important significance to decrease the power loss of high frequency transformer and decrease the size of high frequency transformer.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/10/20230174"
    },
    {
        "id": 2434,
        "title": "Transformer-based Hebrew NLP models for Short Answer Scoring in Biology",
        "authors": "Abigail Gurin Schleifer, Beata Beigman Klebanov, Moriah Ariely, Giora Alexandron",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.46"
    },
    {
        "id": 2435,
        "title": "Voicemail Urgency Detection Using Context Dependent and Independent NLP Techniques",
        "authors": "Asma Trabelsi, S√©verine Soussilane, Emmanuel Helbert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011685800003393"
    },
    {
        "id": 2436,
        "title": "PyTAIL: An Open Source Tool for Interactive and Incremental Learning of NLP Models with Human in the Loop for Online Data",
        "authors": "Shubhanshu Mishra, Jana Diesner",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.22"
    },
    {
        "id": 2437,
        "title": "Natural Language Processing (NLP) for Code in Python",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/resmil.v9i1.24"
    },
    {
        "id": 2438,
        "title": "Unveiling the Power of Pre - Trained Language Models in NLP Applications",
        "authors": "Shrinath Pai",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115202502"
    },
    {
        "id": 2439,
        "title": "Improving NLP Model Performance on Small Educational Data Sets Using Self-Augmentation",
        "authors": "Keith Cochran, Clayton Cohn, Peter Hastings",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011857200003470"
    },
    {
        "id": 2440,
        "title": "Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference",
        "authors": "Wangchunshu Zhou, Ronan Le Bras, Yejin Choi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.664"
    },
    {
        "id": 2441,
        "title": "T√ºrk√ße Duygu Sƒ±nƒ±flandƒ±rma ƒ∞√ßin Transformers Tabanlƒ± Mimarilerin Kar≈üƒ±la≈ütƒ±rƒ±lmalƒ± Analizi",
        "authors": "Mehmet ARZU, Murat AYDOƒûAN",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "Transformers tabanlƒ± ile duygu sƒ±nƒ±flandƒ±rma, son zamanlarda doƒüal dil i≈üleme ve makine √∂ƒürenmesi alanƒ±nda yaygƒ±n olarak √ßalƒ±≈üƒ±lan bir konudur. Metinler i√ßerisinde kar≈üƒ±la≈üƒ±lan duygusal ifadelerin anlamlandƒ±rƒ±lmasƒ± ve sƒ±nƒ±flandƒ±rƒ±lmasƒ±, sosyal medya analizi, piyasa ara≈ütƒ±rmasƒ±, kullanƒ±cƒ± deneyimleri vb. gibi kullanƒ±labileceƒüi bir√ßok alan mevcuttur. Bu sebeple, bu √ßalƒ±≈ümada Transformers tabanlƒ± mimariler kullanƒ±larak duygu sƒ±nƒ±flandƒ±rmasƒ±nƒ±n ger√ßekle≈ütirilmesi hedeflenmi≈ütir. Bu √ßalƒ±≈ümada, 150000 veriden olu≈üan TRSAv1 veriseti √ºzerinde, 8 farklƒ± BERTurk ve 2 farklƒ± ELECTRA varyasyonu √ºzerinde duygu sƒ±nƒ±flandƒ±rma i≈ülemi i√ßin kullanƒ±lmƒ±≈ütƒ±r. Bu modeller, T√ºrk√ße metinler √ºzerinde duygu sƒ±nƒ±flandƒ±rƒ±lmasƒ± √ßalƒ±≈ümalarƒ±nda kullanƒ±lmak i√ßin √∂nceden eƒüitilmi≈ü modellerdir. Veri seti √ºzerinde 3 farklƒ± metot kullanƒ±larak modeller eƒüitilmi≈ü ve sonu√ßlar kar≈üƒ±la≈ütƒ±rmalƒ± olarak deƒüerlendirilmi≈ütir. Yapƒ±lan deneyler sonucunda, modellerin duygu sƒ±nƒ±flandƒ±rma performanslarƒ± doƒüruluk ve F1-skor metrikleri kullanƒ±larak √∂l√ß√ºlm√º≈üt√ºr. Deney sonu√ßlarƒ±, Transformers modellerinin duygu sƒ±nƒ±flandƒ±rmasƒ± konusundaki etkinliƒüini ve kullanƒ±lan modellerin performans deƒüerlendirmelerini ortaya koymu≈ütur.",
        "keywords": "",
        "link": "http://dx.doi.org/10.53070/bbd.1350405"
    },
    {
        "id": 2442,
        "title": "Comparative Evaluation of NLP Approaches for Requirements Formalisation",
        "authors": "Shekoufeh Rahimi, Kevin Lano, Sobhan Tehrani, Chenghua Lin, Yiqi Liu, Muhammad Umar",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012318700003645"
    },
    {
        "id": 2443,
        "title": "A CONCEPTUAL MODEL OF A MULTILINGUAL INFORMATION SYSTEM FOR NLP SCIENTIFIC AND EDUCATIONAL ACTIVITIES",
        "authors": "–ñ.–ë. –°–∞–¥–∏—Ä–º–µ–∫–æ–≤–∞, –ú.–ê. –°–∞–º–±–µ—Ç–±–∞–µ–≤–∞, –ì.–°. –ë–æ—Ä–∞–Ω–∫—É–ª–æ–≤–∞, –ê.–°. –ï—Ä–∏–º–±–µ—Ç–æ–≤–∞",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "–¶–µ–ª—å—é —Ä–∞–±–æ—Ç—ã —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª–∏—è–∑—ã—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –Ω–∞—É—á–Ω–æ-–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ NLP. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∏–º –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –≤ —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –æ–Ω—Ç–æ–ª–æ–≥–∏—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∏ —Å—Ä–µ–¥—Å—Ç–≤–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π Semantic Web. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–Ω—Ç–æ–ª–æ–≥–∏—è —Å—Ç–∞–Ω–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–π –æ—Å–Ω–æ–≤–æ–π –ø–æ–ª–∏—è–∑—ã—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∞—É—á–Ω–æ–π –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ NLP, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—é –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º, –µ–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ –µ–¥–∏–Ω–æ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —É–¥–æ–±–Ω—É—é –Ω–∞–≤–∏–≥–∞—Ü–∏—é, –∞ —Ç–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –Ω–µ–π.\nThe goal is to develop a multilingual information system for NLP scientific and educational activities. The originality of the approach proposed in the study is that the ontology of the subject area developed within the framework of this project will be used to systematize knowledge, data and information resources and organize meaningful access to them, as well as the standards and tools of Semantic Web technologies will be used as the basis of the software. The ontology developed within the framework of this study will become the conceptual basis of a multilingual information system for supporting scientific and educational activities of NLP, which will provide systematization of all information on these methods, its integration into a single information space, convenient navigation through it, as well as meaningful access to it.",
        "keywords": "",
        "link": "http://dx.doi.org/10.25743/dir.2022.44.86.038"
    },
    {
        "id": 2444,
        "title": "NLP in the Legal World",
        "authors": "Jerrold Soh",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4670517"
    },
    {
        "id": 2445,
        "title": "Harmful Content on Social Media Detection Using by NLP",
        "authors": "Iqra Naz, Rehhmat Illahi",
        "published": "2023-7-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.11648/j.advances.20230402.13"
    },
    {
        "id": 2446,
        "title": "Partial Tensorized Transformers for Natural Language Processing",
        "authors": "Subhadra Vadlamannati, Ryan Solgi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012366500003636"
    },
    {
        "id": 2447,
        "title": "Prospects for Using of New Types of Current Transformers for Differential Protection for Transformers",
        "authors": "Andrey Yablokov, Anton Panashatenko, Andrey Tychkin",
        "published": "2023-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icieam57311.2023.10139250"
    },
    {
        "id": 2448,
        "title": "CPE-Identifier: Automated CPE Identification and CVE Summaries Annotation with Deep Learning and NLP",
        "authors": "Wanyu Hu, Vrizlynn Thing",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012403500003648"
    },
    {
        "id": 2449,
        "title": "Using NLP to Enrich Scientific Knowledge Graphs: A Case Study to Find Similar Papers",
        "authors": "Xavier Quevedo, Janneth Chicaiza",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011671100003393"
    },
    {
        "id": 2450,
        "title": "Validating the PNL in NLP",
        "authors": "Babak Mahdavi-Damghani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4556106"
    },
    {
        "id": 2451,
        "title": "Impact of Misalignment of Voltage Transformers on Transformers Performance in Mellitah Complex - Case Study",
        "authors": "Khaled M. Alkar, Maulod Khanan, Mohammed A. Issa",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc58660.2023.10449864"
    },
    {
        "id": 2452,
        "title": "Metrological support of digital electronic voltage transformers and low-power voltage transformers",
        "authors": "M. V. Grishin, A. V. Leonov, A. A. Kucobin",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "Metrological support of digital electronic voltage transformers and low-power voltage transformers (sensors) is represented. The methods and reference measuring instruments of the State primary special standard of units of the ratio error and the phase displacement of AC electric voltage of power frequency in the range from to and units of electric capacitance and tangent of loss dissipation factor at AC voltage of power frequency in the range from 1 to 500 kV ‚Äì GET 175-2023, used for metrological support of the measuring instruments mentioned above. The results of research of GET 175-2023 are presented. The expanded uncertainties of GET 175-2023 when reproducing the units of the ratio error and the phase displacement of AC electric voltage of power frequency for metrological support of analog low-power voltage transformers (sensors) include the budget of the uncertainties of the current comparator, low-voltage (reference) and high-voltage measuring measures, the high-voltage MVE-01 bridge (SA7400MA1 amplifi er) and are respectively 9.62¬∑10‚Äì5 and The expanded uncertainties of GET 175-2023 when reproducing the units of the ratio error and the phase displacement of AC electric voltage of power frequency for metrological support of electronic digital voltage transformers and digital low-power voltage transformers (sensors) include the budget of uncertainties of the converter ¬´high voltage ‚Äì current¬ª, converter ¬´current ‚Äì low voltage¬ª, converter ¬´low voltage ‚Äì a digital copy of voltage¬ª, a digital comparison device and constitutes 10.8¬∑10‚Äì5 and respectively. GET 175-2023 allows solving the problems of metrological support of electronic digital voltage transformers and digital low-power voltage transformers (sensors).",
        "keywords": "",
        "link": "http://dx.doi.org/10.32446/0368-1025it.2023-9-46-52"
    },
    {
        "id": 2453,
        "title": "Sentence Transformers and DistilBERT for Arabic Word Sense Induction",
        "authors": "Rakia Saidi, Fethi Jarray",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011891700003393"
    },
    {
        "id": 2454,
        "title": "A Survey of Deep Learning: From Activations to Transformers",
        "authors": "Johannes Schneider, Michalis Vlachos",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012404300003636"
    },
    {
        "id": 2455,
        "title": "NATURAL LANGUAGE PROCESSING (NLP) FOR CODE IN PYTHON (2020)",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/jcr.07.05.470"
    },
    {
        "id": 2456,
        "title": "Traffic Accident Risk Forecasting using Contextual Vision Transformers with Static Map Generation and Coarse-Fine-Coarse Transformers",
        "authors": "Artur Grigorev, Khaled Saleh, Adriana-Simona Mihaita",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421915"
    },
    {
        "id": 2457,
        "title": "TEXT SUMMERIZATION USING NLP",
        "authors": "",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets34791"
    },
    {
        "id": 2458,
        "title": "How Digital Transformation Affects Enter Prisetotal Factor Productivity from the Perspective of NLP",
        "authors": "Èõ™Â¶ç Âêë",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/ecl.2024.131062"
    },
    {
        "id": 2459,
        "title": "When to Replace Aging Transformers, Part 2: Guidelines to Replace Older Transformers Before Failure",
        "authors": "Fred L. Dixon, Derrick Robey",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mias.2022.3214023"
    },
    {
        "id": 2460,
        "title": "Proposing Self-Reflective Debriefing Model for Preventing Burnout among Missionaries: An Approach Based on NLP Therapy and Cognitive Schema Therapy",
        "authors": "Hannah Park,  ",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14493/ksoms.2024.1.129"
    },
    {
        "id": 2461,
        "title": "AI Contact Center IP-R&amp;D Scheme based on Patent NLP Analysis",
        "authors": "JungHeui Kim, Young-Min Kim",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14801/jkiit.2024.22.2.151"
    },
    {
        "id": 2462,
        "title": "Kidney CT Image Analysis Using CNN",
        "authors": "Harshit Mittal",
        "published": "2023-8-19",
        "citations": 0,
        "abstract": "Medical image analysis is a vital component of modern medical practice, and the accuracy of such analysis is critical for accurate diagnosis and treatment. Computed tomography (CT) scans are commonly used to visualize the kidneys and identify abnormalities such as cysts, tumors, and stones. Manual interpretation of CT images can be time-consuming and subject to human error, leading to inaccurate diagnosis and treatment. Deep learning models based on Convolutional Neural Networks (CNNs) have shown promise in improving the accuracy and speed of medical image analysis. In this study, we present a CNN-based model to accurately classify CT images of the kidney into four categories: Normal, Cyst, Tumor, and Stone, using the CT KIDNEY DATASET. The proposed CNN model achieved an accuracy of 99.84% on the test set, with a precision of 0.9964, a recall of 0.9986, and a F1-score of 0.9975 for all categories. The model was able to accurately classify all images in the test set, indicating its high accuracy in identifying abnormalities in CT images of the kidney. The results of this study demonstrate the potential of deep learning models based on CNNs in accurately classifying CT images of the kidney, which could lead to improved diagnosis and treatment outcomes for patients. This study contributes to the growing body of literature on the use of deep learning models in medical image analysis, highlighting the potential of these models in improving the accuracy and efficiency of medical diagnosis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131403"
    },
    {
        "id": 2463,
        "title": "Weak Supervision Approach for Arabic Named Entity Recognition",
        "authors": "Olga Simek, Courtland VanDam",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "Arabic named entity recognition (NER) is a challenging problem, especially in conversational data such as social media posts. To address this problem, we propose an Arabic weak learner NER model called ANER-HMM, which leverages low quality predictions that provide partial recognition of entities. By combining these predictions, we achieve state of the art NER accuracy for cases for out-of-domain predictions. ANER-HMM leverages a hidden markov model to combine multiple predictions from weak learners and gazetteers. We demonstrate that ANER-HMM outperforms the state-of-the-art Arabic NER methods without requiring any labeled data or training deep learning models which often require large computing resources.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131602"
    },
    {
        "id": 2464,
        "title": "Developing Research Projects in SE and NLP",
        "authors": "",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "Research projects are necessary for conducting research or generating work products. Most of the work however, focuses more on the aspects of research within a domain instead of moving towards interdisciplinary work. In this chapter, the author proposes to develop research projects in perspective of SE and NLP. The future scope is also presented herein.",
        "keywords": "",
        "link": "http://dx.doi.org/10.46632/daai/4/1/2"
    },
    {
        "id": 2465,
        "title": "From Descriptive to Predictive: Forecasting Emerging Research Areas in Software Traceability Using NLP from Systematic Studies",
        "authors": "Zaki Pauzi, Andrea Capiluppi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011964100003464"
    },
    {
        "id": 2466,
        "title": "Exploration Advisory Tool: AI and NLP in the Geosciences",
        "authors": "B. Entrup, M. Dillen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.2023101230"
    },
    {
        "id": 2467,
        "title": "Resume Parser Using ML and NLP",
        "authors": "",
        "published": "2023-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets46536"
    },
    {
        "id": 2468,
        "title": "Developing of NLP models by model based software developement",
        "authors": "Zsolt Krutilla",
        "published": "2023",
        "citations": 0,
        "abstract": "In the case of software development, we can talk about several methodologies, such as the waterfall model, test-driven software development, agile software development, but the question rightly arises whether software development methods in the traditional sense are applicable to the development of natural language processing language models, especially from the aspect of AI and dictionary-based NLP models. Learning AI models is substantially different from the classical software development process. Whereas in classical software development, developers explicitly describe the operation and behavior to the computer, in AI model learning, the models themselves learn from the input data. This paper presents a possible solution that applies the agile Scrum methodology used in classical software development to the development of dictionary-based NLP models, and identify the agile development opportunities in case of the machine learning-based NLP models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47833/2023.2.csc.024"
    },
    {
        "id": 2469,
        "title": "Sustainable power transformers: Enel Grids use of natural ester insulating fluid in large power transformers",
        "authors": "M.√Å. Caballero, M. Rizzo, F. Mauri, J.M. Rey, F. Gasbarri, E. Valigi, F. Amadei",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.0778"
    },
    {
        "id": 2470,
        "title": "Cached Transformers: Improving Transformers with Differentiable Memory Cachde",
        "authors": "Zhaoyang Zhang, Wenqi Shao, Yixiao Ge, Xiaogang Wang, Jinwei Gu, Ping Luo",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "This work introduces a new Transformer model called Cached Transformer, which uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens. GRC attention enables attending to both past and current tokens, increasing the receptive field of attention and allowing for exploring long-range dependencies. By utilizing a recurrent gating unit to continuously update the cache, our model achieves significant advancements in \\textbf{six} language and vision tasks, including language modeling, machine translation, ListOPs, image classification, object detection, and instance segmentation. Furthermore, our approach surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29636"
    },
    {
        "id": 2471,
        "title": "DiT-Head: High Resolution Talking Head Synthesis Using Diffusion Transformers",
        "authors": "Aaron Mir, Eduardo Alonso, Esther Mondrag√≥n",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312200003636"
    },
    {
        "id": 2472,
        "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey",
        "authors": "Julian Michael, Ari Holtzman, Alicia Parrish, Aaron Mueller, Alex Wang, Angelica Chen, Divyam Madaan, Nikita Nangia, Richard Yuanzhe Pang, Jason Phang, Samuel R. Bowman",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.903"
    },
    {
        "id": 2473,
        "title": "HiTEK Pre-processing for Speech and Text: NLP",
        "authors": "Naveenkumar T Rudrappa,  , Mallamma V Reddy, M Hanumanthappa",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17485/ijst/v16i19.296"
    },
    {
        "id": 2474,
        "title": "NLP-PIPE Incorporation into ONTO6 Framework Workflow",
        "authors": "Uldis Straujums",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22364/bjmc.2023.11.1.09"
    },
    {
        "id": 2475,
        "title": "ARTIFICIAL INTELLIGENCE AND LEARNING ENVIRONMENTS: THE ROLE OF NLP",
        "authors": "Vincenza Barra, Felice Corona",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/iceri.2023.1504"
    },
    {
        "id": 2476,
        "title": "ChatGPT: Unlocking the Future of NLP in Finance",
        "authors": "Adam Zaremba, Ender Demir",
        "published": "2023",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4323643"
    },
    {
        "id": 2477,
        "title": "Continuous Sign-Language Recognition using Transformers and Augmented Pose Estimation",
        "authors": "Reemt Hinrichs, Angelo Sitcheu, J√∂rn Ostermann",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011709100003411"
    },
    {
        "id": 2478,
        "title": "Convolutional Networks Versus Transformers: A Comparison in Prostate Segmentation",
        "authors": "Fernando V√°sconez, Maria Baldeon Calisto, Daniel Riofr√≠o, Zhouping Wei, Yoga Balagurunathan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011717600003393"
    },
    {
        "id": 2479,
        "title": "Applying Positional Encoding to Enhance Vision-Language Transformers",
        "authors": "Xuehao Liu, Sarah Delany, Susan McKeever",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011796100003417"
    },
    {
        "id": 2480,
        "title": "Leveraging Online GPT Transformers for Sustainable IT Solutions",
        "authors": "Varun K A, Dr. Kamalraj R",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55248/gengpi.5.0324.0723"
    },
    {
        "id": 2481,
        "title": "SENTIMENT ANALYSIS ON TWITTER DATASET USING NLP",
        "authors": "",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets45970"
    },
    {
        "id": 2482,
        "title": "Techniques for Failures Classification in Power Transformers by High-Frequency Current Transformers",
        "authors": "Jo√£o Pedro Gouv√™a, Bruno Albuquerque De Castro, Alceu Ferreira Alves, Guilherme Beraldi Lucas, Jorge Alfredo Ardila-Rey, Colin Smith",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/induscon58041.2023.10374629"
    },
    {
        "id": 2483,
        "title": "Research on Text Classification Method Based on NLP",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23977/acss.2023.070213"
    },
    {
        "id": 2484,
        "title": "NLP-based predictive model for identifying and presenting construction accident scenarios",
        "authors": "",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.36334/modsim.2023.shin436"
    },
    {
        "id": 2485,
        "title": "Transfer Learning in Natural Language Processing (NLP)",
        "authors": "Jasmin Bharadiya",
        "published": "2023-6-5",
        "citations": 1,
        "abstract": "Purpose: The purpose of this study is to address the limited use of transfer learning techniques in radio frequency machine learning and to propose a customized taxonomy for radio frequency applications. The aim is to enable performance gains, improved generalization, and cost-effective training data solutions in this specific domain.\r\nMethodology: The research design employed in this study involves a comprehensive review of existing literature on transfer learning in radio frequency machine learning. The researchers collected relevant papers from reputable sources and analyzed them to identify patterns, trends, and insights. The method of data collection primarily relied on examining and synthesizing existing literature. Data analysis involved identifying key findings and developing a customized taxonomy for radio frequency applications.\r\nFindings: The study's findings highlight the limited utilization of transfer learning techniques in radio frequency machine learning. While transfer learning has shown significant performance improvements in computer vision and natural language processing, its potential in the wireless communications domain has yet to be fully explored. The customized taxonomy proposed in this study provides a consistent framework for analyzing and comparing existing and future efforts in this field.\r\nRecommendations: Based on the findings, the study recommends further research and experimentation to explore the potential of transfer learning techniques in radio frequency machine learning. This includes investigating performance gains, improving generalization capabilities, and addressing concerns related to training data costs. Additionally, collaborations between researchers and practitioners in the field are encouraged to facilitate knowledge exchange and foster innovation. Practice: To practitioners in the field of radio frequency machine learning, this study emphasizes the potential benefits of incorporating transfer learning techniques. It encourages practitioners to explore the application of transfer learning in their specific domain, leveraging prior knowledge to enhance performance and address training data challenges. It also highlights the importance of staying informed about the latest developments and collaborating with experts in the field. Policy: To policy makers, the study underscores the need for supportive policies that promote research and development in radio frequency machine learning. It recommends creating an environment that fosters innovation, encourages collaborations between academia and industry, and provides resources and incentives for further exploration of transfer learning techniques. Policy makers should consider the potential impact of transfer learning on the wireless communications industry and support initiatives that enhance its adoption and implementation.\r\n¬†",
        "keywords": "",
        "link": "http://dx.doi.org/10.47672/ejt.1490"
    },
    {
        "id": 2486,
        "title": "PSYCHOLOGICAL INTERACTIONS LANGUAGE AND THINKING BASED ON NLP",
        "authors": "–ê—Ä—ñ—Ñ –ï–ª—å–º—ñ—Ä",
        "published": "2023-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52058/2786-5274-2023-9(23)-8-16"
    },
    {
        "id": 2487,
        "title": "What's in a Domain? Anaylsis of URL Features",
        "authors": "John Hawkins",
        "published": "2023-8-19",
        "citations": 0,
        "abstract": "Many data science problems require processing log data derived from web pages, apis or other internet traffic sources. URLs are one of the few ubiquitous data fields that describe internet activity, hence they require effective processing for a wide variety of machine learning applications. While URLs are structurally rich, the structure can be both domain specific and subject to change over time, making feature engineering for URLs an ongoing challenge.  In this research we outline the key structural components of URLs and discuss the information available within each. We describe methods for generating features on these URL components and share an open source implementation of these ideas. In addition, we describe a method for exploring URL feature importance that allows for comparison and analysis of the information available inside URLs. We experiment with a collection of URL classification datasets and demonstrate the utility of these tools. Package and source code is open on https://pypi.org/project/url2features.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131409"
    },
    {
        "id": 2488,
        "title": "Survey of transformers and towards ensemble learning using transformers for natural language processing",
        "authors": "Hongzhi Zhang, M. Omair Shafiq",
        "published": "2024-2-4",
        "citations": 0,
        "abstract": "AbstractThe transformer model is a famous natural language processing model proposed by Google in 2017. Now, with the extensive development of deep learning, many natural language processing tasks can be solved by deep learning methods. After the BERT model was proposed, many pre-trained models such as the XLNet model, the RoBERTa model, and the ALBERT model were also proposed¬†in the research community. These models perform very well in various natural language processing tasks. In this paper, we describe and compare these well-known¬†models. In addition, we¬†also apply several types of existing and well-known¬†models which are the BERT model, the XLNet model, the RoBERTa model, the GPT2 model, and the ALBERT model to different existing and¬†well-known¬†natural language processing tasks, and analyze each model based on their performance. There are a¬†few papers that comprehensively compare various transformer models. In our paper, we use six¬†types of well-known¬†tasks, such as¬†sentiment analysis, question answering, text generation, text summarization, name entity recognition, and topic modeling tasks to compare the performance of¬†various transformer models. In addition, using the existing models, we also propose¬†ensemble learning models¬†for the¬†different natural language processing tasks. The results show that our ensemble learning models¬† perform better than a single classifier¬†on specific tasks.\nGraphical Abstract",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40537-023-00842-0"
    },
    {
        "id": 2489,
        "title": "A Survey on Attention mechanism in NLP",
        "authors": "Nan Zhang, Junyeong Kim",
        "published": "2023-2-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceic57457.2023.10049971"
    },
    {
        "id": 2490,
        "title": "The Role of Typological Feature Prediction in NLP and Linguistics",
        "authors": "Johannes Bjerva",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": "Abstract\nComputational typology has gained traction in the field of Natural Language Processing (NLP) in recent years, as evidenced by the increasing number of papers on the topic and the establishment of a Special Interest Group on the topic (SIGTYP), including the organization of successful workshops and shared tasks. A considerable amount of work in this sub-field is concerned with prediction of typological features, for example, for databases such as the World Atlas of Language Structures (WALS) or Grambank. Prediction is argued to be useful either because (1) it allows for obtaining feature values for relatively undocumented languages, alleviating the sparseness in WALS, in turn argued to be useful for both NLP and linguistics; and (2) it allows us to probe models to see whether or not these typological features are encapsulated in, for example, language representations. In this article, we present a critical stance concerning prediction of typological features, investigating to what extent this line of research is aligned with purported needs‚Äîboth from the perspective of NLP practitioners, and perhaps more importantly, from the perspective of linguists specialized in typology and language documentation. We provide evidence that this line of research in its current state suffers from a lack of interdisciplinary alignment. Based on an extensive survey of the linguistic typology community, we present concrete recommendations for future research in order to improve this alignment between linguists and NLP researchers, beyond the scope of typological feature prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/coli_a_00498"
    },
    {
        "id": 2491,
        "title": "Using Ensemble Learning in Language Variety Identification",
        "authors": "Mihaela Gaman",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.vardial-1.23"
    },
    {
        "id": 2492,
        "title": "Multilingual Automatic Extraction of Linguistic Data from Grammars",
        "authors": "Albert Kornilov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.fieldmatters-1.10"
    },
    {
        "id": 2493,
        "title": "A Review of Text Summarization Techniques Using NLP",
        "authors": "Kartik Aggarwal",
        "published": "2023-10-14",
        "citations": 0,
        "abstract": "Techniques that employ natural language processing (NLP), often known as text summarizing, automatically construct summaries of extensive texts. Extractive and abstractive summarization are two main categories that may be used to classify these methods. In extractive summarizing, the most significant lines or phrases from a text are isolated and used to generate a summary. On the other hand, in abstractive summarization, a summary is generated that is clear, short, and accurate in its representation of the text's primary concepts. NLP methods like sentence segmentation, part-of-speech tagging, named entity recognition, and semantic analysis are used in generating a summary from a text and locating and extracting relevant information from the text. Text summarizing is a subject that has received a significant amount of study and has applications in various fields, including the summation of news articles, documents, and emails, among other things.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36647/ciml/04.02.a001"
    },
    {
        "id": 2494,
        "title": "EXPLORING TRENDING TOPICS ON E-COMMERCE USING NLP",
        "authors": "",
        "published": "2023-12-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets47256"
    },
    {
        "id": 2495,
        "title": "ChatGPT: Unlocking the future of NLP in finance",
        "authors": "Adam Zaremba, Ender Demir",
        "published": "2023-11-1",
        "citations": 5,
        "abstract": "This paper reviews the current state of ChatGPT technology in finance and its potential to improve existing NLP-based financial applications. We discuss the ethical and regulatory considerations, as well as potential future research directions in the field. The literature suggests that ChatGPT has the potential to improve NLP-based financial applications, but also raises ethical and regulatory concerns that need to be addressed. The paper highlights the need for research in robustness, interpretability, and ethical considerations to ensure responsible use of ChatGPT technology in finance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.61351/mf.v1i1.43"
    },
    {
        "id": 2496,
        "title": "Temporal Generalizability in Multimodal Misinformation Detection",
        "authors": "Nataliya Stepanova, Bj√∂rn Ross",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.genbench-1.6"
    },
    {
        "id": 2497,
        "title": "Gender Bias in Machine Translation: a statistical evaluation of Google Translate and DeepL for English, Italian and German",
        "authors": "Argentina Anna Rescigno,  , Johanna Monti,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/issn.2683-0078.2023_001"
    },
    {
        "id": 2498,
        "title": "Model Agnostic Approach for NLP Backdoor Detection",
        "authors": "Hema Karnam Surendrababu",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/colcaci59285.2023.10226144"
    },
    {
        "id": 2499,
        "title": "ImbGAFS: GA Feature Selection for AUC in Bird Strike Prediction",
        "authors": "Aji Gautama Putrada, Sidik Prabowo",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "Several studies discuss airplane failure prediction due to bird strikes. However, these studies need to analyze further the imbalance in their dataset. Our research aim is to create an airplane failure prediction by bird strike using a machine learning method optimized using GA feature selection. GA feature selection uses AUC maximization as the objective function to tackle imbalance problems in the bird strike dataset. First, we obtained the airplane bird strike dataset from Kaggle. We carry out preprocessing on the dataset.We then compared and chose one of four stateof-the-art machine learning methods: SVM, MLP, logistic regression, and random forest. The selection process involves oversampling methods, synthetic minority oversampling technique (SMOTE), and optimum threshold selection, which involves geometric mean (g- mean) and area under curve (AUC) values. Finally, we optimize airplane failure prediction by performing AUC maximization using GA feature selection. Our test results show that random forest is the best machinelearning method in airplane failure prediction compared to SVM, logistic regression, and MLP. SMOTE can increase random forest AUC from 0.845 to 0.878. Finally, the random forest model from ImbGAFS is better than the conventional method without feature selection. The increase in the AUC value is from 0.878 to 0.889. Then, after carrying out optimal threshold selection, ImbGAFS+random forest also has better sensitivity, specificity, and g-mean than conventional methods. The increase is from 0.7737, 0.8350, and 0.8037 to 0.8033, 0.8301, and 0.8166, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131614"
    },
    {
        "id": 2500,
        "title": "NormNet: Normalize Noun Phrases for More Robust NLP",
        "authors": "Minlong Peng, Mingming Sun",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.136"
    }
]