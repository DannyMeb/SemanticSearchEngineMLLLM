[
    {
        "id": 6901,
        "title": "Model Interpretability, Explainability and Trust for Manufacturing 4.0",
        "authors": "Bianca Maria Colosimo, Fabio Centofanti",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12402-0_2"
    },
    {
        "id": 6902,
        "title": "Interpretability and Explainability of Logistic Regression Model for Breast Cancer Detection",
        "authors": "Emina Tahirović, Senka Krivić",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011627600003393"
    },
    {
        "id": 6903,
        "title": "Bag-of-Words Algorithms Can Supplement Transformer Sequence Classification &amp; Improve Model Interpretability",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7249/wra1719-1"
    },
    {
        "id": 6904,
        "title": "The Mythos of Model Interpretability",
        "authors": "Zachary C. Lipton",
        "published": "2018-6",
        "citations": 1201,
        "abstract": "Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?",
        "link": "http://dx.doi.org/10.1145/3236386.3241340"
    },
    {
        "id": 6905,
        "title": "Explainable AI Recipes",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3"
    },
    {
        "id": 6906,
        "title": "Enhancing Ensemble Model Accuracy and Interpretability: A Framework Integrating Rough Set Theory and Recursive Feature Elimination for Feature Selection and Interpretability with Association Rule Analysis in Ensemble Models.",
        "authors": "Isaac Kega, Lawrence Nderu, Ronald Mwangi, Dennis Njagi",
        "published": "No Date",
        "citations": 0,
        "abstract": "In machine learning, feature selection is of utmost importance for\naugmenting the predictive capabilities of ensemble models. This paper\npresents an innovative hybrid framework for selecting features in\nensemble models, which combines Rough Set Theory (RST) with Recursive\nFeature Elimination (RFE), complemented by Association Rule Mining, to\nenhance interpretability. The suggested method considerably improves\nensemble models’ prognostic accuracy and comprehensibility, particularly\nRandom Forests and Gradient Boosting Machines. The framework starts with\nthe RFE process, meticulously eliminating less influential features, and\nthen applies RST to refine the feature set further by eliminating\nredundancies. This two-phase approach results in a feature set that is\noptimally reduced yet highly influential. By implementing this hybrid\nmethod on ensemble models, significant improvements in predictive\naccuracy are demonstrated across three diverse datasets: cancer, Pima\nIndians Diabetes, and a weather dataset from Underground. The\naccomplished accuracies for these datasets were 0.9663, 0.8793, and\n0.8427, respectively, highlighting the proposed approach’s\neffectiveness. This article also proposes the incorporation of\nassociation rule mining to analyze the outcomes of the models. This\ntechnique improves the understandability of the models, offering more\nprofound insights into the connections and patterns, thus tackling the\ndifficulty of interpretability in intricate ensemble models. Our\nempirical analysis confirms the effectiveness of the proposed hybrid\nfeature selection model, representing a significant advancement in the\nfield. The integration of RFE and RST optimizes the feature selection\nprocess and bridges the gap in interpretability, offering robust\nsolutions for applications where accuracy and understanding of model\ndecisions are crucial.",
        "link": "http://dx.doi.org/10.22541/au.170993286.68523250/v1"
    },
    {
        "id": 6907,
        "title": "Enhancing Ensemble Model Accuracy and  Interpretability: A Framework Integrating Rough Set Theory and Recursive Feature Elimination for Feature Selection and Interpretability with Association Rule Analysis in Ensemble Models",
        "authors": "Isaac  Kega Mwangi, Lawrence Nderu, Waweru Mwangi, Dennis Njagi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4725862"
    },
    {
        "id": 6908,
        "title": "Enhancing Accuracy and Interpretability in Corporate Credit Rating Classification with the Transformer-Lstm Model",
        "authors": "Kensei Monden, Suguru Yamanaka",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4765632"
    },
    {
        "id": 6909,
        "title": "Adversarial training improves model interpretability in single-cell RNA-seq analysis",
        "authors": "Mehrshad Sadria, Anita Layton, Gary D. Bader",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractFor predictive computational models to be considered reliable in crucial areas such as biology and medicine, it is essential for them to be accurate, robust, and interpretable. A sufficiently robust model should not have its output affected significantly by a slight change in the input. Also, these models should be able to explain how a decision is made. Efforts have been made to improve the robustness and interpretability of these models as independent challenges, however, the effect of robustness and interpretability on each other is poorly understood. Here, we show that predicting cell type based on single-cell RNA-seq data is more robust by adversarially training a deep learning model. Surprisingly, we find this also leads to improved model interpretability, as measured by identifying genes important for classification. We believe that adversarial training will be generally useful to improve deep learning robustness and interpretability, thereby facilitating biological discovery.",
        "link": "http://dx.doi.org/10.1101/2023.05.17.541170"
    },
    {
        "id": 6910,
        "title": "CRESPR: Modular Sparsification of DNNs to Improve Pruning Performance and Model Interpretability",
        "authors": "Tianyu Kang, Wei Ding, Ping Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466297"
    },
    {
        "id": 6911,
        "title": "MODEL INTERPRETABILITY VIA INTERACTION FEATURE DETECTION USING ROUGHSET IN A GENERALIZED LINEAR MODEL FOR WEATHER PREDICTION IN KENYA.",
        "authors": "Isaac Kega, Lawrence Nderu, Ronald Mwangi, Dennis Njagi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.167425256.66042780/v3"
    },
    {
        "id": 6912,
        "title": "MODEL INTERPRETABILITY VIA INTERACTION FEATURE DETECTION USING ROUGHSET IN A GENERALIZED LINEAR MODEL FOR WEATHER PREDICTION IN KENYA.",
        "authors": "Isaac Kega, Lawrence Nderu, Ronald Mwangi, Dennis Njagi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.167425256.66042780/v2"
    },
    {
        "id": 6913,
        "title": "MODEL INTERPRETABILITY VIA INTERACTION FEATURE DETECTION USING ROUGHSET IN A GENERALIZED LINEAR MODEL FOR WEATHER PREDICTION IN KENYA.",
        "authors": "Isaac Kega, Lawrence Nderu, Ronald Mwangi, Dennis Njagi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.167425256.66042780/v1"
    },
    {
        "id": 6914,
        "title": "Interpretability of Neural Networks: A Credit Card Default Model Example",
        "authors": "Ksenia Ponomareva, Simone Caenazzo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3519142"
    },
    {
        "id": 6915,
        "title": "Model Explainability and Interpretability",
        "authors": "Pradeepta Mishra",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7158-2_1"
    },
    {
        "id": 6916,
        "title": "Increased Interpretability for Model-Driven Deception: MARS LDRD Project",
        "authors": "William Hofer, Oceane MS Bel, Burhan Hyder, Matthew Bruggeman, Thomas Edgar",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2203118"
    },
    {
        "id": 6917,
        "title": "Multi-scale Fast Quantification Model for Crop Blue-Green Water Footprint with Physical Process Interpretability",
        "authors": "Yilin Liu, La Zhuo, Pute Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The intensification of land surface evapotranspiration and the increasing frequency of extreme precipitation have resulted in significant temporal and spatial scale changes in the consumption of blue and green water for global crop production. However, due to the complexity of multiple input parameters and the extended computational process of the model, the updated interval for crop blue-green water consumption data worldwide is ten years. This makes it challenging to further elucidate global crop blue-green water consumption dynamics under climate change. To acquire the latest crop blue-green water consumption data more promptly, this study integrates the Fast Track and field crop water requirement approaches. This combination results in a fast quantification model of the blue-green water footprint (WF), suitable for crop production at various temporal and spatial scales with interpretability of physical processes. A quantitative analysis was performed on the spatiotemporal evolution patterns of the WF and the blue-to-total WF ratio of maize production from 2000 to 2021. Additionally, a sensitivity analysis was conducted on the input parameters of model. The results indicate a high R2 between this study and others regarding the blue-to-total WF ratio of maize production, reaching 0.8. The total WF of global maize production reached 859.2&#215;109m3 yr-1 in 2021, a 50% increase compared to 2000, with 33% occurring in East Asia. Regional heterogeneity in the blue-to-total WF ratio of maize production was significant, with the global average ranging from 10% to 12.1%. Among major maize-producing countries, South Africa experienced the most substantial increase in the blue-to-total WF ratio, rising from 1.3% in 2000 to 25.9% in 2021. Additionally, the blue-to-total WF ratio of maize production in the United States and South Africa peaked in 2012 and 2015 (20.7% and 30%, respectively), influenced by extreme climate events. The multiyear standard deviation of the blue-to-total water consumption ratio of maize production at the monthly scale surpassed that at the annual scale, with August exhibiting the highest value. Among the four input parameters (precipitation, crop reference evapotranspiration, crop planting date, and crop coefficient), the sensitivity index of precipitation was the highest, with 57 countries exceeding 20, predominantly concentrated in southern Africa. Under the influence of climate change, the blue-to-total WF ratio of maize production has undergone significant changes at multiple temporal and spatial scales. Timely and efficient quantification of blue-green water consumption in crop production aids researchers in accurately clarifying and predicting changes in crop water consumption. This, in turn, facilitates the development of more effective and sustainable agricultural water-saving plans. Simultaneously, it proves beneficial for managers in identifying water hotspots, implementing dynamic water management, and actively responding to climate change.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-4836"
    },
    {
        "id": 6918,
        "title": "An evaluation of the interpretability and predictive performance of the BayesR model for genomic prediction",
        "authors": "Fanny Mollandin, Andrea Rau, Pascal Croiseau",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTTechnological advances and decreasing costs have led to the rise of increasingly dense genotyping data, making feasible the identification of potential causal markers. Custom genotyping chips, which combine medium-density genotypes with a custom genotype panel, can capitalize on these candidates to potentially yield improved accuracy and interpretability in genomic prediction. A particularly promising model to this end is BayesR, which divides markers into four effect size classes. BayesR has been shown to yield accurate predictions and promise for quantitative trait loci (QTL) mapping in real data applications, but an extensive benchmarking in simulated data is currently lacking. Based on a set of real genotypes, we generated simulated data under a variety of genetic architectures, phenotype heritabilities, and we evaluated the impact of excluding or including causal markers among the genotypes. We define several statistical criteria for QTL mapping, including several based on sliding windows to account for linkage disequilibrium. We compare and contrast these statistics and their ability to accurately prioritize known causal markers. Overall, we confirm the strong predictive performance for BayesR in moderately to highly heritable traits, particularly for 50k custom data. In cases of low heritability or weak linkage disequilibrium with the causal marker in 50k genotypes, QTL mapping is a challenge, regardless of the criterion used. BayesR is a promising approach to simultaneously obtain accurate predictions and interpretable classifications of SNPs into effect size classes. We illustrated the performance of BayesR in a variety of simulation scenarios, and compared the advantages and limitations of each.",
        "link": "http://dx.doi.org/10.1101/2020.10.23.351700"
    },
    {
        "id": 6919,
        "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
        "authors": "Marc Brinner, Sina Zarrieß",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.867"
    },
    {
        "id": 6920,
        "title": "Inter-Model Interpretability: Self-Supervised Models as a Case Study",
        "authors": "Ahmad Mustapha, Wael Khreich, Wassim Masri",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4184454"
    },
    {
        "id": 6921,
        "title": "Pre-model Interpretability and Explainability",
        "authors": "Uday Kamath, John Liu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83356-5_2"
    },
    {
        "id": 6922,
        "title": "Model-Agnostic Interpretability with Shapley Values",
        "authors": "Andreas Messalas, Yiannis Kanellopoulos, Christos Makris",
        "published": "2019-7",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisa.2019.8900669"
    },
    {
        "id": 6923,
        "title": "Simplicity, Flexibility, and Interpretability in a Model of Dendritic Protein Distributions",
        "authors": "Cian O’Donnell",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neuron.2019.09.010"
    },
    {
        "id": 6924,
        "title": "Introduction to Interpretability",
        "authors": "Ayush Somani, Alexander Horsch, Dilip K. Prasad",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-20639-9_1"
    },
    {
        "id": 6925,
        "title": "An evaluation of the interpretability and predictive performance of the BayesR model for genomic prediction",
        "authors": "Fanny Mollandin, Andrea Rau, Pascal Croiseau",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: Technological advances and decreasing costs have led to the rise of increasingly dense genotyping data, making feasible the identification of potential causal or candidate markers. Custom genotyping chips, which represent a cost-effective strategy to combine medium-density genotypes with a custom genotype panel, can capitalize on these candidates to potentially yield improved accuracy and interpretability in genomic prediction. A particularly promising model to this end is BayesR, which divides markers into four effect size classes (null, small, medium, and large). The flexibility of BayesR has been shown to yield accurate predictions and promise for quantitative trait loci (QTL) mapping in real data applications, but an extensive benchmarking in simulated data is currently lacking.Results: Based on a set of real genotypes, we generated simulated data under a variety of genetic architectures, phenotype heritabilities, and polygenic variances, and we evaluated the impact of excluding (50k genotype data) or including (50k custom genotype data) causal markers among the genotypes. We define several statistical criteria for QTL mapping using BayesR output (maximum a posteriori rule, non-null maximum a posteriori rule, posterior variance, weighted cumulative inclusion probability), including several based on sliding windows rather than individual markers to account for linkage disequilibrium. We compare and contrast these statistics and their ability to accurately prioritize known causal markers. Overall, we confirm the strong predictive performance for BayesR in moderately to highly heritable traits, particularly for 50k custom data; in cases of low heritability or weak linkage disequilibrium with the causal marker in 50k genotypes, QTL mapping is a challenge, regardless of the criterion used.Conclusion: BayesR is a promising approach to simultaneously obtain accurate predictions and interpretable classifications of SNPs into effect size classes. Although QTL mapping is unsurprisingly easiest for highly heritable phenotypes and large QTLs, we illustrated the performance of BayesR in a variety of simulation scenarios, and compared the advantages and limitations of each. Among those considered, the weighted cumulative inclusion probability appears to provide the best mapping results, even under less favorable conditions. Finally, we quantify the advantage that can be gained by incorporating causal mutations on a custom genotyping chip.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-94363/v1"
    },
    {
        "id": 6926,
        "title": "Explainability for Linear Supervised Models",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_2"
    },
    {
        "id": 6927,
        "title": "An Analysis Method for Interpretability of CNN Text Classification Model",
        "authors": "Peng Ce, Bao Tie",
        "published": "2020-12-13",
        "citations": 12,
        "abstract": "With continuous development of artificial intelligence, text classification has gradually changed from a knowledge-based method to a method based on statistics and machine learning. Among them, it is a very important and efficient way to classify text based on the convolutional neural network (CNN) model. Text data are a kind of sequence data, while time sequentiality of the general text data is relatively weak, so text classification is usually less relevant to the sequential structure of the full text. Therefore, CNN-based text classification has gradually become a research hotspot when dealing with issues of text classification. For machine learning, especially deep learning, model interpretability has increasingly become the focus of academic research and industrial applications, and also become a key issue for further development and application of deep learning technology. Therefore, we recommend using the backtracking analysis method to conduct in-depth research on deep learning models. This paper proposes an analysis method for interpretability of a CNN text classification model. The method proposed by us can perform multi-angle analysis on the discriminant results of multi-classified text and multi-label classification tasks through backtracking analysis on model prediction results. Finally, the analysis results of the model can be displayed using visualization technology from multiple dimensions based on interpretability. The representative data set IMDB (Internet Movie Database) in text classification is verified by examples, and the results show that the model can be effectively analyzed when using our method.",
        "link": "http://dx.doi.org/10.3390/fi12120228"
    },
    {
        "id": 6928,
        "title": "Explainability for Nonlinear Supervised Models",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_3"
    },
    {
        "id": 6929,
        "title": "Explainability for Time-Series Models",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_6"
    },
    {
        "id": 6930,
        "title": "Handling Correlations in Random Forests: which Impacts on Variable Importance and  Model Interpretability?",
        "authors": "Marie Chavent, Jérôme Lacaille, Alex Mourer, Madalina Olteanu",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-155"
    },
    {
        "id": 6931,
        "title": "Explainability for Natural Language Processing",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_5"
    },
    {
        "id": 6932,
        "title": "Explainability for Deep Learning Models",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_7"
    },
    {
        "id": 6933,
        "title": "Explainability for Ensemble Supervised Models",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_4"
    },
    {
        "id": 6934,
        "title": "Designing an Interpretability-Based Model to Explain the Artificial Intelligence Algorithms in Healthcare",
        "authors": "Mohammad Ennab, Hamid Mcheick",
        "published": "2022-6-26",
        "citations": 8,
        "abstract": "The lack of interpretability in artificial intelligence models (i.e., deep learning, machine learning, and rules-based) is an obstacle to their widespread adoption in the healthcare domain. The absence of understandability and transparency frequently leads to (i) inadequate accountability and (ii) a consequent reduction in the quality of the predictive results of the models. On the other hand, the existence of interpretability in the predictions of AI models will facilitate the understanding and trust of the clinicians in these complex models. The data protection regulations worldwide emphasize the relevance of the plausibility and verifiability of AI models’ predictions. In response and to take a role in tackling this challenge, we designed the interpretability-based model with algorithms that achieve human-like reasoning abilities through statistical analysis of the datasets by calculating the relative weights of the variables of the features from the medical images and the patient symptoms. The relative weights represented the importance of the variables in predictive decision-making. In addition, the relative weights were used to find the positive and negative probabilities of having the disease, which indicated high fidelity explanations. Hence, the primary goal of our model is to shed light and give insights into the prediction process of the models, as well as to explain how the model predictions have resulted. Consequently, our model contributes by demonstrating accuracy. Furthermore, two experiments on COVID-19 datasets demonstrated the effectiveness and interpretability of the new model.",
        "link": "http://dx.doi.org/10.3390/diagnostics12071557"
    },
    {
        "id": 6935,
        "title": "using the XGBoost classification algorithm and model interpretability techniques",
        "authors": "Arsh Anand, Bart Baesens, Rosanne Vanpée",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21314/jcr.2022.008"
    },
    {
        "id": 6936,
        "title": "Chapter Nine Talking to AI: Model Interpretability",
        "authors": "Davey Gibian",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5771/9781538155097-99"
    },
    {
        "id": 6937,
        "title": "Model Interpretability: Advances in Interpretable Machine Learning",
        "authors": "Uday Kamath, John Liu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83356-5_4"
    },
    {
        "id": 6938,
        "title": "The mythos of model interpretability",
        "authors": "Zachary C. Lipton",
        "published": "2018-9-26",
        "citations": 390,
        "abstract": "In machine learning, the concept of interpretability is both important and slippery.",
        "link": "http://dx.doi.org/10.1145/3233231"
    },
    {
        "id": 6939,
        "title": "A Multi-Traits-Output-Chained Neural Network for Building a Rice Biomass Estimation Model Can Achieve Both High Accuracy and Interpretability",
        "authors": "Tomoaki Yamaguchi, Keisuke Katsura",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4549478"
    },
    {
        "id": 6940,
        "title": "A Comparative Analysis of Explainable AI Techniques for Enhanced Model Interpretability",
        "authors": "Swathi Y, Manoj Challa",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpcsn58827.2023.00043"
    },
    {
        "id": 6941,
        "title": "Leveraging Contextual Cues from a Conceptual Model with Predictive Skills of Machine Learning for Improved Predictability and Interpretability in the Hydrological Processes",
        "authors": "Pravin Bhasme, Udit Bhatia",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent years, Machine Learning (ML) techniques have gained the\nattention of the hydrological community for their better predictive\nskills. Specifically, ML models are widely applied for streamflow\npredictions. However, limited interpretability in the ML models\nindicates space for improvement. Leveraging domain knowledge from\nconceptual models can aid in overcoming interpretability issues in ML\nmodels. Here, we have developed the Physics Informed Machine Learning\n(PIML) model at daily timestep, which accounts for memory in the\nhydrological processes and provides an interpretable model structure. We\ndemonstrated three model cases, including lumped model and\nsemi-distributed model structures with and without reservoir. We\nevaluate the first two model structures on three catchments in India,\nand the applicability of the third model structure is shown on the two\nUnited States catchments. Also, we compared the result of the PIML model\nwith the conceptual model (SIMHYD), which is used as the parent model to\nderive contextual cues. Our results show that the PIML model outperforms\nsimple ML model in target variable (streamflow) prediction and SIMHYD\nmodel in predicting target variable and intermediate variables (for\nexample, evapotranspiration, reservoir storage) while being mindful of\nphysical constraints. The water balance and runoff coefficient analysis\nreveals that the PIML model provides physically consistent outputs. The\nPIML modeling approach can make a conceptual model more modular such\nthat it can be applied irrespective of the region for which it is\ndeveloped. The successful application of PIML in different climatic as\nwell as geographical regions shows its generalizability.",
        "link": "http://dx.doi.org/10.22541/essoar.167415209.91340990/v1"
    },
    {
        "id": 6942,
        "title": "Evaluation of the physical interpretability of calibrated building model parameters",
        "authors": "Sarah Juricic, Simon Rouchier, Aurélie Foucquier, Gilles Fraisse",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14305/ibpc.2018.ms-7.03"
    },
    {
        "id": 6943,
        "title": "Semantic Correlations Loss: Improving Model Interpretability for Multi-class Classification",
        "authors": "Xuezhi Tong, Rui Wang, Xiaochun Cao, Wenqi Ren",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9006247"
    },
    {
        "id": 6944,
        "title": "Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-grained Recognition",
        "authors": "Shaoli Huang, Xinchao Wang, Dacheng Tao",
        "published": "2021-10",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00066"
    },
    {
        "id": 6945,
        "title": "An Investigation of Language Model Interpretability via Sentence Editing",
        "authors": "Samuel Stevens, Yu Su",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.blackboxnlp-1.34"
    },
    {
        "id": 6946,
        "title": "Model and Algorithm-Agnostic Clustering Interpretability",
        "authors": "Guilherme S. Oliveira, Fabrício A. Silva, Ricardo V. Ferreira",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "Data clustering through unsupervised algorithms is an important technique in several applications, both in research and industrial projects, allowing similar elements to be associated with each other for knowledge extraction. After grouping, the interpretation and understanding of the created clusters is a crucial step so that they can be used in decision-making. However, this is not a trivial task, since it requires manual and repetitive analyses, which consume time and resources of those involved. In the present work, a solution for the interpretability of clusters generated by unsupervised learning is proposed. Unlike existing solutions in the literature, the proposed approach is independent of the model and algorithm used for clustering, and generates easy-to-understand descriptions for end users, facilitating their use by teams from different areas of the companies. The results showed that the solution was able to provide a friendly description to interpret the 13 clusters created to segment 263,684 customers of a company.",
        "link": "http://dx.doi.org/10.5753/kdmile.2023.232618"
    },
    {
        "id": 6947,
        "title": "Recent Development on Extractive Rationale for Model Interpretability: A Survey",
        "authors": "Hao Wang, Yong Dou",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/3cbit57391.2022.00080"
    },
    {
        "id": 6948,
        "title": "Beyond Model Interpretability: On the Faithfulness and Adversarial Robustness of Contrastive Textual Explanations",
        "authors": "Julia El Zini, Mariette Awad",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.100"
    },
    {
        "id": 6949,
        "title": "Fuzzy Logic Model for Digital Forensics: A Trade-off between Accuracy, Complexity and Interpretability",
        "authors": "Andrii Shalaginov",
        "published": "2017-8",
        "citations": 2,
        "abstract": "The Cyber Crime Investigation is challenged by large and complex data as a key factor of emerging Information and Communication Technologies. The size, the velocity, the variety and the complexity of the data have become so high that data mining approaches are no more efficient since they cannot deal with Big Data. As a result, it can be infeasible to represent specific evidences found in such data in a Court of Law in a human-perceivable manner. Moreover, majority of computational methods result in complex and hardly explainable models. However, Soft Computing, a computing with words, can be beneficial in such case. In particular, hybrid Neuro-Fuzzy is capable of learning understandable and precise fuzzy rule-based model. This paper presents novel improvements of NF architecture and corresponding results.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/763"
    },
    {
        "id": 6950,
        "title": "Machine Learning Methods for Low-Cost Pollen Monitoring – Model Optimisation and Interpretability",
        "authors": "Francis Pope, Sophie  A. Mills, José  M. Maya-Manzano, Fiona Tummon, A.  Rob MacKenzie",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4465373"
    },
    {
        "id": 6951,
        "title": "Different Views of Interpretability",
        "authors": "Bertrand Iooss, Ron Kenett, Piercesare Secchi",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12402-0_1"
    },
    {
        "id": 6952,
        "title": "Introducing Explainability and Setting Up Your Development Environment",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9029-3_1"
    },
    {
        "id": 6953,
        "title": "GraphPath: a graph attention model for molecular stratification with interpretability based on the pathway-pathway interaction network",
        "authors": "Teng Ma, Jianxin Wang",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "Abstract\n\nMotivation\nStudying the molecular heterogeneity of cancer is essential for achieving personalized therapy. At the same time, understanding the biological processes that drive cancer development can lead to the identification of valuable therapeutic targets. Therefore, achieving accurate and interpretable clinical predictions requires paramount attention to thoroughly characterizing patients at both the molecular and biological pathway levels.\n\n\nResults\nHere, we present GraphPath, a biological knowledge-driven graph neural network with multi-head self-attention mechanism that implements the pathway-pathway interaction network. We train GraphPath to classify the cancer status of patients with prostate cancer based on their multi-omics profiling. Experiment results show that our method outperforms P-NET and other baseline methods. Besides, two external cohorts are used to validate that the model can be generalized to unseen samples with adequate predictive performance. We reduce the dimensionality of latent pathway embeddings and visualize corresponding classes to further demonstrate the optimal performance of the model. Additionally, since GraphPath’s predictions are interpretable, we identify target cancer-associated pathways that significantly contribute to the model’s predictions. Such a robust and interpretable model has the potential to greatly enhance our understanding of cancer’s biological mechanisms and accelerate the development of targeted therapies.\n\n\nAvailability\nhttps://github.com/amazingma/GraphPath.\n\n\nSupplementary information\nSupplementary data are available at Bioinformatics online.\n",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btae165"
    },
    {
        "id": 6954,
        "title": "PyTorch Model Interpretability and Interface to Sklearn",
        "authors": "Pradeepta Mishra",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-8925-9_10"
    },
    {
        "id": 6955,
        "title": "Model-agnostic vs. Model-intrinsic Interpretability for Explainable Product Search",
        "authors": "Qingyao Ai, Lakshmi Narayanan.R",
        "published": "2021-10-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3459637.3482276"
    },
    {
        "id": 6956,
        "title": "Evolving stochastic configure network: A more compact model with interpretability",
        "authors": "Qin Wang, Jingna Liu, Wenwu Guo, Xizhao Wang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119006"
    },
    {
        "id": 6957,
        "title": "Shapley values as an interpretability technique in credit scoring",
        "authors": "Hendrik Anders du Toit, Willem Daniël Schutte, Helgard Raubenheimer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21314/jrmv.2023.010"
    },
    {
        "id": 6958,
        "title": "Tree-Based Ensemble Machine Learning Model For Predictive and Interpretability Analysis of Nitrate Reduction by Zero-Valent Iron Rate Constant and Product Conversion",
        "authors": "Donghwi Jung, Nurul Alvia Istiqomah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4462942"
    },
    {
        "id": 6959,
        "title": "Model-Agnostic Soft Sensor Interpretability",
        "authors": "Luca Patanè, Francesca Sapuppo, Giuseppa Scipilliti, Maria Gabriella Xibilia",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/metroxraine58569.2023.10405704"
    },
    {
        "id": 6960,
        "title": "Explaining Autism Diagnosis Model Through Local Interpretability Techniques – A Post-hoc Approach",
        "authors": "D. Swainson Sujana, D. Peter Augustine",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452575"
    },
    {
        "id": 6961,
        "title": "Interpretability",
        "authors": "",
        "published": "2017-3-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316716854.008"
    },
    {
        "id": 6962,
        "title": "Interpretability via Random Forests",
        "authors": "Clément Bénard, Sébastien Da Veiga, Erwan Scornet",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12402-0_3"
    },
    {
        "id": 6963,
        "title": "Improving Interpretability of Seismic Images with Directional Image Partitions and Model-Based Techniques",
        "authors": "O. Zdraveva, J. Zuech, G. Zhao, M. Hegazy, R. Gu",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202011101"
    },
    {
        "id": 6964,
        "title": "A Grey-Box Ensemble Model Exploiting Black-Box Accuracy and White-Box Intrinsic Interpretability",
        "authors": "Emmanuel Pintelas, Ioannis E. Livieris, Panagiotis Pintelas",
        "published": "2020-1-5",
        "citations": 71,
        "abstract": "Machine learning has emerged as a key factor in many technological and scientific advances and applications. Much research has been devoted to developing high performance machine learning models, which are able to make very accurate predictions and decisions on a wide range of applications. Nevertheless, we still seek to understand and explain how these models work and make decisions. Explainability and interpretability in machine learning is a significant issue, since in most of real-world problems it is considered essential to understand and explain the model’s prediction mechanism in order to trust it and make decisions on critical issues. In this study, we developed a Grey-Box model based on semi-supervised methodology utilizing a self-training framework. The main objective of this work is the development of a both interpretable and accurate machine learning model, although this is a complex and challenging task. The proposed model was evaluated on a variety of real world datasets from the crucial application domains of education, finance and medicine. Our results demonstrate the efficiency of the proposed model performing comparable to a Black-Box and considerably outperforming single White-Box models, while at the same time remains as interpretable as a White-Box model.",
        "link": "http://dx.doi.org/10.3390/a13010017"
    },
    {
        "id": 6965,
        "title": "Interpretability in Generalized Additive Models",
        "authors": "S. N. Wood, Y. Goude, M. Fasiolo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12402-0_4"
    },
    {
        "id": 6966,
        "title": "The accuracy versus interpretability trade-off in fraud detection model",
        "authors": "Anna Nesvijevskaia, Sophie Ouillade, Pauline Guilmin, Jean-Daniel Zucker",
        "published": "2021",
        "citations": 7,
        "abstract": "AbstractLike a hydra, fraudsters adapt and circumvent increasingly sophisticated barriers erected by public or private institutions. Among these institutions, banks must quickly take measures to avoid losses while guaranteeing the satisfaction of law-abiding customers. Facing an expanding flow of operations, effective banking relies on data analytics to support established risk control processes, but also on a better understanding of the underlying fraud mechanism. In addition, fraud being a criminal offence, the evidential aspect of the process must also be considered. These legal, operational, and strategic constraints lead to compromises on the means to be implemented for fraud management. This paper first focuses on the translation of practical questions raised in the banking industry at each step of the fraud management process into performance evaluation required to design a fraud detection model. Secondly, it considers a range of machine learning approaches that address these specificities: the imbalance between fraudulent and nonfraudulent operations, the lack of fully trusted labels, the concept-drift phenomenon, and the unavoidable trade-off between accuracy and interpretability of detection. This state-of-the-art review sheds some light on a technology race between black box machine learning models improved by post-hoc interpretation and intrinsic interpretable models boosted to gain accuracy. Finally, it discusses how concrete and promising hybrid approaches can provide pragmatic, short-term answers to banks and policy makers without swallowing up stakeholders with economical and ethical stakes in this technological race.",
        "link": "http://dx.doi.org/10.1017/dap.2021.3"
    },
    {
        "id": 6967,
        "title": "Evaluating MASHAP as a faster alternative to LIME for model-agnostic machine learning interpretability",
        "authors": "Andreas Messalas, Christos Aridas, Yannis Kanellopoulos",
        "published": "2020-12-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378034"
    },
    {
        "id": 6968,
        "title": "Contrastive Explanations for Model Interpretability",
        "authors": "Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg",
        "published": "2021",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.120"
    },
    {
        "id": 6969,
        "title": "CVII: Enhancing Interpretability in Intelligent Sensor Systems via Computer Vision Interpretability Index",
        "authors": "Hossein Mohammadi, Krishnaprasad Thirunarayan, Lingwei Chen",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "In the realm of intelligent sensor systems, the dependence on Artificial Intelligence (AI) applications has heightened the importance of interpretability. This is particularly critical for opaque models such as Deep Neural Networks (DNN), as understanding their decisions is essential, not only for ethical and regulatory compliance, but also for fostering trust in AI-driven outcomes. This paper introduces the novel concept of a Computer Vision Interpretability Index (CVII). The CVII framework is designed to emulate human cognitive processes, specifically in tasks related to vision. It addresses the intricate challenge of quantifying interpretability, a task that is inherently subjective and varies across domains. The CVII is rigorously evaluated using a range of computer vision models applied to the COCO (Common Objects in Context) dataset, a widely recognized benchmark in the field. The findings established a robust correlation between image interpretability, model selection, and CVII scores. This research makes a substantial contribution to enhancing interpretability for human comprehension, as well as within intelligent sensor applications. By promoting transparency and reliability in AI-driven decision-making, the CVII framework empowers its stakeholders to effectively harness the full potential of AI technologies.",
        "link": "http://dx.doi.org/10.3390/s23249893"
    },
    {
        "id": 6970,
        "title": "Enhancing Interpretability in Power Management: A Time-Encoded Forecasting Using Hybrid Deep Learning Model for Household Power Consumption",
        "authors": "Hamza Mubarak, Sascha Stegen, Feifei Bai, Abdallah Abdellatif, Mohammad  J. Sanjari",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4743211"
    },
    {
        "id": 6971,
        "title": "Enhancing Model Interpretability and Accuracy for Disease Progression Prediction via Phenotype-Based Patient Similarity Learning",
        "authors": "Yue Wang, Tong Wu, Yunlong Wang, Gao Wang",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811215636_0045"
    },
    {
        "id": 6972,
        "title": "Model agnostic interpretability of rankers via intent modelling",
        "authors": "Jaspreet Singh, Avishek Anand",
        "published": "2020-1-27",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3351095.3375234"
    },
    {
        "id": 6973,
        "title": "Interpretability in deep learning for finance: a case study for the Heston model",
        "authors": "Damiano Brigo, Xiaoshan Huang, Andrea Pallavicini, Haitz Sáez de Ocáriz Borde",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3829947"
    },
    {
        "id": 6974,
        "title": "Hybridization of model-specific and model-agnostic methods for interpretability of Neural network predictions: Application to a power plant",
        "authors": "Tina Danesh, Rachid Ouaret, Pascal Floquet, Stephane Negny",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compchemeng.2023.108306"
    },
    {
        "id": 6975,
        "title": "Optimized utilization and interpretability of process data with data-driven model",
        "authors": "De Bao, Shihua Li, Yongjian Wang",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isas59543.2023.10164439"
    },
    {
        "id": 6976,
        "title": "LoRMIkA: Local rule-based model interpretability with k-optimal associations",
        "authors": "Dilini Rajapaksha, Christoph Bergmeir, Wray Buntine",
        "published": "2020-11",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2020.05.126"
    },
    {
        "id": 6977,
        "title": "Retracted: Handling Interpretability of Deep Learning Model in real-time application by implementing big data Techniques",
        "authors": "Shaik Vaseem, Akram Anil Kumar Dixit",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon55714.2022.9972650"
    },
    {
        "id": 6978,
        "title": "Deep-insight visible neural network (DI-VNN) for improving interpretability of a non-image deep learning model by data-driven ontology",
        "authors": "Herdiantri Sufriyana, Yu Wei Wu, Emily Chia-Yu Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWe aimed to provide a framework that organizes internal properties of a convolutional neural network (CNN) model using non-image data to be interpretable by human. The interface was represented as ontology map and network respectively by dimensional reduction and hierarchical clustering techniques. The applicability is to implement a prediction model either to classify categorical or to estimate numerical outcome, including but not limited to that using data from electronic health records. This pipeline harnesses invention of CNN algorithms for non-image data while improving the depth of interpretability by data-driven ontology. However, the DI-VNN is only for exploration beyond its predictive ability, which requires further explanatory studies, and needs a human user with specific competences in medicine, statistics, and machine learning to explore the DI-VNN with high confidence. The key stages consisted of data preprocessing, differential analysis, feature mapping, network architecture construction, model training and validation, and exploratory analysis.",
        "link": "http://dx.doi.org/10.21203/rs.3.pex-1637/v2"
    },
    {
        "id": 6979,
        "title": "Preventive control for power system transient security based on XGBoost and DCOPF with consideration of model interpretability",
        "authors": "",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.04780"
    },
    {
        "id": 6980,
        "title": "Adversarial Attack Model Based on Deep Neural Network Interpretability and Artificial Fish Swarm Algorithm",
        "authors": "Yamin Li",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijesdf.2024.10057841"
    },
    {
        "id": 6981,
        "title": "Deep-insight visible neural network (DI-VNN) for improving interpretability of a non-image deep learning model by data-driven ontology",
        "authors": "Herdiantri Sufriyana, Yu Wei Wu, Emily Chia-Yu Su",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nWe aimed to provide a framework that organizes internal properties of a convolutional neural network (CNN) model using non-image data to be interpretable by human. The interface was represented as ontology map and network respectively by dimensional reduction and hierarchical clustering techniques. The applicability is to implement a prediction model either to classify categorical or to estimate numerical outcome, including but not limited to that using data from electronic health records. This pipeline harnesses invention of CNN algorithms for non-image data while improving the depth of interpretability by data-driven ontology. However, the DI-VNN is only for exploration beyond its predictive ability, which requires further explanatory studies, and needs a human user with specific competences in medicine, statistics, and machine learning to explore the DI-VNN with high confidence. The key stages consisted of data preprocessing, differential analysis, feature mapping, network architecture construction, model training and validation, and exploratory analysis.",
        "link": "http://dx.doi.org/10.21203/rs.3.pex-1637/v1"
    },
    {
        "id": 6982,
        "title": "New Trend in Fintech: Research on Artificial Intelligence Model Interpretability in Financial Fields",
        "authors": "Han Yan, Sheng Lin",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4236/ojapps.2019.910062"
    },
    {
        "id": 6983,
        "title": "VEER: enhancing the interpretability of model-based optimizations",
        "authors": "Kewen Peng, Christian Kaltenecker, Norbert Siegmund, Sven Apel, Tim Menzies",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10664-023-10296-w"
    },
    {
        "id": 6984,
        "title": "Retraction Notice: Handling Interpretability of Deep Learning Model in real-time application by implementing big data Techniques",
        "authors": "Shaik Vaseem, Akram Anil Kumar Dixit",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon55714.2022.10478502"
    },
    {
        "id": 6985,
        "title": "CHOmpact: a reduced metabolic model of Chinese hamster ovary cells with enhanced interpretability",
        "authors": "Ioscani Jiménez del Val, Sarantos Kyriakopoulos, Simone Albrecht, Henning Stockmann, Pauline M Rudd, Karen M Polizzi, Cleo Kontoravdi",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractMetabolic modelling has emerged as a key tool for the characterisation of biopharmaceutical cell culture processes. Metabolic models have also been instrumental in identifying genetic engineering targets and developing feeding strategies that optimise the growth and productivity of Chinese hamster ovary (CHO) cells. Despite their success, metabolic models of CHO cells still present considerable challenges. Genome scale metabolic models (GeMs) of CHO cells are very large (>6000 reactions) and are, therefore, difficult to constrain to yield physiologically consistent flux distributions. The large scale of GeMs also makes interpretation of their outputs difficult. To address these challenges, we have developed CHOmpact, a reduced metabolic network that encompasses 101 metabolites linked through 144 reactions. Our compact reaction network allows us to deploy multi-objective optimisation and ensure that the computed flux distributions are physiologically consistent. Furthermore, our CHOmpact model delivers enhanced interpretability of simulation results and has allowed us to identify the mechanisms governing shifts in the anaplerotic consumption of asparagine and glutamate as well as an important mechanism of ammonia detoxification within mitochondria. CHOmpact, thus, addresses key challenges of large-scale metabolic models and, with further development, will serve as a platform to develop dynamic metabolic models for the control and optimisation of biopharmaceutical cell culture processes.",
        "link": "http://dx.doi.org/10.1101/2021.07.19.452953"
    },
    {
        "id": 6986,
        "title": "CRESPR: Modular sparsification of DNNs to improve pruning performance and model interpretability",
        "authors": "Tianyu Kang, Wei Ding, Ping Chen",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.021"
    },
    {
        "id": 6987,
        "title": "A Comparative Study of Faithfulness Metrics for Model Interpretability Methods",
        "authors": "Chun Sik Chan, Huanqi Kong, Liang Guanqing",
        "published": "2022",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-long.345"
    },
    {
        "id": 6988,
        "title": "Unary Interpretability Logics for Sublogics of the Interpretability Logic $$\\textbf{IL}$$",
        "authors": "Yuya Okawa",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11225-023-10068-z"
    },
    {
        "id": 6989,
        "title": "Deep Learning-Based Intelligent Apple Variety Classification System and Model Interpretability Analysis",
        "authors": "Fanqianhui Yu, Tao Lu, Changhu Xue",
        "published": "2023-2-19",
        "citations": 6,
        "abstract": "In this study, series networks (AlexNet and VGG-19) and directed acyclic graph (DAG) networks (ResNet-18, ResNet-50, and ResNet-101) with transfer learning were employed to identify and classify 13 classes of apples from 7439 images. Two training datasets, model evaluation metrics, and three visualization methods were used to objectively assess, compare, and interpret five Convolutional Neural Network (CNN)-based models. The results show that the dataset configuration had a significant impact on the classification results, as all models achieved over 96.1% accuracy on dataset A (training-to-testing = 2.4:1.0) compared to 89.4–93.9% accuracy on dataset B (training-to-testing = 1.0:3.7). VGG-19 achieved the highest accuracy of 100.0% on dataset A and 93.9% on dataset B. Moreover, for networks of the same framework, the model size, accuracy, and training and testing times increased as the model depth (number of layers) increased. Furthermore, feature visualization, strongest activations, and local interpretable model-agnostic explanations techniques were used to show the understanding of apple images by different trained models, as well as to reveal how and why the models make classification decisions. These results improve the interpretability and credibility of CNN-based models, which provides guidance for future applications of deep learning methods in agriculture.",
        "link": "http://dx.doi.org/10.3390/foods12040885"
    },
    {
        "id": 6990,
        "title": "Model interpretability of financial fraud detection by group SHAP",
        "authors": "Kang Lin, Yuzhuo Gao",
        "published": "2022-12",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.118354"
    },
    {
        "id": 6991,
        "title": "Transformation of Rasch model logits for enhanced interpretability",
        "authors": "Joakim Ekstrand, Albert Westergren, Kristofer Årestedt, Amanda Hellström, Peter Hagell",
        "published": "2022-12-23",
        "citations": 7,
        "abstract": "Abstract\nBackground\nThe Rasch model allows for linear measurement based on ordinal item responses from rating scales commonly used to assess health outcomes. Such linear measures may be inconvenient since they are expressed as log-odds units (logits) that differ from scores that users may be familiar with. It can therefore be desirable to transform logits into more user-friendly ranges that preserve their linear properties. In addition to user-defined ranges, three general transformations have been described in the literature: the least measurable difference (LMD), the standard error of measurement (SEM) and the least significant difference (LSD). The LMD represents the smallest possible meaningful unit, SEM relates the transformed scale values to measurement uncertainty (one unit on the transformed scale represents roughly one standard error), and LSD represents a lower bound for how coarse the transformed scale can be without loss of valid information. However, while logit transformations are relatively common in the health sciences, use of LMD, SEM and LSD transformations appear to be uncommon despite their potential role.\n\nMethods\nLogit transformations were empirically illustrated based on 1053 responses to the Epworth Sleepiness Scale. Logit measures were transformed according to the LMD, SEM and LSD, and into 0–10, 0-100, and the original raw score (0–24) ranges. These transformations were conducted using a freely available Excel tool, developed by the authors, that transforms logits into user-defined ranges along with the LMD, SEM and LSD transformations.\n\nResults\nResulting LMD, SEM and LSD transformations ranged 0-34, 0-17 and 0-12, respectively. When considering these relative to the three user-defined ranges, it is seen that the 0-10 range is narrower than the LSD range (i.e., loss of valid information), and a 0-100 range gives the impression of better precision than there is, since it is considerably wider than the LMD range. However, the 0-24 transformation appears reasonable since it is wider than the LSD, but narrower than the LMD ranges.\n\nConclusions\nIt is suggested that LMD, SEM and LSD transformations are valuable for benchmarking in deciding appropriate ranges when transforming logit measures. This process can be aided by the Excel tool presented and illustrated in this paper.\n",
        "link": "http://dx.doi.org/10.1186/s12874-022-01816-1"
    },
    {
        "id": 6992,
        "title": "Model learning with personalized interpretability estimation (ML-PIE)",
        "authors": "Marco Virgolin, Andrea De Lorenzo, Francesca Randone, Eric Medvet, Mattias Wahde",
        "published": "2021-7-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449726.3463166"
    },
    {
        "id": 6993,
        "title": "Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures",
        "authors": "Caitlin Doogan, Wray Buntine",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.300"
    },
    {
        "id": 6994,
        "title": "Role of influence functions in model interpretability (Conference Presentation)",
        "authors": "Supriyo Chakraborty, Jorge Ortiz, Simon Julier",
        "published": "2018-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2306009"
    },
    {
        "id": 6995,
        "title": "Evaluation of keyness metrics: Reliability and interpretability",
        "authors": "Lukas Sönning",
        "published": "No Date",
        "citations": 2,
        "abstract": "While keyword analysis has become an essential tool in corpus-based work, the question of how to quantify keyness has been subject to considerable methodological debate. This has given rise to a variety of computerized metrics for detecting and ranking candidate items based on the comparison of a target to a reference corpus. Building on previous work, the present paper starts out by delineating four dimensions of keyness, which distinguish between frequency- and dispersion-related perspectives and identify substantively different aspects of typicalness. Existing measures are then organized according to these dimensions and evaluated with regard to two specific criteria, their interpretability and reliability. The first of these, which has been neglected in previous work, is a critical feature if metrics are to offer informative indications of keyness. The second criterion is performance-oriented and reflects the degree to which a metric produces stable and replicable rankings across repeated studies on the same pair of text varieties. Our illustrative analysis, which deals with the identification of key verbs in academic writing, shows considerable differences among indicators with regard to these two criteria. Our findings provide further support for the superiority of the Wilcoxon rank sum test and allow us to identify, within each dimension of keyness, metrics that may be given preference in applied work in light of our criteria.",
        "link": "http://dx.doi.org/10.31234/osf.io/eb2n9"
    },
    {
        "id": 6996,
        "title": "MMINN: Management-Model Informed Neural Network with Interpretability and Causal Relationship",
        "authors": "Shuang Li, Si-Ze Hou, Yutong Yao, Yongqi Sun, Bin Ding",
        "published": "2022-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/prai55851.2022.9904239"
    },
    {
        "id": 6997,
        "title": "Postoperative MPA-AUC Prediction for Kidney Transplant Recipients Based on Model Interpretability Technique",
        "authors": "Qiao Pan, Xinyu Li, Kun Shao",
        "published": "2021-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3468920.3468937"
    },
    {
        "id": 6998,
        "title": "BR-NPA: A non-parametric high-resolution attention model to improve the interpretability of attention",
        "authors": "Tristan Gomez, Suiyi Ling, Thomas Fréour, Harold Mouchère",
        "published": "2022-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2022.108927"
    },
    {
        "id": 6999,
        "title": "Will We Trust What We Don’t Understand? Impact of Model Interpretability and Outcome Feedback on Trust in AI",
        "authors": "Daehwan Ahn, Abdullah Almaatouq, Monisha Gulabani, Kartik Hosanagar",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3964332"
    },
    {
        "id": 7000,
        "title": "Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques",
        "authors": "Radwa El Shawi, Youssef Sherif, Mouaz Al-Mallah, Sherif Sakr",
        "published": "2019-6",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cbms.2019.00065"
    }
]