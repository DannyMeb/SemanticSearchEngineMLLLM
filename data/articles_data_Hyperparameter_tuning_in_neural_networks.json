[
    {
        "id": 16305,
        "title": "Multi-objective machine training based on Bayesian hyperparameter tuning",
        "authors": "Pedro J. Zufiria, Carlos Borrajo, Miguel Taibo",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191772"
    },
    {
        "id": 16306,
        "title": "Hyperparameter Tuning for Deep Neural Networks Based Optimization Algorithm",
        "authors": "D. Vidyabharathi, V. Mohanraj",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.032255"
    },
    {
        "id": 16307,
        "title": "Optimizing Convolutional Neural Networks and Support Vector Machines for Spinach Disease Detection: A Hyperparameter Tuning Study",
        "authors": "Ankita Suryavanshi, Vinay Kukreja, Ayush Dogra",
        "published": "2023-10-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcat59970.2023.10353280"
    },
    {
        "id": 16308,
        "title": "Evaluating Designs for Hyperparameter Tuning in Deep Neural Networks",
        "authors": "Chenlu Shi, Ashley Kathleen Chiu, Hongquan Xu",
        "published": "2023",
        "citations": 1,
        "abstract": "The performance of a learning technique relies heavily on hyperparameter settings. It calls for hyperparameter tuning for a deep learning technique, which may be too computationally expensive for sophisticated learning techniques. As such, expeditiously exploring the relationship between hyperparameters and the performance of a learning technique controlled by these hyperparameters is desired, and thus it entails the consideration of design strategies to collect informative data efficiently to do so. Various designs can be considered for this purpose. The question as to which design to use then naturally arises. In this paper, we examine the use of different types of designs in efficiently collecting informative data to study the surface of test accuracy, a measure of the performance of a learning technique, over hyperparameters. Under the settings we considered, we find that the strong orthogonal array outperforms all other comparable designs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.51387/23-nejsds26"
    },
    {
        "id": 16309,
        "title": "Automatic Hyperparameter Tuning in Sparse Matrix Factorization",
        "authors": "Ryota Kawasumi, Koujin Takeda",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "Abstract\nWe study the problem of hyperparameter tuning in sparse matrix factorization under a Bayesian framework. In prior work, an analytical solution of sparse matrix factorization with Laplace prior was obtained by a variational Bayes method under several approximations. Based on this solution, we propose a novel numerical method of hyperparameter tuning by evaluating the zero point of the normalization factor in a sparse matrix prior. We also verify that our method shows excellent performance for ground-truth sparse matrix reconstruction by comparing it with the widely used algorithm of sparse principal component analysis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/neco_a_01581"
    },
    {
        "id": 16310,
        "title": "Genetic Algorithm For Convolutional Neural Network Hyperparameter Tuning",
        "authors": "Fian Yulio Santoso, Eko Sediyono, Hindriyanto Dwi Purnomo",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit58132.2023.10273928"
    },
    {
        "id": 16311,
        "title": "Automated Hyperparameter Tuning for Airfoil Shape Optimization with Neural Network Models",
        "authors": "Taeho Jeong, Pavankumar Koratikere, Leifur T. Leifsson",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2671"
    },
    {
        "id": 16312,
        "title": "Optional Hyperparameter Tuning of Convolutional Neural Network for ECG Classification",
        "authors": "Muhamad Akbar, Siti Nurmaini, Radiyati Umi Partan",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icimcis60089.2023.10349032"
    },
    {
        "id": 16313,
        "title": "Hyperparameter Tuning of Convolutional Neural Network for Fresh and Rotten Fruit Recognition",
        "authors": "Florence Sia, Nur Shabira Baco",
        "published": "2023-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iicaiet59451.2023.10291915"
    },
    {
        "id": 16314,
        "title": "Optimized Convolutional Neural Network with Hyperparameter Tuning for Multi-Class Brain Tumor Classification",
        "authors": "Abir Hasan, Rizoan Toufiq, Md. Zahirul Islam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441068"
    },
    {
        "id": 16315,
        "title": "AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark",
        "authors": "Gaobin Yang, Jun Du, Maokui He, Shutong Niu, Baoxiang Li, Jiakui Li, Chin-Hui Lee",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1167"
    },
    {
        "id": 16316,
        "title": "A New Optimization Model for MLP Hyperparameter Tuning: Modeling and Resolution by Real-Coded Genetic Algorithm",
        "authors": "Fatima Zahrae El-Hassani, Meryem Amri, Nour-Eddine Joudar, Khalid Haddouch",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "AbstractThis paper introduces an efficient real-coded genetic algorithm (RCGA) evolved for constrained real-parameter optimization. This novel RCGA incorporates three specially crafted evolutionary operators: Tournament Selection (RS) with elitism, Simulated Binary Crossover (SBX), and Polynomial Mutation (PM). The application of this RCGA is directed toward optimizing the MLPRGA+5 model. This model is designed to configure Multilayer Perceptron neural networks by optimizing both their architecture and associated hyperparameters, including learning rates, activation functions, and regularization hyperparameters. The objective function employed is the widely recognized learning loss function, commonly used for training neural networks. The integration of this objective function is supported by the introduction of new variables representing MLP hyperparameter values. Additionally, a set of constraints is thoughtfully designed to align with the structure of the Multilayer Perceptron (MLP) and its corresponding hyperparameters. The practicality and effectiveness of the MLPRGA+5 approach are demonstrated through extensive experimentation applied to four datasets from the UCI machine learning repository. The results highlight the remarkable performance of MLPRGA+5, characterized by both complexity reduction and accuracy improvement.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11578-0"
    },
    {
        "id": 16317,
        "title": "Simplifying Hyperparameter Derivation for Integration Neural Networks Using Information Criterion*",
        "authors": "Yoshiharu Iwata, Hidefumi Wakamatsu",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sii58957.2024.10417243"
    },
    {
        "id": 16318,
        "title": "Hyperparameter Optimization of Graph Neural Networks for mRNA Degradation Prediction",
        "authors": "Viktorija Vodilovska, Sonja Gievska, Ilinka Ivanoska",
        "published": "2023-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mipro57284.2023.10159737"
    },
    {
        "id": 16319,
        "title": "Drop edges and adapt: A fairness enforcing fine-tuning for graph neural networks",
        "authors": "Indro Spinelli, Riccardo Bianchini, Simone Scardapane",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.002"
    },
    {
        "id": 16320,
        "title": "Genetic hyperparameter optimization with Modified Scalable-Neighbourhood Component Analysis for breast cancer prognostication",
        "authors": "Shtwai Alsubai, Abdullah Alqahtani, Mohemmed Sha",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.02.035"
    },
    {
        "id": 16321,
        "title": "Hyperparameter Tuning of a Deep Learning EEG-based Neural Network for the Diagnosis of ADHD",
        "authors": "Javier Sanchis, Miguel A. Teruel, Juan Trujillo",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386394"
    },
    {
        "id": 16322,
        "title": "Hyperparameter Optimization for Convolutional Neural Networks using the Salp Swarm Algorithm",
        "authors": "Entesar Abdulsaed, Maytham Alabbas, Raidah Khudeyer",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v47i9.5148"
    },
    {
        "id": 16323,
        "title": "Optimizing Malware Detection Using Back Propagation Neural Network and Hyperparameter Tuning",
        "authors": "Annisa Arrumaisha Siregar, Sopian Soim, Mohammad Fadhli",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "The escalating growth of the internet has led to an increase in cyber threats, particularly malware, posing significant risks to computer systems and networks. This research addresses the challenge of developing sophisticated malware detection systems by optimizing the Back Propagation Neural Network (BPNN) with hyperparameter tuning. The specific focus is on fine-tuning essential hyperparameters, including dropout rate, number of neurons in hidden layers, and number of hidden layers, to enhance the accuracy of malware detection. A Back Propagation Neural Network (BPNN) with dropout regularization is trained on an extensive dataset as part of the research design. Hyperparameter optimization is conducted using GridSearchCV, with experiments varying learning rates and epochs. The best configuration achieves outstanding results, with 98% accuracy, precision, recall, and F1-score. The proposed approach presents an efficient and reliable solution to bolster cybersecurity systems against malware threats.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24014/ijaidm.v6i2.24731"
    },
    {
        "id": 16324,
        "title": "Hyperparameter Tuning on Graph Neural Network for the Classification of SARS-CoV-2 Inhibitors",
        "authors": "Salamet Nur Himawan, Robieth Sohiburoyyan, Iryanto Iryanto",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "COVID-19 is caused by the SARS-CoV-2 virus, which results in a range of symptoms, from mild to severe, and can lead to fatalities. As of October 2023, WHO has recorded 771 cases of COVID-19 globally. Various efforts have been made to control the spread of the virus, including vaccination, isolation measures, and intensive medical care. The emergence of new SARS-CoV-2 variants has led to the ongoing evolution of virus transmission. Continued research is essential to understand this virus and develop strategies to address the pandemic. Inhibitors of SARS-CoV-2 play a crucial role in the vaccine development process. Inhibitors can impede the virus's development, helping reduce disease severity and control the pandemic. The classification of inhibitors is expected to serve as a foundation for selecting compounds that can be developed into vaccines. This research develops a Graph Neural Network model for inhibitor classification and uses the random search method for hyperparameter tuning. Graph Neural Networks are chosen due to their excellent performance in modelling graph data. This study demonstrates the success of hyperparameter tuning in improving the performance of the Graph Neural Network for accurate classification of SARS-CoV-2 inhibitors.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30871/jaic.v7i2.6735"
    },
    {
        "id": 16325,
        "title": "Application of Artificial Neural Network Method using Hyperparameter Tuning for Predicting of Euro Exchange Rupiah",
        "authors": "Dian Kurniasari, Amelia Fallizia Putri, Warsono Warsono, Notiragayu Notiragayu",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "The Covid-19 pandemic has significantly impacted the economic decline in many countries, such as Italy, the United States and the European Union. Indonesia, also affected by Covid-19, was not spared from economic turmoil, especially in the foreign exchange market, where the rupiah exchange rate against the Euro experienced significant fluctuations in early 2020, hampering international trade and investment activities. Therefore, an appropriate method is needed to predict changes in the rupiah exchange rate against the Euro to minimize the obstacles. This study uses the ANN model to predict the Rupiah (Rp) exchange rate against the Euro (€). The best model is obtained through the hyper-tuning process. The optimal parameter values obtained are the input layer with 10 nodes, 2 hidden layers with 19 nodes and 13 nodes, the output layer, dropout of 0.2, 32 batch sizes, 100 epochs, and the Tanh activation function in the distribution scheme of 90% training data and 10 % testing data. Based on the MAPE value of 0.0042% and 0.0041% obtained, the prediction results on the selling and buying rates of the Rupiah against the Euro, it can be concluded that the model has good predictive ability with an accuracy value of 99.996%.Keywords: Currency Exchange Rates, Data Mining, Machine Learning, Artificial Neural Networks ",
        "keywords": "",
        "link": "http://dx.doi.org/10.18495/jsi.v15i1.19867"
    },
    {
        "id": 16326,
        "title": "Deep Neural Network with Hyperparameter Tuning on Early Detection for Symptom Recognition in Suspected Covid-19",
        "authors": "Djuniadi Djuniadi, Nur Iksan, Alfa Faridh Suni, Ahmad Fashiha Hastawan",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "The Coronavirus outbreak (COVID-19) is still a concern for the world according to WHO. Although this virus has been controlled, prevention efforts are still being carried out. Prevention of the virus can be done by identifying patients with symptoms such as fever, respiratory distress, and sore throat. This research aims to develop an early detection system through the recognition of symptoms of COVID-19 infection using thermal camera sensors combined with the DNN using Hyperparameter Tuning. The result is the DNN algorithm can be proposed as the right algorithm to detect someone suspected of COVID-19.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18421/tem122-46"
    },
    {
        "id": 16327,
        "title": "Finger vein identification system using capsule networks with hyperparameter tuning",
        "authors": "Vandy Achmad Yulianto, Nazrul Effendy, Agus Arif",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "<p>Safety and security systems are essential for personnel who need to be protected and valuables. The security and safety system can be supported using a biometric system to identify and verify permitted users or owners. Finger vein is one type of biometric system that has high-level security. The finger vein biometrics system has two primary functions: identification and verification. Safety and security technology development is often followed by hackers' development of science and technology. Therefore, the science and technology of safety and security need to be continuously developed. The paper proposes finger vein identification using capsule networks with hyperparameter tuning. The augmentation, convolution layer parameters, and capsule layers are optimized. The experimental results show that the capsule network with hyperparameter tuning successfully identifies the finger vein images. The system achieves an accuracy of 91.25% using the Shandong University machine learning and applications-homologous multimodal traits (SDUMLA-HMT) dataset.</p>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i4.pp1636-1643"
    },
    {
        "id": 16328,
        "title": "Hyperparameter optimization of pre-trained convolutional neural networks using adolescent identity search algorithm",
        "authors": "Ebubekir Akkuş, Ufuk Bal, Fatma Önay Koçoğlu, Selami Beyhan",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09121-8"
    },
    {
        "id": 16329,
        "title": "Population-Based Hyperparameter Tuning With Multitask Collaboration",
        "authors": "Wendi Li, Ting Wang, Wing W. Y. Ng",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3130896"
    },
    {
        "id": 16330,
        "title": "Hyperparameter Optimization in Convolutional Neural Networks for Maize Seed Classification",
        "authors": "Sertuğ FİDAN, Ali Murat Tiryaki",
        "published": "2023-3-28",
        "citations": 0,
        "abstract": "Corn farming is of great importance for the continuity of our society. Because corn is a cheap and efficient food, especially for animal feeding. However, with the Doubled-haploid technique, the selection of the haploid seeds necessary for this job to be done efficiently creates a problem. Today, the selection of haploid seeds is usually done by trained technicians. With the development of machine learning methods, the parts expected from technicians can be made by machines. In this study, a new model architecture based on a convolutional neural network (CNN) was produced to perform the selection of haploid seeds and the hyperparameters of this model were optimized with the use of tree-structured parzen estimator algorithm. The newly produced model achieved a 94.66% validation score, higher than the VGG-19 model, which proved to be relatively efficient.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56038/ejrnd.v3i1.254"
    },
    {
        "id": 16331,
        "title": "Hyperparameter Tuning of Neural Network for High-Dimensional Problems in the Case of Helmholtz Equation",
        "authors": "D. N. Polyakov, M. M. Stepanova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s0027134923070263"
    },
    {
        "id": 16332,
        "title": "Resampling and Hyperparameter Tuning for Optimizing Breast Cancer Prediction Using Light Gradient Boosting",
        "authors": "Kartika Handayani, Erni Erni, Rangga Pebrianto, Ari Abdilah, Rifky Permana, Eni Pudjiarti",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012446100003848"
    },
    {
        "id": 16333,
        "title": "A generalized CNN model with automatic hyperparameter tuning for millimeter wave channel prediction",
        "authors": "Chengfang Yue, Hui Tang, Jun Yang, Li Chai",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/jcn.2023.000024"
    },
    {
        "id": 16334,
        "title": "Boosting fine-tuning via Conditional Online Knowledge Transfer",
        "authors": "Zhiqiang Liu, Yuhong Li, Chengkai Huang, KunTing Luo, Yanxia Liu",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.035"
    },
    {
        "id": 16335,
        "title": "The Impact of Hyperparameter Tuning in Convolutional Neural Network on Image Classification Model: A Case Study of Plant Disease Detection",
        "authors": "Anselmus Halim, Callista Chow, Michelle Amabel, Said Achmad, Rhio Sutoyo",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoris60118.2023.10352209"
    },
    {
        "id": 16336,
        "title": "Surrogate Model Based on an MLP Neural Network and Bayesian Hyperparameter Tuning for Ship Hull Form Optimization",
        "authors": "Yi Zhang, Ning Ma, Xiechong Gu, QiQi Shi",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17736/ijope.2023.mmm30"
    },
    {
        "id": 16337,
        "title": "Adaptive Hyperparameter Fine-Tuning for Boosting the Robustness and Quality of the Particle Swarm Optimization Algorithm for Non-Linear RBF Neural Network Modelling and Its Applications",
        "authors": "Zohaib Ahmad, Jianqiang Li, Tariq Mahmood",
        "published": "2023-1-3",
        "citations": 9,
        "abstract": "A method is proposed for recognizing and predicting non-linear systems employing a radial basis function neural network (RBFNN) and robust hybrid particle swarm optimization (HPSO) approach. A PSO is coupled with a spiral-shaped mechanism (HPSO-SSM) to optimize the PSO performance by mitigating its constraints, such as sluggish convergence and the local minimum dilemma. Three advancements are incorporated into the hypothesized HPSO-SSM algorithms to achieve remarkable results. First, the diversity of the search process is promoted to update the inertial weight ω based on the logistic map sequence. Then, two distinct parameters are trained in the original position update algorithm to enhance the work efficiency of the successive generation. Finally, the proposed approach employs a spiral-shaped mechanism as a local search operator inside the optimum solution space. Moreover, the HPSO-SSM method concurrently improves the RBFNN parameters and network size, building a model with a compact configuration and higher precision. Two non-linear benchmark functions and the total phosphorus (TP) modelling issue in a waste water treatment process (WWTP) are utilized to assess the overall efficacy of the creative technique. The results of testing indicate that the projected HPSO-SSM-RBFNN algorithm performed very effectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11010242"
    },
    {
        "id": 16338,
        "title": "Prediction of concrete compressive strength using deep neural networks based on hyperparameter optimization",
        "authors": "Mohammed Naved, Mohammed Asim, Tanvir Ahmad",
        "published": "2024-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/23311916.2023.2297491"
    },
    {
        "id": 16339,
        "title": "On a framework of data assimilation for hyperparameter estimation of spiking neuronal networks",
        "authors": "Wenyong Zhang, Boyu Chen, Jianfeng Feng, Wenlian Lu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.016"
    },
    {
        "id": 16340,
        "title": "State of Health Estimation of Lithium-Ion Battery via Bidirectional Long Short-Term Memory Neural Network with Bayesian Hyperparameter Tuning",
        "authors": "Panagiotis Eleftheriadis, Spyridon Giazitzis, Sonia Leva, Emanuele Ogliari",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/etfg55873.2023.10407877"
    },
    {
        "id": 16341,
        "title": "Bayesian Hyperparameter Optimization of stacked Bidirectional Long Short-Term Memory neural network for the State of Charge estimation",
        "authors": "Panagiotis Eleftheriadis, Sonia Leva, Emanuele Ogliari",
        "published": "2023-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101160"
    },
    {
        "id": 16342,
        "title": "Fast Hyperparameter Tuning for Ising Machines",
        "authors": "Matthieu Parizy, Norihiro Kakuko, Nozomu Togawa",
        "published": "2023-1-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce56470.2023.10043382"
    },
    {
        "id": 16343,
        "title": "Hyperparameter Tuning Deep Learning for Imbalanced Data",
        "authors": "Refi Riduan Achmad, Muhammad Haris",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "Imbalanced data is a challenge for the performance of classification algorithms. A situation where two classes consisting of the majority class dominate the minority class. As a result, algorithmic models tend to have high accuracy against the majority class. Imbalanced data can occur on any type of data, including data coming from Twitter. Twitter is one of the social media that is widely used to think about various things, including about the future Presidential candidate of the Republic of Indonesia in 2024. Tweet data was collected from October 8, 2022, to January 10, 2023. Anies Baswedan has a total of 34,962 tweets, Ganjar Pranowo 39,796 tweets, and Prabowo Subianto 12,398 tweets. These tweets can be identified to be categorized into positive sentiments and negative sentiments using several classification algorithm methods, namely Decision Tree, Naïve Bayes, and Deep Learning.  The dataset comes from the tweets of Twitter netizens who are scraped and preprocessed using the RapidMiner tool.  Prabowo Subianto's dataset achieved the best performance using the Deep Learning model with an accuracy rate of 85.42%, precision of 63.30%, recall of 91.77%, and AUC of 0.867.",
        "keywords": "",
        "link": "http://dx.doi.org/10.51967/tepian.v4i2.2216"
    },
    {
        "id": 16344,
        "title": "Neural Networks Architecture and Hyperparameter Exploration for Handover Simulation in 5G Network",
        "authors": "W. Abdullah Rafa, Ali Zayn Murteza, Baud Prananto,  Iskandar",
        "published": "2023-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tssa59948.2023.10366888"
    },
    {
        "id": 16345,
        "title": "Grammatical Evolution-Driven Algorithm for Efficient and Automatic Hyperparameter Optimisation of Neural Networks",
        "authors": "Gauri Vaidya, Meghana Kshirsagar, Conor Ryan",
        "published": "2023-6-29",
        "citations": 0,
        "abstract": "Neural networks have revolutionised the way we approach problem solving across multiple domains; however, their effective design and efficient use of computational resources is still a challenging task. One of the most important factors influencing this process is model hyperparameters which vary significantly with models and datasets. Recently, there has been an increased focus on automatically tuning these hyperparameters to reduce complexity and to optimise resource utilisation. From traditional human-intuitive tuning methods to random search, grid search, Bayesian optimisation, and evolutionary algorithms, significant advancements have been made in this direction that promise improved performance while using fewer resources. In this article, we propose HyperGE, a two-stage model for automatically tuning hyperparameters driven by grammatical evolution (GE), a bioinspired population-based machine learning algorithm. GE provides an advantage in that it allows users to define their own grammar for generating solutions, making it ideal for defining search spaces across datasets and models. We test HyperGE to fine-tune VGG-19 and ResNet-50 pre-trained networks using three benchmark datasets. We demonstrate that the search space is significantly reduced by a factor of ~90% in Stage 2 with fewer number of trials. HyperGE could become an invaluable tool within the deep learning community, allowing practitioners greater freedom when exploring complex problem domains for hyperparameter fine-tuning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16070319"
    },
    {
        "id": 16346,
        "title": "Mood of Song Detection Using Mel Frequency Cepstral Coefficient and Convolutional Neural Network with Tuning Hyperparameter",
        "authors": "Wildan Budiawan Zulfikar, Yana Aditia Gerhana, Aulia Yasmin Putri Almi, Dian Sa’Adillah Maylawati, Muhammad Insan Al Amin",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/citsm60085.2023.10455644"
    },
    {
        "id": 16347,
        "title": "Fast Model Selection and Hyperparameter Tuning for Generative Models",
        "authors": "Luming Chen, Sujit K. Ghosh",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "Generative models have gained significant attention in recent years. They are increasingly used to estimate the underlying structure of high-dimensional data and artificially generate various kinds of data similar to those from the real world. The performance of generative models depends critically on a good set of hyperparameters. Yet, finding the right hyperparameter configuration can be an extremely time-consuming task. In this paper, we focus on speeding up the hyperparameter search through adaptive resource allocation, early stopping underperforming candidates quickly and allocating more computational resources to promising ones by comparing their intermediate performance. The hyperparameter search is formulated as a non-stochastic best-arm identification problem where resources like iterations or training time constrained by some predetermined budget are allocated to different hyperparameter configurations. A procedure which uses hypothesis testing coupled with Successive Halving is proposed to make the resource allocation and early stopping decisions and compares the intermediate performance of generative models by their exponentially weighted Maximum Means Discrepancy (MMD). The experimental results show that the proposed method selects hyperparameter configurations that lead to a significant improvement in the model performance compared to Successive Halving for a wide range of budgets across several real-world applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/e26020150"
    },
    {
        "id": 16348,
        "title": "Breast Cancer Detection Using ResNet with Hyperparameter Tuning",
        "authors": "Jiatai Mu",
        "published": "2023-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca56706.2023.10076197"
    },
    {
        "id": 16349,
        "title": "Confounder balancing in adversarial domain adaptation for pre-trained large models fine-tuning",
        "authors": "Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu, Yukang Lin",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106173"
    },
    {
        "id": 16350,
        "title": "Hyperparameter Tuning for Address Validation using Optuna",
        "authors": "Mariya Evtimova",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Public institutions generally share personal information on their websites. That allows the possibility to find personal information when performing internet searches quickly. However, the personal information that is on the internet is not always accurate and can lead to misunderstandings and ambiguity concerning the accessible postal address information. That can be crucial if the information is used to find the location of the corresponding person or to use it as a postal address for correspondence. Many websites contain personal information, but sometimes as people change the web address, information is not up to date or is incorrect. To synchronize the available personal information on the internet could be used an algorithm for validation and verification of the personal addresses. In the paper, a hyperparameter tuning for address validation using the ROBERTa model of the Hugging Face Transformers library. It discusses the implementation of hyperparameter tuning for address validation and its evaluation to achieve high precision and accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37394/232018.2024.12.10"
    },
    {
        "id": 16351,
        "title": "A Linear Programming Enhanced Genetic Algorithm for Hyperparameter Tuning in Machine Learning",
        "authors": "Ankur Sinha, Paritosh Pankaj",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254162"
    },
    {
        "id": 16352,
        "title": "Enhancing Plant Disease Classification through Manual CNN Hyperparameter Tuning",
        "authors": "Khaoula Taji, Fadoua Ghanimi",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "Diagnosing plant diseases is a challenging task due to the complex nature of plants and the visual similarities among different species. Timely identification and classification of these diseases are crucial to prevent their spread in crops. Convolutional Neural Networks (CNN) have emerged as an advanced technology for image identification in this domain. This study explores deep neural networks and machine learning techniques to diagnose plant diseases using images of affected plants, with a specific emphasis on developing a CNN model and highlighting the importance of hyperparameters for precise results. The research involves processes such as image preprocessing, feature extraction, and classification, along with a manual exploration of diverse hyperparameter settings to evaluate the performance of the proposed CNN model trained on an openly accessible dataset. The study compares customized CNN models for the classification of plant diseases, demonstrating the feasibility of disease classification and automatic identification through machine learning-based approaches. It specifically presents a CNN model and traditional machine learning methodologies for categorizing diseases in apple and maize leaves, utilizing a dataset comprising 7023 images divided into 8 categories. The evaluation criteria indicate that the CNN achieves an impressive accuracy of approximately 98.02%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56294/dm2023112"
    },
    {
        "id": 16353,
        "title": "Gradient Sparsification For Masked Fine-Tuning of Transformers",
        "authors": "James O'Neill, Sourav Dutta",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191206"
    },
    {
        "id": 16354,
        "title": "Efficient hyperparameter tuning for predicting student performance with Bayesian optimization",
        "authors": "Saleh Albahli",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17525-w"
    },
    {
        "id": 16355,
        "title": "Remaining Useful Life Estimation of Lithium-Ion Batteries Via Hyperparameter Optimized Bi-Long Short-Term Memory Recurrent Neural Networks",
        "authors": "Rahul Sahay, Karkulali Pugalenthi, Nagarajan Raghavan",
        "published": "2023-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/phm-hangzhou58797.2023.10482631"
    },
    {
        "id": 16356,
        "title": "Beans classification using decision tree and random forest with randomized search hyperparameter tuning",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.28919/cmbn/8225"
    },
    {
        "id": 16357,
        "title": "Genetic Algorithm-Based Hyperparameter Optimization for Convolutional Neural Networks in the Classification of Crop Pests",
        "authors": "Enes Ayan",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13369-023-07916-4"
    },
    {
        "id": 16358,
        "title": "Grid search hyperparameter tuning in additive manufacturing processes",
        "authors": "Michael Ogunsanya, Joan Isichei, Salil Desai",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mfglet.2023.08.056"
    },
    {
        "id": 16359,
        "title": "Hyperparameter tuning of GDBT models for prediction of heart disease",
        "authors": "Qingcong Lv",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2668449"
    },
    {
        "id": 16360,
        "title": "Hyperparameter importance and optimization of quantum neural networks across small datasets",
        "authors": "Charles Moussa, Yash J. Patel, Vedran Dunjko, Thomas Bäck, Jan N. van Rijn",
        "published": "2024-4",
        "citations": 1,
        "abstract": "AbstractAs restricted quantum computers become available, research focuses on finding meaningful applications. For example, in quantum machine learning, a special type of quantum circuit called a quantum neural network is one of the most investigated approaches. However, we know little about suitable circuit architectures or important model hyperparameters for a given task. In this work, we apply the functional ANOVA framework to the quantum neural network architectures to analyze which of the quantum machine learning hyperparameters are most influential for their predictive performance. We restrict our study to 7 open-source datasets from the OpenML-CC18 classification benchmark, which are small enough for simulations on quantum hardware with fewer than 20 qubits. Using this framework, three main levels of importance were identified, confirming expected patterns and revealing new insights. For instance, the learning rate is identified as the most important hyperparameter on all datasets, whereas the particular choice of entangling gates used is found to be the least important on all except for one dataset. In addition to identifying the relevant hyperparameters, for each of them, we also learned data-driven priors based on values that perform well on previously seen datasets, which can then be used to steer hyperparameter optimization processes. We utilize these priors in the hyperparameter optimization method hyperband and show that these improve performance against uniform sampling across all datasets by, on average, $$0.53 \\%$$\n\n0.53\n%\n\n, up to $$6.11 \\%$$\n\n6.11\n%\n\n, in cross-validation accuracy. We also demonstrate that such improvements hold on average regardless of the configuration hyperband is run with. Our work introduces new methodologies for studying quantum machine learning models toward quantum model selection in practice. All research code is made publicly available.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06389-8"
    },
    {
        "id": 16361,
        "title": "Hyperparameter Optimization and Feature Inclusion in Graph Neural Networks for Spiking Implementation",
        "authors": "Guojing Cong, Shruti Kulkarni, Seung–Hwan Lim, Prasanna Date, Shay Snyder, Maryam Parsa, Dominic Kennedy, Catherine Schuman",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00232"
    },
    {
        "id": 16362,
        "title": "Algorithms for Hyperparameter Tuning of LSTMs for Time Series Forecasting",
        "authors": "Harshal Dhake, Yashwant Kashyap, Panagiotis Kosmopoulos",
        "published": "2023-4-14",
        "citations": 4,
        "abstract": "The rapid growth in the use of Solar Energy for sustaining energy demand around the world requires accurate forecasts of Solar Irradiance to estimate the contribution of solar power to the power grid. Accurate forecasts for higher time horizons help to balance the power grid effectively and efficiently. Traditional forecasting techniques rely on physical weather parameters and complex mathematical models. However, these techniques are time-consuming and produce accurate results only for short forecast horizons. Deep Learning Techniques like Long Short Term Memory (LSTM) networks are employed to learn and predict complex varying time series data. However, LSTM networks are susceptible to poor performance due to improper configuration of hyperparameters. This work introduces two new algorithms for hyperparameter tuning of LSTM networks and a Fast Fourier Transform (FFT) based data decomposition technique. This work also proposes an optimised workflow for training LSTM networks based on the above techniques. The results show a significant fitness increase from 81.20% to 95.23% and a 53.42% reduction in RMSE for 90 min ahead forecast after using the optimised training workflow. The results were compared to several other techniques for forecasting solar energy for multiple forecast horizons.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15082076"
    },
    {
        "id": 16363,
        "title": "A systematic review of hyperparameter tuning techniques for software quality prediction models",
        "authors": "Ruchika Malhotra, Madhukar Cherukuri",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "BACKGROUND: Software quality prediction models play a crucial role in identifying vulnerable software components during early stages of development, and thereby optimizing the resource allocation and enhancing the overall software quality. While various classification algorithms have been employed for developing these prediction models, most studies have relied on default hyperparameter settings, leading to significant variability in model performance. Tuning the hyperparameters of classification algorithms can enhance the predictive capability of quality models by identifying optimal settings for improved accuracy and effectiveness. METHOD: This systematic review examines studies that have utilized hyperparameter tuning techniques to develop prediction models in software quality domain. The review focused on diverse areas such as defect prediction, maintenance estimation, change impact prediction, reliability prediction, and effort estimation, as these domains demonstrate the wide applicability of common learning algorithms. RESULTS: This review identified 31 primary studies on hyperparameter tuning for software quality prediction models. The results demonstrate that tuning the parameters of classification algorithms enhances the performance of prediction models. Additionally, the study found that certain classification algorithms exhibit high sensitivity to their parameter settings, achieving optimal performance when tuned appropriately. Conversely, certain classification algorithms exhibit low sensitivity to their parameter settings, making tuning unnecessary in such instances. CONCLUSION: Based on the findings of this review, the study conclude that the predictive capability of software quality prediction models can be significantly improved by tuning their hyperparameters. To facilitate effective hyperparameter tuning, we provide practical guidelines derived from the insights obtained through this study.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/ida-230653"
    },
    {
        "id": 16364,
        "title": "Optimizing CNNs for Facial Paralysis Detection: A Hyperparameter Tuning Approach",
        "authors": "Salamet Nur Himawan, Adi Suheryadi, Muhamad Mustamiin",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icic60109.2023.10382112"
    },
    {
        "id": 16365,
        "title": "AutoDDC: Hyperparameter Tuning for Direct Data-Driven Control",
        "authors": "Valentina Breschi, Simone Formentin",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mcs.2023.3310368"
    },
    {
        "id": 16366,
        "title": "Unsupervised Sentence Representation Learning with Frequency-induced Adversarial tuning and Incomplete sentence filtering",
        "authors": "Bing Wang, Ximing Li, Zhiyao Yang, Yuanyuan Guan, Jiayin Li, Shengsheng Wang",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106315"
    },
    {
        "id": 16367,
        "title": "HOAX: a hyperparameter optimisation algorithm explorer for neural networks",
        "authors": "Albert Thie, Maximilian F.S.J. Menger, Shirin Faraji",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/00268976.2023.2172732"
    },
    {
        "id": 16368,
        "title": "Agent-Based Collaborative Random Search for Hyperparameter Tuning and Global Function Optimization",
        "authors": "Ahmad Esmaeili, Zahra Ghorrati, Eric T. Matson",
        "published": "2023-5-5",
        "citations": 5,
        "abstract": "Hyperparameter optimization is one of the most tedious yet crucial steps in training machine learning models. There are numerous methods for this vital model-building stage, ranging from domain-specific manual tuning guidelines suggested by the oracles to the utilization of general purpose black-box optimization techniques. This paper proposes an agent-based collaborative technique for finding near-optimal values for any arbitrary set of hyperparameters (or decision variables) in a machine learning model (or a black-box function optimization problem). The developed method forms a hierarchical agent-based architecture for the distribution of the searching operations at different dimensions and employs a cooperative searching procedure based on an adaptive width-based random sampling technique to locate the optima. The behavior of the presented model, specifically against changes in its design parameters, is investigated in both machine learning and global function optimization applications, and its performance is compared with that of two randomized tuning strategies that are commonly used in practice. Moreover, we have compared the performance of the proposed approach against particle swarm optimization (PSO) and simulated annealing (SA) methods in function optimization to provide additional insights into its exploration in the search space. According to the empirical results, the proposed model outperformed the compared random-based methods in almost all tasks conducted, notably in a higher number of dimensions and in the presence of limited on-device computational resources.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/systems11050228"
    },
    {
        "id": 16369,
        "title": "Hyperparameter Tuning Menggunakan GridsearchCV pada Random Forest untuk Deteksi Malware",
        "authors": "Iik Muhamad Malik Matin",
        "published": "2023-5-6",
        "citations": 0,
        "abstract": "Random forest merpuakan algoritma machine learning yang populer digunakan untuk klasifikasi. Dalam mendeteksi malware, Random forest dapat membantu mengidentifikasi malware dengan akurasi yang baik. Namun, untuk meningkatkan performa model, diperlukan proses hyperparameter tuning. GridsearchCV adalah metode hyperparameter tuning yang memungkinkan pengguna untuk melakukan pemindaian pada sejumlah hyperparameter yang dipilih. Dalam paper ini, kami melakukan eksperimen dengan menggunakan GridsearchCV untuk melakukan hyperparameter tuning pada Random forest untuk tugas deteksi malware. Hasil eksperimen menunjukkan bahwa dengan melakukan hyperparameter tuning dapat meningkatkan akurasi model dalam mengidentifikasi malware",
        "keywords": "",
        "link": "http://dx.doi.org/10.32722/multinetics.v9i1.5578"
    },
    {
        "id": 16370,
        "title": "Data Augmentation and Fine Tuning of Convolutional Neural Network during Training for Person Re-Identification in Video Surveillance Systems",
        "authors": "S. Ye, R. Bohush, H. Chen, S. Ihnatsyeva, S. V. Ablameyko",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23040124"
    },
    {
        "id": 16371,
        "title": "Everyone’s a Winner! On Hyperparameter Tuning of Recommendation Models",
        "authors": "Faisal Shehzad, Dietmar Jannach",
        "published": "2023-9-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3604915.3609488"
    },
    {
        "id": 16372,
        "title": "Data-Free Backbone Fine-Tuning for Pruned Neural Networks",
        "authors": "Adrian Holzbock, Achyut Hegde, Klaus Dietmayer, Vasileios Belagiannis",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10290102"
    },
    {
        "id": 16373,
        "title": "Use of Augmentation Data and Hyperparameter Tuning in Batik Type Classification using the CNN Model",
        "authors": "Siti Auliaddina, Toni Arifin",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32520/stmsi.v13i1.3395"
    },
    {
        "id": 16374,
        "title": "Optimizing a Binary Logistic Regression model by Hyperparameter tuning",
        "authors": "Arnav Oberoi, Om Sehgal, Chirag Malik",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.29322/ijsrp.13.07.2023.p13913"
    },
    {
        "id": 16375,
        "title": "Effect of hyperparameter tuning on classical machine learning models in detecting potholes",
        "authors": "Shaolin Lee Govender, Seena Joseph, Alveen Singh",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictas56421.2023.10082724"
    },
    {
        "id": 16376,
        "title": "Investigating the Effects of Hyperparameter Tuning Process on the Performance of Intrusion Detection Systems",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.59287/as-proceedings.473"
    },
    {
        "id": 16377,
        "title": "HYPERPARAMETER TUNING ON RANDOM FOREST  FOR DIAGNOSE COVID-19",
        "authors": "Anna Baita, Inggar Adi Prasetyo, Nuri Cahyono",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "Diagnosis of Covid using the RT-PCR (Reverse Transcription Polymerase Chain Reaction) test requires high costs and takes a long time. For this reason, another method is needed that can be used to diagnose Covid-19 quickly and accurately. Random Forest is one of the popular classification algorithms for making predictive models. Random forest involves many hyperparameters that control the structure of each tree, the forest, and its randomness. Random Forest is a method which very sensitive to hyperparameter values, as their prediction accuracy can increase significantly when optimized hyperparameters are predefined and then adjusted according to the procedure. The purpose of doing hyperparameter tuning on the random forest algorithm is to increase accuracy in the diagnosis of covid-19. Searching for optimal values of hyperparameters is done by the Grid Search method and Random Search. The result explains that the Random Forest can be used to diagnose Covid-19 with an accuracy of 94%, and with hyperparameter tuning, it can increase the accuracy of the random forest by 2%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33387/jiko.v6i2.6389"
    },
    {
        "id": 16378,
        "title": "An Optimized Feature Selection and Hyperparameter Tuning Framework for Automated Heart Disease Diagnosis",
        "authors": "Saleh Ateeq Almutairi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.041609"
    },
    {
        "id": 16379,
        "title": "Bayesian Sharpness-Aware Prompt Tuning for Cross-Domain Few-shot Learning",
        "authors": "Shuo Fan, Liansheng Zhuang, Aodi Li",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191224"
    },
    {
        "id": 16380,
        "title": "Autoencoder-enabled model portability for reducing hyperparameter tuning efforts in side-channel analysis",
        "authors": "Marina Krček, Guilherme Perin",
        "published": "2023-7-21",
        "citations": 0,
        "abstract": "AbstractHyperparameter tuning represents one of the main challenges in deep learning-based profiling side-channel analysis. For each different side-channel dataset, the typical procedure to find a profiling model is applying hyperparameter tuning from scratch. The main reason is that side-channel measurements from various targets contain different underlying leakage distributions. Consequently, the same profiling model hyperparameters are usually not equally efficient for other targets. This paper considers autoencoders for dimensionality reduction to verify if encoded datasets from different targets enable the portability of profiling models and architectures. Successful portability reduces the hyperparameter tuning efforts as profiling model tuning is eliminated for the new dataset, and tuning autoencoders is simpler. We first search for the best autoencoder for each dataset and the best profiling model when the encoded dataset becomes the training set. Our results show no significant difference in tuning efforts using original and encoded traces, meaning that encoded data reliably represents the original data. Next, we verify how portable is the best profiling model among different datasets. Our results show that tuning autoencoders enables and improves portability while reducing the effort in hyperparameter search for profiling models. Lastly, we present a transfer learning case where dimensionality reduction might be necessary if the model is tuned for a dataset with fewer features than the new dataset. In this case, tuning of the profiling model is eliminated and training time reduced.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13389-023-00330-4"
    },
    {
        "id": 16381,
        "title": "A stochastic optimization technique for hyperparameter tuning in reservoir computing",
        "authors": "Nickson Mwamsojo, Frederic Lehmann, Kamel Merghem, Yann Frignac, Badr-Eddine Benkelfat",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127262"
    },
    {
        "id": 16382,
        "title": "Optimizing Deep LSTM Model through Hyperparameter Tuning for Sensor-Based Human Activity Recognition in Smart Home",
        "authors": "Mariam El Ghazi, Noura Aknin",
        "published": "2024-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v47i10.5268"
    },
    {
        "id": 16383,
        "title": "NeuroEvolution-based hyperparameter tuning for reinforcement learning in SDN computation offloading",
        "authors": "Abhijit Banerjee, Subir Gupta, PK Dutta, El-Sayed M. El-Kenawy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2024.0945"
    },
    {
        "id": 16384,
        "title": "Assessing the feasibility of machine learning-based modelling and prediction of credit fraud outcomes using hyperparameter tuning",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23977/acss.2023.070212"
    },
    {
        "id": 16385,
        "title": "Auto Tuning Quantized Graph Neural Networks on GPU Tensor Core via TVM",
        "authors": "Zhiyuan Wang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456077"
    },
    {
        "id": 16386,
        "title": "Hyper-parameter tuning of physics-informed neural networks: Application to Helmholtz problems",
        "authors": "Paul Escapil-Inchauspé, Gonzalo A. Ruz",
        "published": "2023-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126826"
    },
    {
        "id": 16387,
        "title": "Developing New Fully Connected Layers for Convolutional Neural Networks with Hyperparameter Optimization for Improved Multi-Label Image Classification",
        "authors": "Tamás Katona, Gábor Tóth, Mátyás Petró, Balázs Harangi",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "Chest X-ray evaluation is challenging due to its high demand and the complexity of diagnoses. In this study, we propose an optimized deep learning model for the multi-label classification of chest X-ray images. We leverage pretrained convolutional neural networks (CNNs) such as VGG16, ResNet 50, and DenseNet 121, modifying their output layers and fine-tuning the models. We employ a novel optimization strategy using the Hyperband algorithm to efficiently search the hyperparameter space while adjusting the fully connected layers of the CNNs. The effectiveness of our approach is evaluated on the basis of the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) metric. Our proposed methodology could assist in automated chest radiograph interpretation, offering a valuable tool that can be used by clinicians in the future.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math12060806"
    },
    {
        "id": 16388,
        "title": "Faster Convergence for Transformer Fine-tuning with Line Search Methods",
        "authors": "Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192001"
    },
    {
        "id": 16389,
        "title": "Hyperparameter Tuning Algoritma Supervised Learning untuk Klasifikasi Keluarga Penerima Bantuan Pangan Beras",
        "authors": " Joshua Agung Nurcahyo,  Theopilus Bayu Sasongko",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "Indonesia memiliki berbagai macam program untuk menekan kemiskinan, salah satunya adalah program bantuan pangan beras. Namun, berdasarkan temuan di lapangan, program bantuan ini tidak tepat sasaran. Melalui klasifikasi supervised learning dengan hyperparameter tuning, penelitian ini bertujuan untuk mengetahui algoritma klasifikasi umum yang paling optimal dan akurat dalam menentukan keluarga penerima bantuan pangan beras. Algoritma Support Vector Machine (SVM), decision tree, naïve bayes, dan K-nearest neighbor (Knn) serta metode hyperparameter tuning grid search, random search, dan optimasi bayesian digunakan dalam penelitian. Data pada penelitian ini bersumber dari IFLS. Berdasarkan hasil analisis, penerapan hyperparameter tuning memiliki dampak yang signifikan dalam meningkatkan kinerja algoritma KNN, decision tree, dan SVM. Algoritma Knn dengan random search serta optimasi bayesian dan SVM dengan optimasi bayesian memberikan nilai akurasi yang sama, yakni sebesar 74%.Oleh karena itu, model tersebut memiliki kinerja yang setara dan sama baiknya dalam mengklasifikasikan keluarga penerima bantuan pangan beras.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33022/ijcs.v12i3.3254"
    },
    {
        "id": 16390,
        "title": "Hyperparameter Tuning of Machine Learning Model for Price Prediction of Electric Vehicles",
        "authors": "Sayak Maiti, R. C. Mala, Prateek Jain",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icirca57980.2023.10220698"
    },
    {
        "id": 16391,
        "title": "Multistep Hyperparameter Tuning via Reinforcement Learning for Simulated Annealing",
        "authors": "Hiroshi Yoshitake, Takuya Okuyama, Yudai Kamada, Taisuke Ueta, Junya Fujita",
        "published": "2023-7-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/codit58514.2023.10284154"
    },
    {
        "id": 16392,
        "title": "Klasifikasi Cacat Biji Kopi Menggunakan Metode Transfer Learning dengan Hyperparameter Tuning Gridsearch",
        "authors": "Aryo Michael, Juprianus Rusman",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "Defects in coffee beans can significantly impact the quality of coffee production, which can lead to a decrease in the price of coffee beans in the global coffee market. Currently, coffee bean sorting is still conventionally done to separate defective and non-defective coffee beans, which is a time-consuming process and subject to subjective selection, potentially leading to a decline in the quality of the resulting coffee beans. The objective of this research is to design and measure the performance of deep learning algorithms, CNN MobilNetV2 and DenseNet201, using transfer learning methods where hyperparameter tuning grid search is employed to select the optimal combination of hyperparameters for the defective coffee bean classification model. The study began by collecting a dataset of images of abnormal and defective coffee beans, building a classification model using transfer learning methods that utilized pre-trained models and selecting the best hyperparameters, training the model, and finally testing the created classification model. The research results indicate that the pre-trained MobileNetV2 model with hyperparameter tuning achieved an accuracy of 90%, and the pre-trained DenseNet201 model achieved an accuracy of 93%. The research results indicate that this approach enables the model to achieve excellent performance in recognizing and classifying defective coffee beans with high accuracy",
        "keywords": "",
        "link": "http://dx.doi.org/10.26905/jtmi.v9i1.10035"
    },
    {
        "id": 16393,
        "title": "Bio-inspired algorithm-based hyperparameter tuning for drug-target binding affinity prediction in healthcare",
        "authors": "Moolchand Sharma, Suman Deswal",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "The greatest challenge for healthcare in drug repositioning and discovery is identifying interactions between known drugs and targets. Experimental methods can reveal some drug-target interactions (DTI) but identifying all of them is an expensive and time-consuming endeavor. Machine learning-based algorithms currently cover the DTI prediction problem as a binary classification problem. However, the performance of the DTI prediction is negatively impacted by the lack of experimentally validated negative samples due to an imbalanced class distribution. Hence recasting the DTI prediction task as a regression problem may be one way to solve this problem. This paper proposes a novel convolutional neural network with an attention-based bidirectional long short-term memory (CNN-AttBiLSTM), a new deep-learning hybrid model for predicting drug-target binding affinities. Secondly, it can be arduous and time-intensive to tune the hyperparameters of a CNN-AttBiLSTM hybrid model to augment its performance. To tackle this issue, we suggested a Memetic Particle Swarm Optimization (MPSOA) algorithm, for ascertaining the best settings for the proposed model. According to experimental results, the suggested MPSOA-based CNN- Att-BiLSTM model outperforms baseline techniques with a 0.90 concordance index and 0.228 mean square error in DAVIS dataset, and 0.97 concordance index and 0.010 mean square error in the KIBA dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/idt-230145"
    },
    {
        "id": 16394,
        "title": "Tuning Dari Speech Classification Employing Deep Neural Networks",
        "authors": "Mursal Dawodi, Jawid Ahmad Baktash",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "Recently, many researchers have focused on building and improving speech recognition systems to facilitate and enhance human-computer interaction. Today, Automatic Speech Recognition (ASR) system has become an important and common tool from games to translation systems, robots, and so on. However, there is still a need for research on speech recognition systems for low-resource languages. This article deals with the recognition of a separate word for Dari language, using Mel-frequency cepstral coefficients (MFCCs) feature extraction method and three different deep neural networks including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Multilayer Perceptron (MLP), and two hybrid models of CNN and RNN. We evaluate our models on our built-in isolated Dari words corpus that consists of 1000 utterances for 20 short Dari terms. This study obtained the impressive result of 98.365% average accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijci.2023.120220"
    },
    {
        "id": 16395,
        "title": "Tuning Dari Speech Classification Employing Deep Neural Networks",
        "authors": "Mursal Dawodi, Jawid Ahmad Baktash",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "Recently, many researchers have focused on building and improving speech recognition systems to facilitate and enhance human-computer interaction. Today, Automatic Speech Recognition (ASR) system has become an important and common tool from games to translation systems, robots, and so on. However, there is still a need for research on speech recognition systems for low-resource languages. This article deals with the recognition of a separate word for Dari language, using Mel-frequency cepstral coefficients (MFCCs) feature extraction method and three different deep neural networks including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Multilayer Perceptron (MLP). We evaluate our models on our built-in isolated Dari words corpus that consists of 1000 utterances for 20 short Dari terms. This study obtained the impressive result of 98.365% average accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijnlc.2023.12203"
    },
    {
        "id": 16396,
        "title": "Automated single particle tracking hyperparameter tuning across biological systems and experimental conditions",
        "authors": "Athanasios Oikonomou, Jacob Kæstel-Hansen, Nikos S. Hatzakis",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bpj.2023.11.1752"
    },
    {
        "id": 16397,
        "title": "Performance Analysis of Machine Learning Algorithms with Hyperparameter Tuning for Diabetes Prediction",
        "authors": "J Jenitta, L Swetha Rani, S Manasa",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdcece57866.2023.10151206"
    },
    {
        "id": 16398,
        "title": "Pancreatic Cancer Detection Through Hyperparameter Tuning and Ensemble Methods",
        "authors": "Koteswaramma Dodda, G. Muneeswari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141255"
    },
    {
        "id": 16399,
        "title": "Hyperparameter Tuning of Support Vector Machines for Wind Turbine Detection Using Drones",
        "authors": "Jordan Miller, Colton Seegmiller, Mohammad A.S. Masoum, Mohammad Shekaramiz, Abdennour C. Seibi",
        "published": "2023-5-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ietc57902.2023.10152252"
    },
    {
        "id": 16400,
        "title": "The Role of Hyperparameter Tuning and Feature Engineering in Enhancing Machine Learning Models for Crop Disease Analysis",
        "authors": "Amritpal Sidhu",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictbig59752.2023.10456340"
    },
    {
        "id": 16401,
        "title": "Dynamics-Based Location Prediction and Neural Network Fine-Tuning for Task Offloading in Vehicular Networks",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3837/tiis.2023.12.011"
    },
    {
        "id": 16402,
        "title": "A benchmark-based method for evaluating hyperparameter optimization techniques of neural networks for surface water quality prediction",
        "authors": "Xuan Wang, Yan Dong, Jing Yang, Zhipeng Liu, Jinsuo Lu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11783-024-1814-5"
    },
    {
        "id": 16403,
        "title": "Hyperparameter Optimization of a Convolutional Neural Network Model for Pipe Burst Location in Water Distribution Networks",
        "authors": "André Antunes, Bruno Ferreira, Nuno Marques, Nelson Carriço",
        "published": "2023-3-14",
        "citations": 3,
        "abstract": "The current paper presents a hyper parameterization optimization process for a convolutional neural network (CNN) applied to pipe burst locations in water distribution networks (WDN). The hyper parameterization process of the CNN includes the early stopping termination criteria, dataset size, dataset normalization, training set batch size, optimizer learning rate regularization, and model structure. The study was applied using a case study of a real WDN. Obtained results indicate that the ideal model parameters consist of a CNN with a convolutional 1D layer (using 32 filters, a kernel size of 3 and strides equal to 1) for a maximum of 5000 epochs using a total of 250 datasets (using data normalization between 0 and 1 and tolerance equal to max noise) and a batch size of 500 samples per epoch step, optimized with Adam using learning rate regularization. This model was evaluated for distinct measurement noise levels and pipe burst locations. Results indicate that the parameterized model can provide a pipe burst search area with more or less dispersion depending on both the proximity of pressure sensors to the burst or the noise measurement level.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/jimaging9030068"
    },
    {
        "id": 16404,
        "title": "Prediction of CBR by Deep Artificial Neural Networks with Hyperparameter Optimization by Simulated Annealing",
        "authors": "Crespin Prudence Yabi, Sètondji Wadoscky Agongbe, Bio Chéïssou Koto Tamou, Ehsan Noroozinejad Farsangi, Eric Alamou, Mohamed Gibigaye",
        "published": "2024-2-4",
        "citations": 0,
        "abstract": "AbstractThe construction of pavements requires the complete identification of the soils in place and of the added materials. This identification consists in determining the class of the soils and in evaluating their bearing capacity through the California bearing ratio (CBR) index. Obtaining the CBR index is very costly in terms of time and financial resources, especially when it is a large-scale project. It thus leaves prospects of obtaining it by simpler processes; hence, it arises the need to find simpler processes compared to classical processes. This study develops models for predicting the CBR index from physical properties that are less complex to obtain, based on deep neural networks. To achieve this, three databases were used. A first database consists of the proportion of fines, the Atterberg limits and the Proctor references of the soils. A second database uses the methylene blue value instead of the Atterberg limits, and a third database uses only the proportion of fines and the Proctor soil reference. On each of the databases, a deep neural network model was developed using dense layers, regularization layers, residual blocks and parallelization in TensorFlow to predict the CBR value. Each model was formed by combining several deep neural networks developed according to specific architectures. To expedite training, the simulated annealing method was employed to optimize hyperparameters and define the optimal configuration for each network. The predictions obtained are correlated with the true values from 83.6 to 96.5%. In terms of performance, the models have a mean deviation ranging from 3.74 to 5.96%, a maximum deviation ranging from 12.43 to 16.2% and a squared deviation ranging from 0.781 to 2.189. The results suggest that the variable VBS has a negative impact on the accuracy of the networks in predicting the CBR index. The developed models respect the confidence threshold (± 10%) and can be used to set up a local or regional geotechnical platform.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40098-024-00870-4"
    },
    {
        "id": 16405,
        "title": "A Best Exponential Smoothing Method With Hyperparameter Tuning to Predict the Number of Pandemic Cases",
        "authors": "Bernadus Very Christioko,  Khoirudin, April Firman Daru",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icteca60133.2023.10490864"
    },
    {
        "id": 16406,
        "title": "One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer",
        "authors": "Fabian Schmidt, Ivan Vulić, Goran Glavaš",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.815"
    },
    {
        "id": 16407,
        "title": "Optimasi Gaussian Naïve Bayes dengan Hyperparameter Tuning dan Univariate Feature Selection dalam Prediksi Cuaca",
        "authors": "Lindawati Lindawati, Mohammad Fadhli, Antoniy Sandi Wardana",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "The importance of conducting weather prediction research is due to the significant influence of weather changes on daily life. The purpose of this study is to apply an optimal machine-learning classification method for weather prediction. The method used is the Gaussian Naïve Bayes model, which has been optimized using Univariate Feature Selection ANOVA-f test and Hyperparameter Tuning GridsearchCV techniques. The data used consists of 6454 daily weather data in Palembang City. There are 5 tests on the Gaussian Naïve Bayes model before and after optimization. The research results show that the optimization of the model successfully improves the performance in weather prediction. The highest accuracy result after optimization reaches 98.33% with 644 test data, an improvement from the pre-optimization accuracy of only 96.95%. Before optimization, the predictions for weather conditions such as sunny, cloudy/rainy, light rain, and heavy rain match the actual data. However, there were 20 prediction errors when dealing with data that should represent very heavy rain conditions. After optimization, the number of prediction errors for the very heavy rain data was reduced to seven. The optimization approach used in this research helps find the most suitable parameter combinations and eliminates irrelevant features, allowing the model to consider only significant features in weather p",
        "keywords": "",
        "link": "http://dx.doi.org/10.29408/edumatic.v7i2.21179"
    },
    {
        "id": 16408,
        "title": "Optimizing Age Classification Using Hyperparameter Tuning and Handling Imbalanced Dataset: An Algorithm Decision Forest training Algorithms Approach",
        "authors": " Sarwo, Yulius Denny Prabowo",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icced60214.2023.10425640"
    },
    {
        "id": 16409,
        "title": "Data-Driven Hyperparameter Tuning for Point-Based 3D Semantic Segmentation",
        "authors": "Simon Buus Jensen, Galadrielle Humblot-Renaux, Andreas Møgelmose, Thomas B. Moeslund",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipc59416.2023.10328344"
    },
    {
        "id": 16410,
        "title": "Diabetes Disease Detection Classification Using Light Gradient Boosting (LightGBM) With Hyperparameter Tuning",
        "authors": "Elisa Ramadanti, Devi Aprilya Dinathi, Christianskaditya Christianskaditya, Didih Rizki Chandranegara",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "Diabetes is a condition caused by an imbalance between the need for insulin in the body and insufficient insulin production by the pancreas, causing an increase in blood sugar concentration. This study aims to find the best classification performance on diabetes datasets with the LightGBM method. The dataset used consists of 768 rows and 9 columns, with target values of 0 and 1. In this study, resampling is applied to overcome data imbalance using SMOTE and perform hyperparameter optimization. Model evaluation is performed using confusion matrix and various metrics such as accuracy, recall, precision and f1-score. This research conducted several tests. In hyperparameter optimization tests using GridSearchCV and RandomSearchCV, the LightGBM method showed good performance. In tests that apply data resampling, the LightGBM method achieves the highest accuracy, namely the LightGBM method with GridSearchCV optimization with the highest accuracy reaching 84%, while LightGBM with RandomSearchCV optimization reaches 82% accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33395/sinkron.v8i2.13530"
    },
    {
        "id": 16411,
        "title": "Bayesian Optimization based Hyperparameter Tuning of Ensemble Regression Models in Smart City Air Quality Monitoring Data Analytics",
        "authors": "Saptarshi Das, Ahmed Alzimami",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaisc56366.2023.10085504"
    },
    {
        "id": 16412,
        "title": "Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization",
        "authors": "Homayoon Farrahi, A. Rupam Mahmood",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191867"
    },
    {
        "id": 16413,
        "title": "Evaluating Metaheuristic Algorithms and Neural Networks for PID Tuning",
        "authors": "Abhigyan Adarsh, Amal Chawla, Anirudh Singh, Bharat Bhushan",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsses58299.2023.10200433"
    },
    {
        "id": 16414,
        "title": "Hyperparameter Tuning for LSTM and ARIMA Time Series Model: A Comparative Study",
        "authors": "Uphar Singh, Sanghmitra Tamrakar, Kumar Saurabh, Ranjana Vyas, O.P. Vyas",
        "published": "2023-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indiscon58499.2023.10270325"
    },
    {
        "id": 16415,
        "title": "Continual Learning with Pretrained Backbones by Tuning in the Input Space",
        "authors": "Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci, Tinne Tuytelaars",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191069"
    },
    {
        "id": 16416,
        "title": "Novel hybrid success history intelligent optimizer with Gaussian transformation: application in CNN hyperparameter tuning",
        "authors": "Hussam N. Fakhouri, Sadi Alawadi, Feras M. Awaysheh, Faten Hamad",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10586-023-04161-0"
    },
    {
        "id": 16417,
        "title": "Regression-Based Hyperparameter Learning for Support Vector Machines",
        "authors": "Shili Peng, Wenwu Wang, Yinli Chen, Xueling Zhong, Qinghua Hu",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3321685"
    },
    {
        "id": 16418,
        "title": "Improving fine-tuning of self-supervised models with Contrastive Initialization",
        "authors": "Haolin Pan, Yong Guo, Qinyi Deng, Haomin Yang, Jian Chen, Yiqun Chen",
        "published": "2023-2",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.12.012"
    },
    {
        "id": 16419,
        "title": "Diagnosis of Liver Disease Using ANN and ML Algorithms with Hyperparameter Tuning",
        "authors": "Sonwane Suchitra Shivaji Rao, K Gangadhara Rao",
        "published": "2024-1-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/idciot59759.2024.10467855"
    },
    {
        "id": 16420,
        "title": "AI-assisted ISP hyperparameter auto tuning",
        "authors": "Fa Xu, Zihao Liu, Yanheng Lu, Sicheng Li, Susong Xu, Yibo Fan, Yen-Kuang Chen",
        "published": "2023-6-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aicas57966.2023.10168574"
    },
    {
        "id": 16421,
        "title": "Method for Hyperparameter Tuning of Image Classification with PyCaret",
        "authors": "Kohei Arai, Jin Shimazoe, Mariko Oda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140930"
    },
    {
        "id": 16422,
        "title": "Practical hyperparameters tuning of convolutional neural networks for EEG emotional features classification",
        "authors": "Samia Mezzah, Abdelkamel Tari",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.iswa.2023.200212"
    },
    {
        "id": 16423,
        "title": "Development of an Efficient CNN model with Hyperparameter tuning for Early Prediction of Lung Diseases",
        "authors": "D Meenakshi, M Anbarasan, S Murugesan, B Selvalakshmi",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452479"
    },
    {
        "id": 16424,
        "title": "Optimizing Stroke Mortality Prediction: A Comprehensive Study on Risk Factors Analysis and Hyperparameter Tuning Techniques",
        "authors": "Imam Tahyudin, Ades Tikaningsih, Puji Lestari, Eko Winarto, Nazwan Hassa",
        "published": "2024-2-27",
        "citations": 0,
        "abstract": "Stroke is one of the major killer diseases in the world. Understanding the factors that influence the death of stroke patients is vital to improving patient care and outcomes. In this study, we used stroke patient data and machine learning techniques using a variety of algorithms, including Extreme Gradient Boosting, CatBoost, Extra Tree, Decision Tree, and Random Forest, to predict patient death after stroke. After performing hyperparameter settings, the XGBoost model achieved an accuracy of 86% with an AUC of 87. Significant improvements in the accuracy and predictive capability of this model after hyperparameter settings indicate a strong potential for clinical applications. In addition, our findings suggest that factors such as the patient's age, type of stroke, and blood pressure at the time of hospitalization have a significant impact on stroke patients' deaths. By understanding these factors, healthcare providers can improve patient intervention and management to reduce the risk of death after stroke. This research has made an important contribution to the development of a system for predicting the risk of death of stroke patients, which can help doctors and nurses identify high-risk patients and provide appropriate treatment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18421/tem131-74"
    },
    {
        "id": 16425,
        "title": "Classification of buildings' potential for seismic damage using a machine learning model with auto hyperparameter tuning",
        "authors": "Konstantinos Kostinakis, Konstantinos Morfidis, Konstantinos Demertzis, Lazaros Iliadis",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engstruct.2023.116359"
    },
    {
        "id": 16426,
        "title": "Hyperparameter tuning of supervised bagging ensemble machine learning model using Bayesian optimization for estimating stormwater quality",
        "authors": "Mohammadreza Moeini",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40899-024-01064-9"
    },
    {
        "id": 16427,
        "title": "Hyperparameter Tuning in Deep Learning-Based Image Classification to Improve Accuracy using Adam Optimization",
        "authors": "Sekar Janarthanan, Kumar T Ganesh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23940/ijpe.23.09.p3.579586"
    },
    {
        "id": 16428,
        "title": "Improving Pretrained Language Model Fine-Tuning With Noise Stability Regularization",
        "authors": "Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, Jiebo Luo",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3330926"
    },
    {
        "id": 16429,
        "title": "Intelligent Dwelling Price Appraisal A Comprehensive Hierarchical Framework Integrating Data Pre-processing, Exploratory Analysis, Ensemble Learning, and Hyperparameter Tuning for Optimized Prediction Performance Using Machine Learning",
        "authors": "Kamran Abbasi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4594542"
    },
    {
        "id": 16430,
        "title": "Impact of Hyperparameter tuning on Solar PV Performance using Machine Learning for Moscow, Russia",
        "authors": "Kingsley Okoli, Onah Uche, Haastrup Adebayo Ibukun, Victor Achirgbenda, Innocent Joseph",
        "published": "2023-5-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/scm58628.2023.10159093"
    },
    {
        "id": 16431,
        "title": "Transfer Learning Strategies for Fine-Tuning Pretrained Convolutional Neural Networks in Medical Imaging",
        "authors": "Muhamad Angriawan",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "In the area of medical imaging, transfer learning has become a potent technique that uses pretrained Convolutional Neural Networks (CNNs) to improve the performance of particular tasks. An overview of several transfer learning techniques used for optimising pretrained CNNs in the context of medical image analysis is given in this abstract. The size limitations of medical imaging datasets make it difficult to train deep learning models from scratch. Pre-trained CNNs are a good place to start, such as those that have been trained on huge natural picture datasets like ImageNet. When these pre-trained models are applied to medical imaging applications, fine-tuning is frequently used. One common method is feature extraction, where the bottom layers of the pretrained CNN are frozen and operate as feature extractors. Then, for the specific medical task at hand, these features are loaded into a bespoke classifier. The ability of the pretrained network to recognise subtle picture patterns is advantageous in this method. Another strategy is to optimise the CNN architecture as a whole, which enables the model to adjust to the features of medical images. Small learning rates are frequently used in transfer learning techniques to avoid overfitting during fine-tuning. Additionally, to further enhance model generalisation, domain-specific data augmentation is essential. The use of ensemble approaches, which combine several pretrained CNNs, is also investigated. These models are capable of offering various feature representations and improving classification precision. In order to bridge the domain gap between natural photos and medical images, domain adaption techniques are also used. One approach to align feature distributions is by adversarial training, while another is through domain-specific batch normalisation. The feature extraction, network fine-tuning, ensemble approaches, and domain adaptation are all part of transfer learning methodologies for optimising pretrained CNNs in medical imaging. Researchers have made great progress using these techniques in a number of medical image processing tasks, proving the value of transfer learning in this important area.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52710/rjcse.79"
    },
    {
        "id": 16432,
        "title": "ATLAS - A Co-evolutionary Framework for Automatic Tuning of Adversarial Neural Networks",
        "authors": "Saurav Shyju, Ritwik Murali",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583133.3596376"
    },
    {
        "id": 16433,
        "title": "Self-tuning and approximation via RBF neural networks",
        "authors": "Guowei Lei, Lin Xie, Wenqing Ni, Shuicao Zheng",
        "published": "2023-1-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3580219.3580233"
    },
    {
        "id": 16434,
        "title": "ODL-BCI: Optimal deep learning model for brain-computer interface to classify students confusion via hyperparameter tuning",
        "authors": "Md Ochiuddin Miah, Umme Habiba, Md Faisal Kabir",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dscb.2024.100121"
    },
    {
        "id": 16435,
        "title": "Hyperparameter Tuning Based Machine Learning Classifier for Breast Cancer Prediction",
        "authors": "Md. Mijanur Rahman, Asikur Rahman, Swarnali Akter, Sumiea Akter Pinky",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4236/jcc.2023.114007"
    },
    {
        "id": 16436,
        "title": "Exploring Hyperparameter Usage and Tuning in Machine Learning Research",
        "authors": "Sebastian Simon, Nikolay Kolyada, Christopher Akiki, Martin Potthast, Benno Stein, Norbert Siegmund",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cain58948.2023.00016"
    },
    {
        "id": 16437,
        "title": "Effect of Hyperparameter Tuning on Transfer Learning Models for Brain Tumor Detection and Classification",
        "authors": "Andre Isaac Nazareth, Grace Prashant Pereira, Trisha Nitin Nagarkatte, Jagruti Save",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icacta58201.2023.10393629"
    },
    {
        "id": 16438,
        "title": "Using sequential statistical tests for efficient hyperparameter tuning",
        "authors": "Philip Buczak, Andreas Groll, Markus Pauly, Jakob Rehof, Daniel Horn",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "AbstractHyperparameter tuning is one of the most time-consuming parts in machine learning. Despite the existence of modern optimization algorithms that minimize the number of evaluations needed, evaluations of a single setting may still be expensive. Usually a resampling technique is used, where the machine learning method has to be fitted a fixed number of k times on different training datasets. The respective mean performance of the k fits is then used as performance estimator. Many hyperparameter settings could be discarded after less than k resampling iterations if they are clearly inferior to high-performing settings. However, resampling is often performed until the very end, wasting a lot of computational effort. To this end, we propose the sequential random search (SQRS) which extends the regular random search algorithm by a sequential testing procedure aimed at detecting and eliminating inferior parameter configurations early. We compared our SQRS with regular random search using multiple publicly available regression and classification datasets. Our simulation study showed that the SQRS is able to find similarly well-performing parameter settings while requiring noticeably fewer evaluations. Our results underscore the potential for integrating sequential tests into hyperparameter tuning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10182-024-00495-1"
    },
    {
        "id": 16439,
        "title": "Improving Multi-fidelity Optimization with a Recurring Learning Rate for Hyperparameter Tuning",
        "authors": "HyunJae Lee, Gihyeon Lee, Junhwan Kim, Sungjun Cho, Dohyun Kim, Donggeun Yoo",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00234"
    },
    {
        "id": 16440,
        "title": "Rainfall Classification Model for the Philippines using Optimized K-nearest Neighbor Algorithm with GridSearchCV Hyperparameter Tuning",
        "authors": "Deiscart D’Mitrio C. Maceda, Jennifer C.Dela Cruz",
        "published": "2023-8-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccsce58721.2023.10237156"
    },
    {
        "id": 16441,
        "title": "Lung Cancer Detection Refined: A Study on SVM Hyperparameter Tuning Using BayesOptimization",
        "authors": "Et al. Ashok Kumar Gottipalla",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "This research focuses on the application and enhancement of machine learning algorithms for the detection and differentiation of various types of cancers, with a primary emphasis on lung cancer. Central to this study is the integration of the Bayes Optimization algorithm for hyperparameter optimization and the XGBoost algorithm for predictive modelling. A significant aspect of this work involves the strategic reduction of hyper-features, aimed at refining the XGBoost model's performance. This process not only ensures a more efficient model but also contributes to a higher accuracy in cancer-type prediction. Additionally, a comparative analysis is conducted with other ensemble models to evaluate the relative performance improvements. The findings of this study are pivotal, as they demonstrate the optimized model's enhanced capability in accurately detecting different cancer types, particularly lung cancer, and show marked advancements over other contemporary models. The research highlights the potential of combining advanced machine learning techniques for significant improvements in oncological diagnostics and treatment planning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i9.9797"
    },
    {
        "id": 16442,
        "title": "Diabetes Prediction Using Machine Learning Algorithms and Hyperparameter Tuning for Expecting Mothers",
        "authors": "D. Manoj Kumar, Deny J, Nagaraj P, Kudithipudi Dhanveer, Vaddepally Punith Kumar, R Arthi",
        "published": "2023-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aic57670.2023.10263941"
    },
    {
        "id": 16443,
        "title": "Automatic tuning of PID controllers using deep recurrent neural networks with pruning based on tracking error",
        "authors": "Aghil Ahmadi, Reza Mahboobi Esfanjani",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "Abstract\nAlthough PID controllers are common in industry, they are\n  often poorly tuned; especially, in uncertain environments. Modern\n  industries, with increasing complexity, motivate us to employ new\n  intelligent methods in order to extend PID controllers beyond their\n  usual capabilities. In this paper, an advanced machine learning\n  scheme is utilized to improve PID controllers; for the first time, a\n  deep dynamic neural network is employed to tune online the\n  parameters of the traditional PID controller in order to overcome\n  the effects of uncertainties in the closed-loop control system. To\n  reduce the computational burden of deep recurrent neural network, a\n  novel structural learning technique is applied to optimize the\n  configuration. Unlike existing pruning methods, the network is\n  pruned based on the values of neurons and the total value of the\n  corresponding layer. Simulation of a benchmark CSTR system\n  demonstrate that the proposed scheme performs more efficiently\n  compared to a shallow network tuner, in the presence of\n  uncertainties.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1748-0221/19/02/p02028"
    },
    {
        "id": 16444,
        "title": "Tuning Criticality through Modularity in Biological Neural Networks",
        "authors": "Martín Irani, Thomas H. Alderson",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1523/jneurosci.0865-23.2023"
    },
    {
        "id": 16445,
        "title": "Automated Hyperparameter Optimization of Convolutional Neural Network (CNN) for First-Break (FB) Arrival Picking",
        "authors": "Mohammed Ayub, SanLinn Ismail Kaka",
        "published": "2023-3-13",
        "citations": 0,
        "abstract": "AbstractThe Convolutional Neural Network (CNN) has been used successfully to enhance the First-break (FB) automated arrival picking of seismic data. Determining an optimized FB model is challenging as it needs to consider several hyperparameters (HPs) combinations. Tuning the most important HPs manually is infeasible because of a higher number of HP combinations to be tested. Three state-of-the-art automated hyperparameter optimization (HPO) techniques are applied to a CNN model for robust FB arrival picking classification. A CNN model with 4 convolutional (Conv) layers followed by one fully connected (FC) and one output layer is designed to classify the seismic event as FB or non-FB. To control overfitting, dropout (DO), batch normalization are used after every two Conv layers, in addition to only the DO layer after FC. The number and size of kernels, DO rate, Learning rate (Lr), and several neurons in the FC layer are fine-tuned using random search, Bayesian, and Hyper Band HPO techniques. The findings are experimentally evaluated and compared in terms of four performance metrics with respect to classification performance.The five hyperparameters mentioned above are fine-tuned in 13 search spaces for each of the three HPO techniques. From experimental results, applying random search HPO to CNN yields the best accuracy and F1-score of 96.26%, with the best HP combination of 16, 16, 32, and 64 for numbers of kernels in four Conv layers respectively; 2, 2, 2, 5 for the size of kernels in each Conv layer; 0, 0.45, 0.25 for DO rate in each of DO layers; 240 for numbers of neurons in FC layer; and 0.000675 for Lr. In terms of loss on test data, the above combination of HP gives the lowest test loss of 0.1191 among all techniques, making it a robust model. This model outperforms all the other models in terms of precision (96.27%) and recall. Moreover, all HPO models outperformed the baseline in terms of all metrics. The use of DO after Conv layers and FC layers is highly recommended. Moreover, the use of kernel size relatively smaller (i.e. 2) produces the best classification performance. According to the best HP combination results, there is also no harm to use a relatively higher number of neurons in the FC layer than the Conv layer in FB arrival picking classification. The optimal values of Lr range from 0.0001 to 0.000675 depending on the HPO techniques. The model developed in this study improves the accuracy of the auto-picking of FB seismic data and it is anticipated our model to be used more widely in future studies in the processing of seismic data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/214253-ms"
    },
    {
        "id": 16446,
        "title": "Hyperparameter Tuned Hybrid Convolutional Neural Network (H-CNN) for Accurate Plant Disease Classification",
        "authors": "Shubham Sharma, Manu Vardhan",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic3s57698.2023.10169257"
    },
    {
        "id": 16447,
        "title": "A novel hybrid PSO- and GS-based hyperparameter optimization algorithm for support vector regression",
        "authors": "Mustafa Açıkkar, Yunus Altunkol",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08805-5"
    },
    {
        "id": 16448,
        "title": "Hyperparameter Learning Under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization",
        "authors": "Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, Emil C. Lupu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3291648"
    },
    {
        "id": 16449,
        "title": "Indoor Pollutant Classification Modeling using Relevant Sensors under Thermodynamic Conditions with Multilayer Perceptron Hyperparameter Tuning",
        "authors": "Percival J. Forcadilla",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.01402103"
    },
    {
        "id": 16450,
        "title": "Hyperparameter Tuning of Semi-Supervised Learning for Indonesian Text Annotation",
        "authors": "Siti Khomsah, Nur Heri Cahyana, Agus Sasmito Aribowo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140927"
    },
    {
        "id": 16451,
        "title": "Hyperparameter Tuning On Machine Learning Transformers For Mood Classification In Indonesian Music",
        "authors": "Neny Rosmawarni, Thoyyibah. T, Imam Ahmad, Eka Ardhianto, Dede Handayani, Willis Puspita Sari",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icimcis60089.2023.10349008"
    },
    {
        "id": 16452,
        "title": "Neural Networks Referees in 2023",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(23)00713-x"
    },
    {
        "id": 16453,
        "title": "Enhancing deep learning model performance in air quality classification through probabilistic hyperparameter tuning with tree-structured parzen estimators",
        "authors": " M. Rajalakshmi, V. Sulochana",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "The research introduces an innovative approach to enhance deep learning models for air quality classification by integrating tree-structured Parzen estimators (TPE) into the hyperparameter tuning process. It applies this approach to CNN, LSTM, DNN, and DBN models and conducts extensive experiments using an air quality dataset, comparing it with grid search, random search, and genetic algorithm methods. The TPE algorithm consistently outperforms these methods, demonstrating improved classification accuracy and generalization. This approach’s potential extends to enriching water quality classification models, contributing to environmental sustainability and resource management. Bridging deep learning with TPE offers a promising solution for optimized air quality classification, supporting informed environmental preservation efforts.",
        "keywords": "",
        "link": "http://dx.doi.org/10.58414/scientifictemper.2023.14.4.27"
    },
    {
        "id": 16454,
        "title": "ACO-Based Hyperparameter Tuning of a DL Model for Lung Cancer Prediction",
        "authors": "Kumar Dilip, G S Pradeep Ghantasala, D. Nageswara Rao, Manisha Rathee, Priyanka Bathla",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic2pct60090.2024.10486744"
    },
    {
        "id": 16455,
        "title": "Metafeature Selection via Multivariate Sparse-Group Lasso Learning for Automatic Hyperparameter Configuration Recommendation",
        "authors": "Liping Deng, Wen-Sheng Chen, Mingqing Xiao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3263506"
    },
    {
        "id": 16456,
        "title": "Neural Networks Referees in 2022",
        "authors": "",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(22)00489-0"
    },
    {
        "id": 16457,
        "title": "Estimating kinetic energy reduction for terminal ballistics using a hyperparameter-optimized neural network",
        "authors": "Brianna Thompson, Jesse Sherburn, James Ross, Yi Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09382-3"
    },
    {
        "id": 16458,
        "title": "I Choose You: Automated Hyperparameter Tuning for Deep Learning-based Side-channel Analysis",
        "authors": "Lichao Wu, Guilherme Perin, Stjepan Picek",
        "published": "2024",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tetc.2022.3218372"
    },
    {
        "id": 16459,
        "title": "Multimode interference reflectors and output tuning using neural networks",
        "authors": "Paulo Jorge Passos Sério Lourenço, Mário Véstias, Alessandro Fantoni, Manuela Vieira",
        "published": "2024-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3001779"
    },
    {
        "id": 16460,
        "title": "Sparse Mutation Decompositions: Fine Tuning Deep Neural Networks with Subspace Evolution",
        "authors": "Tim Whitaker, Darrell Whitley",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583133.3590705"
    },
    {
        "id": 16461,
        "title": "Hybrid Spiking Neural Networks Fine-Tuning for Hippocampus Segmentation",
        "authors": "Ye Yue, Marc Baltes, Nidal Abujahar, Tao Sun, Charles D. Smith, Trevor Bihl, Jundong Liu",
        "published": "2023-4-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230610"
    },
    {
        "id": 16462,
        "title": "Improving permeability prediction in carbonate reservoirs through gradient boosting hyperparameter tuning",
        "authors": "Mohammed A. Abbas, Watheq J. Al-Mudhafar, David A. Wood",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12145-023-01099-0"
    },
    {
        "id": 16463,
        "title": "A Study on the Implementation of YOLOv4 Algorithm with Hyperparameter Tuning for Car Detection in Unmanned Aerial Vehicle Images",
        "authors": "Muhammad Alfian Ramadhani, Yufis Azhar, Galih Wasis Wicaksono",
        "published": "2023-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoict58202.2023.10262651"
    },
    {
        "id": 16464,
        "title": "Detection of Distributed Denial-of-Service (DDoS) Attack with Hyperparameter Tuning Based on Machine Learning Approach",
        "authors": "Wan Nurulsafawati Wan Manan, Choo Yong Han",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isas60782.2023.10391487"
    },
    {
        "id": 16465,
        "title": "Co-evolving Recurrent Neural Networks and their Hyperparameters with Simplex Hyperparameter Optimization",
        "authors": "Amit Dilip Kini, Swaraj Sambhaji Yadav, Aditya Shankar Thakur, Akshar Bajrang Awari, Zimeng Lyu, Travis Desell",
        "published": "2023-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583133.3596407"
    },
    {
        "id": 16466,
        "title": "Assessing the predictive capability of DeepBoost machine learning algorithm powered by hyperparameter tuning methods for slope stability prediction",
        "authors": "Selçuk Demir, Emrehan Kutlug Sahin",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12665-023-11247-w"
    },
    {
        "id": 16467,
        "title": "Effects of Automatic Hyperparameter Tuning on the Performance of Multi‐Variate Deep Learning‐Based Rainfall Nowcasting",
        "authors": "Amirmasoud Amini, Mehri Dolatshahi, Reza Kerachian",
        "published": "2023-1",
        "citations": 7,
        "abstract": "AbstractRainfall nowcasting has become increasingly important as we move into an era where more and more storms are occurring in many countries as a result of climate change. Developing an accurate rainfall nowcasting model could provide insights into rainfall dynamics and ultimately could prevent significant damages. In this paper, deep neural networks (DNNs) and numerical weather predictions (NWPs) are applied for rainfall and runoff forecasting in an urban catchment with a complex drainage system. DNNs are among the most accurate models for rainfall nowcasting. However, the design and training of DNNs are usually complicated. This paper combines different convolutional, long short‐term memory (LSTM)‐based networks and NWPs using ensemble techniques (i.e., bagging, random forest, and adaboost methods) with automatic hyperparameter tuning for multi‐step rainfall nowcasting. The relative humidity, air temperature, and previous rainfall sequences are considered the inputs of the DNNs. We focus on applying two hyperparameter tuning methods (i.e., random search and tree structured Parzen estimator) to improve the performance of the proposed rainfall nowcasting models. The proposed framework was applied to the eastern drainage catchment (EDC) in Tehran city. The results illustrate that the utilization of automatic hyperparameter tuning along with multivariate DNNs, NWPs, and ensemble techniques could improve the nowcasting performance (10%–25%) compared to the traditional univariate models. Also, Adaboost is more accurate than other ensemble techniques in predicting both extreme and normal rainfall events with average RMSE of 0.765, and random forest obtain better results when predict sub normal rainfall events with overall RMSE of 0.315. The proposed framework is applicable to different climates and catchments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1029/2022wr032789"
    },
    {
        "id": 16468,
        "title": "An improved approach to Arabic news classification based on hyperparameter tuning of machine learning algorithms",
        "authors": "Imad Jamaleddyn, Rachid El ayachi, Mohamed Biniz",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jer.2023.100061"
    },
    {
        "id": 16469,
        "title": "Announcement of the Neural Networks Best Paper Award",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(23)00714-1"
    },
    {
        "id": 16470,
        "title": "Impact of Deep Learning Optimizers and Hyperparameter Tuning on the Performance of Bearing Fault Diagnosis",
        "authors": "Seongjae Lee, Taehyoun Kim",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3281910"
    },
    {
        "id": 16471,
        "title": "Optimization of Boosting Classifier Method for Paddy Growth Phase Classification Using Oversampling and Hyperparameter Tuning",
        "authors": "Nurul Izza Afkharinah, Elisabeth Gunawan, Nadira Fawziyya Masnur, Agustan Agustan, Swasetyo Yulianto, Kusprasapta Mutijarsa",
        "published": "2023-10-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceei59426.2023.10346617"
    },
    {
        "id": 16472,
        "title": "Malayalam Handwritten Character Recognition using Transfer Learning and Fine Tuning of Deep Convolutional Neural Networks",
        "authors": "Pearlsy P V, Deepa Sankar",
        "published": "2023-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access57397.2023.10200336"
    },
    {
        "id": 16473,
        "title": "Self-adaptive Prompt-tuning for Event Extraction in Ancient Chinese Literature",
        "authors": "Jingyi Zhang, Yuting Wei, Yangfu Zhu, Bin Wu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191495"
    },
    {
        "id": 16474,
        "title": "Refining Malware Detection with Enhanced Machine Learning Algorithms using Hyperparameter Tuning",
        "authors": "Karim Ouazzane, Soufyane Mounir, Yassine Maleh, Mohamed E.L. Bakkali, El Mouhtadi Walid",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijccbs.2024.10062989"
    },
    {
        "id": 16475,
        "title": "Development of Optimal Hyperparameter Tuning-Cycle GAN for Photo-realistic Face Age Progression Model",
        "authors": "Tejaswini Yadav, Rajneeshkaur Sachdeo",
        "published": "2023-11",
        "citations": 1,
        "abstract": " Face age progression aims to change an individual’s face from a provided face image to forecast how that image will look in the future. Face aging is gaining much attention in today’s environment, which needs better security and a touchless unique identification mechanism. Researchers are focused on creating face processing algorithms to address the difficulty of producing realistic aged faces for smart system applications over the earlier decades. In the literature, the two basic needs of face age progression, aging accuracy, and identity preservation are not thoroughly addressed. According to the extraordinary gains in image synthesis made by deep generative methods and their significant influence on a wide variety of practical applications such as identifying missing persons using entertainment, childhood images, and so on, face age progression/regression has reawakened attention. The majority of present techniques concentrate on face age progression and is beneficial and productive in learning the transition across age groups utilizing paired data, i.e., face images of the similar individual at various ages. Through the motivation of the important success attained by Generative Adversarial Networks (GANs), this paper uses tactics to implement the improved Cycle GAN-based intelligent face age progression model. Initially, the standard datasets for the face progression are gathered, and the face is detected using the Viola-Jones object detection algorithm. Then, the pre-processing of the facial image is performed by median filtering and contrast enhancement techniques. Once the image is pre-processed, the Hyperparameter Tuning-Cycle GAN (HT-Cycle GAN) is adopted for face age progression. As an improvement, the hyperparameters of the Cycle GAN are optimized or tuned by the modified Galactic Swarm Optimization (GSO), known as Best Fitness-based Galactic Swarm Optimization (BF-GSO). From the evaluation of statistical analysis, the similarity score of BF-GSO-HT-CycleGAN is 0.80%, 3.33%, and 2.86% higher than cGAN, CycleGAN, and Dubbed FaceGAN, respectively. Here, the Dubbed FaceGAN is the 2nd greatest network. Furthermore, compared to traditional models utilizing distinct standard datasets, the experimental findings show that the suggested technique attains efficiency, accuracy, and flexibility. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0218213023500689"
    },
    {
        "id": 16476,
        "title": "Learning smooth dendrite morphological neurons for pattern classification using linkage trees and evolutionary-based hyperparameter tuning",
        "authors": "Samuel Omar Tovias-Alanis, Humberto Sossa, Wilfrido Gómez-Flores",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.05.024"
    },
    {
        "id": 16477,
        "title": "An empirical analysis of hyperparameter tuning impact on ensemble machine learning algorithm for earthquake damage prediction",
        "authors": "Shejuti Binte Feroz, Nusrat Sharmin, Muhammad Samee Sevas",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42107-024-00994-1"
    },
    {
        "id": 16478,
        "title": "Nonsmooth Optimization-Based Hyperparameter-Free Neural Networks for Large-Scale Regression",
        "authors": "Napsu Karmitsa, Sona Taheri, Kaisa Joki, Pauliina Paasivirta, Adil M. Bagirov, Marko M. Mäkelä",
        "published": "2023-9-14",
        "citations": 1,
        "abstract": "In this paper, a new nonsmooth optimization-based algorithm for solving large-scale regression problems is introduced. The regression problem is modeled as fully-connected feedforward neural networks with one hidden layer, piecewise linear activation, and the L1-loss functions. A modified version of the limited memory bundle method is applied to minimize this nonsmooth objective. In addition, a novel constructive approach for automated determination of the proper number of hidden nodes is developed. Finally, large real-world data sets are used to evaluate the proposed algorithm and to compare it with some state-of-the-art neural network algorithms for regression. The results demonstrate the superiority of the proposed algorithm as a predictive tool in most data sets used in numerical experiments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16090444"
    },
    {
        "id": 16479,
        "title": "Application of Data Mining Techniques and Hyperparameter Tuning for Accurate Water Potability Classification",
        "authors": "Jefferson Johan, Jesse Orlanda, William Suryadharma Pangestu, Winita Teukeku Priyanto,  Meiliana, Said Achmad",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390049"
    },
    {
        "id": 16480,
        "title": "A Statistical Approach to Hyperparameter Tuning of Deep Learning for Construction Machine Classification",
        "authors": "André Luiz C. Ottoni, Marcela S. Novo, Marcos S. Oliveira",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13369-023-08330-6"
    },
    {
        "id": 16481,
        "title": "Multiobjective Hyperparameter Optimization of Artificial Neural Networks for Optimal Feedforward Torque Control of Synchronous Machines",
        "authors": "Niklas Monzen, Florian Stroebl, Herbert Palm, Christoph M. Hackl",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ojies.2024.3356721"
    },
    {
        "id": 16482,
        "title": "Automated hyperparameter tuning for crack image classification with deep learning",
        "authors": "André Luiz Carvalho Ottoni, Artur Moura Souza, Marcela Silva Novo",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-09103-x"
    },
    {
        "id": 16483,
        "title": "Medical Diagnosis of Human Heart Diseases with and without Hyperparameter tuning through Machine Learning",
        "authors": "K. K. Baseer, S. B. Asqeem Nas, S. Dharani, S. Sravani, P. Yashwanth, P. Jyothirmai",
        "published": "2023-2-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccmc56507.2023.10084156"
    },
    {
        "id": 16484,
        "title": "GamMa: Efficient Fine-Tuning of Pre-Trained Language Models Using Gradient Activation Mapping Masking",
        "authors": "Anchun Gui, Jinqiang Ye, Han Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191351"
    },
    {
        "id": 16485,
        "title": "Deep Learning based Bone Fracture Prediction using Convolutional Neural Networks: A Comparative Study of Transfer Learning and Fine-tuning Techniques",
        "authors": "Sriram R, O R Aruna",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icotl59758.2023.10435047"
    },
    {
        "id": 16486,
        "title": "Trends and Advances on The K-Hyperparameter Tuning Techniques in High-Dimensional Space Clustering",
        "authors": "Rufus Kinyua Gikera, Jonathan Mwaura, Elizaphan Maina, Shadrack Mambo",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Clustering is one of the tasks performed during exploratory data analysis with an extensive and wealthy history in a variety of disciplines. Application of clustering in computational medicine is one such application of clustering that has proliferated in the recent past. K-means algorithms are the most popular because of their ability to adapt to new examples besides scaling up to large datasets. They are also easy to understand and implement. However, with k-means algorithms, k-hyperparameter tuning is a long standing challenge. The sparse and redundant nature of the high-dimensional datasets makes the k-hyperparameter tuning in high-dimensional space clustering a more challenging task. A proper k-hyperparameter tuning has a significant effect on the clustering results. A number of state-of-the art k-hyperparameter tuning techniques in high-dimensional space have been proposed.  However, these techniques perform differently in a variety of high-dimensional datasets and data-dimensionality reduction methods. This article uses a five-step methodology  to investigate the trends and advances on the state of the art k-hyperparameter tuning techniques in high-dimensional space clustering, data dimensionality reduction methods used with these techniques, their tuning strategies, nature of the datasets applied with them as well as the challenges associated with the cluster analysis in high-dimensional spaces. The metrics used in evaluating these techniques are also reviewed. The results of this review, elaborated in the discussion section, makes it efficient for data science researchers to undertake an empirical study among these techniques; a study that subsequently forms the basis for creating improved solutions to this k-hyperparameter tuning problem.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24014/ijaidm.v6i2.22718"
    },
    {
        "id": 16487,
        "title": "An Efficient Approach for Diabetes Classification Using Feature Selection and Hyperparameter Tuning",
        "authors": "Bhanu Prakash Lohani, Arvind Dagur., Dhirendra Shukla",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "\nBackground:\nDiabetes mellitus, stemming from insulin deficiency or resistance, poses\nacute and chronic health issues driven by factors like age, obesity, genetics, and lifestyle. It significantly\nimpacts health, leading to conditions like heart disease, vision problems, and kidney dysfunction,\nwith a notable mortality rate reported by the WHO in 2019. The modern diet has escalated\ndiabetes risk. Machine learning techniques play a pivotal role in disease prediction, aiding\ntimely interventions.\n\n\nObjective:\nThe primary aim of this research work is to explore and contrast the effectiveness of\nvarious existing machine-learning models for diabetes disease classification. The goal is to identify\nthe optimal solution that yields the highest accuracy.\n\n\nMethods:\nIn the initial phase, we implemented data pre-processing, followed by the application of\na diverse range of machine learning methods to classify diabetes mellitus. Subsequently, a comprehensive\nanalysis was conducted on machine learning algorithms, considering both the complete\ndataset features and those selected through Particle Swarm Optimization (PSO). The assessment\ncovered various metrics such as accuracy score, precision, F1 score, and log loss for Support Vector\nClassifier (SVC), K-Nearest Neighbours (KNN), Random Forest (RF), ADA Boost, XG Boost,\nExtra Tree, and Decision Tree. Ultimately, the introduction of hyperparameter tuning was aimed\nat enhancing performance and attaining the highest level of accuracy.\n\n\nResults:\nThe proposed model HSVC combines the Particle Swarm Optimization (PSO) feature\nselection strategy with optimized hyperparameters, showcasing outstanding performance and\nachieving an accuracy of 98.66%.\n\n\nConclusion:\nThe models developed in this study can potentially be applied or recommended for\nthe classification of other health conditions in different domains, such as Parkinson’s disease,\nheart disease, and many more.\n\n\nconclusion:\nThe use of machine learning (ML) techniques is highly regarded for its effectiveness in disease diagnosis. By detecting diseases early, patients can receive prompt medical attention, leading to improved health outcomes. This study examines several ML classification models for predicting diabetes in patients, with a focus on their accuracy. After examining several ML classification models, we have concluded that SVM is providing the best result on the chosen dataset. Then we have applied the concept of hyper tuning parameter with SVM and the improved accuracy was 98.66% for diabetes classification. The models developed in this study can potentially\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2174/0123520965291885240315051751"
    },
    {
        "id": 16488,
        "title": "Automated Hyperparameter Tuning in Reinforcement Learning for Quadrupedal Robot Locomotion",
        "authors": "MyeongSeop Kim, Jung-Su Kim, Jae-Han Park",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "In reinforcement learning, the reward function has a significant impact on the performance of the agent. However, determining the appropriate value of this reward function requires many attempts and trials. Although many automated reinforcement learning methods have been proposed to find an appropriate reward function, their proof is lacking in complex environments such as quadrupedal locomotion. In this paper, we propose a method to automatically tune the scale of the dominant reward functions in reinforcement learning of a quadrupedal robot. Reinforcement learning of the quadruped robot is very sensitive to the reward function, and recent outstanding research results have put a lot of effort into reward shaping. In this paper, we propose an automated reward shaping method that automatically adjusts the reward function scale appropriately. We select some dominant reward functions, arrange their weights in a certain unit, and then calculate their gait scores so that we can select the agent with the highest score. This gait score was defined to reflect the stable walking of the quadrupedal robot. Additionally, quadrupedal locomotion learning requires reward functions of different scales depending on the robot’s size and shape. Therefore, we evaluate the performance of the proposed method on two different robots.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13010116"
    },
    {
        "id": 16489,
        "title": "Creating Collaborative Data Representations Using Matrix Manifold Optimal Computation and Automated Hyperparameter Tuning",
        "authors": "Keiyu Nosaka, Akiko Yoshise",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceib57887.2023.10170466"
    },
    {
        "id": 16490,
        "title": "Hyperparameter Tuning of Identity Block Uses an Imbalance Dataset with Hyperband Method",
        "authors": "Abdul Rachman Manga, Muhammad Acqmal Fadhilla Latief, Andi Widya Mufila Gaffar, Huzain Azis, Ramdan Satra, Yulita Salim",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imcom60618.2024.10418427"
    },
    {
        "id": 16491,
        "title": "What do the AI methods tell us about predicting price volatility of key natural resources: Evidence from hyperparameter tuning",
        "authors": "Mrinalini Srivastava, Amar Rao, Jaya Singh Parihar, Shubham Chavriya, Surendar Singh",
        "published": "2023-1",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.resourpol.2022.103249"
    },
    {
        "id": 16492,
        "title": "Method for Hyperparameter Tuning of EfficientNetV2-based Image Classification by Deliberately Modifying Optuna Tuned Result",
        "authors": "Jin Shimazoe, Kohei Arai, Mari Oda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141248"
    },
    {
        "id": 16493,
        "title": "Breast Cancer Prediction Framework Based on Iterative Optimization with Bayesian Hyperparameter Tuning",
        "authors": "Ayman Alsabry, Malek Algabri, Amin Mohamed Ahsan, Mogeeb A.A. Mosleh, Aqeel Abdullah Ahmed, Hamzah Ali Qasem",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/esmarta59349.2023.10293277"
    },
    {
        "id": 16494,
        "title": "Optimizing health data analytics in fog computing using hyperparameter tuning and grid search",
        "authors": "Kiran Deep Singh, Prabh Deep Singh, Rohan Verma, Harsh Taneja",
        "published": "2024",
        "citations": 0,
        "abstract": "The integration of fog computing with health data analytics signifies a paradigm shift in the field of healthcare, offering the potential for streamlined and prompt analysis of patient welfare. The increasing volume of health data necessitates the development of efficient analytical models in fog computing settings. The objective of this research is to examine the integration of fog computing and health data analytics, specifically emphasizing the utilization of hyperparameter tuning and grid search techniques to enhance optimization approaches. Hyperparameter tuning and grid search are two techniques utilized in machine learning to optimize the performance of models. These methods are employed in the context of health data analytics inside fog computing with the objective of improving accuracy, reducing latency, and enhancing resource efficiency. Our research endeavors to provide significant contributions to the advancement of adaptable and responsive healthcare systems, therefore promoting enhanced patient outcomes in the era of data-driven decision-making.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47974/jios-1560"
    },
    {
        "id": 16495,
        "title": "Meteorological Factor-Based Tomato Early Blight Prediction Using Hyperparameter Tuning of Intelligent Classifiers",
        "authors": "Ayushi Gupta, Anuradha Chug, Amit Prakash Singh",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40003-023-00691-6"
    },
    {
        "id": 16496,
        "title": "Bidirectionally self-normalizing neural networks",
        "authors": "Yao Lu, Stephen Gould, Thalaiyasingam Ajanthan",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.017"
    },
    {
        "id": 16497,
        "title": "Reg-TuneV2: A Hardware-Aware and Multiobjective Regression-Based Fine-Tuning Approach for Deep Neural Networks on Embedded Platforms",
        "authors": "Arnab Neelim Mazumder, Tinoosh Mohsenin",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mm.2023.3316433"
    },
    {
        "id": 16498,
        "title": "Tuning Method for Parameters in Fractional-Order PID Controllers Based on Neural Networks with Improved Borges Derivative",
        "authors": "Mingdi Li, Zhe Gao, Kai Jia, Shasha Xiao",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166688"
    },
    {
        "id": 16499,
        "title": "Announcement of the Neural Networks Best Paper Award",
        "authors": "Taro Toyoizumi, DeLiang Wang",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(22)00490-7"
    },
    {
        "id": 16500,
        "title": "Distance Searching-based Hyperparameter Optimization for Restricted Coulomb Energy-based Neural Network",
        "authors": "Kyou Jung Son, Seokhun Jeon, Jae-Hack Lee, Byung-Soo Kim",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isocc59558.2023.10396476"
    },
    {
        "id": 16501,
        "title": "VC dimensions of group convolutional neural networks",
        "authors": "Philipp Christian Petersen, Anna Sepliarskaia",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.012"
    },
    {
        "id": 16502,
        "title": "A Crowd-Ai Dynamic Neural Network Hyperparameter Optimization Approach for Image-Driven Social Sensing Applications",
        "authors": "Yang Zhang, Ruohan Zhong, Lanyu Shang, Dong Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4399140"
    },
    {
        "id": 16503,
        "title": "Energy Efficient Hyperparameter Tuned Deep Neural Network to Improve Accuracy of Near-Threshold Processor",
        "authors": "K. Chanthirasekaran, Raghu Gundaala",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.036130"
    },
    {
        "id": 16504,
        "title": "Corrigendum to ‘An exact mapping from ReLU networks to spiking neural networks’ [Neural Networks Volume 168 (2023) Pages 74-88]",
        "authors": "Ana Stanojevic, Stanisław Woźniak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, Wulfram Gerstner",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.057"
    }
]