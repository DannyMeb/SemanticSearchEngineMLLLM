[
    {
        "id": 4401,
        "title": "Gradient descent",
        "authors": "Jay Gajera, David Wang",
        "published": "2018-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-61713"
    },
    {
        "id": 4402,
        "title": "Stochastic gradient descent",
        "authors": "Andrew Murphy, David Wang",
        "published": "2018-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-61715"
    },
    {
        "id": 4403,
        "title": "Stochastic Gradient Descent with Gradient Estimator for Categorical Features",
        "authors": "Paul Peseux, Thierry Paquet, Maxime Berar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4439301"
    },
    {
        "id": 4404,
        "title": "Comparision of Solutions of Numerical Gradient Descent Method and Continous Time Gradient Descent Dynamics and Lyapunov Stability",
        "authors": "Nagihan Yağmur, Barış Baykant Alagöz",
        "published": "2019-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu.2019.8806396"
    },
    {
        "id": 4405,
        "title": "Mini-batch gradient descent",
        "authors": "Andrew Murphy, David Wang",
        "published": "2018-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-61717"
    },
    {
        "id": 4406,
        "title": "Computational Complexity of Gradient Descent Algorithm",
        "authors": "Nishchal J, neel bhandari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Information is mounting exponentially, and the world is moving to hunt knowledge with the help of Big Data. The labelled data is used for automated learning and data analysis which is termed as Machine Learning. Linear Regression is a statistical method for predictive analysis. Gradient Descent is the process which uses cost function on gradients for minimizing the complexity in computing mean square error. This work presents an insight into the different types of Gradient descent algorithms namely, Batch Gradient Descent, Stochastic Gradient Descent and Mini-Batch Gradient Descent, which are implemented on a Linear regression dataset, and hence determine the computational complexity and other factors like learning rate, batch size and number of iterations which affect the efficiency of the algorithm.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14544000"
    },
    {
        "id": 4407,
        "title": "Computational Complexity of Gradient Descent Algorithm",
        "authors": "Nishchal J, neel bhandari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Information is mounting exponentially, and the world is moving to hunt knowledge with the help of Big Data. The labelled data is used for automated learning and data analysis which is termed as Machine Learning. Linear Regression is a statistical method for predictive analysis. Gradient Descent is the process which uses cost function on gradients for minimizing the complexity in computing mean square error. This work presents an insight into the different types of Gradient descent algorithms namely, Batch Gradient Descent, Stochastic Gradient Descent and Mini-Batch Gradient Descent, which are implemented on a Linear regression dataset, and hence determine the computational complexity and other factors like learning rate, batch size and number of iterations which affect the efficiency of the algorithm.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14544000.v1"
    },
    {
        "id": 4408,
        "title": "A Gradient Descent based Heuristic for Solving Regression Clustering Problems",
        "authors": "Enis Kayış",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009836701020108"
    },
    {
        "id": 4409,
        "title": "Gradient Descent",
        "authors": "",
        "published": "2021-10-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108699211.008"
    },
    {
        "id": 4410,
        "title": "A comparative study of gradient descent and stochastic gradient descent method for optimization",
        "authors": "Sapna Shrimali, Govind S. Sharma, Sunil K. Srivastava",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0148736"
    },
    {
        "id": 4411,
        "title": "Gradient Descent: Practice",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781526469106"
    },
    {
        "id": 4412,
        "title": "Comparative Analysis of the Rate of Convergence of the Methods of Gradient Descent and Natural Gradient Descent in Regression Analysis Problems",
        "authors": "Alexei Tyurin",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349623"
    },
    {
        "id": 4413,
        "title": "Accelerated Gradient Descent",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108699211.010"
    },
    {
        "id": 4414,
        "title": "A Gradient-Descent-Based k-NN Algorithm",
        "authors": "Wenqi Guo",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we reviewed a few works that solved the speed issue of the k-NN classification algorithm. This makes it possible to solve real problems using the classificational k-NN algorithm. We then use gradient descent to achieve the equivalent goal of feature scaling, increasing the accuracy of the k-NN classification algorithm, including on nominal data. I was just a part-time college student when I wrote this paper, things might be wrong. Don't assume its credibility is high.",
        "link": "http://dx.doi.org/10.31219/osf.io/kqdxu"
    },
    {
        "id": 4415,
        "title": "Gradient Descent: Theory",
        "authors": "",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781526469069"
    },
    {
        "id": 4416,
        "title": "Gradient Descent – Where Magic Happens",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-4"
    },
    {
        "id": 4417,
        "title": "Real Time Estimation of Population Receptive Fields Using Gradient Descent",
        "authors": "Mario Senden",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractA real-time population receptive field mapping procedure based on gradient descent is proposed. Model-free receptive fields produced by the algorithm are evaluated in context of simulated data exhibiting different levels of temporally autocorrelated noise and spatial point spread. As with any model-free approach, the exact shape of receptive fields produced by the real-time algorithm depends on the stimulus. Nevertheless, estimated receptive fields show good correspondence with ground-truth receptive fields in terms of both position and size. Furthermore, fitting a parametric model to the previously obtained estimates approximates the exact shape of the true underlying receptive fields well.",
        "link": "http://dx.doi.org/10.1101/194621"
    },
    {
        "id": 4418,
        "title": "Stochastic Gradient Descent",
        "authors": "Guanghui Lan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-54621-2_777-1"
    },
    {
        "id": 4419,
        "title": "Gradient-Descent Method",
        "authors": "",
        "published": "2022-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009218146.013"
    },
    {
        "id": 4420,
        "title": "Stochastic Gradient Descent",
        "authors": "Nikhil Ketkar",
        "published": "2017",
        "citations": 126,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-2766-4_8"
    },
    {
        "id": 4421,
        "title": "The Gradient Clusteron: A model neuron that learns via dendritic nonlinearities, structural plasticity, and gradient descent",
        "authors": "Toviah Moldwin, Menachem Kalmenson, Idan Segev",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSynaptic clustering on neuronal dendrites has been hypothesized to play an important role in implementing pattern recognition. Neighboring synapses on a dendritic branch can interact in a synergistic, cooperative manner via the nonlinear voltage-dependence of NMDA receptors. Inspired by the NMDA receptor, the single-branch clusteron learning algorithm (Mel 1991) takes advantage of location-dependent multiplicative nonlinearities to solve classification tasks by randomly shuffling the locations of “under-performing” synapses on a model dendrite during learning (“structural plasticity”), eventually resulting in synapses with correlated activity being placed next to each other on the dendrite. We propose an alternative model, the gradient clusteron, or G-clusteron, which uses an analytically-derived gradient descent rule where synapses are “attracted to” or “repelled from” each other in an input- and location- dependent manner. We demonstrate the classification ability of this algorithm by testing it on the MNIST handwritten digit dataset and show that, when using a softmax activation function, the accuracy of the G-clusteron on the All-vs-All MNIST task (∼85%) approaches that of logistic regression (∼93%). In addition to the location update rule, we also derive a learning rule for the synaptic weights of the G-clusteron (“functional plasticity”) and show that a G-clusteron that utilizes the weight update rule can achieve ∼89% accuracy on the MNIST task. We also show that a G-clusteron with both the weight and location update rules can learn to solve the XOR problem from arbitrary initial conditions.",
        "link": "http://dx.doi.org/10.1101/2020.12.15.417790"
    },
    {
        "id": 4422,
        "title": "Accelerated Gradient Descent Using Instance Eliminating Back Propagation",
        "authors": "Farzad Hosseinali",
        "published": "No Date",
        "citations": 0,
        "abstract": "Artificial Intelligence is dominated by Artificial Neural Networks (ANNs). Currently, the Batch Gradient Descent (BGD) is the only solution to train ANN weights when dealing with large datasets. In this article, a modification to the BGD is proposed which significantly reduces the training time and improves the convergence. The modification, called Instance Eliminating Back Propagation (IEBP), eliminates correctly-predicted-instances from the Back Propagation. The speedup is due to the elimination of unnecessary matrix multiplication operations from the Back Propagation. The proposed modification does not add any training hyperparameter to the existing ones and reduces the memory consumption during the training.",
        "link": "http://dx.doi.org/10.20944/preprints202008.0181.v1"
    },
    {
        "id": 4423,
        "title": "Resampling Stochastic Gradient Descent Cheaply",
        "authors": "Henry Lam, Zitong Wang",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408023"
    },
    {
        "id": 4424,
        "title": "Improving Image Recognition Accuracy in Neural Networks Using Fractional Natural Gradient Descent",
        "authors": "Ruslan Abdulkadirov",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a modified natural gradient descent, containing fractional derivatives of Riemann-Liouville, Caputo and Grunwald-Letnikov types. Such approach belongs to information-geometric optimization methods, which take into account not only directions of gradients or momentum parameters, but the convexity (curvature) of minimizing function. This technique, comparing with second order optimization algorithms, lets to increase the rate of convergence. With fractional order derivatives it is possible to adjust the descent toward the neighborhood of the global minimum. In experiments, we demonstrated the increasing accuracy of image recognition on MNIST and CIFAR10, using the proposed optimization algorithm.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22006742"
    },
    {
        "id": 4425,
        "title": "Improving Image Recognition Accuracy in Neural Networks Using Fractional Natural Gradient Descent",
        "authors": "Ruslan Abdulkadirov",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a modified natural gradient descent, containing fractional derivatives of Riemann-Liouville, Caputo and Grunwald-Letnikov types. Such approach belongs to information-geometric optimization methods, which take into account not only directions of gradients or momentum parameters, but the convexity (curvature) of minimizing function. This technique, comparing with second order optimization algorithms, lets to increase the rate of convergence. With fractional order derivatives it is possible to adjust the descent toward the neighborhood of the global minimum. In experiments, we demonstrated the increasing accuracy of image recognition on MNIST and CIFAR10, using the proposed optimization algorithm.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22006742.v1"
    },
    {
        "id": 4426,
        "title": "Analytic Quantum Gradient Descent in Quantum Chemistry Simulations",
        "authors": "Harshdeep Singh",
        "published": "2022",
        "citations": 0,
        "abstract": "The AQGD optimization technique requires the presence of some specific gates in the quantum circuit and the hydrogen molecule simulation using AQGD in a VQA reveals the incompatibility of Unitary Coupled-Cluster ansatz with the method. Further, varying the parameters of the optimizer results in a significant reduction of simulation run-time.",
        "link": "http://dx.doi.org/10.1364/quantum.2022.qw2a.4"
    },
    {
        "id": 4427,
        "title": "Controlled Gradient Descent: A Control Theoretical Perspective for Optimization",
        "authors": "Revati Gunjal, Syed  Shadab Nayyer",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4639069"
    },
    {
        "id": 4428,
        "title": "Human learning follows the dynamics of gradient descent",
        "authors": "Daniel N Barry, Bradley C. Love",
        "published": "No Date",
        "citations": 2,
        "abstract": "Artificial neural networks (ANNs) have achieved near human-level performance on many tasks and can account for human behavioural and brain measures in a number of domains. Although a principal strength of ANNs is learning representations from experience, only a handful of contributions have evaluated this process to ask whether ANN learning dynamics provide a good model of human learning. We investigated whether humans learn similarly to an ANN, which adjusts its representations through gradient descent. Gradient descent learning is steep at first and initially ignores covariance between features. ANNs can theoretically display a non-monotonic behaviour in which early in learning, multiple weak predictors determine the ANN’s decision whereas late in learning a single strong predictor can dominate. This initial behaviour was confirmed in a simple ANN and in half of human participants performing a comparable task. Later in gradient descent learning, the ANN changed to placing a greater weight on the stronger predictor, and humans also shifted their preferences in the same way. Hidden Markov modelling of the behaviour of ANNs and humans predicted similar transitions from weak-feature to strong-feature states. Our results suggest a significant proportion of people learn about categories in a manner analogous to ANNs, possibly by updating their mental representations by a process akin to gradient descent. Our findings demonstrate how ANNs can be used to not only explain the products of human learning but also the process.",
        "link": "http://dx.doi.org/10.31234/osf.io/v6bt9"
    },
    {
        "id": 4429,
        "title": "Adaptive stochastic gradient descent for large-scale learning problems",
        "authors": "Zhuang Yang, Li Ma",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nAs an effective strategy to enhance stochastic optimization, determining an appropriate step size sequence when performing these algorithms has been warmly encouraged in solving large-scale learning problems. This paper equips stochastic optimization algorithms with an adaptive step size strategy by using the diagonal Barzilai-Borwein (DBB) step size. Specifically, this work proposes a novel stochastic variance reduced algorithm by incorporating DBB into the variance-reduced algorithm, termed mS2GD-DBB. mS2GD-DBB uses a diagonal matrix to approximate the curvature information and accesses to a better curvature information than the conventional BarzilaiBorwein (BB) technique, where the latter works with the scalar approximation of the curvature information. We prove that the proposed algorithm attains a linear convergence rate for strongly convex cases. Moreover, we show that the complexity of the proposed algorithm matches the best known complexity of stochastic optimization algorithms. Numerical experiments suggest that the proposed algorithm shows much promise.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1066512/v1"
    },
    {
        "id": 4430,
        "title": "Human learning follows the dynamics of gradient descent",
        "authors": "Daniel N Barry, Bradley C. Love",
        "published": "No Date",
        "citations": 1,
        "abstract": "Artificial neural networks (ANNs) have achieved near human-level performance on many tasks and can account for human behavioural and brain measures in a number of domains. Although a principal strength of ANNs is learning representations from experience, only a handful of contributions have evaluated this process to ask whether ANN learning dynamics provide a good model of human learning. We investigated whether humans learn similarly to an ANN, which adjusts its representations through gradient descent. Gradient descent learning is steep at first and initially ignores covariance between features. ANNs can theoretically display a non-monotonic behaviour in which early in learning, multiple weak predictors determine the ANN’s decision whereas late in learning a single strong predictor can dominate. This initial behaviour was confirmed in a simple ANN and in half of human participants performing a comparable task. Later in gradient descent learning, the ANN changed to placing a greater weight on the stronger predictor, and humans also shifted their preferences in the same way. Hidden Markov modelling of the behaviour of ANNs and humans predicted similar transitions from weak-feature to strong-feature states. Our results suggest a significant proportion of people learn about categories in a manner analogous to ANNs, possibly by updating their mental representations by a process akin to gradient descent. Our findings demonstrate how ANNs can be used to not only explain the products of human learning but also the process.",
        "link": "http://dx.doi.org/10.31234/osf.io/75e4t"
    },
    {
        "id": 4431,
        "title": "Coordinate Gradient Descent Methods",
        "authors": "Ion Necoara",
        "published": "2017-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155678-15"
    },
    {
        "id": 4432,
        "title": "Development of a Gradient Descent Algorithm for Pathway Routing Based on Functional-Voxel Modeling",
        "authors": "Alexey Tolok, Pavel Petukhov",
        "published": "2021",
        "citations": 1,
        "abstract": "This paper considers a pathfinding algorithm using the gradient method for functional-voxel modeling problems. The basic principles of constructing gradient lines based on the functional voxel model are investigated. The tool to describe the scene with obstacles is the mathematical apparatus of R-functions. To solve the problem of pathfinding in a weakly deterministic environment, we propose an algorithm that is based on the use of: gradient method analyzing the color palette of images of local features of the function; mathematical apparatus of R- functions describing the topography of the solution surface at the current moment in time. An algorithm of target control that solves the problem of getting out of possible \"trapped objects\" is considered. For this, the principle of changing the position of the target was developed to control the gradient direction.",
        "link": "http://dx.doi.org/10.20948/graphicon-2021-3027-689-696"
    },
    {
        "id": 4433,
        "title": "Gradient Descent Analysis: On Visualizing the Training of Deep Neural Networks",
        "authors": "Martin Becker, Jens Lippel, Thomas Zielke",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007583400002108"
    },
    {
        "id": 4434,
        "title": "Gradient descent-type methods",
        "authors": "Quoc Tran-Dinh, Marten van Dijk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-44-319037-7.00008-9"
    },
    {
        "id": 4435,
        "title": "Pointwise Convergence Theorem of Generalized Mini-Batch Gradient Descent in Deep Neural Network",
        "authors": "Tsuyoshi Yoneda",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4445228"
    },
    {
        "id": 4436,
        "title": "Scaling Stratified Stochastic Gradient Descent for Distributed Matrix Completion",
        "authors": "Nabil Abubaker, M. Ozan Karsavuran, Cevdet Aykanat",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Stratified SGD (SSGD) is the primary approach for achieving serializable parallel SGD for matrix completion. State-of-the-art parallelizations of SSGD fail to scale due to huge communication overhead. During an SGD epoch, these methods send data proportional to one of the dimensions of the rating matrix. We propose a framework for scalable SSGD through significantly reducing the communication overhead via exchanging point-to-point messages utilizing the sparsity of the input matrix. We provide formulas to represent the essential communication for correctly performing parallel SSGD and we propose a dynamic programming algorithm for efficiently computing them to establish the point-to-point message schedules. This scheme, however, significantly increases the number of messages sent by a processor per epoch from O(K) to O(K^2) for a K-processor system which might limit the scalability. To remedy this, we propose a Hold-and-Combine strategy to limit the upper-bound on the number of messages sent per processor to O(KlgK). We also propose a hypergraph partitioning model that correctly encapsulates reducing the communication volume. Experimental results show that the framework successfully achieves a scalable distributed SSGD through significantly reducing the communication overhead. Our code is publicly available at: github.com/nfabubaker/CESSGD</p><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19350536.v1"
    },
    {
        "id": 4437,
        "title": "Gradient Descent Analysis: On Visualizing the Training of Deep Neural Networks",
        "authors": "Martin Becker, Jens Lippel, Thomas Zielke",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007583403380345"
    },
    {
        "id": 4438,
        "title": "On the Convergence of Stochastic Gradient Descent in Low-Precision Number Formats",
        "authors": "Matteo Cacciola, Antonio Frangioni, Masoud Asgharian, Alireza Ghaffari, Vahid Nia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011795500003411"
    },
    {
        "id": 4439,
        "title": "Scaling Stratified Stochastic Gradient Descent for Distributed Matrix Completion",
        "authors": "Nabil Abubaker, M. Ozan Karsavuran, Cevdet Aykanat",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Stratified SGD (SSGD) is the primary approach for achieving serializable parallel SGD for matrix completion. State-of-the-art parallelizations of SSGD fail to scale due to huge communication overhead. During an SGD epoch, these methods send data proportional to one of the dimensions of the rating matrix. We propose a framework for scalable SSGD through significantly reducing the communication overhead via exchanging point-to-point messages utilizing the sparsity of the input matrix. We provide formulas to represent the essential communication for correctly performing parallel SSGD and we propose a dynamic programming algorithm for efficiently computing them to establish the point-to-point message schedules. This scheme, however, significantly increases the number of messages sent by a processor per epoch from O(K) to O(K^2) for a K-processor system which might limit the scalability. To remedy this, we propose a Hold-and-Combine strategy to limit the upper-bound on the number of messages sent per processor to O(KlgK). We also propose a hypergraph partitioning model that correctly encapsulates reducing the communication volume. Experimental results show that the framework successfully achieves a scalable distributed SSGD through significantly reducing the communication overhead. Our code is publicly available at: github.com/nfabubaker/CESSGD</p><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19350536"
    },
    {
        "id": 4440,
        "title": "Gradient Descent",
        "authors": "David Paper",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3597-3_4"
    },
    {
        "id": 4441,
        "title": "Stochastic Gradient Descent",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108860604.006"
    },
    {
        "id": 4442,
        "title": "Advancing Stein Variational Gradient Descent for geophysical uncertainty estimation",
        "authors": "Muhammad Izzatullah, Matteo Ravasi, Tariq Alkhalifah",
        "published": "No Date",
        "citations": 0,
        "abstract": "&lt;p&gt;Stein Variational Gradient Descent (SVGD) is a powerful algorithm for uncertainty quantification algorithm introduced by the statistics community that has recently found applications in geophysical inverse problems. It is a non-parametric, iterative method for making probabilistic predictions by sampling from a set of particles, which are updated using gradient information of the Kullback-Leibler divergence between the target posterior distribution and a user-defined proposal distribution. One of the main benefits of SVGD is its ability to handle complex, high-dimensional distributions. This is particularly useful in geophysics, where datasets can be large, and the subsurface model of interest is high-dimensional. However, the computational cost of SVGD increases with the number of particles used to characterize the posterior. Its performance also depends on the choice of the prior distribution, as it influences the SVGD ability to provide geophysical and geological realism in the posterior samples.&lt;/p&gt;\n&lt;p&gt;There have been efforts to improve the efficiency of SVGD for large data sets, such as by introducing mini-batch techniques and deep learning-based prior aimed at producing high-resolution posterior samples. Along this direction, we are advancing the frontier of the SVGD algorithm by coupling it with the Plug-and-Play (PnP) framework, which allows sampling from a regularized target posterior distribution, where the target posterior distribution is regularized by a convolutional neural network (CNN) based denoiser. We showcase its ability to produce high-resolution, geologically trustworthy samples representative of the subsurface structures on a variety of geophysical problems for reservoir characterization and velocity model building (e.g., full waveform inversion).&lt;/p&gt;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-3610"
    },
    {
        "id": 4443,
        "title": "Accelerating Gradient Descent and Adam Via Fractional Gradients",
        "authors": "Yeonjong Shin, Jérôme Darbon, George  Em Karniadakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4157878"
    },
    {
        "id": 4444,
        "title": "Online Learning: the Stochastic Gradient Descent Family of Algorithms",
        "authors": "Sergios Theodoridis",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-818803-3.00014-3"
    },
    {
        "id": 4445,
        "title": "Functional Gradient Descent for n-Tuple Regression",
        "authors": "Rafael Katopodis, Priscila Lima, Felipe França",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-35"
    },
    {
        "id": 4446,
        "title": "Optimisation: Steepest Ascent, Steepest Descent and Gradient Methods",
        "authors": "Richard G. Brereton",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-409547-2.14835-8"
    },
    {
        "id": 4447,
        "title": "Constraint Guided Gradient Descent: Guided Training with Inequality Constraints",
        "authors": "Quinten Van Baelen, Peter Karsmakers",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-105"
    },
    {
        "id": 4448,
        "title": "Comparing the Effectiveness of Support Vector Classifier and Stochastic Gradient Descent in  Hate-Speech Detection",
        "authors": "Dania Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47611/harp.315"
    },
    {
        "id": 4449,
        "title": "Distributed Gradient Descent with Coded Partial Gradient Computations",
        "authors": "E. Ozfatura, S. Ulukus, D. Gunduz",
        "published": "2019-5",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2019.8683267"
    },
    {
        "id": 4450,
        "title": "Gradient Descent",
        "authors": "Robert H. Chen, Chelsea Chen",
        "published": "2022-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003214892-20"
    },
    {
        "id": 4451,
        "title": "Gradient Descent",
        "authors": "Christopher M. Bishop, Hugh Bishop",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45468-4_7"
    },
    {
        "id": 4452,
        "title": "Variable selection of regularized stochastic gradient descent in logistic regression",
        "authors": "Ping Guo",
        "published": "2022-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54647/mathematics11319"
    },
    {
        "id": 4453,
        "title": "Stochastic Gradient Descent in Continuous Time",
        "authors": "Justin Sirignano, Konstantinos Spiliopoulos",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.2954149"
    },
    {
        "id": 4454,
        "title": "Consensus seeking gradient descent flows on boundaries of convex sets",
        "authors": "Johan Markdahl",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147648"
    },
    {
        "id": 4455,
        "title": "Gradient Descent Algorithms",
        "authors": "Phillip A. Regalia",
        "published": "2018-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315136653-7"
    },
    {
        "id": 4456,
        "title": "Water budget estimation under parameter uncertainty using Stein Variational Gradient Descent",
        "authors": "Maximilian Ramgraber, Robin Weatherl, Mario Schirmer",
        "published": "No Date",
        "citations": 0,
        "abstract": "\n        &lt;p&gt;Increasingly intensive drought periods during the summer months put stresses even on traditionally water-rich regions such as Switzerland. In the particularly dry year of 2018, several Swiss municipalities were forced to place bans on agricultural irrigation, while others were forced to import water from neighbouring catchments to sustain water supply. The preparation for and management during such droughts demands sustainable management plans which are often informed by numerical models providing decision support.&lt;/p&gt;&lt;p&gt;Unfortunately, sustainable water resources management of alpine regions often demands a greater degree of system complexity than usual. This complexity must be reflected in the models used for decision-support: fixed head boundaries must be used cautiously, the aquifer&amp;#8217;s depth and properties are often uncertain and highly heterogeneous, and inflow and recharge are similarly difficult to quantify. Considering these diverse sources of uncertainty renders the Bayesian parameter inference problem highly challenging.&lt;/p&gt;&lt;p&gt;Towards this end, we explore a technique known as Stein Variational Gradient Descent (SVGD). This variational method implements a series of smooth transformations resulting in a particle flow, incrementally transforming an ensemble of particles into samples of the posterior. The method has been shown to be able to reproduce non-Gaussian and even multi-modal distributions, provided the underlying posterior is sufficiently smooth.&lt;/p&gt;&lt;p&gt;In this study, we test this algorithm with a groundwater model of the catchment of Fehraltorf implemented in MODFLOW 6. We consider parameter uncertainty for the aquifer depth and topology, its hydraulic parameters, and control variables for recharge and inflow. We report the resulting water table and budget and discuss the optimization performance.&lt;/p&gt;\n        ",
        "link": "http://dx.doi.org/10.5194/egusphere-egu2020-19396"
    },
    {
        "id": 4457,
        "title": "An Improved Adagrad Gradient Descent Optimization Algorithm",
        "authors": "N. Zhang, D. Lei, J.F. Zhao",
        "published": "2018-11",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623271"
    },
    {
        "id": 4458,
        "title": "Accelerated Distributed Composite Nesterov Gradient Descent Algorithm",
        "authors": "Xinhui Yue, Yuan-Hua Ni",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901596"
    },
    {
        "id": 4459,
        "title": "Denoising Gradient Descent in Variational Quantum Algorithms",
        "authors": "Lars Simon, Holger Eble, Hagen-Henrik Kowalski, Manuel Radons",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this article we introduce an algorithm for mitigating the adverse effects of noise on gradient descent in variational quantum algorithms. This is accomplished by computing a {\\emph{regularized}} local classical approximation to the objective function at every gradient descent step. The computational overhead of our algorithm is entirely classical, i.e., the number of circuit evaluations is exactly the same as when carrying out gradient descent using the parameter-shift rules. We empirically demonstrate the advantages offered by our algorithm on randomized parametrized quantum circuits.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4023677/v1"
    },
    {
        "id": 4460,
        "title": "Communication-Efficient Distributed Stochastic Gradient Descent with Pooling Operator",
        "authors": "Zhengao Cai, Aiguo Chen, Yi Luo, Jiahao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327869"
    },
    {
        "id": 4461,
        "title": "Hyperparameter-free optimizer of stochastic gradient descent that incorporates unit correction and moment estimation",
        "authors": "Kazunori D Yamada",
        "published": "No Date",
        "citations": 2,
        "abstract": "ABSTRACTIn the deep learning era, stochastic gradient descent is the most common method used for optimizing neural network parameters. Among the various mathematical optimization methods, the gradient descent method is the most naive. Adjustment of learning rate is necessary for quick convergence, which is normally done manually with gradient descent. Many optimizers have been developed to control the learning rate and increase convergence speed. Generally, these optimizers adjust the learning rate automatically in response to learning status. These optimizers were gradually improved by incorporating the effective aspects of earlier methods. In this study, we developed a new optimizer: YamAdam. Our optimizer is based on Adam, which utilizes the first and second moments of previous gradients. In addition to the moment estimation system, we incorporated an advantageous part of AdaDelta, namely a unit correction system, into YamAdam. According to benchmark tests on some common datasets, our optimizer showed similar or faster convergent performance compared to the existing methods. YamAdam is an option as an alternative optimizer for deep learning.",
        "link": "http://dx.doi.org/10.1101/348557"
    },
    {
        "id": 4462,
        "title": "Sensor array current consumption optimizer with Gradient Descent",
        "authors": "Boyan Y. Shabanski",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/et59121.2023.10279770"
    },
    {
        "id": 4463,
        "title": "Review on Gradient Descent Algorithms in Deep Learning Approaches",
        "authors": "Martin Henry",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3817511"
    },
    {
        "id": 4464,
        "title": "Learning to Learn Gradient Aggregation by Gradient Descent",
        "authors": "Jinlong Ji, Xuhui Chen, Qianlong Wang, Lixing Yu, Pan Li",
        "published": "2019-8",
        "citations": 14,
        "abstract": "In the big data era, distributed machine learning emerges as an important learning paradigm to mine large volumes of data by taking advantage of distributed computing resources. In this work, motivated by learning to learn, we propose a meta-learning approach to coordinate the learning process in the master-slave type of distributed systems. Specifically, we utilize a recurrent neural network (RNN) in the parameter server (the master) to learn to aggregate the gradients from the workers (the slaves). We design a coordinatewise preprocessing and postprocessing method to make the neural network based aggregator more robust. Besides, to address the fault tolerance, especially the Byzantine attack, in distributed machine learning systems, we propose an RNN aggregator with additional loss information (ARNN) to improve the system resilience. We conduct extensive experiments to demonstrate the effectiveness of the RNN aggregator, and also show that it can be easily generalized and achieve remarkable performance when transferred to other distributed systems. Moreover, under majoritarian Byzantine attacks, the ARNN aggregator outperforms the Krum, the state-of-art fault tolerance aggregation method, by 43.14%. In addition, our RNN aggregator enables the server to aggregate gradients from variant local models, which significantly improve the scalability of distributed learning.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/363"
    },
    {
        "id": 4465,
        "title": "On Gradient Descent and Co-ordinate Descent methods and its variants.",
        "authors": "Sajjadul Bari, Md. Rajib Arefin, Sohana Jahan",
        "published": "2020-12-31",
        "citations": 0,
        "abstract": "This research is focused on Unconstrained Optimization problems. Among a number of methods that can be used to solve Unconstrained Optimization problems we have worked on Gradient and Coordinate Descent methods. Step size plays an important role for optimization. Here we have performed numerical experiment with Gradient and Coordinate Descent method for several step size choices. Comparison between different variants of Gradient and Coordinate Descent methods and their efficiency are demonstrated by implementing in loss functions minimization problem.",
        "link": "http://dx.doi.org/10.53799/ajse.v19i3.103"
    },
    {
        "id": 4466,
        "title": "Parameterization of Turbulent Diffusivity using Gradient Descent",
        "authors": "Thomas Pendergast",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Fluid mixing and turbulent processes such as double diffusion are chaotic by nature and can be very difficult to parameterize. Experts have called for further investigation into parameterizing double diffusion and other vertical mixing processes for the implication that it may have on large-scale ocean and climate models. Interference from lateral flows and lateral mixing can often make field-data-driven parameterizations difficult and isolated experiments may have much more accurate results. By conducting isolated experiments which target a specific process, we can better quantify the effect that the individual process has. Using a variational method, the turbulent diffusivity associated with double diffusion can be parameterized by minimizing a cost function comparing a basic diffusion model to laboratory data.",
        "link": "http://dx.doi.org/10.24908/iqurcp16254"
    },
    {
        "id": 4467,
        "title": "Differentially Private Variance Reduced Stochastic Gradient Descent",
        "authors": "Jaewoo Lee",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictcs.2017.60"
    },
    {
        "id": 4468,
        "title": "Function Space Approach for Gradient Descent in Optimal Control",
        "authors": "Maurice Filo, Bassam Bamieh",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8430794"
    },
    {
        "id": 4469,
        "title": "A Gradient Descent-Ascent Method for Continuous-Time Risk-Averse Optimal Control",
        "authors": "Gabriel Velho, Jean Auriol, Riccardo Bonalli",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4588364"
    },
    {
        "id": 4470,
        "title": "Density Estimation-based Stein Variational Gradient Descent",
        "authors": "Jeongho Kim, Byungjoon Lee, Chohong Min, Jaewoo Park, Keunkwan Ryu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nApproximating a target distribution is important in many statistical problems, such as Bayesian inference. We introduce a variant of Stein variational gradient descent, called the density estimation-based Stein variational gradient descent (DESVGD). DESVGD utilizes kernel density estimation techniques to replace the empirical measure in SVGD with its continuous counterpart, which allows direct computation of the KL divergence between the current approximation and the target and thereby helps monitoring the numerical convergence of the iterative optimization process. DESVGD also offers derivatives of the KL divergence, which can be used to better design learning rates and thus to achieve faster convergence. By simply replacing the kernel used in SVGD with its weighted average, the implementation of DESVGD reduces to that of SVGD. Our numerical experiments demonstrate that DESVGD well approximates the target distribution and that it outperforms the original SVGD in terms of approximation quality.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3684152/v1"
    },
    {
        "id": 4471,
        "title": "Fast Armijo line search for stochastic gradient descent",
        "authors": "Sajad Fathi Hafshejani, Daya Gaur, Shahadat Hossain, Robert Benkoczi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWe give an improved non-monotone line search algorithm for stochastic gradient descent (SGD) for functions that satisfy interpolation conditions. We establish theoretical convergence guarantees for the algorithm for strongly convex, convex and non-convex functions. We conduct a detailed empirical evaluation to validate the theoretical results.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2285238/v1"
    },
    {
        "id": 4472,
        "title": "Differentially Private Gossip Gradient Descent",
        "authors": "Yang Liu, Ji Liu, Tamer Basar",
        "published": "2018-12",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619437"
    },
    {
        "id": 4473,
        "title": "SPGD: Search Party Gradient Descent Algorithm, a Simple Gradient-Based Parallel Algorithm for Bound-Constrained Optimization",
        "authors": "A. Syed Shahul Hameed, Narendran Rajagopalan",
        "published": "2022-3-2",
        "citations": 4,
        "abstract": "Nature-inspired metaheuristic algorithms remain a strong trend in optimization. Human-inspired optimization algorithms should be more intuitive and relatable. This paper proposes a novel optimization algorithm inspired by a human search party. We hypothesize the behavioral model of a search party searching for a treasure. Motivated by the search party’s behavior, we abstract the “Divide, Conquer, Assemble” (DCA) approach. The DCA approach allows us to parallelize the traditional gradient descent algorithm in a strikingly simple manner. Essentially, multiple gradient descent instances with different learning rates are run parallelly, periodically sharing information. We call it the search party gradient descent (SPGD) algorithm. Experiments performed on a diverse set of classical benchmark functions show that our algorithm is good at optimizing. We believe our algorithm’s apparent lack of complexity will equip researchers to solve problems efficiently. We compare the proposed algorithm with SciPy’s optimize library and it is found to be competent with it.",
        "link": "http://dx.doi.org/10.3390/math10050800"
    },
    {
        "id": 4474,
        "title": "Who Needs Gradient Descent? An Analysis of Random Optimization Approaches for Neural Network Optimization",
        "authors": "Harshvardhan Sikka",
        "published": "No Date",
        "citations": 0,
        "abstract": "This report is comprised of 2 sections. The first focuses on the analysis of three optimization problem domains that highlight the strengths of local random search algorithms. Algorithms implemented and analyzed include Randomized Hill Climbing (RHC), simulated annealing (SA), a genetic algorithm (GA) [1], and the MIMIC algorithm (MIMIC) [2]. Their strengths and weaknesses are discussed from both a theoretical standpoint as well as a practical one in the context of their performance on the three optimization problem domains. Following this, the second section explores the use of the first 3 of those local random search algorithms in the optimization of the neural network classifier on the Car Evaluation Dataset, sourced from Kaggle. The dataset and basic network architecture was chosen because of certain interesting characteristics. The general supervised task and analysis are motivated in detail in Section 2.",
        "link": "http://dx.doi.org/10.31219/osf.io/yshpr"
    },
    {
        "id": 4475,
        "title": "Gradient descent learning for quaternionic Hopfield neural networks",
        "authors": "Masaki Kobayashi",
        "published": "2017-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2017.04.025"
    },
    {
        "id": 4476,
        "title": "Comparing Stochastic Gradient Descent and Mini-batch Gradient Descent Algorithms in Loan Risk Assessment",
        "authors": "Abodunrin AbdulGafar Adigun, Chika Yinka-Banjo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-95630-1_20"
    },
    {
        "id": 4477,
        "title": "Self-calibration of UAV Thermal Imagery Using Gradient Descent Algorithm",
        "authors": "Radosław Szostak, Miroslaw Zimnoch, Przemyslaw Wachniew",
        "published": "No Date",
        "citations": 0,
        "abstract": "Unmanned aerial vehicles (UAV) thermal imagery offers several advantages in environmental monitoring, as it can provide a low-cost, high-resolution, and flexible solution to measure the temperature of the surface of the land. Limitations related to the maximum load of the drone lead to use of lightweight uncooled thermal cameras whose internal components are not stabilized to a constant temperature. Such cameras suffer from several unwanted effects that contribute to the increase in temperature measurement error from ±0.5 °C in laboratory conditions, to ±5 °C in unstable flight conditions. This article describes a post processing procedure, that reduces the above unwanted effects. It consists of following steps: i) devignetting using single image vignette correction algorithm, ii) georeferencing of images using EXIF data, scale-invariant feature transform (SIFT) stitching, and gradient descent optimisation, and iii) temperature calibration by minimisation of bias between overlapping thermal images using gradient descent optimisation. The solution was tested in several case studies of river areas, where natural water bodies were used as a reference temperature benchmark. In all tests, the precision of the measurements was increased. The root of the mean of the Square of Errors RMSE on average was reduced by 39.0% and Mean of the absolute value of Errors MAE by 40.5%. The proposed algorithm can be called self-calibrating, as in contrast to other known solutions is fully automatic, uses only field data and does not require any calibration equipment or additional operator effort. A Python implementation of the solution is available on GitHub.",
        "link": "http://dx.doi.org/10.20944/preprints202310.0187.v1"
    },
    {
        "id": 4478,
        "title": "Gradient descent in sample-based single-query path planning algorithm",
        "authors": "Rajaneesh Acharya, Debashisha Jena",
        "published": "2018-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etechnxt.2018.8385319"
    },
    {
        "id": 4479,
        "title": "A Gradient Descent Inspired Approach to Optimization of Physics Question",
        "authors": "Feihong Liu, Yu Sun",
        "published": "2022-10-22",
        "citations": 0,
        "abstract": "Many people believe that the crouch start was the best way to start a sprint [1]. While it seems intuitive, when the process of running is dissected using specific physical and mathematical representations, the question of “what is the best starting position” becomes harder to answer [2]. This paper aims to examine this phenomenon through a computer science approach inspired by gradient descent. Specifically, this paper aims to maximise the distance covered by a runner in ten steps. Assuming that runners do their best on every step and that their motion is not slowed by friction or air resistance, we will generate a hypothetical environment to study what the best strategy is for reaching the furthest distance within ten steps.",
        "link": "http://dx.doi.org/10.5121/csit.2022.121708"
    },
    {
        "id": 4480,
        "title": "Adaptive Low-Rank Gradient Descent",
        "authors": "Ali Jadbabaie, Anuran Makur, Amirhossein Reisizadeh",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383982"
    },
    {
        "id": 4481,
        "title": "Multi-Dimensional Deconvolution with Stochastic Gradient Descent",
        "authors": "M. Ravasi, T. Selvan Pandurangan, N. Luiken",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202210234"
    },
    {
        "id": 4482,
        "title": "Federated quantum natural gradient descent for quantum federated learning",
        "authors": "Jun Qi, Min-Hsiu Hsieh",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-44-319037-7.00028-4"
    },
    {
        "id": 4483,
        "title": "An Effective Partitional Crisp Clustering Method Using Gradient Descent Approach",
        "authors": "Soroosh Shalileh",
        "published": "2023-6-7",
        "citations": 2,
        "abstract": "Enhancing the effectiveness of clustering methods has always been of great interest. Therefore, inspired by the success story of the gradient descent approach in supervised learning in the current research, we proposed an effective clustering method using the gradient descent approach. As a supplementary device for further improvements, we implemented our proposed method using an automatic differentiation library to facilitate the users in applying any differentiable distance functions. We empirically validated and compared the performance of our proposed method with four popular and effective clustering methods from the literature on 11 real-world and 720 synthetic datasets. Our experiments proved that our proposed method is valid, and in the majority of the cases, it is more effective than the competitors.",
        "link": "http://dx.doi.org/10.3390/math11122617"
    },
    {
        "id": 4484,
        "title": "Iterative Pre-Conditioning to Expedite the Gradient-Descent Method",
        "authors": "Kushal Chakrabarti, Nirupam Gupta, Nikhil Chopra",
        "published": "2020-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147603"
    },
    {
        "id": 4485,
        "title": "On Byzantine-Resilient High-Dimensional Stochastic Gradient Descent",
        "authors": "Deepesh Data, Suhas Diggavi",
        "published": "2020-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isit44484.2020.9174363"
    },
    {
        "id": 4486,
        "title": "Static &amp; Dynamic Appointment Scheduling with Stochastic Gradient Descent",
        "authors": "Gary Cheng, Kabir Chandrasekher, Jean Walrand",
        "published": "2019-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8814666"
    },
    {
        "id": 4487,
        "title": "Linear Precoder Design for PAPR Reduction of GFDM Signals Using Gradient Descent Methods",
        "authors": "Mohsen Sheikh-Hosseini, Farhad Rahdari, Mohammad Hasheminejad",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper addresses linear precoder design for Peak-to-Average Power Ratio (PARP) reduction of Generalized Frequency Division Multiplexing (GFDM). A general framework, which is composed of four different scenarios and utilizes Gradient-based iterative methods to reduce PAPR through minimizing statistical parameters of the instantaneous power of GFDM signal including variance, power, and third moment, is suggested. Numerical results confirm when the step-size of the Gradient method is dynamically computed using the Wolf line search rule, the suggested algorithm circumvents drawbacks of existing studies and converges to a precoder providing advantages in design speed, obtained PAPR, symbol error rate, and out-of-band emission.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16436559.v1"
    },
    {
        "id": 4488,
        "title": "Hybrid approximate gradient and stochastic descent for falsification of nonlinear systems",
        "authors": "Shakiba Yaghoubi, Georgios Fainekos",
        "published": "2017-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963007"
    },
    {
        "id": 4489,
        "title": "Evaluation of Gradient Descent Algorithm for Attitude Estimation",
        "authors": "Karla Sever, Ivan Indir, Ivan Vnucec, Josip Loncar",
        "published": "2021-9-13",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elmar52657.2021.9550764"
    },
    {
        "id": 4490,
        "title": "Gradient-Descent Based Optimization of Constant Envelope OFDM Waveforms",
        "authors": "David G. Felton, David A. Hague",
        "published": "2023-5-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149677"
    },
    {
        "id": 4491,
        "title": "Gradient descent method for group particles based on Improved Genetic Algorithm",
        "authors": "yulong shan, Shijun Zhao, Qiuhan Li, Ren Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Artificial intelligence (AI) has been a hot research topic in recent\nyears and the algorithm is its technical core. Many new algorithms have\nbeen proposed to solve complex non-linear optimization problems, and\neach of them has its own advantages and disadvantages. This study\nimproves the selection operator of Genetic Algorithm (GA) by combining\nartificial selection with probabilistic selection to improve the\nconvergence speed and the success rate when searching for the global\noptimal solution using GA. At the same time, considering that Gradient\nDescent (GD) method has high convergence rate, and GA has strong global\nsearch ability, we propose the algorithm of Gradient Descent method for\ngroup particles (GDFGP) based on GD and improved GA. Two experiments are\ncarried out to verify the effectiveness of the new algorithm on\nconvergence rate and global search ability. The results show that the\nimproved GA had better convergence rate than the traditional GA. The\nsuccess rate for obtaining the global optimal solution of GDFGP is\nhigher than that of traditional and improved GA, especially when solving\nthe more complex nonlinear optimization problems.",
        "link": "http://dx.doi.org/10.22541/au.170669025.52099157/v1"
    },
    {
        "id": 4492,
        "title": "Angle based dynamic learning rate for gradient descent",
        "authors": "Neel Mishra, Pawan Kumar",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191702"
    },
    {
        "id": 4493,
        "title": "Gradient Descent Optimization Method for AVO Inversion in Viscoelastic Media",
        "authors": "N. Ahmed, W.W. Weibull",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202210467"
    },
    {
        "id": 4494,
        "title": "Linear Precoder Design for PAPR Reduction of GFDM Signals Using Gradient Descent Methods",
        "authors": "Mohsen Sheikh-Hosseini, Farhad Rahdari, Mohammad Hasheminejad",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper addresses linear precoder design for Peak-to-Average Power Ratio (PARP) reduction of Generalized Frequency Division Multiplexing (GFDM). A general framework, which is composed of four different scenarios and utilizes Gradient-based iterative methods to reduce PAPR through minimizing statistical parameters of the instantaneous power of GFDM signal including variance, power, and third moment, is suggested. Numerical results confirm when the step-size of the Gradient method is dynamically computed using the Wolf line search rule, the suggested algorithm circumvents drawbacks of existing studies and converges to a precoder providing advantages in design speed, obtained PAPR, symbol error rate, and out-of-band emission.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16436559"
    },
    {
        "id": 4495,
        "title": "A bug prediction method for mobile app versioning using Gradient Descent Algorithm",
        "authors": "Mamta Pandey, Ratnesh Litoriya, Prateek Pandey",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMobile software (apps) is increasing day by day and human being is mostly dependent on these apps. New methodologies, tools, and technologies have continuously made the lifecycle for the mobile apps development more technology dependent. Very few research in the field of mobile app bug targets on bug evaluation, bug recognition, and bug prediction. Prediction of a bug in software field is almost contemporary research area that includes adopting numerous approaches such as fuzzy logic, artificial intelligence, data mining etc. Nevertheless, bug prediction of mobile apps is the very latest area of research and existing literature shows that there are very superficial work has done in this area. Bug detection and correction is complex phenomena for mobile apps. There are various advantages to measuring the mobile app such as accuracy of estimation, mobile app cost to boost the quality of apps. The purpose of this study is to provide a model that predicts the number of bugs in a future version of a mobile app compared to an earlier version of the app, and we used a gradient descent algorithm to accomplish this task. The latest version of the app may have advance feature, new content, modified design etc. The aim of our study is to anticipate the new bugs recognized in the latest version of the app by evaluating the category of modifications in an objective and recognizing various types of bugs. Mobile app developer and project manager both can get help from bug predictor’s model. The bug predictor model helps to improve the quality codes and minimize the test time.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1503957/v1"
    },
    {
        "id": 4496,
        "title": "Efficient phase-locking of 60 fiber lasers by stochastic parallel gradient descent algorithm",
        "authors": "Hongxiang Chang, Jiachao Xi, Rongtao Su, Pengfei Ma, Yanxing Ma, Pu Zhou",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/col202018.101403"
    },
    {
        "id": 4497,
        "title": "Distributed Source Seeking and Robust Obstacle Avoidance Through Hybrid Gradient Descent",
        "authors": "Hannah Mohr, Kevin Schroeder, Jonathan Black",
        "published": "2019-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero.2019.8741882"
    },
    {
        "id": 4498,
        "title": "Homotopy analysis method for Burgers’ equation: Application of gradient descent approach",
        "authors": "Bhavin Sangani, Karthik Engolikar, Ranjan Kumar Jana, Mahesh Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.165942007.79344292/v1"
    },
    {
        "id": 4499,
        "title": "Network Revenue Management with Online Inverse Batch Gradient Descent Method",
        "authors": "Yiwei Chen, Cong Shi",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3331939"
    },
    {
        "id": 4500,
        "title": "Hyper-parameter optimization for support vector machines using stochastic gradient descent and dual coordinate descent",
        "authors": "W.e.i. Jiang, Sauleh Siddiqui",
        "published": "2020-3",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13675-019-00115-7"
    }
]