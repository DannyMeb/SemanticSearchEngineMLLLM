[
    {
        "id": 7501,
        "title": "<scp>DARPA</scp>'s explainable <scp>AI</scp> (<scp>XAI</scp>) program: A retrospective",
        "authors": "David Gunning, Eric Vorm, Jennifer Yunyan Wang, Matt Turek",
        "published": "2021-12",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/ail2.61"
    },
    {
        "id": 7502,
        "title": "xAI-EWS — an explainable AI model predicting acute critical illness",
        "authors": "",
        "published": "2020-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32907/ro-118-3033"
    },
    {
        "id": 7503,
        "title": "Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare",
        "authors": "Tim Hulsen",
        "published": "2023-8-10",
        "citations": 15,
        "abstract": "Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.",
        "link": "http://dx.doi.org/10.3390/ai4030034"
    },
    {
        "id": 7504,
        "title": "Explainable AI (XAI)",
        "authors": "Tanvir Habib Sardar, Sunanda Das, Bishwajeet Kumar Pandey",
        "published": "2023-9-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003257721-1"
    },
    {
        "id": 7505,
        "title": "DARPA’s Explainable AI (XAI) program: A retrospective",
        "authors": "David Gunning, Eric Vorm, Yunyan Wang, Matt Turek",
        "published": "No Date",
        "citations": 8,
        "abstract": "DARPA formulated the Explainable Artificial Intelligence (XAI) program\nin 2015 with the goal to enable end users to better understand, trust,\nand effectively manage artificially intelligent systems. In 2017, the\nfour-year XAI research program began. Now, as XAI comes to an end in\n2021, it is time to reflect on what succeeded, what failed, and what was\nlearned. This article summarizes the goals, organization, and research\nprogress of the XAI Program.",
        "link": "http://dx.doi.org/10.22541/au.163699841.19031727/v1"
    },
    {
        "id": 7506,
        "title": "Explainable Artificial Intelligence (XAI): Conception, Visualization and Assessment Approaches Towards Amenable XAI",
        "authors": "Tasleem Nizam, Sherin Zafar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-18292-1_3"
    },
    {
        "id": 7507,
        "title": "A Study on Modeling of Activated Sludge Process in Wastewater Treatment System Utilizing XAI(eXplainable AI)",
        "authors": "Eui-Seok Nahm",
        "published": "2023-2-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5370/kiee.2023.72.2.263"
    },
    {
        "id": 7508,
        "title": "VI. eXplainable AI (XAI)",
        "authors": "",
        "published": "2024-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33196/9783704693877-106"
    },
    {
        "id": 7509,
        "title": "Explainable AI (XAI): Bridging the Gap between Machine Learning and Human Understanding",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.48047/resmil.v10i1.19"
    },
    {
        "id": 7510,
        "title": "Model-Agnostic Methods for XAI",
        "authors": "Leonida Gianfagna, Antonio Di Cecco",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68640-6_4"
    },
    {
        "id": 7511,
        "title": "An Explainable Ai (Xai) Model for Text-Based Patent Novelty Analysis",
        "authors": "Hyejin Jang, Sunhye Kim, Byungun Yoon",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4341594"
    },
    {
        "id": 7512,
        "title": "Transparency in Decision-making: the Role of Explainable Ai (Xai) in Customer Churn Analysis",
        "authors": "Cem ÖZKURT",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn many industries, as in the telecommunications sector, identifying the reasons for customer loss is a primary challenge. Predicting which customers will churn or continue their subscriptions is of utmost importance in the telecommunications sector. Machine learning and data science offer numerous solutions to this issue. These proposed solutions hold a significant place in decision-making processes across various sectors. This study aims to predict lost customers and explain the reasons behind it using machine learning algorithms. The dataset used includes Linear Regression, Logistic Regression, Naive Bayes, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), Gradient Boosting, XGBoost (eXtreme Gradient Boosting), LightGBM, AdaBoost and CatBoost algorithms to find the best-performing classification model. Performance metrics such as $R^2$ score, Mean Squared Error, Mean Absolute Error, Root Mean Squared Error, and Accuracy are used in this process. Finally, the best prediction model is explained using eXplainable Artificial Intelligence (XAI).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3937355/v1"
    },
    {
        "id": 7513,
        "title": "Prologue: Introduction to Explainable Artificial Intelligence",
        "authors": "Moamar Sayed-Mouchaweh",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8_1"
    },
    {
        "id": 7514,
        "title": "Explainable AI Within the Digital Transformation and Cyber Physical Systems",
        "authors": "",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8"
    },
    {
        "id": 7515,
        "title": "Explainable AI (XAI): A Survey of Current and Future Opportunities",
        "authors": "Meet Kumari, Akshit Chaudhary, Yogendra Narayan",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-18292-1_4"
    },
    {
        "id": 7516,
        "title": "Need for Artificial Intelligence (Ai) to Be Explainable in Banking and Finance: Review of Ai Applications, Ai Black Box, Xai Tools and Principles",
        "authors": "Rahul Meena, rajdeep raut, Akshay Mishra",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4554614"
    },
    {
        "id": 7517,
        "title": "Automated machine learning and explainable AI (AutoML-XAI) for metabolomics: improving cancer diagnostics",
        "authors": "Olatomiwa O. Bifarin, Facundo M. Fernández",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotivationMetabolomics generates complex data necessitating advanced computational methods for generating biological insight. While machine learning (ML) is promising, the challenges of selecting the best algorithms and tuning hyperparameters, particularly for nonexperts, remain. Automated machine learning (AutoML) can streamline this process; however, the issue of interpretability could persist. This research introduces a unified pipeline that combines AutoML with explainable AI (XAI) techniques to optimize metabolomics analysis.ResultsWe tested our approach on two datasets: renal cell carcinoma (RCC) urine metabolomics and ovarian cancer (OC) serum metabolomics. AutoML, using auto-sklearn, surpassed standalone ML algorithms such as SVM and random forest in differentiating between RCC and healthy controls, as well as OC patients and those with other gynecological cancers (Non-OC). Autosklearn employed a mix of algorithms and ensemble techniques, yielding a superior performance (AUC of 0.97 for RCC and 0.85 for OC). Shapley Additive Explanations (SHAP) provided a global ranking of feature importance, identifying dibutylamine and ganglioside GM(d34:1) as the top discriminative metabolites for RCC and OC, respectively. Waterfall plots offered local explanations by illustrating the influence of each metabolite on individual predictions. Dependence plots spotlighted metabolite interactions, such as the connection between hippuric acid and one of its derivatives in RCC, and between GM3(d34:1) and GM3(18:1_16:0) in OC, hinting at potential mechanistic relationships. Through decision plots, a detailed error analysis was conducted, contrasting feature importance for correctly versus incorrectly classified samples. In essence, our pipeline emphasizes the importance of harmonizing AutoML and XAI, facilitating both simplified ML application and improved interpretability in metabolomics data science.Availabilityhttps://github.com/obifarin/automl-xai-metabolomicsContact:facundo.fernandez@chemistry.gatech.edu",
        "link": "http://dx.doi.org/10.1101/2023.10.26.564244"
    },
    {
        "id": 7518,
        "title": "The Need For Explainable AI (XAI) Is Especially Crucial In The Law",
        "authors": "Lance Eliot",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3975778"
    },
    {
        "id": 7519,
        "title": "Explainable AI (XAI) for AI-Acceptability: The Coming Age of Digital Management 5.0",
        "authors": "Samia Chehbi Gamoura",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc58704.2023.10319030"
    },
    {
        "id": 7520,
        "title": "EXPLAINABLE AI (XAI): BRIDGING THE GAP BETWEEN MACHINE LEARNING AND HUMAN UNDERSTANDING (2020)",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.48047/jcr.07.03.374"
    },
    {
        "id": 7521,
        "title": "Explainable Artificial Intelligence (XAI): Understanding and Future Perspectives",
        "authors": "Megha Gupta",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-18292-1_2"
    },
    {
        "id": 7522,
        "title": "Making Science with Machine Learning and XAI",
        "authors": "Leonida Gianfagna, Antonio Di Cecco",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68640-6_6"
    },
    {
        "id": 7523,
        "title": "Earthquake-Induced Building-Damage Mapping Using Explainable AI (XAI)",
        "authors": "Sahar S. Matin, Biswajeet Pradhan",
        "published": "2021-6-30",
        "citations": 28,
        "abstract": "Building-damage mapping using remote sensing images plays a critical role in providing quick and accurate information for the first responders after major earthquakes. In recent years, there has been an increasing interest in generating post-earthquake building-damage maps automatically using different artificial intelligence (AI)-based frameworks. These frameworks in this domain are promising, yet not reliable for several reasons, including but not limited to the site-specific design of the methods, the lack of transparency in the AI-model, the lack of quality in the labelled image, and the use of irrelevant descriptor features in building the AI-model. Using explainable AI (XAI) can lead us to gain insight into identifying these limitations and therefore, to modify the training dataset and the model accordingly. This paper proposes the use of SHAP (Shapley additive explanation) to interpret the outputs of a multilayer perceptron (MLP)—a machine learning model—and analyse the impact of each feature descriptor included in the model for building-damage assessment to examine the reliability of the model. In this study, a post-event satellite image from the 2018 Palu earthquake was used. The results show that MLP can classify the collapsed and non-collapsed buildings with an overall accuracy of 84% after removing the redundant features. Further, spectral features are found to be more important than texture features in distinguishing the collapsed and non-collapsed buildings. Finally, we argue that constructing an explainable model would help to understand the model’s decision to classify the buildings as collapsed and non-collapsed and open avenues to build a transferable AI model.",
        "link": "http://dx.doi.org/10.3390/s21134489"
    },
    {
        "id": 7524,
        "title": "eXplainable AI (XAI)",
        "authors": "Rowan Hughes, Cameron Edmond, Lindsay Wells, Mashhuda Glencross, Liming Zhu, Tomasz Bednarz",
        "published": "2020-11-17",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3415263.3419166"
    },
    {
        "id": 7525,
        "title": "Explainable AI for stock price prediction in stock market",
        "authors": "Pattabiraman Venkatasubbu, Sharath Kumar Jagannathan, Anusooya Govindarajan, Maheswari Raja",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch18"
    },
    {
        "id": 7526,
        "title": "Foundations for Human-AI teaming for self-regulated learning with explainable AI (XAI)",
        "authors": "Judy Kay",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.chb.2023.107848"
    },
    {
        "id": 7527,
        "title": "Explainable AI (XAI): Explained",
        "authors": "G. Pradeep Reddy, Y. V. Pavan Kumar",
        "published": "2023-4-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/estream59056.2023.10134984"
    },
    {
        "id": 7528,
        "title": "Explainable AI(XAI): A Review",
        "authors": "Deepak Sharma, Vikash Koundilya, Shivam Verma",
        "published": "2020",
        "citations": 0,
        "abstract": "Explainable AI (XAI) has emerged as a essential region of research within the realm of device getting to know, that specialize in improving the interpretability and comprehensibility of complex fashions. The opacity of many device getting to know algorithms, specially deep neural networks, has raised worries regarding the transparency and responsibility of computerized choice-making systems. This research targets to delve into various methodologies geared toward making those fashions greater interpretable, understandable, and in the long run extra straightforward. The primary motivation in the back of this research lies inside the imperative to bridge the space among the inherent complexity of superior device studying models and the need for obvious decision-making methods. Achieving explainability is vital for gaining user agree with, ensuring regulatory compliance, and facilitating the adoption of AI systems in touchy domains which include healthcare, finance, and crook justice. One road of exploration entails growing novel techniques for model interpretation, permitting stakeholders to understand the cause behind a model's predictions. This could consist of growing visualization gear that offer insights into characteristic significance, decision obstacles, and universal model conduct. Another component below scrutiny is the incorporation of inherently interpretable models or the amendment of present complex fashions to render them extra obvious with out compromising performance. Furthermore, the research scrutinizes the alternate-off among version complexity and interpretability, aiming to strike a balance that ensures both accuracy and explainability. Ethical concerns related to bias and fairness in interpretable AI models also are examined, acknowledging the significance of keeping off unintentional results in choice-making processes. As the deployment of AI structures turns into more and more pervasive, the effects of this studies are poised to have a huge impact at the responsible and ethical use of synthetic intelligence. By dropping mild on the inner workings of these structures, explainable AI contributes to constructing a basis of trust among users, builders, and society at big, fostering a greater obvious and accountable era within the application of system studying technology.",
        "link": "http://dx.doi.org/10.61841/v24i5/400345"
    },
    {
        "id": 7529,
        "title": "From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness",
        "authors": "Niclas Feldkamp, Steffen Strassburger",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3573900.3591114"
    },
    {
        "id": 7530,
        "title": "Principles of Explainable Artificial Intelligence",
        "authors": "Riccardo Guidotti, Anna Monreale, Dino Pedreschi, Fosca Giannotti",
        "published": "2021",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8_2"
    },
    {
        "id": 7531,
        "title": "Explainable AI (XAI) and its Applications in Building Trust and Understanding in AI Decision Making",
        "authors": "Rudra Tiwari",
        "published": "2023-1-27",
        "citations": 1,
        "abstract": "In recent years, there has been a growing need for Explainable AI (XAI) to build trust and understanding in AI decision making. XAI is a field of AI research that focuses on developing algorithms and models that can be easily understood and interpreted by humans. The goal of XAI is to make the inner workings of AI systems transparent and explainable, which can help people to understand the reasoning behind the decisions made by AI and make better decisions. In this paper, we will explore the various applications of XAI in different domains such as healthcare, finance, autonomous vehicles, and legal and government decisions. We will also discuss the different techniques used in XAI such as feature importance analysis, model interpretability, and natural language explanations. Finally, we will examine the challenges and future directions of XAI research. This paper aims to provide an overview of the current state of XAI research and its potential impact on building trust and understanding in AI decision making.",
        "link": "http://dx.doi.org/10.55041/ijsrem17592"
    },
    {
        "id": 7532,
        "title": "Stroke Risk Prediction from Medical Survey Data: AI-Driven Risk Analysis with Insightful Feature Importance using Explainable AI (XAI)",
        "authors": "Simon Bin Akter, Sumya Akter, Tanmoy Sarkar Pias",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractPrioritizing dataset dependability, model performance, and interoperability is a compelling demand for improving stroke risk prediction from medical surveys using AI in healthcare. These collective efforts are required to enhance the field of stroke risk assessment and demonstrate the transformational potential of AI in healthcare. This novel study leverages the CDC’s recently published 2022 BRFSS dataset to explore AI-based stroke risk prediction. Numerous substantial and notable contributions have been established from this study. To start with, the dataset’s dependability is improved through a unique RF-based imputation technique that overcomes the challenges of missing data. In order to identify the most promising models, six different AI models are meticulously evaluated including DT, RF, GNB, RusBoost, AdaBoost, and CNN. The study combines top-performing models such as GNB, RF, and RusBoost using fusion approaches such as soft voting, hard voting, and stacking to demonstrate the combined prediction performance. The stacking model demonstrated superior performance, achieving an F1 score of 88%. The work also employs Explainable AI (XAI) approaches to highlight the subtle contributions of important dataset features, improving model interpretability. The comprehensive approach to stroke risk prediction employed in this study enhanced dataset reliability, model performance, and interpretability, demonstrating AI’s fundamental impact in healthcare.",
        "link": "http://dx.doi.org/10.1101/2023.11.17.23298646"
    },
    {
        "id": 7533,
        "title": "Endorsed Attributions: eXplainable AI (XAI) with Voting Mechanism with Application in Healthcare",
        "authors": "Erico Tjoa, Feng Xie, Chi-Hung Shu, Lei Xue, Nima Aghaeepour, Cuntai Guan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Feature attribution methods have been widely employed in the <u>eXplainable</u> Artificial Intelligence (<u>XAI</u>) community and they have been designed with various underlying concepts. As a consequence, these methods sometimes assign different feature importance on the same data. To resolve the possible discrepancies, we introduce \\textit{endorsed attributions}, a voting mechanism that integrates the strengths of different feature attribution methods on healthcare datasets: maternal health risk classification, body signal of smoking, <u>fetal</u> health and heart disease classification. Our methods generate simplified visualization charts as an explainable AI use case in clinical settings, avoiding the need for end users like patients or medical practitioners to focus too deeply on technical nuances. Graph visualizations from our partitioning methods also reveal the differences between correct and wrong predictions. Finally, we demonstrate how the extraction of endorsement cores (EEC) can be used for data pruning, and models trained on the resulting EEC data subset can achieve competitive performance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23949135.v1"
    },
    {
        "id": 7534,
        "title": "Explainable AI (XAI) in Rules as Code (RaC): The DataLex approach",
        "authors": "Andrew Mowbray, Philip Chung, Graham Greenleaf",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4093026"
    },
    {
        "id": 7535,
        "title": "Extensive Review of Literature on Explainable AI (XAI) in Healthcare\nApplications",
        "authors": "Ramasamy Mariappan",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "\nAbstract:\nArtificial Intelligence (AI) techniques are widely being used in the medical fields or\nvarious applications including diagnosis of diseases, prediction and classification of diseases,\ndrug discovery, etc. However, these AI techniques are lacking in the transparency of the predictions\nor decisions made due to their black box-type operations. The explainable AI (XAI)\naddresses such issues faced by AI to make better interpretations or decisions by physicians.\nThis article explores XAI techniques in the field of healthcare applications, including the Internet\nof Medical Things (IoMT). XAI aims to provide transparency, accountability, and traceability\nin AI-based systems in healthcare applications. It can help in interpreting the predictions\nor decisions made in medical diagnosis systems, medical decision support systems, smart\nwearable healthcare devices, etc. Nowadays, XAI methods have been utilized in numerous\nmedical applications over the Internet of Things (IOT), such as medical diagnosis, prognosis,\nand explanations of the AI models, and hence, XAI in the context of IoMT and healthcare has\nthe potential to enhance the reliability and trustworthiness of AI systems.\n",
        "link": "http://dx.doi.org/10.2174/0126662558296699240314055348"
    },
    {
        "id": 7536,
        "title": "Quality framework for explainable artificial intelligence (XAI) and machine learning applications",
        "authors": "Muthu Ramachandran",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch7"
    },
    {
        "id": 7537,
        "title": "Application of Explainable AI (Xai) For Anomaly Detection and Prognostic of Gas Turbines with Uncertainty Quantification.",
        "authors": "Ahmad Kamal Mohd Nor, Srinivasa Rao Pedapati, Masdi Muhammad",
        "published": "No Date",
        "citations": 3,
        "abstract": "XAI is presently in its early assimilation phase in Prognostic and Health Management (PHM) domain. However, the handful of PHM-XAI articles suffer from various deficiencies, amongst others, lack of uncertainty quantification and explanation evaluation metric. This paper proposes an anomaly detection and prognostic of gas turbines using Bayesian deep learning (DL) model with SHapley Additive exPlanations (SHAP). SHAP was not only applied to explain both tasks, but also to improve the prognostic performance, the latter trait being left undocumented in the previous PHM-XAI works. Uncertainty measure serves to broaden explanation scope and was also exploited as anomaly indicator. Real gas turbine data was tested for the anomaly detection task while NASA CMAPSS turbofan datasets were used for prognostic. The generated explanation was evaluated using two metrics: Local Accuracy and Consistency. All anomalies were successfully detected thanks to the uncertainty indicator. Meanwhile, the turbofan prognostic results show up to 9% improvement in RMSE and 43% enhancement in early prognostic due to SHAP, making it comparable to the best published methods in the problem. XAI and uncertainty quantification offer a comprehensive explanation package, assisting decision making. Additionally, SHAP ability in boosting PHM performance solidifies its worth in AI-based reliability research.",
        "link": "http://dx.doi.org/10.20944/preprints202109.0034.v1"
    },
    {
        "id": 7538,
        "title": "Endorsed Attributions: eXplainable AI (XAI) with Voting Mechanism with Application in Healthcare",
        "authors": "Erico Tjoa, Feng Xie, Chi-Hung Shu, Lei Xue, Nima Aghaeepour, Cuntai Guan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Feature attribution methods have been widely employed in the <u>eXplainable</u> Artificial Intelligence (<u>XAI</u>) community and they have been designed with various underlying concepts. As a consequence, these methods sometimes assign different feature importance on the same data. To resolve the possible discrepancies, we introduce \\textit{endorsed attributions}, a voting mechanism that integrates the strengths of different feature attribution methods on healthcare datasets: maternal health risk classification, body signal of smoking, <u>fetal</u> health and heart disease classification. Our methods generate simplified visualization charts as an explainable AI use case in clinical settings, avoiding the need for end users like patients or medical practitioners to focus too deeply on technical nuances. Graph visualizations from our partitioning methods also reveal the differences between correct and wrong predictions. Finally, we demonstrate how the extraction of endorsement cores (EEC) can be used for data pruning, and models trained on the resulting EEC data subset can achieve competitive performance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23949135"
    },
    {
        "id": 7539,
        "title": "Illustrating the significance of explainable artificial intelligence (XAI)",
        "authors": "Pethuru Raj, Chellammal Surianarayanan",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch3"
    },
    {
        "id": 7540,
        "title": "Explanation and optimization of ML for coagulant dosing in Water treatment plant using XAI(eXplainable AI)",
        "authors": "Hongeun Park, Yunhwan Nam, Hohyun Lee, Sungyun Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the water treatment process, determining the appropriate dosing of coagulants is crucial for both ensuring water quality and optimizing operational costs. Traditional methods predominantly rely on heuristic approaches or manual calibrations. However, the actual reactions between various water qualities and chemicals are intricate and nonlinear, often making it challenging to stably secure the targeted water quality. In particular, due to industrialization-induced water pollution and rapid climate change, conventional automated water treatment systems alone are insufficient to adequately respond. This research introduces an innovative approach for clarifying and optimizing the dosing of coagulants in water treatment plants using Explainable Artificial Intelligence (XAI), specifically the SHAP and ICE techniques. These methods allow for a nuanced understanding of feature contributions and individual predictions. Our findings suggest that the XAI-driven approach not only improves the accuracy of coagulant dosing but also offers valuable insights to operators, promoting more informed decision-making. The integration of machine learning and XAI in the water treatment field promises a future of more efficient, transparent, and responsible operations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3973418/v1"
    },
    {
        "id": 7541,
        "title": "A methodology to compare XAI explanations on natural language processing",
        "authors": "Gaëlle Jouis, Harold Mouchère, Fabien Picarougne, Alexandre Hardouin",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-396098-4.00016-8"
    },
    {
        "id": 7542,
        "title": "40 xAI: eXplainable AI",
        "authors": "",
        "published": "2020-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783110629453-040"
    },
    {
        "id": 7543,
        "title": "Explainable AI (XAI) Models Applied to Planning in Financial Markets",
        "authors": "Eric Benhamou, Jean-Jacques Ohana, David Saltiel, Beatrice Guez",
        "published": "No Date",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3862437"
    },
    {
        "id": 7544,
        "title": "Judicial Decision-Making and Explainable AI (XAI) – Insights from the Japanese Judicial System",
        "authors": "Yachiko Yamada",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "The recent development of artificial intelligence (AI) in information technology (IT) is remarkable. These developments have led to claims that AI can be used in courts to replace judges. In the article, the author addresses a matrix of these issues using the concept of explainable AI (XAI). The article examines how regulation can ensure that AI is ethical, and how this ethicality is closely related to (XAI). It concludes that, in the current context, the contribution of AI to the decision-making process is limited by the lack of sufficient explainability and interpretability of AI, although these aspects are adequately addressed and discussed. In addition, it is crucial to consider the impact of AI’s contribution on the legal authority that forms the foundation of the justice system, and a possible approach is suggested to consider conducting an experimental study as AI arbitration.",
        "link": "http://dx.doi.org/10.17951/sil.2023.32.4.157-173"
    },
    {
        "id": 7545,
        "title": "XAI-LCS: Explainable AI-Based Fault Diagnosis of Low-Cost Sensors",
        "authors": "Aparna Sinha, Debanjan Das",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsens.2023.3330046"
    },
    {
        "id": 7546,
        "title": "Knowledge visualization: AI integration with 360-degree dashboards",
        "authors": "K. Sasikala Rani, Chandrasekar Nagarajan",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch10"
    },
    {
        "id": 7547,
        "title": "Explainable AI in Cybersecurity Operations: Lessons Learned from xAI Tool Deployment",
        "authors": "Megan Nyre-Yu, Elizabeth Morris, Michael Smith, Blake Moss, Charles Smutz",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14722/usec.2022.23014"
    },
    {
        "id": 7548,
        "title": "Counterfactual Explanations for XAI Models",
        "authors": "Pradeepta Mishra",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7158-2_10"
    },
    {
        "id": 7549,
        "title": "AI applications - computer vision and natural language processing",
        "authors": "J. Granty Regina Elwin, Vijayalakshmi Karunakaran",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch14"
    },
    {
        "id": 7550,
        "title": "Explainable Artificial Intelligence in Health Care: How XAI Improves User Trust in High-Risk Decisions",
        "authors": "Sheeba Praveen, Kapil Joshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-18292-1_6"
    },
    {
        "id": 7551,
        "title": "Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions",
        "authors": "Luke Guerdan, Alex Raymond, Hatice Gunes",
        "published": "2021-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00423"
    },
    {
        "id": 7552,
        "title": "Stroke Probability Prediction from Medical Survey Data: AI-Driven Analysis with Insightful Feature Importance using Explainable AI (XAI)",
        "authors": "Simon Bin Akter, Sumya Akter, Tanmoy Sarkar Pias",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441480"
    },
    {
        "id": 7553,
        "title": "Explainable AI in Neural Networks Using Shapley Values",
        "authors": "Deepshikha Bhargava, Lav Kumar Gupta",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-1476-8_5"
    },
    {
        "id": 7554,
        "title": "Deceiving Post-Hoc Explainable AI (XAI) Methods in Network Intrusion Detection",
        "authors": "Thulitha Senevirathna, Bartlomiej Siniarski, Madhusanka Liyanage, Shen Wang",
        "published": "2024-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccnc51664.2024.10454633"
    },
    {
        "id": 7555,
        "title": "Constrained Interval Type-2 Fuzzy Classification Systems for Explainable AI (XAI)",
        "authors": "Pasquale D'Alterio, Jonathan M. Garibaldi, Robert I. John",
        "published": "2020-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fuzz48607.2020.9177671"
    },
    {
        "id": 7556,
        "title": "Explainable Artificial Intelligence (XAI) in Biomedicine: Making AI Decisions Trustworthy for Physicians and Patients",
        "authors": "Jörn Lötsch, Dario Kringel, Alfred Ultsch",
        "published": "2021-12-22",
        "citations": 42,
        "abstract": "The use of artificial intelligence (AI) systems in biomedical and clinical settings can disrupt the traditional doctor–patient relationship, which is based on trust and transparency in medical advice and therapeutic decisions. When the diagnosis or selection of a therapy is no longer made solely by the physician, but to a significant extent by a machine using algorithms, decisions become nontransparent. Skill learning is the most common application of machine learning algorithms in clinical decision making. These are a class of very general algorithms (artificial neural networks, classifiers, etc.), which are tuned based on examples to optimize the classification of new, unseen cases. It is pointless to ask for an explanation for a decision. A detailed understanding of the mathematical details of an AI algorithm may be possible for experts in statistics or computer science. However, when it comes to the fate of human beings, this “developer’s explanation” is not sufficient. The concept of explainable AI (XAI) as a solution to this problem is attracting increasing scientific and regulatory interest. This review focuses on the requirement that XAIs must be able to explain in detail the decisions made by the AI to the experts in the field.",
        "link": "http://dx.doi.org/10.3390/biomedinformatics2010001"
    },
    {
        "id": 7557,
        "title": "Statutory Professions in AI Governance and Their Consequences for Explainable AI",
        "authors": "Labhaoise NiFhaolain, Andrew Hines, Vivek Nallur",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44064-9_5"
    },
    {
        "id": 7558,
        "title": "Ethics and the explainable artificial intelligence(XAI) movement",
        "authors": "Greg Adamson",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Since 2016 a significant program of work has been developed under the title of explainable artificial intelligence (XAI). This program, prompted and extensively funded by DARPA, has sought to address the reasoning behind decisions or recommendations of AI systems. As AI systems often have concealed or <em>black box</em> characteristics, the problem of explainability is a significant challenge. XAI has been described as a movement rather than a single technology approach. Many thousands of papers have examined the problem, and diverse approaches have been put forward. One approach encouraged by DARPA has since become known as post-hoc reasoning, applying inductive reasoning. This research examines the claim to accuracy of post-hoc explanations, applying the perspective of the philosophy of technologyAs AI systems are already being used to determine who should have access to scarce resources and who should be punished and in what way, the accuracy of an explanation is an important ethical issue. . This paper asserts that technologists as experts in AI technology hold a unique ethical responsibility to clarify to a wider audience the limitations of knowledge about the workings of black box AI systems, and to avoid narratives that encourage uncritical acceptance of technology promise. The paper also proposes practical ways in which to approach the use of post-hoc reasoning where appropriate. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20439192.v1"
    },
    {
        "id": 7559,
        "title": "Explainable Artificial Intelligence (XAI) for Knowledge Management (KM)",
        "authors": "Soumi Majumder, Nilanjan Dey",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0316-8_6"
    },
    {
        "id": 7560,
        "title": "An overview of AI platforms, frameworks, libraries, and processes",
        "authors": "Sruthi Anand, Ram Gurusamy Raja, T. Sheela",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch6"
    },
    {
        "id": 7561,
        "title": "Justifying Arabic Text Sentiment Analysis Using Explainable AI (XAI): LASIK Surgeries Case Study",
        "authors": "Youmna Abdelwahab, Mohamed Kholief, Ahmed Ahmed Hesham Sedky",
        "published": "2022-11-11",
        "citations": 4,
        "abstract": "With the increasing use of machine learning across various fields to address several aims and goals, the complexity of the ML and Deep Learning (DL) approaches used to provide solutions has also increased. In the last few years, Explainable AI (XAI) methods to further justify and interpret deep learning models have been introduced across several domains and fields. While most papers have applied XAI to English and other Latin-based languages, this paper aims to explain attention-based long short-term memory (LSTM) results across Arabic Sentiment Analysis (ASA), which is considered an uncharted area in previous research. With the use of Local Interpretable Model-agnostic Explanation (LIME), we intend to further justify and demonstrate how the LSTM leads to the prediction of sentiment polarity within ASA in domain-specific Arabic texts regarding medical insights on LASIK surgery across Twitter users. In our research, the LSTM reached an accuracy of 79.1% on the proposed data set. Throughout the representation of sentiments using LIME, it demonstrated accurate results regarding how specific words contributed to the overall sentiment polarity classification. Furthermore, we compared the word count with the probability weights given across the examples, in order to further validate the LIME results in the context of ASA.",
        "link": "http://dx.doi.org/10.3390/info13110536"
    },
    {
        "id": 7562,
        "title": "xAI: An Explainable AI Model for the Diagnosis of COPD from CXR Images",
        "authors": "Agughasi Victor Ikechukwu, S. Murali",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdds59137.2023.10434619"
    },
    {
        "id": 7563,
        "title": "An explainable AI (XAI) model for landslide susceptibility modeling",
        "authors": "Biswajeet Pradhan, Abhirup Dikshit, Saro Lee, Hyesu Kim",
        "published": "2023-7",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.110324"
    },
    {
        "id": 7564,
        "title": "Explainable Ai (Xai) for Constructing a Lexicon for Classifying Green Energy Jobs: A Comparative Analysis of Occupation, Industry and Location Composition with Traditional Energy Jobs",
        "authors": "Haohui Chen, Claire Mason",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4565898"
    },
    {
        "id": 7565,
        "title": "Explainable artificial intelligence (XAI) post-hoc explainability methods: risks and limitations in non-discrimination law",
        "authors": "Daniel Vale, Ali El-Sharif, Muhammed Ali",
        "published": "2022-11",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s43681-022-00142-y"
    },
    {
        "id": 7566,
        "title": "Explainable Artificial Intelligence for Predictive Analytics on Customer Turnover: A User-Friendly Interface for Non-expert Users",
        "authors": "Joglas Souza, Carson K. Leung",
        "published": "2021",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8_4"
    },
    {
        "id": 7567,
        "title": "Toward explainable AI (XAI) for mental health detection based on language behavior",
        "authors": "Elma Kerz, Sourabh Zanwar, Yu Qiao, Daniel Wiechmann",
        "published": "2023-12-7",
        "citations": 1,
        "abstract": "Advances in artificial intelligence (AI) in general and Natural Language Processing (NLP) in particular are paving the new way forward for the automated detection and prediction of mental health disorders among the population. Recent research in this area has prioritized predictive accuracy over model interpretability by relying on deep learning methods. However, prioritizing predictive accuracy over model interpretability can result in a lack of transparency in the decision-making process, which is critical in sensitive applications such as healthcare. There is thus a growing need for explainable AI (XAI) approaches to psychiatric diagnosis and prediction. The main aim of this work is to address a gap by conducting a systematic investigation of XAI approaches in the realm of automatic detection of mental disorders from language behavior leveraging textual data from social media. In pursuit of this aim, we perform extensive experiments to evaluate the balance between accuracy and interpretability across predictive mental health models. More specifically, we build BiLSTM models trained on a comprehensive set of human-interpretable features, encompassing syntactic complexity, lexical sophistication, readability, cohesion, stylistics, as well as topics and sentiment/emotions derived from lexicon-based dictionaries to capture multiple dimensions of language production. We conduct extensive feature ablation experiments to determine the most informative feature groups associated with specific mental health conditions. We juxtapose the performance of these models against a “black-box” domain-specific pretrained transformer adapted for mental health applications. To enhance the interpretability of the transformers models, we utilize a multi-task fusion learning framework infusing information from two relevant domains (emotion and personality traits). Moreover, we employ two distinct explanation techniques: the local interpretable model-agnostic explanations (LIME) method and a model-specific self-explaining method (AGRAD). These methods allow us to discern the specific categories of words that the information-infused models rely on when generating predictions. Our proposed approaches are evaluated on two public English benchmark datasets, subsuming five mental health conditions (attention-deficit/hyperactivity disorder, anxiety, bipolar disorder, depression and psychological stress).",
        "link": "http://dx.doi.org/10.3389/fpsyt.2023.1219479"
    },
    {
        "id": 7568,
        "title": "Designing accessible, explainable AI (XAI) experiences",
        "authors": "Christine T. Wolf, Kathryn E. Ringland",
        "published": "2020-3-2",
        "citations": 10,
        "abstract": "Explainable Artificial Intelligence (XAI) has taken off in recent years, a field that develops techniques to render complex AI and machine learning (ML) models comprehensible to humans. Despite the growth of XAI techniques, we know little about the challenges of leveraging such explainability capabilities in situated settings of use. In this article, we discuss some particular issues around the intersection between accessibility and XAI. We outline two primary concerns: one, accessibility at the interface; and two, tailoring explanations to individuals' diverse and changing explainability needs. We illustrate these issues by discussing two application areas for AI/Ml systems (aging-in-place and mental health support) and discuss how issues arise at the nexus between explainability and accessibility.",
        "link": "http://dx.doi.org/10.1145/3386296.3386302"
    },
    {
        "id": 7569,
        "title": "Unlocking the Black Box: Explainable Artificial Intelligence (XAI) for Trust and Transparency in AI Systems",
        "authors": "Nipuna Thalpage",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical field in AI research, addressing the lack of transparency and interpretability in complex AI models. This conceptual review explores the significance of XAI in promoting trust and transparency in AI systems. The paper analyzes existing literature on XAI, identifies patterns and gaps, and presents a coherent conceptual framework. Various XAI techniques, such as saliency maps, attention mechanisms, rule-based explanations, and model-agnostic approaches, are discussed to enhance interpretability. The paper highlights the challenges posed by black-box AI models, explores the role of XAI in enhancing trust and transparency, and examines the ethical considerations and responsible deployment of XAI. By promoting transparency and interpretability, this review aims to build trust, encourage accountable AI systems, and contribute to the ongoing discourse on XAI.",
        "link": "http://dx.doi.org/10.33847/2712-8148.4.1_4"
    },
    {
        "id": 7570,
        "title": "Explainable Artificial Intelligence (XAI) in Manufacturing",
        "authors": "Tin-Chih Toly Chen",
        "published": "2023",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-27961-4_1"
    },
    {
        "id": 7571,
        "title": "Ethics and the explainable artificial intelligence(XAI) movement",
        "authors": "Greg Adamson",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Since 2016 a significant program of work has been developed under the title of explainable artificial intelligence (XAI). This program, prompted and extensively funded by DARPA, has sought to address the reasoning behind decisions or recommendations of AI systems. As AI systems often have concealed or <em>black box</em> characteristics, the problem of explainability is a significant challenge. XAI has been described as a movement rather than a single technology approach. Many thousands of papers have examined the problem, and diverse approaches have been put forward. One approach encouraged by DARPA has since become known as post-hoc reasoning, applying inductive reasoning. This research examines the claim to accuracy of post-hoc explanations, applying the perspective of the philosophy of technologyAs AI systems are already being used to determine who should have access to scarce resources and who should be punished and in what way, the accuracy of an explanation is an important ethical issue. . This paper asserts that technologists as experts in AI technology hold a unique ethical responsibility to clarify to a wider audience the limitations of knowledge about the workings of black box AI systems, and to avoid narratives that encourage uncritical acceptance of technology promise. The paper also proposes practical ways in which to approach the use of post-hoc reasoning where appropriate. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20439192"
    },
    {
        "id": 7572,
        "title": "XAI-Fall: Explainable AI for Fall Detection on Wearable Devices Using Sequence Models and XAI Techniques",
        "authors": "Harsh Mankodiya, Dhairya Jadav, Rajesh Gupta, Sudeep Tanwar, Abdullah Alharbi, Amr Tolba, Bogdan-Constantin Neagu, Maria Simona Raboaca",
        "published": "2022-6-9",
        "citations": 9,
        "abstract": "A fall detection system is vital for the safety of older people, as it contacts emergency services when it detects a person has fallen. There have been various approaches to detect falls, such as using a single tri-axial accelerometer to detect falls or fixing sensors on the walls of a room to detect falls in a particular area. These approaches have two major drawbacks: either (i) they use a single sensor, which is insufficient to detect falls, or (ii) they are attached to a wall that does not detect a person falling outside its region. Hence, to provide a robust method for detecting falls, the proposed approach uses three different sensors for fall detection, which are placed at five different locations on the subject’s body to gather the data used for training purposes. The UMAFall dataset is used to attain sensor readings to train the models for fall detection. Five models are trained corresponding to the five sensors models, and a majority voting classifier is used to determine the output. Accuracy of 93.5%, 93.5%, 97.2%, 94.6%, and 93.1% is achieved on each of the five sensors models, and 92.54% is the overall accuracy achieved by the majority voting classifier. The XAI technique called LIME is incorporated into the system in order to explain the model’s outputs and improve the model’s interpretability.",
        "link": "http://dx.doi.org/10.3390/math10121990"
    },
    {
        "id": 7573,
        "title": "Clinical Evaluation Framework Using Behavioural &amp;amp; Visual Attention Read-Outs for Explainable AI (XAI)",
        "authors": "Myura Nagendran, Paul  Gérard René Emile Festor, Matthieu Komorowski, Anthony  C. Gordon, Aldo  AA Faisal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496127"
    },
    {
        "id": 7574,
        "title": "XAI Handbook: Towards a Unified Framework for Explainable AI",
        "authors": "Sebastian Palacio, Adriano Lucieri, Mohsin Munir, Sheraz Ahmed, Jorn Hees, Andreas Dengel",
        "published": "2021-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00420"
    },
    {
        "id": 7575,
        "title": "Explainable AI (XAI) in Healthcare - Tools and Regulations",
        "authors": "Paweł Raif, Renata Suchanek-Raif, Ewaryst Tkacz",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf58974.2023.10405096"
    },
    {
        "id": 7576,
        "title": "Explainable Artificial Intelligence (XAI) techniques for energy and power systems: Review, challenges and opportunities",
        "authors": "R. Machlev, L. Heistrene, M. Perl, K.Y. Levy, J. Belikov, S. Mannor, Y. Levron",
        "published": "2022-8",
        "citations": 101,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egyai.2022.100169"
    },
    {
        "id": 7577,
        "title": "Explainable Artificial Intelligence (XAI) in Healthcare",
        "authors": "Tim Hulsen",
        "published": "No Date",
        "citations": 1,
        "abstract": "Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors to make better decisions (‘clinical decision support’), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a ‘black box’, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at the current status of XAI in healthcare, describe several issues around XAI, and discuss whether it can really help healthcare to advance, for example by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.",
        "link": "http://dx.doi.org/10.20944/preprints202303.0116.v1"
    },
    {
        "id": 7578,
        "title": "Methods for explainable artificial intelligence",
        "authors": "Sajid Ali",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch8"
    },
    {
        "id": 7579,
        "title": "Explainable AI, But Explainable to Whom? An Exploratory Case Study of xAI in Healthcare",
        "authors": "Julie Gerlings, Millie Søndergaard Jensen, Arisa Shollo",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83620-7_7"
    },
    {
        "id": 7580,
        "title": "Agnostic eXplainable Artificial Intelligence (XAI) Method Based on Volterra Series",
        "authors": "Jhonatan Contreras, Thomas Bocklitz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011889700003411"
    },
    {
        "id": 7581,
        "title": "Factor analysis of evacuation behavior during heavy rain using eXplainable AI(XAI)",
        "authors": "Michirou Tsukamoto, Akiyoshi Takagi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11361/cpijchubu.33.0_63"
    },
    {
        "id": 7582,
        "title": "Using Explainable AI (XAI) for the Prediction of Falls in the Older Population",
        "authors": "Yue Ting Tang, Roman Romero-Ortuno",
        "published": "2022-9-28",
        "citations": 2,
        "abstract": "The prevention of falls in older people requires the identification of the most important risk factors. Frailty is associated with risk of falls, but not all falls are of the same nature. In this work, we utilised data from The Irish Longitudinal Study on Ageing to implement Random Forests and Explainable Artificial Intelligence (XAI) techniques for the prediction of different types of falls and analysed their contributory factors using 46 input features that included those of a previously investigated frailty index. Data of participants aged 65 years and older were fed into four random forest models (all falls or syncope, simple fall, complex fall, and syncope). Feature importance rankings were based on mean decrease in impurity, and Shapley additive explanations values were calculated and visualised. Female sex and a previous fall were found to be of high importance in all of the models, and polypharmacy (being on five or more regular medications) was ranked high in the syncope model. The more ‘accidental’ (extrinsic) nature of simple falls was demonstrated in its model, where the presence of many frailty features had negative model contributions. Our results highlight that falls in older people are heterogenous and XAI can provide new insights to help their prevention.",
        "link": "http://dx.doi.org/10.3390/a15100353"
    },
    {
        "id": 7583,
        "title": "A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",
        "authors": "Erico Tjoa, Cuntai Guan",
        "published": "2021-11",
        "citations": 673,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3027314"
    },
    {
        "id": 7584,
        "title": "XAI-3DP: Diagnosis and Understanding Faults of 3-D Printer With Explainable Ensemble AI",
        "authors": "Deepraj Chowdhury, Aparna Sinha, Debanjan Das",
        "published": "2023-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsens.2022.3228327"
    },
    {
        "id": 7585,
        "title": "Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities",
        "authors": "Waddah Saeed, Christian Omlin",
        "published": "2023-3",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110273"
    },
    {
        "id": 7586,
        "title": "Feature Importance versus Feature Influence and What It Signifies for Explainable AI",
        "authors": "Kary Främling",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44064-9_14"
    },
    {
        "id": 7587,
        "title": "Interpretable and explainable AI (XAI) model for spatial drought prediction",
        "authors": "Abhirup Dikshit, Biswajeet Pradhan",
        "published": "2021-12",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.scitotenv.2021.149797"
    },
    {
        "id": 7588,
        "title": "Explainable AI (XAI) Applied in Machine Learning for Pain Modeling: A Review",
        "authors": "Ravichandra Madanu, Maysam F. Abbod, Fu-Jung Hsiao, Wei-Ta Chen, Jiann-Shing Shieh",
        "published": "2022-6-14",
        "citations": 9,
        "abstract": "Pain is a complex term that describes various sensations that create discomfort in various ways or types inside the human body. Generally, pain has consequences that range from mild to severe in different organs of the body and will depend on the way it is caused, which could be an injury, illness or medical procedures including testing, surgeries or therapies, etc. With recent advances in artificial-intelligence (AI) systems associated in biomedical and healthcare settings, the contiguity of physician, clinician and patient has shortened. AI, however, has more scope to interpret the pain associated in patients with various conditions by using any physiological or behavioral changes. Facial expressions are considered to give much information that relates with emotions and pain, so clinicians consider these changes with high importance for assessing pain. This has been achieved in recent times with different machine-learning and deep-learning models. To accentuate the future scope and importance of AI in medical field, this study reviews the explainable AI (XAI) as increased attention is given to an automatic assessment of pain. This review discusses how these approaches are applied for different pain types.",
        "link": "http://dx.doi.org/10.3390/technologies10030074"
    },
    {
        "id": 7589,
        "title": "Critical Thinking About Explainable AI (XAI) for Rule-Based Fuzzy Systems",
        "authors": "Jerry M. Mendel, Piero P. Bonissone",
        "published": "2021-12",
        "citations": 36,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tfuzz.2021.3079503"
    },
    {
        "id": 7590,
        "title": "Advancements of XAI in healthcare sector",
        "authors": "Gudi Varaprasad, J. Sheela",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpc062e_ch19"
    },
    {
        "id": 7591,
        "title": "XAI-AMD-DL: An Explainable AI Approach for Android Malware Detection System Using Deep Learning",
        "authors": "Santosh K. Smmarwar, Govind P. Gupta, Sanjay Kumar",
        "published": "2023-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aic57670.2023.10263974"
    },
    {
        "id": 7592,
        "title": "Explainable AI (XAI) in Rules as Code (RaC): The DataLex approach",
        "authors": "Andrew Mowbray, Philip Chung, Graham Greenleaf",
        "published": "2023-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.clsr.2022.105771"
    },
    {
        "id": 7593,
        "title": "Understanding Interpretability: Explainable AI Approaches for Hate Speech Classifiers",
        "authors": "Sargam Yadav, Abhishek Kaushik, Kevin McDaid",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44070-0_3"
    },
    {
        "id": 7594,
        "title": "Classification of Kinematic Data Using Explainable Artificial Intelligence (XAI) for Smart Motion",
        "authors": "C.Y. Yong",
        "published": "2021-9-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003172772-10"
    },
    {
        "id": 7595,
        "title": "Neural Network Modeling based XAI of Activated Sludge Process in Wastewater Treatment System for Dissolved Oxygen Control",
        "authors": "Eui-Seok Nahm",
        "published": "2022-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5370/kiee.2022.71.8.1176"
    },
    {
        "id": 7596,
        "title": "Explainable AI (XAI) for Agriculture",
        "authors": "Eudes Smith M. Linheiro, Gitanjali R. Shinde, Parikshit N. Mahalle, Riddhi Mirajkar",
        "published": "2023-12-25",
        "citations": 0,
        "abstract": "&nbsp;In most nations, agriculture is the main industry providing employment.\nAgricultural activities used to be restricted to the cultivation of food and crops, but they\nhave expanded over time to include the processing, production, marketing, and\ndistribution of crops and livestock products. Agriculture related approaches or practices\nmust be continuously reviewed with the goal of presenting innovative approaches to\nsustaining and improving agricultural activities. Currently, agricultural activities serve\nas the primary source of livelihood, increasing GDP, being one of the sources of\nnational trade, reducing unemployment, and providing raw materials for production in\nother industries.\nInadequate soil treatment, disease and pest infestation, among other issues, are only a\nfew of the difficulties this industry must overcome in order to maximize productivity.\nThere have been some difficulties with the increased use of technology in this industry,\nincluding the need for large amounts of data, low output, and the most obvious\ndifficulty, the knowledge gap between farmers and technology.\nWhen compared to earlier more conventional methods, agricultural practices, and\nactivities have significantly improved since technology entered the field. Technologies\nlike the Internet of Things (IoT) and Artificial Intelligence (AI) have been a few of the\ntechnologies that are widely used in these sectors with projects for improving crop\nproduction, disease prediction, continuous monitoring, efficient supply chain\nmanagement, water waste and operational efficiency just to name a few but, this of this\nproject will focus more on AI, more specifically on Explainable Artificial Intelligence\n(ExAI or XAI).",
        "link": "http://dx.doi.org/10.2174/9789815179187123040014"
    },
    {
        "id": 7597,
        "title": "Science of Data: A New Ladder for Causation",
        "authors": "Usef Faghihi, Sioui Maldonado Bouchard, Ismail Biskri",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8_3"
    },
    {
        "id": 7598,
        "title": "On the Transparent Predictive Models for Ecological Momentary Assessment Data",
        "authors": "Kirill I. Tumanov, Gerasimos Spanakis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-76409-8_6"
    },
    {
        "id": 7599,
        "title": "Propaganda Detection Robustness Through Adversarial Attacks Driven by eXplainable AI",
        "authors": "Danilo Cavaliere, Mariacristina Gallo, Claudio Stanzione",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44067-0_21"
    },
    {
        "id": 7600,
        "title": "XAI-FR: Explainable AI-Based Face Recognition Using Deep Neural Networks",
        "authors": "Ankit Rajpal, Khushwant Sehra, Rashika Bagri, Pooja Sikka",
        "published": "2023-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11277-022-10127-z"
    }
]