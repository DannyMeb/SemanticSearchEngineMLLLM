[
    {
        "id": 9960,
        "title": "Rethinking Attention Mechanism: Channel Re-Attention and Spatial Multi-Region Attention for Fine-Grained Visual Classification",
        "authors": "Kun Wang, Wenzong Jiang, Yanjiang Wang, Weifeng Liu, Baodi Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4757424"
    },
    {
        "id": 9961,
        "title": "Semantic Segmentation using Light Attention Mechanism",
        "authors": "Yuki Hiramatsu, Kazuhiro Hotta",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009347206220625"
    },
    {
        "id": 9962,
        "title": "Curvature-Informed Attention Mechanism for Long Short-Term Memory Networks",
        "authors": "Lynda Ayachi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012463500003636"
    },
    {
        "id": 9963,
        "title": "Attention Constraint Mechanism through Auxiliary Attention",
        "authors": "Yingda Fan, Amrinder Arora",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai56018.2022.00022"
    },
    {
        "id": 9964,
        "title": "Combining Transformer and Reverse Attention Mechanism for Polyp Segmentation",
        "authors": "Jianzhuang Lin, Wenzhong Yang, Sixiang Tan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012014800003633"
    },
    {
        "id": 9965,
        "title": "Generalized Attention Mechanism and Relative Position for Transformer",
        "authors": "Raja Vikram Pandya",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2476"
    },
    {
        "id": 9966,
        "title": "Enet Semantic Segmentation Combined with Attention Mechanism",
        "authors": "Wei Bai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nImage semantic segmentation is one of the core tasks of computer vision. It is widely used in fields such as unmanned driving, medical image processing, geographic information systems and intelligent robots. Aiming at the problem that the existing semantic segmentation algorithm ignores the different channel and location features of the feature map and the simple method when the feature map is fused, this paper designs a semantic segmentation algorithm that combines the attention mechanism. Firstly, dilated convolution is used, and a smaller downsampling factor is used to maintain the resolution of the image and obtain the detailed information of the image. Secondly, the attention mechanism module is introduced to assign weights to different parts of the feature map, which reduces the accuracy loss. The design feature fusion module assigns weights to the feature maps of different receptive fields obtained by the two paths, and merges them together to obtain the final segmentation result. Finally, through experiments, it was verified on the Camvid, Cityscapes and PASCAL VOC2012 datasets. Mean intersection over union (MIoU) and mean pixel accuracy (MPA) are used as metrics. The method in this paper can make up for the loss of accuracy caused by downsampling while ensuring the receptive field and improving the resolution, which can better guide the model learning. And the proposed feature fusion module can better integrate the features of different receptive fields. Therefore, the proposed method can significantly improve the segmentation performance compared to the traditional method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-425438/v1"
    },
    {
        "id": 9967,
        "title": "Direct Coupling Analysis and the Attention Mechanism",
        "authors": "Francesco Caredda, Andrea Pagnani",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProteins serve as the foundation for nearly all biological functions within cells, encompassing roles in transport, signaling, enzymatic activity, and more. Their functionalities hinge significantly on their intricate three-dimensional structures, often posing challenges in terms of difficulty, time, and expense for accurate determination. The introduction of AlphaFold 2 marked a groundbreaking solution to the enduring challenge of predicting a protein’s tertiary structure from its amino acid sequence. However, the inherent complexity of AlphaFold’s architecture presents obstacles in deciphering its learning process and understanding the decision-making that ultimately shapes the protein’s final structure.In this study, we introduce a shallow, unsupervised model designed to understand the selfattention layer within the Evoformer block of AlphaFold. We establish a method based on Direct Coupling Analysis (DCA), wherein the interaction tensor undergoes decomposition, leveraging the same structure employed in Transformer architectures. The model’s parameters, notably fewer than those in standard DCA, are interpretable through an examination of the resulting attention matrices. These matrices enable the extraction of contact information, subsequently utilized for constructing the contact map of a protein family. Additionally, the self-attention decomposition in the DCA Hamiltonian form adopted here facilitates the definition of multifamily learning architecture, enabling the inference of parameter sets shared across diverse protein families. Finally, an autoregressive generative version of the model is implemented, capable of efficiently generating new proteins in silico. This generative model reproduces the summary statistics of the original protein family while concurrently inferring direct contacts in the tertiary structure of the protein. The effectiveness of our Attention-Based DCA architecture is evaluated using Multiple Sequence Alignments (MSAs) of varying lengths and depths, with structural data sourced from the Pfam database.",
        "link": "http://dx.doi.org/10.1101/2024.02.06.579080"
    },
    {
        "id": 9968,
        "title": "Visible-infrared image matching based on parameter-free attention mechanism and target-aware graph attention mechanism",
        "authors": "Wuxin Li, Qian Chen, Guohua Gu, Xiubao Sui",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122038"
    },
    {
        "id": 9969,
        "title": "Speaker Adaptive Training for Speech Recognition Based on Attention-Over-Attention Mechanism",
        "authors": "Genshun Wan, Jia Pan, Qingran Wang, Jianqing Gao, Zhongfu Ye",
        "published": "2020-10-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1727"
    },
    {
        "id": 9970,
        "title": "Multi-view Stereo 3D Reconstruction Based on Attention Mechanism",
        "authors": "Wenjing Bian, Dongren Liu, Lifeng Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2023.038"
    },
    {
        "id": 9971,
        "title": "Personalized Recommendation of Children's Literature Based on Neural Attention Mechanism",
        "authors": "Lina Si",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4335511"
    },
    {
        "id": 9972,
        "title": "Improving Attention Allocation on the Web Using a Market Mechanism",
        "authors": "Sandeep Jaykumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "This project aims to examine the ways that attention allocation on the Web can be improved by using a market mechanism.",
        "link": "http://dx.doi.org/10.35543/osf.io/r3up9"
    },
    {
        "id": 9973,
        "title": "5 Attention: Mechanism and Virtue",
        "authors": "Carlos Montemayor",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7312/burn21118-006"
    },
    {
        "id": 9974,
        "title": "Stvanet: A Spatio-Temporal Visual Attention Framework with Large Kernel Attention Mechanism for Citywide Traffic Dynamics Prediction",
        "authors": "Hongtai Yang, Junbo Jiang, Zhan Zhao, Renbin Pan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673691"
    },
    {
        "id": 9975,
        "title": "Underwater image imbalance attenuation compensation based on attention and self-attention mechanism",
        "authors": "Danxu Wang, Yanhui Wei, Junnan Liu, Wenjia Ouyang, Xilin Zhou",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9977186"
    },
    {
        "id": 9976,
        "title": "Image classification model based on large kernel attention mechanism and relative position self-attention mechanism",
        "authors": "Siqi Liu, Jiangshu Wei, Gang Liu, Bei Zhou",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "The Transformer has achieved great success in many computer vision tasks. With the in-depth exploration of it, researchers have found that Transformers can better obtain long-range features than convolutional neural networks (CNN). However, there will be a deterioration of local feature details when the Transformer extracts local features. Although CNN is adept at capturing the local feature details, it cannot easily obtain the global representation of features. In order to solve the above problems effectively, this paper proposes a hybrid model consisting of CNN and Transformer inspired by Visual Attention Net (VAN) and CoAtNet. This model optimizes its shortcomings in the difficulty of capturing the global representation of features by introducing Large Kernel Attention (LKA) in CNN while using the Transformer blocks with relative position self-attention variant to alleviate the problem of detail deterioration in local features of the Transformer. Our model effectively combines the advantages of the above two structures to obtain the details of local features more accurately and capture the relationship between features far apart more efficiently on a large receptive field. Our experiments show that in the image classification task without additional training data, the proposed model in this paper can achieve excellent results on the cifar10 dataset, the cifar100 dataset, and the birds400 dataset (a public dataset on the Kaggle platform) with fewer model parameters. Among them, SE_LKACAT achieved a Top-1 accuracy of 98.01% on the cifar10 dataset with only 7.5M parameters.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1344"
    },
    {
        "id": 9977,
        "title": "Smart Contract Vulnerability Detection Using Word Vector Attention Mechanism",
        "authors": "Yanxia Dui, Hongchun Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4748509"
    },
    {
        "id": 9978,
        "title": "Prediction of solar irradiance using convolutional neural network and attention mechanism-based long short-term memory network based on similar day analysis and an attention mechanism",
        "authors": "Xinxing Hou, Chao Ju, Bo Wang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2023.e21484"
    },
    {
        "id": 9979,
        "title": "Optimize YoloStereo3D Binocular Detection Network by Attention Mechanism",
        "authors": "Yuliang Gao, Zhen Li, Aoran Xi, Yuting Wang, Lifeng Zhang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2022.020"
    },
    {
        "id": 9980,
        "title": "Convolution pyramid attention: an efficient channel attention mechanism",
        "authors": "Ruitong Wang, Xiangju Jiang",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3021302"
    },
    {
        "id": 9981,
        "title": "Human trajectory prediction using LSTM with Attention mechanism",
        "authors": "Amin manafi, samaneh hosseini semnani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, we propose a human trajectory prediction model that combines a Long Short-Term Memory (LSTM) network with an attention mechanism. To do that, we use attention scores to determine which parts of the input data the model should focus on when making predictions. Attention scores are calculated for each input feature, with a higher score indicating the greater significance of that feature in predicting the output. Initially, these scores are determined for the target human’s position, velocity, and their neighboring individuals’ positions and velocities. By using attention scores, our model can prioritize the most relevant information in the input data and make more accurate predictions. We extract attention scores from our attention mechanism and integrate them into the trajectory prediction module to predict humans’ future trajectories. To achieve this, we introduce a new neural layer that processes attention scores after extracting them and concatenates them with positional information. We evaluate our approach on the publicly available ETH and UCY datasets and measure its performance using the final displacement error (FDE) and average displacement error (ADE) metrics. We show that our modified algorithm performs better than the Social LSTM in predicting the future trajectory of pedestrians in crowded spaces. Specifically, our model achieves an improvement of 6.2% in ADE and 6.3% in FDE compared to the Social LSTM results in the literature.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3974731/v1"
    },
    {
        "id": 9982,
        "title": "Peer Review #2 of \"Improved YOLOv4-tiny based on attention mechanism for skin detection (v0.1)\"",
        "authors": "",
        "published": "2023-3-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1288v0.1/reviews/2"
    },
    {
        "id": 9983,
        "title": "Peer Review #1 of \"Improved YOLOv4-tiny based on attention mechanism for skin detection (v0.1)\"",
        "authors": "",
        "published": "2023-3-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1288v0.1/reviews/1"
    },
    {
        "id": 9984,
        "title": "Peer Review #2 of \"Improved YOLOv4-tiny based on attention mechanism for skin detection (v0.3)\"",
        "authors": "",
        "published": "2023-3-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1288v0.3/reviews/2"
    },
    {
        "id": 9985,
        "title": "Peer Review #2 of \"Improved YOLOv4-tiny based on attention mechanism for skin detection (v0.2)\"",
        "authors": "",
        "published": "2023-3-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1288v0.2/reviews/2"
    },
    {
        "id": 9986,
        "title": "PM2.5 prediction based on attention mechanism and Bi-LSTM",
        "authors": "Xin Huang, Zuhan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study enhances the Bi-LSTM model by incorporating an attention mechanism, which could provide the model with stronger data generalization capabilities. Moreover, it can predict a broader range of data and exhibits enhanced handling and adaptability to anomalies. Through the utilization of the attention mechanism, this research partitions the weights of the feature values, precisely dividing the input LSTM's feature values based on their weights. This enables the Bi-LSTM to more accurately capture relationships between different feature values in time series and dependencies on various features. Given the diverse air quality conditions in different regions, the introduced attention mechanism in Bi-LSTM manages the weights of different feature values. The Bi-LSTM, enhanced with attention mechanisms, excels at handling relationships in time series data, allowing it to predict PM2.5 values in more complex air quality environments. It demonstrates improved capabilities in handling anomalies. Even in air quality scenarios with various complex conditions, the model maintains satisfactory predictive quality.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3763888/v1"
    },
    {
        "id": 9987,
        "title": "Easy attention: A simple self-attention mechanism for transformer-based time-series reconstruction and prediction",
        "authors": "Ricardo Vinuesa, Marcial Sanchis-Agudo, Yuning Wang, Luca Guastoni, Karthik Duraisamy",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention which we\ndemonstrate in time-series reconstruction and prediction. As a consequence of the fact that self attention only makes use of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture\nlong-term dependencies in temporal sequences. Through the singular-value decomposition (SVD)\non the softmax attention score, we further observe that self attention compresses the contributions\nfrom both queries and keys in the spanned space of the attention score. Therefore, our proposed\neasy-attention method directly treats the attention scores as learnable parameters. This approach\nproduces excellent results when reconstructing and predicting the temporal dynamics of chaotic\nsystems exhibiting more robustness and less complexity than self attention or the widely-used long\nshort-term memory (LSTM) network. Our results show great potential for applications in more\ncomplex high-dimensional dynamical systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3545247/v1"
    },
    {
        "id": 9988,
        "title": "Recovery of underwater images based on the attention mechanism and SOS mechanism",
        "authors": "",
        "published": "2022-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.08.005"
    },
    {
        "id": 9989,
        "title": "Attention Branch Network: Learning of Attention Mechanism for Visual Explanation",
        "authors": "Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2019-6",
        "citations": 280,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.01096"
    },
    {
        "id": 9990,
        "title": "A Regularized Attention Mechanism for Graph Attention Networks",
        "authors": "Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan, Andreas Spanias",
        "published": "2020-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9054363"
    },
    {
        "id": 9991,
        "title": "Extended Attention Mechanism for TSP Problem",
        "authors": "Hua Yang",
        "published": "2021-7-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533472"
    },
    {
        "id": 9992,
        "title": "Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey",
        "authors": "Benyamin Ghojogh, Ali Ghodsi",
        "published": "No Date",
        "citations": 14,
        "abstract": "This is a tutorial and survey paper on the attention mechanism, transformers, BERT, and GPT. We first explain attention mechanism, sequence-to-sequence model without and with attention, self-attention, and attention in different areas such as natural language processing and computer vision. Then, we explain transformers which do not use any recurrence. We explain all the parts of encoder and decoder in the transformer, including positional encoding, multihead self-attention and cross-attention, and masked multihead attention. Thereafter, we introduce the Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT) as the stacks of encoders and decoders of transformer, respectively. We explain their characteristics and how they work.",
        "link": "http://dx.doi.org/10.31219/osf.io/m6gcn"
    },
    {
        "id": 9993,
        "title": "Object tracking based on spatial attention mechanism",
        "authors": "Yu Xie, Ying Chen",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866530"
    },
    {
        "id": 9994,
        "title": "Pay Attention to Virality: Understanding Popularity of Social Media Videos with the Attention Mechanism",
        "authors": "Adam Bielski, Tomasz Trzcinski",
        "published": "2018-6",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw.2018.00309"
    },
    {
        "id": 9995,
        "title": "Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation",
        "authors": "Ehsan Khodapanah Aghdam, Reza Azad, Maral Zarvani, Dorit Merhof",
        "published": "2023-4-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230337"
    },
    {
        "id": 9996,
        "title": "Classification of flower image based on attention mechanism and multi-loss attention network",
        "authors": "Mei Zhang, Huihui Su, Jinghua Wen",
        "published": "2021-11",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.comcom.2021.09.001"
    },
    {
        "id": 9997,
        "title": "Review for \"Combination of deep neural network with attention mechanism enhances the explainability of protein contact prediction\"",
        "authors": "",
        "published": "2020-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26052/v1/review1"
    },
    {
        "id": 9998,
        "title": "Review for \"Abnormal state prediction of flotation process based on dual attention mechanism and multivariate information fusion\"",
        "authors": "",
        "published": "2024-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25225/v2/review2"
    },
    {
        "id": 9999,
        "title": "OANet: Ortho-Attention Net Based on Attention Mechanism for Database Performance Prediction",
        "authors": "Chanho Yeom, Jieun Lee, Sanghyun Park",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/jok.2022.49.11.1026"
    },
    {
        "id": 10000,
        "title": "Effective Piecewise CNN with Attention Mechanism for Distant Supervision on Relation Extraction Task",
        "authors": "Yuming Li, Pin Ni, Gangmin Li, Victor Chang",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009582700530060"
    },
    {
        "id": 10001,
        "title": "Knowledge Graph Embedding based on Line Graph Attention Mechanism",
        "authors": "Bo Liu, Jiahui Wu, Enju Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe broad application prospects of knowledge graphs have spawned many research work in the field of knowledge graph completion. In these works, models based on convolution neural networks perform well on link prediction tasks by generating expressive feature embedding. Some models based on graph neural networks also obtain good knowledge graph embedding results by capturing graph structure features and node features in the knowledge graph. However, these models often rely more on the entity features in the triples and ignore the importance of the relationship features, so they cannot guarantee the generalization ability on the triples containing unseen entities. To this end, we propose a method of learning Knowledge Graph embedding on Line Graphs using Graph Attention mechanism (KG-LGAT). By transforming the neighborhood of the target triples on the knowledge graph into a line graph, and using the graph attention mechanism to fuse the relationship features on the line graph, this method cannot only reduce the difficulty of modeling the heterogeneous map structure of the knowledge graph, but strengthen the connection between the triples and the entire knowledge graph, thereby reducing the model’s dependence on entity features. The comparison results with related works on the benchmark data sets WN18RR and FB15k-237 prove the superiority of proposed method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2304993/v1"
    },
    {
        "id": 10002,
        "title": "Vehicle Detection in Low Illumination Based on Attention Mechanism and RetinexNet",
        "authors": "Rongdan Qu, Xingke Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study proposes a method to detect vehicles by enhancing the YOLOv5 network 1\nstructure and incorporating RetinexNet to address the problem of limited detection capabilities of 2\ntarget identification algorithms in low illumination conditions, such as at night time. CBAM attention 3\nmodule is implemented in the network’s Neck detection layer to extract the vehicle’s primary features, 4\nreduce the extraction of unused features, and improve the vehicle’s detection performance. The 5\nDIoU is introduced as the loss function of the model to solve the imprecise location of the prediction 6\nbox and speed up the convergence of the model. RetinexNet is applied to enhance the detection 7\ncapabilities of low-illumination images by enhancing and denoising them. The experimental results 8\nindicate that the enhanced model detects vehicles with 90% accuracy in low-light conditions and has 9\na strong detection performance overall.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1727102/v2"
    },
    {
        "id": 10003,
        "title": "Review for \"Abnormal state prediction of flotation process based on dual attention mechanism and multivariate information fusion\"",
        "authors": "",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25225/v1/review3"
    },
    {
        "id": 10004,
        "title": "YOLOv4-A: Research on Traffic Sign Detection Based on Hybrid Attention Mechanism",
        "authors": "Songlin Yin Songlin Yin, Fei Tan Songlin Yin",
        "published": "2022-12",
        "citations": 0,
        "abstract": "\n                        <p>Aiming at the problem of false detection and missed detection in the traffic sign detection task, an improved YOLOv4 detection algorithm is proposed. Based on the YOLOv4 algorithm, the Efficient Channel Attention Module (ECA) and the Convolutional Block Attention Module (CBAM) are added to form YOLOv4-A algorithm. At the same time, the global K-means clustering algorithm is used to regenerate smaller anchors, which makes the network converge faster and reduces the error rate. The YOLOv4-A algorithm re-calibrates the detection branch features in the two dimensions of channel and space, so that the network can focus and enhance the effective features, and suppress the interference features, which improves the detection ability of the algorithm. Experiments on the TT100K traffic sign dataset show that the proposed algorithm has a particularly significant improvement in the performance of small target detection. Compared with the YOLOv4 algorithm, the precision and mAP@0.5 of the proposed algorithm are increased by 5.38% and 5.75%.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992022123306015"
    },
    {
        "id": 10005,
        "title": "Efficient Image Dehazing Algorithm Based on Spatial Pyramid Attention Mechanism",
        "authors": "Haoqing Zhang, Liguo Zhang, Mei Jin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4601108"
    },
    {
        "id": 10006,
        "title": "Review for \"Abnormal state prediction of flotation process based on dual attention mechanism and multivariate information fusion\"",
        "authors": "",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25225/v2/review1"
    },
    {
        "id": 10007,
        "title": "Review for \"Abnormal state prediction of flotation process based on dual attention mechanism and multivariate information fusion\"",
        "authors": "",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25225/v1/review1"
    },
    {
        "id": 10008,
        "title": "Revolutionizing Wireless Traffic Usage Forecasting: Transformer with Attention Mechanism",
        "authors": "Bandu Uppalaiah, D. Mallikarjuna Reddy, A. Srilath",
        "published": "No Date",
        "citations": 0,
        "abstract": "Revolutionizing wireless traffic forecasting empowers proactive resource\nallocation, optimizing network performance and ensuring efficient\nutilization of resources in dynamic wireless environments. real-time\ntraffic data from a business network with There are 470 APs.), this\nresearch provides a thorough examination of the temporal and\ngeographical dynamics of network traffic. Time series data forecasting\nis given a new spin with the help of machine learning models built on\nthe Transformer framework. This approach uses the brain’s attentional\nprocesses to analyze time series data for hidden dynamics and complex\npatterns. Notably, the analysis identifies high-traffic-utilization AP\ngroups exhibiting robust seasonality patterns, alongside those devoid of\nsuch patterns. Several different types of forecasting methods are used\nand evaluated in this research, among them the Holt-Winters technique, a\nSARIMA model, a GRU model, a CNN model, and a model based on\nconvolutional neural networks. In conclusion, the research sheds light\non the complex patterns underlying network traffic and presents an\ninnovative forecasting approach, bolstering the potential for improved\nwireless network resource management.",
        "link": "http://dx.doi.org/10.22541/au.169769165.55793181/v1"
    },
    {
        "id": 10009,
        "title": "Review for \"Combination of deep neural network with attention mechanism enhances the explainability of protein contact prediction\"",
        "authors": "",
        "published": "2021-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26052/v2/review1"
    },
    {
        "id": 10010,
        "title": "Two-Level Attention Mechanism for Heterogenous Graph Embedding",
        "authors": "Mahnaz Moradi, Parham Moradi, Azadeh F, Mahdi Jalili",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4572684"
    },
    {
        "id": 10011,
        "title": "Vehicle Detection in Low Illumination Based on Attention Mechanism and RetinexNet",
        "authors": "Rongdan Qu, Xingke Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study proposes a method to detect vehicles by enhancing the YOLOv5 network 1\nstructure and incorporating RetinexNet to address the problem of limited detection capabilities of 2\ntarget identification algorithms in low illumination conditions, such as at night time. CBAM attention 3\nmodule is implemented in the network’s Neck detection layer to extract the vehicle’s primary features, 4\nreduce the extraction of unused features, and improve the vehicle’s detection performance. The 5\nDIoU is introduced as the loss function of the model to solve the imprecise location of the prediction 6\nbox and speed up the convergence of the model. RetinexNet is applied to enhance the detection 7\ncapabilities of low-illumination images by enhancing and denoising them. The experimental results 8\nindicate that the enhanced model detects vehicles with 90% accuracy in low-light conditions and has 9\na strong detection performance overall.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1727102/v3"
    },
    {
        "id": 10012,
        "title": "Review for \"Abnormal state prediction of flotation process based on dual attention mechanism and multivariate information fusion\"",
        "authors": "",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25225/v1/review2"
    },
    {
        "id": 10013,
        "title": "A dual attention mechanism network with self-attention and frequency channel attention for intelligent diagnosis of multiple rolling bearing fault types",
        "authors": "Wenxing Zhang, Jianhong Yang, Xinyu Bo, Zhenkai Yang",
        "published": "2024-3-1",
        "citations": 1,
        "abstract": "Abstract\nDifferent fault types of rolling bearings correspond to different features, and classical deep learning models using a single attention mechanism (AM) have limitations in capturing feature diversity. Therefore, a novel dual attention mechanism network (DAMN) with self-attention (SA) and frequency channel attention (FCA) is proposed for rolling bearing fault diagnosis. The SA mechanism is used to capture global relationships between the input features and fault types, and the FCA mechanism applies multi-spectral attention to learn the local useful information among different input channels. The results of the ablation study on the effects of FCA blocks showed that including a proper combination of multiple frequency components is helpful in achieving higher accuracy. Experiments were conducted to diagnose rolling bearings with multiple types of faults. The results show that, compared with current fault diagnosis models, the proposed DAMN has better comprehensive performance in terms of diagnosis accuracy and model convergence speed. It was also demonstrated that the backbone of DAMN based on a dual AM could achieve better performance than the backbone based on a single AM.",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad1811"
    },
    {
        "id": 10014,
        "title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
        "authors": "Kun Wei, Pengcheng Guo, Ning Jiang",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10066"
    },
    {
        "id": 10015,
        "title": "Internal Attention is the Only Retroactive Mechanism for Controlling Precision in Working Memory",
        "authors": "Fatih Serin, Eren Gunseli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recent research has suggested that humans can assert control over the precision of working memory (WM) items. However, the mechanisms that enable this control are unclear. While some studies suggest that internal attention improves precision, it may not be the only factor, as previous work also demonstrated that WM storage is disentangled from attention. To test whether there is a precision control mechanism beyond internal attention, we contrasted internal attention and precision requirements within the same trial in three experiments. In every trial, participants memorized two items briefly. Before the test, a retro-cue indicated which item would be tested first, thus should be attended. Importantly, we encouraged participants to store the unattended item with higher precision by testing it using more similar lure colors at the probe display. Accuracy was analyzed on a small proportion of trials where the target-lure similarity, hence the task difficulty, was equal for attended and unattended items. Experiments 2 and 3 controlled for output interference by the first test and involuntary precision boost by the retro-cue, respectively. In all experiments, the unattended item had lower accuracy than the attended item suggesting that individuals were not able to remember it more precisely than the attended item. Thus, we conclude that there is no precision control mechanism beyond internal attention, highlighting the close relationship between attentional and qualitative prioritization within WM. We discuss the important implications of these findings for our understanding of the fundamentals of WM and WM-driven behaviors.",
        "link": "http://dx.doi.org/10.31234/osf.io/d2wrp"
    },
    {
        "id": 10016,
        "title": "Multi-label Text Classification on TextCNN fused BiLSTM with Attention Mechanism",
        "authors": "Aihua Duan, RODOLFO C. RAGA",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to effectively manage and utilize the network text information and realize the automatic labeling of text content, this paper proposes to use a variety of deep learning models to study multi-label text classification. In this paper, GloVe is used to obtain the semantic features of text data, and the convolutional neural network and BiLSTM neural network are fused. The latter introduces the Attention mechanism to form a parallel neural network model of TextCNN and BiLSTM_Attention. Experimental results show that TextCNN and BiLSTM_Attention model structures combine the advantages of convolutional neural network model and recurrent neural network, and can better understand local and global semantic information. The Attention mechanism is more reasonable for text feature extraction, so that the model focuses its attention on the features that contribute more to the text classification task, and the classification effect is better.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3814441/v1"
    },
    {
        "id": 10017,
        "title": "Multi-label text classification based on graph attention network and self-attention mechanism",
        "authors": "Can Lin, Cui Zhu, Wenjun Zhu",
        "published": "2022-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2653459"
    },
    {
        "id": 10018,
        "title": "A Detection Algorithm of Malicious Domain Based on Deep Learning and Multi-Head Attention Mechanism",
        "authors": "Siqi Huang, Bo Yan, Dongmei Zhang",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008098200840091"
    },
    {
        "id": 10019,
        "title": "Non-Invasive Load Recognition Model Based on CNN and Mixed Attention Mechanism",
        "authors": "Chenchen Zhang, Yujun Song, Dong Wang, Shifang Song, Xuesong Pan, Lanzhou Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012274000003807"
    },
    {
        "id": 10020,
        "title": "Fabric defects detection via visual attention mechanism",
        "authors": "Ning Li, Jianyu Zhao, Ping Jiang",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8243281"
    },
    {
        "id": 10021,
        "title": "Review for \"Deep learning pan-specific model for interpretable MHC-I peptide binding prediction with improved attention mechanism\"",
        "authors": "",
        "published": "2021-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26065/v2/review2"
    },
    {
        "id": 10022,
        "title": "Predictiveness and Reward Effects on Attention can be Explained by a Single Mechanism",
        "authors": "Samuel Paskewitz, Matt Jones",
        "published": "No Date",
        "citations": 0,
        "abstract": "The authors have withdrawn their manuscript because of a failure to replicate its main empirical result. Therefore, the authors do not wish this work to be cited as a reference. If you have any questions, please contact the corresponding author.",
        "link": "http://dx.doi.org/10.1101/469809"
    },
    {
        "id": 10023,
        "title": "Efficient Detection of Focal Cortical Dysplasia Using Novel Two Fold Attention Mechanism",
        "authors": "N Gopika, A. Meena Kowshalya",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4756667"
    },
    {
        "id": 10024,
        "title": "Bird detection Algorithm Incorporating Attention Mechanism",
        "authors": "Yuanqing Liang, Bin Wang, Houxin Huang, Hai Pang, Xiang Yue",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe safety of the substation is related to the stability of social order and people's daily lives, and the habitat and reproduction of birds can cause serious safety accidents in the power system. In this paper, to solve the problem of low accuracy rate when the YOLOv5l model is applied to the bird-repelling robot in the substation for detection, a C3ECA-YOLOv5l algorithm is proposed to accurately detect the four common bird species near the substation in real time: pigeon, magpie, sparrow and swallow. Four attention modules—Squeeze-and-Excitation (SE), Convolutional Block Attention Module (CBAM), an efficient channel attention module (ECA), and Coordinate Attention (CA)—were added to the backbone network at different times—after the C3-3 network layer, before the SPPF network layer, and in the C3 network layer (C3-3, C3-6, C3-9, and C3-3)—to determine the best network detection performance option. After comparing the network mean average precision rates (mAP@0.5), we incorporated the ECA attention module into the C3 network layer (C3-3, C3-6, C3-9, and C3-3) as the final test method. In the validation set, the mAP@0.5 of the C3ECA-YOLOv5l network was 94.7%, which, after incorporating the SE, CBAM, ECA, and CA attention modules before the SPPF network layer following the C3-3 network layer of the backbone, resulted in mean average precisions of 92.9%, 92.0%, 91.8%, and 93.1%, respectively, indicating a decrease of 1.8%, 2.7%, 2.9%, and 1.6%, respectively. Incorporating the SE, CBAM, and CA attention modules into the C3 network layer (C3-3, C3-6, C3-9, and C3-3) resulted in mean average precision rates of 93.5%, 94.1%, and 93.4%, respectively, which were 1.2%, 0.6%, and 1.3% lower than that obtained for the C3ECA-YOLOv5l model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3319901/v1"
    },
    {
        "id": 10025,
        "title": "Review for \"Deep learning pan-specific model for interpretable MHC-I peptide binding prediction with improved attention mechanism\"",
        "authors": "",
        "published": "2020-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26065/v1/review2"
    },
    {
        "id": 10026,
        "title": "Characteristic analysis of epileptic brain network based on attention mechanism",
        "authors": "Hong-Shi Yu, Xiang-Fu Meng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nConstructing an efficient and accurate epilepsy detection system is an urgent research task. In this paper, we developed an EEG-based multi-frequency multilayer brain network (MMBN) and an attentional mechanism based convolutional neural network (AM-CNN) model to study epilepsy detection. Specifically, based on the multi-frequency characteristics and correlation analysis of the brain, we first construct MMBN, where each layer corresponds to a specific frequency band. The time, frequency and channel related information of EEG signals are mapped into the multilayer network topology. On this basis, a multi branch AM-CNN model is designed, which completely matches the multilayer structure of the proposed brain network. The experimental results on public CHB-MIT dataset show that the accuracy of brain state detection is positively correlated with the fineness of frequency band division. When the raw EEG signal is divided into eight frequency bands, this method can accurately detect epilepsy, with an average accuracy of 99.75%, sensitivity of 99.43%, and specificity of 99.83%. All of these provide a reliable technical solution for epilepsy detection.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2136000/v1"
    },
    {
        "id": 10027,
        "title": "IDS-attention: an efficient algorithm for intrusion detection systems using attention mechanism",
        "authors": "FatimaEzzahra Laghrissi, Samira Douzi, Khadija Douzi, Badr Hssina",
        "published": "2021-12",
        "citations": 17,
        "abstract": "AbstractNetwork attacks are illegal activities on digital resources within an organizational network with the express intention of compromising systems. A cyber attack can be directed by individuals, communities, states or even from an anonymous source. Hackers commonly conduct network attacks to alter, damage, or steal private data. Intrusion detection systems (IDS) are the best and most effective techniques when it comes to tackle these threats. An IDS is a software application or hardware device that monitors traffic to search for malevolent activity or policy breaches. Moreover, IDSs are designed to be deployed in different environments, and they can either be host-based or network-based. A host-based intrusion detection system is installed on the client computer, while a network-based intrusion detection system is located on the network. IDSs based on deep learning have been used in the past few years and proved their effectiveness. However, these approaches produce a big false negative rate, which impacts the performance and potency of network security. In this paper, a detection model based on long short-term memory (LSTM) and Attention mechanism is proposed. Furthermore, we used four reduction algorithms, namely: Chi-Square, UMAP, Principal Components Analysis (PCA), and Mutual information. In addition, we evaluated the proposed approaches on the NSL-KDD dataset. The experimental results demonstrate that using Attention with all features and using PCA with 03 components had the best performance, reaching an accuracy of 99.09% and 98.49% for binary and multiclass classification, respectively.",
        "link": "http://dx.doi.org/10.1186/s40537-021-00544-5"
    },
    {
        "id": 10028,
        "title": "Ecg Recognition Based on Deep Convolutional Neural Network with Dual Attention Mechanism",
        "authors": "Xiaoshan Zhang, Yinwei Li, Yiming Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4639450"
    },
    {
        "id": 10029,
        "title": "Efficient Detection of Focal Cortical Dysplasia Using Novel Two Fold Attention Mechanism",
        "authors": "N. Gopika, A. Meena Kowshalya",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nFocal Cortical Dysplasia (FCD) is a malformation of cortical development that leads to frequent pharmacological pediatric epilepsy. The only treatment for FCD is surgery, and the use of imaging techniques helps physicians plan the surgery. Magnetic Resonance Imaging (MRI) is an effective tool to predict the FCD lesion. The automatic segmentation of FCD lesions technique may help locate the lesions in the patient’s MRI slices. This research work proposes a novel two fold attention mechanism namely Hybrid Attention Gate_U shaped encoder decoder Network (HAG_UNET) model to detect the FCD accurately. The proposed model exploiting its novel attention mechanism is effective in accurate FCD lesion detection. The model tends to identify crucial features for FCD detection using the proposed novel two fold attention mechanism compared to state of art model. Experiment are done in python using standard datasets. A total of 11 subjects are used for the experiment. Metrics, namely IOU, precision, recall, and F1_score, are used for evaluation. Compared to UNET, the proposed model showed 5.89%, 4.92% and 3.15% improvement in terms of IOU, Recall and F1_score respectively. Compared to Attention_UNET, the proposed model showed 5.03%, 4.9%, 1.34% and 2.1% improvements in terms of IOU, Recall, Precision and F1_score respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3996330/v1"
    },
    {
        "id": 10030,
        "title": "Review for \"Mechanistic block‐based attention mechanism stacked autoencoder for describing typical unit connection industrial processes and their monitoring\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25016/v2/review1"
    },
    {
        "id": 10031,
        "title": "SiameseBERT: A Bert-Based Siamese Network Enhanced with a Soft Attention Mechanism for Arabic Semantic Textual Similarity",
        "authors": "Rakia Saidi, Fethi Jarray, Mohammed Alsuhaibani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011624800003393"
    },
    {
        "id": 10032,
        "title": "AttentionRNN: A Structured Spatial Attention Mechanism",
        "authors": "Siddhesh Khandelwal, Leonid Sigal",
        "published": "2019-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00352"
    },
    {
        "id": 10033,
        "title": "Siamese tracking network with multi-attention mechanism",
        "authors": "Yuzhuo Xu, Ting Li, Bing Zhu, Fasheng Wang, Fuming Sun",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nObject trackers based on Siamese networks view tracking as a similarity-matching process. However, the correlation operation operates as a local linear matching process, limiting the tracker's ability to capture the intricate nonlinear relationship between the template and search region branches. Moreover, most trackers don't update the template, and often use the first frame of an image as the initial template, which will easily lead to poor tracking performance of the algorithm when facing instances of deformation, scale variation and occlusion of the tracking target. To this end, we propose a Simases tracking network with multi-attention mechanism, including a template branch and a search branch. To adapt to changes in target appearance, we integrate dynamic templates and multi-attention mechanism in the template branch to obtain more effective feature representation by fusing the features of initial templates and dynamic templates. To enhance the robustness of the tracking model, we utilize a multi-attention mechanism in the search branch that shares weights with the template branch to obtain multi-scale feature representation by fusing search region features at different scales. In addition, we design a lightweight and simple feature fusion mechanism, in which the Transformer encoder structure is utilized to fuse the information of the template area and search area, and the dynamic template is updated online based on confidence. Experimental results on publicly tracking datasets show that the proposed method achieves competitive results compared to several state-of-the-art trackers.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3296460/v1"
    },
    {
        "id": 10034,
        "title": "A multimodal sentiment model based on causal gating attention mechanism",
        "authors": "Zhiqiang Gan, Xiang-e Sun, Meihua Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nUtilising a cross-modal attention mechanism for multimodal feature integration may engender confounding effects, resulting indetrimental biases during modal interaction, and consequently impacting the outcomes of emotion classification. To addressthis issue, a cross-modal fusion network based on causal gating attention mechanism was proposed. First, a feature-maskingtext embedding module is utilised to enhance the semantic representation capability of both the audio and video modalities.Subsequently, a cross-modal attention fusion module is employed to complementarily merge the audio and video modalities,obtaining the fused audio-video modality features. Next, a causal gating cross-modal fusion network is used to fully integratethe heterogeneous data of text, audio, and video modalities. Finally, the sentiment analysis results are classified using SoftMax.The proposed cross-modal fusion network demonstrated superior performance in sentiment classification when compared tobaseline techniques on the CMU-MOSEI dataset. It effectively associated and combined pertinent multi-modal information.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3932545/v1"
    },
    {
        "id": 10035,
        "title": "Review for \"Combination of deep neural network with attention mechanism enhances the explainability of protein contact prediction\"",
        "authors": "Björn Wallner",
        "published": "2021-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26052/v2/review2"
    },
    {
        "id": 10036,
        "title": "PAUNet: A Lightweight Medical Segmentation Network Integrating Shifted Window and Attention Mechanism",
        "authors": "Baijing Chen, Xiurui Guo, Yuanjie Zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4756058"
    },
    {
        "id": 10037,
        "title": "Review for \"Combination of deep neural network with attention mechanism enhances the explainability of protein contact prediction\"",
        "authors": "Björn Wallner",
        "published": "2020-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26052/v1/review2"
    },
    {
        "id": 10038,
        "title": "Multimodal Fusion with Co-attention Mechanism",
        "authors": "Pei Li, Xinde Li",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/fusion45008.2020.9190483"
    },
    {
        "id": 10039,
        "title": "A Novel Meta-Attention Mechanism Based Few-Shot Learning Method",
        "authors": "Xu Jia, Na Ma, Fuming Sun, Xiaohui Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4289241"
    },
    {
        "id": 10040,
        "title": "Local Spatial and Temporal Relation Discovery Model Based on Attention Mechanism for Traffic Forecasting",
        "authors": "Chenyang Xu, Changqing Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4495237"
    },
    {
        "id": 10041,
        "title": "Internal attention is the only retroactive mechanism for controlling precision in working memory",
        "authors": "Fatih Serin, Eren Günseli",
        "published": "2023-7",
        "citations": 0,
        "abstract": "AbstractRecent research has suggested that humans can assert control over the precision of working memory (WM) items. However, the mechanisms that enable this control are unclear. While some studies suggest that internal attention improves precision, it may not be the only factor, as previous work also demonstrated that WM storage is disentangled from attention. To test whether there is a precision control mechanism beyond internal attention, we contrasted internal attention and precision requirements within the same trial in three experiments. In every trial, participants memorized two items briefly. Before the test, a retro-cue indicated which item would be tested first, thus should be attended. Importantly, we encouraged participants to store the unattended item with higher precision by testing it using more similar lure colors at the probe display. Accuracy was analyzed on a small proportion of trials where the target-lure similarity, hence the task difficulty, was equal for attended and unattended items. Experiments 2 and 3 controlled for output interference by the first test and involuntary precision boost by the retro-cue, respectively. In all experiments, the unattended item had lower accuracy than the attended item, suggesting that individuals were not able to remember it more precisely than the attended item. Thus, we conclude that there is no precision control mechanism beyond internal attention, highlighting the close relationship between attentional and qualitative prioritization within WM. We discuss the important implications of these findings for our understanding of the fundamentals of WM and WM-driven behaviors.",
        "link": "http://dx.doi.org/10.3758/s13414-022-02628-7"
    },
    {
        "id": 10042,
        "title": "Review for \"Mechanistic block‐based attention mechanism stacked autoencoder for describing typical unit connection industrial processes and their monitoring\"",
        "authors": "",
        "published": "2023-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25016/v1/review1"
    },
    {
        "id": 10043,
        "title": "Review for \"Mechanistic block‐based attention mechanism stacked autoencoder for describing typical unit connection industrial processes and their monitoring\"",
        "authors": "",
        "published": "2023-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25016/v1/review2"
    },
    {
        "id": 10044,
        "title": "Review for \"Mechanistic block‐based attention mechanism stacked autoencoder for describing typical unit connection industrial processes and their monitoring\"",
        "authors": "",
        "published": "2023-5-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.25016/v3/review1"
    },
    {
        "id": 10045,
        "title": "Optimizing Recommender Model: Integrating Knowledge Graph Information Fusion and Attention Mechanism",
        "authors": "Liam Patel, Ethan Chan, Lucien Tremblay",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nGraphs are versatile for capturing relationships between objects, as seen in social networks, enabling the implementation of algorithms like community discovery and clustering. The growing interest in deep learning within the graph domain has led to the development of various graph neural network algorithms in recent years. These algorithms offer an effective solution to address graph learning challenges by incorporating graph operations into traditional deep learning models and leveraging both graph structure and attribute information to handle the intricacies of graph data. Graph neural network algorithms represent an extension of traditional deep learning methods, like convolution, into the realm of graph data. These algorithms incorporate the concept of data propagation to formulate deep learning approaches specifically designed for graphs. Notably, they have demonstrated success in diverse domains such as social networks, recommendation systems, knowledge graphs, and others. In addressing the aforementioned challenges, this paper introduces a novel approach utilizing a graph neural network. To overcome the lack of temporal information in the recommendation network's neighbor structure, an ordered input-based gated cyclic unit is incorporated for state aggregation and updating. The model leverages the unit's capacity for capturing contextual relationships, thereby enhancing its ability to capture temporal features in the neighbor structure and improve predictions on new datasets. Additionally, the paper places emphasis on the shallow output of the intermediate layer. A multi-headed attention mechanism is employed to integrate information from multiple layers of output, ensuring that the shallow structural features provided by the intermediate layer play a more significant role in the scoring prediction task. This enhancement further refines the application of graph neural networks in recommendation systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3992886/v1"
    },
    {
        "id": 10046,
        "title": "Intelligent Pavement Roughness Forecasting Based on A Long Short-Term Memory Model with Attention Mechanism",
        "authors": "Feng Guo, Yu Qian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.63285c6cf30377bc3baf9ace"
    },
    {
        "id": 10047,
        "title": "Attention-DPU: Dual-path UNet with an attention mechanism for ultrasound image segmentation",
        "authors": "Pengli Wei, Mengjun Tong",
        "published": "2020-12-1",
        "citations": 2,
        "abstract": "Abstract\nWith the continuous advancement in computer vision, image segmentation has achieved fruitful results in many applications such as medical image processing. In recent years, UNet and Dual Path Network (DPN) have achieved promising results in medical image segmentation. UNet cannot effectively obtain new features and reuse features. Also, DPN cannot effectively transfer the contour information of shallow blocks to the subsequent deep blocks. This paper proposes a dual path U-shaped network with the attention mechanism (Attention-DPU); taking advantage of the two networks. In the proposed network, the ordinary convolutional layer is replaced by the micro block with a dual path. Also, the attention mechanism is adopted to improve the efficiency and accuracy of segmentation.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1693/1/012155"
    },
    {
        "id": 10048,
        "title": "A Radio Environment Map Updating Mechanism Based on an Attention Mechanism and Siamese Neural Networks",
        "authors": "Pan Zhen, Bangning Zhang, Chen Xie, Daoxing Guo",
        "published": "2022-9-8",
        "citations": 2,
        "abstract": "A radio environment map (REM) is an effective spectrum management tool. With the increase in the number of mobile devices, the wireless environment changes more and more frequently, bringing new challenges to REM updates. Traditional update methods usually rely on the amount of data collected for updating without paying attention to whether the wireless environment has changed enough. In particular, a waste of computational resources results from the frequently updated REM when the wireless environment does not change much. When the wireless environment changes a lot, the REM is not updated promptly, resulting in a decrease in REM accuracy. To overcome the above problems, this work combines the Siamese neural network and an attention mechanism in computer vision and proposes an update mechanism based on the amount of wireless environmental change starting from image data. The method compares the newly collected crowdsourced data with the constructed REM in terms of similarity. It uses similarity to measure the necessity of the REM to be updated. The algorithm in this paper can achieve a controlled update by setting a similarity threshold with good controllability. In addition, the effectiveness of the algorithm in detecting changes of the wireless environment has been demonstrated by combing simulation data.",
        "link": "http://dx.doi.org/10.3390/s22186797"
    },
    {
        "id": 10049,
        "title": "Attention Mixup: An Accurate Mixup Scheme Based On Interpretable Attention Mechanism for Multi-Label Audio Classification",
        "authors": "Wuyang Liu, Yanzhen Ren, Jingru Wang",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096755"
    },
    {
        "id": 10050,
        "title": "Surface Defect Detection on Strip Steel Based on Hybrid Attention Mechanism and Yolov8",
        "authors": "Yongping Zhang, Sijie Shen, Sen Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4588486"
    },
    {
        "id": 10051,
        "title": "Speech Emotion Recognition Using Convolutional Neural Networks with Attention Mechanism",
        "authors": "Konstantinos Mountzouris, Isidoros Perikos, Ioannis Hatzilygeroudis",
        "published": "No Date",
        "citations": 0,
        "abstract": "Speech Emotion Recognition (SER) is an interesting and difficult problem to handle. In this paper, we deal with it through the implementation of deep learning networks. We have designed and implemented six different deep learning networks, a Deep Belief Network (DBN), a simple deep neural network (SDNN), a LSTM network (LSTM), a LSTM network with the addition of an attention mechanism (LSTM-ATN), a Convolutional neural network (CNN), and a Convolutional neural network with the addition of an attention mechanism (CNN-ATN), having in mind, apart from solving the SER problem, to test the impact of attention mechanism to the results. Dropout and Batch Normalization techniques are also used to improve the generalization ability (prevention of overfitting) of the models as well as to speed up the training process. The Surrey Audio-Visual Expressed Emotion database (SAVEE), and the Ryerson Audio-Visual Database (RAVDESS) database were used for training and evaluation of our models. The results showed that networks with the addition of the attention mechanism did better than the others. Furthermore, they showed that CNN-ATN was the best among tested networks, achieving an accuracy of 74% for the SAVEE and 77% for the RAVDESS dataset, and exceeded existing state-of-the-art systems for the same datasets.",
        "link": "http://dx.doi.org/10.20944/preprints202309.1202.v1"
    },
    {
        "id": 10052,
        "title": "Multi-perspective Feature Generation Based on Attention Mechanism",
        "authors": "Longxuan Ma, Lei Zhang",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852465"
    },
    {
        "id": 10053,
        "title": "Review of: \"Attention Mechanism Model Combined with Adversarial Learning for E-commerce User Behavior Classification and Personality Recommendation\"",
        "authors": "Ebrahim Khalili",
        "published": "2023-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/8n0ain"
    },
    {
        "id": 10054,
        "title": "Automatic non-HD license plate recognition with dual attention mechanism",
        "authors": "Danchun Yang, Hui. He, Guoping Xie, Haihua Xing",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDue to the influence of weather such as image blurring, license plate deformation, rain and snow, etc., in response to the problem of high- precision automatic detection and recognition of non-HD license plates in complex environments, on the basis of the more popular detection and recognition deep network, the introduction of dual attention mechanism for license plate recognition is proposed Model At_LPRNET, and on this basis, an integrated end-to-end network of integrated end-to-end network of detection and recognition is constructed that is organically combined with the license plate detection batch The license plate recognition test results of non-HD license plate datasets such as chinese city parking dataset(CCPD) show that the At_LPRNET model has stronger multi-resolution feature extraction capabilities, and the integrated network design combined with the multi-task cascaded convolution detection The integrated network design combined with the multi-task cascaded convolution detection model multi-task cascaded convolutional networks(MTCNN) can further improve license plate recognition Accuracy. Schemes, the average accuracy of the license plate recognition of the proposed model is 13.1% higher, and it can still achieve no less than 84% under challenging, rainy, foggy, very dark or bright conditions Recognition accuracy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2965899/v1"
    },
    {
        "id": 10055,
        "title": "Review of: \"Attention Mechanism Model Combined with Adversarial Learning for E-commerce User Behavior Classification and Personality Recommendation\"",
        "authors": "Haokun Wen",
        "published": "2023-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hjdmwr"
    },
    {
        "id": 10056,
        "title": "Review of: \"Attention Mechanism Model Combined with Adversarial Learning for E-commerce User Behavior Classification and Personality Recommendation\"",
        "authors": "Jongsub Moon",
        "published": "2023-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/zz4stj"
    },
    {
        "id": 10057,
        "title": "Review of: \"Attention Mechanism Model Combined with Adversarial Learning for E-commerce User Behavior Classification and Personality Recommendation\"",
        "authors": "Ömer Kasım",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/wa3p7k"
    },
    {
        "id": 10058,
        "title": "Peer Review #2 of \"An enhanced CNN-LSTM remaining useful life prediction model for aircraft engine with attention mechanism (v0.1)\"",
        "authors": "",
        "published": "2022-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1084v0.1/reviews/2"
    },
    {
        "id": 10059,
        "title": "Peer Review #1 of \"An enhanced CNN-LSTM remaining useful life prediction model for aircraft engine with attention mechanism (v0.2)\"",
        "authors": "",
        "published": "2022-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1084v0.2/reviews/1"
    }
]