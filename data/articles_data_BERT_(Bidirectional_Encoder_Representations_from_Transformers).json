[
    {
        "id": 8901,
        "title": "Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Uday Kamath, Kenneth L. Graham, Wael Emara",
        "published": "2022-4-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003170082-3"
    },
    {
        "id": 8902,
        "title": "LyBERT: Multi-class classification of lyrics using Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Revathy V Rajendran, Anitha S Pillai, Fatemah Daneshfar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecent developments in music streaming applications and websites have made the music emotion recognition task continually active and exciting. Recognizing the music mood has many advantages. One of them is that it helps find out the prominent feeling or state of mind it brings to a listener. Identifying a listener's taste is the primary motive of currently available music emotion recognition systems such as music streaming systems (YouTube), music recommender systems (Spotify), automatic playlist generation, etc. Being a subdomain of music information retrieval (MIR), for the past years, several challenges of music emotion recognition have been studied and solved by researchers. Music emotion recognition's significant challenges include data accessibility, data volume, recognizing emotionally relevant features, etc. Several researchers have proved that emotionally relevant features can be identified by analyzing multiple features and the semantic features of lyrics and effects of audio signals create music emotion. Then one can initiate the music recognition task from a lyrical perspective because it has semantic features and audio features such as valence and arousal. The challenging part is the availability of these features that influence the music's emotion.\nThe lyrical features relevant for identifying four emotions (happy, sad, relaxed, and angry) were learned with the help of state-of-the-art algorithms. After that, those features were used to predict the feelings of Music4All dataset lyrics from the Music Emotion Recognition (MER) dataset. This work tries to identify the importance of these features through transfer learning and combining pre-trained model BERT. After transfer learning, the BERT model is then applied to the dataset to improve the model's accuracy. The overall accuracy achieved is 92%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1501499/v1"
    },
    {
        "id": 8903,
        "title": "Transfer Learning for Sentiment Classification Using Bidirectional Encoder Representations from Transformers (BERT) Model",
        "authors": "Ali Areshey, Hassan Mathkour",
        "published": "2023-5-31",
        "citations": 2,
        "abstract": "Sentiment is currently one of the most emerging areas of research due to the large amount of web content coming from social networking websites. Sentiment analysis is a crucial process for recommending systems for most people. Generally, the purpose of sentiment analysis is to determine an author’s attitude toward a subject or the overall tone of a document. There is a huge collection of studies that make an effort to predict how useful online reviews will be and have produced conflicting results on the efficacy of different methodologies. Furthermore, many of the current solutions employ manual feature generation and conventional shallow learning methods, which restrict generalization. As a result, the goal of this research is to develop a general approach using transfer learning by applying the “BERT (Bidirectional Encoder Representations from Transformers)”-based model. The efficiency of BERT classification is then evaluated by comparing it with similar machine learning techniques. In the experimental evaluation, the proposed model demonstrated superior performance in terms of outstanding prediction and high accuracy compared to earlier research. Comparative tests conducted on positive and negative Yelp reviews reveal that fine-tuned BERT classification performs better than other approaches. In addition, it is observed that BERT classifiers using batch size and sequence length significantly affect classification performance.",
        "link": "http://dx.doi.org/10.3390/s23115232"
    },
    {
        "id": 8904,
        "title": "DIBERT: Dependency Injected Bidirectional Encoder Representations from Transformers",
        "authors": "Abdul Wahab, Rafet Sifa",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>\n\t\t\n\t\n\t\n\t\t</p><div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>In this paper, we propose a new model named DIBERT\nwhich stands for Dependency Injected Bidirectional Encoder\nRepresentations from Transformers. DIBERT is a variation of\nthe BERT and has an additional third objective called Parent\nPrediction (PP) apart from Masked Language Modeling (MLM)\nand Next Sentence Prediction (NSP). PP injects the syntactic\nstructure of a dependency tree while pre-training the DIBERT\nwhich generates syntax-aware generic representations. We use\nthe WikiText-103 benchmark dataset to pre-train both BERT-\nBase and DIBERT. After fine-tuning, we observe that DIBERT\nperforms better than BERT-Base on various downstream tasks\nincluding Semantic Similarity, Natural Language Inference and\nSentiment Analysis. </p>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16444611.v1"
    },
    {
        "id": 8905,
        "title": "DIBERT: Dependency Injected Bidirectional Encoder Representations from Transformers",
        "authors": "Abdul Wahab, Rafet Sifa",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>\n\t\t\n\t\n\t\n\t\t</p><div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>In this paper, we propose a new model named DIBERT\nwhich stands for Dependency Injected Bidirectional Encoder\nRepresentations from Transformers. DIBERT is a variation of\nthe BERT and has an additional third objective called Parent\nPrediction (PP) apart from Masked Language Modeling (MLM)\nand Next Sentence Prediction (NSP). PP injects the syntactic\nstructure of a dependency tree while pre-training the DIBERT\nwhich generates syntax-aware generic representations. We use\nthe WikiText-103 benchmark dataset to pre-train both BERT-\nBase and DIBERT. After fine-tuning, we observe that DIBERT\nperforms better than BERT-Base on various downstream tasks\nincluding Semantic Similarity, Natural Language Inference and\nSentiment Analysis. </p>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16444611.v2"
    },
    {
        "id": 8906,
        "title": "DIBERT: Dependency Injected Bidirectional Encoder Representations from Transformers",
        "authors": "Abdul Wahab, Rafet Sifa",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>\n\t\t\n\t\n\t\n\t\t</p><div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>In this paper, we propose a new model named DIBERT\nwhich stands for Dependency Injected Bidirectional Encoder\nRepresentations from Transformers. DIBERT is a variation of\nthe BERT and has an additional third objective called Parent\nPrediction (PP) apart from Masked Language Modeling (MLM)\nand Next Sentence Prediction (NSP). PP injects the syntactic\nstructure of a dependency tree while pre-training the DIBERT\nwhich generates syntax-aware generic representations. We use\nthe WikiText-103 benchmark dataset to pre-train both BERT-\nBase and DIBERT. After fine-tuning, we observe that DIBERT\nperforms better than BERT-Base on various downstream tasks\nincluding Semantic Similarity, Natural Language Inference and\nSentiment Analysis. </p>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16444611"
    },
    {
        "id": 8907,
        "title": "Survice-BERT: A BERT (Bidirectional Encoder Representations from Transformers) Model for Biomedical Named Entity Recognition in Disease Surveillance Reports (Preprint)",
        "authors": "Saeyeon Cheon Jr, Insung Ahn Sr",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nSince the emergence of the novel coronavirus disease (COVID-19), it is becoming increasingly important to predict infectious diseases to prevent their spread. Researchers have conducted studies on forecasting infectious diseases to control post pandemics. However, the lack of infectious disease surveillance datasets and the difficulty of monitoring outbreaks obstruct the research development.\n\n\nOBJECTIVE\nThis study aims to create language model which can extract disease outbreak information from periodic reports of infectious diseases. We present a developing approach about our NER (Named Entity Recognition) datasets based on infectious disease surveillance reports and our Survice-BERT (Bidirectional Encoder Representations from Transformers) model fine-tuned by our datasets for immediate pandemics response.\n\n\nMETHODS\nWe built our datasets by crawling and pre-processing infectious disease surveillance reports. The structure of datasets consist of 7000 sentences and eight NER tags (Year, Month, Week, Day, Location, Disease, Case, and Death of disease). We fine-tuned two BERT model among biomedical domain-specific pre-trained models. Totally, we experimented 40 times by differing batch sizes and learning rates. We evaluated F1-scores of models by NER tags and compared each classifier performance.\n\n\nRESULTS\nThe proposed model achieved a high F1 average of 0.99. We demonstrated this model extracts data with high performance and remarkably improve research forecasting of infectious diseases outbreaks.\n\n\nCONCLUSIONS\nIt will help save lives from infectious diseases. The Survice-BERT model will be freely available on our GitHub.\n",
        "link": "http://dx.doi.org/10.2196/preprints.48841"
    },
    {
        "id": 8908,
        "title": "BERT (Bidirectional Encoder Representations from Transformers) for Missing Data Imputation in Solar Irradiance Time Series",
        "authors": "Llinet Benavides Cesar, Miguel-Ángel Manso-Callejo, Calimanut-Ionut Cira",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023039026"
    },
    {
        "id": 8909,
        "title": "Protein Sequence Classification Using Bidirectional Encoder Representations from Transformers (BERT) Approach",
        "authors": "R. Balamurugan, Saurabh Mohite, S. P. Raja",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-023-01980-1"
    },
    {
        "id": 8910,
        "title": "News Topic Modeling for International Construction Projects Based on Text Clustering Using Bidirectional Encoder Representations from Transformers (Bert)",
        "authors": "Sehwan Chung, Jungyeon Kim, Joonwoo Baik, Seokho Chi, Du Yon Kim",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4588468"
    },
    {
        "id": 8911,
        "title": "Study of Low Resource Language Document Extractive Summarization using Lexical chain and Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Pranjali Deshpande, Sunita Jahirabadkar",
        "published": "2021-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compe53109.2021.9751919"
    },
    {
        "id": 8912,
        "title": "On the Dependability of Bidirectional Encoder Representations from Transformers (BERT) to Soft Errors",
        "authors": "Zhen Gao, Rui Su, Jingyan Wang, Jie Deng, Qiang Liu, Pedro Reviriego, Shanshan Liu, Fabrizio Lombardi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170654743.34129663/v1"
    },
    {
        "id": 8913,
        "title": "Over-Sampling Effect in Pre-Training for Bidirectional Encoder Representations from Transformers (BERT) to Localize Medical BERT and Enhance Biomedical BERT (Preprint)",
        "authors": "Shoya Wada, Toshihiro Takeda, Katsuki Okada, Shirou Manabe, Shozo Konishi, Jun Kamohara, Yasushi Matsumura",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nPre-training large-scale neural language models on raw texts has made a significant contribution to improving transfer learning in natural language processing. With the introduction of transformer-based language models, such as bidirectional encoder representations from transformers (BERT), the performance of information extraction from free text has significantly improved for both the general and medical domains; however, it is difficult to train specific BERT models that perform well for domains in which there are few publicly available databases of high quality and large size.\n\n\nOBJECTIVE\nWe hypothesize that this problem can be addressed by over-sampling a domain-specific corpus and using it for pre-training with a larger corpus in a balanced manner. In this study, we verify our hypothesis by developing pre-training models using our method and evaluating their performance.\n\n\nMETHODS\nOur proposed method was based on simultaneous pre-training after over-sampling. We conducted three experiments in which we generated (1) English biomedical BERT from a small biomedical corpus, (2) Japanese medical BERT from a small medical corpus, and (3) enhanced biomedical BERT pre-trained from complete PubMed abstracts in a balanced manner and compared their performance with the conventional models.\n\n\nRESULTS\nWe first confirmed that our English BERT pre-trained using both general and small medical-domain corpora performed sufficiently well for practical use in the biomedical language understanding evaluation (BLUE) benchmark. Moreover, our proposed method was more effective than conventional methods for each different biomedical corpus size with the same corpus size for the general domain. Next, our Japanese medical BERT outperformed the other BERT models built using a conventional method concerning the medical document classification task. It demonstrated the same trend as in the first experiment in English. Lastly, our enhanced biomedical BERT model, in which clinical notes were not used during pre-training, achieved both clinical and biomedical scores on the BLUE benchmark that were 0.3 points above those of the model trained without our proposed method.\n\n\nCONCLUSIONS\nWell-balanced pre-training by over-sampling instances derived from a corpus appropriate for the target task allowed us to construct a high-performance BERT model.\n",
        "link": "http://dx.doi.org/10.2196/preprints.40992"
    },
    {
        "id": 8914,
        "title": "Using Bidirectional Encoder Representations from Transformers (BERT) to predict criminal charges and sentences from Taiwanese court judgments",
        "authors": "Yi-Ting Peng, Chin-Laung Lei",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "People unfamiliar with the law may not know what kind of behavior is considered criminal behavior or the lengths of sentences tied to those behaviors. This study used criminal judgments from the district court in Taiwan to predict the type of crime and sentence length that would be determined. This study pioneers using Taiwanese criminal judgments as a dataset and proposes improvements based on Bidirectional Encoder Representations from Transformers (BERT). This study is divided into two parts: criminal charges prediction and sentence prediction. Injury and public endangerment judgments were used as training data to predict sentences. This study also proposes an effective solution to BERT’s 512-token limit. The results show that using the BERT model to train Taiwanese criminal judgments is feasible. Accuracy reached 98.95% in predicting criminal charges and 72.37% in predicting the sentence in injury trials, and 80.93% in predicting the sentence in public endangerment trials.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1841"
    },
    {
        "id": 8915,
        "title": "Financial Sentiment Analysis for Predicting Direction of Stocks using Bidirectional Encoder Representations from Transformers (BERT) and Deep Learning Models",
        "authors": "",
        "published": "2019-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17758/uruae8.ul12191013"
    },
    {
        "id": 8916,
        "title": "Automated detection of contractual risk clauses from construction specifications using bidirectional encoder representations from transformers (BERT)",
        "authors": "Seonghyeon Moon, Seokho Chi, Seok-Been Im",
        "published": "2022-10",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.autcon.2022.104465"
    },
    {
        "id": 8917,
        "title": "Secure file transfer between multiple clients using bidirectional encoder representations from transformers (BERT) algorithm compared with XLnet algorithm",
        "authors": "Janaki Sasidhar, S. Christy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0159812"
    },
    {
        "id": 8918,
        "title": "Identification of Misogyny on Social Media in Indonesian Using Bidirectional Encoder Representations From Transformers (BERT)",
        "authors": "Bagas Tri Wibowo, Dade Nurjanah, Hani Nurrahmi",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067106"
    },
    {
        "id": 8919,
        "title": "Humor Detection Using a Bidirectional Encoder Representations from Transformers (BERT) based Neural Ensemble Model",
        "authors": "Rida Miraj, Masaki Aono",
        "published": "2021-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta53211.2021.9640260"
    },
    {
        "id": 8920,
        "title": "exBAKE: Automatic Fake News Detection Model Based on Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Heejung Jwa, Dongsuk Oh, Kinam Park, Jang Kang, Hueiseok Lim",
        "published": "2019-9-28",
        "citations": 144,
        "abstract": "News currently spreads rapidly through the internet. Because fake news stories are designed to attract readers, they tend to spread faster. For most readers, detecting fake news can be challenging and such readers usually end up believing that the fake news story is fact. Because fake news can be socially problematic, a model that automatically detects such fake news is required. In this paper, we focus on data-driven automatic fake news detection methods. We first apply the Bidirectional Encoder Representations from Transformers model (BERT) model to detect fake news by analyzing the relationship between the headline and the body text of news. To further improve performance, additional news data are gathered and used to pre-train this model. We determine that the deep-contextualizing nature of BERT is best suited for this task and improves the 0.14 F-score over older state-of-the-art models.",
        "link": "http://dx.doi.org/10.3390/app9194062"
    },
    {
        "id": 8921,
        "title": "Fake News Detection by Fine Tuning of Bidirectional Encoder Representations from Transformers",
        "authors": "MSVPJ SATHVIK, Mukesh Kumar Mishra, Sibasankar Padhy",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Everyone now has internet and social media access, making it simple to get information and news. On the other hand, there are fake news articles, also. It not only makes difficult for the public to find their truthfulness but also misleads them. Consequently, developing intelligent systems for separating news is critical. In this paper, four deep learning techniques such as Bidirectional Encoder Representations from Transformers (BERT), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), Convolutional Neural Networks-BiLSTM (CNN-BiLSTM) are used to detect fake news. We trained these models on three large datasets, namely, CoAID, GossipCop, and PolitiFact, that include fake news from social media and news articles. The text is divided into two categories: fake and real. We also trained and compared the results with ten well-known machine-learning classifiers, e.g., Naive Bayes, Decision Tree, Support Vector Machine, K-Nearest Neighbors Classifier, etc. The experimental results indicate that the deep learning-based BERT method achieves better accuracy and F1-score of 91.94\\% and 92.06\\% for PolitiFact, 99.09\\% and 98.38\\% for CoAID, 85.59\\% and 82.40\\% for GossipCop, respectively. These results suggest that deep learning algorithms provide favorably accurate results and outperform machine learning models in terms of accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22215403.v1"
    },
    {
        "id": 8922,
        "title": "Fake News Detection by Fine Tuning of Bidirectional Encoder Representations from Transformers",
        "authors": "MSVPJ SATHVIK, Mukesh Kumar Mishra, Sibasankar Padhy",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Everyone now has internet and social media access, making it simple to get information and news. On the other hand, there are fake news articles, also. It not only makes difficult for the public to find their truthfulness but also misleads them. Consequently, developing intelligent systems for separating news is critical. In this paper, four deep learning techniques such as Bidirectional Encoder Representations from Transformers (BERT), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), Convolutional Neural Networks-BiLSTM (CNN-BiLSTM) are used to detect fake news. We trained these models on three large datasets, namely, CoAID, GossipCop, and PolitiFact, that include fake news from social media and news articles. The text is divided into two categories: fake and real. We also trained and compared the results with ten well-known machine-learning classifiers, e.g., Naive Bayes, Decision Tree, Support Vector Machine, K-Nearest Neighbors Classifier, etc. The experimental results indicate that the deep learning-based BERT method achieves better accuracy and F1-score of 91.94\\% and 92.06\\% for PolitiFact, 99.09\\% and 98.38\\% for CoAID, 85.59\\% and 82.40\\% for GossipCop, respectively. These results suggest that deep learning algorithms provide favorably accurate results and outperform machine learning models in terms of accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22215403"
    },
    {
        "id": 8923,
        "title": "A Comparative Sentiment Analysis of Airline Customer Reviews Using Bidirectional Encoder Representations from Transformers (BERT) and Its Variants",
        "authors": "Zehong Li, Chuyang Yang, Chenyu Huang",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "The applications of artificial intelligence (AI) and natural language processing (NLP) have significantly empowered the safety and operational efficiency within the aviation sector for safer and more efficient operations. Airlines derive informed decisions to enhance operational efficiency and strategic planning through extensive contextual analysis of customer reviews and feedback from social media, such as Twitter and Facebook. However, this form of analytical endeavor is labor-intensive and time-consuming. Extensive studies have investigated NLP algorithms for sentiment analysis based on textual customer feedback, thereby underscoring the necessity for an in-depth investigation of transformer architecture-based NLP models. In this study, we conducted an exploration of the large language model BERT and three of its derivatives using an airline sentiment tweet dataset for downstream tasks. We further honed this fine-tuning by adjusting the hyperparameters, thus improving the model’s consistency and precision of outcomes. With RoBERTa distinctly emerging as the most precise and overall effective model in both the binary (96.97%) and tri-class (86.89%) sentiment classification tasks and persisting in outperforming others in the balanced dataset for tri-class sentiment classification, our results validate the BERT models’ application in analyzing airline industry customer sentiment. In addition, this study identifies the scope for improvement in future studies, such as investigating more systematic and balanced datasets, applying other large language models, and using novel fine-tuning approaches. Our study serves as a pivotal benchmark for future exploration in customer sentiment analysis, with implications that extend from the airline industry to broader transportation sectors, where customer feedback plays a crucial role.",
        "link": "http://dx.doi.org/10.3390/math12010053"
    },
    {
        "id": 8924,
        "title": "Impact of Using Bidirectional Encoder Representations from Transformers (BERT) Models for Arabic Dialogue Acts Identification",
        "authors": "Alaa Joukhadar, Nada Ghneim, Ghaida Rebdawi",
        "published": "2021-10-31",
        "citations": 2,
        "abstract": "In Human-Computer dialogue systems, the correct identification of the intent underlying a speaker's utterance is crucial to the success of a dialogue. Several researches have studied the Dialogue Act Classification (DAC) task to identify Dialogue Acts (DA) for different languages. Recently, the emergence of Bidirectional Encoder Representations from Transformers (BERT) models, enabled establishing state-of-the-art results for a variety of natural language processing tasks in different languages. Very few researches have been done in the Arabic Dialogue acts identification task. The BERT representation model has not been studied yet in Arabic Dialogue acts detection task. In this paper, we propose a model using BERT language representation to identify Arabic Dialogue Acts. We explore the impact of using different BERT models: AraBERT Original (v0.1, v1), AraBERT Base (v0.2, and v2) and AraBERT Large (v0.2, and v2), which are pretrained on different Arabic corpora (different in size, morphological segmentation, language model window, …). The comparison was performed on two available Arabic datasets. Using AraBERTv0.2-base model for dialogue representations outperformed all other pretrained models. Moreover, we compared the performance of AraBERTv0.2-base model to the state-of-the-art approaches applied on the two datasets. The comparison showed that this representation model outperformed the performance both state-of-the-art models.",
        "link": "http://dx.doi.org/10.18280/isi.260506"
    },
    {
        "id": 8925,
        "title": "P-BERT: Polished Up Bidirectional Encoder Representations from Transformers for Predicting Malicious URL to Preserve Privacy",
        "authors": "Supriya B N, C.B. Akki",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/upcon56432.2022.9986461"
    },
    {
        "id": 8926,
        "title": "Using Bidirectional Encoder Representations from Transformers (BERT) to classify traffic crash severity types",
        "authors": "Amir Hossein Oliaee, Subasish Das, Jinli Liu, M. Ashifur Rahman",
        "published": "2023-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100007"
    },
    {
        "id": 8927,
        "title": "On the Dependable Operation of Bidirectional Encoder Representations from Transformers (BERT) in the Presence of Soft Errors",
        "authors": "Zhen Gao, Jingyan Wang, Rui Su, Pedro Reviriego, Shanshan Liu, Lombardi Fabrizio",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nano58406.2023.10231204"
    },
    {
        "id": 8928,
        "title": "Classification of Fire Related Tweets on Twitter Using Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Jairus Mingua, Dionis Padilla, Evan Joy Celino",
        "published": "2021-11-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hnicem54116.2021.9731956"
    },
    {
        "id": 8929,
        "title": "RETRACTED: Named Entity Detection and Classification for Afaan Oromoo Text based on Bidirectional Encoder Representations from Transformers",
        "authors": "SORESSA BEYENE LEMU",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe authors have requested that this preprint be removed from Research Square.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3837159/v1"
    },
    {
        "id": 8930,
        "title": "Do syntactic trees enhance Bidirectional Encoder Representations from Transformers (BERT) models for chemical–drug relation extraction?",
        "authors": "Anfu Tang, Louise Deléger, Robert Bossy, Pierre Zweigenbaum, Claire Nédellec",
        "published": "2022-8-25",
        "citations": 0,
        "abstract": "Abstract\nCollecting relations between chemicals and drugs is crucial in biomedical research. The pre-trained transformer model, e.g. Bidirectional Encoder Representations from Transformers (BERT), is shown to have limitations on biomedical texts; more specifically, the lack of annotated data makes relation extraction (RE) from biomedical texts very challenging. In this paper, we hypothesize that enriching a pre-trained transformer model with syntactic information may help improve its performance on chemical–drug RE tasks. For this purpose, we propose three syntax-enhanced models based on the domain-specific BioBERT model: Chunking-Enhanced-BioBERT and Constituency-Tree-BioBERT in which constituency information is integrated and a Multi-Task-Learning framework Multi-Task-Syntactic (MTS)-BioBERT in which syntactic information is injected implicitly by adding syntax-related tasks as training objectives. Besides, we test an existing model Late-Fusion which is enhanced by syntactic dependency information and build ensemble systems combining syntax-enhanced models and non-syntax-enhanced models. Experiments are conducted on the BioCreative VII DrugProt corpus, a manually annotated corpus for the development and evaluation of RE systems. Our results reveal that syntax-enhanced models in general degrade the performance of BioBERT in the scenario of biomedical RE but improve the performance when the subject–object distance of candidate semantic relation is long. We also explore the impact of quality of dependency parses. [Our code is available at: https://github.com/Maple177/syntax-enhanced-RE/tree/drugprot (for only MTS-BioBERT); https://github.com/Maple177/drugprot-relation-extraction (for the rest of experiments)]\nDatabase URL https://github.com/Maple177/drugprot-relation-extraction",
        "link": "http://dx.doi.org/10.1093/database/baac070"
    },
    {
        "id": 8931,
        "title": "Improving Case Duration Accuracy of Orthopedic Surgery Using Bidirectional Encoder Representations from Transformers (BERT) on Radiology Reports",
        "authors": "William Zhong, Phil Y. Yao, Sri Harsha Boppana, Fernanda V. Pacheco, Brenton S. Alexander, Sierra Simpson, Rodney A. Gabriel",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nPurpose: A major source of inefficiency in the operating room is the mismatch between scheduled versus actual surgical time. The purpose of this study was to demonstrate a proof-of-concept study for predicting case duration by applying natural language processing (NLP) and machine learning that interpret radiology reports for patients undergoing radius fracture repair.\nMethods: Logistic regression, random forest, and artificial neural networks (ANN) were tested without NLP and with bag-of-words. Another NLP method tested used ANN and Bidirectional Encoder Representations from Transformers specifically pre-trained on clinical notes (ClinicalBERT). A total of 201 cases were included. The data were split into 70% training and 30% test sets. The average root mean squared error (RMSE) (and 95% confidence interval [CI]) from 10-fold cross-validation on the training set were used to develop each model. Models were then compared to a baseline model, which used historic averages to predict surgical time.\nResults: The average RMSE was lowest using ANN with ClinicalBERT (25.6 minutes, 95% CI: 21.5 - 29.7), which was significantly (P<0.001) lower than the baseline model (39.3 minutes, 95% CI: 30.9 - 47.7). Using the ANN and ClinicalBERT on the test set, the percentage of accurately predicted cases, which was defined by the actual surgical duration within 15% of the predicted surgical duration, increased from 26.8% to 58.9% (P<0.001).\nConclusion: This proof-of-concept study demonstrated the successful application of NLP and machine leaning to extract features from unstructured clinical data resulting in improved prediction accuracy for surgical case duration.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2808364/v1"
    },
    {
        "id": 8932,
        "title": "Feel the Market: An Attempt to Identify Additional Factor in the Capital Asset Pricing Model (CAPM) Using Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Christopher Lingwei Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4521946"
    },
    {
        "id": 8933,
        "title": "Context-Based Profanity Detection and Censorship Using Bidirectional Encoder Representations from Transformers",
        "authors": "Valfrid Galinato, Lawrence Amores, Gino Ben Magsino, David Rafael Sumawang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4341604"
    },
    {
        "id": 8934,
        "title": "Forecasting cyber security threats landscape and associated technical trends in telehealth using Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Usharani Hareesh Govindarajan, Dhiraj Kumar Singh, Hardik A. Gohel",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cose.2023.103404"
    },
    {
        "id": 8935,
        "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
        "authors": "Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",
        "published": "No Date",
        "citations": 18,
        "abstract": "ABSTRACTDeciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios. To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, that forms global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on many sequence predictions tasks, after easy fine-tuning using small task-specific data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variants. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance.",
        "link": "http://dx.doi.org/10.1101/2020.09.17.301879"
    },
    {
        "id": 8936,
        "title": "BERTrand - peptide:TCR binding prediction using Bidirectional Encoder Representations from Transformers augmented with random TCR pairing",
        "authors": "Alexander Myronov, Giovanni Mazzocco, Paulina Król, Dariusz Plewczynski",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractMotivationThe advent of T cell receptor (TCR) sequencing experiments allowed for a significant increase in the amount of peptide:TCR binding data available and a number of machine learning models appeared in recent years. High-quality prediction models for a fixed epitope sequence are feasible, provided enough known binding TCR sequences are available. However, their performance drops significantly for previously unseen peptides.ResultsWe prepare the dataset of known peptide:TCR binders and augment it with negative decoys created using healthy donors’ T-cell repertoires. We employ deep learning methods commonly applied in Natural Language Processing (NLP) to train part a peptide:TCR binding model with a degree of cross-peptide generalization (0.66 AUROC). We demonstrate that BERTrand outperforms the published methods when evaluated on peptide sequences not used during model training.AvailabilityThe datasets and the code for model training are available athttps://github.com/SFGLab/bertrandContactalexander.myronov@gmail.com,dariusz.plewczynski@pw.edu.plSupplementary informationSupplementary data are available atBioinformaticsonline.",
        "link": "http://dx.doi.org/10.1101/2023.06.12.544613"
    },
    {
        "id": 8937,
        "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
        "authors": "Guan-Lin Chao, Ian Lane",
        "published": "2019-9-15",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1355"
    },
    {
        "id": 8938,
        "title": "Adapting Bidirectional Encoder Representations from Transformers (BERT) to Assess Clinical Semantic Textual Similarity: Algorithm Development and Validation Study (Preprint)",
        "authors": "Klaus Kades, Jan Sellner, Gregor Koehler, Peter M Full, T Y Emmy Lai, Jens Kleesiek, Klaus H Maier-Hein",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nNatural Language Understanding enables automatic extraction of relevant information from clinical text data, which are acquired every day in hospitals. In 2018, the language model Bidirectional Encoder Representations from Transformers (BERT) was introduced, generating new state-of-the-art results on several downstream tasks. The National NLP Clinical Challenges (n2c2) is an initiative that strives to tackle such downstream tasks on domain-specific clinical data. In this paper, we present the results of our participation in the 2019 n2c2 and related work completed thereafter.\n\n\nOBJECTIVE\nThe objective of this study was to optimally leverage BERT for the task of assessing the semantic textual similarity of clinical text data.\n\n\nMETHODS\nWe used BERT as an initial baseline and analyzed the results, which we used as a starting point to develop 3 different approaches where we (1) added additional, handcrafted sentence similarity features to the classifier token of BERT and combined the results with more features in multiple regression estimators, (2) incorporated a built-in ensembling method, <i>M</i>-Heads, into BERT by duplicating the regression head and applying an adapted training strategy to facilitate the focus of the heads on different input patterns of the medical sentences, and (3) developed a graph-based similarity approach for medications, which allows extrapolating similarities across known entities from the training set. The approaches were evaluated with the Pearson correlation coefficient between the predicted scores and ground truth of the official training and test dataset.\n\n\nRESULTS\nWe improved the performance of BERT on the test dataset from a Pearson correlation coefficient of 0.859 to 0.883 using a combination of the M-Heads method and the graph-based similarity approach. We also show differences between the test and training dataset and how the two datasets influenced the results.\n\n\nCONCLUSIONS\nWe found that using a graph-based similarity approach has the potential to extrapolate domain specific knowledge to unseen sentences. We observed that it is easily possible to obtain deceptive results from the test dataset, especially when the distribution of the data samples is different between training and test datasets.\n",
        "link": "http://dx.doi.org/10.2196/preprints.22795"
    },
    {
        "id": 8939,
        "title": "A Study of Bidirectional Encoder Representations from Transformers for Sequential Recommendations",
        "authors": "Amine Kheldouni, Jaouad Boumhidi",
        "published": "2022-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscv54655.2022.9806062"
    },
    {
        "id": 8940,
        "title": "BERT-Kgly: A Bidirectional Encoder Representations From Transformers (BERT)-Based Model for Predicting Lysine Glycation Site for Homo sapiens",
        "authors": "Yinbo Liu, Yufeng Liu, Gang-Ao Wang, Yinchu Cheng, Shoudong Bi, Xiaolei Zhu",
        "published": "2022-2-18",
        "citations": 9,
        "abstract": "As one of the most important posttranslational modifications (PTMs), protein lysine glycation changes the characteristics of the proteins and leads to the dysfunction of the proteins, which may cause diseases. Accurately detecting the glycation sites is of great benefit for understanding the biological function and potential mechanism of glycation in the treatment of diseases. However, experimental methods are expensive and time-consuming for lysine glycation site identification. Instead, computational methods, with their higher efficiency and lower cost, could be an important supplement to the experimental methods. In this study, we proposed a novel predictor, BERT-Kgly, for protein lysine glycation site prediction, which was developed by extracting embedding features of protein segments from pretrained Bidirectional Encoder Representations from Transformers (BERT) models. Three pretrained BERT models were explored to get the embeddings with optimal representability, and three downstream deep networks were employed to build our models. Our results showed that the model based on embeddings extracted from the BERT model pretrained on 556,603 protein sequences of UniProt outperforms other models. In addition, an independent test set was used to evaluate and compare our model with other existing methods, which indicated that our model was superior to other existing models.",
        "link": "http://dx.doi.org/10.3389/fbinf.2022.834153"
    },
    {
        "id": 8941,
        "title": "Advancing natural language processing (NLP) applications of morphologically rich languages with bidirectional encoder representations from transformers (BERT): an empirical case study for Turkish",
        "authors": "Akın Özçift, Kamil Akarsu, Fatma Yumuk, Cevhernur Söylemez",
        "published": "2021-4-3",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/00051144.2021.1922150"
    },
    {
        "id": 8942,
        "title": "Feature Extraction with Bidirectional Encoder Representations from Transformers in Hyperspectral Images",
        "authors": "Ibrahim Onur Sigirci, Hakan Ozgur, Gokhan Bilgin",
        "published": "2020-10-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302204"
    },
    {
        "id": 8943,
        "title": "A Preliminary Study on Personalized Spam E-mail Filtering Using Bidirectional Encoder Representations from Transformers (BERT) and TensorFlow 2.0",
        "authors": "Kashif Iqbal, Salman A. Khan, Shamim Anisa, Ayesha Tasneem, Nazeeruddin Mohammad",
        "published": "2022-2-15",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12785/ijcds/110173"
    },
    {
        "id": 8944,
        "title": "Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": " Diyah Utami Kusumaning Putri,  Dinar Nugroho Pratomo",
        "published": "2022-7-30",
        "citations": 1,
        "abstract": "The problem of the existence of news article that does not match with content, called clickbait, has seriously interfered readers from getting the information they expect. The number of clickbait news continues significantly increased in recent years. According to this problem, a clickbait detector is required to automatically identify news article headlines that include clickbait and non-clickbait. Additionally, many currently existing solutions use handcrafted features and traditional machine learning methods, which limit the generalization. Therefore, this study fine-tunes the Bidirectional Encoder Representations from Transformers (BERT) and uses the Indonesian news headlines dataset CLICK-ID to predict clickbait (BERT). In this research, we use IndoBERT as the pre-trained model, a state-of-the-art BERT-based language model for Indonesian. Then, the usefulness of BERT-based classifiers is then assessed by comparing the performance of IndoBERT classifiers with different pre-trained models with that of two word-vectors-based approaches (i.e., bag-of-words and TF-IDF) and five machine learning classifiers (i.e., NB, KNN, SVM, DT, and RF). The evaluation results indicate that all fine-tuned IndoBERT classifiers outperform all word-vectors-based machine learning classifiers in classifying clickbait and non-clickbait Indonesian news headlines. The IndoBERTBASE using the two training phases model gets the highest accuracy of 0.8247, which is 0.064 (6%), outperforming the SVM classifier's accuracy with the bag-of-words model 0.7607.",
        "link": "http://dx.doi.org/10.25139/inform.v7i2.4686"
    },
    {
        "id": 8945,
        "title": "Fine-Tuning Bidirectional Encoder Representations From Transformers (BERT)–Based Models on Large-Scale Electronic Health Record Notes: An Empirical Study",
        "authors": "Fei Li, Yonghao Jin, Weisong Liu, Bhanu Pratap Singh Rawat, Pengshan Cai, Hong Yu",
        "published": "2019-9-12",
        "citations": 96,
        "abstract": "BackgroundThe bidirectional encoder representations from transformers (BERT) model has achieved great success in many natural language processing (NLP) tasks, such as named entity recognition and question answering. However, little prior work has explored this model to be used for an important task in the biomedical and clinical domains, namely entity normalization.ObjectiveWe aim to investigate the effectiveness of BERT-based models for biomedical or clinical entity normalization. In addition, our second objective is to investigate whether the domains of training data influence the performances of BERT-based models as well as the degree of influence.MethodsOur data was comprised of 1.5 million unlabeled electronic health record (EHR) notes. We first fine-tuned BioBERT on this large collection of unlabeled EHR notes. This generated our BERT-based model trained using 1.5 million electronic health record notes (EhrBERT). We then further fine-tuned EhrBERT, BioBERT, and BERT on three annotated corpora for biomedical and clinical entity normalization: the Medication, Indication, and Adverse Drug Events (MADE) 1.0 corpus, the National Center for Biotechnology Information (NCBI) disease corpus, and the Chemical-Disease Relations (CDR) corpus. We compared our models with two state-of-the-art normalization systems, namely MetaMap and disease name normalization (DNorm).ResultsEhrBERT achieved 40.95% F1 in the MADE 1.0 corpus for mapping named entities to the Medical Dictionary for Regulatory Activities and the Systematized Nomenclature of Medicine—Clinical Terms (SNOMED-CT), which have about 380,000 terms. In this corpus, EhrBERT outperformed MetaMap by 2.36% in F1. For the NCBI disease corpus and CDR corpus, EhrBERT also outperformed DNorm by improving the F1 scores from 88.37% and 89.92% to 90.35% and 93.82%, respectively. Compared with BioBERT and BERT, EhrBERT outperformed them on the MADE 1.0 corpus and the CDR corpus.ConclusionsOur work shows that BERT-based models have achieved state-of-the-art performance for biomedical and clinical entity normalization. BERT-based models can be readily fine-tuned to normalize any kind of named entities.",
        "link": "http://dx.doi.org/10.2196/14830"
    },
    {
        "id": 8946,
        "title": "Aspect-based Sentiment Analysis for Bengali Text using Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Moythry Manir Samia, Alimul Rajee, Md. Rakib Hasan, Mohammad Omar Faruq, Pintu Chandra Paul",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2022.01312112"
    },
    {
        "id": 8947,
        "title": "Korean Semantic Role Labeling with Bidirectional Encoder Representations from Transformers and Simple Semantic Information",
        "authors": "Jangseong Bae, Changki Lee",
        "published": "2022-6-13",
        "citations": 1,
        "abstract": "State-of-the-art semantic role labeling (SRL) performance has been achieved using neural network models by incorporating syntactic feature information such as dependency trees. In recent years, breakthroughs achieved using end-to-end neural network models have resulted in a state-of-the-art SRL performance even without syntactic features. With the advent of a language model called bidirectional encoder representations from transformers (BERT), another breakthrough was witnessed. Even though the semantic information of each word constituting a sentence is important in determining the meaning of a word, previous studies regarding the end-to-end neural network method did not utilize semantic information. In this study, we propose a BERT-based SRL model that uses simple semantic information without syntactic feature information. To obtain the latter, we used PropBank, which described the relational information between predicates and arguments. In addition, text-originated feature information obtained from the training text data was utilized. Our proposed model achieved state-of-the-art results on both Korean PropBank and CoNLL-2009 English benchmarks.",
        "link": "http://dx.doi.org/10.3390/app12125995"
    },
    {
        "id": 8948,
        "title": "Bidirectional Encoder Representations from Transformers for Modelling Stock Prices",
        "authors": "Parinnay Chaudhry",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "Abstract: Bidirectional Encoder Representations from Transformers (BERT) is a transformer neural network architecture designed for natural language processing (NLP). The model’s architecture allows for an efficient, contextual understanding of words in sentences. Empirical evidence regarding the usage of BERT has proved a high degree of accuracy in NLP tasks such as sentiment analysis and next sentence classification. This study utilises BERT’s sentiment analysis capability, proposes and tests a framework to model a quantitative relation between the news and reportings of a company, and the movement of its stock price. This study also aims to explore the nature of human psychology in terms of modelling risk and opportunity and gain insight into the subjectivity of the human mind. Keywords: natural language processing, BERT, sentiment analysis, stock price modelling, transformers, neural networks, selfattention",
        "link": "http://dx.doi.org/10.22214/ijraset.2022.40406"
    },
    {
        "id": 8949,
        "title": "Investigation of Pre-Trained Bidirectional Encoder Representations from Transformers Checkpoints for Indonesian Abstractive Text Summarization",
        "authors": "Henry Lucky, Derwin Suhartono",
        "published": "2021",
        "citations": 2,
        "abstract": "Text summarization aims to reduce text by removing less useful information to obtain information quickly and precisely. In Indonesian abstractive text summarization, the research mostly focuses on multi-document summarization which methods will not work optimally in single-document summarization. As the public summarization datasets and works in English are focusing on single-document summarization, this study emphasized on Indonesian single-document summarization. Abstractive text summarization studies in English frequently use Bidirectional Encoder Representations from Transformers (BERT), and since Indonesian BERT checkpoint is available, it was employed in this study. This study investigated the use of Indonesian BERT in abstractive text summarization on the IndoSum dataset using the BERTSum model. The investigation proceeded by using various combinations of model encoders, model embedding sizes, and model decoders. Evaluation results showed that models with more embedding size and used Generative Pre-Training (GPT)-like decoder could improve the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score and BERTScore of the model results.",
        "link": "http://dx.doi.org/10.32890/jict2022.21.1.4"
    },
    {
        "id": 8950,
        "title": "Towards a system for predicting the category of educational and vocational guidance questions using bidirectional encoder representations of transformers (BERT)",
        "authors": "Omar Zahour",
        "published": "2020-2-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.30534/ijatcse/2020/69912020"
    },
    {
        "id": 8951,
        "title": "DIBERT: Dependency Injected Bidirectional Encoder Representations from Transformers",
        "authors": "Abdul Wahab, Rafet Sifa",
        "published": "2021-12-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659898"
    },
    {
        "id": 8952,
        "title": "Adapting Bidirectional Encoder Representations from Transformers (BERT) to Assess Clinical Semantic Textual Similarity: Algorithm Development and Validation Study",
        "authors": "Klaus Kades, Jan Sellner, Gregor Koehler, Peter M Full, T Y Emmy Lai, Jens Kleesiek, Klaus H Maier-Hein",
        "published": "2021-2-3",
        "citations": 13,
        "abstract": "\nBackground\nNatural Language Understanding enables automatic extraction of relevant information from clinical text data, which are acquired every day in hospitals. In 2018, the language model Bidirectional Encoder Representations from Transformers (BERT) was introduced, generating new state-of-the-art results on several downstream tasks. The National NLP Clinical Challenges (n2c2) is an initiative that strives to tackle such downstream tasks on domain-specific clinical data. In this paper, we present the results of our participation in the 2019 n2c2 and related work completed thereafter.\n\n\nObjective\nThe objective of this study was to optimally leverage BERT for the task of assessing the semantic textual similarity of clinical text data.\n\n\nMethods\nWe used BERT as an initial baseline and analyzed the results, which we used as a starting point to develop 3 different approaches where we (1) added additional, handcrafted sentence similarity features to the classifier token of BERT and combined the results with more features in multiple regression estimators, (2) incorporated a built-in ensembling method, M-Heads, into BERT by duplicating the regression head and applying an adapted training strategy to facilitate the focus of the heads on different input patterns of the medical sentences, and (3) developed a graph-based similarity approach for medications, which allows extrapolating similarities across known entities from the training set. The approaches were evaluated with the Pearson correlation coefficient between the predicted scores and ground truth of the official training and test dataset.\n\n\nResults\nWe improved the performance of BERT on the test dataset from a Pearson correlation coefficient of 0.859 to 0.883 using a combination of the M-Heads method and the graph-based similarity approach. We also show differences between the test and training dataset and how the two datasets influenced the results.\n\n\nConclusions\nWe found that using a graph-based similarity approach has the potential to extrapolate domain specific knowledge to unseen sentences. We observed that it is easily possible to obtain deceptive results from the test dataset, especially when the distribution of the data samples is different between training and test datasets.\n",
        "link": "http://dx.doi.org/10.2196/22795"
    },
    {
        "id": 8953,
        "title": "Analyze Detection Depression In Social Media Twitter Using Bidirectional Encoder Representations from Transformers",
        "authors": "Fikri Ilham, Warih Maharani",
        "published": "2022-7-31",
        "citations": 0,
        "abstract": "Human health is an essential part of the welfare of a country. Early detection of a disease is necessary to prevent it from spreading in an area. Social media is now a rapid and widespread development of information to provide convenience for the public to communicate. Depressed people have a variety of depressive symptoms from every human behaviour. Psychological doctors often conduct face-to-face interviews on commonly used diagnoses and statistical manual criteria for mental disorders. Depression is a mental disorder that typically appears in humans with the characteristics of depressed mood, loss of interest and pleasure, unstable body energy, and poor concentration. In conducting this research, the aim is to detect people who are depressed by using the Machine Learning-based BERT (Bidirectional Encoder Representations from Transformers) method. BERT can binarily classify text on social media, namely Twitter, which contains Depression detection. Based on the tests that have been carried out, the best accuracy value is 0.7176 or 71%.",
        "link": "http://dx.doi.org/10.47065/josh.v3i4.1885"
    },
    {
        "id": 8954,
        "title": "Tuned bidirectional encoder representations from transformers for fake news detection",
        "authors": "Amsal Pardamean, Hilman F. Pardede",
        "published": "2021-6-1",
        "citations": 0,
        "abstract": "Online medias are currently the dominant source of Information due to not being limited by time and place, fast and wide distributions. However, inaccurate news, or often referred as fake news is a major problem in news dissemination for online medias. Inaccurate news is information that is not true, that is engineered to cover the real information and has no factual basis. Usually, inaccurate news is made in the form of news that has mass appeal and is presented in the guise of genuine and legitimate news nuances to deceive or change the reader's mind or opinion. Identification of inaccurate news from real news can be done with natural language processing (NLP) technologies. In this paper, we proposed bidirectional encoder representations from transformers (BERT) for inaccurate news identification. BERT is a language model based on deep learning technologies and it has found effective for many NLP tasks. In this study, we use transfer learning and fine-tuning to adapt BERT for inaccurate news identification. The experiments show that our method could achieve accuracy of 99.23%, recall 99.46%, precision 98.86%, and F-Score of 99.15%. It is largely better than traditional method for the same tasks.",
        "link": "http://dx.doi.org/10.11591/ijeecs.v22.i3.pp1667-1671"
    },
    {
        "id": 8955,
        "title": "Sentiment Analysis of Cooking Oil using Bidirectional Encoder Representations from Transformers",
        "authors": "Mochammad Haldi Widianto, Yaya Heryadi,  Lukas, Wayan Suparta, Antoni Wibowo",
        "published": "2022-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoiact55506.2022.9971861"
    },
    {
        "id": 8956,
        "title": "Use of BERT (Bidirectional Encoder Representations from Transformers)-Based Deep Learning Method for Extracting Evidences in Chinese Radiology Reports: Development of a Computer-Aided Liver Cancer Diagnosis Framework (Preprint)",
        "authors": "Honglei Liu, Zhiqiang Zhang, Yan Xu, Ni Wang, Yanqun Huang, Zhenghan Yang, Rui Jiang, Hui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nLiver cancer is a substantial disease burden in China. As one of the primary diagnostic tools for detecting liver cancer, dynamic contrast-enhanced computed tomography provides detailed evidences for diagnosis that are recorded in free-text radiology reports.\n\n\nOBJECTIVE\nThe aim of our study was to apply a deep learning model and rule-based natural language processing (NLP) method to identify evidences for liver cancer diagnosis automatically.\n\n\nMETHODS\nWe proposed a pretrained, fine-tuned BERT (Bidirectional Encoder Representations from Transformers)-based BiLSTM-CRF (Bidirectional Long Short-Term Memory-Conditional Random Field) model to recognize the phrases of APHE (hyperintense enhancement in the arterial phase) and PDPH (hypointense in the portal and delayed phases). To identify more essential diagnostic evidences, we used the traditional rule-based NLP methods for the extraction of radiological features. APHE, PDPH, and other extracted radiological features were used to design a computer-aided liver cancer diagnosis framework by random forest.\n\n\nRESULTS\nThe BERT-BiLSTM-CRF predicted the phrases of APHE and PDPH with an F1 score of 98.40% and 90.67%, respectively. The prediction model using combined features had a higher performance (F1 score, 88.55%) than those using APHE and PDPH (84.88%) or other extracted radiological features (83.52%). APHE and PDPH were the top 2 essential features for liver cancer diagnosis.\n\n\nCONCLUSIONS\nThis work was a comprehensive NLP study, wherein we identified evidences for the diagnosis of liver cancer from Chinese radiology reports, considering both clinical knowledge and radiology findings. The BERT-based deep learning method for the extraction of diagnostic evidence achieved state-of-the-art performance. The high performance proves the feasibility of the BERT-BiLSTM-CRF model in information extraction from Chinese radiology reports. The findings of our study suggest that the deep learning–based method for automatically identifying evidences for diagnosis can be extended to other types of Chinese clinical texts.\n",
        "link": "http://dx.doi.org/10.2196/preprints.19689"
    },
    {
        "id": 8957,
        "title": "DERİN TRANSFORMATÖRLERDEN ÇİFT YÖNLÜ KODLAYICI TEMSİLLERİ VE DESTEK VEKTÖR MAKİNELERİ İLE TÜRKÇE FİLM YORUMLARI ÜZERİNE DUYGU ANALİZİ",
        "authors": "Hakan GÜNDÜZ",
        "published": "2023-6-3",
        "citations": 1,
        "abstract": "Görüş madenciliği olarak da bilinen duygu analizi bir dizi kelimenin ardındaki görüşü belirlemenin yoludur. Duygu analizi, metinsel bir ifadede iletilen algıyı, düşünceleri ve duyguları daha iyi anlamak için kullanılır. Bu çalışmada Türkçe film sitesi beyazperde.com'dan derlenen film yorumları üzerinde duygu analizi yapılmıştır. Önerilen yöntem ön eğitimli BERTurk modelini temel almıştır. Yapılan ilk deneyde BERTurk modelinin sondan bir önceki dönüştürücü katmanından derin temsiller çıkarılmış ve bu temsiller Destek Vektör Makineleri (DVM) modeline girdi olarak verilmiştir. İkinci deneyde BERTurk üzerinde ince ayarlama yapılarak sınıflandırma gerçekleştirilirken, son deneyde ince ayarlı BERTurk modelinden ilk deneyde olduğu gibi derin temsiller çıkarılmış ve DVM ile sınıflandırma yapılmıştır. Yapılan deneylerde en yüksek doğruluk oranına 0.984 ile ince ayarlı BERTurk temsilleriyle ulaşılmıştır. İnce ayar işlemi sonunda elde edilen temsiller doğruluk oranında yaklaşık %10'luk artışa neden olurken, sınıflandırmada direkt olarak BERTurk yerine BERTurk'ten elde edilen temsiller ile DVM’nin birleşiminin kullanılması yaklaşık %5'lik doğruluk artışıyla sonuçlanmıştır.",
        "link": "http://dx.doi.org/10.17780/ksujes.1241043"
    },
    {
        "id": 8958,
        "title": "Classifying Textual Sentiment Using Bidirectional Encoder Representations from Transformers",
        "authors": "Shawly Ahsan, Fairooz Tasnia, Nafisa Tabassum, Avishek Das, Mohammed Moshiul Hoque, Nazmul Siddique",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441046"
    },
    {
        "id": 8959,
        "title": "(Retracted) News image text classification algorithm with bidirectional encoder representations from transformers model",
        "authors": "Zhan Shi, Chongjun Fan, Ying Li, Hongliu Zhang",
        "published": "2022-9-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.jei.32.1.011217"
    },
    {
        "id": 8960,
        "title": "Analisis Sentimen Review Film Berbahasa Inggris Dengan Pendekatan Bidirectional Encoder Representations from Transformers",
        "authors": "Cindy Alifia Putri",
        "published": "2020-1-8",
        "citations": 2,
        "abstract": "Sentimen Analisis adalah proses analisis terhadap suatu pendapat atau sikap seseorang. Sentimen analisis digunakan untuk mendapatkan suatu hasil analisa terhadap berbagai macam pendapat atau sikap seseorang dalam memberikan komentar atau opininya. Pada penelitian ini, penulis melakukan klasifikasi sentiment analysis terhadap review film dengan menggunakan Dataset cornell edu dari pabo untuk movie review dengan proses klasifikasi menggunakan algoritma Bidirectional Encoder Representations from Transformers (BERT) yang dilakukan fine tuning dengan beberapa layer untuk klasifikasi. Dari penelitian ini, didapatkan hasil akurasi yang dihitung dengan sparse categorical cross entropy sebesar 73%.",
        "link": "http://dx.doi.org/10.35957/jatisi.v6i2.206"
    },
    {
        "id": 8961,
        "title": "MalBERT: Malware Detection using Bidirectional Encoder Representations from Transformers",
        "authors": "Abir Rahali, Moulay A. Akhloufi",
        "published": "2021-10-17",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9659287"
    },
    {
        "id": 8962,
        "title": "Bilingual Question Answering System Using Bidirectional Encoder Representations from Transformers and Best Matching Method",
        "authors": "Dini Adni Navastara,  Ihdiannaja, Agus Zainal Arifin",
        "published": "2021-10-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icts52701.2021.9608905"
    },
    {
        "id": 8963,
        "title": "Quasi Bidirectional Encoder Representations from Transformers for Word Sense Disambiguation",
        "authors": "Michele Bevilacqua,  , Roberto Navigli",
        "published": "2019-10-22",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-056-4_015"
    },
    {
        "id": 8964,
        "title": "Using Multilingual Bidirectional Encoder Representations from Transformers on Medical Corpus for Kurdish Text Classification",
        "authors": "Soran S. Badawi",
        "published": "2023-1-15",
        "citations": 3,
        "abstract": "Technology has dominated a huge part of human life. Furthermore, technology users use language continuously to express feelings and sentiments about things. The science behind identifying human attitudes toward a particular product, service,or topic is one of the most active fields of research, and it is called sentiment analysis. While the English language is making real progress in sentiment analysis daily, other less-resourced languages, such as Kurdish, still suffer from fundamental issues and challenges in Natural Language Processing (NLP). This paper experimentswith the recently published medical corpus using the classical machine learning method and the latest deep learning tool in NLP and Bidirectional Encoder Representations from Transformers (BERT). We evaluated the findings of both machine learning and deep learning. The outcome indicates that BERT outperforms all the machine learning classifiers by scoring (92%) in accuracy, which is by two points higher than machine learning classifiers.",
        "link": "http://dx.doi.org/10.14500/aro.11088"
    },
    {
        "id": 8965,
        "title": "Multilingual Obnoxious Message Classification using Bidirectional Encoder Representation from Transformers (BERT)",
        "authors": "Nikhil Banka, Sparsh Narang, Mridul Gupta, Abhay Sharma, Deepak Upadhyay",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2e357697.2023.10262469"
    },
    {
        "id": 8966,
        "title": "ADVANCED TURKISH FAKE NEWS PREDICTION WITH BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS",
        "authors": "Mehmet BOZUYLA",
        "published": "2022-9-1",
        "citations": 1,
        "abstract": "The increasing usage of social media and internet generates a significant amount of information to be analyzed from various perspectives. In particular, fake news is defined as the false news that is presented as factual news. Fake news are in general fabricated toward a manipulation aim. Fake news identification is in general a natural language analysis problem and machine learning algorithms are emerged as automated predictors. Well-known machine learning algorithms such as Naïve Bayes (NB) and Random Forest (RF) are successfully used for fake-news identification problem. Turkish is a morphologically rich language and it has agglutinative complexity that requires dense language pre-processing steps and feature selection. Recent neural language models such as Bidirectional Encoder Representations from Transformers (BERT) proposes an opportunity for Turkish-like morphologically rich languages a relatively straightforward pipeline in the solution of natural language problems. In this work, we compared NB, RF, Support Vector Machine (SVM), Naïve Bayes Multinomial (NBM) and Logistics Regression (LR) on top of correlation based feature selection and newly proposed Turkish-BERT (BERTurk) to identify Turkish fake news. And we obtained 99.90 % accuracy in fake news identification which is a highly efficient model without substantial language pre-processing tasks.",
        "link": "http://dx.doi.org/10.36306/konjes.995060"
    },
    {
        "id": 8967,
        "title": "Analisis Sentimen Ulasan Aplikasi DLU Ferry Pada Google Play Store Menggunakan Bidirectional Encoder Representations from Transformers",
        "authors": "Ekka Pujo Ariesanto Akhmad",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "DLU Ferry merupakan aplikasi yang dikeluarkan oleh PT. Dharma Lautan Utama untuk memudahkan pelanggan dalam melakukan pemesanan tiket. Aplikasi DLU Ferry masih memiliki kelemahan, oleh karena itu perlu perbaikan untuk meningkatkan kualitas aplikasi ini. Untuk mengetahui kelemahan aplikasi ini dapat diperoleh dari ulasan yang ditulis konsumen pada Google Play Store. Penelitian ini berisi tentang analisis data ulasan konsumen terhadap aplikasi DLU Ferry. Penelitian ini menggunakan data sebanyak 1575 ulasan yang terdiri dari sentimen positif, netral, dan negatif. Penelitian ini akan mengukur performa metode Bidirectional Encoder Representations from Transformers (BERT) dalam melakukan klasifikasi sentimen menggunakan Pretrained model IndoBERT-base dengan teknik fine-tuning. Hasil pengujian pada penelitian memperoleh akurasi sebesar 86% dengan pemilihan hyperparameter, yaitu batch size 32, learning rate 3e-6, dan epoch 5.",
        "link": "http://dx.doi.org/10.30649/japk.v13i2.94"
    },
    {
        "id": 8968,
        "title": "Sentiment analysis of Malayalam tweets using bidirectional encoder representations from transformers: a study",
        "authors": "Syam Mohan Elankath, Sunitha Ramamirtham",
        "published": "2023-3-1",
        "citations": 2,
        "abstract": "Sentiment analysis on views and opinions expressed in Indian regional languages has become the current focus of research. But, compared to a globally accepted language like English, research on sentiment analysis in Indian regional languages like Malayalam are very low. One of the major hindrances is the lack of publicly available Malayalam datasets. This work focuses on building a Malayalam dataset for facilitating sentiment analysis on Malayalam texts and studying the efficiency of a pre-trained deep learning model in analyzing the sentiments latent in Malayalam texts. In this work, a Malayalam dataset has been created by extracting 2,000 tweets from Twitter. The bidirectional encoder representations from transformers (BERT) is a pretrained model that has been used for various natural language processing tasks. This work employs a transformer-based BERT model for Malayalam sentiment analysis. The efficacy of BERT in analyzing the sentiments latent in Malayalam texts has been studied by comparing the performance of BERT with various machine learning models as well as deep learning models. By analyzing the results, it is found that a substantial increase in accuracy of 5% for BERT when compared with that of Bi-GRU, which is the next bestperforming model.",
        "link": "http://dx.doi.org/10.11591/ijeecs.v29.i3.pp1817-1826"
    },
    {
        "id": 8969,
        "title": "Sarcasm Detection Using Bidirectional Encoder Representations from Transformers and Graph Convolutional Networks",
        "authors": "Anuraj Mohan, Abhilash M Nair, Bhadra Jayakumar, Sanjay Muraleedharan",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.12.405"
    },
    {
        "id": 8970,
        "title": "Guided Interactive Learning through Chatbot using Bi-directional Encoder Representations from Transformers (BERT)",
        "authors": "Richeeka Bathija, Pranav Agarwal, Rakshith Somanna, G B Pallavi",
        "published": "2020-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icimia48430.2020.9074905"
    },
    {
        "id": 8971,
        "title": "Retracted: Software solution for text summarisation using machine learning based Bidirectional Encoder Representations from Transformers algorithm",
        "authors": "Abdulwahid Al Abdulwahid",
        "published": "2023-8",
        "citations": 1,
        "abstract": "AbstractRetraction: [Abdulwahid Al Abdulwahid, Software solution for text summarisation using machine learning based Bidirectional Encoder Representations from Transformers algorithm, IET Software 2023 (https://doi.org/10.1049/sfw2.12098)].The above article from IET Software, published online on 2 February 2023 in Wiley Online Library (wileyonlinelibrary.com), has been retracted by agreement between the Editor‐in‐Chief, Hana Chockler, the Institution of Engineering and Technology (the IET) and John Wiley and Sons Ltd. This article was published as part of a Guest Edited special issue. Following an investigation, the IET and the journal have determined that the article was not reviewed in line with the journal’s peer review standards and there is evidence that the peer review process of the special issue underwent systematic manipulation. Accordingly, we cannot vouch for the integrity or reliability of the content. As such we have taken the decision to retract the article. The authors have been informed of the decision to retract.",
        "link": "http://dx.doi.org/10.1049/sfw2.12098"
    },
    {
        "id": 8972,
        "title": "Question answering method for infrastructure damage information retrieval from textual data using bidirectional encoder representations from transformers",
        "authors": "Yohan Kim, Seongdeok Bang, Jiu Sohn, Hyoungkwan Kim",
        "published": "2022-2",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.autcon.2021.104061"
    },
    {
        "id": 8973,
        "title": "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation From Transformers by Self-Supervised Learning of Sketch Gestalt",
        "authors": "Hangyu Lin, Yanwei Fu, Xiangyang Xue, Yu-Gang Jiang",
        "published": "2020-6",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr42600.2020.00679"
    },
    {
        "id": 8974,
        "title": "A Fine-Tuned Bidirectional Encoder Representations From Transformers Model for Food Named-Entity Recognition: Algorithm Development and Validation (Preprint)",
        "authors": "Riste Stojanov, Gorjan Popovski, Gjorgjina Cenikj, Barbara Koroušić Seljak, Tome Eftimov",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nRecently, food science has been garnering a lot of attention. There are many open research questions on food interactions, as one of the main environmental factors, with other health-related entities such as diseases, treatments, and drugs. In the last 2 decades, a large amount of work has been done in natural language processing and machine learning to enable biomedical information extraction. However, machine learning in food science domains remains inadequately resourced, which brings to attention the problem of developing methods for food information extraction. There are only few food semantic resources and few rule-based methods for food information extraction, which often depend on some external resources. However, an annotated corpus with food entities along with their normalization was published in 2019 by using several food semantic resources.\n\n\nOBJECTIVE\nIn this study, we investigated how the recently published bidirectional encoder representations from transformers (BERT) model, which provides state-of-the-art results in information extraction, can be fine-tuned for food information extraction.\n\n\nMETHODS\nWe introduce FoodNER, which is a collection of corpus-based food named-entity recognition methods. It consists of 15 different models obtained by fine-tuning 3 pretrained BERT models on 5 groups of semantic resources: food versus nonfood entity, 2 subsets of Hansard food semantic tags, FoodOn semantic tags, and Systematized Nomenclature of Medicine Clinical Terms food semantic tags.\n\n\nRESULTS\nAll BERT models provided very promising results with 93.30% to 94.31% macro F1 scores in the task of distinguishing food versus nonfood entity, which represents the new state-of-the-art technology in food information extraction. Considering the tasks where semantic tags are predicted, all BERT models obtained very promising results once again, with their macro F1 scores ranging from 73.39% to 78.96%.\n\n\nCONCLUSIONS\nFoodNER can be used to extract and annotate food entities in 5 different tasks: food versus nonfood entities and distinguishing food entities on the level of food groups by using the closest Hansard semantic tags, the parent Hansard semantic tags, the FoodOn semantic tags, or the Systematized Nomenclature of Medicine Clinical Terms semantic tags.\n",
        "link": "http://dx.doi.org/10.2196/preprints.28229"
    },
    {
        "id": 8975,
        "title": "Bidirectional Encoder Representation from Transformers (BERT) Variants for Procedural Long-Form Answer Extraction",
        "authors": "S Nitish, R. Darsini, G S Shashank, V Tejas, Arti Arya",
        "published": "2022-1-27",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/confluence52989.2022.9734142"
    },
    {
        "id": 8976,
        "title": "KARNA at COIN Shared Task 1: Bidirectional Encoder Representations from Transformers with relational knowledge for machine comprehension with common sense",
        "authors": "Yash Jain, Chinmay Singh",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-6008"
    },
    {
        "id": 8977,
        "title": "Adam Adadelta Optimization based bidirectional encoder representations from transformers model for fake news detection on social media",
        "authors": "Steni Mol T S, P.S. Sreeja",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "Social platform have disseminated the news in rapid speed and has been considered an important news resource for many people over worldwide because of easy access and less cost benefits when compared with the traditional news organizations. Fake news is the news deliberately written by bad writers that manipulates the original contents and this rapid dissemination of fake news may mislead the people in the society. As a result, it is critical to investigate the veracity of the data leaked via social media platforms. Even so, the reliability of information reported via this platform is still doubtful and remains a significant obstacle. As a result, this study proposes a promising technique for identifying fake information in social media called Adam Adadelta Optimization based Deep Long Short-Term Memory (Deep LSTM). The tokenization operation in this case is carried out with the Bidirectional Encoder Representations from Transformers (BERT) approach. The measurement of the features is reduced with the assistance of Kernel Linear Discriminant Analysis (LDA), and Singular Value Decomposition (SVD) and the top-N attributes are chosen by employing Renyi joint entropy. Furthermore, the LSTM is applied to identify false information in social media, with Adam Adadelta Optimization, which comprises a combo of Adam Optimization and Adadelta Optimization . The Deep LSTM based on Adam Adadelta Optimization achieved maximum accuracy, sensitivity, specificity of 0.936, 0.942, and 0.925.",
        "link": "http://dx.doi.org/10.3233/mgs-230033"
    },
    {
        "id": 8978,
        "title": "Use of BERT (Bidirectional Encoder Representations from Transformers)-Based Deep Learning Method for Extracting Evidences in Chinese Radiology Reports: Development of a Computer-Aided Liver Cancer Diagnosis Framework",
        "authors": "Honglei Liu, Zhiqiang Zhang, Yan Xu, Ni Wang, Yanqun Huang, Zhenghan Yang, Rui Jiang, Hui Chen",
        "published": "2021-1-12",
        "citations": 30,
        "abstract": "\nBackground\nLiver cancer is a substantial disease burden in China. As one of the primary diagnostic tools for detecting liver cancer, dynamic contrast-enhanced computed tomography provides detailed evidences for diagnosis that are recorded in free-text radiology reports.\n\n\nObjective\nThe aim of our study was to apply a deep learning model and rule-based natural language processing (NLP) method to identify evidences for liver cancer diagnosis automatically.\n\n\nMethods\nWe proposed a pretrained, fine-tuned BERT (Bidirectional Encoder Representations from Transformers)-based BiLSTM-CRF (Bidirectional Long Short-Term Memory-Conditional Random Field) model to recognize the phrases of APHE (hyperintense enhancement in the arterial phase) and PDPH (hypointense in the portal and delayed phases). To identify more essential diagnostic evidences, we used the traditional rule-based NLP methods for the extraction of radiological features. APHE, PDPH, and other extracted radiological features were used to design a computer-aided liver cancer diagnosis framework by random forest.\n\n\nResults\nThe BERT-BiLSTM-CRF predicted the phrases of APHE and PDPH with an F1 score of 98.40% and 90.67%, respectively. The prediction model using combined features had a higher performance (F1 score, 88.55%) than those using APHE and PDPH (84.88%) or other extracted radiological features (83.52%). APHE and PDPH were the top 2 essential features for liver cancer diagnosis.\n\n\nConclusions\nThis work was a comprehensive NLP study, wherein we identified evidences for the diagnosis of liver cancer from Chinese radiology reports, considering both clinical knowledge and radiology findings. The BERT-based deep learning method for the extraction of diagnostic evidence achieved state-of-the-art performance. The high performance proves the feasibility of the BERT-BiLSTM-CRF model in information extraction from Chinese radiology reports. The findings of our study suggest that the deep learning–based method for automatically identifying evidences for diagnosis can be extended to other types of Chinese clinical texts.\n",
        "link": "http://dx.doi.org/10.2196/19689"
    },
    {
        "id": 8979,
        "title": "Bengali language-based Disease diagnosis system for rural people using Bidirectional Encoder Representations from Transformers",
        "authors": "Kailash Pati Mandal, Prasenjit Mukherjee, Baisakhi Chakraborty, Souvik Ganguly",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/otcon56053.2023.10113944"
    },
    {
        "id": 8980,
        "title": "Crime prediction using a hybrid sentiment analysis approach based on the bidirectional encoder representations from transformers",
        "authors": "Mohammed Boukabous, Mostafa Azizi",
        "published": "2022-2-1",
        "citations": 9,
        "abstract": "Sentiment analysis (SA) is widely used today in many areas such as crime detection (security intelligence) to detect potential security threats in realtime using social media platforms such as Twitter. The most promising techniques in sentiment analysis are those of deep learning (DL), particularly bidirectional encoder representations from transformers (BERT) in the field of natural language processing (NLP). However, employing the BERT algorithm to detect crimes requires a crime dataset labeled by the lexiconbased approach. In this paper, we used a hybrid approach that combines both lexicon-based and deep learning, with BERT as the DL model. We employed the lexicon-based approach to label our Twitter dataset with a set of normal and crime-related lexicons; then, we used the obtained labeled dataset to train our BERT model. The experimental results show that our hybrid technique outperforms existing approaches in several metrics, with 94.91% and 94.92% in accuracy and F1-score respectively.",
        "link": "http://dx.doi.org/10.11591/ijeecs.v25.i2.pp1131-1139"
    },
    {
        "id": 8981,
        "title": "Improving case duration accuracy of orthopedic surgery using bidirectional encoder representations from Transformers (BERT) on Radiology Reports",
        "authors": "William Zhong, Phil Y. Yao, Sri Harsha Boppana, Fernanda V. Pacheco, Brenton S. Alexander, Sierra Simpson, Rodney A. Gabriel",
        "published": "2024-2",
        "citations": 2,
        "abstract": "Abstract\nPurpose\nA major source of inefficiency in the operating room is the mismatch between scheduled versus actual surgical time. The purpose of this study was to demonstrate a proof-of-concept study for predicting case duration by applying natural language processing (NLP) and machine learning that interpret radiology reports for patients undergoing radius fracture repair.\n\nMethods\nLogistic regression, random forest, and feedforward neural networks were tested without NLP and with bag-of-words. Another NLP method tested used feedforward neural networks and Bidirectional Encoder Representations from Transformers specifically pre-trained on clinical notes (ClinicalBERT). A total of 201 cases were included. The data were split into 70% training and 30% test sets. The average root mean squared error (RMSE) were calculated (and 95% confidence interval [CI]) from 10-fold cross-validation on the training set. The models were then tested on the test set to determine proportion of times surgical cases would have scheduled accurately if ClinicalBERT was implemented versus historic averages.\n\nResults\nThe average RMSE was lowest using feedforward neural networks using outputs from ClinicalBERT (25.6 min, 95% CI: 21.5–29.7), which was significantly (P < 0.001) lower than the baseline model (39.3 min, 95% CI: 30.9–47.7). Using the feedforward neural network and ClinicalBERT on the test set, the percentage of accurately predicted cases, which was defined by the actual surgical duration within 15% of the predicted surgical duration, increased from 26.8 to 58.9% (P < 0.001).\n\nConclusion\nThis proof-of-concept study demonstrated the successful application of NLP and machine leaning to extract features from unstructured clinical data resulting in improved prediction accuracy for surgical case duration.\n",
        "link": "http://dx.doi.org/10.1007/s10877-023-01070-w"
    },
    {
        "id": 8982,
        "title": "Neutral Group Prediction with Bidirectional Encoder Representations from Transformer",
        "authors": "Tazizur Rahman, Yang Sok Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4382826"
    },
    {
        "id": 8983,
        "title": "Using Character-Level and Entity-Level Representations to Enhance Bidirectional Encoder Representation From Transformers-Based Clinical Semantic Textual Similarity Model: ClinicalSTS Modeling Study (Preprint)",
        "authors": "Ying Xiong, Shuai Chen, Qingcai Chen, Jun Yan, Buzhou Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nWith the popularity of electronic health records (EHRs), the quality of health care has been improved. However, there are also some problems caused by EHRs, such as the growing use of copy-and-paste and templates, resulting in EHRs of low quality in content. In order to minimize data redundancy in different documents, Harvard Medical School and Mayo Clinic organized a national natural language processing (NLP) clinical challenge (n2c2) on clinical semantic textual similarity (ClinicalSTS) in 2019. The task of this challenge is to compute the semantic similarity among clinical text snippets.\n\n\nOBJECTIVE\nIn this study, we aim to investigate novel methods to model ClinicalSTS and analyze the results.\n\n\nMETHODS\nWe propose a semantically enhanced text matching model for the 2019 n2c2/Open Health NLP (OHNLP) challenge on ClinicalSTS. The model includes 3 representation modules to encode clinical text snippet pairs at different levels: (1) character-level representation module based on convolutional neural network (CNN) to tackle the out-of-vocabulary problem in NLP; (2) sentence-level representation module that adopts a pretrained language model bidirectional encoder representation from transformers (BERT) to encode clinical text snippet pairs; and (3) entity-level representation module to model clinical entity information in clinical text snippets. In the case of entity-level representation, we compare 2 methods. One encodes entities by the entity-type label sequence corresponding to text snippet (called entity I), whereas the other encodes entities by their representation in MeSH, a knowledge graph in the medical domain (called entity II).\n\n\nRESULTS\nWe conduct experiments on the ClinicalSTS corpus of the 2019 n2c2/OHNLP challenge for model performance evaluation. The model only using BERT for text snippet pair encoding achieved a Pearson correlation coefficient (PCC) of 0.848. When character-level representation and entity-level representation are individually added into our model, the PCC increased to 0.857 and 0.854 (entity I)/0.859 (entity II), respectively. When both character-level representation and entity-level representation are added into our model, the PCC further increased to 0.861 (entity I) and 0.868 (entity II).\n\n\nCONCLUSIONS\nExperimental results show that both character-level information and entity-level information can effectively enhance the BERT-based STS model.\n",
        "link": "http://dx.doi.org/10.2196/preprints.23357"
    },
    {
        "id": 8984,
        "title": "Multi-label text classification of Indonesian customer reviews using bidirectional encoder representations from transformers language model",
        "authors": "Nuzulul Khairu Nissa, Evi Yulianti",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "<p><span lang=\"EN-US\">Customer review is a critical resource to support the decision-making process in various industries. To understand how customers perceived each aspect of the product, we can first identify all aspects discussed in the customer reviews by performing multi-label text classification. In this work, we want to know the effectiveness of our two proposed strategies using bidirectional encoder representations from transformers (BERT) language model that was<br /> pre-trained on the Indonesian language, referred to as IndoBERT, to perform multi-label text classification. First, IndoBERT is used as feature representation to be combined with convolutional neural network-extreme gradient boosting (CNN-XGBoost). Second, IndoBERT is used both as the feature representation as well as the classifier to directly solve the classification task. Additional analysis is performed to compare our results with those using multilingual BERT model. According to our experimental results, our first model using IndoBERT as feature representation shows significant performance over some baselines. Our second model using IndoBERT as both feature representation and classifier can significantly enhance the effectiveness of our first model. In summary, our proposed models can improve the effectiveness of the baseline using Word2Vec-CNN-XGBoost by 19.19% and 6.17%, in terms of accuracy and F-1 score, respectively.</span></p>",
        "link": "http://dx.doi.org/10.11591/ijece.v13i5.pp5641-5652"
    },
    {
        "id": 8985,
        "title": "Multimodal Abstractive Summarization using bidirectional encoder representations from transformers with attention mechanism",
        "authors": "Dakshata Argade, Vaishali Khairnar, Deepali Vora, Shruti Patil, Ketan Kotecha, Sultan Alfarhood",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e26162"
    },
    {
        "id": 8986,
        "title": "Towards a System that Predicts the Category of Educational and Vocational Guidance Questions, Utilizing Bidirectional Encoder Representations of Transformers (BERT)",
        "authors": "Omar Zahour, El Habib Benlahmar, Ahmed Eddaoui, Oumaima Hourane",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-50300-9_5"
    },
    {
        "id": 8987,
        "title": "Natural Language Processing of Movie Reviews to Detect the Sentiments using Novel Bidirectional Encoder Representation-BERT for Transformers over Support Vector Machine",
        "authors": "",
        "published": "2022-1-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47750/pnr.2022.13.s04.069"
    },
    {
        "id": 8988,
        "title": "Topic modelling of legal documents using NLP and bidirectional encoder representations from transformers",
        "authors": "Amar Jeet Rawat, Sunil Ghildiyal, Anil Kumar Dixit",
        "published": "2022-12-1",
        "citations": 1,
        "abstract": "<span>Modeling legal text is a difficult task because of its unique features, such as lengthy texts, complex language structures, and technical terms. During the last decade, there has been a big rise in the number of legislative documents, which makes it hard for law professionals to keep up with legislation like analyzing judgements and implementing acts. The relevancy of topics is heavily influenced by the processing and presentation of legal documents in some contexts. The objective of this work is to understand the legal judgement corpus related to cases under the Hindu Marriage Act of India. The study looked into various methods to generate sentence embeddings from the judgement. This paper employs the power of the BERTopic algorithm for generating significant topics.</span>",
        "link": "http://dx.doi.org/10.11591/ijeecs.v28.i3.pp1749-1755"
    },
    {
        "id": 8989,
        "title": "Extracting Multiple Worries From Breast Cancer Patient Blogs Using Multilabel Classification With the Natural Language Processing Model Bidirectional Encoder Representations From Transformers: Infodemiology Study of Blogs (Preprint)",
        "authors": "Tomomi Watanabe, Shuntaro Yada, Eiji Aramaki, Hiroshi Yajima, Hayato Kizaki, Satoko Hori",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nPatients with breast cancer have a variety of worries and need multifaceted information support. Their accumulated posts on social media contain rich descriptions of their daily worries concerning issues such as treatment, family, and finances. It is important to identify these issues to help patients with breast cancer to resolve their worries and obtain reliable information.\n\n\nOBJECTIVE\nThis study aimed to extract and classify multiple worries from text generated by patients with breast cancer using Bidirectional Encoder Representations From Transformers (BERT), a context-aware natural language processing model.\n\n\nMETHODS\nA total of 2272 blog posts by patients with breast cancer in Japan were collected. Five worry labels, “treatment,” “physical,” “psychological,” “work/financial,” and “family/friends,” were defined and assigned to each post. Multiple labels were allowed. To assess the label criteria, 50 blog posts were randomly selected and annotated by two researchers with medical knowledge. After the interannotator agreement had been assessed by means of Cohen kappa, one researcher annotated all the blogs. A multilabel classifier that simultaneously predicts five worries in a text was developed using BERT. This classifier was fine-tuned by using the posts as input and adding a classification layer to the pretrained BERT. The performance was evaluated for precision using the average of 5-fold cross-validation results.\n\n\nRESULTS\nAmong the blog posts, 477 included “treatment,” 1138 included “physical,” 673 included “psychological,” 312 included “work/financial,” and 283 included “family/friends.” The interannotator agreement values were 0.67 for “treatment,” 0.76 for “physical,” 0.56 for “psychological,” 0.73 for “work/financial,” and 0.73 for “family/friends,” indicating a high degree of agreement. Among all blog posts, 544 contained no label, 892 contained one label, and 836 contained multiple labels. It was found that the worries varied from user to user, and the worries posted by the same user changed over time. The model performed well, though prediction performance differed for each label. The values of precision were 0.59 for “treatment,” 0.82 for “physical,” 0.64 for “psychological,” 0.67 for “work/financial,” and 0.58 for “family/friends.” The higher the interannotator agreement and the greater the number of posts, the higher the precision tended to be.\n\n\nCONCLUSIONS\nThis study showed that the BERT model can extract multiple worries from text generated from patients with breast cancer. This is the first application of a multilabel classifier using the BERT model to extract multiple worries from patient-generated text. The results will be helpful to identify breast cancer patients’ worries and give them timely social support.\n",
        "link": "http://dx.doi.org/10.2196/preprints.37840"
    },
    {
        "id": 8990,
        "title": "Analisis Sentimen Masyarakat Pengguna Media Sosial Twitter Terhadap Motogp Mandalika Lombok Menggunakan Metode Bidirectional Encoder Representation From Transformers (BERT)",
        "authors": " Nelly Sofi,  Tri Sulistyorini,  Muhammad Nazaruddin",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "The MotoGP One race in West Nusa Tenggara Lombok, Mandalika which was held on March 18 2022, received many responses or reactions from the public on social media, especially Twitter. There are those who agree and disagree about the holding of MotoGP in Mandalika, to find out the responses of the people who agree or disagree is needed that can process tweets data using the sentiment analysis method. The use of BERT (Bidirectional Encoder Representations from Transformers) for sentiment analysis produces a bidirectional language model that can understand the context of all words from a sentence. The dataset used goes through preprocessing stages such as case folding, data cleaning, tokenization, normalization, and removal of stopwords before sentiment analysis is carried out. This study uses several hyperparameters, namely a batch size of 32, the optimizer uses Adam with a learning rate of 3e-6 or 0.000003, and an epoch of 25. The evaluation results of the model obtain an accuracy of 55%. Precision for positive by 56%, neutral by 59%, and negative by 44%. Recall for positive is 74%, neutral is 29%, and negative is 54%. F1-score for positive is 64%, neutral is 38%, and negative is 48%.",
        "link": "http://dx.doi.org/10.55606/isaintek.v6i1.103"
    },
    {
        "id": 8991,
        "title": "ArSentBERT: fine-tuned bidirectional encoder representations from transformers model for Arabic sentiment classification",
        "authors": "Mohamed Fawzy Abdelfattah, Mohamed Waleed Fakhr, Mohamed Abo Rizka",
        "published": "2023-4-1",
        "citations": 2,
        "abstract": "Sentiment analysis in the Arabic language is challenging because of its linguistic complexity. Arabic is complex in words, paragraphs, and sentence structure. Moreover, most Arabic documents contain multiple dialects, writing alphabets, and styles (e.g., Franco-Arab). Nevertheless, fine-tuned bidirectional encoder representations from transformers (BERT) models can provide a reasonable prediction accuracy for Arabic sentiment classification tasks. This paper presents a fine-tuning approach for BERT models for classifying Arabic sentiments. It uses Arabic BERT pre-trained models and tokenizers and includes three stages. The first stage is text preprocessing and data cleaning. The second stage uses transfer-learning of the pre-trained models’ weights and trains all encoder layers. The third stage uses a fully connected layer and a drop-out layer for classification. We tested our fine-tuned models on five different datasets that contain reviews in Arabic with different dialects and compared the results to 11 state-of-the-art models. The experiment results show that our models provide better prediction accuracy than our competitors. We show that the choice of the pre-trained BERT model and the tokenizer type improves the accuracy of Arabic sentiment classification.",
        "link": "http://dx.doi.org/10.11591/eei.v12i2.3914"
    },
    {
        "id": 8992,
        "title": "A Study on the journey of Natural Language Processing models: from Symbolic Natural Language Processing to Bidirectional Encoder Representations from Transformers",
        "authors": " Rajarshi SinhaRoy",
        "published": "2021-12-26",
        "citations": 0,
        "abstract": "In this digital era, Natural language Processing is not just a computational process rather it is a way to communicate with machines as humanlike. It has been used in several fields from smart artificial assistants to health or emotion analyzers. Imagine a digital era without Natural language processing is something which we cannot even think of. In Natural language Processing, firstly it reads the information given and after that begins making sense of the information. After the data has been properly processed, the real steps are taken by the machine throwing some responses or completing the work. In this paper, I review the journey of natural language processing from the late 1940s to the present. This paper also contains several salient and most important works in this timeline which leads us to where we currently stand in this field. The review separates four eras in the history of Natural language Processing, each marked by a focus on machine translation, artificial intelligence impact, the adoption of a logico-grammatical style, and an attack on huge linguistic data. This paper helps to understand the historical aspects of Natural language processing and also inspires others to work and research in this domain.",
        "link": "http://dx.doi.org/10.32628/cseit217688"
    },
    {
        "id": 8993,
        "title": "Detecting emotions using a combination of bidirectional encoder representations from transformers embedding and bidirectional long short-term memory",
        "authors": "Aji Prasetya Wibawa, Denis Eka Cahyani, Didik Dwi Prasetya, Langlang Gumilar, Andrew Nafalski",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "<span>One of the most difficult topics in natural language understanding (NLU) is emotion detection in text because human emotions are difficult to understand without knowing facial expressions. Because the structure of Indonesian differs from other languages, this study focuses on emotion detection in Indonesian text. The nine experimental scenarios of this study incorporate word embedding (bidirectional encoder representations from transformers (BERT), Word2Vec, and GloVe) and emotion detection models (bidirectional long short-term memory (BiLSTM), LSTM, and convolutional neural network (CNN)). <span>With values of 88.28%, 88.42%, and 89.20% for Commuter Line, Transjakarta, and Commuter Line+Transjakarta, respectively,</span> BERT-BiLSTM generates the highest accuracy on the data. In general, BiLSTM produces the highest accuracy, followed by LSTM, and finally CNN. When it came to word embedding, BERT embedding outperformed Word2Vec and GloVe. In addition, the BERT-BiLSTM model generates the highest precision, recall, and F1-measure values in each data scenario when compared to other models. According to the results of this study, BERT-BiLSTM can enhance the performance of the classification model when compared to previous studies that only used BERT or BiLSTM for emotion detection in Indonesian texts.</span>",
        "link": "http://dx.doi.org/10.11591/ijece.v13i6.pp7137-7146"
    },
    {
        "id": 8994,
        "title": "Analyzing Arabic Twitter-Based Patient Experience Sentiments Using Multi-Dialect Arabic Bidirectional Encoder Representations from Transformers",
        "authors": "Sarab AlMuhaideb, Yasmeen AlNegheimish, Taif AlOmar, Reem AlSabti, Maha AlKathery, Ghala AlOlyyan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.038368"
    },
    {
        "id": 8995,
        "title": "Personality Prediction Based on Text Analytics Using Bidirectional Encoder Representations from Transformers from English Twitter Dataset",
        "authors": "Joshua Evan Arijanto, Steven Geraldy, Cyrena Tania, Derwin Suhartono",
        "published": "2021-9-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5391/ijfis.2021.21.3.310"
    },
    {
        "id": 8996,
        "title": "Bidirectional Encoder Representations from Transformers in Radiology: A Systematic Review of Natural Language Processing Applications",
        "authors": "Larisa Gorenstein, Eli Konen, Michael Green, Eyal Klang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jacr.2024.01.012"
    },
    {
        "id": 8997,
        "title": "Bidirectional encoder representations from transformers and deep learning model for analyzing smartphone-related tweets",
        "authors": "Sudheesh R, Muhammad Mujahid, Furqan Rustam, Bhargav Mallampati, Venkata Chunduri, Isabel de la Torre Díez, Imran Ashraf",
        "published": "2023-8-3",
        "citations": 2,
        "abstract": "Nearly six billion people globally use smartphones, and reviews about smartphones provide useful feedback concerning important functions, unique characteristics, etc. Social media platforms like Twitter contain a large number of such reviews containing feedback from customers. Conventional methods of analyzing consumer feedback such as business surveys or questionnaires and focus groups demand a tremendous amount of time and resources, however, Twitter’s reviews are unstructured and manual analysis is laborious and time-consuming. Machine learning and deep learning approaches have been applied for sentiment analysis, but classification accuracy is low. This study utilizes a transformer-based BERT model with the appropriate preprocessing pipeline to obtain higher classification accuracy. Tweets extracted using Tweepy SNS scrapper are used for experiments, while fine-tuned machine and deep learning models are also employed. Experimental results demonstrate that the proposed approach can obtain a 99% classification accuracy for three sentiments.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1432"
    },
    {
        "id": 8998,
        "title": "A Literature Review on Bidirectional Encoder Representations from Transformers",
        "authors": "S. Shreyashree, Pramod Sunagar, S. Rajarajeswari, Anita Kanavalli",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-6723-7_23"
    },
    {
        "id": 8999,
        "title": "Emotions Classification using Bidirectional Encoder Representations from Transformers",
        "authors": "Denis Eka Cahyani, Darmawan Satyananda, Lucky Tri Oktoviana, A Susy Kaspambudi, A Syihabul Anwar, W Adefa Sekti",
        "published": "2021-9-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitacee53184.2021.9617473"
    },
    {
        "id": 9000,
        "title": "Acquisition of a Lexicon for Family History Information: Bidirectional Encoder Representations From Transformers–Assisted Sublanguage Analysis (Preprint)",
        "authors": "Liwei Wang, Huan He, Andrew Wen, Sungrim Moon, Sunyang Fu, Kevin J Peterson, Xuguang Ai, Sijia Liu, Ramakanth Kavuluru, Hongfang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nA patient’s family history (FH) information significantly influences downstream clinical care. Despite this importance, there is no standardized method to capture FH information in electronic health records and a substantial portion of FH information is frequently embedded in clinical notes. This renders FH information difficult to use in downstream data analytics or clinical decision support applications. To address this issue, a natural language processing system capable of extracting and normalizing FH information can be used.\n\n\nOBJECTIVE\nIn this study, we aimed to construct an FH lexical resource for information extraction and normalization.\n\n\nMETHODS\nWe exploited a transformer-based method to construct an FH lexical resource leveraging a corpus consisting of clinical notes generated as part of primary care. The usability of the lexicon was demonstrated through the development of a rule-based FH system that extracts FH entities and relations as specified in previous FH challenges. We also experimented with a deep learning–based FH system for FH information extraction. Previous FH challenge data sets were used for evaluation.\n\n\nRESULTS\nThe resulting lexicon contains 33,603 lexicon entries normalized to 6408 concept unique identifiers of the Unified Medical Language System and 15,126 codes of the Systematized Nomenclature of Medicine Clinical Terms, with an average number of 5.4 variants per concept. The performance evaluation demonstrated that the rule-based FH system achieved reasonable performance. The combination of the rule-based FH system with a state-of-the-art deep learning–based FH system can improve the recall of FH information evaluated using the BioCreative/N2C2 FH challenge data set, with the F1 score varied but comparable.\n\n\nCONCLUSIONS\nThe resulting lexicon and rule-based FH system are freely available through the Open Health Natural Language Processing GitHub.\n",
        "link": "http://dx.doi.org/10.2196/preprints.48072"
    }
]