[
    {
        "id": 10160,
        "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
        "authors": "Yukun Qian, Xuyi Zhuang, Mingjiang Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1450"
    },
    {
        "id": 10161,
        "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 7,
        "abstract": "<p>Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.</p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14561901"
    },
    {
        "id": 10162,
        "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 5,
        "abstract": "Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14561901.v1"
    },
    {
        "id": 10163,
        "title": "Pvt2dnet:Polyp Segmentation with Vision Transformer and Dual Decoder Refinement Strategy",
        "authors": "Yibiao Hu, Yan Jin, Zhiwei Jiang, Qiufu Zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458919"
    },
    {
        "id": 10164,
        "title": "Transformer with Bidirectional Decoder for Speech Recognition",
        "authors": "Xi Chen, Songyang Zhang, Dandan Song, Peng Ouyang, Shouyi Yin",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2677"
    },
    {
        "id": 10165,
        "title": "Body Part Information Additional in Multi-decoder Transformer-Based Network for Human Object Interaction Detection",
        "authors": "Zihao Guo, Fei Li, Rujie Liu, Ryo Ishida, Genta Suzuki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011755300003417"
    },
    {
        "id": 10166,
        "title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition",
        "authors": "Xinyuan Zhou, Emre Yılmaz, Yanhua Long, Yijie Li, Haizhou Li",
        "published": "2020-10-25",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2488"
    },
    {
        "id": 10167,
        "title": "Fully Transformer Detector with Multiscale Encoder and Dynamic Decoder",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2023.06.016"
    },
    {
        "id": 10168,
        "title": "Enhancing the Transformer Decoder with Transition-based Syntax",
        "authors": "Leshem Choshen, Omri Abend",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.conll-1.27"
    },
    {
        "id": 10169,
        "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
        "authors": "Jindřich Libovický, Jindřich Helcl, David Mareček",
        "published": "2018",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6326"
    },
    {
        "id": 10170,
        "title": "Transformer-Based Online Speech Recognition with Decoder-end Adaptive Computation Steps",
        "authors": "Mohan Li, Catalin Zorila, Rama Doddipatla",
        "published": "2021-1-19",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383613"
    },
    {
        "id": 10171,
        "title": "Transformer Decoder Based Reinforcement Learning Approach for Conversational Response Generation",
        "authors": "Farshid Faal, Jia Yuan Yu, Ketra Schmitt",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207289"
    },
    {
        "id": 10172,
        "title": "Encoder/Decoder Transformer-Based Framework to Detect Hate Speech from Tweets",
        "authors": " Usman, S. M. K. Quadri",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003371380-19"
    },
    {
        "id": 10173,
        "title": "CNN-Based Encoder and Transformer-Based Decoder for Efficient Semantic Segmentation",
        "authors": "Seunghun Moon, Suk-ju Kang",
        "published": "2024-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceic61013.2024.10457284"
    },
    {
        "id": 10174,
        "title": "Transformer-Based Dual-Path Decoder Framework for Thoracic Diseases Classification Using Chest X-Ray",
        "authors": "Xiaoben Jiang, Yu Zhu, Fengyao Zhu, Gan Cai, Dawei Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4402923"
    },
    {
        "id": 10175,
        "title": "Decoder Transformer for Temporally-Embedded Health Outcome Predictions",
        "authors": "Omar Boursalie, Reza Samavi, Thomas E. Doyle",
        "published": "2021-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00235"
    },
    {
        "id": 10176,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 10177,
        "title": "Glass Objects Detection Based on Transformer Encoder-Decoder",
        "authors": "Xiaonan Hou, Minghao Zhan, Chunlei Wang, Chunhui Fan",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacr55854.2022.9935562"
    },
    {
        "id": 10178,
        "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
        "authors": "Jianyuan Sun, Xubo Liu, Xinhao Mei, Volkan Kılıç, Mark D. Plumbley, Wenwu Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-943"
    },
    {
        "id": 10179,
        "title": "Image Captioning in Nepali Using CNN and Transformer Decoder",
        "authors": "Rabin Budhathoki, Suresh Timilsina",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "Image captioning has attracted huge attention from deep learning researchers. This approach combines image and text-based deep learning techniques to create the written descriptions of images automatically. There has been limited research on image captioning using the Nepali language, with most studies focusing on English datasets. Therefore, there are no publicly available datasets in the Nepali language. Most previous works are based on the RNN-CNN approach, which produces inferior results compared to image captioning using the Transformer model. Similarly, using the BLEU score as the only evaluation metric cannot justify the quality of the produced captions. To address this gap, in this research work, the well-known “Flickr8k” English data set is translated into Nepali language and then manually corrected to ensure accurate translations. The conventional Transformer is comprised of encoder and decoder modules. Both modules contain a multi-head attention mechanism. This makes the model complex and computationally expensive. Hence, we propose a noble approach where the encoder module of the Transformer is completely removed and only the decoder part of the Transformer is used, in conjunction with CNN, which acts as a feature extractor. The image features are extracted using the MobileNetV3 Large while the Transformer decoder processes these feature vectors and the input text sequence to generate appropriate captions. The system's effectiveness is measured using metrics to judge the caliber and precision of the generated captions, such as the BLEU and Meteor scores.",
        "link": "http://dx.doi.org/10.3126/jes2.v2i1.60391"
    },
    {
        "id": 10180,
        "title": "More than Encoder: Introducing Transformer Decoder to Upsample",
        "authors": "Yijiang Li, Wentian Cai, Ying Gao, Chengming Li, Xiping Hu",
        "published": "2022-12-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995378"
    },
    {
        "id": 10181,
        "title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-Based LVCSR",
        "authors": "Xinyuan Zhou, Grandee Lee, Emre Yılmaz, Yanhua Long, Jiaen Liang, Haizhou Li",
        "published": "2020-10-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2556"
    },
    {
        "id": 10182,
        "title": "Comparative Analysis of Pretrained Encoder-Decoder Transformer Models for Extreme Text Summarization",
        "authors": "Tamma RajyaLakshmi, K.S. Kuppusamy",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacic59454.2023.10435363"
    },
    {
        "id": 10183,
        "title": "Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer",
        "authors": " Mohanad Sameer,  Ahmed Talib,  Alla Hussein",
        "published": "2023-3-21",
        "citations": 2,
        "abstract": "Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when utilizing the Common Voice dataset from Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 hours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition systems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training requires a very large amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language model.",
        "link": "http://dx.doi.org/10.51173/jt.v5i1.749"
    },
    {
        "id": 10184,
        "title": "Transformer with Enhanced Encoder and Monotonic Decoder for Automatic Speech Recognition",
        "authors": "Priyabrata Karmakar, Shyh Wei Teng, Guojun Lu",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dicta56598.2022.10034576"
    },
    {
        "id": 10185,
        "title": "On the Sub-layer Functionalities of Transformer Decoder",
        "authors": "Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, Zhaopeng Tu",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.432"
    },
    {
        "id": 10186,
        "title": "Effectiveness of Decoder Transformer Network in breaking Low-resource Real-time text Captcha System",
        "authors": "Rajat Subhra Bhowmick, Isha Ganguli, Jayanta Paul, Jaya Sil",
        "published": "2021-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cw52790.2021.00054"
    },
    {
        "id": 10187,
        "title": "Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks",
        "authors": "Barry Ridge, Rok Pahic, Ales Ude, Jun Morimoto",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794253"
    },
    {
        "id": 10188,
        "title": "TransDoubleU-Net: Dual Scale Swin Transformer With Dual Level Decoder for 3D Multimodal Brain Tumor Segmentation",
        "authors": "Marjan Vatanpour, Javad Haddadnia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3330958"
    },
    {
        "id": 10189,
        "title": "Classification and Generation of Arabic News Titles from Raw Text Based on an Encoder-Decoder Transformer Model (mT5)",
        "authors": "‪Ayedh Abdulaziz Mohsen‬‏, Marwah Yahya Al-Nahari, Akram Alsubari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMultilingual Transformer 5 (MT5) is a versatile architecture in natural language processing (NLP) that demonstrates proficiency across various languages. This study aimed to improve the performance of the MT5 model in two key tasks: topic classification and headline generation. The datasets used were 183K and 294K samples. The classification task involved categorizing news articles, while the news generation task aimed to create coherent and contextually relevant Arabic news content. Through careful fine-tuning and rigorous evaluation, the MT5 model significantly advances its ability to address complex challenges in Arabic NLP. This study provides practical insights into real-world applications in processing Arab news. The performance of the MT5 model was evaluated using various online platforms. The mT5small model achieved an accuracy of 0.7858 and an F1 score of 0.7858, while the mT5base model achieved an accuracy of 0.8230 and an F1 score of 0.8230. The generative approach for headline generation yielded Rouge-1, Rouge-2, and Rouge-L scores under the task \"Generative of Headlines.\" These outcomes demonstrate the effectiveness of the fine-tuned MT5 model across various evaluation metrics and tasks, confirming its potential for practical applications in Arabic NLP.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3982909/v1"
    },
    {
        "id": 10190,
        "title": "A Comparison of Transformer and LSTM Encoder Decoder Models for ASR",
        "authors": "Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schluter, Hermann Ney",
        "published": "2019-12",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9004025"
    },
    {
        "id": 10191,
        "title": "Lightweight Transformer with GRU Integrated Decoder for Image Captioning",
        "authors": "Dhruv Sharma, Rishabh Dingliwal, Chhavi Dhiman, Dinesh Kumar",
        "published": "2022-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sitis57111.2022.00072"
    },
    {
        "id": 10192,
        "title": "DeMerge Transformer: A Learnable Decoder for Near-infrared Blurred Vessel Segmentation with Domain Adaptation",
        "authors": "Jiazhe Wang, Yoshie Osamu, Koichi Shimizu",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icivc58118.2023.10270774"
    },
    {
        "id": 10193,
        "title": "Stacked encoder–decoder transformer with boundary smoothing for action segmentation",
        "authors": "Gyeong‐hyeon Kim, Eunwoo Kim",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/ell2.12678"
    },
    {
        "id": 10194,
        "title": "Recurrent Glimpse-based Decoder for Detection with Transformer",
        "authors": "Zhe Chen, Jing Zhang, Dacheng Tao",
        "published": "2022-6",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00519"
    },
    {
        "id": 10195,
        "title": "Learning to Generate Diverse and Authentic Reviews via an Encoder-Decoder Model with Transformer and GRU",
        "authors": "Kaifu Jin, Xi Zhang, Jiayuan Zhang",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9006577"
    },
    {
        "id": 10196,
        "title": "MSEDTNet: Multi-Scale Encoder and Decoder with Transformer for Bladder Tumor Segmentation",
        "authors": "Yixing Wang, Xiufen Ye",
        "published": "2022-10-17",
        "citations": 1,
        "abstract": "The precise segmentation of bladder tumors from MRI is essential for bladder cancer diagnosis and personalized therapy selection. Limited by the properties of tumor morphology, achieving precise segmentation from MRI images remains challenging. In recent years, deep convolutional neural networks have provided a promising solution for bladder tumor segmentation from MRI. However, deep-learning-based methods still face two weakness: (1) multi-scale feature extraction and utilization are inadequate, being limited by the learning approach. (2) The establishment of explicit long-distance dependence is difficult due to the limited receptive field of convolution kernels. These limitations raise challenges in the learning of global semantic information, which is critical for bladder cancer segmentation. To tackle the problem, a newly auxiliary segmentation algorithm integrating a multi-scale encoder and decoder with a transformer is proposed, which is called MSEDTNet. Specifically, the designed encoder with multi-scale pyramidal convolution (MSPC) is utilized to generate compact feature maps which capture the richly detailed local features of the image. Furthermore, the transformer bottleneck is then leveraged to model the long-distance dependency between high-level tumor semantics from a global space. Finally, a decoder with a spatial context fusion module (SCFM) is adopted to fuse the context information and gradually produce high-resolution segmentation results. The experimental results of T2-weighted MRI scans from 86 patients show that MSEDTNet achieves an overall Jaccard index of 83.46%, a Dice similarity coefficient of 92.35%, and a complexity less than that of other, similar models. This suggests that the method proposed in this article can be used as an efficient tool for clinical bladder cancer segmentation.",
        "link": "http://dx.doi.org/10.3390/electronics11203347"
    },
    {
        "id": 10197,
        "title": "Video Anomaly Detection Using Encoder-Decoder Networks with Video Vision Transformer and Channel Attention Blocks",
        "authors": "Shimpei Kobayashi, Akiyoshi Hizukuri, Ryohei Nakayama",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10215921"
    },
    {
        "id": 10198,
        "title": "Transformer Decoder-Based Enhanced Exploration Method to Alleviate Initial Exploration Problems in Reinforcement Learning",
        "authors": "Dohyun Kyoung, Yunsick Sung",
        "published": "2023-8-25",
        "citations": 1,
        "abstract": "In reinforcement learning, the epsilon (ε)-greedy strategy is commonly employed as an exploration technique This method, however, leads to extensive initial exploration and prolonged learning periods. Existing approaches to mitigate this issue involve constraining the exploration range using expert data or utilizing pretrained models. Nevertheless, these methods do not effectively reduce the initial exploration range, as the exploration by the agent is limited to states adjacent to those included in the expert data. This paper proposes a method to reduce the initial exploration range in reinforcement learning through a pretrained transformer decoder on expert data. The proposed method involves pretraining a transformer decoder with massive expert data to guide the agent’s actions during the early learning stages. After achieving a certain learning threshold, the actions are determined using the epsilon-greedy strategy. An experiment was conducted in the basketball game FreeStyle1 to compare the proposed method with the traditional Deep Q-Network (DQN) using the epsilon-greedy strategy. The results indicated that the proposed method yielded approximately 2.5 times the average reward and a 26% higher win rate, proving its enhanced performance in reducing exploration range and optimizing learning times. This innovative method presents a significant improvement over traditional exploration techniques in reinforcement learning.",
        "link": "http://dx.doi.org/10.3390/s23177411"
    },
    {
        "id": 10199,
        "title": "Transformer-based Sparse Encoder and Answer Decoder for Visual Question Answering",
        "authors": "Longkun Peng, Gaoyun An, Qiuqi Ruan",
        "published": "2022-10-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp56322.2022.9965298"
    },
    {
        "id": 10200,
        "title": "MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation",
        "authors": "Rezaul Karim, He Zhao, Richard P. Wildes, Mennatullah Siam",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00612"
    },
    {
        "id": 10201,
        "title": "TNFormer: Single-Pass Multilingual Text Normalization with a Transformer Decoder Model",
        "authors": "Binbin Shen, Jie Wang, Meng Meng, Yujun Wang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446848"
    },
    {
        "id": 10202,
        "title": "Camera-Lidar fusion algorithm formed by fast sparse encoder and transformer decoder",
        "authors": "W. Chen, S. Chen, L. X. Chen, J. Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.2945"
    },
    {
        "id": 10203,
        "title": "Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model",
        "authors": "Seonho Kim, Juntae Yoon, Ohyoung Kwon",
        "published": "2023-5-12",
        "citations": 2,
        "abstract": "The identification of drug–drug and chemical–protein interactions is essential for understanding unpredictable changes in the pharmacological effects of drugs and mechanisms of diseases and developing therapeutic drugs. In this study, we extract drug-related interactions from the DDI (Drug–Drug Interaction) Extraction-2013 Shared Task dataset and the BioCreative ChemProt (Chemical–Protein) dataset using various transfer transformers. We propose BERTGAT that uses a graph attention network (GAT) to take into account the local structure of sentences and embedding features of nodes under the self-attention scheme and investigate whether incorporating syntactic structure can help relation extraction. In addition, we suggest T5slim_dec, which adapts the autoregressive generation task of the T5 (text-to-text transfer transformer) to the relation classification problem by removing the self-attention layer in the decoder block. Furthermore, we evaluated the potential of biomedical relation extraction of GPT-3 (Generative Pre-trained Transformer) using GPT-3 variant models. As a result, T5slim_dec, which is a model with a tailored decoder designed for classification problems within the T5 architecture, demonstrated very promising performances for both tasks. We achieved an accuracy of 91.15% in the DDI dataset and an accuracy of 94.29% for the CPR (Chemical–Protein Relation) class group in ChemProt dataset. However, BERTGAT did not show a significant performance improvement in the aspect of relation extraction. We demonstrated that transformer-based approaches focused only on relationships between words are implicitly eligible to understand language well without additional knowledge such as structural information.",
        "link": "http://dx.doi.org/10.3390/bioengineering10050586"
    },
    {
        "id": 10204,
        "title": "Traffic Accident Detection Using Background Subtraction and CNN Encoder–Transformer Decoder in Video Frames",
        "authors": "Yihang Zhang, Yunsick Sung",
        "published": "2023-6-27",
        "citations": 3,
        "abstract": "Artificial intelligence plays a significant role in traffic-accident detection. Traffic accidents involve a cascade of inadvertent events, making traditional detection approaches challenging. For instance, Convolutional Neural Network (CNN)-based approaches cannot analyze temporal relationships among objects, and Recurrent Neural Network (RNN)-based approaches suffer from low processing speeds and cannot detect traffic accidents simultaneously across multiple frames. Furthermore, these networks dismiss background interference in input video frames. This paper proposes a framework that begins by subtracting the background based on You Only Look Once (YOLOv5), which adaptively reduces background interference when detecting objects. Subsequently, the CNN encoder and Transformer decoder are combined into an end-to-end model to extract the spatial and temporal features between different time points, allowing for a parallel analysis between input video frames. The proposed framework was evaluated on the Car Crash Dataset through a series of comparison and ablation experiments. Our framework was benchmarked against three accident-detection models to evaluate its effectiveness, and the proposed framework demonstrated a superior accuracy of approximately 96%. The results of the ablation experiments indicate that when background subtraction was not incorporated into the proposed framework, the values of all evaluation indicators decreased by approximately 3%.",
        "link": "http://dx.doi.org/10.3390/math11132884"
    },
    {
        "id": 10205,
        "title": "Explainable Encoder-Decoder Crack Segmentation: Convolutional Network Vs. Transformer",
        "authors": "Zaid Al-Huda, Mugahed A. Al-antari, Bo Peng, Radhwan A.A. Saleh",
        "published": "2023-10-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/esmarta59349.2023.10293616"
    },
    {
        "id": 10206,
        "title": "Dealing with Unreliable Annotations: A Noise-Robust Network for Semantic Segmentation through A Transformer-Improved Encoder and Convolution Decoder",
        "authors": "Ziyang Wang, Irina Voiculescu",
        "published": "2023-7-7",
        "citations": 2,
        "abstract": "Conventional deep learning methods have shown promising results in the medical domain when trained on accurate ground truth data. Pragmatically, due to constraints like lack of time or annotator inexperience, the ground truth data obtained from clinical environments may not always be impeccably accurate. In this paper, we investigate whether the presence of noise in ground truth data can be mitigated. We propose an innovative and efficient approach that addresses the challenge posed by noise in segmentation labels. Our method consists of four key components within a deep learning framework. First, we introduce a Vision Transformer-based modified encoder combined with a convolution-based decoder for the segmentation network, capitalizing on the recent success of self-attention mechanisms. Second, we consider a public CT spine segmentation dataset and devise a preprocessing step to generate (and even exaggerate) noisy labels, simulating real-world clinical situations. Third, to counteract the influence of noisy labels, we incorporate an adaptive denoising learning strategy (ADL) into the network training. Finally, we demonstrate through experimental results that the proposed method achieves noise-robust performance, outperforming existing baseline segmentation methods across multiple evaluation metrics.",
        "link": "http://dx.doi.org/10.3390/app13137966"
    },
    {
        "id": 10207,
        "title": "Traffic Flow Prediction Based on Transformer and Multi-Spatial-Temporal Encoder-Decoder",
        "authors": "Yidi Zhang, Xiang Gu",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135489"
    },
    {
        "id": 10208,
        "title": "Incremental Transformer with Deliberation Decoder for Document Grounded Conversations",
        "authors": "Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li, Jie Zhou",
        "published": "2019",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1002"
    },
    {
        "id": 10209,
        "title": "D-MmT: A concise decoder-only multi-modal transformer for abstractive summarization in videos",
        "authors": "Nayu Liu, Xian Sun, Hongfeng Yu, Wenkai Zhang, Guangluan Xu",
        "published": "2021-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.04.072"
    },
    {
        "id": 10210,
        "title": "An Efficient Transformer Decoder with Compressed Sub-layers",
        "authors": "Yanyang Li, Ye Lin, Tong Xiao, Jingbo Zhu",
        "published": "2021-5-18",
        "citations": 9,
        "abstract": "The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i15.17572"
    },
    {
        "id": 10211,
        "title": "Swinsam: Fine-Grained Polyp Segmentation in Colonoscopy Images Via Segment Anything Model Integrated with a Swin Transformer Decoder",
        "authors": "Zhoushan Feng, Yuliang Zhang, Yu Liu, Wen Sun, Yanhong Chen, Lili Du, Dunjin Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673046"
    },
    {
        "id": 10212,
        "title": "Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation",
        "authors": "Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent Besacier",
        "published": "2020",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.314"
    },
    {
        "id": 10213,
        "title": "Research on the Decoder Attention Structure of Multi-encoder Transformer-based Automatic Post-Editing Model",
        "authors": "Jaehun Shin, WonKee Lee, Youngkil Kim, H.Jeung Han, Jong-Hyeok Lee",
        "published": "2020-8-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2020.26.8.367"
    },
    {
        "id": 10214,
        "title": "Fracture Extraction From Logging Image Using a Dual Encoder-Decoder Architecture With Swin Transformer",
        "authors": "",
        "published": "2023-2-1",
        "citations": 1,
        "abstract": "Imaging logging is a method of imaging the physical parameters of the borehole wall or the objects around the borehole according to the observation of the geophysical field in the borehole. Imaging logging data can determine the dip angle and structural characteristics of the formation and observe the geometry and development degree of fractures. The performance of existing target segmentation networks relies on large volumes of data. However, logging images are expensive to acquire, so how to effectively extract fractures from small samples of logging images is an urgent problem to be solved. Therefore, we developed a dual encoder-decoder structure using the Swin Transformer, which uses the self-attention mechanism of a hierarchical Vision Transformer with shifted window to model the remote context information. It can overcome the limitations of most convolutional neural network-based methods that cannot establish long-term dependencies and global contextual connections in convolutional operations. In addition, the shifted window mechanism substantially improves the computational efficiency of the model, and the hierarchical structure allows flexibility in modeling at different scales. At the same time, skip connections are established between adjacent layers of the structure, and the higher-level feature maps are stitched with the lower-level feature maps in channel dimensions, which can obtain more high-resolution detail information of fractures, and thus improve the segmentation accuracy. The experimental results show that the performance is better than the mainstream segmentation networks under small training sets of logging images. The effectiveness of our method reveals that it is practical in fracture extraction of logging images.",
        "link": "http://dx.doi.org/10.30632/pjv64n1-2023a3"
    },
    {
        "id": 10215,
        "title": "A Decoder-free Transformer-like Architecture for High-efficiency Single Image Deraining",
        "authors": "Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Tian-Jing Zhang",
        "published": "2022-7",
        "citations": 2,
        "abstract": "Despite the success of vision Transformers for the image deraining task, they are limited by computation-heavy and slow runtime. In this work, we investigate Transformer decoder is not necessary and has huge computational costs. Therefore, we revisit the standard vision Transformer as well as its successful variants and propose a novel Decoder-Free Transformer-Like (DFTL) architecture for fast and accurate single image deraining. Specifically, we adopt a cheap linear projection to represent visual information with lower\n\ncomputational costs than previous linear projections. Then we replace standard Transformer decoder block with designed Progressive Patch Merging (PPM), which attains comparable performance and efficiency. DFTL could significantly alleviate the computation and GPU memory requirements through proposed modules. Extensive experiments\n\ndemonstrate the superiority of DFTL compared with competitive Transformer architectures, e.g., ViT, DETR, IPT, Uformer, and Restormer. The code is available at https://github.com/XiaoXiao-Woo/derain.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/205"
    },
    {
        "id": 10216,
        "title": "FeedFormer: Revisiting Transformer Decoder for Efficient Semantic Segmentation",
        "authors": "Jae-hun Shim, Hyunwoo Yu, Kyeongbo Kong, Suk-Ju Kang",
        "published": "2023-6-26",
        "citations": 2,
        "abstract": "With the success of Vision Transformer (ViT) in image classification, its variants have yielded great success in many downstream vision tasks. Among those, the semantic segmentation task has also benefited greatly from the advance of ViT variants. However, most studies of the transformer for semantic segmentation only focus on designing efficient transformer encoders, rarely giving attention to designing the decoder. Several studies make attempts in using the transformer decoder as the segmentation decoder with class-wise learnable query. Instead, we aim to directly use the encoder features as the queries. This paper proposes the Feature Enhancing Decoder transFormer (FeedFormer) that enhances structural information using the transformer decoder. Our goal is to decode the high-level encoder features using the lowest-level encoder feature. We do this by formulating high-level features as queries, and the lowest-level feature as the key and value. This enhances the high-level features by collecting the structural information from the lowest-level feature. Additionally, we use a simple reformation trick of pushing the encoder blocks to take the place of the existing self-attention module of the decoder to improve efficiency. We show the superiority of our decoder with various light-weight transformer-based decoders on popular semantic segmentation datasets. Despite the minute computation, our model has achieved state-of-the-art performance in the performance computation trade-off. Our model FeedFormer-B0 surpasses SegFormer-B0 with 1.8% higher mIoU and 7.1% less computation on ADE20K, and 1.7% higher mIoU and 14.4% less computation on Cityscapes, respectively. Code will be released at: https://github.com/jhshim1995/FeedFormer.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i2.25321"
    },
    {
        "id": 10217,
        "title": "Boosting Unknown-Number Speaker Separation with Transformer Decoder-Based Attractor",
        "authors": "Younglo Lee, Shukjae Choi, Byeong-Yeol Kim, Zhong-Qiu Wang, Shinji Watanabe",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446032"
    },
    {
        "id": 10218,
        "title": "TransCFD: A transformer-based decoder for flow field prediction",
        "authors": "Jundou Jiang, Guanxiong Li, Yi Jiang, Laiping Zhang, Xiaogang Deng",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106340"
    },
    {
        "id": 10219,
        "title": "Clairvoyant",
        "authors": "Khalid Ayedh Alharthi, Arshad Jhumka, Sheng Di, Franck Cappello",
        "published": "2022-6-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3524059.3532374"
    },
    {
        "id": 10220,
        "title": "PoseGTAC: Graph Transformer Encoder-Decoder with Atrous Convolution for 3D Human Pose Estimation",
        "authors": "Yiran Zhu, Xing Xu, Fumin Shen, Yanli Ji, Lianli Gao, Heng Tao Shen",
        "published": "2021-8",
        "citations": 5,
        "abstract": "Graph neural networks (GNNs) have been widely used in the 3D human pose estimation task, since the pose representation of a human body can be naturally modeled by the graph structure. Generally, most of the existing GNN-based models utilize the restricted receptive ﬁelds of ﬁlters and single-scale information, while neglecting the valuable multi-scale contextual information. To tackle this issue, we propose a novel Graph Transformer Encoder-Decoder with Atrous Convolution, named PoseGTAC, to effectively extract multi-scale context and long-range information. In our proposed PoseGTAC model, Graph Atrous Convolution (GAC) and Graph Transformer Layer (GTL), respectively for the extraction of local multi-scale and global long-range information, are combined and stacked in an encoder-decoder structure, where graph pooling and unpooling are adopted for the interaction of multi-scale information from local to global (e.g., part-scale and body-scale). Extensive experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that the proposed PoseGTAC model exceeds all previous methods and achieves state-of-the-art performance.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/188"
    },
    {
        "id": 10221,
        "title": "DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting",
        "authors": "Maoyuan Ye, Jing Zhang, Shanshan Zhao, Juhua Liu, Tongliang Liu, Bo Du, Dacheng Tao",
        "published": "2023-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01854"
    },
    {
        "id": 10222,
        "title": "Transformer-based Encoder-Decoder Model for Surface Defect Detection",
        "authors": "Xiaofeng Lu, Wentao Fan",
        "published": "2022-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529466.3529471"
    },
    {
        "id": 10223,
        "title": "Continuous Action Space-Based Spoken Language Acquisition Agent Using Residual Sentence Embedding and Transformer Decoder",
        "authors": "Ryota Komatsu, Yusuke Kimura, Takuma Okamoto, Takahiro Shinozaki",
        "published": "2023-6-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096250"
    },
    {
        "id": 10224,
        "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
        "authors": "Junyi Ao, Ziqiang Zhang, Long Zhou, Shujie Liu, Haizhou Li, Tom Ko, Lirong Dai, Jinyu Li, Yao Qian, Furu Wei",
        "published": "2022-9-18",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10368"
    },
    {
        "id": 10225,
        "title": "GenGen: Analysing the Performance of Transformer-based Encoder-Decoder in a General Natural Language Generation Paradigm",
        "authors": "Angad Sethi, Aman Kumar Gond, Abhishek Kumar Singh, Prashant Giridhar Shambharkar",
        "published": "2022-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaaic53929.2022.9793209"
    },
    {
        "id": 10226,
        "title": "Image Caption Generation Related to Object Detection and Colour Recognition Using Transformer-Decoder",
        "authors": "Zainab Umair Kamangar, Ghulam Mutjaba Shaikh, Saif Hassan, Nimra Mughal, Umair Ayaz Kamangar",
        "published": "2023-3-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icomet57998.2023.10099161"
    },
    {
        "id": 10227,
        "title": "Fake news detection using a deep learning transformer based encoder-decoder architecture",
        "authors": "M. Badri Narayanan, Arun Kumar Ramesh, K.S. Gayathri, A. Shahina",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "Fake news production, accessibility, and consumption have all increased with the rise of internet-connected gadgets and social media platforms. A good fake news detection system is essential because the news readers receive can affect their opinions. Several works on fake news detection have been done using machine learning and deep learning approaches. Recently, the deep learning approach has been preferred over machine learning because of its ability to comprehend the intricacies of textual data. The introduction of transformer architecture changed the NLP paradigm and distinguished itself from recurrent models by enabling the processing of sentences as a whole rather than word by word. The attention mechanisms introduced in Transformers allowed them to understand the relationship between far-apart tokens in a sentence. Numerous deep learning works on fake news detection have been published by focusing on different features to determine the authenticity of a news source. We performed an extensive analysis of the comprehensive NELA-GT 2020 dataset, which revealed that the title and content of a news source contain discernible information critical for determining its integrity. To this objective, we introduce ‘FakeNews Transformer’ — a specialized Transformer-based architecture that considers the news story’s title and content to assess its veracity. Our proposed work achieved an accuracy of 74.0% on a subset of the NELA-GT 2020 dataset. To our knowledge, FakeNews Transformer is the first published work that considers both title and content for evaluating a news article; thus, we compare the performance of our work against two BERT and two LSTM models working independently on title and content. Our work outperformed the BERT and LSTM models working independently on title by 7.6% and 9.6%, while performing better than the BERT and LSTM models working independently on content by 8.9% and 10.5%, respectively.",
        "link": "http://dx.doi.org/10.3233/jifs-223980"
    },
    {
        "id": 10228,
        "title": "Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder",
        "authors": "WonKee Lee, Jaehun Shin, Jong-Hyeok Lee",
        "published": "2019",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-5412"
    },
    {
        "id": 10229,
        "title": "BiTSRS: A Bi-Decoder Transformer Segmentor for High-Spatial-Resolution Remote Sensing Images",
        "authors": "Yuheng Liu, Yifan Zhang, Ye Wang, Shaohui Mei",
        "published": "2023-2-2",
        "citations": 5,
        "abstract": "Semantic segmentation of high-spatial-resolution (HSR) remote sensing (RS) images has been extensively studied, and most of the existing methods are based on convolutional neural network (CNN) models. However, the CNN is regarded to have less power in global representation modeling. In the past few years, methods using transformer have attracted increasing attention and generate improved results in semantic segmentation of natural images, owing to their powerful ability in global information acquisition. Nevertheless, these transformer-based methods exhibit limited performance in semantic segmentation of RS images, probably because of the lack of comprehensive understanding in the feature decoding process. In this paper, a novel transformer-based model named the bi-decoder transformer segmentor for remote sensing (BiTSRS) is proposed, aiming at alleviating the problem of flexible feature decoding, through a bi-decoder design for semantic segmentation of RS images. In the proposed BiTSRS, the Swin transformer is adopted as encoder to take both global and local representations into consideration, and a unique design module (ITM) is designed to deal with the limitation of input size for Swin transformer. Furthermore, BiTSRS adopts a bi-decoder structure consisting of a Dilated-Uper decoder and a fully deformable convolutional network (FDCN) module embedded with focal loss, with which it is capable of decoding a wide range of features and local detail deformations. Both ablation experiments and comparison experiments were conducted on three representative RS images datasets. The ablation analysis demonstrates the contributions of specifically designed modules in the proposed BiTSRS to performance improvement. The comparison experimental results illustrate that the proposed BiTSRS clearly outperforms some state-of-the-art semantic segmentation methods.",
        "link": "http://dx.doi.org/10.3390/rs15030840"
    },
    {
        "id": 10230,
        "title": "Idea Generation using Transformer Decoder Models",
        "authors": "Musammet Rafia Karim, Siam Shibly Antar, Mohammad Ashrafuzzaman Khan",
        "published": "2022-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3579654.3579706"
    },
    {
        "id": 10231,
        "title": "Enhancing image captioning performance based on efficientnet B0 model and transformer encoder-decoder",
        "authors": "Abhisht Joshi, Ahmed Alkhayyat, Harsh Gunwant, Abhay Tripathi, Moolchand Sharma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0184395"
    },
    {
        "id": 10232,
        "title": "CRefNet: Learning Consistent Reflectance Estimation With a Decoder-Sharing Transformer",
        "authors": "Jundan Luo, Nanxuan Zhao, Wenbin Li, Christian Richardt",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tvcg.2023.3337870"
    },
    {
        "id": 10233,
        "title": "CTFCD: Channel transformer based on full convolutional decoder for single image deraining",
        "authors": "Shaohan Tan, Hui Chen, Songhao Zhu",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2023.103992"
    },
    {
        "id": 10234,
        "title": "Revalidating the Encoder-Decoder Depths and Activation Function to Find Optimum Vanilla Transformer Model",
        "authors": "Yaya Heryadi, Bambang Dwi Wijanarko, Dina Fitria Murad, Cuk Tho, Kiyota Hashimoto",
        "published": "2023-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccosite57641.2023.10127790"
    },
    {
        "id": 10235,
        "title": "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
        "authors": "Hongfei Xu, Josef van Genabith, Qiuhui Liu, Deyi Xiong",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.7"
    },
    {
        "id": 10236,
        "title": "A Novel Encoder-Decoder Structure-based Transformer for Fine-Resolution Remote Sensing Images",
        "authors": "Guixian Wang, Dandan Huang, ZhenYe Geng, Zhi Liu, Jin Duan",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "Abstract\nFull convolution neural network (FCN) based on an encoder-decoder structure has become a standard network in the semantic segmentation domain. Encoder-decoder architecture is an effective means to get finer-grained performance. Encoders constantly extract multilevel features, and then use decoders to gradually introduce low-level features into high-level features. Context information is critical for accurate segmentation, which is the main direction of semantic segmentation at present. So many efforts have been made to make better use of this kind of information, including codec structure, void convolution (expanded convolution), and attention mechanism. However, most of these schemes are based on Resnet or other variants of convolution network FCN, which makes it unable to get rid of the defective local receptive field of convolution itself. In this work, we introduce the pyramid visual converter (PVT) to replace the traditional full convolution network architecture, and design a novel encoder-decoder architecture to more effectively utilize the context information.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2517/1/012017"
    },
    {
        "id": 10237,
        "title": "TEDNet: transformer-aware encoder-decoder network for salient object detection in remote sensing images",
        "authors": "Huiming SUN Sun, Yuewei Lin, Zibo Meng, Hongkai Yu",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3023731"
    },
    {
        "id": 10238,
        "title": "TransFlowLog: Log Anomaly Detection Based on Transformer Encoder and Interflow Decoder",
        "authors": "Zaichao Lin, Siyang Lu, Ningning Han, Dongdong Wang, Xiang Wei, Mingquan Wang",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpads60453.2023.00196"
    },
    {
        "id": 10239,
        "title": "Dual-TranSpeckle: Dual-pathway transformer based encoder-decoder network for medical ultrasound image despeckling",
        "authors": "Yuqing Chen, Zhitao Guo, Jinli Yuan, Xiaozeng Li, Hengyong Yu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2024.108313"
    },
    {
        "id": 10240,
        "title": "A Method of Integrating Length Constraints into Encoder-Decoder Transformer for Abstractive Text Summarization",
        "authors": "Ngoc-Khuong Nguyen, Dac-Nhuong Le, Viet-Ha Nguyen, Anh-Cuong Le",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/iasc.2023.037083"
    },
    {
        "id": 10241,
        "title": "FDR-TransUNet: A novel encoder-decoder architecture with vision transformer for improved medical image segmentation",
        "authors": "Zhang Chaoyang, Sun Shibao, Hu Wenmao, Zhao Pengcheng",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2023.107858"
    },
    {
        "id": 10242,
        "title": "Non-Autoregressive Transformer with Unified Bidirectional Decoder for Automatic Speech Recognition",
        "authors": "Chuan-Fei Zhang, Yan Liu, Tian-Hao Zhang, Song-Lu Chen, Feng Chen, Xu-Cheng Yin",
        "published": "2022-5-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp43922.2022.9746903"
    },
    {
        "id": 10243,
        "title": "Vision Intelligence Assisted Lung Function Estimation Based on Transformer Encoder-Decoder Network with Invertible Modeling",
        "authors": "Liuyin Chen, Di Lu, Jianxue Zhai, Kaican Cai, Long Wang, Zijun Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2023.3348428"
    },
    {
        "id": 10244,
        "title": "Remote Heart Rate Estimation Based on Transformer with Multi-Skip Connection Decoder: Method and Evaluation in the Wild",
        "authors": "Walaa Othman, Alexey Kashevnik, Ammar Ali, Nikolay Shilov, Dmitry Ryumin",
        "published": "2024-1-25",
        "citations": 1,
        "abstract": "Heart rate is an essential vital sign to evaluate human health. Remote heart monitoring using cheaply available devices has become a necessity in the twenty-first century to prevent any unfortunate situation caused by the hectic pace of life. In this paper, we propose a new method based on the transformer architecture with a multi-skip connection biLSTM decoder to estimate heart rate remotely from videos. Our method is based on the skin color variation caused by the change in blood volume in its surface. The presented heart rate estimation framework consists of three main steps: (1) the segmentation of the facial region of interest (ROI) based on the landmarks obtained by 3DDFA; (2) the extraction of the spatial and global features; and (3) the estimation of the heart rate value from the obtained features based on the proposed method. This paper investigates which feature extractor performs better by captioning the change in skin color related to the heart rate as well as the optimal number of frames needed to achieve better accuracy. Experiments were conducted using two publicly available datasets (LGI-PPGI and Vision for Vitals) and our own in-the-wild dataset (12 videos collected by four drivers). The experiments showed that our approach achieved better results than the previously published methods, making it the new state of the art on these datasets.",
        "link": "http://dx.doi.org/10.3390/s24030775"
    },
    {
        "id": 10245,
        "title": "Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input",
        "authors": "Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan Su, Helen Meng",
        "published": "2021-6-6",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9414694"
    },
    {
        "id": 10246,
        "title": "GLFormer: Efficient Global and Local Transformer with Lightweight Decoder for Semantic Segmentation",
        "authors": "Lihua Fu, Xiangping Zhai, Jing Zhu",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eiect60552.2023.10442014"
    },
    {
        "id": 10247,
        "title": "A Symmetrical Encoder-Decoder Network with Transformer for Noise-Robust Iris Segmentation",
        "authors": "Zhengjie Gu, Caiyong Wang, Qichuan Tian, Qi Zhang",
        "published": "2022-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3724/sp.j.1089.2022.19235"
    },
    {
        "id": 10248,
        "title": "Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images",
        "authors": "Teerapong Panboonyuen, Kulsawasd Jitkajornwanich, Siam Lawawirojwong, Panu Srestasathiern, Peerapon Vateekul",
        "published": "2021-12-15",
        "citations": 30,
        "abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
        "link": "http://dx.doi.org/10.3390/rs13245100"
    },
    {
        "id": 10249,
        "title": "Advancing Abstractive Bangla Text Summarization: A Deep Learning Approach Using Seq2seq Encoder- Decoder Model and T5 Transformer",
        "authors": "S M Tasnimul Hasan, Md Ashfaqur Rahman, Md Mahamudul Hasan, Mohammad Rakibul Hasan, Md Moinul Hoque",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sti59863.2023.10464712"
    },
    {
        "id": 10250,
        "title": "Transformer Encoder-Decoder based Title Generation Model with Word Loss and Repetition Penalty",
        "authors": "Su-Jin Seong, Jeong-Won Cha",
        "published": "2021-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2021.27.4.210"
    },
    {
        "id": 10251,
        "title": "A Transformer decoder-based Generative Adversarial Model with TrajLoss Function for Privacy-Preserving Trajectory Publishing",
        "authors": "Xiaoqian Cao, Juan Yu, Jianmin Han, Xin Yao, Jianfeng Lu, Hao Peng",
        "published": "2022-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3578741.3578795"
    },
    {
        "id": 10252,
        "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "2022-5-9",
        "citations": 104,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/acs.jcim.1c00600"
    },
    {
        "id": 10253,
        "title": "Attentional bias for hands: Cascade dual‐decoder transformer for sign language production",
        "authors": "Xiaohan Ma, Rize Jin, Jianming Wang, Tae‐Sun Chung",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "AbstractSign Language Production (SLP) refers to the task of translating textural forms of spoken language into corresponding sign language expressions. Sign languages convey meaning by means of multiple asynchronous articulators, including manual and non‐manual information channels. Recent deep learning‐based SLP models directly generate the full‐articulatory sign sequence from the text input in an end‐to‐end manner. However, these models largely down weight the importance of subtle differences in the manual articulation due to the effect of regression to the mean. To explore these neglected aspects, an efficient cascade dual‐decoder Transformer (CasDual‐Transformer) for SLP is proposed to learn, successively, two mappings SLPhand: Text → Hand pose and SLPsign: Text → Sign pose, utilising an attention‐based alignment module that fuses the hand and sign features from previous time steps to predict more expressive sign pose at the current time step. In addition, to provide more efficacious guidance, a novel spatio‐temporal loss to penalise shape dissimilarity and temporal distortions of produced sequences is introduced. Experimental studies are performed on two benchmark sign language datasets from distinct cultures to verify the performance of the proposed model. Both quantitative and qualitative results show that the authors’ model demonstrates competitive performance compared to state‐of‐the‐art models, and in some cases, achieves considerable improvements over them.",
        "link": "http://dx.doi.org/10.1049/cvi2.12273"
    },
    {
        "id": 10254,
        "title": "Take an Irregular Route: Enhance the Decoder of Time-Series Forecasting Transformer",
        "authors": "Li Shen, Yuning Wei, Yangzhu Wang, Huaxin Qiu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jiot.2023.3341099"
    },
    {
        "id": 10255,
        "title": "Transformer with large convolution kernel decoder network for salient object detection in optical remote sensing images",
        "authors": "Pengwei Dong, Bo Wang, Runmin Cong, Hai-Han Sun, Chongyi Li",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103917"
    },
    {
        "id": 10256,
        "title": "Dual-decoder transformer network for answer grounding in visual question answering",
        "authors": "Liangjun Zhu, Li Peng, Weinan Zhou, Jielong Yang",
        "published": "2023-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.04.003"
    },
    {
        "id": 10257,
        "title": "A combined encoder–transformer–decoder network for volumetric segmentation of adrenal tumors",
        "authors": "Liping Wang, Mingtao Ye, Yanjie Lu, Qicang Qiu, Zhongfeng Niu, Hengfeng Shi, Jian Wang",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "Abstract\nBackground\nThe morphology of the adrenal tumor and the clinical statistics of the adrenal tumor area are two crucial diagnostic and differential diagnostic features, indicating precise tumor segmentation is essential. Therefore, we build a CT image segmentation method based on an encoder–decoder structure combined with a Transformer for volumetric segmentation of adrenal tumors.\n\nMethods\nThis study included a total of 182 patients with adrenal metastases, and an adrenal tumor volumetric segmentation method combining encoder–decoder structure and Transformer was constructed. The Dice Score coefficient (DSC), Hausdorff distance, Intersection over union (IOU), Average surface distance (ASD) and Mean average error (MAE) were calculated to evaluate the performance of the segmentation method.\n\nResults\nAnalyses were made among our proposed method and other CNN-based and transformer-based methods. The results showed excellent segmentation performance, with a mean DSC of 0.858, a mean Hausdorff distance of 10.996, a mean IOU of 0.814, a mean MAE of 0.0005, and a mean ASD of 0.509. The boxplot of all test samples' segmentation performance implies that the proposed method has the lowest skewness and the highest average prediction performance.\n\nConclusions\nOur proposed method can directly generate 3D lesion maps and showed excellent segmentation performance. The comparison of segmentation metrics and visualization results showed that our proposed method performed very well in the segmentation.\n",
        "link": "http://dx.doi.org/10.1186/s12938-023-01160-5"
    },
    {
        "id": 10258,
        "title": "Dual Encoder Decoder Shifted Window-Based Transformer Network for Polyp Segmentation with Self-Learning Approach",
        "authors": "Lijin P., Mohib Ullah, Anuja Vats, Faouzi Alaya Cheikh, Santhosh Kumar G., Madhu S. Nair",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2024.3366146"
    },
    {
        "id": 10259,
        "title": "Abstractive Text Summarization of Hindi Corpus Using Transformer Encoder-Decoder Model",
        "authors": "Rashi Bhansali, Anushka Bhave, Gauri Bharat, Vedant Mahajan, Manikrao Laxmanrao Dhore",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-8094-7_13"
    }
]