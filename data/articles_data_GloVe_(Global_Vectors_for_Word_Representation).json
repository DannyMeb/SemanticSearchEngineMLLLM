[
    {
        "id": 9660,
        "title": "Comparative Study of Content-Based Phishing Email Detection Using Global Vector (GloVe) and Bidirectional Encoder Representation from Transformer (BERT) Word Embedding Models",
        "authors": "Surajit Giri, Siddhartha Banerjee, Kunal Bag, Dipanjan Maiti",
        "published": "2022-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeict53079.2022.9768612"
    },
    {
        "id": 9661,
        "title": "Analysis of Internet Movie Database with Global Vectors for Word Representation",
        "authors": "Christine Dewi, Gouwei Dai, Henoch Juli Christanto",
        "published": "2024-2-17",
        "citations": 0,
        "abstract": " Sentiment analysis (SA) involves utilizing natural language processing (NLP) methods to identify the sentiment conveyed by a given text. This study is grounded on the dataset sourced from the internet movie database (IMDB), encompassing evaluations of films and their corresponding positive or negative classifications. Our research experiment aims to ascertain the model with the highest accuracy and generality. Our research utilizes diverse classifiers, comprising unsupervised learning approaches such as Valence Aware Dictionary and sEntiment Reasoner (VADER) and Text Blob, alongside Supervised Learning methods like Naïve Bayes, which encompasses both the Bernoulli NB and Multinomial NB. Several methodologies have been utilized, including the Count Vectorizer, and the Term Frequency-Inverse Document Frequency model (TFIDF) Vectorizer. Subsequently, word embedding and bidirectional LSTM are executed, utilizing various embeddings such as the Long Short-Term Memory (LSTM) base model. Finally, GloVe embeddings achieve the best performance with an accuracy of 90.64% and a sensitivity of 91.07%. ",
        "link": "http://dx.doi.org/10.1142/s2196888823500215"
    },
    {
        "id": 9662,
        "title": "Experimental Comparison of Pre-Trained Word Embedding Vectors of Word2Vec, Glove, FastText for Word Level Semantic Text Similarity Measurement in Turkish",
        "authors": "Cagatay Tulu",
        "published": "2022-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12913/22998624/152453"
    },
    {
        "id": 9663,
        "title": "Complex-Valued Vectors for Word Representation",
        "authors": "Guangcheng Liu, Yuexian Hou",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaibd51990.2021.9458998"
    },
    {
        "id": 9664,
        "title": "Learning Word Embeddings without Context Vectors",
        "authors": "Alexey Zobnin, Evgenia Elistratova",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-4329"
    },
    {
        "id": 9665,
        "title": "Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation",
        "authors": "Ivan Vulić",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-3018"
    },
    {
        "id": 9666,
        "title": "VecShare: A Framework for Sharing Word Representation Vectors",
        "authors": "Jared Fernandez, Zhaocheng Yu, Doug Downey",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1032"
    },
    {
        "id": 9667,
        "title": "Emotion Classification of Song Lyrics using Bidirectional LSTM Method with GloVe Word Representation Weighting",
        "authors": " Jiddy Abdillah,  Ibnu Asror,  Yanuar Firdaus Arie Wibowo",
        "published": "2020-8-20",
        "citations": 10,
        "abstract": "The rapid change of the music market from analog to digital has caused a rapid increase in the amount of music that is spread throughout the world as well because music is easier to make and sell. The amount of music available has changed the way people find music, one of which is based on the emotion of the song. The existence of music emotion recognition and recommendation helps music listeners find songs in accordance with their emotions. Therefore, the classification of emotions is needed to determine the emotions of a song. The emotional classification of a song is largely based on feature extraction and learning from the available data sets. Various learning algorithms have been used to classify song emotions and produce different accuracy. In this study, the Bidirectional Long-short Term Memory (Bi-LSTM) deep learning method with weighting words using GloVe is used to classify the song's emotions using the lyrics of the song. The result shows that the Bi-LSTM model with dropout layer and activity regularization can produce an accuracy of 91.08%. Dropout, activity regularization and learning rate decay parameters can reduce the difference between training loss and validation loss by 0.15.",
        "link": "http://dx.doi.org/10.29207/resti.v4i4.2156"
    },
    {
        "id": 9668,
        "title": "Text classification supervised algorithms with term frequency inverse document frequency and global vectors for word representation: a comparative study",
        "authors": "Zakia Labd, Said Bahassine, Khalid Housni, Fatima Zahrae Ait Hamou Aadi, Khalid Benabbes",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "Over the course of the previous two decades, there has been a rise in the quantity of text documents stored digitally. The ability to organize and categorize those documents in an automated mechanism, is known as text categorization which is used to classify them into a set of predefined categories so they may be preserved and sorted more efficiently. Identifying appropriate structures, architectures, and methods for text classification presents a challenge for researchers. This is due to the significant impact this concept has on content management, contextual search, opinion mining, product review analysis, spam filtering, and text sentiment mining. This study analyzes the generic categorization strategy and examines supervised machine learning approaches and their ability to comprehend complex models and nonlinear data interactions. Among these methods are k-nearest neighbors (KNN), support vector machine (SVM), and ensemble learning algorithms employing various evaluation techniques. Thereafter, an evaluation is conducted on the constraints of every technique and how they can be applied to real-life situations.",
        "link": "http://dx.doi.org/10.11591/ijece.v14i1.pp589-599"
    },
    {
        "id": 9669,
        "title": "Sentiment classification of user's reviews on drugs based on global vectors for word representation and bidirectional long short-term memory recurrent neural network",
        "authors": "Hadab Khalid Obayed, Firas Sabah Al-Turaihi, Khaldoon H. Alhussayni",
        "published": "2021-7-1",
        "citations": 7,
        "abstract": "<p>The process of product development in the health sector, especially pharmaceuticals, goes through a series of precise procedures due to its directrelevance to human life. The opinion of patients or users of a particular drugcan be relied upon in this development process, as the patients convey their experience with the drugs through their opinion. The social media field provides many datasets related to drugs through knowing the user's ratingand opinion on a drug after using it. In this work, a dataset is used that includes the user’s rating and review on the drug, for the purpose of classifying the user’s opinions (reviews) whether they are positive ornegative. The proposed method in this article includes two phases. The first phase is to use the Global vectors for word representation model for converting texts into the vector of embedded words. As for the second stage, the deep neural network (Bidirectional longshort-termmemory) is employedin the classification of reviews. The user's rating is used as a ground truth inevaluating the classification results. The proposed method present sencouraging results, as the classification results are evaluated through threecriteria, namely Precision, Recall and F-score, whose obtained values equal(0.9543, 0.9597and0.9558), respectively. The classification results of theproposed method are compared to a number of classifiers, and it was noticed that the results of the proposed method exceed those of the alternative classifiers.</p>",
        "link": "http://dx.doi.org/10.11591/ijeecs.v23.i1.pp345-353"
    },
    {
        "id": 9670,
        "title": "Topic Enhanced Word Vectors for Documents Representation",
        "authors": "Dayu Li, Yang Li, Suge Wang",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-6805-8_14"
    },
    {
        "id": 9671,
        "title": "Better Word Representation Vectors Using Syllabic Alphabet: A Case Study of Swahili",
        "authors": "Casper S. Shikali, Zhou Sijie, Liu Qihe, Refuoe Mokhosi",
        "published": "2019-9-4",
        "citations": 4,
        "abstract": "Deep learning has extensively been used in natural language processing with sub-word representation vectors playing a critical role. However, this cannot be said of Swahili, which is a low resource and widely spoken language in East and Central Africa. This study proposed novel word embeddings from syllable embeddings (WEFSE) for Swahili to address the concern of word representation for agglutinative and syllabic-based languages. Inspired by the learning methodology of Swahili in beginner classes, we encoded respective syllables instead of characters, character n-grams or morphemes of words and generated quality word embeddings using a convolutional neural network. The quality of WEFSE was demonstrated by the state-of-art results in the syllable-aware language model on both the small dataset (31.229 perplexity value) and the medium dataset (45.859 perplexity value), outperforming character-aware language models. We further evaluated the word embeddings using word analogy task. To the best of our knowledge, syllabic alphabets have not been used to compose the word representation vectors. Therefore, the main contributions of the study are a syllabic alphabet, WEFSE, a syllabic-aware language model and a word analogy dataset for Swahili.",
        "link": "http://dx.doi.org/10.3390/app9183648"
    },
    {
        "id": 9672,
        "title": "Systematic Literature Review terhadap Klasifikasi Emosi pada Lirik Lagu Berbahasa Ambon menggunakan Metode Bidirectional LSTM dengan Glove Word Representation Weighting",
        "authors": " Kholida Zia Abidin,  Arief Setyanto,  Rudyanto Arief",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "One form of text that can express emotions is lyrics. Lyrics are a type of literary work expressed in the form of words, the contents of which can express the songwriter's personal feelings, thoughts, and emotions. Therefore, the lyrics can be used as an object of research on the classification of emotions. The classification of song lyrics really requires bi-LSTM to be the input value when classifying data in the form of song lyrics in order to get high accuracy results. This research was carried out systematically and the results were measurable. Descriptive qualitative research was used in this research. The results of identification based on case studies and statistics show that the reviews of popular topics are identical. The classification of song lyrics really requires bi-LSTM to be the input value when classifying data in the form of song lyrics in order to get high accuracy results.",
        "link": "http://dx.doi.org/10.51903/pixel.v16i2.1641"
    },
    {
        "id": 9673,
        "title": "Deriving Word Vectors from Contextualized Language Models using Topic-Aware Mention Selection",
        "authors": "Yixiao Wang, Zied Bouraoui, Luis Espinosa Anke, Steven Schockaert",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.repl4nlp-1.19"
    },
    {
        "id": 9674,
        "title": "An Approach Based Natural Language Processing for DNA Sequences Encoding Using the Global Vectors for Word Representation",
        "authors": "Brahim Matougui, Hacene Belhadef, Ilham Kitouni",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-70713-2_53"
    },
    {
        "id": 9675,
        "title": "A Personality Mining System for German Twitter Posts With Global Vectors Word Embedding",
        "authors": "Henning Usselmann, Rangina Ahmad, Dominik Siemon",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3130937"
    },
    {
        "id": 9676,
        "title": "WOVe:  Incorporating Word Order in GloVe Word Embeddings",
        "authors": "Mohammed Salah Ibrahim, Susan Gauch, Tyler Gerth, Brandon Cox",
        "published": "2022-6-17",
        "citations": 1,
        "abstract": "Word vector representations open up new opportunities to extract useful information from unstructured text. Defining a word as a vector made it easy for the machine learning algorithms to understand a text and extract information from. Word vector representations have been used in many applications such word synonyms, word analogy, syntactic parsing, and many others. GloVe, based on word contexts and matrix vectorization, is an effective vector-learning algorithm. It improves on previous vector-learning algorithms. However, the GloVe model fails to explicitly consider the order in which words appear within their contexts. In this paper, multiple methods of incorporating word order in GloVe word embeddings are proposed. Experimental results show that our Word Order Vector (WOVe) word embeddings approach outperforms unmodified GloVe on the natural language tasks of analogy completion and word similarity. WOVe with direct concatenation slightly outperformed GloVe on the word similarity task, increasing average rank by 2%.  However, it greatly improved on the GloVe baseline on a word analogy task, achieving an average 36.34% improvement in accuracy.",
        "link": "http://dx.doi.org/10.46328/ijonest.83"
    },
    {
        "id": 9677,
        "title": "Term-based Website Evaluation Applying Word Vectors and Readability Measures",
        "authors": "Kiyoshi Nagata",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011383900003318"
    },
    {
        "id": 9678,
        "title": "Jointly Embedding Knowledge Graph and Word Vectors",
        "authors": "Tomu Kadoki, Runhe Huang, Satoru Fujita",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2019.011"
    },
    {
        "id": 9679,
        "title": "Word Representation",
        "authors": "Omer Levy",
        "published": "2018-9-10",
        "citations": 2,
        "abstract": "A fundamental challenge in natural-language processing is to represent words as mathematical entities that can be read, reasoned, and manipulated by computational models. The current leading approach represents words as vectors in a continuous real-valued space, in such a way that similarities in the vector space correlate with semantic similarities between words. This chapter surveys various frameworks and methods for acquiring word vectors, while tying together related ideas and concepts.",
        "link": "http://dx.doi.org/10.1093/oxfordhb/9780199573691.013.57"
    },
    {
        "id": 9680,
        "title": "Square-Free Word-Representation of Word-Representable Graphs",
        "authors": "Ramesh Hariharasubramanian, Biswajit Das",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4741574"
    },
    {
        "id": 9681,
        "title": "Peer Review #2 of \"An automated method to enrich consumer health vocabularies using GloVe word embeddings and an auxiliary lexical resource (v0.1)\"",
        "authors": "",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.668v0.1/reviews/2"
    },
    {
        "id": 9682,
        "title": "Word Replaceability Through Word Vectors",
        "authors": "Peter Taraba",
        "published": "2020-5",
        "citations": 0,
        "abstract": "AbstractThere have been many numerical methods developed recently that try to capture the semantic meaning of words through word vectors. In this study, we present a new way to learn word vectors using only word co-appearances and their average distances. However, instead of claiming semantic or syntactic word representation, we lower our assertions and claim only that we learn word vectors, which express word’s replaceability in sentences based on their Euclidean distances. Synonyms are a subgroup of words which can replace each other, and we will use them to show differences between training on words that appear close to each other in a local window and training that uses distances between words, which we use in this study. Using ConceptNet 5.5.0’s synonyms, we show that word vectors trained on word distances create higher contrast in distributions of word similarities than was done with Glove, where only word appearances close to each other were engaged. We introduce a measure, which looks at intersection of histograms of word distances for synonyms and non-synonyms.",
        "link": "http://dx.doi.org/10.1007/s42979-020-00164-5"
    },
    {
        "id": 9683,
        "title": "Character-to-Word Representation and Global Contextual Representation for Named Entity Recognition",
        "authors": "Jun Chang, Xiaohong Han",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-023-11168-6"
    },
    {
        "id": 9684,
        "title": "Peer Review #1 of \"An automated method to enrich consumer health vocabularies using GloVe word embeddings and an auxiliary lexical resource (v0.1)\"",
        "authors": "",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.668v0.1/reviews/1"
    },
    {
        "id": 9685,
        "title": "Can Network Embedding of Distributional Thesaurus Be Combined with\n            Word Vectors for Better Representation?",
        "authors": "Abhik Jana, Pawan Goyal",
        "published": "2018",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n18-1043"
    },
    {
        "id": 9686,
        "title": "Manifold Learning-based Word Representation Refinement Incorporating Global and Local Information",
        "authors": "Wenyu Zhao, Dong Zhou, Lin Li, Jinjun Chen",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.301"
    },
    {
        "id": 9687,
        "title": "Classification of News Texts with GloVe Word Embeddings and Neural Networks",
        "authors": "Hulya HARK, Meral KARAKURT, Cengiz HARK, Ali KARCİ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "Dijital haberlerin artan miktarları, istenilen türdeki haberlere doğru ve hızlı bir şekilde erişim için haber metinlerinin kategorilere ayrılmasını gerektirmektedir. Bu çalışmada, ön-eğitimli kelime gömülmelerinin, Uzun Ömürlü Kısa Dönem Bellek Ağı (Long-Short Term Memory, LSTM) ve Evrişimsel Sinir Ağları (Convolutional Neural Network, CNN) gibi derin öğrenme modelleri üzerindeki etkisi araştırılmaktadır. Global Vektör (GloVe) kelime gömülmelerinden alınan bağlamsal temsilleri girdi olarak alan LSTM ve CNN ağları kullanılarak haber metinleri sınıflandırılmıştır. Kapsamlı ve karşılaştırmalı araştırmaların eksikliği nedeniyle GloVe gömme katmanı tarafından sağlanan bağlamsal temsiller farklı sınıflandırıcılar ve veri setleri üzerinde test edilmektedir. Deneysel süreçler boyunca Türkçe Haber başlıklarından oluşan Turkish Headlines veri seti ve BBC News Classification veri setleri kullanılmıştır. Kelime gömülmelerinin ağlar üzerindeki etkisini ortaya koymak için deneysel süreçler aynı parametreler ile tekrarlanmıştır. LSTM modelinde Glove kelime gömülme yöntemi kullanıldığında modelin başarısının %81’den %91’e çıktığı gözlemlenmektedir. CNN modelinde ise Glove kelime gömülmelerinin modelin başarısının olumlu yansımadığı görülmektedir.",
        "link": "http://dx.doi.org/10.29132/ijpas.1265301"
    },
    {
        "id": 9688,
        "title": "Sentiment Classification Performance Analysis Based on Glove Word Embedding",
        "authors": "Yasin KIRELLİ, Şebnem ÖZDEMİR",
        "published": "2021-6-30",
        "citations": 2,
        "abstract": "Representation of words in mathematical expressions is an essential issue in natural language processing. In this study, data sets in different categories are classified as positive or negative according to their content. Using the Glove (Global Vector for Word Representation) method, which is one of the word embedding methods, the effect of the vector set based on the word similarities previously calculated on the classification performance has been analyzed. In this study, the effect of pretrained, embedded and deterministic word embedding classification performance has analyzed by using Long Short Term Memory (LSTM). The porposed LSTM based deep learning model has been tested on three different data sets and the results was evaluated.",
        "link": "http://dx.doi.org/10.16984/saufenbilder.886583"
    },
    {
        "id": 9689,
        "title": "RGloVe: An Improved Approach of Global Vectors for Distributional Entity Relation Representation",
        "authors": "Ziyan Chen, Yu Huang, Yuexian Liang, Yang Wang, Xingyu Fu, Kun Fu",
        "published": "2017-4-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/a10020042"
    },
    {
        "id": 9690,
        "title": "The Significance of Global Vectors Representation in Sarcasm Analysis",
        "authors": "Christopher Ifeanyi Eke, Azah Norman, Liyana Shuib, Faith B. Fatokun, Isaiah Omame",
        "published": "2020-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmcecs47690.2020.246997"
    },
    {
        "id": 9691,
        "title": "Unsupervised modeling anomaly detection in discussion forums posts using global vectors for text representation",
        "authors": "Paweł Cichosz",
        "published": "2020-9",
        "citations": 8,
        "abstract": "AbstractAnomaly detection can be seen as an unsupervised learning task in which a predictive model created on historical data is used to detect outlying instances in new data. This work addresses possibly promising but relatively uncommon application of anomaly detection to text data. Two English-language and one Polish-language Internet discussion forums devoted to psychoactive substances received from home-grown plants, such as hashish or marijuana, serve as text sources that are both realistic and possibly interesting on their own, due to potential associations with drug-related crime. The utility of two different vector text representations is examined: the simple bag of words representation and a more refined Global Vectors (GloVe) representation, which is an example of the increasingly popular word embedding approach. They are both combined with two unsupervised anomaly detection methods, based on one-class support vector machines (SVM) and based on dissimilarity to k-medoids clusters. The GloVe representation is found definitely more useful for anomaly detection, permitting better detection quality and ameliorating the curse of dimensionality issues with text clustering. The cluster dissimilarity approach combined with this representation outperforms one-class SVM with respect to detection quality and appears a more promising approach to anomaly detection in text data.",
        "link": "http://dx.doi.org/10.1017/s1351324920000066"
    },
    {
        "id": 9692,
        "title": "Geometric Probing of Word Vectors",
        "authors": "Madina Babazhanova, Maxat Tezekbayev, Zhenisbek Assylbekov",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-105"
    },
    {
        "id": 9693,
        "title": "Improving Semantic Similarity of Words by Retrofitting Word Vectors in Sense Level",
        "authors": "Rui Zhang, Peter Schneider-Kamp, Arthur Zimek",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008953001080119"
    },
    {
        "id": 9694,
        "title": "Constructing a Word Similarity Graph from Vector based Word Representation for Named Entity Recognition",
        "authors": "Miguel Feria, Juan Paolo Balbin, Francis Michael Bautista",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006926201660171"
    },
    {
        "id": 9695,
        "title": "Context Vectors are Reflections of Word Vectors in Half the Dimensions",
        "authors": "Zhenisbek Assylbekov, Rustem Takhanov",
        "published": "2019-9-18",
        "citations": 3,
        "abstract": "This paper takes a step towards theoretical analysis of the relationship between word embeddings and context embeddings in models such as word2vec. We start from basic probabilistic assumptions on the nature of word vectors, context vectors, and text generation. These assumptions are supported either empirically or theoretically by the existing literature. Next, we show that under these assumptions the widely-used word-word PMI matrix is approximately a random symmetric Gaussian ensemble. This, in turn, implies that context vectors are reflections of word vectors in approximately half the dimensions. As a direct application of our result, we suggest a theoretically grounded way of tying weights in the SGNS model.",
        "link": "http://dx.doi.org/10.1613/jair.1.11368"
    },
    {
        "id": 9696,
        "title": "TeamCEN at SemEval-2018 Task 1: Global Vectors Representation in\n            Emotion Detection",
        "authors": "Anon George, Barathi Ganesh H. B., Anand Kumar M, Soman K P",
        "published": "2018",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/s18-1050"
    },
    {
        "id": 9697,
        "title": "Representation and Processing of Multi-Word Expressions in First and Second Language Speakers",
        "authors": "Anna Siyanova",
        "published": "2023-8-22",
        "citations": 0,
        "abstract": "Multi-word expressions (henceforth, MWEs) are familiar, conventional sequences of two or more words in length. Common examples of MWEs include collocations (e.g., provide service, helping hand), binomials (e.g., bride and groom, research and development), lexical bundles (e.g., in line with, contrary to), idioms (e.g., tie the knot, spill the beans), proverbs (e.g., better late than never, early bird catches the worm), and other phrasal elements. MWEs vary vastly in their properties, but what they have in common is that proficient language users recognize them as familiar, conventional sequences. Because MWEs are familiar, established, and predictable ways of expressing thoughts and ideas, they render our discourse natural, idiomatic, and easily comprehensible. As has been shown in the literature, it is easier and more efficient to acquire and use language in chunks rather than having to create word combinations anew. Chunking has been recognized as an important strategy in linguistic processing, necessary for smooth and efficient language comprehension and production. MWEs have been of interest to scholars from a range of related disciplines—theoretical and applied linguistics, computational and corpus linguistics, psycholinguistics and cognitive science, neurolinguistics, and clinical linguistics. As a result, MWEs have been studied from a variety of distinct yet complementary perspectives that probed their acquisition, on-line processing (comprehension and production), and use by first (L1) and second (L2) language speakers, adults and children, as well as individuals with language disorders. These complementary approaches have painted an extremely detailed picture of MWEs as a complex linguistic phenomenon. The perspective covered in the present article will largely center on the cognitive and psycholinguistic properties and realities of MWEs, with a particular focus on MWE representation and on-line processing (that is, processing happening in real time).",
        "link": "http://dx.doi.org/10.1093/obo/9780199772810-0308"
    },
    {
        "id": 9698,
        "title": "A Word Selection Method for Producing Interpretable Distributional Semantic Word Vectors",
        "authors": "Atefe Pakzad, Morteza Analoui",
        "published": "2021-12-11",
        "citations": 0,
        "abstract": "Distributional semantic models represent the meaning of words as vectors. We introduce a selection method to learn a vector space that each of its dimensions is a natural word. The selection method starts from the most frequent words and selects a subset, which has the best performance. The method produces a vector space that each of its dimensions is a word. This is the main advantage of the method compared to fusion methods such as NMF, and neural embedding models. We apply the method to the ukWaC corpus and train a vector space of N=1500 basis words. We report tests results on word similarity tasks for MEN, RG-65, SimLex-999, and WordSim353 gold datasets. Also, results show that reducing the number of basis vectors from 5000 to 1500 reduces accuracy by about 1.5-2%. So, we achieve good interpretability without a large penalty. Interpretability evaluation results indicate that the word vectors obtained by the proposed method using N=1500 are more interpretable than word embedding models, and the baseline method. We report the top 15 words of 1500 selected basis words in this paper.",
        "link": "http://dx.doi.org/10.1613/jair.1.13353"
    },
    {
        "id": 9699,
        "title": "A Comparative Study on Improving Word Embeddings Beyond Word2Vec and GloVe",
        "authors": "Rajdeep Biswas, Suman De",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pdgc56933.2022.10053200"
    },
    {
        "id": 9700,
        "title": "Context Vectors Are Reflections of Word Vectors in Half the Dimensions (Extended Abstract)",
        "authors": "Zhenisbek Assylbekov, Rustem Takhanov",
        "published": "2020-7",
        "citations": 0,
        "abstract": "This paper takes a step towards the theoretical analysis of the relationship between word embeddings and context embeddings in models such as word2vec. We start from basic probabilistic assumptions on the nature of word vectors, context vectors, and text generation. These assumptions are supported either empirically or theoretically by the existing literature. Next, we show that under these assumptions the widely-used word-word PMI matrix is approximately a random symmetric Gaussian ensemble. This, in turn, implies that context vectors are reflections of word vectors in approximately half the dimensions. As a direct application of our result, we suggest a theoretically grounded way of tying weights in the SGNS model.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/718"
    },
    {
        "id": 9701,
        "title": "Is this URL Safe: Detection of Malicious URLs Using Global Vector for Word Representation",
        "authors": "Rohit Bharadwaj, Ashutosh Bhatia, Laxmi Divya Chhibbar, Kamlesh Tiwari, Ankit Agrawal",
        "published": "2022-1-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin53446.2022.9687204"
    },
    {
        "id": 9702,
        "title": "Multidocument Summarization using GloVe Word Embedding and Agglomerative Cluster Methods",
        "authors": "Rosalina Rosalina, Rafiqul Huda, Genta Sahuri",
        "published": "2020-12-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsecc51444.2020.9557393"
    },
    {
        "id": 9703,
        "title": "Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors",
        "authors": "Shoaib Jameel, Steven Schockaert",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1321"
    },
    {
        "id": 9704,
        "title": "Non-word Strings in Open Source Word Vectors",
        "authors": "Xingyuan Chen, Peng Jin, Jiuhua Zhang, Caiming Liu",
        "published": "2022-6-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic54216.2022.9836708"
    },
    {
        "id": 9705,
        "title": "Traces of Meaning Itself: Encoding distributional word vectors in brain activity",
        "authors": "Jona Sassenhagen, Christian J. Fiebach",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractHow is semantic information stored in the human mind and brain? Some philosophers and cognitive scientists argue for vectorial representations of concepts, where the meaning of a word is represented as its position in a high-dimensional neural state space. At the intersection of natural language processing and artificial intelligence, a class of very successful distributional word vector models has developed that can account for classic EEG findings of language, i.e., the ease vs. difficulty of integrating a word with its sentence context. However, models of semantics have to account not only for context-based word processing, but should also describe how word meaning is represented. Here, we investigate whether distributional vector representations of word meaning can model brain activity induced by words presented without context. Using EEG activity (event-related brain potentials) collected while participants in two experiments (English, German) read isolated words, we encode and decode word vectors taken from the family of prediction-based word2vec algorithms. We find that, first, the position of a word in vector space allows the prediction of the pattern of corresponding neural activity over time, in particular during a time window of 300 to 500 ms after word onset. Second, distributional models perform better than a human-created taxonomic baseline model (WordNet), and this holds for several distinct vector-based models. Third, multiple latent semantic dimensions of word meaning can be decoded from brain activity. Combined, these results suggest that empiricist, prediction-based vectorial representations of meaning are a viable candidate for the representational architecture of human semantic knowledge.",
        "link": "http://dx.doi.org/10.1101/603837"
    },
    {
        "id": 9706,
        "title": "Sarcasm Identification in text with deep learning models and Glove word embedding",
        "authors": "Ganesh Chandrasekaran, D.Jude Hemanth, M. Saravanan",
        "published": "2022-11-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccis56430.2022.10037615"
    },
    {
        "id": 9707,
        "title": "Glove Word Embedding and DBSCAN algorithms for Semantic Document Clustering",
        "authors": "Shapol M. Mohammed, Karwan Jacksi, Subhi R. M. Zeebaree",
        "published": "2020-12-23",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoase51841.2020.9436540"
    },
    {
        "id": 9708,
        "title": "Extremal GloVe : Theoretically Accurate Distributed Word Embedding by Tail Inference",
        "authors": "Hao Wang",
        "published": "2021-12-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3507971.3507972"
    },
    {
        "id": 9709,
        "title": "Adaptive GloVe and FastText Model for Hindi Word Embeddings",
        "authors": "Vijay Gaikwad, Yashodhara Haribhakta",
        "published": "2020-1-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3371158.3371179"
    },
    {
        "id": 9710,
        "title": "Tom Jumbo-Grumbo at SemEval-2019 Task 4: Hyperpartisan News Detection with GloVe vectors and SVM",
        "authors": "Chia-Lun Yeh, Babak Loni, Anne Schuth",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/s19-2187"
    },
    {
        "id": 9711,
        "title": "Learning Sentiment-Specific Word Embedding via Global Sentiment Representation",
        "authors": "Peng Fu, Zheng Lin, Fengcheng Yuan, Weiping Wang, Dan Meng",
        "published": "2018-4-26",
        "citations": 15,
        "abstract": "\n      \n        Context-based word embedding learning approaches can model rich semantic and syntactic information. However, it is problematic for sentiment analysis because the words with similar contexts but opposite sentiment polarities, such as good and bad, are mapped into close word vectors in the embedding space. Recently, some sentiment embedding learning methods have been proposed, but most of them are designed to work well on sentence-level texts. Directly applying those models to document-level texts often leads to unsatisfied results. To address this issue, we present a sentiment-specific word embedding learning architecture that utilizes local context informationas well as global sentiment representation. The architecture is applicable for both sentence-level and document-level texts. We take global sentiment representation as a simple average of word embeddings in the text, and use a corruption strategy as a sentiment-dependent regularization. Extensive experiments conducted on several benchmark datasets demonstrate that the proposed architecture outperforms the state-of-the-art methods for sentiment classification.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.11916"
    },
    {
        "id": 9712,
        "title": "From positional representation of numbers to positional representation of vectors",
        "authors": "Izabella  Ingrid Farkas, Edita Pelantová, Milena Svobodová",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "To represent real m-dimensional vectors, a positional vector system given by a non-singular matrix M ∈ ℤm×m and a digit set Ɗ ⊂ ℤm is used. If m = 1, the system coincides with the well known numeration system used to represent real numbers. We study some properties of the vector systems which are transformable from the case m = 1 to higher dimensions. We focus on an algorithm for parallel addition and on systems allowing an eventually periodic representation of vectors with rational coordinates.",
        "link": "http://dx.doi.org/10.14311/ap.2023.63.0188"
    },
    {
        "id": 9713,
        "title": "Lexicographic Representation of the Word Bible and Its Word-Building Potential",
        "authors": "M.A. Malkerova",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31862/2073-9613-2022-1-296-303"
    },
    {
        "id": 9714,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Vipul Pandey",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/mbf6tt"
    },
    {
        "id": 9715,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Mihai Prunescu",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fcsz4n"
    },
    {
        "id": 9716,
        "title": "Supervised and Unsupervised Word Sense Disambiguation on Word\n            Embedding Vectors of Unambigous Synonyms",
        "authors": "Aleksander Wawer, Agnieszka Mykowiecka",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-1915"
    },
    {
        "id": 9717,
        "title": "Vector representation of words for sentiment analysis using GloVe",
        "authors": "Yash Sharma, Gaurav Agrawal, Pooja Jain, Tapan Kumar",
        "published": "2017-12",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intelcct.2017.8324059"
    },
    {
        "id": 9718,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Marc Cahay",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/rhylnp"
    },
    {
        "id": 9719,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Punam Gupta",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/kgiv9x"
    },
    {
        "id": 9720,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Sumaira Akhtar",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ep2omc"
    },
    {
        "id": 9721,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Chiara Hergl",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/rj280j"
    },
    {
        "id": 9722,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Yves Gourinat",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/1dkiq2"
    },
    {
        "id": 9723,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Venkatraman Gopalan",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/gbvg7v"
    },
    {
        "id": 9724,
        "title": "Memory-efficient Word Embedding Vectors",
        "authors": "Jun Suzuki, Masaaki Nagata",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53829/ntr201711fa4"
    },
    {
        "id": 9725,
        "title": "‘Word-for-Word’ Bible Comics and Sarah",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9780567687982.0009"
    },
    {
        "id": 9726,
        "title": "Distributional Word Vectors as Semantic Maps Framework",
        "authors": "Amir Bakarov",
        "published": "2022-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13053/cys-26-3-4356"
    },
    {
        "id": 9727,
        "title": "Towards Learning Word Representation",
        "authors": "Magdalena Wiercioch",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4467/20838476si.16.008.6189"
    },
    {
        "id": 9728,
        "title": "A Hierarchical Book Representation of Word Embeddings for Effective Semantic Clustering and Search",
        "authors": "Avi Bleiweiss",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006192701540163"
    },
    {
        "id": 9729,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Krunal B. Kachhia",
        "published": "2024-2-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/xxbr50"
    },
    {
        "id": 9730,
        "title": "Binary Paragraph Vectors",
        "authors": "Karol Grzegorczyk, Marcin Kurdziel",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-2615"
    },
    {
        "id": 9731,
        "title": "An Automated Method To Enrich Consumer Health Vocabularies Using GloVe Word Embeddings and An Auxiliary Lexical Resource (Preprint)",
        "authors": "Mohammed Ibrahim, Susan Gauch, Omar Salman, Mohammed Alqahatani",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nClear language makes communication easier between any two parties. A layman may have difficulty communicating with a professional due to not understanding the specialized terms common to the domain.  In healthcare, it is rare to find a layman knowledgeable in medical jargon which can lead to poor understanding of their condition and/or treatment.  To bridge this gap, several professional vocabularies and ontologies have been created to map laymen medical terms to professional medical terms and vice versa.\n\n\nOBJECTIVE\nMany of the presented vocabularies are built manually or semi-automatically requiring large investments of time and human effort and consequently the slow growth of these vocabularies.  In this paper, we present an automatic method to enrich laymen's vocabularies that has the benefit of being able to be applied to vocabularies in any domain.\n\n\nMETHODS\nOur entirely automatic approach uses machine learning, specifically Global Vectors for Word Embeddings (GloVe), on a corpus collected from a social media healthcare platform to extend and enhance consumer health vocabularies (CHV).  Our approach further improves the CHV by incorporating synonyms and hyponyms from the WordNet ontology. The basic GloVe and our novel algorithms incorporating WordNet were evaluated using two laymen datasets from the National Library of Medicine (NLM), Open-Access Consumer Health Vocabulary (OAC CHV) and MedlinePlus Healthcare Vocabulary.\n\n\nRESULTS\nThe results show that GloVe was able to find new laymen terms with an F-score of 48.44%. Furthermore, our enhanced GloVe approach outperformed basic GloVe with an average F-score of 61%, a relative improvement of 25%.\n\n\nCONCLUSIONS\nThis paper presents an automatic approach to enrich consumer health vocabularies using the GloVe word embeddings and an auxiliary lexical source, WordNet. Our approach was evaluated used a healthcare text downloaded from MedHelp.org, a healthcare social media platform using two standard laymen vocabularies, OAC CHV, and MedlinePlus. We used the WordNet ontology to expand the healthcare corpus by including synonyms, hyponyms, and hypernyms for each CHV layman term occurrence in the corpus. Given a seed term selected from a concept in the ontology, we measured our algorithms’ ability to automatically extract synonyms for those terms that appeared in the ground truth concept.  We found that enhanced GloVe outperformed GloVe with a relative improvement of 25% in the F-score.\n",
        "link": "http://dx.doi.org/10.2196/preprints.26160"
    },
    {
        "id": 9732,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Sirkka Liisa Eriksson",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/33dl1a"
    },
    {
        "id": 9733,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "Dr. Fiaz Hussain",
        "published": "2024-1-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/f9xl95"
    },
    {
        "id": 9734,
        "title": "Word Representation Learning",
        "authors": "Shengding Hu, Zhiyuan Liu, Yankai Lin, Maosong Sun",
        "published": "2023",
        "citations": 0,
        "abstract": "AbstractWords are the building blocks of phrases, sentences, and documents. Word representation is thus critical for natural language processing (NLP). In this chapter, we introduce the approaches for word representation learning to show the paradigm shift from symbolic representation to distributed representation. We also describe the valuable efforts in making word representations more informative and interpretable. Finally, we present applications of word representation learning to NLP and interdisciplinary fields, including psychology, social sciences, history, and linguistics.",
        "link": "http://dx.doi.org/10.1007/978-981-99-1600-9_2"
    },
    {
        "id": 9735,
        "title": "Representation of Race in Children’s Books",
        "authors": "Carol Westby",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1177/10483950231225114d"
    },
    {
        "id": 9736,
        "title": "Global Vectors Representation of Protein Sequences and Its Application for Predicting Self-Interacting Proteins with Multi-Grained Cascade Forest Model",
        "authors": " Chen,  You,  Zhang,  Wang,  Cheng,  Alghazzawi",
        "published": "2019-11-12",
        "citations": 9,
        "abstract": "Self-interacting proteins (SIPs) is of paramount importance in current molecular biology. There have been developed a number of traditional biological experiment methods for predicting SIPs in the past few years. However, these methods are costly, time-consuming and inefficient, and often limit their usage for predicting SIPs. Therefore, the development of computational method emerges at the times require. In this paper, we for the first time proposed a novel deep learning model which combined natural language processing (NLP) method for potential SIPs prediction from the protein sequence information. More specifically, the protein sequence is de novo assembled by k-mers. Then, we obtained the global vectors representation for each protein sequences by using natural language processing (NLP) technique. Finally, based on the knowledge of known self-interacting and non-interacting proteins, a multi-grained cascade forest model is trained to predict SIPs. Comprehensive experiments were performed on yeast and human datasets, which obtained an accuracy rate of 91.45% and 93.12%, respectively. From our evaluations, the experimental results show that the use of amino acid semantics information is very helpful for addressing the problem of sequences containing both self-interacting and non-interacting pairs of proteins. This work would have potential applications for various biological classification problems.",
        "link": "http://dx.doi.org/10.3390/genes10110924"
    },
    {
        "id": 9737,
        "title": "Learning Sense Representation from Word Representation for Unsupervised Word Sense Disambiguation (Student Abstract)",
        "authors": "Jie Wang, Zhenxin Fu, Moxin Li, Haisong Zhang, Dongyan Zhao, Rui Yan",
        "published": "2020-4-3",
        "citations": 1,
        "abstract": "Unsupervised WSD methods do not rely on annotated training datasets and can use WordNet. Since each ambiguous word in the WSD task exists in WordNet and each sense of the word has a gloss, we propose SGM and MGM to learn sense representations for words in WordNet using the glosses. In the WSD task, we calculate the similarity between each sense of the ambiguous word and its context to select the sense with the highest similarity. We evaluate our method on several benchmark WSD datasets and achieve better performance than the state-of-the-art unsupervised WSD systems.",
        "link": "http://dx.doi.org/10.1609/aaai.v34i10.7246"
    },
    {
        "id": 9738,
        "title": "Lexical stress representation in spoken word recognition",
        "authors": "Angeliki Andrikopoulou, Athanassios Protopapas, Amalia Arvaniti",
        "published": "No Date",
        "citations": 0,
        "abstract": "According to a popular model of speech production, stress is underspecified in the lexicon, that is, it is specified only for words with stress patterns other than the default, termed the “default metrics” assumption. Alternatively, stress may be fully specified in the lexicon as part of every lexical representation. In the current study the two accounts are tested in the perceptual domain using behavioral and eye-tracking data in Greek. In a first experiment, cross-modal fragment priming was used in a lexical decision task. According to default metrics, priming should occur for targets with antepenultimate- or final-syllable stress but not for targets with the default penultimate-syllable stress. The same word pairs were used in two subsequent visual world experiments. Default metrics predicts an asymmetric pattern of results, namely that incoming spoken words with the default stress pattern should inhibit the activation of lexical representations with nondefault stress, whereas the converse should not be observed; that is, spoken words with nondefault stress should not inhibit representations of words with the default stress. None of the results provided support for the idea of default metrics, leading to alternative conceptualizations regarding the representation of stress.",
        "link": "http://dx.doi.org/10.31234/osf.io/stjha"
    },
    {
        "id": 9739,
        "title": "PERBANDINGAN KINERJA WORD EMBEDDING WORD2VEC, GLOVE, DAN FASTTEXT PADA KLASIFIKASI TEKS",
        "authors": "Arliyanti Nurdin, Bernadus Anggo Seno Aji, Anugrayani Bustamin, Zaenal Abidin",
        "published": "2020-8-15",
        "citations": 19,
        "abstract": "Karakteristik teks yang tidak terstruktur menjadi tantangan dalam ekstraksi fitur pada bidang pemrosesan teks. Penelitian ini bertujuan untuk membandingkan kinerja dari word embedding  seperti Word2Vec, GloVe dan FastText dan diklasifikasikan dengan algoritma Convolutional Neural Network. Ketiga metode ini dipilih karena dapat menangkap makna semantik, sintatik, dan urutan bahkan konteks di sekitar kata jika dibandingkan dengan feature engineering tradisional seperti Bag of Words. Proses word embedding dari metode tersebut akan dibandingkan kinerjanya pada klasifikasi berita dari dataset 20 newsgroup dan Reuters Newswire. Evaluasi kinerja diukur menggunakan F-measure. Performa terbaik menunjukkan FastText unggul dibanding dua metode word embedding lainnya dengan nilai F-Measure  sebesar 0.979 untuk dataset 20 Newsgroup dan 0.715 untuk Reuters. Namun, perbedaan kinerja yang tidak begitu signifikan antar ketiga word embedding tersebut menunjukkan bahwa ketiga word embedding ini memiliki kinerja yang kompetitif. Penggunaannya sangat bergantung pada dataset yang digunakan dan permasalahan yang ingin diselesaikan.Kata kunci: word embedding, word2vec, glove, fasttext, klasfikasi teks, convolutional neural network, cnn.",
        "link": "http://dx.doi.org/10.33365/jtk.v14i2.732"
    },
    {
        "id": 9740,
        "title": "Review of: \"Representation of physical quantities: From scalars, vectors, tensors and spinors to multivectors\"",
        "authors": "A. R. P. Rau",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/vi6at9"
    },
    {
        "id": 9741,
        "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
        "authors": "Samik Sadhu, Hynek Hermansky",
        "published": "2019-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2723"
    },
    {
        "id": 9742,
        "title": "Music genre classification with word and document vectors",
        "authors": "Onder Coban, Isil Karabey",
        "published": "2017-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu.2017.7960145"
    },
    {
        "id": 9743,
        "title": "Self-supervised Post-processing Method to Enrich Pretrained Word Vectors",
        "authors": "Hwiyeol Jo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.54"
    },
    {
        "id": 9744,
        "title": "Sentiment-semantic word vectors - A new method to estimate management sentiment",
        "authors": "Minh Tri Phan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4723125"
    },
    {
        "id": 9745,
        "title": "Behavioral correlates of cortical semantic representations modeled by word vectors",
        "authors": "Satoshi Nishida, Antione Blanc, Naoya Maeda, Masataka Kado, Shinji Nishimoto",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe quantitative modeling of semantic representations in the brain plays a key role in understanding the neural basis of semantic processing. Previous studies have demonstrated that word vectors, which were originally developed for use in the field of natural language processing, provide a powerful tool for such quantitative modeling. However, whether semantic representations revealed by the word vector-based models actually capture our perception of semantic information remains unclear, as there has been no study explicitly examining the behavioral correlates of the modeled semantic representations. To address this issue, we compared the semantic structure of nouns and adjectives estimated from word vector-based brain models with that evaluated from human behavior. The brain models were constructed using voxelwise modeling to predict the functional magnetic resonance imaging (fMRI) response to natural movies from semantic contents in each movie scene through a word vector space. The semantic dissimilarity of word representations was then evaluated using the brain models. Meanwhile, data on human behavior reflecting the perception of semantic dissimilarity between words were collected in psychological experiments. We found a significant correlation between brain model- and behavior-derived semantic dissimilarities of words. This finding suggests that semantic representations in the brain modeled via word vectors appropriately capture our perception of word meanings.Author summeryWord vectors, which have been originally developed in the field of engineering (natural language processing), have been extensively leveraged in neuroscience studies to model semantic representations in the human brain. These studies have attempted to model brain semantic representations by associating them with the meanings of thousands of words via a word vector space. However, there has been no study explicitly examining whether the modeled semantic representations actually capture our perception of semantic information. To address this issue, we compared the semantic representational structure of words estimated from word vector-based brain models with that evaluated from behavioral data in psychological experiments. The results revealed a significant correlation between these model- and behavior-derived semantic representational structures of words. This indicates that the brain semantic representations modeled using word vectors actually reflect the human perception of word meanings. Our findings contribute to the establishment of word vector-based brain modeling as a useful tool in studying human semantic processing.",
        "link": "http://dx.doi.org/10.1101/2020.08.06.240705"
    },
    {
        "id": 9746,
        "title": "Motion Capture Glove: An Intuitive Wearable Input Device",
        "authors": "Suzuki Yasuhiro, Jing Lei",
        "published": "2018-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce.2018.8574808"
    },
    {
        "id": 9747,
        "title": "Deteksi Bot Spammer Twitter Berbasis Time Interval Entropy dan Global Vectors for Word Representations Tweet’s Hashtag",
        "authors": "Arif Mudi Priyatno, Muhammad Mirza Muttaqi, Fahmi Syuhada, Agus Zainal Arifin",
        "published": "2019-1-1",
        "citations": 1,
        "abstract": "Bot spammer merupakan penyalahgunaan user dalam menggunakan Twitter untuk menyebarkan pesan spam sesuai dengan keinginan user. Tujuan spam mencapai trending topik yang ingin dibuatnya. Penelitian ini mengusulkan deteksi bot spammer pada Twitter berbasis Time Interval Entropy dan global vectors for word representations (Glove). Time Interval Entropy digunakan untuk mengklasifikasi akun bot berdasarkan deret waktu pembuatan tweet. Glove digunakan untuk melihat co-occurrence kata tweet yang disertai Hashtag untuk proses klasifikasi menggunakan Convolutional Neural Network (CNN). Penelitian ini menggunakan data API Twitter dari 18 akun bot dan 14 akun legitimasi dengan 1.000 tweet per akunnya. Hasil terbaik recall, precision, dan f-measure yang didapatkan yaitu 100%; 100%, dan 100%. Hal ini membuktikan bahwa Glove dan Time Interval Entropy sukses mendeteksi bot spammer dengan sangat baik. Hashtag memiliki pengaruh untuk meningkatkan deteksi bot spammer.  Spam spammers are users' misuse of using Twitter to spread spam messages in accordance with user wishes. The purpose of spam is to reach the required trending topic. This study proposes detection of bot spammers on Twitter based on Time Interval Entropy and global vectors for word representations (Glove). Time Interval Entropy is used to classify bot accounts based on the tweet's time series, while glove views the co-occurrence of tweet words with Hashtags for classification processes using the Convolutional Neural Network (CNN). This study uses Twitter API data from 18 bot accounts and 14 legitimacy accounts with 1000 tweets per account. The best results of recall, precision, and f-measure were 100%respectively. This proves that Glove and Time Interval Entropy successfully detects spams, with Hash tags able to increase the detection of bot spammers.",
        "link": "http://dx.doi.org/10.26594/register.v5i1.1382"
    },
    {
        "id": 9748,
        "title": "S&amp;P 500 Daily Index Time Series Forecasting Given Global News Headlines Using LSTM, BERT, and GloVe Embeddings",
        "authors": "Arav Santhanam",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58445/rars.54"
    },
    {
        "id": 9749,
        "title": "Representation of Frege’s Logical Symbols",
        "authors": "",
        "published": "2020-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/oso/9780198865476.002.0009"
    },
    {
        "id": 9750,
        "title": "The Glove Capital of America",
        "authors": "",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/jj.9345414.11"
    },
    {
        "id": 9751,
        "title": "Evaluation of Domain-Specific Word Vectors for Biomedical Word Sense Disambiguation",
        "authors": "Dennis Toddenroth",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "Among medical applications of natural language processing (NLP), word sense disambiguation (WSD) estimates alternative meanings from text around homonyms. Recently developed NLP methods include word vectors that combine easy computability with nuanced semantic representations. Here we explore the utility of simple linear WSD classifiers based on aggregating word vectors from a modern biomedical NLP library in homonym contexts. We evaluated eight WSD tasks that consider literature abstracts as textual contexts. Discriminative performance was measured in held-out annotations as the median area under sensitivity-specificity curves (AUC) across tasks and 200 bootstrap repetitions. We find that classifiers trained on domain-specific vectors outperformed those from a general language model by 4.0 percentage points, and that a preprocessing step of filtering stopwords and punctuation marks enhanced discrimination by another 0.7 points. The best models achieved a median AUC of 0.992 (interquartile range 0.975 – 0.998). These improvements suggest that more advanced WSD methods might also benefit from leveraging domain-specific vectors derived from large biomedical corpora.",
        "link": "http://dx.doi.org/10.3233/shti220314"
    },
    {
        "id": 9752,
        "title": "Word Distributions: Document-Term Matrices of Word Frequencies and the “Bag of Words” Representation",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781071878828.n8"
    },
    {
        "id": 9753,
        "title": "Learning Word and Sub-word Vectors for Amharic (Less Resourced Language)",
        "authors": "Abebawu Eshetu, Getenesh Teshome, Tewodros Abebe",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22161/ijaers.78.39"
    },
    {
        "id": 9754,
        "title": "Combining FastText and Glove Word Embedding for Offensive and Hate speech Text Detection",
        "authors": "Nabil Badri, Ferihane Kboubi, Anja Habacha Chaibi",
        "published": "2022",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.09.132"
    },
    {
        "id": 9755,
        "title": "Disaster Event Classification on Twitter: A Comparison of LSTM and GRU Algorithms using Word Embedding FastText and GloVe",
        "authors": "Ade Febrian, Arif Dwi Laksito",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwaiip58158.2023.10462842"
    },
    {
        "id": 9756,
        "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation",
        "authors": "Marianna Apidianaki",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "Abstract\nVector-based word representation paradigms situate lexical meaning at different levels of abstraction. Distributional and static embedding models generate a single vector per word type, which is an aggregate across the instances of the word in a corpus. Contextual language models, on the contrary, directly capture the meaning of individual word instances. The goal of this survey is to provide an overview of word meaning representation methods, and of the strategies that have been proposed for improving the quality of the generated vectors. These often involve injecting external knowledge about lexical semantic relationships, or refining the vectors to describe different senses. The survey also covers recent approaches for obtaining word type-level representations from token-level ones, and for combining static and contextualized representations. Special focus is given to probing and interpretation studies aimed at discovering the lexical semantic knowledge that is encoded in contextualized representations. The challenges posed by this exploration have motivated the interest towards static embedding derivation from contextualized embeddings, and for methods aimed at improving the similarity estimates that can be drawn from the space of contextual language models.",
        "link": "http://dx.doi.org/10.1162/coli_a_00474"
    },
    {
        "id": 9757,
        "title": "Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons",
        "authors": "Hwiyeol Jo, Stanley Jungkyu Choi",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-3003"
    },
    {
        "id": 9758,
        "title": "Chinese Sentiment Analysis of MOOC Reviews Based on Word Vectors",
        "authors": "Hua Yang",
        "published": "2021-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaie53562.2021.00021"
    },
    {
        "id": 9759,
        "title": "Word Vectors for Criticism of a Korean Film - 'Decision to Leave' by Chanwook Park",
        "authors": "Kwangho Ko, Juryon Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "This research presents a novel approach to film analysis, leveraging word vectors to objectively evaluate movie critiques. Focusing on Director Chanwook Park’s award-winning film, ’Decision to Leave’, the study employs word vectors derived from the movie’s script of Korean text. Traditional critiques often emphasize contrasting elements and themes, but their subjective nature poses challenges in objective validation. To address this, we trained a language model using LSTM on the film’s script, obtaining word vectors that capture the essence of the narrative. These vectors were then used to perform various text analyses, including similarity and analogy operation for the keywords suggested by the critiques. By comparing the semantic relationships in the critiques with those derived from the word vectors, we could objectively validate the critiques’ assertions. Furthermore, we visualized the word vectors in a two-dimensional space, confirming the spatial relationships of key terms highlighted in critiques. The study underscores the potential of word vectors in providing a more objective lens for film analysis, bridging the gap between traditional film criticism and data-driven insights.",
        "link": "http://dx.doi.org/10.20944/preprints202311.1558.v1"
    }
]