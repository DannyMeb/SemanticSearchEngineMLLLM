[
    {
        "id": 21971,
        "title": "Learning Accurate Integer Transformer Machine-Translation Models",
        "authors": "Ephrem Wu",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-021-00688-4"
    },
    {
        "id": 21972,
        "title": "ANLP-RG at NADI 2023 shared task: Machine Translation of Arabic Dialects: A Comparative Study of Transformer Models",
        "authors": "Wiem Derouich, Sameh Kchaou, Rahma Boujelbane",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.75"
    },
    {
        "id": 21973,
        "title": "Incorporating Source Syntax into Transformer-Based Neural Machine Translation",
        "authors": "Anna Currey, Kenneth Heafield",
        "published": "2019",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-5203"
    },
    {
        "id": 21974,
        "title": "Neural Translation Models",
        "authors": "",
        "published": "2020-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108608480.009"
    },
    {
        "id": 21975,
        "title": "The En-Ru Two-way Integrated Machine Translation System Based on Transformer",
        "authors": "Doron Yu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-5349"
    },
    {
        "id": 21976,
        "title": "Implementation of Traditional Vs. Transformer Machine\nLearning Models",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29121/web/v18i4/144"
    },
    {
        "id": 21977,
        "title": "Improving machine translation and post-editing for Chinese tourism texts using transformer-based models",
        "authors": "Yuan Feng, Qiwei Hu, Siran Yang",
        "published": "2024-2-4",
        "citations": 0,
        "abstract": "As the digital age and globalization continue to evolve, the demand for accurate machine translation of tourism texts has increased substantially. This paper investigates how to improve the quality of machine translation (MT) and machine translation post-editing (MTPE) of Chinese tourism texts for non-native speakers. A review of the machine translation literature reveals a significant progression in translation methods from rule-based to corpus-based, statistical, and finally to the current neural machine translation (NMT) models. Despite its advanced capabilities, NMT requires large amounts of parallel data for training, which often presents challenges. This study proposes the use of Transformer-based models for MT and MTPE to improve translation quality. A dataset was curated from online sources, mainly Chinese tourism websites. The methodology involved pre-processing the data, performing machine translation using the Transformer model, and post-editing the results. The experiment demonstrated an increase in the BLEU score, suggesting an improvement in translation quality. However, challenges such as the handling of synonyms and geographical nouns were encountered, indicating the need for further research and model optimization.",
        "link": "http://dx.doi.org/10.54254/2755-2721/34/20230286"
    },
    {
        "id": 21978,
        "title": "Simultaneous paraphrasing and translation by fine-tuning Transformer models",
        "authors": "Rakesh Chada",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.ngt-1.23"
    },
    {
        "id": 21979,
        "title": "Advanced Machine Translation with Linguistic-Enhanced Transformer",
        "authors": "Richard Patricia, Judith Sadok, Rodolfo Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recent advancements in neural language models, particularly attention-based architectures like the Transformer, have substantially surpassed traditional methods in various natural language processing tasks. These models adeptly generate nuanced token representations by considering contextual relations within a sequence. However, augmenting these models with explicit syntactic knowledge, such as part of speech tags, has been found to remarkably bolster their effectiveness, especially under constrained data scenarios. This study introduces the Linguistic Enhanced Transformer (LET), which integrates multiple syntactic features, showing a notable increase in translation accuracy, evidenced by an improvement of up to 1.99 BLEU points on subsets of the WMT '14 English-German dataset. Furthermore, this paper demonstrates that enriching BERT models with syntax-aware embeddings enhances their performance on several GLUE benchmark tasks.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0186.v1"
    },
    {
        "id": 21980,
        "title": "Learning Deep Transformer Models for Machine Translation",
        "authors": "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao",
        "published": "2019",
        "citations": 223,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1176"
    },
    {
        "id": 21981,
        "title": "Parallel feature weight decay algorithms for fast development of machine translation models",
        "authors": "Ergun Biçici",
        "published": "2021-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10590-021-09275-z"
    },
    {
        "id": 21982,
        "title": "Multi-Source Transformer for Kazakh-Russian-English Neural Machine Translation",
        "authors": "Patrick Littell, Chi-kiu Lo, Samuel Larkin, Darlene Stewart",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-5326"
    },
    {
        "id": 21983,
        "title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem",
        "authors": "Daria Grechishnikova",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractDrug discovery for a protein target is a very laborious, long and costly process. Machine learning approaches and, in particular, deep generative networks can substantially reduce development time and costs. However, the majority of methods imply prior knowledge of protein binders, their physicochemical characteristics or the three-dimensional structure of the protein. The method proposed in this work generates novel molecules with predicted ability to bind a target protein by relying on its amino acid sequence only. We consider target-specific de novo drug design as a translational problem between the amino acid “language” and SMILES (Simplified Molecular Input Line Entry System) representation of the molecule. To tackle this problem, we apply Transformer neural network architecture, a state-of-the-art approach in sequence transduction tasks. Transformer is based on a self-attention technique, which allows the capture of long-range dependencies between items in sequence. The model generates realistic diverse compounds with structural novelty. The computed physicochemical properties and common metrics used in drug discovery fall within the plausible drug-like range of values.",
        "link": "http://dx.doi.org/10.1101/863415"
    },
    {
        "id": 21984,
        "title": "Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task",
        "authors": "Brian Tubay, Marta R. Costa-jussà",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6449"
    },
    {
        "id": 21985,
        "title": "A Closer Look at Transformer Attention for Multilingual Translation",
        "authors": "Jingyi Zhang, Gerard de Melo, Hongfei Xu, Kehai Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.45"
    },
    {
        "id": 21986,
        "title": "NLPRL at WAT2019: Transformer-based Tamil – English Indic Task Neural Machine Translation System",
        "authors": "Amit Kumar, Anil Kumar Singh",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5222"
    },
    {
        "id": 21987,
        "title": "TranSFormer: Slow-Fast Transformer for Machine Translation",
        "authors": "Bei Li, Yi Jing, Xu Tan, Zhen Xing, Tong Xiao, Jingbo Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.430"
    },
    {
        "id": 21988,
        "title": "Accelerating Transformer for Neural Machine Translation",
        "authors": "Li Huang, Wenyu Chen, Hong Qu",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3457682.3457711"
    },
    {
        "id": 21989,
        "title": "Ensembling Factored Neural Machine Translation Models for Automatic\n            Post-Editing and Quality Estimation",
        "authors": "Chris Hokamp",
        "published": "2017",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-4775"
    },
    {
        "id": 21990,
        "title": "The representational geometry of word meanings acquired by neural machine translation models",
        "authors": "Felix Hill, Kyunghyun Cho, Sébastien Jean, Yoshua Bengio",
        "published": "2017-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10590-017-9194-2"
    },
    {
        "id": 21991,
        "title": "Genetic Algorithm-based Transformer Architecture Design for Neural Machine Translation",
        "authors": "Jie Wu, Ben Feng, Yanan Sun",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3568199.3568215"
    },
    {
        "id": 21992,
        "title": "Transformer: Linking Atom Mapping and Neural Machine Translation",
        "authors": "Chengyun Zhang, Ling Wang, Yejian Wu, Yun Zhang, An Su, Hongliang Duan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div><br></div><div><p> Atom mapping reveals the corresponding relationship between reactant and product atoms in chemical reactions, which is important for drug design, exploration for underlying chemical mechanism, reaction classification and so on. Here, we present a new method that links atom mapping and neural machine translation using the transformer model. In contrast to the previous algorithms, our method runs reaction prediction and captures the information of corresponding atoms in parallel. Meanwhile, we use a set of approximately 360K reactions without atom mapping information for obtaining general chemical knowledge and transfer it to atom mapping task on another dataset which contains 50K atom-mapped reactions. With manual evaluation, the top-1 accuracy of the transformer model in atom mapping reaches 91.4%. we hope our work can provide an important step toward solving the challenge problem of atom mapping in a linguistic perspective.</p></div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13173674"
    },
    {
        "id": 21993,
        "title": "CUNI Transformer Neural MT System for WMT18",
        "authors": "Martin Popel",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6424"
    },
    {
        "id": 21994,
        "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models",
        "authors": "Vivek Iyer, Pinzhen Chen, Alexandra Birch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.44"
    },
    {
        "id": 21995,
        "title": "Neural Language Models",
        "authors": "",
        "published": "2020-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108608480.008"
    },
    {
        "id": 21996,
        "title": "Transformer: Linking Atom Mapping and Neural Machine Translation",
        "authors": "Chengyun Zhang, Ling Wang, Yejian Wu, Yun Zhang, An Su, Hongliang Duan",
        "published": "No Date",
        "citations": 1,
        "abstract": " Atom mapping reveals the corresponding relationship between reactant and product atoms in chemical reactions, which is important for drug design, exploration for underlying chemical mechanism, reaction classification and so on. Here, we present a new method that links atom mapping and neural machine translation using the transformer model. In contrast to the previous algorithms, our method runs reaction prediction and captures the information of corresponding atoms in parallel. Meanwhile, we use a set of approximately 360K reactions without atom mapping information for obtaining general chemical knowledge and transfer it to atom mapping task on another dataset which contains 50K atom-mapped reactions. With manual evaluation, the top-1 accuracy of the transformer model in atom mapping reaches 91.4%. we hope our work can provide an important step toward solving the challenge problem of atom mapping in a linguistic perspective.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13173674.v1"
    },
    {
        "id": 21997,
        "title": "Document-Level Language Models for Machine Translation",
        "authors": "Frithjof Petrick, Christian Herold, Pavel Petrushkov, Shahram Khadivi, Hermann Ney",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.39"
    },
    {
        "id": 21998,
        "title": "An Improved LA-Transformer Machine Translation Model",
        "authors": "Zumin Wang, Chengye Zhang, Fengbo Bai, Yingjie Wang",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10448708"
    },
    {
        "id": 21999,
        "title": "Fully Quantized Transformer for Machine Translation",
        "authors": "Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh",
        "published": "2020",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.1"
    },
    {
        "id": 22000,
        "title": "Transformer-Based Unified Neural Network for Quality Estimation and Transformer-Based Re-decoding Model for Machine Translation",
        "authors": "Cong Chen, Qinqin Zong, Qi Luo, Bailian Qiu, Maoxi Li",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6162-1_6"
    },
    {
        "id": 22001,
        "title": "Boosting English-Amharic machine translation using corpus augmentation and Transformer",
        "authors": "Yohannes Biadgligne, Kamel Smaili",
        "published": "2024",
        "citations": 0,
        "abstract": "The Transformer-based neural machine translation (NMT) model has been very successful in recent years and has become a new mainstream method. However, using them in lowresourced languages requires large amounts of data and efficient model configuration (hyperparameter tuning) mechanisms. The scarcity of parallel texts is a bottleneck for high quality (N) MTs, especially for under resourced languages like Amharic. As a result, this paper presents an attempt to improve English-Amharic MT by introducing three different vanilla Transformer architectures, with different hyper-parameter values. To obtain additional training material, offline token level corpus augmentation was applied to the previously collected English-Amharic parallel corpus. Compared to previous work on Amharic MT, the best of the three Transformer models have achieved state-of-the-art BLEU scores. In fact, we were able to achieve this result by employing corpus augmentation techniques and hyper-parameter tuning.",
        "link": "http://dx.doi.org/10.59671/mbulj"
    },
    {
        "id": 22002,
        "title": "Optimizing Transformer for Low-Resource Neural Machine Translation",
        "authors": "Ali Araabi, Christof Monz",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.304"
    },
    {
        "id": 22003,
        "title": "Semantic and syntactic information for neural machine translation",
        "authors": "Jordi Armengol-Estapé, Marta R. Costa-jussà",
        "published": "2021-4",
        "citations": 5,
        "abstract": "AbstractIntroducing factors such as linguistic features has long been proposed in machine translation to improve the quality of translations. More recently, factored machine translation has proven to still be useful in the case of sequence-to-sequence systems. In this work, we investigate whether this gains hold in the case of the state-of-the-art architecture in neural machine translation, the Transformer, instead of recurrent architectures. We propose a new model, the Factored Transformer, to introduce an arbitrary number of word features in the source sequence in an attentional system. Specifically, we suggest two variants depending on the level at which the features are injected. Moreover, we suggest two combination mechanisms for the word features and words themselves. We experiment both with classical linguistic features and semantic features extracted from a linked data database, and with two low-resource datasets. With the best-found configuration, we show improvements of 0.8 BLEU over the baseline Transformer in the IWSLT German-to-English task. Moreover, we experiment with the more challenging FLoRes English-to-Nepali benchmark, which includes both  low-resource and very distant languages, and obtain an improvement of 1.2 BLEU. These improvements are achieved with linguistic and not with semantic information.",
        "link": "http://dx.doi.org/10.1007/s10590-021-09264-2"
    },
    {
        "id": 22004,
        "title": "Compressed-Transformer",
        "authors": "Yuan, Chen, Pan, Rong",
        "published": "2020-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3443279.3443302"
    },
    {
        "id": 22005,
        "title": "Multimodal Transformer for Multimodal Machine Translation",
        "authors": "Shaowei Yao, Xiaojun Wan",
        "published": "2020",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.400"
    },
    {
        "id": 22006,
        "title": "Guided Transformer for Machine Translation: English to Hindi",
        "authors": "Akhilesh Bisht, Deepa Gupta, Shantipriya Parida",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440876"
    },
    {
        "id": 22007,
        "title": "Transformer-based Automatic Post-Editing for Machine Translation",
        "authors": "Jaehun Shin, Youngkil Kim, Jong-hyeok Lee",
        "published": "2019-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2019.25.1.64"
    },
    {
        "id": 22008,
        "title": "Multimodal Neural Machine Translation Using CNN and Transformer Encoder",
        "authors": "Hiroki Takushima, Akihiro Tamura, Takashi Ninomiya, Hideki Nakayama",
        "published": "2019-4-2",
        "citations": 2,
        "abstract": "Multimodal machine translation uses images related to source language sentences as inputs to improve translation quality. Previous multimodal Neural Machine Translation (NMT) models, which incorporate visual features of each image region into an encoder for source language sentences or an attention mechanism between an encoder and a decoder, cannot catch the relation between visual features from each image region. This paper proposes a new multimodal NMT model, which encodes an input image using a Convolutional Neural Network (CNN) and a Transformer encoder. In particular, the proposed image encoder first extracts visual features from each image region using a CNN, and then encodes an input image on the basis of the extracted visual features using a Transformer encoder, where the relation between visual features from each image region are captured by a self-attention mechanism of the Transformer encoder. The experiments on the English-German translation task using the Multi30k data set show that the proposed model achieves 0.96 BLEU points improvement against a baseline Transformer NMT model without image inputs and 0.47 BLEU points improvement against a baseline multimodal Transformer NMT model without a Transformer encoder for images.",
        "link": "http://dx.doi.org/10.29007/hxhn"
    },
    {
        "id": 22009,
        "title": "A comparison of discriminative training criteria for continuous space translation models",
        "authors": "Alexandre Allauzen, Quoc Khanh Do, François Yvon",
        "published": "2017-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10590-017-9195-1"
    },
    {
        "id": 22010,
        "title": "Exploring Prompt Engineering with GPT Language Models for Document-Level Machine Translation: Insights and Findings",
        "authors": "Yangjian Wu, Gang Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.15"
    },
    {
        "id": 22011,
        "title": "Dual-Source Transformer Model for Neural Machine Translation with Linguistic Knowledge",
        "authors": "Yirong Pan, Xiao Li, Yating Yang, Rui Dong",
        "published": "No Date",
        "citations": 3,
        "abstract": "Incorporating source-side linguistic knowledge into the neural machine translation (NMT) model has recently achieved impressive performance on machine translation tasks. One popular method is to generalize the word embedding layer of the encoder to encode each word and its linguistic features. The other method is to change the architecture of the encoder to encode syntactic information. However, the former cannot explicitly balance the contribution from the word and its linguistic features. The latter cannot flexibly utilize various types of linguistic information. Focusing on the above issues, this paper proposes a novel NMT approach that models the words in parallel to the linguistic knowledge by using two separate encoders. Compared with the single encoder based NMT model, the proposed approach additionally employs the knowledge-based encoder to specially encode linguistic features. Moreover, it shares parameters across encoders to enhance the model representation ability of the source-side language. Extensive experiments show that the approach achieves significant improvements of up to 2.4 and 1.1 BLEU points on Turkish→English and English→Turkish machine translation tasks, respectively, which indicates that it is capable of better utilizing the external linguistic knowledge and effective improving the machine translation quality.",
        "link": "http://dx.doi.org/10.20944/preprints202002.0273.v1"
    },
    {
        "id": 22012,
        "title": "Transformer: A General Framework from Machine Translation to Others",
        "authors": "Yang Zhao, Jiajun Zhang, Chengqing Zong",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11633-022-1393-5"
    },
    {
        "id": 22013,
        "title": "Thai-English and English-Thai Translation Performance of Transformer Machine Translation",
        "authors": "Kanchana Saengthongpattana, Kanyanut Kriengket, Peerachet Porkaew, Thepchai Supnithi",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isai-nlp48611.2019.9045174"
    },
    {
        "id": 22014,
        "title": "Syntax-aware Transformer Encoder for Neural Machine Translation",
        "authors": "Sufeng Duan, Hai Zhao, Junru Zhou, Rui Wang",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037672"
    },
    {
        "id": 22015,
        "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
        "authors": "Jindřich Libovický, Jindřich Helcl, David Mareček",
        "published": "2018",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6326"
    },
    {
        "id": 22016,
        "title": "Multi-encoder Transformer Network for Automatic Post-Editing",
        "authors": "Jaehun Shin, Jong-Hyeok Lee",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6470"
    },
    {
        "id": 22017,
        "title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study",
        "authors": "Peyman Passban, Puneeth Saladi, Qun Liu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.323"
    },
    {
        "id": 22018,
        "title": "Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT",
        "authors": "Shivam Mhaskar, Aditya Jain, Aakash Banerjee, Pushpak Bhattacharyya",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.wat-1.28"
    },
    {
        "id": 22019,
        "title": "Flood Prediction for Tropical Climates Using Lstm and Transformer Machine Learning Models",
        "authors": "Tharindu Madhushanka, Thishan Jayasinghe, Ruwan Rajapakse",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4736261"
    },
    {
        "id": 22020,
        "title": "Machine Translation Using Improved Attention-based Transformer with Hybrid Input",
        "authors": "Mahsa Abrishami, Mohammad Javad Rashti, Marjan Naderan",
        "published": "2020-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwr49608.2020.9122317"
    },
    {
        "id": 22021,
        "title": "Debugging Translations of Transformer-based Neural Machine Translation Systems",
        "authors": "Matīss Rikters, Mārcis Pinnis",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22364/bjmc.2018.6.4.06"
    },
    {
        "id": 22022,
        "title": "Models and Tasks for Human-Centered Machine Translation",
        "authors": "Marine Carpuat,  ",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-073-1_001"
    },
    {
        "id": 22023,
        "title": "Domain Terminology Integration into Machine Translation: Leveraging Large Language Models",
        "authors": "Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, John D. Kelleher, Rejwanul Haque, Andy Way",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.82"
    },
    {
        "id": 22024,
        "title": "On Block g-Circulant Matrices with Discrete Cosine and Sine Transforms for Transformer-Based Translation Machine",
        "authors": "Euis Asriani, Intan Muchtadi-Alamsyah, Ayu Purwarianti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer has emerged as one of the modern neural networks that applied in numerous applications. However, the large and deep architecture of transformers makes themselves both computation and memory-intensive. In this paper, we propose the block g-circulant matrices to replace the dense weight matrices in feed forward layers of the transformer and leverage the DCT-DST algorithm to multiply these matrices with the input vector. Our experiment on Portuguese-English datasets show that the proposed approach achieves a significant gain in model memory efficiency compared to dense transformer, with a slight accuracy degradation. We found that the highest model memory efficiency is achieved by the model Dense-block 1-circulant DCT-DST of 128 dimension at 22,14%. We further show that the same model achieved a BLEU score of 26.47%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3853113/v1"
    },
    {
        "id": 22025,
        "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Yves Scherrer, Jörg Tiedemann",
        "published": "2020",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.49"
    },
    {
        "id": 22026,
        "title": "Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",
        "authors": "Xuan Zhang, Navid Rajabi, Kevin Duh, Philipp Koehn",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.43"
    },
    {
        "id": 22027,
        "title": "The Unreasonable Volatility of Neural Machine Translation Models",
        "authors": "Marzieh Fadaee, Christof Monz",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.ngt-1.10"
    },
    {
        "id": 22028,
        "title": "Translation Norms, Translation Behavior, and Continuous Vector Space Models",
        "authors": "Michael Carl",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-69777-8_14"
    },
    {
        "id": 22029,
        "title": "Analysing Coreference in Transformer Outputs",
        "authors": "Ekaterina Lapshinova-Koltunski, Cristina España-Bonet, Josef van Genabith",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-6501"
    },
    {
        "id": 22030,
        "title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem",
        "authors": "Daria Grechishnikova",
        "published": "2021-1-11",
        "citations": 94,
        "abstract": "AbstractDrug discovery for a protein target is a very laborious, long and costly process. Machine learning approaches and, in particular, deep generative networks can substantially reduce development time and costs. However, the majority of methods imply prior knowledge of protein binders, their physicochemical characteristics or the three-dimensional structure of the protein. The method proposed in this work generates novel molecules with predicted ability to bind a target protein by relying on its amino acid sequence only. We consider target-specific de novo drug design as a translational problem between the amino acid “language” and simplified molecular input line entry system representation of the molecule. To tackle this problem, we apply Transformer neural network architecture, a state-of-the-art approach in sequence transduction tasks. Transformer is based on a self-attention technique, which allows the capture of long-range dependencies between items in sequence. The model generates realistic diverse compounds with structural novelty. The computed physicochemical properties and common metrics used in drug discovery fall within the plausible drug-like range of values.",
        "link": "http://dx.doi.org/10.1038/s41598-020-79682-4"
    },
    {
        "id": 22031,
        "title": "Parameter Sharing Methods for Multilingual Self-Attentional Translation Models",
        "authors": "Devendra Sachan, Graham Neubig",
        "published": "2018",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6327"
    },
    {
        "id": 22032,
        "title": "Predicative Language Models",
        "authors": "Peng Wang, David B. Sawyer",
        "published": "2023-2-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003321538-5"
    },
    {
        "id": 22033,
        "title": "Compact and Robust Models for Japanese-English Character-level Machine Translation",
        "authors": "Jinan Dai, Kazunori Yamaguchi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5202"
    },
    {
        "id": 22034,
        "title": "Auto-Sizing the Transformer Network: Improving Speed, Efficiency, and Performance for Low-Resource Machine Translation",
        "authors": "Kenton Murray, Jeffery Kinnison, Toan Q. Nguyen, Walter Scheirer, David Chiang",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5625"
    },
    {
        "id": 22035,
        "title": "Gex'ez-English Bi-Directional Neural Machine Translation Using Transformer",
        "authors": "Sefineh Getachew, Yirga Yayeh",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ict4da59526.2023.10302254"
    },
    {
        "id": 22036,
        "title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation",
        "authors": "Chenze Shao, Zhengrui Ma, Yang Feng",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.322"
    },
    {
        "id": 22037,
        "title": "Crosslingual Content Scoring in Five Languages Using Machine-Translation and Multilingual Transformer Models",
        "authors": "Andrea Horbach, Joey Pehlke, Ronja Laarmann-Quante, Yuning Ding",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "AbstractThis paper investigates crosslingual content scoring, a scenario where scoring models trained on learner data in one language are applied to data in a different language. We analyze data in five different languages (Chinese, English, French, German and Spanish) collected for three prompts of the established English ASAP content scoring dataset. We cross the language barrier by means of both shallow and deep learning crosslingual classification models using both machine translation and multilingual transformer models. We find that a combination of machine translation and multilingual models outperforms each method individually - our best results are reached when combining the available data in different languages, i.e. first training a model on the large English ASAP dataset before fine-tuning on smaller amounts of training data in the target language.",
        "link": "http://dx.doi.org/10.1007/s40593-023-00370-1"
    },
    {
        "id": 22038,
        "title": "Compressing Neural Machine Translation Models with 4-bit Precision",
        "authors": "Alham Fikri Aji, Kenneth Heafield",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.ngt-1.4"
    },
    {
        "id": 22039,
        "title": "Improving Transformer based Neural Machine Translation with Source-side Morpho-linguistic Features",
        "authors": "Santwana Chimalamarri, Dinkar Sitaram, Rithik Mali, Alex Johnson, K A Adeab",
        "published": "2020-12-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlant50963.2020.9355969"
    },
    {
        "id": 22040,
        "title": "Evolving transformer architecture for neural machine translation",
        "authors": "Ben Feng, Dayiheng Liu, Yanan Sun",
        "published": "2021-7-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449726.3459441"
    },
    {
        "id": 22041,
        "title": "Factorized Transformer for Multi-Domain Neural Machine Translation",
        "authors": "Yongchao Deng, Hongfei Yu, Heng Yu, Xiangyu Duan, Weihua Luo",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.377"
    },
    {
        "id": 22042,
        "title": "Lexically Constrained Neural Machine Translation with Levenshtein Transformer",
        "authors": "Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.325"
    },
    {
        "id": 22043,
        "title": "Fast streaming translation using machine learning with transformer",
        "authors": "Jiabao Qiu, Melody Moh, Teng-Sheng Moh",
        "published": "2021-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3409334.3452059"
    },
    {
        "id": 22044,
        "title": "Dual Frequency Transformer for Efficient SDR-to-HDR Translation",
        "authors": "Gang Xu, Qibin Hou, Ming-Ming Cheng",
        "published": "2024-1-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11633-023-1418-8"
    },
    {
        "id": 22045,
        "title": "Efficient Wait-k Models for Simultaneous Machine Translation",
        "authors": "Maha Elbayad, Laurent Besacier, Jakob Verbeek",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1241"
    },
    {
        "id": 22046,
        "title": "Revolutionizing Translation with AI: Unravelling Neural Machine Translation and Generative Pre-Trained Large Language Models",
        "authors": "Sai Cheong Siu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4499768"
    },
    {
        "id": 22047,
        "title": "X-Transformer: A Green Self-attention Based Machine Translation Model",
        "authors": "Huey-Ing Liu, Wei-Lin Chen",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9734/bpi/rhst/v5/18982d"
    },
    {
        "id": 22048,
        "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Jörg Tiedemann",
        "published": "2018",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5431"
    },
    {
        "id": 22049,
        "title": "X-Transformer: A Machine Translation Model Enhanced by the Self-Attention Mechanism",
        "authors": "Huey-Ing Liu, Wei-Lin Chen",
        "published": "2022-4-29",
        "citations": 3,
        "abstract": "Machine translation has received significant attention in the field of natural language processing not only because of its challenges but also due to the translation needs that arise in the daily life of modern people. In this study, we design a new machine translation model named X-Transformer, which refines the original Transformer model regarding three aspects. First, the model parameter of the encoder is compressed. Second, the encoder structure is modified by adopting two layers of the self-attention mechanism consecutively and reducing the point-wise feed forward layer to help the model understand the semantic structure of sentences precisely. Third, we streamline the decoder model size, while maintaining the accuracy. Through experiments, we demonstrate that having a large number of decoder layers not only affects the performance of the translation model but also increases the inference time. The X-Transformer reaches the state-of-the-art result of 46.63 and 55.63 points in the BiLingual Evaluation Understudy (BLEU) metric of the World Machine Translation (WMT), from 2014, using the English–German and English–French translation corpora, thus outperforming the Transformer model with 19 and 18 BLEU points, respectively. The X-Transformer significantly reduces the training time to only 1/3 times that of the Transformer. In addition, the heat maps of the X-Transformer reach token-level precision (i.e., token-to-token attention), while the Transformer model remains at the sentence level (i.e., token-to-sentence attention).",
        "link": "http://dx.doi.org/10.3390/app12094502"
    },
    {
        "id": 22050,
        "title": "Syntax-based Transformer for Neural Machine Translation",
        "authors": "Chunpeng Ma",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5715/jnlp.28.682"
    },
    {
        "id": 22051,
        "title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation",
        "authors": "Mitchell Gordon, Kevin Duh",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.ngt-1.12"
    },
    {
        "id": 22052,
        "title": "Fusion of Image-text attention for Transformer-based Multimodal Machine Translation",
        "authors": "Junteng Ma, Shihao Qin, Lan Su, Xia Li, Lixian Xiao",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037732"
    },
    {
        "id": 22053,
        "title": "A Transformer-Based Multi-Source Automatic Post-Editing System",
        "authors": "Santanu Pal, Nico Herbig, Antonio Krüger, Josef van Genabith",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6468"
    },
    {
        "id": 22054,
        "title": "Neural Machine Translation for English-Assamese Language Pair using Transformer",
        "authors": "Rudra Dutt, Tarun Aditya Kusupati, Akshat Srivastava, Basab Nath",
        "published": "2022-10-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat55367.2022.9972085"
    },
    {
        "id": 22055,
        "title": "Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation",
        "authors": "Maximiliana Behnke, Kenneth Heafield",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.211"
    },
    {
        "id": 22056,
        "title": "Multi-source transformer with combined losses for automatic post editing",
        "authors": "Amirhossein Tebbifakhr, Ruchit Agrawal, Matteo Negri, Marco Turchi",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6471"
    },
    {
        "id": 22057,
        "title": "DialectNLU at NADI 2023 Shared Task: Transformer Based Multitask Approach Jointly Integrating Dialect and Machine Translation Tasks in Arabic",
        "authors": "Hariram Veeramani, Surendrabikram Thapa, Usman Naseem",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.63"
    },
    {
        "id": 22058,
        "title": "Multi-source Transformer for Automatic Post-Editing of Machine Translation Output",
        "authors": "Amirhossein Tebbifakhr, Matteo Negri, Marco Turchi",
        "published": "2019-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/ijcol.464"
    },
    {
        "id": 22059,
        "title": "Re-Transformer: A Self-Attention Based Model for Machine Translation",
        "authors": "Huey-Ing Liu, Wei-Lin Chen",
        "published": "2021",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2021.05.065"
    },
    {
        "id": 22060,
        "title": "Deep Transformer modeling via grouping skip connection for neural machine translation",
        "authors": "Yachao Li, Junhui Li, Min Zhang",
        "published": "2021-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2021.107556"
    },
    {
        "id": 22061,
        "title": "A Densely Connected Transformer for Machine Translation",
        "authors": "Zhikui Zhu, Jun Ruan, Kehao Wang, Jingfan Zhou, Guanglu Ye, Chenchen Wu",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscid.2019.00057"
    },
    {
        "id": 22062,
        "title": "Peer Review #3 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.1)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.1/reviews/3"
    },
    {
        "id": 22063,
        "title": "Peer Review #3 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.3)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.3/reviews/3"
    },
    {
        "id": 22064,
        "title": "Translation Performance from the User’s Perspective of Large Language Models and Neural Machine Translation Systems",
        "authors": "Jungha Son, Boyoung Kim",
        "published": "2023-10-19",
        "citations": 1,
        "abstract": "The rapid global expansion of ChatGPT, which plays a crucial role in interactive knowledge sharing and translation, underscores the importance of comparative performance assessments in artificial intelligence (AI) technology. This study concentrated on this crucial issue by exploring and contrasting the translation performances of large language models (LLMs) and neural machine translation (NMT) systems. For this aim, the APIs of Google Translate, Microsoft Translator, and OpenAI’s ChatGPT were utilized, leveraging parallel corpora from the Workshop on Machine Translation (WMT) 2018 and 2020 benchmarks. By applying recognized evaluation metrics such as BLEU, chrF, and TER, a comprehensive performance analysis across a variety of language pairs, translation directions, and reference token sizes was conducted. The findings reveal that while Google Translate and Microsoft Translator generally surpass ChatGPT in terms of their BLEU, chrF, and TER scores, ChatGPT exhibits superior performance in specific language pairs. Translations from non-English to English consistently yielded better results across all three systems compared with translations from English to non-English. Significantly, an improvement in translation system performance was observed as the token size increased, hinting at the potential benefits of training models on larger token sizes.",
        "link": "http://dx.doi.org/10.3390/info14100574"
    },
    {
        "id": 22065,
        "title": "Peer Review #2 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.1)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.1/reviews/2"
    },
    {
        "id": 22066,
        "title": "Peer Review #3 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.2)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.2/reviews/3"
    },
    {
        "id": 22067,
        "title": "Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",
        "authors": "Marzena Karpinska, Mohit Iyyer",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.41"
    },
    {
        "id": 22068,
        "title": "Peer Review #2 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.3)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.3/reviews/2"
    },
    {
        "id": 22069,
        "title": "Peer Review #2 of \"The neural machine translation models for the low-resource Kazakh–English language pair (v0.2)\"",
        "authors": "",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1224v0.2/reviews/2"
    },
    {
        "id": 22070,
        "title": "Regularized Context Gates on Transformer for Machine Translation",
        "authors": "Xintong Li, Lemao Liu, Rui Wang, Guoping Huang, Max Meng",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.757"
    }
]