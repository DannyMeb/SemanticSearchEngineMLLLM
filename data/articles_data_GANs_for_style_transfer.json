[
    {
        "id": 22071,
        "title": "EST-GAN: Enhancing Style Transfer GANs with Intermediate Game Render Passes",
        "authors": "Martina Mittermueller, Zhanxiang Ye, Helmut Hlavacs",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893673"
    },
    {
        "id": 22072,
        "title": "Style-transfer GANs for bridging the domain gap in synthetic pose estimator training",
        "authors": "Pavel Rojtberg, Thomas Pollabauer, Arjan Kuijper",
        "published": "2020-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aivr50618.2020.00039"
    },
    {
        "id": 22073,
        "title": "Neural map style transfer exploration with GANs",
        "authors": "Sidonie Christophe, Samuel Mermet, Morgan Laurent, Guillaume Touya",
        "published": "2022-1-2",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/23729333.2022.2031554"
    },
    {
        "id": 22074,
        "title": "Style transfer in conditional GANs for cross-modality synthesis of brain magnetic resonance images",
        "authors": "Zhiwei Qin, Zhao Liu, Ping Zhu, Wenyuan Ling",
        "published": "2022-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2022.105928"
    },
    {
        "id": 22075,
        "title": "Synthesizing Depth Hand Images with GANs and Style Transfer for Hand Pose Estimation",
        "authors": "Wangyong He, Zhongzhao Xie, Yongbo Li, Xinmei Wang, Wendi Cai",
        "published": "2019-7-1",
        "citations": 11,
        "abstract": "Hand pose estimation is a critical technology of computer vision and human-computer interaction. Deep-learning methods require a considerable amount of tagged data. Accordingly, numerous labeled training data are required. This paper aims to generate depth hand images. Given a ground-truth 3D hand pose, the developed method can generate depth hand images. To be specific, a ground truth can be 3D hand poses with the hand structure contained, while the synthesized image has an identical size to that of the training image and a similar visual appearance to the training set. The developed method, inspired by the progress in the generative adversarial network (GAN) and image-style transfer, helps model the latent statistical relationship between the ground-truth hand pose and the corresponding depth hand image. The images synthesized using the developed method are demonstrated to be feasible for enhancing performance. On public hand pose datasets (NYU, MSRA, ICVL), comprehensive experiments prove that the developed method outperforms the existing works.",
        "link": "http://dx.doi.org/10.3390/s19132919"
    },
    {
        "id": 22076,
        "title": "Neural Style Transfer for image within images and conditional GANs for destylization",
        "authors": " Mallika, Jagpal Singh Ubhi, Ashwani Kumar Aggarwal",
        "published": "2022-5",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2022.103483"
    },
    {
        "id": 22077,
        "title": "Enhancing Style Transfer with GANs: Perceptual Loss and Semantic Segmentation",
        "authors": "A Satchidanandam, R. Mohammed Saleh Al Ansari, A L Sreenivasulu, Vuda Sreenivasa Rao, Sanjiv Rao Godla, Chamandeep Kaur",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141132"
    },
    {
        "id": 22078,
        "title": "Editing in Style: Uncovering the Local Semantics of GANs",
        "authors": "Edo Collins, Raja Bala, Bob Price, Sabine Susstrunk",
        "published": "2020-6",
        "citations": 159,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr42600.2020.00581"
    },
    {
        "id": 22079,
        "title": "Say Yes to the Dress: Shape and Style Transfer Using Conditional GANs",
        "authors": "Michael A. Hobley, Victor A. Prisacariu",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-20893-6_9"
    },
    {
        "id": 22080,
        "title": "Review 1: \"Maternal Immune Response and Placental Antibody Transfer After COVID-19 Vaccination Across Trimester and Platforms\"",
        "authors": "Hayley Gans",
        "published": "2022-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/2e3983f5.a42b1e81"
    },
    {
        "id": 22081,
        "title": "Reviews of \"Maternal Immune Response and Placental Antibody Transfer After COVID-19 Vaccination Across Trimester and Platforms\"",
        "authors": "Hayley Gans",
        "published": "2022-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/2e3983f5.e4b714ab"
    },
    {
        "id": 22082,
        "title": "基于Transformer-GANs生成有风格调节的音乐",
        "authors": "Weining Wang, Jiahui Li, Yifan Li, Xiaofen Xing",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300359"
    },
    {
        "id": 22083,
        "title": "Arbitrary Style Transfer with Style Enhancement and Structure Retention",
        "authors": "sijia Yang, Yun Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4305461"
    },
    {
        "id": 22084,
        "title": "Style Transfer Using AI",
        "authors": "Mourya Teja Kunuku",
        "published": "No Date",
        "citations": 0,
        "abstract": "Style Transfer Using AI",
        "link": "http://dx.doi.org/10.31219/osf.io/7detr"
    },
    {
        "id": 22085,
        "title": "Style transfer and evaluation: an intuitive style transfer quantitative evaluation method",
        "authors": "Yuchen Jiang, Bowen Zhang, Wanjie Zhang",
        "published": "2022-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2636921"
    },
    {
        "id": 22086,
        "title": "Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS",
        "authors": "Alexander Sorin, Slava Shechtman, Ron Hoory",
        "published": "2020-10-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1854"
    },
    {
        "id": 22087,
        "title": "SDWD: Style Diversity Weighted Distance Evaluates the Intra-Class Data Diversity of Distributed GANs",
        "authors": "Wei Wang, Ziwen Wu, Mingwei Zhang, Yue Li",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222285"
    },
    {
        "id": 22088,
        "title": "Neural Style Transfer",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_300367"
    },
    {
        "id": 22089,
        "title": "Neural Style Transfer with automatic style weight searching",
        "authors": "Xiang Zhou",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Neural Style Transfer (NST) has garnered significant attention in the field of computer vision. Previous research in this area has made important breakthroughs, creating various style transfer models and innovative architectural designs, and has achieved success in commercial applications. This paper presents a novel design for automatic style weight determination based on constraining the initial total loss within a healthy range and finding the optimal solution through grid search. This design aims to harness the potential of existing NST models by automating the optimization of neural style transfer hyperparameters. The paper first discusses the impact of content weights and loss weights on the generated images and validates the influence of weight ratios on image quality through experimental adjustments of weight proportions. It also confirms that the initial loss essentially defines the optimization space of the optimizer. The paper explores the significance of the initial loss and proposes a method to improve image generation quality by constraining the initial loss range.",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241242"
    },
    {
        "id": 22090,
        "title": "Hair Shading Style Transfer for Manga with cGAN",
        "authors": "Masashi Aizawa, Ryohei Orihara, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008961405870594"
    },
    {
        "id": 22091,
        "title": "Separating Style and Content for Generalized Style Transfer",
        "authors": "Yexun Zhang, Ya Zhang, Wenbin Cai",
        "published": "2018-6",
        "citations": 83,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00881"
    },
    {
        "id": 22092,
        "title": "Arbitrary Style Transfer With Style-Attentional Networks",
        "authors": "Dae Young Park, Kwang Hee Lee",
        "published": "2019-6",
        "citations": 196,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00603"
    },
    {
        "id": 22093,
        "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
        "authors": "Daxin Tan, Tan Lee",
        "published": "2021-8-30",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1129"
    },
    {
        "id": 22094,
        "title": "Appearance Constrained Topology Optimization Using Neural Style Transfer",
        "authors": "Praveen Vulimiri, Albert To",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0004695v"
    },
    {
        "id": 22095,
        "title": "Neural Artistic Style Transfer with Conditional Adversarial Network",
        "authors": "Pathirage  Nipun Deelaka",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358610"
    },
    {
        "id": 22096,
        "title": "A Comparative Analysis of Fast-style Transfer and VGG19-GramMatrix Approach to Neural Style Transfer",
        "authors": "Aryan Ratra, Aryan Agarwal, Vikrant Sharma, Satvik Vats, Sunny Singh, Vinay Kukreja",
        "published": "2023-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiss58487.2023.10250595"
    },
    {
        "id": 22097,
        "title": "Content and Style Disentanglement for Artistic Style Transfer",
        "authors": "Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, Bjorn Ommer",
        "published": "2019-10",
        "citations": 88,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00452"
    },
    {
        "id": 22098,
        "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
        "authors": "Yookyung Shin, Younggun Lee, Suhee Jo, Yeongtae Hwang, Taesu Kim",
        "published": "2022-9-18",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10131"
    },
    {
        "id": 22099,
        "title": "Improving Universal Style Transfer using Sub-style Decomposition",
        "authors": "Paraskevas Pegios, Nikolaos Passalis, Anastasios Tefas",
        "published": "2020-9-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3411408.3411411"
    },
    {
        "id": 22100,
        "title": "Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss",
        "authors": "Dino Rattcliffe, You Wang, Alex Mansbridge, Penny Karanasou, Alexis Moinet, Marius Cotescu",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10572"
    },
    {
        "id": 22101,
        "title": "How Positive Are You: Text Style Transfer using Adaptive Style Embedding",
        "authors": "Heejin Kim, Kyung-Ah Sohn",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.191"
    },
    {
        "id": 22102,
        "title": "STaDA: Style Transfer as Data Augmentation",
        "authors": "Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic",
        "published": "2019",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007353401070114"
    },
    {
        "id": 22103,
        "title": "STaDA: Style Transfer as Data Augmentation",
        "authors": "Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007353400002108"
    },
    {
        "id": 22104,
        "title": "Neural Style Transfer for Vector Graphics",
        "authors": "Ivan Jarsky, Valeria Efimova, Artyom Chebykin, Viacheslav Shalamov, Andrey Filchenkov",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012438200003660"
    },
    {
        "id": 22105,
        "title": "One-shot Image Style Transfer via Pre-trained GAN Inversion",
        "authors": "Zegang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.164402981.12432631/v1"
    },
    {
        "id": 22106,
        "title": "Supervised Learning for Makeup Style Transfer",
        "authors": "Natalia Strawa, Grzegorz Sarwas",
        "published": "2022",
        "citations": 0,
        "abstract": "This paper addresses the problem of using deep learning for makeup style transfer. For solving this problem, we propose a new supervised method. Additionally, we present a technique for creating a synthetic dataset for makeup transfer used to train our model. The obtained results were compared with six popular methods for makeup transfer using three metrics. The tests were carried out on four available data sets. The proposed method, in many respects, is competitive with the methods used in the literature. Thanks to images of faces with generated synthetic makeup, the proposed method learns to better transfer details, and the learning process is significantly accelerated.",
        "link": "http://dx.doi.org/10.24132/csrn.3201.25"
    },
    {
        "id": 22107,
        "title": "Flexible Selecting of Style to Content Ratio in Neural Style Transfer",
        "authors": "Taehee Jeong, Anubha Mandal",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00046"
    },
    {
        "id": 22108,
        "title": "SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text Style Transfer",
        "authors": "Jicheng Li, Yang Feng, Jiao Ou",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533731"
    },
    {
        "id": 22109,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 22110,
        "title": "Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs",
        "authors": "Enis Simsar, Umut Kocasari, Ezgi Gulperi Er, Pinar Yanardag",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00471"
    },
    {
        "id": 22111,
        "title": "Improving Material Translation Based on Style Image Retrieval for Neural Style Transfer",
        "authors": "Gibran Benitez-Garcia, Keiji Yanai",
        "published": "2020-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36463/idw.2020.0981"
    },
    {
        "id": 22112,
        "title": "Network of Steel: Neural Font Style Transfer from Heavy Metal to Corporate Logos",
        "authors": "Aram Ter-Sarkisov",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009343906210629"
    },
    {
        "id": 22113,
        "title": "Style Agnostic 3D Reconstruction via Adversarial Style Transfer",
        "authors": "Felix Petersen, Bastian Goldluecke, Oliver Deussen, Hilde Kuehne",
        "published": "2022-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv51458.2022.00233"
    },
    {
        "id": 22114,
        "title": "Transfer and Extraction of the Style of Handwritten Letters using Deep Learning",
        "authors": "Omar Mohammed, Gérard Bailly, Damien Pellier",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007388606770684"
    },
    {
        "id": 22115,
        "title": "Font Style Transfer Using Neural Style Transfer and Unsupervised Cross-domain Transfer",
        "authors": "Atsushi Narusawa, Wataru Shimoda, Keiji Yanai",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-21074-8_9"
    },
    {
        "id": 22116,
        "title": "Training Data Independent Image Registration with Gans Using Transfer Learning and Segmentation Information",
        "authors": "Dwarikanath Mahapatra, Zongyuan Ge",
        "published": "2019-4",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi.2019.8759247"
    },
    {
        "id": 22117,
        "title": "GANs, GANs, and More GANs",
        "authors": "Micheal Lanham",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7092-9_4"
    },
    {
        "id": 22118,
        "title": "On Text Style Transfer via Style-Aware Masked Language Models",
        "authors": "Sharan Narasimhan, Pooja H, Suvodip Dey, Maunendra Sankar Desarkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.25"
    },
    {
        "id": 22119,
        "title": "Image Style Transfer Method Based on Improved Style Loss Function",
        "authors": "Hanmin Ye, Wenjie Liu, Yingzhi Liu",
        "published": "2020-12-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic49862.2020.9338927"
    },
    {
        "id": 22120,
        "title": "Preserving Fine-Grained Style Consistency for Universal Image Style Transfer",
        "authors": "Yubo Zhu, Xinxiao Wu, Jialu Chen",
        "published": "2022-11-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac57282.2022.10023773"
    },
    {
        "id": 22121,
        "title": "Arbitrary style transfer via content consistency and style consistency",
        "authors": "Xiaoming Yu, Gan Zhou",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00371-023-02855-5"
    },
    {
        "id": 22122,
        "title": "Diving Deeper into Volume Style Transfer",
        "authors": "Mike Navarro",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587421.3595448"
    },
    {
        "id": 22123,
        "title": "Neural Style Transfer for 3d Meshes",
        "authors": "Hongyuan Kang, Xiao Dong, Juan Cao, Zhonggui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4363664"
    },
    {
        "id": 22124,
        "title": "Tackling Data Bias in Painting Classification with Style Transfer",
        "authors": "Mridula Vijendran, Frederick Li, Hubert P. H. Shum",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011776600003417"
    },
    {
        "id": 22125,
        "title": "Style Fader Generative Adversarial Networks for Style Degree Controllable Artistic Style Transfer",
        "authors": "Zhiwen Zuo, Lei Zhao, Shuobin Lian, Haibo Chen, Zhizhong Wang, Ailin Li, Wei Xing, Dongming Lu",
        "published": "2022-7",
        "citations": 4,
        "abstract": "Artistic style transfer is the task of synthesizing content images with learned artistic styles. Recent studies have shown the potential of Generative Adversarial Networks (GANs) for producing artistically rich stylizations. Despite the promising results, they usually fail to control the generated images' style degree, which is inflexible and limits their applicability for practical use. To address the issue, in this paper, we propose a novel method that for the first time allows adjusting the style degree for existing GAN-based artistic style transfer frameworks in real time after training. Our method introduces two novel modules into existing GAN-based artistic style transfer frameworks: a Style Scaling Injection (SSI) module and a Style Degree Interpretation (SDI) module. The SSI module accepts the value of Style Degree Factor (SDF)  as the input and outputs parameters that scale the feature activations in existing models, offering control signals to alter the style degrees of the stylizations.  And the SDI module interprets the output probabilities of a multi-scale content-style binary classifier as the style degrees, providing a mechanism to parameterize the style degree of the stylizations. Moreover, we show that after training our method can enable existing GAN-based frameworks to produce over-stylizations. The proposed method can facilitate many existing GAN-based artistic style transfer frameworks with marginal extra training overheads and modifications. Extensive qualitative evaluations on two typical GAN-based style transfer models demonstrate the effectiveness of the proposed method for gaining style degree control for them.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/693"
    },
    {
        "id": 22126,
        "title": "Looks Like Magic: Transfer Learning in GANs to Generate New Card Illustrations",
        "authors": "Matheus K. Venturelli, Pedro H. Gomes, Jonatas Wehrmann",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892463"
    },
    {
        "id": 22127,
        "title": "Applying Neural Style Transfer to Transform Images into a Different Style Domain by the NST Method",
        "authors": "A Vishnukumar, Kamalanaban E, S Gugan, Abdul Gafoor A, Mukesh T",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA group of software algorithms known as neural style transfer (NST) alter digital pictures or videos to take on the look or visual style of another image. It is a machine learning-based optimization approach. In this study, we employ software that translates a picture's style onto a second image, referred to as the content image. Images may be encrypted, and the data can then be transferred to the recipient as an encrypted picture. The purpose of picture alteration is served through the usage of deep neural networks. The NST method may be used with this paper to modify two photos in photo-editing software. The created image can be outputted in whatever size the user desires. Hence The neural style transfer.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2647275/v1"
    },
    {
        "id": 22128,
        "title": "Text Style Transfer via Learning Style Instance Supported Latent Space",
        "authors": "Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, Maosong Sun",
        "published": "2020-7",
        "citations": 14,
        "abstract": "Text style transfer pursues altering the style of a sentence while remaining its main content unchanged. Due to the lack of parallel corpora, most recent work focuses on unsupervised methods and has achieved noticeable progress. Nonetheless, the intractability of completely disentangling content from style for text leads to a contradiction of content preservation and style transfer accuracy. To address this problem, we propose a style instance supported method, StyIns. Instead of representing styles with embeddings or latent variables learned from single sentences, our model leverages the generative flow technique to extract underlying stylistic properties from multiple instances of each style, which form a more discriminative and expressive latent style space. By combining such a space with the attention-based structure, our model can better maintain the content and simultaneously achieve high transfer accuracy. Furthermore, the proposed method can be flexibly extended to semi-supervised learning so as to utilize available limited paired data. Experiments on three transfer tasks, sentiment modification, formality rephrasing, and poeticness generation, show that StyIns obtains a better balance between content and style, outperforming several recent baselines.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/526"
    },
    {
        "id": 22129,
        "title": "Balancing Content and Style with Two-Stream FCNs for Style Transfer",
        "authors": "Duc Minh Vo, Trung-Nghia Le, Akihiro Sugimoto",
        "published": "2018-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv.2018.00152"
    },
    {
        "id": 22130,
        "title": "Text Style Transfer: Leveraging a Style Classifier on Entangled Latent Representations",
        "authors": "Xiaoyan Li, Sun Sun, Yunli Wang",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.repl4nlp-1.9"
    },
    {
        "id": 22131,
        "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
        "authors": "Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang",
        "published": "2019",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1601"
    },
    {
        "id": 22132,
        "title": "QR code arbitrary style transfer algorithm based on style matching layer",
        "authors": "Hai-Sheng Li, Jingyin Chen, Huafeng Huang",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-17231-7"
    },
    {
        "id": 22133,
        "title": "Style-ERD: Responsive and Coherent Online Motion Style Transfer",
        "authors": "Tianxin Tao, Xiaohang Zhan, Zhongquan Chen, Michiel van de Panne",
        "published": "2022-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00648"
    },
    {
        "id": 22134,
        "title": "PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models",
        "authors": "Tai-Yin Chiu, Danna Gurari",
        "published": "2022-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00769"
    },
    {
        "id": 22135,
        "title": "Style-Aware Normalized Loss for Improving Arbitrary Style Transfer",
        "authors": "Jiaxin Cheng, Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Prem Natarajan",
        "published": "2021-6",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.00020"
    },
    {
        "id": 22136,
        "title": "“One-Shot” Super-Resolution via Backward Style Transfer for Fast High-Resolution Style Transfer",
        "authors": "Jikang Cheng, Zhen Han, Zhongyuan Wang, Liang Chen",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsp.2021.3098230"
    },
    {
        "id": 22137,
        "title": "Voice Conversion Using Speech-to-Speech Neuro-Style Transfer",
        "authors": "Ehab A. AlBadawy, Siwei Lyu",
        "published": "2020-10-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3056"
    },
    {
        "id": 22138,
        "title": "In-camera, Photorealistic Style Transfer for On-set Automatic Grading",
        "authors": "Itziar Zabaleta, Marcelo Bertalmio",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5594/m001835"
    },
    {
        "id": 22139,
        "title": "Fonts Style Transfer using Conditional GAN",
        "authors": "Naho Sakao, Yoshinori Dobashi",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cw.2019.00075"
    },
    {
        "id": 22140,
        "title": "Reimagining Animation Making through Style Transfer",
        "authors": "Sujin Kim",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3610591.3616435"
    },
    {
        "id": 22141,
        "title": "EI-StyleGAN: A High Quality Face Cartoon Style Transfer Model",
        "authors": "Rui Li, Fenli Fu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCartoonization of face images is a new art forms applicable to various scenes, but there are problems in the research of incomplete extraction of image details after style transfer, simple superposition between domains, and poor generation quality. Since StyleGAN has better results in the generation of artistic images, on this basis, this paper proposes a new EI-StyleGAN to construct a network model applicable to face style migration, in which the inner and outer style restriction modules are introduced to obtain different detailed features in the images of the two domains, respectively. Meanwhile, the latent space coding is obtained by image inversion before inputting into the generated model, which can perform the transformation between images between the source and target domains. The whole process adopts an incremental generation strategy to smoothly transform the generation space of the model to the target domain, and gradually generates high-quality face cartoon style transfer result image. Experiments demonstrate the effectiveness of the method in improving the image quality after style transfer and the smooth conversion between different styles.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230687/v1"
    },
    {
        "id": 22142,
        "title": "Towards Real-time G-buffer-Guided Style Transfer in Computer Games",
        "authors": "Eleftherios Ioannou, Steve Maddock",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170654648.86162009/v1"
    },
    {
        "id": 22143,
        "title": "Name your style: text-guided artistic style transfer",
        "authors": "Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, Vicky Kalogeiton",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00359"
    },
    {
        "id": 22144,
        "title": "New Image Processing: VGG Image Style Transfer with Gram Matrix Style Features",
        "authors": "Longqing Zhang, Zishang Wang, Jinwen He, Yixuan Li",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405398"
    },
    {
        "id": 22145,
        "title": "Shanghai-Style Realistic Style Watercolor Painting Style Transfer by Using RSIM Evaluation",
        "authors": "Rongrong Fu, Yuanyuan Wang, Jiajun Lin, Ruiyang Fan, Han Zhao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-6372-7_72"
    },
    {
        "id": 22146,
        "title": "Deep Style Transfer",
        "authors": "Dongdong Chen, Lu Yuan, Gang Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_863"
    },
    {
        "id": 22147,
        "title": "An Investigation of Applications of Neural Style Transfer to Forensic Footwear Comparison",
        "authors": "Gregory Stock",
        "published": "2023",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.6028/nist.ir.8460"
    },
    {
        "id": 22148,
        "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
        "authors": "Shifeng Pan, Lei He",
        "published": "2021-8-30",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-979"
    },
    {
        "id": 22149,
        "title": "Restore the Incomplete Calligraphy Based on Style Transfer",
        "authors": "Mengxi Qin, Xin Chen",
        "published": "2019-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866187"
    },
    {
        "id": 22150,
        "title": "Beyond Simple Text Style Transfer: Unveiling Compound Text Style Transfer with Prompt-Based Pre-Trained Language Models",
        "authors": "Shuai Ju, Chenxu Wang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447801"
    },
    {
        "id": 22151,
        "title": "Virtual Hairstyle Service Using GANs &amp; Segmentation Mask (Hairstyle Transfer System)",
        "authors": "Mohamed S. Abdallah, Young-Im Cho",
        "published": "2022-10-13",
        "citations": 1,
        "abstract": "The virtual hair styling service, which now is necessary for cosmetics companies and beauty centers, requires significant improvement efforts. In the existing technologies, the result is unnatural as the hairstyle image is serviced in the form of a ‘composite’ on the face image, image, extracts and synthesizing simple hair images. Because of complicated interactions in illumination, geometrical, and occlusions, that generate pairing among distinct areas of an image, blending features from numerous photos is extremely difficult. To compensate for the shortcomings of the current state of the art, based on GAN-Style, we address and propose an approach to image blending, specifically for the issue of visual hairstyling to increase accuracy and reproducibility, increase user convenience, increase accessibility, and minimize unnaturalness. Based on the extracted real customer image, we provide a virtual hairstyling service (Live Try-On service) that presents a new approach for image blending with maintaining details and mixing spatial features, as well as a new embedding approach-based GAN that can gradually adjust images to fit a segmentation mask, thereby proposing optimal styling and differentiated beauty tech service to users. The visual features from many images, including precise details, can be extracted using our system representation, which also enables image blending and the creation of consistent images. The Flickr-Faces-HQ Dataset (FFHQ) and the CelebA-HQ datasets, which are highly diversified, high quality datasets of human faces images, are both used by our system. In terms of the image evaluation metrics FID, PSNR, and SSIM, our system significantly outperforms the existing state of the art.",
        "link": "http://dx.doi.org/10.3390/electronics11203299"
    },
    {
        "id": 22152,
        "title": "Digital Image Art Style Transfer Algorithm and Simulation Based on Deep Learning Model",
        "authors": "Nitin Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to solve the problems of poor region delineation and boundary artifacts in Indian style migration of images, an improved Variational Autoencoder (VAE) method for dress style migration is proposed. Firstly, the Yolo v3 model is used to quickly identify the dress localization of the input image, and then the classical semantic segmentation algorithm (FCN) is used to finely delineate the desired dress style migration region twice, and finally the trained VAE model is used to generate the migrated Indian style image using a decision support system. The results show that, compared with the traditional style migration model, the improved VAE style migration model can obtain finer synthetic images for dress style migration, and can adapt to different Indian traditional styles to meet the application requirements of dress style migration scenarios. We evaluated several deep learning based models and achieved BLEU value of 0.6 on average. The transformer-based model outperformed the other models, achieving a BLEU value of up to 0.72.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1221051/v1"
    },
    {
        "id": 22153,
        "title": "Photorealistic Style Transfer for Cinema Shoots",
        "authors": "Itziar Zabaleta, Marcelo Bertalmio",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvcs.2018.8496499"
    },
    {
        "id": 22154,
        "title": "Style Mixer: Semantic‐aware Multi‐Style Transfer Network",
        "authors": "Zixuan Huang, Jinghuai Zhang, Jing Liao",
        "published": "2019-10",
        "citations": 11,
        "abstract": "AbstractRecent neural style transfer frameworks have obtained astonishing visual quality and flexibility in Single‐style Transfer (SST), but little attention has been paid to Multi‐style Transfer (MST) which refers to simultaneously transferring multiple styles to the same image. Compared to SST, MST has the potential to create more diverse and visually pleasing stylization results. In this paper, we propose the first MST framework to automatically incorporate multiple styles into one result based on regional semantics. We first improve the existing SST backbone network by introducing a novel multi‐level feature fusion module and a patch attention module to achieve better semantic correspondences and preserve richer style details. For MST, we designed a conceptually simple yet effective region‐based style fusion module to insert into the backbone. It assigns corresponding styles to content regions based on semantic matching, and then seamlessly combines multiple styles together. Comprehensive evaluations demonstrate that our framework outperforms existing works of SST and MST.",
        "link": "http://dx.doi.org/10.1111/cgf.13853"
    },
    {
        "id": 22155,
        "title": "Combine model fine-tuning freezing layers and adaptive filter modulation to implement transfer learning for GANs",
        "authors": "Chen Yang",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "Generative Adversarial Network (GAN) requires more resources to train than other deep learning models and its loss function converges more slowly. For this reason, scholars at home and abroad have proposed a GANS algorithm based on transfer learning, which is applied to fewer samples, thus improving the training effect of GANS algorithm. In this paper, we provide a new way to perform the transfer of genetic algorithms and combine the two ways. On this basis, we will compare and analyze a variety of transfer learning algorithms to verify the feasibility and effectiveness of the joint application.",
        "link": "http://dx.doi.org/10.54254/2755-2721/27/20230189"
    },
    {
        "id": 22156,
        "title": "Line Search-Based Feature Transformation for Fast, Stable, and Tunable Content-Style Control in Photorealistic Style Transfer",
        "authors": "Tai-Yin Chiu, Danna Gurari",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00033"
    },
    {
        "id": 22157,
        "title": "Attention-guided GANs for human pose transfer",
        "authors": "Jinsong Zhang, Yuyang Zhao, Kun Li, Yebin Liu, Jingyu Yang, Qionghai Dai",
        "published": "2019-11-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2538638"
    },
    {
        "id": 22158,
        "title": "Arbitrary Style Transfer Based on Content Integrity and Style Consistency Enhancement",
        "authors": "Lu Kang, Guoqiang Xiao, Michael S. Lew, Song Wu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447437"
    },
    {
        "id": 22159,
        "title": "Adaptive Style Modulation for Artistic Style Transfer",
        "authors": "Yipeng Zhang, Bingliang Hu, Yingying Huang, Chi Gao, Quan Wang",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-022-11135-7"
    },
    {
        "id": 22160,
        "title": "Artistic Style Transfer using Deep Learning and Style Fusion- a Review",
        "authors": " Mohammed Mutahar,  Dr. R. Chinnaiyan",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "In recent years, after the study ‘A Neural Algorithm of Artistic Style’ published by Gatys et al. in 2016b, research on style transfer boomed drastically. Style transfer is the process of copying an art style from a ‘style image’ to the contents of the ‘content image’ and producing a ‘draft image’ that is on par with respect to quality expectations. This paper explores different techniques of achieving style transformations namely Style Fusion and Convolutional Neural Networks (CNNs). Although CNNs are the state-of-the-art architecture to tackle cognitive visual tasks, and that they clearly perform much better than most conventional algorithms, the image processing-based style fusion method comes close to the CNN in terms of image output quality and supersedes in terms of time and computation and resources complexity. The procedure of both of these methods has been discussed in detail in this paper and it was concluded that CNNs have a lot more room for improvement that can be facilitated by the availability of better and larger datasets.",
        "link": "http://dx.doi.org/10.32628/ijsrst523103146"
    },
    {
        "id": 22161,
        "title": "Style Permutation for Diversified Arbitrary Style Transfer",
        "authors": "Pan Li, Dan Zhang, Lei Zhao, Duanqing Xu, Dongming Lu",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.3034653"
    },
    {
        "id": 22162,
        "title": "Mask-Guided Font Style Transfer in Natural Scene",
        "authors": "Jing Yang, Siyu Xia",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902806"
    },
    {
        "id": 22163,
        "title": "FormalStyler: GPT based Model for Formal Style Transfer based on Formality and Meaning Preservation",
        "authors": "Mariano de Rivero, Cristhiam Tirado, Willy Ugarte",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010674300003064"
    },
    {
        "id": 22164,
        "title": "Detail-Preserving Arbitrary Style Transfer",
        "authors": "Ting Zhu, Shiguang Liu",
        "published": "2020-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme46284.2020.9102931"
    },
    {
        "id": 22165,
        "title": "Content and style transfer with generative adversarial network",
        "authors": "Wenhua Ding, Junwei Du, Lei Hou, Jinhuan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe generative adversarial network is often used for image conversion tasks such as image coloring, semantic composition,style transfer, etc.,but at this stage, the training of image generation models often depends on a large number of paired datasets, and can only achieve conversion between two image domains.When processing tasks in more than two domains,it lacks scalability and robustness.To solve the above problems,this paper proposes a Content and Style transfer model based on Generative Adversarial Network (CS-GAN).This model can fuse style features (such as monet style,cubism) and content features (such as color ,texture) of fashion items on unpaired datasets at the same time,which can realize the conversion of multiple image domains,so as to effectively complete the task of transferring the content and style of fashion items.In particular,we propose a layer consistent dynamic convolution (LCDC) method,which encodes the style image as a learnable convolution parameter,which can adaptively learn style features,and more flexibly and efficiently complete the arbitrary style transfer of fashion items.To validate the performance of our model,we conducts comparative experiments and results analysis on the public fashion dataset. Compared with other mainstream methods,this method has improved in image synthesis quality,Inception Score (IS) and Frechet Inception Dinstance score (FID) evaluation index.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2423729/v1"
    },
    {
        "id": 22166,
        "title": "Neural Style Transfer for Image-Based Garment Interchange Through Multi-Person Human Views",
        "authors": "Hajer Ghodhbani, Mohamed Neji, Adel Alimi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011694200003417"
    },
    {
        "id": 22167,
        "title": "Redefining Textual Dynamics for Enhanced Text Style Transfer",
        "authors": "Carlos Asanka, Conti Vatsalan, Rodolfo Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Conventional text style transfer (TST) methodologies primarily utilize style classifiers to segregate the content and stylistic elements of text for effective style transformation. Despite the pivotal role of these classifiers, their influence on TST techniques remains largely unexplored. This study embarks on a detailed exploration of the limitations inherent in style classifiers within current TST frameworks. We reveal that these classifiers often inadequately comprehend sentence syntax, leading to diminished performance in TST models. In response, we introduce the Syntax-Enhanced Style Transfer (SEST) model, a groundbreaking approach incorporating a syntax-sensitive style classifier. This classifier ensures that the extracted style representations robustly encapsulate syntax nuances, enhancing TST effectiveness. Rigorous evaluations across diverse TST benchmarks demonstrate that SEST significantly surpasses contemporary models in performance. Additionally, our case studies highlight SEST's proficiency in producing syntactically coherent sentences that aptly retain original content.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0144.v1"
    },
    {
        "id": 22168,
        "title": "FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer",
        "authors": "Pavol Harar, Lukas Herrmann, Philipp Gross, David Haselbach",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4517676"
    },
    {
        "id": 22169,
        "title": "Effects of dimples’ arrangement style of rough surface and jet geometry on impinging jet heat transfer",
        "authors": "Nevin Celik",
        "published": "2020-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00231-019-02714-x"
    },
    {
        "id": 22170,
        "title": "Neural Rendering-Based 3D Scene Style Transfer Method via Semantic Understanding Using a Single Style Image",
        "authors": "Jisun Park, Kyungeun Cho",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "In the rapidly emerging era of untact (“contact-free”) technologies, the requirement for three-dimensional (3D) virtual environments utilized in virtual reality (VR)/augmented reality (AR) and the metaverse has seen significant growth, owing to their extensive application across various domains. Current research focuses on the automatic transfer of the style of rendering images within a 3D virtual environment using artificial intelligence, which aims to minimize human intervention. However, the prevalent studies on rendering-based 3D environment-style transfers have certain inherent limitations. First, the training of a style transfer network dedicated to 3D virtual environments demands considerable style image data. These data must align with viewpoints that closely resemble those of the virtual environment. Second, there was noticeable inconsistency within the 3D structures. Predominant studies often neglect 3D scene geometry information instead of relying solely on 2D input image features. Finally, style adaptation fails to accommodate the unique characteristics inherent in each object. To address these issues, we propose a novel approach: a neural rendering-based 3D scene-style conversion technique. This methodology employs semantic nearest-neighbor feature matching, thereby facilitating the transfer of style within a 3D scene while considering the distinctive characteristics of each object, even when employing a single style image. The neural radiance field enables the network to comprehend the geometric information of a 3D scene in relation to its viewpoint. Subsequently, it transfers style features by employing the unique features of a single style image via semantic nearest-neighbor feature matching. In an empirical context, our proposed semantic 3D scene style transfer method was applied to 3D scene style transfers for both interior and exterior environments. This application utilizes the replica, 3DFront, and Tanks and Temples datasets for testing. The results illustrate that the proposed methodology surpasses existing style transfer techniques in terms of maintaining 3D viewpoint consistency, style uniformity, and semantic coherence.",
        "link": "http://dx.doi.org/10.3390/math11143243"
    }
]