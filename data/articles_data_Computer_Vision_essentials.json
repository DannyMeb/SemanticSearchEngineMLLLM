[
    {
        "id": 1101,
        "title": "A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials",
        "authors": "Chuqiao Li, Zhiwu Huang, Danda Pani Paudel, Yabin Wang, Mohamad Shahbazi, Xiaopeng Hong, Luc Van Gool",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00139"
    },
    {
        "id": 1102,
        "title": "Context understanding in computer vision: A survey",
        "authors": "Xuan Wang, Zhigang Zhu",
        "published": "2023-3",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103646"
    },
    {
        "id": 1103,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00007-3"
    },
    {
        "id": 1104,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00019-x"
    },
    {
        "id": 1105,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00250-3"
    },
    {
        "id": 1106,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00044-4"
    },
    {
        "id": 1107,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00069-9"
    },
    {
        "id": 1108,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00085-7"
    },
    {
        "id": 1109,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00275-8"
    },
    {
        "id": 1110,
        "title": "Guest Editorial: Multi‐view representation learning for computer vision",
        "authors": "Xin Ning, Jun Zhou, Jian Cheng, Jing Wu, Chen Wang, Lin Gu",
        "published": "2023-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12176"
    },
    {
        "id": 1111,
        "title": "Guest Editorial: Spectral imaging powered computer vision",
        "authors": "Jun Zhou, Fengchao Xiong, Lei Tong, Naoto Yokoya, Pedram Ghamisi",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12242"
    },
    {
        "id": 1112,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00227-8"
    },
    {
        "id": 1113,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00110-8"
    },
    {
        "id": 1114,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00079-6"
    },
    {
        "id": 1115,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00172-8"
    },
    {
        "id": 1116,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00055-3"
    },
    {
        "id": 1117,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00135-2"
    },
    {
        "id": 1118,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00004-3"
    },
    {
        "id": 1119,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00048-6"
    },
    {
        "id": 1120,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(22)00185-0"
    },
    {
        "id": 1121,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00205-9"
    },
    {
        "id": 1122,
        "title": "A Computer Vision Approach to Compute Bubble Flow of Offshore Wells",
        "authors": "Rogerio Hart, Aura Conci",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012433500003660"
    },
    {
        "id": 1123,
        "title": "Essentials for Digitalizing Maintenance Activities in SMEs",
        "authors": "Oliver Fuglsang Grooss",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2024.01.146"
    },
    {
        "id": 1124,
        "title": "Towards Generating 3D City Models with GAN and Computer Vision Methods",
        "authors": "Sarun Poolkrajang, Anand Bhojan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012315000003660"
    },
    {
        "id": 1125,
        "title": "Guest Editorial: Learning from limited annotations for computer vision tasks",
        "authors": "Yazhou Yao, Wenguan Wang, Qiang Wu, Dongfang Liu, Jin Zheng",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12229"
    },
    {
        "id": 1126,
        "title": "Evaluation of Computer Vision-Based Person Detection on Low-Cost Embedded Systems",
        "authors": "Francesco Pasti, Nicola Bellotto",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011797400003417"
    },
    {
        "id": 1127,
        "title": "OmDet: Large‐scale vision‐language multi‐dataset pre‐training with multimodal detection network",
        "authors": "Tiancheng Zhao, Peng Liu, Kyusong Lee",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "AbstractThe advancement of object detection (OD) in open‐vocabulary and open‐world scenarios is a critical challenge in computer vision. OmDet, a novel language‐aware object detection architecture and an innovative training mechanism that harnesses continual learning and multi‐dataset vision‐language pre‐training is introduced. Leveraging natural language as a universal knowledge representation, OmDet accumulates “visual vocabularies” from diverse datasets, unifying the task as a language‐conditioned detection framework. The multimodal detection network (MDN) overcomes the challenges of multi‐dataset joint training and generalizes to numerous training datasets without manual label taxonomy merging. The authors demonstrate superior performance of OmDet over strong baselines in object detection in the wild, open‐vocabulary detection, and phrase grounding, achieving state‐of‐the‐art results. Ablation studies reveal the impact of scaling the pre‐training visual vocabulary, indicating a promising direction for further expansion to larger datasets. The effectiveness of our deep fusion approach is underscored by its ability to learn jointly from multiple datasets, enhancing performance through knowledge sharing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12268"
    },
    {
        "id": 1128,
        "title": "2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)",
        "authors": "",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvmi59935.2023.10464399"
    },
    {
        "id": 1129,
        "title": "Towards Better User Studies in Computer Graphics and Vision",
        "authors": "Zoya Bylinskii, Laura Herman, Aaron Hertzmann, Stefanie Hutka, Yile Zhang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/0600000106"
    },
    {
        "id": 1130,
        "title": "Does Image Anonymization Impact Computer Vision Training?",
        "authors": "Håkon Hukkelås, Frank Lindseth",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00019"
    },
    {
        "id": 1131,
        "title": "Adaptive Testing of Computer Vision Models",
        "authors": "Irena Gao, Gabriel Ilharco, Scott Lundberg, Marco Tulio Ribeiro",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00370"
    },
    {
        "id": 1132,
        "title": "Discoloration Resistance of Various Computer Aided Design/Computer Aided Manufacturing Restorative Materials",
        "authors": "Rana Turunç Oğuzman,  , Soner Şişmanoğlu,  ",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5152/essentdent.2024.24005"
    },
    {
        "id": 1133,
        "title": "3D Human Motion Data Compression Based on Computer Vision",
        "authors": "Peng Wang, Xiaolin Jiang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424871"
    },
    {
        "id": 1134,
        "title": "Attention-based multimodal image matching",
        "authors": "Aviad Moreshet, Yosi Keller",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103949"
    },
    {
        "id": 1135,
        "title": "Analyzing lower half facial gestures for lip reading applications: Survey on vision techniques",
        "authors": "Preethi S.J., Niranjana Krupa B.",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103738"
    },
    {
        "id": 1136,
        "title": "Proceedings of the 2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)",
        "authors": "",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvmi59935.2023.10465136"
    },
    {
        "id": 1137,
        "title": "Learnable fusion mechanisms for multimodal object detection in autonomous vehicles",
        "authors": "Yahya Massoud, Robert Laganiere",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "AbstractPerception systems in autonomous vehicles need to accurately detect and classify objects within their surrounding environments. Numerous types of sensors are deployed on these vehicles, and the combination of such multimodal data streams can significantly boost performance. The authors introduce a novel sensor fusion framework using deep convolutional neural networks. The framework employs both camera and LiDAR sensors in a multimodal, multiview configuration. The authors leverage both data types by introducing two new innovative fusion mechanisms: element‐wise multiplication and multimodal factorised bilinear pooling. The methods improve the bird's eye view moderate average precision score by +4.97% and +8.35% on the KITTI dataset when compared to traditional fusion operators like element‐wise addition and feature map concatenation. An in‐depth analysis of key design choices impacting performance, such as data augmentation, multi‐task learning, and convolutional architecture design is offered. The study aims to pave the way for the development of more robust multimodal machine vision systems. The authors conclude the paper with qualitative results, discussing both successful and problematic cases, along with potential ways to mitigate the latter.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12259"
    },
    {
        "id": 1138,
        "title": "Unsupervised image blind super resolution via real degradation feature learning",
        "authors": "Cheng Yang, Guanming Lu",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "AbstractIn recent years, many methods for image super‐resolution (SR) have relied on pairs of low‐resolution (LR) and high‐resolution (HR) images for training, where the degradation process is predefined by bicubic downsampling. While such approaches perform well in standard benchmark tests, they often fail to accurately replicate the complexity of real‐world image degradation. To address this challenge, researchers have proposed the use of unpaired image training to implicitly model the degradation process. However, there is a significant domain gap between the real‐world LR and the synthetic LR images from HR, which severely degrades the SR performance. A novel unsupervised image‐blind super‐resolution method that exploits degradation feature‐based learning for real‐image super‐resolution reconstruction (RDFL) is proposed. Their approach learns the degradation process from HR to LR using a generative adversarial network (GAN) and constrains the data distribution of the synthetic LR with real degraded images. The authors then encode the degraded features into a Transformer‐based SR network for image super‐resolution reconstruction through degradation representation learning. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the RDFL method, which achieves visually pleasing reconstruction results.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12262"
    },
    {
        "id": 1139,
        "title": "An efficient mixed attention module",
        "authors": "Kuang Sheng, Pinghua Chen",
        "published": "2023-6",
        "citations": 1,
        "abstract": "AbstractRecently, the application of attention mechanisms in convolutional neural networks (CNNs) has become a hot area in computer vision. Most existing methods focus on channel attention or spatial attention. Some mixed attention usually achieves better performance than channel attention or spatial attention with the help of a complex model structure, which increases the complexity of the model. This article proposes an efficient mixed attention that combines channel information with spatial information using learnable broadcast addition to reduce this complexity. In particular, this module can simplify learning and improve performance with fewer parameters. Furthermore, our method uses an excitation method based on the Tanh function to reduce computational resources while maintaining model performance, and it is a lightweight attention module that can be used in arbitrary CNNs to improve performance. Experiments on ImageNet and Cifar confirm the effectiveness of the proposed method. Besides, our method remains highly competitive for object detection tasks and image segmentation tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12184"
    },
    {
        "id": 1140,
        "title": "FAM: Improving columnar vision transformer with feature attention mechanism",
        "authors": "Lan Huang, Xingyu Bai, Jia Zeng, Mengqiang Yu, Wei Pang, Kangping Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103981"
    },
    {
        "id": 1141,
        "title": "Transformer-based computer vision technology empowers drones",
        "authors": "Mingzheng Lai, PengJie Wang, YiFan Zeng, Wei Lv",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166286"
    },
    {
        "id": 1142,
        "title": "A temporal shift reconstruction network for compressive video sensing",
        "authors": "Zhenfei Gu, Chao Zhou, Guofeng Lin",
        "published": "2023-9-9",
        "citations": 0,
        "abstract": "AbstractCompressive sensing provides a promising sampling paradigm for video acquisition for resource‐limited sensor applications. However, the reconstruction of original video signals from sub‐sampled measurements is still a great challenge. To exploit the temporal redundancies within videos during the recovery, previous works tend to perform alignment on initial reconstructions, which are too coarse to provide accurate motion estimations. To solve this problem, the authors propose a novel reconstruction network, named TSRN, for compressive video sensing. Specifically, the authors utilise a number of stacked temporal shift reconstruction blocks (TSRBs) to enhance the initial reconstruction progressively. Each TSRB could learn the temporal structures by exchanging information with last and next time step, and no additional computations is imposed on the network compared to regular 2D convolutions due to the high efficiency of temporal shift operations. After the enhancement, a bidirectional alignment module to build accurate temporal dependencies directly with the help of optical flows is employed. Different from previous methods that only extract supplementary information from the key frames, the proposed alignment module can receive temporal information from the whole video sequence via bidirectional propagations, thus yielding better performance. Experimental results verify the superiority of the proposed method over other state‐of‐the‐art approaches quantitatively and qualitatively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12234"
    },
    {
        "id": 1143,
        "title": "Editor’s Note: Special Issue on 3D Computer Vision",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-023-01751-8"
    },
    {
        "id": 1144,
        "title": "Pollinators as Data Collectors: Estimating Floral Diversity with Bees and Computer Vision",
        "authors": "Frederic Tausch, Jan Wagner, Simon Klaus",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00071"
    },
    {
        "id": 1145,
        "title": "Computer Vision for Ocean Eddy Detection in Infrared Imagery",
        "authors": "Evangelos Moschos, Alisa Kugusheva, Paul Coste, Alexandre Stegner",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00633"
    },
    {
        "id": 1146,
        "title": "Single and multiple illuminant estimation using convex functions",
        "authors": "Zeinab Abedini, Mansour Jamzad",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103711"
    },
    {
        "id": 1147,
        "title": "Difficulty Estimation with Action Scores for Computer Vision Tasks",
        "authors": "Octavio Arriaga, Sebastian Palacio, Matias Valdenegro-Toro",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00030"
    },
    {
        "id": 1148,
        "title": "Concept Study for Dynamic Vision Sensor Based Insect Monitoring",
        "authors": "Regina Pohle-Fröhlich, Tobias Bolten",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011775500003417"
    },
    {
        "id": 1149,
        "title": "Image amodal completion: A survey",
        "authors": "Jiayang Ao, Qiuhong Ke, Krista A. Ehinger",
        "published": "2023-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103661"
    },
    {
        "id": 1150,
        "title": "Video frame interpolation via spatial multi‐scale modelling",
        "authors": "Zhe Qu, Weijing Liu, Lizhen Cui, Xiaohui Yang",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "AbstractVideo frame interpolation (VFI) is a technique that synthesises intermediate frames between adjacent original video frames to enhance the temporal super‐resolution of the video. However, existing methods usually rely on heavy model architectures with a large number of parameters. The authors introduce an efficient VFI network based on multiple lightweight convolutional units and a Local three‐scale encoding (LTSE) structure. In particular, the authors introduce a LTSE structure with two‐level attention cascades. This design is tailored to enhance the efficient capture of details and contextual information across diverse scales in images. Secondly, the authors introduce recurrent convolutional layers (RCL) and residual operations, designing the recurrent residual convolutional unit to optimise the LTSE structure. Additionally, a lightweight convolutional unit named separable recurrent residual convolutional unit is introduced to reduce the model parameters. Finally, the authors obtain the three‐scale decoding features from the decoder and warp them for a set of three‐scale pre‐warped maps. The authors fuse them into the synthesis network to generate high‐quality interpolated frames. The experimental results indicate that the proposed approach achieves superior performance with fewer model parameters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12281"
    },
    {
        "id": 1151,
        "title": "Domain-aware triplet loss in domain generalization",
        "authors": "Kaiyu Guo, Brian C. Lovell",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103979"
    },
    {
        "id": 1152,
        "title": "Eulerian Single-Photon Vision",
        "authors": "Shantanu Gupta, Mohit Gupta",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00960"
    },
    {
        "id": 1153,
        "title": "Improvement of Vision Transformer Using Word Patches",
        "authors": "Ayato Takama, Sota Kato, Satoshi Kamiya, Kazuhiro Hotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011732900003417"
    },
    {
        "id": 1154,
        "title": "Penalizing proposals using classifiers for semi-supervised object detection",
        "authors": "Somnath Hazra, Pallab Dasgupta",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103772"
    },
    {
        "id": 1155,
        "title": "Masking Strategies for Background Bias Removal in Computer Vision Models",
        "authors": "Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00474"
    },
    {
        "id": 1156,
        "title": "GradPaint: Gradient-guided inpainting with diffusion models",
        "authors": "Asya Grechka, Guillaume Couairon, Matthieu Cord",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103928"
    },
    {
        "id": 1157,
        "title": "Improved triplet loss for domain adaptation",
        "authors": "Xiaoshun Wang, Yunhan Li, Xiangliang Zhang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractA technique known as domain adaptation is utilised to address classification challenges in an unlabelled target domain by leveraging labelled source domains. Previous domain adaptation approaches have predominantly focussed on global domain adaptation, neglecting class‐level information and resulting in suboptimal transfer performance. In recent years, a considerable number of researchers have explored class‐level domain adaptation, aiming to precisely align the distribution of diverse domains. Nevertheless, existing research on class‐level alignment tends to align domain features either on or in proximity to classification boundaries, which introduces ambiguous samples that can impact classification accuracy. In this study, the authors propose a novel strategy called class guided constraints (CGC) to tackle this issue. Specifically, CGC is employed to preserve the compactness within classes and separability between classes of domain features prior to class‐level alignment. Furthermore, the authors incorporate CGC in conjunction with similarity guided constraint. Comprehensive evaluations conducted on four public datasets demonstrate that our approach outperforms numerous state‐of‐the‐art domain adaptation methods significantly and achieves greater improvements compared to the baseline approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12226"
    },
    {
        "id": 1158,
        "title": "Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad",
        "authors": "Thomas Manzini, Robin Murphy",
        "published": "2023-10-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00409"
    },
    {
        "id": 1159,
        "title": "Applying Positional Encoding to Enhance Vision-Language Transformers",
        "authors": "Xuehao Liu, Sarah Delany, Susan McKeever",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011796100003417"
    },
    {
        "id": 1160,
        "title": "Trajectory-Prediction with Vision: A Survey",
        "authors": "Apoorv Singh",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00356"
    },
    {
        "id": 1161,
        "title": "MATTE: Multi-task multi-scale attention",
        "authors": "Gjorgji Strezoski, Nanne van Noord, Marcel Worring",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103622"
    },
    {
        "id": 1162,
        "title": "Extending function mixture network for improved spectral super-resolution",
        "authors": "Sadia Hussain, Brejesh Lall",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103834"
    },
    {
        "id": 1163,
        "title": "Robotics and Computer Vision in the Brazilian Electoral Context: A Case Study",
        "authors": "Jairton Falcão Filho, Matheus Costa, Jonas Silva, Cecília Santos da Silva, Felipe Mendonca, Jefferson Norberto, Riei Rodrigues, Marcondes Silva Júnior",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012379300003660"
    },
    {
        "id": 1164,
        "title": "Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks",
        "authors": "Fawaz Sammani, Nikos Deligiannis",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00498"
    },
    {
        "id": 1165,
        "title": "Fully synthetic training for image restoration tasks",
        "authors": "Raphaël Achddou, Yann Gousseau, Saïd Ladjal",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103723"
    },
    {
        "id": 1166,
        "title": "Multi-layered self-attention mechanism for weakly supervised semantic segmentation",
        "authors": "Avinash Yaganapu, Mingon Kang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103886"
    },
    {
        "id": 1167,
        "title": "Action Capsules: Human skeleton action recognition",
        "authors": "Ali Farajzadeh Bavil, Hamed Damirchi, Hamid D. Taghirad",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103722"
    },
    {
        "id": 1168,
        "title": "Scene context‐aware graph convolutional network for skeleton‐based action recognition",
        "authors": "Wenxian Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractSkeleton‐based action recognition methods commonly employ graph neural networks to learn different aspects of skeleton topology information However, these methods often struggle to capture contextual information beyond the skeleton topology. To address this issue, a Scene Context‐aware Graph Convolutional Network (SCA‐GCN) that leverages potential contextual information in the scene is proposed. Specifically, SCA‐GCN learns the co‐occurrence probabilities of actions in specific scenarios from a common knowledge base and fuses these probabilities into the original skeleton topology decoder, producing more robust results. To demonstrate the effectiveness of SCA‐GCN, extensive experiments on four widely used datasets, that is, SBU, N‐UCLA, NTU RGB + D, and NTU RGB + D 120 are conducted. The experimental results show that SCA‐GCN surpasses existing methods, and its core idea can be extended to other methods with only some concatenation operations that consume less computational complexity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12253"
    },
    {
        "id": 1169,
        "title": "ParticleAugment: Sampling-based data augmentation",
        "authors": "Alexander Tsaregorodtsev, Vasileios Belagiannis",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103633"
    },
    {
        "id": 1170,
        "title": "Visual tracking in video sequences based on biologically inspired mechanisms",
        "authors": "Alireza Sokhandan, Amirhassan Monadjemi",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2018.10.002"
    },
    {
        "id": 1171,
        "title": "IncludeVote: Development of an Assistive Technology Based on Computer Vision and Robotics for Application in the Brazilian Electoral Context",
        "authors": "Felipe Mendonça, João Teixeira, Marcondes Silva Júnior",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011787000003417"
    },
    {
        "id": 1172,
        "title": "Editor’s Note: Special Issue on Computer Vision and Cultural Heritage Preservation",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-023-01769-y"
    },
    {
        "id": 1173,
        "title": "Domain generalized federated learning for Person Re-identification",
        "authors": "Fangyi Liu, Mang Ye, Bo Du",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103969"
    },
    {
        "id": 1174,
        "title": "On the coherency of quantitative evaluation of visual explanations",
        "authors": "Benjamin Vandersmissen, José Oramas",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103934"
    },
    {
        "id": 1175,
        "title": "Streaming egocentric action anticipation: An evaluation scheme and approach",
        "authors": "Antonino Furnari, Giovanni Maria Farinella",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103763"
    },
    {
        "id": 1176,
        "title": "RecViT: Enhancing Vision Transformer with Top-Down Information Flow",
        "authors": "Štefan Pócoš, Iveta Bečková, Igor Farkaš",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012464700003660"
    },
    {
        "id": 1177,
        "title": "Multi-timescale boosting for efficient and improved event camera face pose alignment",
        "authors": "Arman Savran",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103817"
    },
    {
        "id": 1178,
        "title": "The following article for this Special Issue was published in a different issue",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12211"
    },
    {
        "id": 1179,
        "title": "Certifiable planar relative pose estimation with gravity prior",
        "authors": "Mercedes Garcia-Salguero, Javier Gonzalez-Jimenez",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103887"
    },
    {
        "id": 1180,
        "title": "Literatur Reviu Sistematis: Identifikasi Jenis Ular Berbasis Computer Vision",
        "authors": " Eva Putriany,  Dhani Ariatmanto",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "Systematic Literature Review ini bertujuan untuk mengidentifikasi algoritma-algoritma yang digunakan dalam identifikasi spesies ular yang menggunakan computer vision, mengevaluasi dataset, tingkat akurasi, faktor-faktor yang memengaruhi akurasi, dan keterbatasan yang dihadapi. Melalui tinjauan literatur sistematis, 20 paper terpilih dari tahun 2019-2023, yang didapat dari berbagai sumber literatur. Penelitian-penelitian tersebut mengeksplorasi berbagai strategi untuk mengatasi tantangan pengenalan objek ular secara otomatis, termasuk peningkatan kinerja model, eksplorasi pendekatan baru, dan penerapan solusi efektif. Hasil dari studi literatur menyoroti pentingnya pemrosesan data yang cermat, pemilihan arsitektur model yang tepat, serta penyesuaian parameter algoritma yang optimal dalam mencapai kinerja maksimal pada model-model yang dikembangkan. Beberapa peneliti juga mengemukakan keterbatasan dalam penelitiannya, seperti kualitas dan jumlah dataset, kompleksitas morfologi ular, dan variasi pose ular. Diperlukan kerja sama lintas disiplin dan berbagi pengetahuan untuk mengatasi tantangan ini dan memajukan bidang identifikasi spesies ular melalui computer vision.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36802/jnanaloka.2024.v5-no01-43-50"
    },
    {
        "id": 1181,
        "title": "A point‐image fusion network for event‐based frame interpolation",
        "authors": "Chushu Zhang, Wei An, Ye Zhang, Miao Li",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "AbstractTemporal information in event streams plays a critical role in event‐based video frame interpolation as it provides temporal context cues complementary to images. Most previous event‐based methods first transform the unstructured event data to structured data formats through voxelisation, and then employ advanced CNNs to extract temporal information. However, voxelisation inevitably leads to information loss, and processing the sparse voxels introduces severe computation redundancy. To address these limitations, this study proposes a point‐image fusion network (PIFNet). In our PIFNet, rich temporal information from the events can be directly extracted at the point level. Then, a fusion module is designed to fuse complementary cues from both points and images for frame interpolation. Extensive experiments on both synthetic and real datasets demonstrate that our PIFNet achieves state‐of‐the‐art performance with high efficiency.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12220"
    },
    {
        "id": 1182,
        "title": "Deep network with double reuses and convolutional shortcuts",
        "authors": "Qian Liu, Cunbao Wang",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "AbstractThe authors design a novel convolutional network architecture, that is, deep network with double reuses and convolutional shortcuts, in which new compressed reuse units are presented. Compressed reuse units combine the reused features from the first 3 × 3 convolutional layer and the features from the last 3 × 3 convolutional layer to produce new feature maps in the current compressed reuse unit, simultaneously reuse the feature maps from all previous compressed reuse units to generate a shortcut by an 1 × 1 convolution, and then concatenate these new maps and this shortcut as the input to next compressed reuse unit. Deep network with double reuses and convolutional shortcuts uses the feature reuse concatenation from all compressed reuse units as the final features for classification. In deep network with double reuses and convolutional shortcuts, the inner‐ and outer‐unit feature reuses and the convolutional shortcut compressed from the previous outer‐unit feature reuses can alleviate the vanishing‐gradient problem by strengthening the forward feature propagation inside and outside the units, improve the effectiveness of features and reduce calculation cost. Experimental results on CIFAR‐10, CIFAR‐100, ImageNet ILSVRC 2012, Pascal VOC2007 and MS COCO benchmark databases demonstrate the effectiveness of authors’ architecture for object recognition and detection, as compared with the state‐of‐the‐art.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12260"
    },
    {
        "id": 1183,
        "title": "Attribute‐guided transformer for robust person re‐identification",
        "authors": "Zhe Wang, Jun Wang, Junliang Xing",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractRecent studies reveal the crucial role of local features in learning robust and discriminative representations for person re‐identification (Re‐ID). Existing approaches typically rely on external tasks, for example, semantic segmentation, or pose estimation, to locate identifiable parts of given images. However, they heuristically utilise the predictions from off‐the‐shelf models, which may be sub‐optimal in terms of both local partition and computational efficiency. They also ignore the mutual information with other inputs, which weakens the representation capabilities of local features. In this study, the authors put forward a novel Attribute‐guided Transformer (AiT), which explicitly exploits pedestrian attributes as semantic priors for discriminative representation learning. Specifically, the authors first introduce an attribute learning process, which generates a set of attention maps highlighting the informative parts of pedestrian images. Then, the authors design a Feature Diffusion Module (FDM) to iteratively inject attribute information into global feature maps, aiming at suppressing unnecessary noise and inferring attribute‐aware representations. Last, the authors propose a Feature Aggregation Module (FAM) to exploit mutual information for aggregating attribute characteristics from different images, enhancing the representation capabilities of feature embedding. Extensive experiments demonstrate the superiority of our AiT in learning robust and discriminative representations. As a result, the authors achieve competitive performance with state‐of‐the‐art methods on several challenging benchmarks without any bells and whistles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12215"
    },
    {
        "id": 1184,
        "title": "Enabling ISPless Low-Power Computer Vision",
        "authors": "Gourav Datta, Zeyu Liu, Zihan Yin, Linyu Sun, Akhilesh R. Jaiswal, Peter A. Beerel",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00246"
    },
    {
        "id": 1185,
        "title": "Unsupervised real image super-resolution via knowledge distillation network",
        "authors": "Nianzeng Yuan, Bangyong Sun, Xiangtao Zheng",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103736"
    },
    {
        "id": 1186,
        "title": "Computer Vision for International Border Legibility",
        "authors": "Trevor Ortega, Thomas Nelson, Skyler Crane, Josh Myers-Dean, Scott Wehrwein",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00383"
    },
    {
        "id": 1187,
        "title": "Unsupervised soft-to-hard hashing with contrastive learning",
        "authors": "Wonju Lee, Seok-Yong Byun, Minje Park",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103713"
    },
    {
        "id": 1188,
        "title": "Patch-based stochastic attention for image editing",
        "authors": "Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103866"
    },
    {
        "id": 1189,
        "title": "Collaborative three-stream transformers for video captioning",
        "authors": "Hao Wang, Libo Zhang, Heng Fan, Tiejian Luo",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103799"
    },
    {
        "id": 1190,
        "title": "Predicting Various Architectural Styles Using Computer Vision Methods",
        "authors": "Meryem ÖZTÜRKOĞLU",
        "published": "2023-11-25",
        "citations": 1,
        "abstract": "Computer Vision (CV), subfield of artificial intelligence (AI), enables computers to process visual data and recognize objects. CV is widely used in, automotive, food industry and diseases diagnosis. AI achieves this by algorithms. One of the important algorithms based on object detection is YOLO (You Only Look Once), provides more accurate results with high processing speed. The aim of this study is to perform an object detection-based CV project, to determine the structures in given video belong to one of the architectural styles: Gothic, Baroque, Palladian, or Art Nouveau. The study consists of data set creation, data labeling, model creation and model training. Roboflow was used as the data labeling platform and YOLOv8 was used for model building and training phases. At the end of the process, the fact that the model predicts architectural styles with high accuracy in a short time revealed that the model is a successful real-time object detection algorithm, and it was emphasized that CV can be used in the field of architecture and can contribute to other fields related to architecture.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30785/mbud.1334044"
    },
    {
        "id": 1191,
        "title": "Scene adaptive mechanism for action recognition",
        "authors": "Cong Wu, Xiao-Jun Wu, Tianyang Xu, Josef Kittler",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103854"
    },
    {
        "id": 1192,
        "title": "Periocular biometrics and its relevance to partially masked faces: A survey",
        "authors": "Renu Sharma, Arun Ross",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103583"
    },
    {
        "id": 1193,
        "title": "Foreground discovery in streaming videos with dynamic construction of content graphs",
        "authors": "Sepehr Farhand, Gavriil Tsechpenakis",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103620"
    },
    {
        "id": 1194,
        "title": "Context‐aware relation enhancement and similarity reasoning for image‐text retrieval",
        "authors": "Zheng Cui, Yongli Hu, Yanfeng Sun, Baocai Yin",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "AbstractImage‐text retrieval is a fundamental yet challenging task, which aims to bridge a semantic gap between heterogeneous data to achieve precise measurements of semantic similarity. The technique of fine‐grained alignment between cross‐modal features plays a key role in various successful methods that have been proposed. Nevertheless, existing methods cannot effectively utilise intra‐modal information to enhance feature representation and lack powerful similarity reasoning to get a precise similarity score. Intending to tackle these issues, a context‐aware Relation Enhancement and Similarity Reasoning model, called RESR, is proposed, which conducts both intra‐modal relation enhancement and inter‐modal similarity reasoning while considering the global‐context information. For intra‐modal relation enhancement, a novel context‐aware graph convolutional network is introduced to enhance local feature representations by utilising relation and global‐context information. For inter‐modal similarity reasoning, local and global similarity features are exploited by the bidirectional alignment of image and text, and the similarity reasoning is implemented among multi‐granularity similarity features. Finally, refined local and global similarity features are adaptively fused to get a precise similarity score. The experimental results show that our effective model outperforms some state‐of‐the‐art approaches, achieving average improvements of 2.5% and 6.3% in R@sum on the Flickr30K and MS‐COCO dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12270"
    },
    {
        "id": 1195,
        "title": "Transformer-based image generation from scene graphs",
        "authors": "Renato Sortino, Simone Palazzo, Francesco Rundo, Concetto Spampinato",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103721"
    },
    {
        "id": 1196,
        "title": "Joint image restoration for object detection in snowy weather",
        "authors": "Jing Wang, Meimei Xu, Huazhu Xue, Zhanqiang Huo, Fen Luo",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "AbstractAlthough existing object detectors achieve encouraging performance of object detection and localisation under real ideal conditions, the detection performance in adverse weather conditions (snowy) is very poor and not enough to cope with the detection task in adverse weather conditions. Existing methods do not deal well with the effect of snow on the identity of object features or usually ignore or even discard potential information that can help improve the detection performance. To this end, the authors propose a novel and improved end‐to‐end object detection network joint image restoration. Specifically, in order to address the problem of identity degradation of object detection due to snow, an ingenious restoration‐detection dual branch network structure combined with a Multi‐Integrated Attention module is proposed, which can well mitigate the effect of snow on the identity of object features, thus improving the detection performance of the detector. In order to make more effective use of the features that are beneficial to the detection task, a Self‐Adaptive Feature Fusion module is introduced, which can help the network better learn the potential features that are beneficial to the detection and eliminate the effect of heavy or large local snow in the object area on detection by a special feature fusion, thus improving the network's detection capability in snowy. In addition, the authors construct a large‐scale, multi‐size snowy dataset called Synthetic and Real Snowy Dataset (SRSD), and it is a good and necessary complement and improvement to the existing snowy‐related tasks. Extensive experiments on a public snowy dataset (Snowy‐weather Datasets) and SRSD indicate that our method outperforms the existing state‐of‐the‐art object detectors.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12274"
    },
    {
        "id": 1197,
        "title": "Estimating the vertical direction in a photogrammetric 3D model, with application to visualization",
        "authors": "Maxime Lhuillier",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103814"
    },
    {
        "id": 1198,
        "title": "Computer vision algorithm practice of multiple video streams in distributed AI cluster",
        "authors": "Jiewei Li, Bo Gao, Jin Xie, Suzhou Hu",
        "published": "2023-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2673211"
    },
    {
        "id": 1199,
        "title": "Sparse graph matching network for temporal language localization in videos",
        "authors": "Guangli Wu, Tongjie Xu, Jing Zhang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103908"
    },
    {
        "id": 1200,
        "title": "Clean, performance‐robust, and performance‐sensitive historical information based adversarial self‐distillation",
        "authors": "Shuyi Li, Hongchao Hu, Shumin Huo, Hao Liang",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "AbstractAdversarial training suffers from poor effectiveness due to the challenging optimisation of loss with hard labels. To address this issue, adversarial distillation has emerged as a potential solution, encouraging target models to mimic the output of the teachers. However, reliance on pre‐training teachers leads to additional training costs and raises concerns about the reliability of their knowledge. Furthermore, existing methods fail to consider the significant differences in unconfident samples between early and late stages, potentially resulting in robust overfitting. An adversarial defence method named Clean, Performance‐robust, and Performance‐sensitive Historical Information based Adversarial Self‐Distillation (CPr & PsHI‐ASD) is presented. Firstly, an adversarial self‐distillation replacement method based on clean, performance‐robust, and performance‐sensitive historical information is developed to eliminate pre‐training costs and enhance guidance reliability for the target model. Secondly, adversarial self‐distillation algorithms that leverage knowledge distilled from the previous iteration are introduced to facilitate the self‐distillation of adversarial knowledge and mitigate the problem of robust overfitting. Experiments are conducted to evaluate the performance of the proposed method on CIFAR‐10, CIFAR‐100, and Tiny‐ImageNet datasets. The results demonstrate that the CPr&PsHI‐ASD method is more effective than existing adversarial distillation methods in enhancing adversarial robustness and mitigating robust overfitting issues against various adversarial attacks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12265"
    }
]