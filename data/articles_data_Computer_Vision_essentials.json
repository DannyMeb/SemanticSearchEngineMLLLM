[
    {
        "id": 3505,
        "title": "A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials",
        "authors": "Chuqiao Li, Zhiwu Huang, Danda Pani Paudel, Yabin Wang, Mohamad Shahbazi, Xiaopeng Hong, Luc Van Gool",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00139"
    },
    {
        "id": 3506,
        "title": "Context understanding in computer vision: A survey",
        "authors": "Xuan Wang, Zhigang Zhu",
        "published": "2023-3",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103646"
    },
    {
        "id": 3507,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00007-3"
    },
    {
        "id": 3508,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00019-x"
    },
    {
        "id": 3509,
        "title": "Guest Editorial: Multi‐view representation learning for computer vision",
        "authors": "Xin Ning, Jun Zhou, Jian Cheng, Jing Wu, Chen Wang, Lin Gu",
        "published": "2023-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12176"
    },
    {
        "id": 3510,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00250-3"
    },
    {
        "id": 3511,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00044-4"
    },
    {
        "id": 3512,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00069-9"
    },
    {
        "id": 3513,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00085-7"
    },
    {
        "id": 3514,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00275-8"
    },
    {
        "id": 3515,
        "title": "Guest Editorial: Spectral imaging powered computer vision",
        "authors": "Jun Zhou, Fengchao Xiong, Lei Tong, Naoto Yokoya, Pedram Ghamisi",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12242"
    },
    {
        "id": 3516,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00227-8"
    },
    {
        "id": 3517,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00110-8"
    },
    {
        "id": 3518,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00079-6"
    },
    {
        "id": 3519,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00172-8"
    },
    {
        "id": 3520,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00055-3"
    },
    {
        "id": 3521,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00048-6"
    },
    {
        "id": 3522,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(22)00185-0"
    },
    {
        "id": 3523,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00205-9"
    },
    {
        "id": 3524,
        "title": "Editorial Board",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(23)00135-2"
    },
    {
        "id": 3525,
        "title": "Editorial Board",
        "authors": "",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1077-3142(24)00004-3"
    },
    {
        "id": 3526,
        "title": "A Computer Vision Approach to Compute Bubble Flow of Offshore Wells",
        "authors": "Rogerio Hart, Aura Conci",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012433500003660"
    },
    {
        "id": 3527,
        "title": "Essentials for Digitalizing Maintenance Activities in SMEs",
        "authors": "Oliver Fuglsang Grooss",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2024.01.146"
    },
    {
        "id": 3528,
        "title": "Towards Generating 3D City Models with GAN and Computer Vision Methods",
        "authors": "Sarun Poolkrajang, Anand Bhojan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012315000003660"
    },
    {
        "id": 3529,
        "title": "Guest Editorial: Learning from limited annotations for computer vision tasks",
        "authors": "Yazhou Yao, Wenguan Wang, Qiang Wu, Dongfang Liu, Jin Zheng",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12229"
    },
    {
        "id": 3530,
        "title": "Evaluation of Computer Vision-Based Person Detection on Low-Cost Embedded Systems",
        "authors": "Francesco Pasti, Nicola Bellotto",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011797400003417"
    },
    {
        "id": 3531,
        "title": "OmDet: Large‐scale vision‐language multi‐dataset pre‐training with multimodal detection network",
        "authors": "Tiancheng Zhao, Peng Liu, Kyusong Lee",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "AbstractThe advancement of object detection (OD) in open‐vocabulary and open‐world scenarios is a critical challenge in computer vision. OmDet, a novel language‐aware object detection architecture and an innovative training mechanism that harnesses continual learning and multi‐dataset vision‐language pre‐training is introduced. Leveraging natural language as a universal knowledge representation, OmDet accumulates “visual vocabularies” from diverse datasets, unifying the task as a language‐conditioned detection framework. The multimodal detection network (MDN) overcomes the challenges of multi‐dataset joint training and generalizes to numerous training datasets without manual label taxonomy merging. The authors demonstrate superior performance of OmDet over strong baselines in object detection in the wild, open‐vocabulary detection, and phrase grounding, achieving state‐of‐the‐art results. Ablation studies reveal the impact of scaling the pre‐training visual vocabulary, indicating a promising direction for further expansion to larger datasets. The effectiveness of our deep fusion approach is underscored by its ability to learn jointly from multiple datasets, enhancing performance through knowledge sharing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12268"
    },
    {
        "id": 3532,
        "title": "2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)",
        "authors": "",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvmi59935.2023.10464399"
    },
    {
        "id": 3533,
        "title": "2024 IEEE Winter Conference on Applications of Computer Vision Workshops",
        "authors": "",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacvw60836.2024.00002"
    },
    {
        "id": 3534,
        "title": "Towards Better User Studies in Computer Graphics and Vision",
        "authors": "Zoya Bylinskii, Laura Herman, Aaron Hertzmann, Stefanie Hutka, Yile Zhang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/0600000106"
    },
    {
        "id": 3535,
        "title": "Does Image Anonymization Impact Computer Vision Training?",
        "authors": "Håkon Hukkelås, Frank Lindseth",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00019"
    },
    {
        "id": 3536,
        "title": "Adaptive Testing of Computer Vision Models",
        "authors": "Irena Gao, Gabriel Ilharco, Scott Lundberg, Marco Tulio Ribeiro",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00370"
    },
    {
        "id": 3537,
        "title": "2024 IEEE Winter Conference on Applications of Computer Vision Workshops WACVW 2024",
        "authors": "",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacvw60836.2024.00001"
    },
    {
        "id": 3538,
        "title": "Discoloration Resistance of Various Computer Aided Design/Computer Aided Manufacturing Restorative Materials",
        "authors": "Rana Turunç Oğuzman,  , Soner Şişmanoğlu,  ",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5152/essentdent.2024.24005"
    },
    {
        "id": 3539,
        "title": "3D Human Motion Data Compression Based on Computer Vision",
        "authors": "Peng Wang, Xiaolin Jiang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424871"
    },
    {
        "id": 3540,
        "title": "Attention-based multimodal image matching",
        "authors": "Aviad Moreshet, Yosi Keller",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103949"
    },
    {
        "id": 3541,
        "title": "Analyzing lower half facial gestures for lip reading applications: Survey on vision techniques",
        "authors": "Preethi S.J., Niranjana Krupa B.",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103738"
    },
    {
        "id": 3542,
        "title": "Proceedings of the 2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)",
        "authors": "",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvmi59935.2023.10465136"
    },
    {
        "id": 3543,
        "title": "Learnable fusion mechanisms for multimodal object detection in autonomous vehicles",
        "authors": "Yahya Massoud, Robert Laganiere",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "AbstractPerception systems in autonomous vehicles need to accurately detect and classify objects within their surrounding environments. Numerous types of sensors are deployed on these vehicles, and the combination of such multimodal data streams can significantly boost performance. The authors introduce a novel sensor fusion framework using deep convolutional neural networks. The framework employs both camera and LiDAR sensors in a multimodal, multiview configuration. The authors leverage both data types by introducing two new innovative fusion mechanisms: element‐wise multiplication and multimodal factorised bilinear pooling. The methods improve the bird's eye view moderate average precision score by +4.97% and +8.35% on the KITTI dataset when compared to traditional fusion operators like element‐wise addition and feature map concatenation. An in‐depth analysis of key design choices impacting performance, such as data augmentation, multi‐task learning, and convolutional architecture design is offered. The study aims to pave the way for the development of more robust multimodal machine vision systems. The authors conclude the paper with qualitative results, discussing both successful and problematic cases, along with potential ways to mitigate the latter.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12259"
    },
    {
        "id": 3544,
        "title": "Unsupervised image blind super resolution via real degradation feature learning",
        "authors": "Cheng Yang, Guanming Lu",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "AbstractIn recent years, many methods for image super‐resolution (SR) have relied on pairs of low‐resolution (LR) and high‐resolution (HR) images for training, where the degradation process is predefined by bicubic downsampling. While such approaches perform well in standard benchmark tests, they often fail to accurately replicate the complexity of real‐world image degradation. To address this challenge, researchers have proposed the use of unpaired image training to implicitly model the degradation process. However, there is a significant domain gap between the real‐world LR and the synthetic LR images from HR, which severely degrades the SR performance. A novel unsupervised image‐blind super‐resolution method that exploits degradation feature‐based learning for real‐image super‐resolution reconstruction (RDFL) is proposed. Their approach learns the degradation process from HR to LR using a generative adversarial network (GAN) and constrains the data distribution of the synthetic LR with real degraded images. The authors then encode the degraded features into a Transformer‐based SR network for image super‐resolution reconstruction through degradation representation learning. Extensive experiments on both synthetic and real datasets demonstrate the effectiveness and superiority of the RDFL method, which achieves visually pleasing reconstruction results.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12262"
    },
    {
        "id": 3545,
        "title": "An efficient mixed attention module",
        "authors": "Kuang Sheng, Pinghua Chen",
        "published": "2023-6",
        "citations": 1,
        "abstract": "AbstractRecently, the application of attention mechanisms in convolutional neural networks (CNNs) has become a hot area in computer vision. Most existing methods focus on channel attention or spatial attention. Some mixed attention usually achieves better performance than channel attention or spatial attention with the help of a complex model structure, which increases the complexity of the model. This article proposes an efficient mixed attention that combines channel information with spatial information using learnable broadcast addition to reduce this complexity. In particular, this module can simplify learning and improve performance with fewer parameters. Furthermore, our method uses an excitation method based on the Tanh function to reduce computational resources while maintaining model performance, and it is a lightweight attention module that can be used in arbitrary CNNs to improve performance. Experiments on ImageNet and Cifar confirm the effectiveness of the proposed method. Besides, our method remains highly competitive for object detection tasks and image segmentation tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12184"
    },
    {
        "id": 3546,
        "title": "Transformer-based computer vision technology empowers drones",
        "authors": "Mingzheng Lai, PengJie Wang, YiFan Zeng, Wei Lv",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166286"
    },
    {
        "id": 3547,
        "title": "FAM: Improving columnar vision transformer with feature attention mechanism",
        "authors": "Lan Huang, Xingyu Bai, Jia Zeng, Mengqiang Yu, Wei Pang, Kangping Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103981"
    },
    {
        "id": 3548,
        "title": "Editor’s Note: Special Issue on 3D Computer Vision",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-023-01751-8"
    },
    {
        "id": 3549,
        "title": "A temporal shift reconstruction network for compressive video sensing",
        "authors": "Zhenfei Gu, Chao Zhou, Guofeng Lin",
        "published": "2023-9-9",
        "citations": 0,
        "abstract": "AbstractCompressive sensing provides a promising sampling paradigm for video acquisition for resource‐limited sensor applications. However, the reconstruction of original video signals from sub‐sampled measurements is still a great challenge. To exploit the temporal redundancies within videos during the recovery, previous works tend to perform alignment on initial reconstructions, which are too coarse to provide accurate motion estimations. To solve this problem, the authors propose a novel reconstruction network, named TSRN, for compressive video sensing. Specifically, the authors utilise a number of stacked temporal shift reconstruction blocks (TSRBs) to enhance the initial reconstruction progressively. Each TSRB could learn the temporal structures by exchanging information with last and next time step, and no additional computations is imposed on the network compared to regular 2D convolutions due to the high efficiency of temporal shift operations. After the enhancement, a bidirectional alignment module to build accurate temporal dependencies directly with the help of optical flows is employed. Different from previous methods that only extract supplementary information from the key frames, the proposed alignment module can receive temporal information from the whole video sequence via bidirectional propagations, thus yielding better performance. Experimental results verify the superiority of the proposed method over other state‐of‐the‐art approaches quantitatively and qualitatively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12234"
    },
    {
        "id": 3550,
        "title": "Pollinators as Data Collectors: Estimating Floral Diversity with Bees and Computer Vision",
        "authors": "Frederic Tausch, Jan Wagner, Simon Klaus",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00071"
    },
    {
        "id": 3551,
        "title": "Computer Vision for Ocean Eddy Detection in Infrared Imagery",
        "authors": "Evangelos Moschos, Alisa Kugusheva, Paul Coste, Alexandre Stegner",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00633"
    },
    {
        "id": 3552,
        "title": "Single and multiple illuminant estimation using convex functions",
        "authors": "Zeinab Abedini, Mansour Jamzad",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103711"
    },
    {
        "id": 3553,
        "title": "Difficulty Estimation with Action Scores for Computer Vision Tasks",
        "authors": "Octavio Arriaga, Sebastian Palacio, Matias Valdenegro-Toro",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00030"
    },
    {
        "id": 3554,
        "title": "Image amodal completion: A survey",
        "authors": "Jiayang Ao, Qiuhong Ke, Krista A. Ehinger",
        "published": "2023-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103661"
    },
    {
        "id": 3555,
        "title": "Concept Study for Dynamic Vision Sensor Based Insect Monitoring",
        "authors": "Regina Pohle-Fröhlich, Tobias Bolten",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011775500003417"
    },
    {
        "id": 3556,
        "title": "Eulerian Single-Photon Vision",
        "authors": "Shantanu Gupta, Mohit Gupta",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00960"
    },
    {
        "id": 3557,
        "title": "Improvement of Vision Transformer Using Word Patches",
        "authors": "Ayato Takama, Sota Kato, Satoshi Kamiya, Kazuhiro Hotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011732900003417"
    },
    {
        "id": 3558,
        "title": "Video frame interpolation via spatial multi‐scale modelling",
        "authors": "Zhe Qu, Weijing Liu, Lizhen Cui, Xiaohui Yang",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "AbstractVideo frame interpolation (VFI) is a technique that synthesises intermediate frames between adjacent original video frames to enhance the temporal super‐resolution of the video. However, existing methods usually rely on heavy model architectures with a large number of parameters. The authors introduce an efficient VFI network based on multiple lightweight convolutional units and a Local three‐scale encoding (LTSE) structure. In particular, the authors introduce a LTSE structure with two‐level attention cascades. This design is tailored to enhance the efficient capture of details and contextual information across diverse scales in images. Secondly, the authors introduce recurrent convolutional layers (RCL) and residual operations, designing the recurrent residual convolutional unit to optimise the LTSE structure. Additionally, a lightweight convolutional unit named separable recurrent residual convolutional unit is introduced to reduce the model parameters. Finally, the authors obtain the three‐scale decoding features from the decoder and warp them for a set of three‐scale pre‐warped maps. The authors fuse them into the synthesis network to generate high‐quality interpolated frames. The experimental results indicate that the proposed approach achieves superior performance with fewer model parameters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12281"
    },
    {
        "id": 3559,
        "title": "Domain-aware triplet loss in domain generalization",
        "authors": "Kaiyu Guo, Brian C. Lovell",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103979"
    },
    {
        "id": 3560,
        "title": "Penalizing proposals using classifiers for semi-supervised object detection",
        "authors": "Somnath Hazra, Pallab Dasgupta",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103772"
    },
    {
        "id": 3561,
        "title": "Masking Strategies for Background Bias Removal in Computer Vision Models",
        "authors": "Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00474"
    },
    {
        "id": 3562,
        "title": "Improved triplet loss for domain adaptation",
        "authors": "Xiaoshun Wang, Yunhan Li, Xiangliang Zhang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractA technique known as domain adaptation is utilised to address classification challenges in an unlabelled target domain by leveraging labelled source domains. Previous domain adaptation approaches have predominantly focussed on global domain adaptation, neglecting class‐level information and resulting in suboptimal transfer performance. In recent years, a considerable number of researchers have explored class‐level domain adaptation, aiming to precisely align the distribution of diverse domains. Nevertheless, existing research on class‐level alignment tends to align domain features either on or in proximity to classification boundaries, which introduces ambiguous samples that can impact classification accuracy. In this study, the authors propose a novel strategy called class guided constraints (CGC) to tackle this issue. Specifically, CGC is employed to preserve the compactness within classes and separability between classes of domain features prior to class‐level alignment. Furthermore, the authors incorporate CGC in conjunction with similarity guided constraint. Comprehensive evaluations conducted on four public datasets demonstrate that our approach outperforms numerous state‐of‐the‐art domain adaptation methods significantly and achieves greater improvements compared to the baseline approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12226"
    },
    {
        "id": 3563,
        "title": "Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad",
        "authors": "Thomas Manzini, Robin Murphy",
        "published": "2023-10-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00409"
    },
    {
        "id": 3564,
        "title": "GradPaint: Gradient-guided inpainting with diffusion models",
        "authors": "Asya Grechka, Guillaume Couairon, Matthieu Cord",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103928"
    },
    {
        "id": 3565,
        "title": "Applying Positional Encoding to Enhance Vision-Language Transformers",
        "authors": "Xuehao Liu, Sarah Delany, Susan McKeever",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011796100003417"
    },
    {
        "id": 3566,
        "title": "MATTE: Multi-task multi-scale attention",
        "authors": "Gjorgji Strezoski, Nanne van Noord, Marcel Worring",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103622"
    },
    {
        "id": 3567,
        "title": "Extending function mixture network for improved spectral super-resolution",
        "authors": "Sadia Hussain, Brejesh Lall",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103834"
    },
    {
        "id": 3568,
        "title": "Uni-NLX: Unifying Textual Explanations for Vision and Vision-Language Tasks",
        "authors": "Fawaz Sammani, Nikos Deligiannis",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00498"
    },
    {
        "id": 3569,
        "title": "Trajectory-Prediction with Vision: A Survey",
        "authors": "Apoorv Singh",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00356"
    },
    {
        "id": 3570,
        "title": "Fully synthetic training for image restoration tasks",
        "authors": "Raphaël Achddou, Yann Gousseau, Saïd Ladjal",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103723"
    },
    {
        "id": 3571,
        "title": "Robotics and Computer Vision in the Brazilian Electoral Context: A Case Study",
        "authors": "Jairton Falcão Filho, Matheus Costa, Jonas Silva, Cecília Santos da Silva, Felipe Mendonca, Jefferson Norberto, Riei Rodrigues, Marcondes Silva Júnior",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012379300003660"
    },
    {
        "id": 3572,
        "title": "IEEE Computer Society",
        "authors": "",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacvw60836.2024.00007"
    },
    {
        "id": 3573,
        "title": "Multi-layered self-attention mechanism for weakly supervised semantic segmentation",
        "authors": "Avinash Yaganapu, Mingon Kang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103886"
    },
    {
        "id": 3574,
        "title": "IncludeVote: Development of an Assistive Technology Based on Computer Vision and Robotics for Application in the Brazilian Electoral Context",
        "authors": "Felipe Mendonça, João Teixeira, Marcondes Silva Júnior",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011787000003417"
    },
    {
        "id": 3575,
        "title": "ParticleAugment: Sampling-based data augmentation",
        "authors": "Alexander Tsaregorodtsev, Vasileios Belagiannis",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103633"
    },
    {
        "id": 3576,
        "title": "Visual tracking in video sequences based on biologically inspired mechanisms",
        "authors": "Alireza Sokhandan, Amirhassan Monadjemi",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2018.10.002"
    },
    {
        "id": 3577,
        "title": "Action Capsules: Human skeleton action recognition",
        "authors": "Ali Farajzadeh Bavil, Hamed Damirchi, Hamid D. Taghirad",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103722"
    },
    {
        "id": 3578,
        "title": "Scene context‐aware graph convolutional network for skeleton‐based action recognition",
        "authors": "Wenxian Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractSkeleton‐based action recognition methods commonly employ graph neural networks to learn different aspects of skeleton topology information However, these methods often struggle to capture contextual information beyond the skeleton topology. To address this issue, a Scene Context‐aware Graph Convolutional Network (SCA‐GCN) that leverages potential contextual information in the scene is proposed. Specifically, SCA‐GCN learns the co‐occurrence probabilities of actions in specific scenarios from a common knowledge base and fuses these probabilities into the original skeleton topology decoder, producing more robust results. To demonstrate the effectiveness of SCA‐GCN, extensive experiments on four widely used datasets, that is, SBU, N‐UCLA, NTU RGB + D, and NTU RGB + D 120 are conducted. The experimental results show that SCA‐GCN surpasses existing methods, and its core idea can be extended to other methods with only some concatenation operations that consume less computational complexity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12253"
    },
    {
        "id": 3579,
        "title": "Editor’s Note: Special Issue on Computer Vision and Cultural Heritage Preservation",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-023-01769-y"
    },
    {
        "id": 3580,
        "title": "Domain generalized federated learning for Person Re-identification",
        "authors": "Fangyi Liu, Mang Ye, Bo Du",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103969"
    },
    {
        "id": 3581,
        "title": "On the coherency of quantitative evaluation of visual explanations",
        "authors": "Benjamin Vandersmissen, José Oramas",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103934"
    },
    {
        "id": 3582,
        "title": "Streaming egocentric action anticipation: An evaluation scheme and approach",
        "authors": "Antonino Furnari, Giovanni Maria Farinella",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103763"
    },
    {
        "id": 3583,
        "title": "Multi-timescale boosting for efficient and improved event camera face pose alignment",
        "authors": "Arman Savran",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103817"
    },
    {
        "id": 3584,
        "title": "The following article for this Special Issue was published in a different issue",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12211"
    },
    {
        "id": 3585,
        "title": "Certifiable planar relative pose estimation with gravity prior",
        "authors": "Mercedes Garcia-Salguero, Javier Gonzalez-Jimenez",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103887"
    },
    {
        "id": 3586,
        "title": "Literatur Reviu Sistematis: Identifikasi Jenis Ular Berbasis Computer Vision",
        "authors": " Eva Putriany,  Dhani Ariatmanto",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "Systematic Literature Review ini bertujuan untuk mengidentifikasi algoritma-algoritma yang digunakan dalam identifikasi spesies ular yang menggunakan computer vision, mengevaluasi dataset, tingkat akurasi, faktor-faktor yang memengaruhi akurasi, dan keterbatasan yang dihadapi. Melalui tinjauan literatur sistematis, 20 paper terpilih dari tahun 2019-2023, yang didapat dari berbagai sumber literatur. Penelitian-penelitian tersebut mengeksplorasi berbagai strategi untuk mengatasi tantangan pengenalan objek ular secara otomatis, termasuk peningkatan kinerja model, eksplorasi pendekatan baru, dan penerapan solusi efektif. Hasil dari studi literatur menyoroti pentingnya pemrosesan data yang cermat, pemilihan arsitektur model yang tepat, serta penyesuaian parameter algoritma yang optimal dalam mencapai kinerja maksimal pada model-model yang dikembangkan. Beberapa peneliti juga mengemukakan keterbatasan dalam penelitiannya, seperti kualitas dan jumlah dataset, kompleksitas morfologi ular, dan variasi pose ular. Diperlukan kerja sama lintas disiplin dan berbagi pengetahuan untuk mengatasi tantangan ini dan memajukan bidang identifikasi spesies ular melalui computer vision.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36802/jnanaloka.2024.v5-no01-43-50"
    },
    {
        "id": 3587,
        "title": "A point‐image fusion network for event‐based frame interpolation",
        "authors": "Chushu Zhang, Wei An, Ye Zhang, Miao Li",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "AbstractTemporal information in event streams plays a critical role in event‐based video frame interpolation as it provides temporal context cues complementary to images. Most previous event‐based methods first transform the unstructured event data to structured data formats through voxelisation, and then employ advanced CNNs to extract temporal information. However, voxelisation inevitably leads to information loss, and processing the sparse voxels introduces severe computation redundancy. To address these limitations, this study proposes a point‐image fusion network (PIFNet). In our PIFNet, rich temporal information from the events can be directly extracted at the point level. Then, a fusion module is designed to fuse complementary cues from both points and images for frame interpolation. Extensive experiments on both synthetic and real datasets demonstrate that our PIFNet achieves state‐of‐the‐art performance with high efficiency.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12220"
    },
    {
        "id": 3588,
        "title": "RecViT: Enhancing Vision Transformer with Top-Down Information Flow",
        "authors": "Štefan Pócoš, Iveta Bečková, Igor Farkaš",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012464700003660"
    },
    {
        "id": 3589,
        "title": "Deep network with double reuses and convolutional shortcuts",
        "authors": "Qian Liu, Cunbao Wang",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "AbstractThe authors design a novel convolutional network architecture, that is, deep network with double reuses and convolutional shortcuts, in which new compressed reuse units are presented. Compressed reuse units combine the reused features from the first 3 × 3 convolutional layer and the features from the last 3 × 3 convolutional layer to produce new feature maps in the current compressed reuse unit, simultaneously reuse the feature maps from all previous compressed reuse units to generate a shortcut by an 1 × 1 convolution, and then concatenate these new maps and this shortcut as the input to next compressed reuse unit. Deep network with double reuses and convolutional shortcuts uses the feature reuse concatenation from all compressed reuse units as the final features for classification. In deep network with double reuses and convolutional shortcuts, the inner‐ and outer‐unit feature reuses and the convolutional shortcut compressed from the previous outer‐unit feature reuses can alleviate the vanishing‐gradient problem by strengthening the forward feature propagation inside and outside the units, improve the effectiveness of features and reduce calculation cost. Experimental results on CIFAR‐10, CIFAR‐100, ImageNet ILSVRC 2012, Pascal VOC2007 and MS COCO benchmark databases demonstrate the effectiveness of authors’ architecture for object recognition and detection, as compared with the state‐of‐the‐art.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12260"
    },
    {
        "id": 3590,
        "title": "Attribute‐guided transformer for robust person re‐identification",
        "authors": "Zhe Wang, Jun Wang, Junliang Xing",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractRecent studies reveal the crucial role of local features in learning robust and discriminative representations for person re‐identification (Re‐ID). Existing approaches typically rely on external tasks, for example, semantic segmentation, or pose estimation, to locate identifiable parts of given images. However, they heuristically utilise the predictions from off‐the‐shelf models, which may be sub‐optimal in terms of both local partition and computational efficiency. They also ignore the mutual information with other inputs, which weakens the representation capabilities of local features. In this study, the authors put forward a novel Attribute‐guided Transformer (AiT), which explicitly exploits pedestrian attributes as semantic priors for discriminative representation learning. Specifically, the authors first introduce an attribute learning process, which generates a set of attention maps highlighting the informative parts of pedestrian images. Then, the authors design a Feature Diffusion Module (FDM) to iteratively inject attribute information into global feature maps, aiming at suppressing unnecessary noise and inferring attribute‐aware representations. Last, the authors propose a Feature Aggregation Module (FAM) to exploit mutual information for aggregating attribute characteristics from different images, enhancing the representation capabilities of feature embedding. Extensive experiments demonstrate the superiority of our AiT in learning robust and discriminative representations. As a result, the authors achieve competitive performance with state‐of‐the‐art methods on several challenging benchmarks without any bells and whistles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12215"
    },
    {
        "id": 3591,
        "title": "Enabling ISPless Low-Power Computer Vision",
        "authors": "Gourav Datta, Zeyu Liu, Zihan Yin, Linyu Sun, Akhilesh R. Jaiswal, Peter A. Beerel",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00246"
    },
    {
        "id": 3592,
        "title": "Unsupervised real image super-resolution via knowledge distillation network",
        "authors": "Nianzeng Yuan, Bangyong Sun, Xiangtao Zheng",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103736"
    },
    {
        "id": 3593,
        "title": "Unsupervised soft-to-hard hashing with contrastive learning",
        "authors": "Wonju Lee, Seok-Yong Byun, Minje Park",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103713"
    },
    {
        "id": 3594,
        "title": "Patch-based stochastic attention for image editing",
        "authors": "Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103866"
    },
    {
        "id": 3595,
        "title": "Computer Vision for International Border Legibility",
        "authors": "Trevor Ortega, Thomas Nelson, Skyler Crane, Josh Myers-Dean, Scott Wehrwein",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00383"
    },
    {
        "id": 3596,
        "title": "Collaborative three-stream transformers for video captioning",
        "authors": "Hao Wang, Libo Zhang, Heng Fan, Tiejian Luo",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103799"
    },
    {
        "id": 3597,
        "title": "Predicting Various Architectural Styles Using Computer Vision Methods",
        "authors": "Meryem ÖZTÜRKOĞLU",
        "published": "2023-11-25",
        "citations": 1,
        "abstract": "Computer Vision (CV), subfield of artificial intelligence (AI), enables computers to process visual data and recognize objects. CV is widely used in, automotive, food industry and diseases diagnosis. AI achieves this by algorithms. One of the important algorithms based on object detection is YOLO (You Only Look Once), provides more accurate results with high processing speed. The aim of this study is to perform an object detection-based CV project, to determine the structures in given video belong to one of the architectural styles: Gothic, Baroque, Palladian, or Art Nouveau. The study consists of data set creation, data labeling, model creation and model training. Roboflow was used as the data labeling platform and YOLOv8 was used for model building and training phases. At the end of the process, the fact that the model predicts architectural styles with high accuracy in a short time revealed that the model is a successful real-time object detection algorithm, and it was emphasized that CV can be used in the field of architecture and can contribute to other fields related to architecture.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30785/mbud.1334044"
    },
    {
        "id": 3598,
        "title": "Scene adaptive mechanism for action recognition",
        "authors": "Cong Wu, Xiao-Jun Wu, Tianyang Xu, Josef Kittler",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103854"
    },
    {
        "id": 3599,
        "title": "Periocular biometrics and its relevance to partially masked faces: A survey",
        "authors": "Renu Sharma, Arun Ross",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103583"
    },
    {
        "id": 3600,
        "title": "Estimating the vertical direction in a photogrammetric 3D model, with application to visualization",
        "authors": "Maxime Lhuillier",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103814"
    },
    {
        "id": 3601,
        "title": "Foreground discovery in streaming videos with dynamic construction of content graphs",
        "authors": "Sepehr Farhand, Gavriil Tsechpenakis",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103620"
    },
    {
        "id": 3602,
        "title": "Context‐aware relation enhancement and similarity reasoning for image‐text retrieval",
        "authors": "Zheng Cui, Yongli Hu, Yanfeng Sun, Baocai Yin",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "AbstractImage‐text retrieval is a fundamental yet challenging task, which aims to bridge a semantic gap between heterogeneous data to achieve precise measurements of semantic similarity. The technique of fine‐grained alignment between cross‐modal features plays a key role in various successful methods that have been proposed. Nevertheless, existing methods cannot effectively utilise intra‐modal information to enhance feature representation and lack powerful similarity reasoning to get a precise similarity score. Intending to tackle these issues, a context‐aware Relation Enhancement and Similarity Reasoning model, called RESR, is proposed, which conducts both intra‐modal relation enhancement and inter‐modal similarity reasoning while considering the global‐context information. For intra‐modal relation enhancement, a novel context‐aware graph convolutional network is introduced to enhance local feature representations by utilising relation and global‐context information. For inter‐modal similarity reasoning, local and global similarity features are exploited by the bidirectional alignment of image and text, and the similarity reasoning is implemented among multi‐granularity similarity features. Finally, refined local and global similarity features are adaptively fused to get a precise similarity score. The experimental results show that our effective model outperforms some state‐of‐the‐art approaches, achieving average improvements of 2.5% and 6.3% in R@sum on the Flickr30K and MS‐COCO dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12270"
    },
    {
        "id": 3603,
        "title": "Transformer-based image generation from scene graphs",
        "authors": "Renato Sortino, Simone Palazzo, Francesco Rundo, Concetto Spampinato",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103721"
    },
    {
        "id": 3604,
        "title": "Joint image restoration for object detection in snowy weather",
        "authors": "Jing Wang, Meimei Xu, Huazhu Xue, Zhanqiang Huo, Fen Luo",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "AbstractAlthough existing object detectors achieve encouraging performance of object detection and localisation under real ideal conditions, the detection performance in adverse weather conditions (snowy) is very poor and not enough to cope with the detection task in adverse weather conditions. Existing methods do not deal well with the effect of snow on the identity of object features or usually ignore or even discard potential information that can help improve the detection performance. To this end, the authors propose a novel and improved end‐to‐end object detection network joint image restoration. Specifically, in order to address the problem of identity degradation of object detection due to snow, an ingenious restoration‐detection dual branch network structure combined with a Multi‐Integrated Attention module is proposed, which can well mitigate the effect of snow on the identity of object features, thus improving the detection performance of the detector. In order to make more effective use of the features that are beneficial to the detection task, a Self‐Adaptive Feature Fusion module is introduced, which can help the network better learn the potential features that are beneficial to the detection and eliminate the effect of heavy or large local snow in the object area on detection by a special feature fusion, thus improving the network's detection capability in snowy. In addition, the authors construct a large‐scale, multi‐size snowy dataset called Synthetic and Real Snowy Dataset (SRSD), and it is a good and necessary complement and improvement to the existing snowy‐related tasks. Extensive experiments on a public snowy dataset (Snowy‐weather Datasets) and SRSD indicate that our method outperforms the existing state‐of‐the‐art object detectors.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12274"
    },
    {
        "id": 3605,
        "title": "Computer vision algorithm practice of multiple video streams in distributed AI cluster",
        "authors": "Jiewei Li, Bo Gao, Jin Xie, Suzhou Hu",
        "published": "2023-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2673211"
    },
    {
        "id": 3606,
        "title": "Sparse graph matching network for temporal language localization in videos",
        "authors": "Guangli Wu, Tongjie Xu, Jing Zhang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103908"
    },
    {
        "id": 3607,
        "title": "Clean, performance‐robust, and performance‐sensitive historical information based adversarial self‐distillation",
        "authors": "Shuyi Li, Hongchao Hu, Shumin Huo, Hao Liang",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "AbstractAdversarial training suffers from poor effectiveness due to the challenging optimisation of loss with hard labels. To address this issue, adversarial distillation has emerged as a potential solution, encouraging target models to mimic the output of the teachers. However, reliance on pre‐training teachers leads to additional training costs and raises concerns about the reliability of their knowledge. Furthermore, existing methods fail to consider the significant differences in unconfident samples between early and late stages, potentially resulting in robust overfitting. An adversarial defence method named Clean, Performance‐robust, and Performance‐sensitive Historical Information based Adversarial Self‐Distillation (CPr & PsHI‐ASD) is presented. Firstly, an adversarial self‐distillation replacement method based on clean, performance‐robust, and performance‐sensitive historical information is developed to eliminate pre‐training costs and enhance guidance reliability for the target model. Secondly, adversarial self‐distillation algorithms that leverage knowledge distilled from the previous iteration are introduced to facilitate the self‐distillation of adversarial knowledge and mitigate the problem of robust overfitting. Experiments are conducted to evaluate the performance of the proposed method on CIFAR‐10, CIFAR‐100, and Tiny‐ImageNet datasets. The results demonstrate that the CPr&PsHI‐ASD method is more effective than existing adversarial distillation methods in enhancing adversarial robustness and mitigating robust overfitting issues against various adversarial attacks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12265"
    },
    {
        "id": 3608,
        "title": "Cross-domain multi-style merge for image captioning",
        "authors": "Yiqun Duan, Zhen Wang, Yi Li, Jingya Wang",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103617"
    },
    {
        "id": 3609,
        "title": "ReIDTracker_Sea: Multi-Object Tracking in Maritime Computer Vision",
        "authors": "Kaer Huang, Weitu Chong, Hui Yang, Kanokphan Lertniphonphan, Jun Xie, Feng Chen",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacvw60836.2024.00130"
    },
    {
        "id": 3610,
        "title": "Space–time recurrent memory network",
        "authors": "Hung Nguyen, Chanho Kim, Fuxin Li",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103943"
    },
    {
        "id": 3611,
        "title": "Vegetation Coverage and Urban Amenity Mapping Using Computer Vision and Machine Learning",
        "authors": "Nicholas Karkut, Alexey Kiriluk, Zihao Zhang, Zhigang Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011705100003497"
    },
    {
        "id": 3612,
        "title": "Survey on fast dense video segmentation techniques",
        "authors": "Quentin Monnier, Tania Pouli, Kidiyo Kpalma",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103959"
    },
    {
        "id": 3613,
        "title": "STFT: Spatial and temporal feature fusion for transformer tracker",
        "authors": "Hao Zhang, Yan Piao, Nan Qi",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractSiamese‐based trackers have demonstrated robust performance in object tracking, while Transformers have achieved widespread success in object detection. Currently, many researchers use a hybrid structure of convolutional neural networks and Transformers to design the backbone network of trackers, aiming to improve performance. However, this approach often underutilises the global feature extraction capability of Transformers. The authors propose a novel Transformer‐based tracker that fuses spatial and temporal features. The tracker consists of a multilayer spatial feature fusion network (MSFFN), a temporal feature fusion network (TFFN), and a prediction head. The MSFFN includes two phases: feature extraction and feature fusion, and both phases are constructed with a Transformer. Compared with the hybrid structure of “CNNs + Transformer,” the proposed method enhances the continuity of feature extraction and the ability of information interaction between features, enabling comprehensive feature extraction. Moreover, to consider the temporal dimension, the authors propose a TFFN for updating the template image. The network utilises the Transformer to fuse the tracking results of multiple frames with the initial frame, allowing the template image to continuously incorporate more information and maintain the accuracy of target features. Extensive experiments show that the tracker STFT achieves state‐of‐the‐art results on multiple benchmarks (OTB100, VOT2018, LaSOT, GOT‐10K, and UAV123). Especially, the tracker STFT achieves remarkable area under the curve score of 0.652 and 0.706 on the LaSOT and OTB100 benchmark respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12233"
    },
    {
        "id": 3614,
        "title": "Progressive Recurrent Network for shadow removal",
        "authors": "Yonghui Wang, Wengang Zhou, Hao Feng, Li Li, Houqiang Li",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103861"
    },
    {
        "id": 3615,
        "title": "Guest Editorial : Learning with Manifolds in Computer Vision",
        "authors": "Mohamed Daoudi, Mehrtash Harandi, Vittorio Murino",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.imavis.2022.104599"
    },
    {
        "id": 3616,
        "title": "Image Retrieval Based on ConvNets and Hashing Algorithm",
        "authors": "Nam Hoang Tran,  , Khoa Nhat Nguyen, Viet Hoai Vo,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "Image retrieval is a prominent subject of study in the fields of image processing and computer vision. With its application in various domains, such as logo search, product search or general image search in Google, Bing, etc., image retrieval has received significant attention for many years. In this work, we study and investigate a framework that leverages visual transferring fea-tures and hashing algorithms for the purpose of finding similar images in a dataset. The key idea of our solution is to find the answer to the following question: “How can we convert an image into binary code and search it more efficiently in a large-scale dataset?”. To achieve this pur-pose, we use pretrained CNN models from ImageNet for image representation and then convert them into binary code by using hashing algorithms. These images in the dataset are represented by binary codes, and the Hamming distance is used to find the images in the dataset that are in-dexed. To demonstrate the robustness of the system, we systematically tested the performance of the system based on speed with raw indexing and hashing indexing on 4 datasets: CIFAR-10, Caltech-101, Oxford-102-Flowers, and MS-COCO 2017. The experimental results show that lo-cal sensitive hashing (LSH) algorithms with 2,048 bits in binary code demonstrate the same or greater precision than raw indexing. Furthermore, the findings show that the MobileNet architec-ture consistently outperforms other architectures across these datasets, effectively balancing speed and precision.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32508/stdj.v26i4.4193"
    },
    {
        "id": 3617,
        "title": "Learning-based Visual Compression",
        "authors": "Ruolei Ji, Lina J. Karam",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/0600000101"
    },
    {
        "id": 3618,
        "title": "TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition",
        "authors": "Zhenyu Li, Zhenbiao Dong",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103929"
    },
    {
        "id": 3619,
        "title": "Deep Bregman divergence for self-supervised representations learning",
        "authors": "Mina Rezaei, Farzin Soleymani, Bernd Bischl, Shekoofeh Azizi",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103801"
    },
    {
        "id": 3620,
        "title": "Multi-guided-based image matting via boundary detection",
        "authors": "Guilin Yao, Anming Sun",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103998"
    },
    {
        "id": 3621,
        "title": "Infrared and visible image fusion via mutual information maximization",
        "authors": "Aiqing Fang, Junsheng Wu, Ying Li, Ruimin Qiao",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103683"
    },
    {
        "id": 3622,
        "title": "Learning single and multi-scene camera pose regression with transformer encoders",
        "authors": "Yoli Shavit, Ron Ferens, Yosi Keller",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103982"
    },
    {
        "id": 3623,
        "title": "Spatial location constraint prototype loss for open set recognition",
        "authors": "Ziheng Xia, Penghui Wang, Ganggang Dong, Hongwei Liu",
        "published": "2023-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103651"
    },
    {
        "id": 3624,
        "title": "Computational Imaging Through Atmospheric Turbulence",
        "authors": "Stanley H. Chan, Nicholas Chimitt",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/0600000103"
    },
    {
        "id": 3625,
        "title": "Instance segmentation by blend U‐Net and VOLO network",
        "authors": "Hongfei Deng, Bin Wen, Rui Wang, Zuwei Feng",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "AbstractInstance segmentation is still challengeable to correctly distinguish different instances on overlapping, dense and large number of target objects. To address this, the authors simplify the instance segmentation problem to an instance classification problem and propose a novel end‐to‐end trained instance segmentation algorithm CotuNet. Firstly, the algorithm combines convolutional neural networks (CNN), Outlooker and Transformer to design a new hybrid Encoder (COT) to further feature extraction. It consists of extracting low‐level features of the image using CNN, which is passed through the Outlooker to extract more refined local data representations. Then global contextual information is generated by aggregating the data representations in local space using Transformer. Finally, the combination of cascaded upsampling and skip connection modules is used as Decoders (C‐UP) to enable the blend of multiple different scales of high‐resolution information to generate accurate masks. By validating on the CVPPP 2017 dataset and comparing with previous state‐of‐the‐art methods, CotuNet shows superior competitiveness and segmentation performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12275"
    },
    {
        "id": 3626,
        "title": "Keynotes",
        "authors": "",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00006"
    },
    {
        "id": 3627,
        "title": "High‐precision skeleton‐based human repetitive action counting",
        "authors": "Chengxian Li, Ming Shao, Qirui Yang, Siyu Xia",
        "published": "2023-9",
        "citations": 1,
        "abstract": "AbstractA novel counting model is presented by the authors to estimate the number of repetitive actions in temporal 3D skeleton data. As per the authors’ knowledge, this is the first work of this kind using skeleton data for high‐precision repetitive action counting. Different from existing works on RGB video data, the authors’ model follows a bottom‐up pipeline to clip the sub‐action first followed by robust aggregation in inference. First, novel counting loss functions and robust inference with backtracking is proposed to pursue precise per‐frame count as well as overall count with boundary frames. Second, an efficient synthetic approach is proposed to augment skeleton data in training and thus avoid time‐consuming repetitive action data collection work. Finally, a challenging human repetitive action counting dataset named VSRep is collected with various types of action to evaluate the proposed model. Experiments demonstrate that the proposed counting model outperforms existing video‐based methods by a large margin in terms of accuracy in real‐time inference.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12193"
    },
    {
        "id": 3628,
        "title": "Improved Short-term Dense Bottleneck network for efficient scene analysis",
        "authors": "Tanmay Singha, Duc-Son Pham, Aneesh Krishna",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103795"
    },
    {
        "id": 3629,
        "title": "CMGNet: Collaborative multi-modal graph network for video captioning",
        "authors": "Qi Rao, Xin Yu, Guang Li, Linchao Zhu",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103864"
    },
    {
        "id": 3630,
        "title": "MULLER: Multilayer Laplacian Resizer for Vision",
        "authors": "Zhengzhong Tu, Peyman Milanfar, Hossein Talebi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00633"
    },
    {
        "id": 3631,
        "title": "Salient Mask-Guided Vision Transformer for Fine-Grained Classification",
        "authors": "Dmitry Demidov, Muhammad Sharif, Aliakbar Abdurahimov, Hisham Cholakkal, Fahad Khan",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011611100003417"
    },
    {
        "id": 3632,
        "title": "Copyright",
        "authors": "",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00003"
    },
    {
        "id": 3633,
        "title": "Erratum: Integration graph attention network and multi‐centre constrained loss for cross‐modality person re‐identification",
        "authors": "",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12210"
    },
    {
        "id": 3634,
        "title": "FACET: Fairness in Computer Vision Evaluation Benchmark",
        "authors": "Laura Gustafson, Chloe Rolland, Nikhila Ravi, Quentin Duval, Aaron Adcock, Cheng-Yang Fu, Melissa Hall, Candace Ross",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01863"
    },
    {
        "id": 3635,
        "title": "Sketch-based 3D shape retrieval via teacher–student learning",
        "authors": "Shuang Liang, Weidong Dai, Yiyang Cai, Chi Xie",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103903"
    },
    {
        "id": 3636,
        "title": "Online multiple object tracking with enhanced Re‐identification",
        "authors": "Wenyu Yang, Yong Jiang, Shuai Wen, Yong Fan",
        "published": "2023-9",
        "citations": 1,
        "abstract": "AbstractIn existing online multiple object tracking algorithms, schemes that combine object detection and re‐identification (ReID) tasks in a single model for simultaneous learning have drawn great attention due to their balanced speed and accuracy. However, different tasks require to focus different features. Learning two different tasks in the same model extracted features can lead to competition between the two tasks, making it difficult to achieve optimal performance. To reduce this competition, a task‐related attention network, which uses a self‐attention mechanism to allow each branch to learn on feature maps related to its task is proposed. Besides, a smooth gradient‐boosting loss function, which improves the quality of the extracted ReID features by gradually shifting the focus to the hard negative samples of each object during training is introduced. Extensive experiments on MOT16, MOT17, and MOT20 datasets demonstrate the effectiveness of the proposed method, which is also competitive in current mainstream algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12191"
    },
    {
        "id": 3637,
        "title": "Visual tracking in camera-switching outdoor sport videos: Benchmark and baselines for skiing",
        "authors": "Matteo Dunnhofer, Christian Micheloni",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103978"
    },
    {
        "id": 3638,
        "title": "Organizers",
        "authors": "",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00005"
    },
    {
        "id": 3639,
        "title": "Robustifying Token Attention for Vision Transformers",
        "authors": "Yong Guo, David Stutz, Bernt Schiele",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01610"
    },
    {
        "id": 3640,
        "title": "Vision + Language Applications: A Survey",
        "authors": "Yutong Zhou, Nobutaka Shimada",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00090"
    },
    {
        "id": 3641,
        "title": "A latent topic‐aware network for dense video captioning",
        "authors": "Tao Xu, Yuanyuan Cui, Xinyu He, Caihua Liu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "AbstractMultiple events in a long untrimmed video possess the characteristics of similarity and continuity. These characteristics can be considered as a kind of topic semantic information, which probably behaves as same sports, similar scenes, same objects etc. Inspired by this, a novel latent topic‐aware network (LTNet) is proposed in this article. The LTNet explores potential themes within videos and generates more continuous captions. Firstly, a global visual topic finder is employed to detect the similarity among events and obtain latent topic‐level features. Secondly, a latent topic‐oriented relation learner is designed to further enhance the topic‐level representations by capturing the relationship between each event and the video themes. Benefiting from the finder and the learner, the caption generator is capable of predicting more accurate and coherent descriptions. The effectiveness of our proposed method is demonstrated on ActivityNet Captions and YouCook2 datasets, where LTNet shows a relative performance of over 3.03% and 0.50% in CIDEr score respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12195"
    },
    {
        "id": 3642,
        "title": "Facial attribute classification by deep mining inter‐attribute correlations",
        "authors": "Na Liu, Fan Zhang, Liang Chang, Fuqing Duan",
        "published": "2023-4",
        "citations": 0,
        "abstract": "AbstractFace attribute classification (FAC) has received considerable attention due to its excellent application value in bio‐metric verification and face retrieval. Current FAC methods suffer two typical challenges: complex inter‐attribute correlations and imbalanced learning. Aims at the challenges, presents an end‐to‐end FAC framework with integrated use of multiple strategies, which consists of a convolutional neural network (CNN) and a graph convolutional network (GCN). The GCN is used to model the semantic correlations among attributes and capture inter‐dependency among them. The correlation information learnt via the GCN is used to guide the learning of the inter‐dependent classification features of the FAC network. An adaptive thresholding strategy and a boosting scheme are adopted to alleviate the effect of the class‐imbalance. To deal with the task imbalance problem, a new dynamic weighting scheme is proposed to update the weight of each attribute classification task in the training process. We apply four evaluation metrics to evaluate the proposed method. Experimental results show all the proposed strategies are effective, and our approach outperforms state‐of‐the‐art FAC methods on two challenging datasets CelebA and LFWA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12171"
    },
    {
        "id": 3643,
        "title": "Infrared and visible image fusion using a guiding network to leverage perceptual similarity",
        "authors": "Jun-Hyung Kim, Youngbae Hwang",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103598"
    },
    {
        "id": 3644,
        "title": "Multi-granularity Pseudo-label Collaboration for unsupervised person re-identification",
        "authors": "Xiaobao Li, Qingyong Li, Fengjiao Liang, Wen Wang",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103616"
    },
    {
        "id": 3645,
        "title": "A novel multi‐model 3D object detection framework with adaptive voxel‐image feature fusion",
        "authors": "Zhao Liu, Zhongliang Fu, Gang Li, Shengyuan Zhang",
        "published": "2024-1-17",
        "citations": 0,
        "abstract": "AbstractThe multifaceted nature of sensor data has long been a hurdle for those seeking to harness its full potential in the field of 3D object detection. Although the utilisation of point clouds as input has yielded exceptional results, the challenge of effectively combining the complementary properties of multi‐sensor data looms large. This work presents a new approach to multi‐model 3D object detection, called adaptive voxel‐image feature fusion (AVIFF). Adaptive voxel‐image feature fusion is an end‐to‐end single‐shot framework that can dynamically and adaptively fuse point cloud and image features, resulting in a more comprehensive and integrated analysis of the camera sensor and the LiDar sensor data. With the aid of the adaptive feature fusion module, spatialised image features can be adroitly fused with voxel‐based point cloud features, while the Dense Fusion module ensures the preservation of the distinctive characteristics of 3D point cloud data through the use of a heterogeneous architecture. Notably, the authors’ framework features a novel generalised intersection over union loss function that enhances the perceptibility of object localsation and rotation in 3D space. Comprehensive experimentation has validated the efficacy of the authors’ proposed modules, firmly establishing AVIFF as a novel framework in the field of 3D object detection.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12269"
    },
    {
        "id": 3646,
        "title": "Hyperbolic Deep Learning in Computer Vision: A Survey",
        "authors": "Pascal Mettes, Mina Ghadimi Atigh, Martin Keller-Ressel, Jeffrey Gu, Serena Yeung",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "AbstractDeep representation learning is a ubiquitous part of modern computer vision. While Euclidean space has been the de facto standard manifold for learning visual representations, hyperbolic space has recently gained rapid traction for learning in computer vision. Specifically, hyperbolic learning has shown a strong potential to embed hierarchical structures, learn from limited samples, quantify uncertainty, add robustness, limit error severity, and more. In this paper, we provide a categorization and in-depth overview of current literature on hyperbolic learning for computer vision. We research both supervised and unsupervised literature and identify three main research themes in each direction. We outline how hyperbolic learning is performed in all themes and discuss the main research problems that benefit from current advances in hyperbolic learning for computer vision. Moreover, we provide a high-level intuition behind hyperbolic geometry and outline open research questions to further advance research in this direction. \n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-024-02043-5"
    },
    {
        "id": 3647,
        "title": "SpATr: MoCap 3D human action recognition based on spiral auto-encoder and transformer network",
        "authors": "Hamza Bouzid, Lahoucine Ballihi",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103974"
    },
    {
        "id": 3648,
        "title": "Grow-push-prune: Aligning deep discriminants for effective structural network compression",
        "authors": "Qing Tian, Tal Arbel, James J. Clark",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103682"
    },
    {
        "id": 3649,
        "title": "Fréchet AutoEncoder Distance: A new approach for evaluation of Generative Adversarial Networks",
        "authors": "Lucas F. Buzuti, Carlos E. Thomaz",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103768"
    },
    {
        "id": 3650,
        "title": "Binocular vision localization based on vision SLAM system with multi-sensor fusion",
        "authors": "Dekai Chen, Ke Xu, Wentao Ma",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166820"
    },
    {
        "id": 3651,
        "title": "A Review of Attention Mechanisms in Computer Vision",
        "authors": "Qi Xuanhao, Zhi Min",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icivc58118.2023.10270435"
    },
    {
        "id": 3652,
        "title": "VLSlice: Interactive Vision-and-Language Slice Discovery",
        "authors": "Eric Slyman, Minsuk Kahng, Stefan Lee",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01403"
    },
    {
        "id": 3653,
        "title": "A Cross Sectional Study on Computer Vision Syndrome and Its Associated Factors among Computer Users",
        "authors": "J. Anusha, K. Vijaya",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231119074033"
    },
    {
        "id": 3654,
        "title": "Feature preserving 3D mesh denoising with a Dense Local Graph Neural Network",
        "authors": "Wenming Tang, Yuanhao Gong, Guoping Qiu",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103710"
    },
    {
        "id": 3655,
        "title": "Human skeletons and change detection for efficient violence detection in surveillance videos",
        "authors": "Guillermo Garcia-Cobo, Juan C. SanMiguel",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103739"
    },
    {
        "id": 3656,
        "title": "A closer look at branch classifiers of multi-exit architectures",
        "authors": "Shaohui Lin, Bo Ji, Rongrong Ji, Angela Yao",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103900"
    },
    {
        "id": 3657,
        "title": "Incorporating structural prior for depth regularization in shape from focus",
        "authors": "Usman Ali, Ik Hyun Lee, Muhammad Tariq Mahmood",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103619"
    },
    {
        "id": 3658,
        "title": "Snow Mask Guided Adaptive Residual Network for Image Snow Removal",
        "authors": "Bodong Cheng, Juncheng Li, Ying Chen, Tieyong Zeng",
        "published": "2023-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103819"
    },
    {
        "id": 3659,
        "title": "Computer Vision on the Edge: Individual Cattle Identification in Real-time with ReadMyCow System",
        "authors": "Moniek Smink, Haotian Liu, Dörte Döpfer, Yong Jae Lee",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00690"
    },
    {
        "id": 3660,
        "title": "X-Pruner: eXplainable Pruning for Vision Transformers",
        "authors": "Lu Yu, Wei Xiang",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.02333"
    },
    {
        "id": 3661,
        "title": "Image Inpainting on the Sketch-Pencil Domain with Vision Transformers",
        "authors": "Jose Campana, Luís Decker, Marcos Souza, Helena Maia, Helio Pedrini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363500003660"
    },
    {
        "id": 3662,
        "title": "Spatial constraint for efficient semi-supervised video object segmentation",
        "authors": "Yadang Chen, Chuanjun Ji, Zhi-Xin Yang, Enhua Wu",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103843"
    },
    {
        "id": 3663,
        "title": "Feature learning based on connectivity estimation for unbiased mammography mass classification",
        "authors": "Guobin Li, Reyer Zwiggelaar",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103884"
    },
    {
        "id": 3664,
        "title": "Image and AIS Data Fusion Technique for Maritime Computer Vision Applications",
        "authors": "Emre Gülsoylu, Paul Koch, Mert Yıldız, Manfred Constapel, André Peter Kelm",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacvw60836.2024.00098"
    },
    {
        "id": 3665,
        "title": "Editor’s Note: Special Issue from Winter Conference on Applications of Computer Vision - WACV 2023",
        "authors": "",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00138-023-01441-y"
    },
    {
        "id": 3666,
        "title": "An Introduction to Neural Data Compression",
        "authors": "Yibo Yang, Stephan Mandt, Lucas Theis",
        "published": "2023",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/0600000107"
    },
    {
        "id": 3667,
        "title": "Training Strategies for Vision Transformers for Object Detection",
        "authors": "Apoorv Singh",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00016"
    },
    {
        "id": 3668,
        "title": "Reviewers",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cipcv58883.2023.00021"
    },
    {
        "id": 3669,
        "title": "Organizers",
        "authors": "",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00005"
    },
    {
        "id": 3670,
        "title": "Understanding of Feature Representation in Convolutional Neural Networks and Vision Transformer",
        "authors": "Hiroaki Minoura, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011621300003417"
    },
    {
        "id": 3671,
        "title": "Research on daily behavior recognition method based on computer vision",
        "authors": "Jiao Wan, Kai Li, Meihui Hu, Dawei Yang, Jinping Cao",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2684235"
    },
    {
        "id": 3672,
        "title": "Spatial feature embedding for robust visual object tracking",
        "authors": "Kang Liu, Long Liu, Shangqi Yang, Zhihao Fu",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "AbstractRecently, the offline‐trained Siamese pipeline has drawn wide attention due to its outstanding tracking performance. However, the existing Siamese trackers utilise offline training to extract ‘universal’ features, which is insufficient to effectively distinguish between the target and fluctuating interference in embedding the information of the two branches, leading to inaccurate classification and localisation. In addition, the Siamese trackers employ a pre‐defined scale for cropping the search candidate region based on the previous frame's result, which might easily introduce redundant background noise (clutter, similar objects etc.), affecting the tracker's robustness. To solve these problems, the authors propose two novel sub‐network spatial employed to spatial feature embedding for robust object tracking. Specifically, the proposed spatial remapping (SRM) network enhances the feature discrepancy between target and distractor categories by online remapping, and improves the discriminant ability of the tracker on the embedding space. The MAML is used to optimise the SRM network to ensure its adaptability to complex tracking scenarios. Moreover, a temporal information proposal‐guided (TPG) network that utilises a GRU model to dynamically predict the search scale based on temporal motion states to reduce potential background interference is introduced. The proposed two network is integrated into two popular trackers, namely SiamFC++ and TransT, which achieve superior performance on six challenging benchmarks, including OTB100, VOT2019, UAV123, GOT10K, TrackingNet and LaSOT, TrackingNet and LaSOT denoting them as SiamSRMC and SiamSRMT, respectively. Moreover, the proposed trackers obtain competitive tracking performance compared with the state‐of‐the‐art trackers in the attribute of background clutter and similar object, validating the effectiveness of our method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12263"
    },
    {
        "id": 3673,
        "title": "Combinational sign language recognition",
        "authors": "Liqing Gao, Wei Feng, Fan Lyu, Liang Wan",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2024.103972"
    },
    {
        "id": 3674,
        "title": "MFMAM: Image inpainting via multi-scale feature module with attention module",
        "authors": "Yuantao Chen, Runlong Xia, Kai Yang, Ke Zou",
        "published": "2024-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103883"
    },
    {
        "id": 3675,
        "title": "WMCP-EM: An integrated dehazing framework for visibility restoration in single image",
        "authors": "Sidharth Gautam, Tapan Kumar Gandhi, B.K. Panigrahi",
        "published": "2023-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103648"
    },
    {
        "id": 3676,
        "title": "Keynotes",
        "authors": "",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00006"
    },
    {
        "id": 3677,
        "title": "Preface",
        "authors": "",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipcv57033.2023.00005"
    },
    {
        "id": 3678,
        "title": "Progressive scene text erasing with self-supervision",
        "authors": "Xiangcheng Du, Zhao Zhou, Yingbin Zheng, Xingjiao Wu, Tianlong Ma, Cheng Jin",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103712"
    },
    {
        "id": 3679,
        "title": "Robust visual question answering via semantic cross modal augmentation",
        "authors": "Akib Mashrur, Wei Luo, Nayyar A. Zaidi, Antonio Robles-Kelly",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103862"
    },
    {
        "id": 3680,
        "title": "Precondition and effect reasoning for action recognition",
        "authors": "Hongsang Yoo, Haopeng Li, Qiuhong Ke, Liangchen Liu, Rui Zhang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103691"
    },
    {
        "id": 3681,
        "title": "Copyright",
        "authors": "",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00003"
    },
    {
        "id": 3682,
        "title": "Cumulative Spatial Knowledge Distillation for Vision Transformers",
        "authors": "Borui Zhao, Renjie Song, Jiajun Liang",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00565"
    },
    {
        "id": 3683,
        "title": "Contrastive Feature Masking Open-Vocabulary Vision Transformer",
        "authors": "Dahun Kim, Anelia Angelova, Weicheng Kuo",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01430"
    },
    {
        "id": 3684,
        "title": "Human Fall Detection from Sequences of Skeleton Features using Vision Transformer",
        "authors": "Ali Raza, Muhammad Yousaf, Sergio Velastin, Serestina Viriri",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678800003417"
    },
    {
        "id": 3685,
        "title": "A novel parameter decoupling approach of personalised federated learning for image analysis",
        "authors": "Ruizheng Su, Xiongwen Pang, Hui Wang",
        "published": "2023-12",
        "citations": 1,
        "abstract": "AbstractGiven the importance of privacy protection in databases and other institutions, federated learning (FL) is used to benefit training machine learning models based on these decentralised and private data so as to address the growing vision tasks. However, for federated learning, statistical heterogeneity continues to be a major problem. Recently, plenty of personalised federated learning methods have been explored to solve the problem of statistical heterogeneity. The usage of trained base layers and the effect of feature extraction in personalised layers, however, are hardly considered in those methods that employ the learning personalised models approach. To address the problem of the statistical heterogeneity in image analysis, PCCFED, a personalised federated learning method utilizing the strategy of parameter decoupling is proposed. It should be emphasised that the authors’ personalised federated learning method decouples the personalised (P) layers into a connecting (C) layer and classifier (C) layer in order to enhance the effectiveness of feature learning for personalised layers. Further, an approach is proposed to fully use the base layers to adapt a personalised model based on the newly admitted institution's dataset through meta‐transfer. The performance of the proposed PCCFED on three datasets is evaluated under the practical non‐independent and identically distributed (non‐IID) setting. Extensive experiments demonstrate that compared with baseline methods, the proposed framework achieves the best performance in federated learning and fine‐tuning. Through FL, the investigation reveals a method to reduce statistical heterogeneity while protecting the institutions' privacy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12204"
    },
    {
        "id": 3686,
        "title": "CR‐Net: Robot grasping detection method integrating convolutional block attention module and residual module",
        "authors": "Song Yan, Lei Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractGrasping detection, which involves identifying and assessing the grasp ability of objects by robotic systems, has garnered significant attention in recent years due to its pivotal role in the development of robotic systems and automated assembly processes. Despite notable advancements in this field, current methods often grapple with both practical and theoretical challenges that hinder their real‐world applicability. These challenges encompass low detection accuracy, the burden of oversized model parameters, and the inherent complexity of real‐world scenarios. In response to these multifaceted challenges, a novel lightweight grasping detection model that not only addresses the technical aspects but also delves into the underlying theoretical complexities is introduced. The proposed model incorporates attention mechanisms and residual modules to tackle the theoretical challenges posed by varying object shapes, sizes, materials, and environmental conditions. To enhance its performance in the face of these theoretical complexities, the proposed model employs a Convolutional Block Attention Module (CBAM) to extract features from RGB and depth channels, recognising the multifaceted nature of object properties. Subsequently, a feature fusion module effectively combines these diverse features, providing a solution to the theoretical challenge of information integration. The model then processes the fused features through five residual blocks, followed by another CBAM attention module, culminating in the generation of three distinct images representing capture quality, grasping angle, and grasping width. These images collectively yield the final grasp detection results, addressing the theoretical complexities inherent in this task. The proposed model's rigorous training and evaluation on the Cornell Grasp dataset demonstrate remarkable detection accuracy rates of 98.44% on the Image‐wise split and 96.88% on the Object‐wise split. The experimental results strongly corroborate the exceptional performance of the proposed model, underscoring its ability to overcome the theoretical challenges associated with grasping detection. The integration of the residual module ensures rapid training, while the attention module facilitates precise feature extraction, ultimately striking an effective balance between detection time and accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12252"
    },
    {
        "id": 3687,
        "title": "Enhancing human parsing with region‐level learning",
        "authors": "Yanghong Zhou, P. Y. Mok",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractHuman parsing is very important in a diverse range of industrial applications. Despite the considerable progress that has been achieved, the performance of existing methods is still less than satisfactory, since these methods learn the shared features of various parsing labels at the image level. This limits the representativeness of the learnt features, especially when the distribution of parsing labels is imbalanced or the scale of different labels is substantially different. To address this limitation, a Region‐level Parsing Refiner (RPR) is proposed to enhance parsing performance by the introduction of region‐level parsing learning. Region‐level parsing focuses specifically on small regions of the body, for example, the head. The proposed RPR is an adaptive module that can be integrated with different existing human parsing models to improve their performance. Extensive experiments are conducted on two benchmark datasets, and the results demonstrated the effectiveness of our RPR model in terms of improving the overall parsing performance as well as parsing rare labels. This method was successfully applied to a commercial application for the extraction of human body measurements and has been used in various online shopping platforms for clothing size recommendations. The code and dataset are released at this link https://github.com/applezhouyp/PRP.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12222"
    },
    {
        "id": 3688,
        "title": "Real‐time vehicle detection using segmentation‐based detection network and trajectory prediction",
        "authors": "Nafiseh Zarei, Payman Moallem, Mohammadreza Shams",
        "published": "2024-3",
        "citations": 0,
        "abstract": "AbstractThe position of vehicles is determined using an algorithm that includes two stages of detection and prediction. The more the number of frames in which the detection network is used, the more accurate the detector is, and the more the prediction network is used, the algorithm is faster. Therefore, the algorithm is very flexible to achieve the required accuracy and speed. YOLO's base detection network is designed to be robust against vehicle scale changes. Also, feature maps are produced in the detector network, which contribute greatly to increasing the accuracy of the detector. In these maps, using differential images and a u‐net‐based module, image segmentation has been done into two classes: vehicle and background. To increase the accuracy of the recursive predictive network, vehicle manoeuvres are classified. For this purpose, the spatial and temporal information of the vehicles are considered simultaneously. This classifier is much more effective than classifiers that consider spatial and temporal information separately. The Highway and UA‐DETRAC datasets demonstrate the performance of the proposed algorithm in urban traffic monitoring systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12236"
    },
    {
        "id": 3689,
        "title": "Mirror complementary transformer network for RGB‐thermal salient object detection",
        "authors": "Xiurong Jiang, Yifan Hou, Hui Tian, Lin Zhu",
        "published": "2024-2",
        "citations": 2,
        "abstract": "AbstractConventional RGB‐T salient object detection treats RGB and thermal modalities equally to locate the common salient regions. However, the authors observed that the rich colour and texture information of the RGB modality makes the objects more prominent compared to the background; and the thermal modality records the temperature difference of the scene, so the objects usually contain clear and continuous edge information. In this work, a novel mirror‐complementary Transformer network (MCNet) is proposed for RGB‐T SOD, which supervise the two modalities separately with a complementary set of saliency labels under a symmetrical structure. Moreover, the attention‐based feature interaction and serial multiscale dilated convolution (SDC)‐based feature fusion modules are introduced to make the two modalities complement and adjust each other flexibly. When one modality fails, the proposed model can still accurately segment the salient regions. To demonstrate the robustness of the proposed model under challenging scenes in real world, the authors build a novel RGB‐T SOD dataset VT723 based on a large public semantic segmentation RGB‐T dataset used in the autonomous driving domain. Extensive experiments on benchmark and VT723 datasets show that the proposed method outperforms state‐of‐the‐art approaches, including CNN‐based and Transformer‐based methods. The code and dataset can be found at https://github.com/jxr326/SwinMCNet.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12221"
    },
    {
        "id": 3690,
        "title": "IEEE Computer Society Conferences on Computer Vision",
        "authors": "",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mcg.2023.3290890"
    },
    {
        "id": 3691,
        "title": "Author Index",
        "authors": "",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.02163"
    },
    {
        "id": 3692,
        "title": "Global key knowledge distillation framework",
        "authors": "Junhuang Wang, Weiwei Zhang, Yufeng Guo, Peng Liang, Ming Ji, Chenghui Zhen, Hanmeng Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103902"
    },
    {
        "id": 3693,
        "title": "Weakly supervised fine-grained semantic segmentation via spatial correlation-guided learning",
        "authors": "Zihao Dong, Tiyu Fang, Jinping Li, Xiuli Shao",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103815"
    },
    {
        "id": 3694,
        "title": "Lite‐weight semantic segmentation with AG self‐attention",
        "authors": "Bing Liu, Yansheng Gao, Hai Li, Zhaohao Zhong, Hongwei Zhao",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractDue to the large computational and GPUs memory cost of semantic segmentation, some works focus on designing a lite weight model to achieve a good trade‐off between computational cost and accuracy. A common method is to combined CNN and vision transformer. However, these methods ignore the contextual information of multi receptive fields. And existing methods often fail to inject detailed information losses in the downsampling of multi‐scale feature. To fix these issues, we propose AG Self‐Attention, which is Enhanced Atrous Self‐Attention (EASA), and Gate Attention. AG Self‐Attention adds the contextual information of multi receptive fields into the global semantic feature. Specifically, the Enhanced Atrous Self‐Attention uses weight shared atrous convolution with different atrous rates to get the contextual information under the specific different receptive fields. Gate Attention introduces gating mechanism to inject detailed information into the global semantic feature and filter detailed information by producing “fusion” gate and “update” gate. In order to prove our insight. We conduct numerous experiments in common semantic segmentation datasets, consisting of ADE20 K, COCO‐stuff, PASCAL Context, Cityscapes, to show that our method achieves state‐of‐the‐art performance and achieve a good trade‐off between computational cost and accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cvi2.12225"
    },
    {
        "id": 3695,
        "title": "Fully-attentive iterative networks for region-based controllable image and video captioning",
        "authors": "Marcella Cornia, Lorenzo Baraldi, Ayellet Tal, Rita Cucchiara",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103857"
    },
    {
        "id": 3696,
        "title": "RGB No More: Minimally-Decoded JPEG Vision Transformers",
        "authors": "Jeongsoo Park, Justin Johnson",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.02139"
    },
    {
        "id": 3697,
        "title": "Surround-View Vision-based 3D Detection for Autonomous Driving: A Survey",
        "authors": "Apoorv Singh",
        "published": "2023-10-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00348"
    },
    {
        "id": 3698,
        "title": "TrojViT: Trojan Insertion in Vision Transformers",
        "authors": "Mengxin Zheng, Qian Lou, Lei Jiang",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00392"
    },
    {
        "id": 3699,
        "title": "Multi-Scale Semantic and Detail Extraction Network for Lightweight Person Re-Identification",
        "authors": "Yunzuo Zhang, Weili Kang, Yameng Liu, Pengfei Zhu",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103813"
    },
    {
        "id": 3700,
        "title": "On learning distribution alignment for video-based visible-infrared person re-identification",
        "authors": "Pengfei Fang, Yaojun Hu, Shipeng Zhu, Hui Xue",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103833"
    },
    {
        "id": 3701,
        "title": "The Use of Computer Vision to Combat Losses from Disease in Grapevines",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7176/ceis/14-3-01"
    },
    {
        "id": 3702,
        "title": "Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations",
        "authors": "Xiaofei Huang, Michael Wan, Lingfei Luan, Bethany Tunik, Sarah Ostadabbas",
        "published": "2023-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00195"
    },
    {
        "id": 3703,
        "title": "Is Multimodal Vision Supervision Beneficial to Language?",
        "authors": "Avinash Madasu, Vasudev Lal",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00263"
    },
    {
        "id": 3704,
        "title": "Fcaformer: Forward Cross Attention in Hybrid Vision Transformer",
        "authors": "Haokui Zhang, Wenze Hu, Xiaoyu Wang",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00557"
    }
]