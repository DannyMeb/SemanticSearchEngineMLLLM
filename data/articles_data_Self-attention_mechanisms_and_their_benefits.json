[
    {
        "id": 22905,
        "title": "Bidirectional LSTM and Self-Attention Mechanisms based Multi-Label Sentiment Analysis",
        "authors": "Aruna A.R",
        "published": "2025",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijsse.2025.10059046"
    },
    {
        "id": 22906,
        "title": "Integrating Self-Attention Mechanisms and ResNet for Grain Storage Ventilation Decision Making: A Study",
        "authors": "Yuhua Zhu, Hang Li, Tong Zhen, Zhihui Li",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "Food security is a widely discussed topic globally. The key to ensuring the safety of food storage is to control temperature and humidity, with ventilation being an effective and fast method for temperature and humidity control. This paper proposes a new approach called “grain condition multimodal” based on the theory of computer multimodality. Under changing external environments, grain conditions can be classified according to different ventilation modes, including cooling ventilation, dehumidification ventilation, anti-condensation ventilation, heat dissipation ventilation, and quality adjustment ventilation. Studying intelligent ventilation decisions helps achieve grain temperature balance, prevent moisture condensation, control grain heating, reduce grain moisture, and create a low-temperature environment to improve grain storage performance. Combining deep learning models with data such as grain stack temperature and humidity can significantly improve the accuracy of ventilation decisions. This paper proposes a neural network model based on residual networks and self-attention mechanisms that performs better than basic models such as LSTM (Long Short-Term Memory), CNN (Convolutional Neural Network), GRU (Gated Recurrent Unit), and ResNet (Residual Network). The model’s accuracy, precision, recall, and F1 scores are 94.38%, 94.92%, 98.94%, and 96.89%, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13137655"
    },
    {
        "id": 22907,
        "title": "Smart vehicles networks: BERT self-attention mechanisms for cyber-physical system security",
        "authors": "Sultan Mesfer Aldossary",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13198-023-02065-1"
    },
    {
        "id": 22908,
        "title": "A GAN-Based Framework Combining Memory and Self-Attention Mechanisms for Video Anomaly Detection in Online Gaming Environments",
        "authors": "Li-ting Xiong, Bin Qu, Zhi-Ping Cheng",
        "published": "2023-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s5.91-105"
    },
    {
        "id": 22909,
        "title": "Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard",
        "authors": "Erik M. Benau",
        "published": "2023-5-9",
        "citations": 2,
        "abstract": "\nBackground\nInteroception, the perception of bodily functions and sensations, is a crucial contributor to cognition, emotion, and well-being. However, the relationship between these three processes is not well understood. Further, it is increasingly clear that dimensions of interoception differentially corresponds to these processes, yet this is only recently being explored. The present study addresses two important questions: Are subjective interoceptive accuracy and interoceptive attention related to self-regard and well-being? And are they related to exteroceptive (visual) attention?\n\n\nMethods\nParticipants (N = 98; 29% women; aged 23–64 years) completed: a battery of questionnaires to assess subjective accuracy (how well one predicts bodily sensations), interoceptive attention (a tendency to notice bodily signals), self-regard (self-esteem, self-image, life satisfaction), state negative affect (depression, anxiety, and stress), a self-esteem Implicit Association Task (a measure of implicit self-esteem), and a flanker task to assess visual selective attention. Subjective interoceptive accuracy and attention served as dependent variables. Correlations and principal component analysis was used to establish correlations among variables and determine how, or whether, these measures are associated with subjective interoceptive accuracy or attention.\n\n\nResults\nGreater scores on measures of self-regard, implicit self-esteem, cognition and lower negative affect were broadly associated with greater subjective interoceptive accuracy. Conversely, only explicit self-esteem, satisfaction with life, and self-image corresponded to subjective interoceptive attention. An exploratory analysis with a more inclusive scale of interoceptive attention was conducted. Results of this exploratory analysis showed that the broader measure was a stronger correlate to self-regard than subjective interoceptive accuracy, though it, too, did not correlate with visual attention. In short, both subjective interoceptive accuracy and attention corresponded to well-being and mental health, but only accuracy was associated with exteroceptive attention.\n\n\nConclusion\nThese results add to a growing literature suggesting different dimensions of (subjective) interoception differentially correspond to indices of well-being. The links between exteroceptive and interoceptive attention, and their association with merit further study.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.7717/peerj.15348"
    },
    {
        "id": 22910,
        "title": "TrumpetNet: A Convolutional Neural Network with Self-Attention Mechanisms for visual detection of trumpet fingering",
        "authors": "José E. Valdez-Rodríguez, Nahum Rangel, Marco A. Moreno-Armendáriz",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "Visual detection of fingering on the trumpet is an increasingly interesting topic in music research. The ability to recognize and track the movements of the trumpet player’s fingers during the performance of a musical piece can provide valuable information for analyzing and improving instrument technique. However, this is a largely unexplored task, as most works focus on audio quality rather than instrument fingering techniques. Developing techniques for identifying essential finger positions on a musical instrument is crucial, as poor fingering techniques can harm instrument performance. In this work, we propose the visual detection of this fingering using convolutional neural networks with a proprietary dataset created for this purpose. Additionally, to improve the results and focus on the essential parts of the instrument, we use self-attention mechanisms by extracting these features automatically.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/jifs-219342"
    },
    {
        "id": 22911,
        "title": "AANet: Motorcycle ReID Using Multi-Atrous Convolution and Self-Attention Mechanisms",
        "authors": "Trong-Hieu Nguyen-Mau, Kim-Trang Phu-Thi, Anh-Duy Le-Dinh, Minh-Triet Tran, Hai-Dang Nguyen",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mapr59823.2023.10288740"
    },
    {
        "id": 22912,
        "title": "Exploring Self-Attention Mechanisms for Speech Separation",
        "authors": "Cem Subakan, Mirco Ravanelli, Samuele Cornell, François Grondin, Mirko Bronzi",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/taslp.2023.3282097"
    },
    {
        "id": 22913,
        "title": "Boosting Persuasion: The Attention Benefits of Multiple Narrating Voices",
        "authors": "Hannah Chang, Anirban Mukherjee, Amitava Chattopadhyay",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4420429"
    },
    {
        "id": 22914,
        "title": "Shuffle Mixing: An Efficient Alternative to Self Attention",
        "authors": "Ryouichi Furukawa, Kazuhiro Hotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011720200003417"
    },
    {
        "id": 22915,
        "title": "Hybrid self-attention NEAT: a novel evolutionary self-attention approach to improve the NEAT algorithm in high dimensional inputs",
        "authors": "Saman Khamesian, Hamed Malek",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12530-023-09510-3"
    },
    {
        "id": 22916,
        "title": "The mechanisms and benefits of exercise",
        "authors": "Robert Wessells",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "\nThe mechanisms and benefits of exercise \nAt Wayne State University, Robert Wessells and his team are making significant strides in identifying potential exercise mediators or mimetics that could help mitigate pathologies resulting from prolonged sedentary periods. Exercise is an indispensable part of our daily life to maintain a healthy body and brain across ages. Regular exercise has been shown to reduce the incidence of many age-related diseases and preserves healthy function during normal aging, improving quality of life and independence. However, chronic exercise remains inaccessible to portions of the population due to injury, illness, advanced age or job-enforced sedentary periods. Therefore, identifying potential exercise mediators or mimetics that can deliver the benefits of exercise to sedentary people would be potentially transformative in reducing disease burden worldwide. At Wayne State University in Detroit, Michigan, USA, Dr Robert (RJ) Wessells and his lab team have used the many genetic tools available for use in fruit flies to identify several single molecules that act as powerful exercise mimetics in the brain and muscle of sedentary flies.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.56367/oag-040-10954"
    },
    {
        "id": 22917,
        "title": "Understanding the Emotional Benefits of Animal Sounds: Insights from Stress Reduction and Attention Restoration Theories",
        "authors": "Hansen Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4651804"
    },
    {
        "id": 22918,
        "title": "An Improved Siamese Tracking Network Based On Self-Attention And Cross-Attention",
        "authors": "Lai Yijun, Song Jianmei, She Haoping",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326870"
    },
    {
        "id": 22919,
        "title": "Editorial: New paradigm of attention and attention training: Mechanisms and applications",
        "authors": "Fushun Wang, Roy F. Baumeister, Yi-Yuan Tang",
        "published": "2023-1-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnhum.2023.1122941"
    },
    {
        "id": 22920,
        "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
        "authors": "Aoi Ito, Shota Horiguchi",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-270"
    },
    {
        "id": 22921,
        "title": "RacPixGAN: An Enhanced Sketch-to-Face Synthesis GAN Based on Residual modules, Multi-Head Self-Attention Mechanisms, and CLIP Loss",
        "authors": "Yuxin Wang, Yuanyuan Xie, Xiangmin Ji, Ziao Liu, Xiaolong Liu",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icecai58670.2023.10176715"
    },
    {
        "id": 22922,
        "title": "Endogenous temporal attention benefits performance even under temporal uncertainty",
        "authors": "Aysun Duyar, Shiyang Ren, Marisa Carrasco",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1167/jov.23.9.5120"
    },
    {
        "id": 22923,
        "title": "Short Term Power Load Forecasting Based on VMD Self Attention-LSTM",
        "authors": "俞霖 朵",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/aam.2023.123121"
    },
    {
        "id": 22924,
        "title": "Personalized Federated Learning with Attention Mechanisms",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.061113"
    },
    {
        "id": 22925,
        "title": "Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing",
        "authors": "Soohyeong Kim, Whanhee Cho, Minji Kim, Yong Choi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.25"
    },
    {
        "id": 22926,
        "title": "Empowering Employees: Unlocking the Benefits of Employee Self - Service by AI Driven HCM Platforms",
        "authors": "Ramesh Nyathani",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231030131808"
    },
    {
        "id": 22927,
        "title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
        "authors": "Dipesh Gyawali, Jian Zhang, Bijaya Karki",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012424500003660"
    },
    {
        "id": 22928,
        "title": "NOVEL TRANSFORMER-BASED APPROACH ENHANCED BY REINFORCEMENT LEARNING AND ATTENTION MECHANISMS",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.35741/issn.0258-2724.58.6.5"
    },
    {
        "id": 22929,
        "title": "Enhancing reinforcement learning for <i>de novo</i> molecular design applying self-attention mechanisms",
        "authors": "Tiago O Pereira, Maryam Abbasi, Joel P Arrais",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "Abstract\nThe drug discovery process can be significantly improved by applying deep reinforcement learning (RL) methods that learn to generate compounds with desired pharmacological properties. Nevertheless, RL-based methods typically condense the evaluation of sampled compounds into a single scalar value, making it difficult for the generative agent to learn the optimal policy. This work combines self-attention mechanisms and RL to generate promising molecules. The idea is to evaluate the relative significance of each atom and functional group in their interaction with the target, and to utilize this information for optimizing the Generator. Therefore, the framework for de novo drug design is composed of a Generator that samples new compounds combined with a Transformer-encoder and a biological affinity Predictor that evaluate the generated structures. Moreover, it takes the advantage of the knowledge encapsulated in the Transformer’s attention weights to evaluate each token individually. We compared the performance of two output prediction strategies for the Transformer: standard and masked language model (MLM). The results show that the MLM Transformer is more effective in optimizing the Generator compared with the state-of-the-art works. Additionally, the evaluation models identified the most important regions of each molecule for the biological interaction with the target. As a case study, we generated synthesizable hit compounds that can be putative inhibitors of the enzyme ubiquitin-specific protein 7 (USP7).",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bib/bbad368"
    },
    {
        "id": 22930,
        "title": "Multi-spectral fusion and self-attention mechanisms for Gentiana origin identification via near-infrared spectroscopy",
        "authors": "Sihai Li, Yangyang Wang, Hang Song, Mingqi Liu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chemolab.2024.105068"
    },
    {
        "id": 22931,
        "title": "TCN-Attention-BIGRU: Building energy modelling based on attention mechanisms and temporal convolutional networks",
        "authors": "Yi Deng, Zhanpeng Yue, Ziyi Wu, Yitong Li, Yifei Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "<abstract>\n\n<p>Accurate and effective building energy consumption prediction is an important basis for carrying out energy-saving evaluation and the main basis for building energy-saving optimization design. However, due to the influence of environmental and human factors, energy consumption prediction is often inaccurate. Therefore, this paper presents a building energy consumption prediction model based on an attention mechanism, time convolutional neural (TCN) network fusion, and a bidirectional gated cycle unit (BIGRU). First, t-distributed stochastic neighbor embedding (T-SNE) was used to preprocess the data and extract the key features, and then a BIGRU was employed to acquire past and future data while capturing immediate connections. Then, to catch the long-term dependence, the dataset was partitioned into the TCN network, and the extended sequence was transformed into several short sequences. Consequently, the gradient explosion or vanishing problem is mitigated when the BIGRU handles lengthy sequences while reducing the spatial complexity. Second, the self-attention mechanism was introduced to enhance the model's capability to address data periodicity. The proposed model is superior to the other four models in accuracy, with an mean absolute error of 0.023, an mean-square error of 0.029, and an coefficient of determination of 0.979. Experimental results indicate that T-SNE can significantly improve the model performance, and the accuracy of predictions can be improved by the attention mechanism and the TCN network.</p>\n\n\t      </abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/era.2024098"
    },
    {
        "id": 22932,
        "title": "What are the benefits and harms of methylphenidate in children and adolescents with attention deficit hyperactivity disorder (ADHD)?",
        "authors": "Agustín Ciapponi, Pedro V Magalhães",
        "published": "2023-4-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cca.4282"
    },
    {
        "id": 22933,
        "title": "Catch You if Pay Attention: Temporal Sensor Attack Diagnosis Using Attention Mechanisms for Cyber-Physical Systems",
        "authors": "Zifan Wang, Lin Zhang, Qinru Qiu, Fanxin Kong",
        "published": "2023-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rtss59052.2023.00016"
    },
    {
        "id": 22934,
        "title": "1D-SalsaSAN: Semantic Segmentation of LiDAR Point Cloud with Self-Attention",
        "authors": "Takahiro Suzuki, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011624100003417"
    },
    {
        "id": 22935,
        "title": "Rice yield prediction and self-attention visualization using Video Vision Transformer",
        "authors": "Dahyun Kim, Myung Hwan Na,  ",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "The government and farmers' organizations are paying much attention to the problem of predicting how much rice can be produced each year. However, it is difficult to accurately predict the yield of rice due to variable factors such as extreme climate change and various pests and diseases that change every year. In this study, images were collected several times during the growing season of rice through a multi-spectral sensor mounted on an unmanned aerial vehicle, and rice yield was predicted using a deep learning algorithm. Multispectral images can be viewed as a kind of image data taken several times at regular intervals, and rice yield was predicted using the Video Vision Transformer (ViViT) model, which applies the Transformer structure to image computer vision among deep learning algorithms. The ViViT model generates patches by dividing the input image into a certain size, and as a result of learning the model by setting the size of these patches differently, it was found that the smaller the patch size, the better the predictive power. In addition, as a result of comparing prediction performance with a 3D CNN model that receives an image as an input in a CNN (Convolutional Neural Network) structure used in the image processing field, it was found that the ViViT model using a small patch size performed better. As a result of visualizing the weight matrix of the ViViT model as a heat map, images taken in mid- to late August appear to be important in yield prediction, making it possible to predict yield about two months before rice harvest.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37727/jkdas.2023.25.4.1249"
    },
    {
        "id": 22936,
        "title": "A dual attention mechanism network with self-attention and frequency channel attention for intelligent diagnosis of multiple rolling bearing fault types",
        "authors": "Wenxing Zhang, Jianhong Yang, Xinyu Bo, Zhenkai Yang",
        "published": "2024-3-1",
        "citations": 1,
        "abstract": "Abstract\nDifferent fault types of rolling bearings correspond to different features, and classical deep learning models using a single attention mechanism (AM) have limitations in capturing feature diversity. Therefore, a novel dual attention mechanism network (DAMN) with self-attention (SA) and frequency channel attention (FCA) is proposed for rolling bearing fault diagnosis. The SA mechanism is used to capture global relationships between the input features and fault types, and the FCA mechanism applies multi-spectral attention to learn the local useful information among different input channels. The results of the ablation study on the effects of FCA blocks showed that including a proper combination of multiple frequency components is helpful in achieving higher accuracy. Experiments were conducted to diagnose rolling bearings with multiple types of faults. The results show that, compared with current fault diagnosis models, the proposed DAMN has better comprehensive performance in terms of diagnosis accuracy and model convergence speed. It was also demonstrated that the backbone of DAMN based on a dual AM could achieve better performance than the backbone based on a single AM.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad1811"
    },
    {
        "id": 22937,
        "title": "The Drawbacks (and Potential Benefits) of Cross-Domain Attention Residue",
        "authors": "Marcie LePine, Soohyun Yoon",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5465/amproc.2023.18910abstract"
    },
    {
        "id": 22938,
        "title": "Mindfulness mechanisms in everyday life: examining variance in acceptance, attention monitoring, decentering, self-compassion, and nonreactivity and their links to negative emotions among a workplace sample",
        "authors": "Larisa Gavrilova, Matthew J. Zawadzki",
        "published": "2023-10-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/02699931.2023.2252960"
    },
    {
        "id": 22939,
        "title": "Interpreting Decision of Self-Attention Network’s in the Context of Image Recognition by Efficiently Utilizing Attention Scores",
        "authors": "S.M. Haider Ali Shuvo",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eict61409.2023.10427964"
    },
    {
        "id": 22940,
        "title": "Self-Supervised Temporal Graph Learning based on Multi-Head Self-Attention Weighted Neighborhood Sequences",
        "authors": "Yulong Cao",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bdai59165.2023.10256426"
    },
    {
        "id": 22941,
        "title": "A Review of Attention Mechanisms in Computer Vision",
        "authors": "Qi Xuanhao, Zhi Min",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icivc58118.2023.10270435"
    },
    {
        "id": 22942,
        "title": "An Anomaly Prediction of Spark Log Based on Self-Attention GRU Network",
        "authors": "Yanyu Gong, Xinjiang Chen, Xiaoli Zhang, Haotian Xu, Xue Zhang, Haifeng Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012285100003807"
    },
    {
        "id": 22943,
        "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
        "authors": "Saebom Leem, Hyunseok Seo",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i4.28077"
    },
    {
        "id": 22944,
        "title": "Disentangling the Contributions of Repeating Targets, Distractors, and Stimulus Positions to Practice Benefits in D2-Like Tests of Attention",
        "authors": "Peter Wühr, Bianca Wühr",
        "published": "2023-2-27",
        "citations": 0,
        "abstract": "When a test of attention, such as the d2 test, is repeated, performance improves. These practice benefits threaten the validity of a test because it is impossible to separate the contributions of ability and practice, respectively, to a particular result. A possible solution to this dilemma would be to determine the sources of practice effects, and to use this knowledge for constructing tests that are less prone to practice. The present study investigates the contribution of three components of a d2-like test of attention to practice benefits: targets, distractors, and stimulus configurations. In Experiment 1, we compared practice effects in a target-change condition, where targets changed between sessions, to a target-repetition condition. Similarly, in Experiment 2, we compared practice effects in a distractor-change condition to a distractor-repetition condition. Finally, in Experiment 3, we compared practice effects in a position-repetition condition, where stimulus configurations were repeated within and between tests, to a position-change condition. Results showed that repeating targets and repeating distractors contribute to practice effects, whereas repeating stimulus configurations does not. Hence, in order to reduce practice effects, one might construct tests in which target learning is prevented, for example, by using multiple targets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1525/collabra.71297"
    },
    {
        "id": 22945,
        "title": "Does Adapted Self-Exercise Have Benefits for Stiff Shoulders?",
        "authors": "Kriangkrai Benjawongsathien",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "Purpose: Stiff shoulders restrict shoulder motion and affect the quality of life. Several rehabilitation programs have been implemented to improve these conditions. Various exercises have been designed to achieve positive clinical outcomes. However, too many different sets of exercises can confuse patients and lead to infrequent exercises.\nWe aimed to compare the clinical outcomes of a small set of adapted self-exercises to a usual set in patients with stiff shoulders.\nMethods: Seventy patients with stiff shoulders were randomly assigned to two groups, each performing self-exercises. Self-exercise in group I (the usual set) was composed of ‘wall climbing in front,’ ‘wall climbing at the side,’ and ‘shoulder stretching with a towel,’ and in group II (the adapted set), it was composed of ‘assisted forward flexion stretching in the standing position,’ ‘sleeper stretching in the standing position,’ and ‘doorway or corner stretching.’ The outcome measurements included pain score, functional score, and range of motion.\nResults: There were no significant differences in the baseline patient characteristics between the groups in terms of sex (p=0.759), age (p=0.521), underlying disease (p=0.322), or body mass index (BMI) (p=0.687). Group II demonstrated significantly higher improvement in mean pain score decrement (-4.5±1.7 vs. -3.5±2.4, p=0.049), mean ASES score improvement (23.1±9.9 vs. 18.3±13.1, p=0.038) and mean degree improvement of shoulder motion in all directions than in group I.\nConclusions: The adapted self-exercise set may offer favorable results in treating patients with stiff shoulders and may also be a treatment option for overweight patients.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56929/jseaortho-2024-0211"
    },
    {
        "id": 22946,
        "title": "ConvAttenMixer: Brain tumor detection and type classification using convolutional mixer with external and self-attention mechanisms",
        "authors": "Salha M. Alzahrani",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jksuci.2023.101810"
    },
    {
        "id": 22947,
        "title": "Integrating Multiple Visual Attention Mechanisms in Deep Neural Networks",
        "authors": "Fernando Martinez, Yijun Zhao",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00180"
    },
    {
        "id": 22948,
        "title": "Attention Mechanisms in Process Mining: A Systematic Literature Review",
        "authors": "Gonzalo Rivera-Lazo, Hernán Astudillo, Ricardo Ñanculef",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/clei60451.2023.10346135"
    },
    {
        "id": 22949,
        "title": "Attention-based Self-Supervised Hierarchical Semantic Segmentation for Underwater Imagery",
        "authors": "Kurran Singh, Nick Rypkema, John Leonard",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/oceanslimerick52467.2023.10244736"
    },
    {
        "id": 22950,
        "title": "Convolutional Neural Network Modulation Recognition by Incorporating Attention Mechanisms",
        "authors": "Mingze Zuo, Yifan Liu",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc59986.2023.10421616"
    },
    {
        "id": 22951,
        "title": "Transparent autoencoding of network packets with self-attention-based transformers",
        "authors": "",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcn58197.2023.10223390"
    },
    {
        "id": 22952,
        "title": "Long-Term Cardiovascular Effects of Medications for Attention-Deficit/Hyperactivity Disorder—Balancing Benefits and Risks of Treatment",
        "authors": "Samuele Cortese, Cristiano Fava",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamapsychiatry.2023.4126"
    },
    {
        "id": 22953,
        "title": "Learning Attention from Attention:  Efficient Self-Refinement Transformer for Face Super-Resolution",
        "authors": "Guanxin Li, Jingang Shi, Yuan Zong, Fei Wang, Tian Wang, Yihong Gong",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Recently, Transformer-based architecture has been introduced into face super-resolution task due to its advantage in capturing long-range dependencies. However, these approaches tend to integrate global information in a large searching region, which neglect to focus on the most relevant information and induce blurry effect by the irrelevant textures. Some improved methods simply constrain self-attention in a local window to suppress the useless information. But it also limits the capability of recovering high-frequency details when flat areas dominate the local searching window. To improve the above issues, we propose a novel self-refinement mechanism which could adaptively achieve texture-aware reconstruction in a coarse-to-fine procedure. Generally, the primary self-attention is first conducted to reconstruct the coarse-grained textures and detect the fine-grained regions required further compensation. Then, region selection attention is performed to refine the textures on these key regions. Since self-attention considers the channel information on tokens equally, we employ a dual-branch feature integration module to privilege the important channels in feature extraction. Furthermore, we design the wavelet fusion module which integrate shallow-layer structure and deep-layer detailed feature to recover realistic face images in frequency domain. Extensive experiments demonstrate the effectiveness on a variety of datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/115"
    },
    {
        "id": 22954,
        "title": "On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions",
        "authors": "Santiago Cuervo, Ricard Marxer",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1476"
    },
    {
        "id": 22955,
        "title": "Supplemental Material for Shining Our Humanity: The Benefits of Awe on Self-Humanity",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1037/emo0001293.supp"
    },
    {
        "id": 22956,
        "title": "Self-Attention for Visual Reinforcement Learning",
        "authors": "Zachary Fernandes, Ethan Joseph, Dean Vogel, Mei Si",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333243"
    },
    {
        "id": 22957,
        "title": "When Medical Imaging Met Self-Attention: A Love Story That Didn’t Quite Work out",
        "authors": "Tristan Piater, Niklas Penzel, Gideon Stein, Joachim Denzler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382600003660"
    },
    {
        "id": 22958,
        "title": "MRSNet: Joint consistent optic disc and cup segmentation based on large kernel residual convolutional attention and self-attention",
        "authors": "Shiliang Yan, Xiaoqin Pan, Yinling Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dsp.2023.104308"
    },
    {
        "id": 22959,
        "title": "Pulmonary Rehabilitation: Mechanisms of Functional Loss and Benefits of Exercise",
        "authors": "Linda Nici",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4187/respcare.11705"
    },
    {
        "id": 22960,
        "title": "Neural Mechanisms and Benefits of Flow: A Meta Analysis",
        "authors": "Austin Yan",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "Known as “in the groove” by musicians and “in the zone” for athletes, flow is a sensation of total concentration on a single task. Since the introduction of flow theory by Mihaly Csikszentmihalyi in the 1970s, researchers have continued to explore the brain science and performance benefits of such state. The three hypotheses at the forefront of flow theory include brain waves, transient hypofrontality hypothesis, and synchronization theory of flow. Beyond detailing research that supports the three hypotheses, this meta analysis works to communicate the benefits of flow and also techniques to achieve the state. Flow state can be broken down into nine main characteristics, and working to improve each individual feature can contribute to optimizing the overall flow experience. The real world implications of flow state are immeasurable, allowing humans to be more efficient learners and better creative thinkers. Most importantly, flow is not an esoteric phenomenon exclusive to people at the top of their profession. Everyone can achieve flow state, which makes the benefits tangible and realistic for all. Flow theory continues to be a heavily researched subject in psychology, and a greater public interest in the topic can further expand the field of flow science.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22158/jpbr.v6n1p1"
    },
    {
        "id": 22961,
        "title": "Gut Probiotics and Health of Dogs and Cats: Benefits, Applications, and Underlying Mechanisms",
        "authors": "Qing Yang, Zhenlong Wu",
        "published": "2023-9-29",
        "citations": 2,
        "abstract": "Pets (mostly domestic dogs and cats) play an important role in the daily lives of humans and their health has attracted growing attention from pet owners. The intestinal microbiota, a complex microbial community with barrier-protective, nutritional, metabolic, and immunological functions, is integral to host health. Dysbiosis has been related to a variety of diseases in humans and animals. Probiotics have been used in functional foods and dietary supplements to modulate intestinal microbiota and promote host health, which has been introduced in pet dogs and cats in recent years. Various canine- and feline-derived probiotic strains have been isolated and characterized. The administration of probiotics has shown positive effects on the gut health and can alleviate some intestinal diseases and disorders in dogs and cats, although the underlying mechanisms are largely unresolved. In this review, we summarize the current knowledge on the benefits of probiotics and discuss their possible mechanisms in dogs and cats in order to provide new insights for the further development and application of probiotics in pets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/microorganisms11102452"
    },
    {
        "id": 22962,
        "title": "Paying attention to cyber-attacks: A multi-layer perceptron with self-attention mechanism",
        "authors": "Fernando J. Rendón-Segador, Juan A. Álvarez-García, Angel Jesús Varela-Vaca",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cose.2023.103318"
    },
    {
        "id": 22963,
        "title": "Fusion of label attention and mask attention for unsupervised and self-supervised anomaly detection",
        "authors": "Fengqian Pang, Chunyue Lei, Jingsheng Zeng",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Abstract\nMany uncontrollable factors during industrial flow lead to products’ unforeseen defects. Anomaly detection for the defects is inclined to use unsupervised frameworks to formulate one-class classification tasks, as well as segmentation tasks. The attention mechanism has proved to be effective in defect classification and segmentation. However, previous works with attention mechanisms have not investigated the combination of the two kinds of attention for the above tasks in depth. In this paper, we propose a framework that fuses Label Attention and Mask Attention (LAMA). Specifically, the LAMA is implemented by a loss prediction module and a decoder in a transform-based segmentation model. Moreover, a network based on normalizing flow is introduced as the architecture of anomaly detection, which integrates with the LAMA. The experimental results on MVTecAD demonstrate that the proposed method outperforms the contrast algorithms. The proposed framework achieves an overall image-wise AUC of 99.6% and 98.6% on pixel-wise AUC.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2638/1/012005"
    },
    {
        "id": 22964,
        "title": "A Stock Prediction Model Based on CNN-BiLSTM and Multiple Attention Mechanisms",
        "authors": "Guojie Zhao, Pengwei Yuan",
        "published": "2023-7-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaml60083.2023.00049"
    },
    {
        "id": 22965,
        "title": "Attention-Based Mechanisms for Cognitive Reinforcement Learning",
        "authors": "Yue Gao, Di Li, Xiangjian Chen, Junwu Zhu",
        "published": "2023-6-21",
        "citations": 1,
        "abstract": "In this paper, we propose a cognitive reinforcement learning method based on an attention mechanism (CRL-CBAM) to address the problems of complex interactive communication, limited range, and time-varying communication topology in multi-intelligence collaborative work. The method not only combines the efficient decision-making capability of reinforcement learning, the representational capability of deep learning, and the self-learning capability of cognitive learning but also inserts a convolutional block attention module to increase the representational capability by using the attention mechanism to focus on important features and suppress unnecessary ones. The use of two modules, channel and spatial axis, to emphasize meaningful features in the two main dimensions can effectively aid the flow of information in the network. Results from simulation experiments show that the method has more rewards and is more efficient than other methods in formation control, which means a greater advantage when dealing with scenarios with a large number of agents. In group containment, the agents learn to sacrifice individual rewards to maximize group rewards. All tasks are successfully completed, even if the simulation scenario changes from the training scenario. The method can therefore be applied to new environments with effectiveness and robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13137361"
    },
    {
        "id": 22966,
        "title": "Short-term photovoltaic power forecasting with feature extraction and attention mechanisms",
        "authors": "Wencheng Liu, Zhizhong Mao",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.renene.2024.120437"
    },
    {
        "id": 22967,
        "title": "DeepLeaf-Attentive: Integrating Attention Mechanisms for Crop Leaf Disease Identification",
        "authors": "Tanvir Ahmed, Farzana Sharmin Mou",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441566"
    },
    {
        "id": 22968,
        "title": "Hierarchical Modeling for Fault-Controlled Karst Reservoirs Assisted by the Statistical Thresholding Method and Stochastic Attention Mechanisms",
        "authors": "M. Tian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202335065"
    },
    {
        "id": 22969,
        "title": "Beyond Variational Models and Self-Similarity in Super-Resolution: Unfolding Models and Multi-Head Attention",
        "authors": "Ivan Pereira-Sánchez, Eloi Sans, Julia Navarro, Joan Duran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012395400003660"
    },
    {
        "id": 22970,
        "title": "OPTIMIZING SOCIAL IMPACT: MULTI-MODEL SELF-ATTENTION MECHANISM",
        "authors": "",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets52088"
    },
    {
        "id": 22971,
        "title": "Wildfire Spread Prediction Using Attention Mechanisms in U-NET",
        "authors": "Kamen Shah, Maria Pantoja",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10252734"
    },
    {
        "id": 22972,
        "title": "Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization",
        "authors": "Ye-Rin Jeoung, Jeong-Hwan Choi, Ju-Seok Seong, Jehyun Kyung, Joon-Hyuk Chang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1404"
    },
    {
        "id": 22973,
        "title": "Software Development Efficiency Metrics Prediction Using Fed-Layered Self Attention Grid",
        "authors": "Purab Ojha",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce59016.2024.10444141"
    },
    {
        "id": 22974,
        "title": "Hybrid Dilated Convolution with Attention Mechanisms for Image Denoising",
        "authors": "Shengqin Bian, Xinyu He, Zhengguang Xu, Lixin Zhang",
        "published": "2023-9-6",
        "citations": 2,
        "abstract": "In the field of image denoising, convolutional neural networks (CNNs) have become increasingly popular due to their ability to learn effective feature representations from large amounts of data. In the field of image denoising, CNNs are widely used to improve performance. However, increasing network depth can weaken the influence of shallow layers on deep layers, especially for complex denoising tasks such as real denoising and blind denoising, where conventional networks fail to achieve high-quality results. To address this issue, this paper proposes a hybrid dilated convolution-based denoising network (AMDNet) that incorporates attention mechanisms. In specific, AMDNet consists of four modules: the sparse module (SM), the feature fusion module (FFM), the attention guidance module (AGM), and the image residual module (IRM). The SM employs hybrid dilated convolution to extract local features, while the FFM is used to integrate global and local features. The AGM accurately extracts noise information hidden in complex backgrounds. Finally, the IRM reconstructs images in a residual manner to obtain high-quality results after denoising. AMDNet has the following features: (1) The sparse mechanism in hybrid dilated convolution enables better extraction of local features, enhancing the network’s ability to capture noise information. (2) The feature fusion module, through long-range connections, fully integrates global and local features, improving the performance of the model; (3) the attention module is ingeniously designed to precisely extract features in complex backgrounds. The experimental results demonstrate that AMDNet achieves outstanding performance on three tasks (Gaussian noise, real noise, and blind denoising).",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12183770"
    },
    {
        "id": 22975,
        "title": "STCA-SNN: self-attention-based temporal-channel joint attention for spiking neural networks",
        "authors": "Xiyan Wu, Yong Song, Ya Zhou, Yurong Jiang, Yashuo Bai, Xinyi Li, Xin Yang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "Spiking Neural Networks (SNNs) have shown great promise in processing spatio-temporal information compared to Artificial Neural Networks (ANNs). However, there remains a performance gap between SNNs and ANNs, which impedes the practical application of SNNs. With intrinsic event-triggered property and temporal dynamics, SNNs have the potential to effectively extract spatio-temporal features from event streams. To leverage the temporal potential of SNNs, we propose a self-attention-based temporal-channel joint attention SNN (STCA-SNN) with end-to-end training, which infers attention weights along both temporal and channel dimensions concurrently. It models global temporal and channel information correlations with self-attention, enabling the network to learn ‘what’ and ‘when’ to attend simultaneously. Our experimental results show that STCA-SNNs achieve better performance on N-MNIST (99.67%), CIFAR10-DVS (81.6%), and N-Caltech 101 (80.88%) compared with the state-of-the-art SNNs. Meanwhile, our ablation study demonstrates that STCA-SNNs improve the accuracy of event stream classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1261543"
    },
    {
        "id": 22976,
        "title": "Retracted: Magnetic Tile Surface Defect Detection Methodology Based on Self-Attention and Self-Supervised Learning",
        "authors": "",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9890718"
    },
    {
        "id": 22977,
        "title": "Costs and benefits of voluntary attention in crows",
        "authors": "Linus Hahner, Andreas Nieder",
        "published": "2023-8",
        "citations": 1,
        "abstract": "\n            Behavioural signatures of voluntary, endogenous selective attention have been found in both mammals and birds, but the relationship between performance benefits at attended and costs at unattended locations remains unclear. We trained two carrion crows (\n            Corvus corone\n            ) on a Posner-like spatial cueing task with dissociated cue and target locations, using both highly predictive and neutral central cues to compare reaction time (RT) and detection accuracy for validly, invalidly and neutrally cued targets. We found robust RT effects of predictive cueing at varying stimulus-onset asynchronies (SOA) that resulted from both advantages at cued locations and costs at un-cued locations. Both crows showed cueing effects around 15–25 ms with an early onset at 100 ms SOA, comparable to macaques. Our results provide a direct assessment of costs and benefits of voluntary attention in a bird species. They show that crows are able to guide spatial attention using associative cues, and that the processing advantage at attended locations impairs performance at unattended locations.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1098/rsos.230517"
    },
    {
        "id": 22978,
        "title": "The Ability of Self-Report Methods to Accurately Diagnose Attention Deficit Hyperactivity Disorder: A Systematic Review",
        "authors": "Allyson G. Harrison, Melanie J. Edwards",
        "published": "2023-10",
        "citations": 4,
        "abstract": "Objective: To identify and analyze all studies validating rating scales or interview-based screeners commonly used to evaluate ADHD in adults. Method: A systematic literature search identified all studies providing diagnostic accuracy statistics, including sensitivity and specificity, supplemented by relevant articles or test manuals referenced in reviewed manuscripts. Results: Only 20 published studies or manuals provided data regarding sensitivity and specificity when tasked with differentiating those with and without ADHD. While all screening measures have excellent ability to correctly classify non-ADHD individuals (with negative predictive values exceeding 96%), false positive rates were high. At best, positive predictive values in clinical samples reached 61%, but most fell below 20%. Conclusion: Clinicians cannot rely on scales alone to diagnose ADHD and must undertake more rigorous evaluation of clients who screen positive. Furthermore, relevant classification statistics must be included in publications to help clinicians make statistically defensible decisions. Otherwise, clinicians risk inappropriately diagnosing ADHD. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10870547231177470"
    },
    {
        "id": 22979,
        "title": "Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation",
        "authors": "Reza Azad, Leon Niggemeier, Michael Hüttemann, Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Yury Velichko, Ulas Bagci, Dorit Merhof",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00132"
    },
    {
        "id": 22980,
        "title": "View self-attention network for 3D object recognition",
        "authors": "Lianggneg Yu, Jiangzhong Cao",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135399"
    },
    {
        "id": 22981,
        "title": "In-depth Recommendation Model Based on Self-Attention Factorization",
        "authors": "",
        "published": "2023-3-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3837/tiis.2023.03.003"
    },
    {
        "id": 22982,
        "title": "Self-Benefits, Fiscal Risk, and Political Support for the Social Security System",
        "authors": "Daiki Kishishita, Tomoko Matsumoto",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4433915"
    },
    {
        "id": 22983,
        "title": "Who Benefits from Remote Schooling? Self-Selection and Match Effects",
        "authors": "Jesse Bruhn, Christopher Campos, Eric Chyn",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4533255"
    },
    {
        "id": 22984,
        "title": "Mapping of attention mechanisms to a generalized Potts model",
        "authors": "Riccardo Rende, Federica Gerace, Alessandro Laio, Sebastian Goldt",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physrevresearch.6.023057"
    },
    {
        "id": 22985,
        "title": "Predicting the Continuous Spatiotemporal State of Ground Fire Based on the Expended LSTM Model with Self-Attention Mechanisms",
        "authors": "Xinyu Wang, Xinquan Wang, Mingxian Zhang, Chun Tang, Xingdong Li, Shufa Sun, Yangwei Wang, Dandan Li, Sanping Li",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "Fire spread prediction is a crucial technology for fighting forest fires. Most existing fire spread models focus on making predictions after a specific time, and their predicted performance decreases rapidly in continuous prediction due to error accumulation when using the recursive method. Given that fire spread is a dynamic spatiotemporal process, this study proposes an expanded neural network of long short-term memory based on self-attention (SA-EX-LSTM) to address this issue. The proposed model predicted the combustion image sequence based on wind characteristics. It had two detailed feature transfer paths, temporal memory flow and spatiotemporal memory flow, which assisted the model in learning complete historical fire features as well as possible. Furthermore, self-attention mechanisms were integrated into the model’s forgetting gates, enabling the model to select the important features associated with the increase in fire spread from massive historical fire features. Datasets for model training and testing were derived from nine experimental ground fires. Compared with the state-of-the-art spatiotemporal prediction models, SA-EX-LSTM consistently exhibited the highest predicted performance and stability throughout the continuous prediction process. The experimental results in this paper have the potential to positively impact the application of spatiotemporal prediction models and UAV-based methods in the field of fire spread prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fire6060237"
    },
    {
        "id": 22986,
        "title": "Enhancing mask detection performance based on YOLOv5 model optimization and attention mechanisms",
        "authors": "Guangyuan Yang",
        "published": "2024-3-25",
        "citations": 0,
        "abstract": "Due to the COVID-19 pandemic, there has been a significant increase in the usage of masks, leading to more complex scenarios for mask detection techniques. This paper focuses on optimizing the performance of mask detection using the You Only Look Once (YOLO) v5 model. In this study, the yolov5 target detection model was employed for training the mask dataset. Diverse model improvement techniques were explored to enhance the model's capability to capture crucial features and differentiate masks from the background in complex scenarios. Finally, the modified model was compared with the earlier original target detection model to identify the most considerable performance gain. The CSPDarknet design with the TensorFlow framework is utilized in this study, and the Attention Mechanism module is implemented through the Keras library.  The objective is to optimize the three feature layers between the backbone network and the neck by integrating multiple attention mechanisms. This will enable the model to more quickly and accurately capture important features when dealing with complex scenarios by adjusting the feature map weights. Additionally, in the feature pyramid network, shallow feature maps are fused with deeper feature maps in a certain order to determine the most efficient feature fusion method. Finally, this study identified the optimal combination of attention mechanism and feature fusion through ablation experiments. The results of the experiment demonstrate that the combination of SE block and shallow feature fusion (SE + FF2 model) can greatly enhance category confidence, leading to an improved model performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/50/20241160"
    },
    {
        "id": 22987,
        "title": "Classification of Sleep Stages based on Multiple Attention Mechanisms",
        "authors": "Mingxue Sheng, Honglin Wan, Qiuyue Li",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiecc60864.2023.10456701"
    },
    {
        "id": 22988,
        "title": "The Effectiveness of the Attention Guidance mechanisms in VR Museum",
        "authors": "Haowen Guo, Danhua Zhao, Liyang Zhang, Yihong Lin, Hao Xin",
        "published": "2023",
        "citations": 0,
        "abstract": "The COVID-19 pandemic has led to a shift towards digital exhibitions on virtual platforms and the increasing use of VR devices. In most exhibitions, audiences are free to explore and understand the content. However, in themed museums, this freedom may result in missing important exhibits.In this study, we examine the effectiveness of various attention guidance mechanisms and user satisfaction in virtual museums. We designed an immersive VR experiment in a simulated virtual museum environment with four mechanisms: no guide, arrows, characters, and characters that acknowledge the user and environment. There were two guiding directions (left and right) and three museum spacial compositions (60-degree angle, 90-degree angle, and 180-degree angle).Through mixed-method analysis, we found that all three attention guidance mechanisms were effective in guiding users in the three museum spacial compositions. Among them, the virtual arrow had the longest visibility time, and the acknowledging character had the highest user satisfaction.In conclusion, attention guidance mechanisms in VR museums significantly improve user attention to the target. The acknowledging character and arrow have the best guidance efficacy, and spacial composition also has a significant impact on attention effectiveness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54941/ahfe1003378"
    },
    {
        "id": 22989,
        "title": "SAU-Net: Monocular Depth Estimation Combining Multi-Scale Features and Attention Mechanisms",
        "authors": "Wei Zhao, Yunqing Song, Tingting Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3339152"
    },
    {
        "id": 22990,
        "title": "Improving Plant Disease Recognition Through Gradient-Based Few-shot Learning with Attention Mechanisms",
        "authors": "Gültekin IŞIK",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "This study investigates the use of few-shot learning algorithms to improve classification performance in situations where traditional deep learning methods fail due to a lack of training data. Specifically, we propose a few-shot learning approach using the Almost No Inner Loop (ANIL) algorithm and attention modules to classify tomato diseases in the Plant Village dataset. The attended features obtained from the five separate attention modules are classified using a Multi Layer Perceptron (MLP) classifier, and the soft voting method is used to weigh the classification scores from each classifier. The results demonstrate that our proposed approach achieves state-of-the-art accuracy rates of 97.05% and 97.66% for 10-shot and 20-shot classification, respectively. Our approach demonstrates the potential for incorporating attention mechanisms in feature extraction processes and suggests new avenues for research in few-shot learning methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.21597/jist.1283491"
    },
    {
        "id": 22991,
        "title": "Magnetic Resonance Image Reconstruction Based on Multi-Scale and Attention Mechanisms",
        "authors": "Zhipeng Lan, Li Gao, Jianfei Liu",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291707"
    },
    {
        "id": 22992,
        "title": "MonoVAN: Visual Attention for Self-Supervised Monocular Depth Estimation",
        "authors": "Ilia Indyk, Ilya Makarov",
        "published": "2023-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismar59233.2023.00138"
    },
    {
        "id": 22993,
        "title": "Learning precise feature via self-attention and self-cooperation YOLOX for smoke detection",
        "authors": "Jingjing Wang, Xinman Zhang, Kunlei Jing, Cong Zhang",
        "published": "2023-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120330"
    },
    {
        "id": 22994,
        "title": "Self- and cross-attention accurately predicts metabolite–protein interactions",
        "authors": "Pedro Alonso Campana, Zoran Nikoloski",
        "published": "2023-1-10",
        "citations": 3,
        "abstract": "AbstractMetabolites regulate activity of proteins and thereby affect cellular processes in all organisms. Despite extensive efforts to catalogue the metabolite–protein interactome in different organisms by employing experimental and computational approaches, the coverage of such interactions remains fragmented, particularly for eukaryotes. Here, we make use of two most comprehensive collections, BioSnap and STITCH, of metabolite–protein interactions from seven eukaryotes as gold standards to train a deep learning model that relies on self- and cross-attention over protein sequences. This innovative protein-centric approach results in interaction-specific features derived from protein sequence alone. In addition, we designed and assessed a first double-blind evaluation protocol for metabolite–protein interactions, demonstrating the generalizability of the model. Our results indicated that the excellent performance of the proposed model over simpler alternatives and randomized baselines is due to the local and global features generated by the attention mechanisms. As a results, the predictions from the deep learning model provide a valuable resource for studying metabolite–protein interactions in eukaryotes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/nargab/lqad008"
    },
    {
        "id": 22995,
        "title": "Self-supervised category selective attention classifier network for diabetic macular edema classification",
        "authors": "Sachin Chavan, Nitin Choubey",
        "published": "2024-3-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00592-024-02257-6"
    },
    {
        "id": 22996,
        "title": "Shyness and academic procrastination among Chinese adolescents: a moderated mediation model of self-regulation and self-focused attention",
        "authors": "Hong Sun, Yang Yu, Chao Peng",
        "published": "2024-3-21",
        "citations": 0,
        "abstract": "Academic procrastination is a common concern among adolescents, but the correlation between shyness and academic procrastination and the internal mechanisms have not yet been thoroughly investigated. Based on a questionnaire survey with 1,279 Chinese middle school students, this study examined the effect of shyness on academic procrastination and its underlying mechanism of self-regulation and self-focused attention. Results revealed that: (1) shyness significantly predicted academic procrastination. (2) Self-regulation mediated the relationship between shyness and academic procrastination. (3) Self-focused attention played a moderating role in the first half of this mediation process. Specifically, higher level of self-focused attention strengthened the predictive effect of shyness on self-regulation. These results underscored the latent risks and protective factors associated with shyness, self-regulation, and self-focused attention in adolescent academic procrastination. In future research and interventions, attention may be directed towards improving individual internal factors to assist adolescents in effectively addressing issues related to academic procrastination.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpsyg.2024.1352342"
    },
    {
        "id": 22997,
        "title": "Self-attention based ResNet model for Cervical Cancer Detection",
        "authors": "Tania Ganguly, Rimjhim Padam Singh, Priyanka Kumar",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ici60088.2023.10421309"
    },
    {
        "id": 22998,
        "title": "An Intention Inference Method for BiGRU Integrating Multi-head Self-Attention in Share Control",
        "authors": "Wenshan Zhao, Hua Wang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451784"
    },
    {
        "id": 22999,
        "title": "Meditation as an Adjunctive Practice in Cardiology: Review of Benefits and Mechanisms",
        "authors": "Nitin Sethi, Niharika Sethi",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ihj.2023.11.161"
    },
    {
        "id": 23000,
        "title": "Health Benefits of Tea Polyphenols and Mechanisms via Regulating Gut Microbiota",
        "authors": "Xinyue Xu",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Tea has been a beloved beverage for centuries, enjoyed by people all over the world. Tea polyphenols, natural bioactive compounds abundant in tea, have gained increasing attention for their potential health-promoting properties. This study provides a comprehensive overview of the multifaceted interactions between tea polyphenols and gut microbiota, shedding light on their roles in health and disease prevention. The opening section provides an in-depth examination of the definition and categorization of tea polyphenols, and their prevalence in various types of tea. Subsequently, the following section conducts an extensive analysis of the crucial role played by gut microbiota and the ways in which tea polyphenols modulate their composition, including their potential as prebiotic agents. In addition, this paper discusses the diverse health-promoting aspects of tea polyphenols, such as their antioxidative properties, anti-inflammatory effects, and their potential impact on conditions like inflammatory bowel diseases (IBD) and colorectal cancer, furthermore, explores their role in addressing functional disorders like depression and combating fatigue. This paper highlights the intricate interplay between tea polyphenols and gut microbiota and underscores the potential significance of these interactions in promoting health, advancing the understanding of the role of tea polyphenols in wellness and disease prevention.",
        "keywords": "",
        "link": "http://dx.doi.org/10.62051/qv11as62"
    },
    {
        "id": 23001,
        "title": "Image classification model based on large kernel attention mechanism and relative position self-attention mechanism",
        "authors": "Siqi Liu, Jiangshu Wei, Gang Liu, Bei Zhou",
        "published": "2023-4-21",
        "citations": 1,
        "abstract": "The Transformer has achieved great success in many computer vision tasks. With the in-depth exploration of it, researchers have found that Transformers can better obtain long-range features than convolutional neural networks (CNN). However, there will be a deterioration of local feature details when the Transformer extracts local features. Although CNN is adept at capturing the local feature details, it cannot easily obtain the global representation of features. In order to solve the above problems effectively, this paper proposes a hybrid model consisting of CNN and Transformer inspired by Visual Attention Net (VAN) and CoAtNet. This model optimizes its shortcomings in the difficulty of capturing the global representation of features by introducing Large Kernel Attention (LKA) in CNN while using the Transformer blocks with relative position self-attention variant to alleviate the problem of detail deterioration in local features of the Transformer. Our model effectively combines the advantages of the above two structures to obtain the details of local features more accurately and capture the relationship between features far apart more efficiently on a large receptive field. Our experiments show that in the image classification task without additional training data, the proposed model in this paper can achieve excellent results on the cifar10 dataset, the cifar100 dataset, and the birds400 dataset (a public dataset on the Kaggle platform) with fewer model parameters. Among them, SE_LKACAT achieved a Top-1 accuracy of 98.01% on the cifar10 dataset with only 7.5M parameters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1344"
    },
    {
        "id": 23002,
        "title": "Pharmacodynamics, metabolomics and pathological studies on mechanisms of traditional benefits of Angelica sinensis in blood circulation",
        "authors": "",
        "published": "2023-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56042/ijtk.v22i1.53106"
    },
    {
        "id": 23003,
        "title": "The Roles of Attention Deficit Hyperactivity Disorder Symptoms, Coping Self-Efficacy, and Resilience in Satisfaction With College and Life",
        "authors": "Shelia Kennison",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3102/ip.23.2006550"
    },
    {
        "id": 23004,
        "title": "Multi-information Self-attention Autoencoder Sequential recommendation",
        "authors": "Nan Wang, Yang Liu",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456330"
    },
    {
        "id": 23005,
        "title": "Dynamic Self-Attention with Guided-Attention Network for Visual Question Answering using Capsules Dynamic Routing",
        "authors": "Doaa B. Ebaid, Magda M. Madbouly, Adel A. El-Zoghabi",
        "published": "2023-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3596947.3596951"
    },
    {
        "id": 23006,
        "title": "Who Benefits from Remote Schooling? Self-Selection and Match Effects",
        "authors": "Jesse Bruhn, Christopher Campos, Eric T. Chyn",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4515426"
    },
    {
        "id": 23007,
        "title": "Object detection algorithm based on channel-spatial-skip connection attention mechanisms",
        "authors": "Z. Wu, J. Zheng, L. Kong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.3312"
    },
    {
        "id": 23008,
        "title": "Rethinking Attention Mechanisms in Vision Transformers with Graph Structures",
        "authors": "Hyeongjin Kim, Byoung Chul Ko",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "In this paper, we propose a new type of vision transformer (ViT) based on graph head attention (GHA). Because the multi-head attention (MHA) of a pure ViT requires multiple parameters and tends to lose the locality of an image, we replaced MHA with GHA by applying a graph to the attention head of the transformer. Consequently, the proposed GHA maintains both the locality and globality of the input patches and guarantees the diversity of the attention. The proposed GHA-ViT commonly outperforms pure ViT-based models using small-sized CIFAR-10/100, MNIST, and MNIST-F datasets and a medium-sized ImageNet-1K dataset in scratch training. A Top-1 accuracy of 81.7% was achieved for ImageNet-1K using GHA-B, which is a base model with approximately 29 M parameters. In addition, with CIFAR-10/100, the existing ViT and parameters are reduced 17-fold and the performance increased by 0.4/4.3%, respectively. The proposed GHA-ViT shows promising results in terms of the number of parameters and operations and the level of accuracy in comparison with other state-of-the-art ViT-lightweight models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s24041111"
    },
    {
        "id": 23009,
        "title": "A self-supervised dual-channel self-attention acoustic encoder for underwater acoustic target recognition",
        "authors": "Xingmei Wang, Peiran Wu, Boquan Li, Ge Zhan, Jinghan Liu, Zijian Liu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2024.117305"
    },
    {
        "id": 23010,
        "title": "Polycystic Ovarian Syndrome Identification Through Self-Attention Guided Convolutional Neural Network",
        "authors": "Shamik Tiwari, Piyush Maheshwari",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acit58888.2023.10453748"
    },
    {
        "id": 23011,
        "title": "Improving word mover’s distance by leveraging self-attention matrix",
        "authors": "Hiroaki Yamagiwa, Sho Yokoi, Hidetoshi Shimodaira",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.746"
    },
    {
        "id": 23012,
        "title": "Self-Attention Based Generative Adversarial Networks For Unsupervised Video Summarization",
        "authors": "Maria Nektaria Minaidi, Charilaos Papaioannou, Alexandros Potamianos",
        "published": "2023-9-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289808"
    },
    {
        "id": 23013,
        "title": "Video Salient Object Detection Using Multi-Scale Self-Attention",
        "authors": "Jiahao Liu, Haoyuan Liu, Hiroshi Watanabe",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315584"
    },
    {
        "id": 23014,
        "title": "Lightweight Human Pose Estimation Based on Self-Attention Mechanism",
        "authors": "Youtao Luo, Xiaoming Gao",
        "published": "2023-3-21",
        "citations": 0,
        "abstract": "To tackle the issues of numerous parameters, high computational complexity, and extended detection time prevalent in current human pose estimation network models, we have incorporated an hourglass structure to create a lightweight single-path network model, which has fewer parameters and a shorter computation time. To ensure model accuracy, we have implemented a window self-attention mechanism with a reduced parameter count. Additionally, we have redesigned this self-attention module to effectively extract local and global information, thereby enriching the feature information learned by the model. This module merges with the inverted residual network architecture, creating a separate module of WGNet. Finally, WGNet can be flexibly embedded into different stages of the model. Training and validation on COCO and MPII datasets demonstrate that this model reduces the number of parameters by 25%, computational complexity by 41%, and inference time by nearly two times, compared to Hrformer, which also utilizes the windowed self-attention mechanism, at the cost of only 3.5% accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56028/aetr.4.1.253.2023"
    },
    {
        "id": 23015,
        "title": "From Classroom to Screen: Analyzing the Mechanisms Shaping E-Learning Benefits Amidst COVID-19",
        "authors": "Hyeon Jo",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13132-023-01614-0"
    },
    {
        "id": 23016,
        "title": "Impact of Accessibility and Network Attention Coupling Coordination on Operating Benefits of Scenic Spots",
        "authors": "Fan Xu",
        "published": "2023-5-21",
        "citations": 0,
        "abstract": "This paper takes the scenic spots in the outskirts of Beijing as the research area and uses a BP neural network model based on the perspective of coupling relationship to study the impact of accessibility, network attention, and their coupled coordination relationship on operational efficiency of scenic spots in the outskirts of Beijing between November 27, 2018 and January 20, 2020. The results show that the coupled coordination relationship between accessibility and network attention has a significant positive impact on operational efficiency. Under the situation of coupled coordination, there is a positive impact between accessibility and operational efficiency as well as between network attention and operational efficiency; however, under the situation of mismatching, there is a negative non-linear relationship between accessibility and operational efficiency, and between network attention and operational efficiency, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/ajmss.v2i3.8717"
    },
    {
        "id": 23017,
        "title": "Multi-Label Classification Method Based on Self-Attention Mechanism",
        "authors": "Jie Guo, Bing Fang",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceace60673.2023.10441996"
    },
    {
        "id": 23018,
        "title": "Expected Benefits of Crowdsourcing in Government: Identifying and Explaining some of the Mechanisms",
        "authors": "Cesar Renteria",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/eg.2023.10047236"
    },
    {
        "id": 23019,
        "title": "Memory benefits when actively, rather than passively, viewing images",
        "authors": "Briana L. Kennedy, Steven B. Most, Tijl Grootswagers, Vanessa K. Bowden",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13414-023-02814-1"
    },
    {
        "id": 23020,
        "title": "Fine-grained sequence-to-sequence lip reading based on self-attention and self-distillation",
        "authors": "Junxiao Xue, Shibo Huang, Huawei Song, Lei Shi",
        "published": "2023-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-2230-x"
    },
    {
        "id": 23021,
        "title": "The Path to Translating Focus of Attention Research Into Canadian Physiotherapy, Part 1: Physiotherapists’ Self-Reported Focus of Attention Use Via a Study-Specific Questionnaire",
        "authors": "Julia Hussien, Diane Ste-Marie",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "The focus of attention literature has shown robust findings for the benefits of providing statements that focus on the movement effect or outcome (external focus of attention [EFOA]) as opposed to focusing on the movement kinematics (internal focus of attention). Observational studies, however, have revealed that physiotherapists use fewer EFOA statements than internal focus of attention statements in their practice. Most evidence in this regard has been from non-Canadian physiotherapists working in stroke rehabilitation; consequently, we sought to examine whether Canadian physiotherapists working with various rehabilitation populations also use EFOA statements to a lesser extent than internal focus of attention statements. The “Therapists’ Perceptions of Motor Learning Principles Questionnaire (TPMLPQ)” was thus designed and data from 121 Canadian physiotherapists showed low relative frequencies of EFOA use (31.3% ± 14%) averaged across six hypothetical scenarios. A higher EFOA was reported, however, for two of the six scenarios: a functional reaching scenario (55.5% ± 37.0%) and pelvic floor task (65.6% ±32.9%). This data suggest that the findings of EFOA benefits have not been widely translated into Canadian physiotherapy settings; furthermore, the findings of the scenario-dependency warrant future investigation into factors, such as task characteristics, that may influence physiotherapists’ FOA use.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1123/jmld.2022-0052"
    },
    {
        "id": 23022,
        "title": "Bearing remaining useful life prediction using self-adaptive graph convolutional networks with self-attention mechanism",
        "authors": "Yupeng Wei, Dazhong Wu, Janis Terpenny",
        "published": "2023-4",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ymssp.2022.110010"
    },
    {
        "id": 23023,
        "title": "An analysis of attention mechanisms and its variance in transformer",
        "authors": "Yuzhong Chen, Hongren Pu, Yang Qu",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Transformer is a machine learning model based on attention mechanism, which is widely used. When the Transformer model was first proposed, it gradually developed many variants and was promoted and applied in many fields, becoming an important research part in the areas of deep learning. However, the critical attention mechanism of Transformers has issues such as square complexity that affect computational speed and data processing efficiency. In order to meet the needs of data processing and related computing, there have been endless efforts to improve the attention mechanism in Transformers in different work areas. This article mainly provides a brief overview of the recent research progress on the attention mechanism in Transformers. Select representative studies from several directions of attention improvement work to introduce, in order to explore the latest research trends in its improvement work and lay a foundation for pointing out potential research directions for future research work and further improving the performance of Transformers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241291"
    },
    {
        "id": 23024,
        "title": "Harmonizing Dynamic Frequency Analysis with Attention Mechanisms for Efficient Facial Image Authenticity Detection",
        "authors": "Yulai Zhao, Jianhua Li, Ling Wang",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csat61646.2023.00097"
    },
    {
        "id": 23025,
        "title": "Prediction of the Deformation of Heritage Building Communities under the Integration of Attention Mechanisms and SBAS Technology",
        "authors": "Chong Ma, Baoli Lu",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "The protection of heritage building communities is of important historical significance, the occurrence of a landslide is related to the safety and stability of the heritage building, and ground monitoring and forecasting are the key steps for the early warning and timely restoration of the heritage building. This study utilizes remote sensing technology to monitor the ground of a cultural heritage building, and employs a Long Short-Term Memory (LSTM) network for prediction. Firstly, we conducted ground subsidence monitoring within a specific time series of the study area using heritage remote sensing images and SBAS-InSAR technology. Following the subsidence monitoring, and incorporating an attention mechanism, we effectively localized and extracted features of heritage building clusters within the region. This approach efficiently addresses the challenge of feature identification resulting from the dense distribution of buildings and the similarity between various objects. The results indicate that the maximum subsidence rate in the research area reached −60 mm/year, reached a maximum uplift rate of 45 mm/year, and that the maximum cumulative subsidence reached −65 mm. Secondly, for the multi-level, multi-scale, and class-specific objects in remote sensing images, the LSTM network enables adaptive contextual information during deep and shallow feature extraction. This allows for better contextual modeling and the correlation between predicted and actual results reaches a 0.95 correlation, demonstrating the accurate predictive performance of the LSTM network. In conclusion, both LSTM and SBAS technologies play a crucial role in decision-making for heritage buildings, facilitating effective early warning and disaster mitigation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12234724"
    },
    {
        "id": 23026,
        "title": "Novel Hybrid Model Coupling WOA with BiLSTM Neural Networks and Attention Mechanisms",
        "authors": "Liuyu Wang, Huatai Pan, Zhengxian Cai",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10258008"
    },
    {
        "id": 23027,
        "title": "An Integrated Time Series Prediction Model Based on Empirical Mode Decomposition and Two Attention Mechanisms",
        "authors": "Xianchang Wang, Siyu Dong, Rui Zhang",
        "published": "2023-11-11",
        "citations": 1,
        "abstract": "In the prediction of time series, Empirical Mode Decomposition (EMD) generates subsequences and separates short-term tendencies from long-term ones. However, a single prediction model, including attention mechanism, has varying effects on each subsequence. To accurately capture the regularities of subsequences using an attention mechanism, we propose an integrated model for time series prediction based on signal decomposition and two attention mechanisms. This model combines the results of three networks—LSTM, LSTM-self-attention, and LSTM-temporal attention—all trained using subsequences obtained from EMD. Additionally, since previous research on EMD has been limited to single series analysis, this paper includes multiple series by employing two data pre-processing methods: ‘overall normalization’ and ‘respective normalization’. Experimental results on various datasets demonstrate that compared to models without attention mechanisms, temporal attention improves the prediction accuracy of short- and medium-term decomposed series by 15~28% and 45~72%, respectively; furthermore, it reduces the overall prediction error by 10~17%. The integrated model with temporal attention achieves a reduction in error of approximately 0.3%, primarily when compared to models utilizing only general forms of attention mechanisms. Moreover, after normalizing multiple series separately, the predictive performance is equivalent to that achieved for individual series.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14110610"
    },
    {
        "id": 23028,
        "title": "Breast Cancer Diagnosis in Thermography Using Pre-Trained VGG16 with Deep Attention Mechanisms",
        "authors": "Alia Alshehri, Duaa AlSaeed",
        "published": "2023-2-23",
        "citations": 2,
        "abstract": "One of the most prevalent cancers in women is breast cancer. The mortality rate related to this disease can be decreased by early, accurate diagnosis to increase the chance of survival. Infrared thermal imaging is one of the breast imaging modalities in which the temperature of the breast tissue is measured using a screening tool. The previous studies did not use pre-trained deep learning (DL) with deep attention mechanisms (AMs) on thermographic images for breast cancer diagnosis. Using thermal images from the Database for Research Mastology with Infrared Image (DMR-IR), the study investigates the use of a pre-trained Visual Geometry Group with 16 layers (VGG16) with AMs that can produce good diagnosis performance utilizing the thermal images of breast cancer. The symmetry of the three models resulting from the combination of VGG16 with three types of AMs is evident in all its stages in methodology. The models were compared to state-of-art breast cancer diagnosis approaches and tested for accuracy, sensitivity, specificity, precision, F1-score, AUC score, and Cohen’s kappa. The test accuracy rates for the AMs using the VGG16 model on the breast thermal dataset were encouraging, at 99.80%, 99.49%, and 99.32%. Test accuracy for VGG16 without AMs was 99.18%, whereas test accuracy for VGG16 with AMs improved by 0.62%. The proposed approaches also performed better than previous approaches examined in the related studies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15030582"
    },
    {
        "id": 23029,
        "title": "Editorial: Cognitive mechanisms of visual attention, working memory, emotion, and their interactions",
        "authors": "Qianru Xu, Qiang Liu, Chaoxiong Ye",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1259002"
    },
    {
        "id": 23030,
        "title": "A multi-scale de-hazing network combining attention mechanisms",
        "authors": "Qiannan Li, Yongfeng Qi",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3026593"
    },
    {
        "id": 23031,
        "title": "Lightweight graph convolutional networks combining multimodal fusion and spatio-temporal attention mechanisms",
        "authors": "Biao Guo, Jinyue Li, Xuetao Wang",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cipae60493.2023.00147"
    },
    {
        "id": 23032,
        "title": "Fluctuations of Attention During Self-Paced Naturalistic Goal-Directed Behavior in Attention-Deficit/Hyperactivity Disorder",
        "authors": "Juha Salmi, Liya Merzon, Tilda Eräste, Erik Seesjärvi, Hanna Huhdanpää, Eeva T. Aronen, Minna Mannerkoski, W. Joseph MacInnes, Matti Laine",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jaacop.2023.12.002"
    },
    {
        "id": 23033,
        "title": "Human Action Recognition Based on Residual Networks with Improved Attention Mechanisms",
        "authors": "Tao Li, Hao Zhu, Xuemei Sun, Lulu He",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291257"
    },
    {
        "id": 23034,
        "title": "Optimizing Medical Image Report Generation with Varied Attention Mechanisms",
        "authors": "Pochamreddy Mukesh Reddy, Vivek Kumar Verma, Mudunuri Venkata Chaitanya Varma",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic3i59117.2023.10398149"
    },
    {
        "id": 23035,
        "title": "Prediction of Material Properties of Inorganic Compounds Using Self-Attention Network",
        "authors": "Kyohei Noda, Hisanao Takahashi, Koji Tsuda, Masahito Hiroshima",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1527/tjsai.38-2_e-m93"
    },
    {
        "id": 23036,
        "title": "Self-attention Guidance Based Crowd Localization and Counting",
        "authors": "Zhouzhou Ma, Guanghua Gu, Wenrui Zhao",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11633-023-1428-6"
    },
    {
        "id": 23037,
        "title": "Self-Enhanced Attention for Image Captioning",
        "authors": "Qingyu Sun, Juan Zhang, Zhijun Fang, Yongbin Gao",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "AbstractImage captioning, which involves automatically generating textual descriptions based on the content of images, has garnered increasing attention from researchers. Recently, Transformers have emerged as the preferred choice for the language model in image captioning models. Transformers leverage self-attention mechanisms to address gradient accumulation issues and eliminate the risk of gradient explosion commonly associated with RNN networks. However, a challenge arises when the input features of the self-attention mechanism belong to different categories, as it may result in ineffective highlighting of important features. To address this issue, our paper proposes a novel attention mechanism called Self-Enhanced Attention (SEA), which replaces the self-attention mechanism in the decoder part of the Transformer model. In our proposed SEA, after generating the attention weight matrix, it further adjusts the matrix based on its own distribution to effectively highlight important features. To evaluate the effectiveness of SEA, we conducted experiments on the COCO dataset, comparing the results with different visual models and training strategies. The experimental results demonstrate that when using SEA, the CIDEr score is significantly higher compared to the scores obtained without using SEA. This indicates the successful addressing of the challenge of effectively highlighting important features with our proposed mechanism.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11527-x"
    },
    {
        "id": 23038,
        "title": "Pest Identification Based on Fusion of Self-Attention With ResNet",
        "authors": "Sk Mahmudul Hassan, Arnab Kumar Maji",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3351003"
    },
    {
        "id": 23039,
        "title": "Encoding and optimizing structured review for recommendation system using deeper self attention",
        "authors": "Devi Kannan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0175604"
    },
    {
        "id": 23040,
        "title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds",
        "authors": "Hejing Zhang, Jian Guan, Qiaoxi Zhu, Feiyang Xiao, Youde Liu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2416"
    },
    {
        "id": 23041,
        "title": "Exploration of Language-Specific Self-Attention Parameters for Multilingual End-to-End Speech Recognition",
        "authors": "Brady Houston, Katrin Kirchhoff",
        "published": "2023-1-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022937"
    },
    {
        "id": 23042,
        "title": "Deep Multi-Object Symbol Learning with Self-Attention Based Predictors",
        "authors": "Alper Ahmetoğlu, Erhan Öztop, Emre Uğur",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223865"
    },
    {
        "id": 23043,
        "title": "Self-supervised transformers predict dynamics of object-based attention in humans",
        "authors": "Hossein Adeli, Seoyoung Ahn, Nikolaus Kriegeskorte, Gregory Zelinsky",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1703-0"
    },
    {
        "id": 23044,
        "title": "Remaining useful life estimation via Cascaded Self-Attention and ResNet models",
        "authors": "Adem AVCI, Nurettin ACIR",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "Prognostics and Health Management occupies an important place in modern industrial maintenance to increase the reliability of systems. Maintenance of critical parts in the system is vital for successful prognostics and health management. For this reason, the determining remaining useful life of the parts should be accurate. This study proposes a data-based remaining useful life prediction method with a network consisting of a cascade-connected Self-Attention and Residual Network layer. The network is fed by multiple sensor signals to monitor the aero-engines. The proposed model contains four main parts Gaussian Noise layer, Self-Attention layer, Residual Network layer, and remaining useful life estimation. The Gaussian Noise layer deals with the noisy input data for a more robust predictor. The Self-Attention layer focuses on the crucial points through time. The Residual Network layer uses feature extraction and makes the model more profound help of the skip connection. Finally, the remaining useful life estimation is made with the highly correlated features obtained from the fully connected layer and the output layer. In addition, a new loss function has been offered in accordance with the evaluation metrics in the literature. With the proposed model and loss function, 11,017 and 12,629 in root mean square error, 157.19 and 218.6 in score function were obtained in the FD001 and FD003, respectively. The superior performance of these results on the C-MAPSS dataset is demonstrated by comparing the other state-of-the-art methods in the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.38088/jise.1206920"
    },
    {
        "id": 23045,
        "title": "Targeted self-supervised attention network for coronavirus disease 2019 detection",
        "authors": "Weixin Ding, Wenbo Yu",
        "published": "2023-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/1.jei.32.3.033008"
    },
    {
        "id": 23046,
        "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech",
        "authors": "Hyungchan Yoon, Changhwan Kim, Eunwoo Song, Hyun-Wook Yoon, Hong-Goo Kang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1301"
    },
    {
        "id": 23047,
        "title": "Self-Attention Multi-Scale Pyramid Stereo Matching Network",
        "authors": "Yebao Qin, Wei Sun, Manqian Hu, Shimeng Fan, Xing Zhang, Jian Liu",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450732"
    },
    {
        "id": 23048,
        "title": "Ignoring Stimuli Related to Self Involves not Only Attention Inhibition but Also Self-Control",
        "authors": "Jarosław Orzechowski, Aleksandra Gruszka, Michał Nowak, Natalia Wójcik, Radosław Wujcik, Edward Nęcka",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5709/acp-0411-8"
    },
    {
        "id": 23049,
        "title": "Cross-modal self-attention mechanism for controlling robot volleyball motion",
        "authors": "Meifang Wang, Zhange Liang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "IntroductionThe emergence of cross-modal perception and deep learning technologies has had a profound impact on modern robotics. This study focuses on the application of these technologies in the field of robot control, specifically in the context of volleyball tasks. The primary objective is to achieve precise control of robots in volleyball tasks by effectively integrating information from different sensors using a cross-modal self-attention mechanism.MethodsOur approach involves the utilization of a cross-modal self-attention mechanism to integrate information from various sensors, providing robots with a more comprehensive scene perception in volleyball scenarios. To enhance the diversity and practicality of robot training, we employ Generative Adversarial Networks (GANs) to synthesize realistic volleyball scenarios. Furthermore, we leverage transfer learning to incorporate knowledge from other sports datasets, enriching the process of skill acquisition for robots.ResultsTo validate the feasibility of our approach, we conducted experiments where we simulated robot volleyball scenarios using multiple volleyball-related datasets. We measured various quantitative metrics, including accuracy, recall, precision, and F1 score. The experimental results indicate a significant enhancement in the performance of our approach in robot volleyball tasks.DiscussionThe outcomes of this study offer valuable insights into the application of multi-modal perception and deep learning in the field of sports robotics. By effectively integrating information from different sensors and incorporating synthetic data through GANs and transfer learning, our approach demonstrates improved robot performance in volleyball tasks. These findings not only advance the field of robotics but also open up new possibilities for human-robot collaboration in sports and athletic performance improvement. This research paves the way for further exploration of advanced technologies in sports robotics, benefiting both the scientific community and athletes seeking performance enhancement through robotic assistance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnbot.2023.1288463"
    },
    {
        "id": 23050,
        "title": "Building Area Detection in SAR Imagery Based on U2Net Architecture and Self-Attention Mechanism",
        "authors": "Feifei Dong, Chisheng Wang",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigsardata59007.2023.10294782"
    },
    {
        "id": 23051,
        "title": "Emotion Recognition from EEG: Self-Attention and Differentiable Pooling Improve SOGNN Performance",
        "authors": "Sahil Katyal, Ramakrishnan Angarai Ganesan",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440863"
    },
    {
        "id": 23052,
        "title": "SA-Pmnet: Utilizing Close-Range Photogrammetry Combined with Image Enhancement and Self-Attention Mechanisms for 3D Reconstruction of Forests",
        "authors": "Xuanhao Yan, Guoqi Chai, Xinyi Han, Lingting Lei, Geng Wang, Xiang Jia, Xiaoli Zhang",
        "published": "2024-1-21",
        "citations": 0,
        "abstract": "Efficient and precise forest surveys are crucial for in-depth understanding of the present state of forest resources and conducting scientific forest management. Close-range photogrammetry (CRP) technology enables the convenient and fast collection of highly overlapping sequential images, facilitating the reconstruction of 3D models of forest scenes, which significantly improves the efficiency of forest surveys and holds great potential for forestry visualization management. However, in practical forestry applications, CRP technology still presents challenges, such as low image quality and low reconstruction rates when dealing with complex undergrowth vegetation or forest terrain scenes. In this study, we utilized an iPad Pro device equipped with high-resolution cameras to collect sequential images of four plots in Gaofeng Forest Farm in Guangxi and Genhe Nature Reserve in Inner Mongolia, China. First, we compared the image enhancement effects of two algorithms: histogram equalization (HE) and median–Gaussian filtering (MG). Then, we proposed a deep learning network model called SA-Pmnet based on self-attention mechanisms for 3D reconstruction of forest scenes. The performance of the SA-Pmnet model was compared with that of the traditional SfM+MVS algorithm and the Patchmatchnet network model. The results show that histogram equalization significantly increases the number of matched feature points in the images and improves the uneven distribution of lighting. The deep learning networks demonstrate better performance in complex environmental forest scenes. The SA-Pmnet network, which employs self-attention mechanisms, improves the 3D reconstruction rate in the four plots to 94%, 92%, 94%, and 96% by capturing more details and achieves higher extraction accuracy of diameter at breast height (DBH) with values of 91.8%, 94.1%, 94.7%, and 91.2% respectively. These findings demonstrate the potential of combining of the image enhancement algorithm with deep learning models based on self-attention mechanisms for 3D reconstruction of forests, providing effective support for forest resource surveys and visualization management.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs16020416"
    },
    {
        "id": 23053,
        "title": "Local Estimation vs Global Information: the Benefits of Slower Timescales",
        "authors": "Payam Zahadat",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acsos-c58168.2023.00030"
    },
    {
        "id": 23054,
        "title": "An Entity Relation Extraction Algorithm Incorporating Multi-Attention Mechanisms and Remote Supervision",
        "authors": "Zhang Cui, Zhou Maojie",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3635175.3635181"
    },
    {
        "id": 23055,
        "title": "Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding",
        "authors": "Sanjana Sankar, Denis Beautemps, Frédéric Elisei, Olivier Perrotin, Thomas Hueber",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1669"
    },
    {
        "id": 23056,
        "title": "Benefits and Challenges of Self-Service Business Intelligence Implementation",
        "authors": "Marcin Pałys, Andrzej Pałys",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.10.066"
    },
    {
        "id": 23057,
        "title": "Recognition of Escherichia Coli Promoters Based on Attention Mechanisms",
        "authors": "Dan Li, Yanchun Yuan, Yuhan Li",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3638569.3638572"
    },
    {
        "id": 23058,
        "title": "Self-attention in vision transformers performs perceptual grouping, not attention",
        "authors": "Paria Mehrani, John K. Tsotsos",
        "published": "2023-6-29",
        "citations": 3,
        "abstract": "Recently, a considerable number of studies in computer vision involve deep neural architectures called vision transformers. Visual processing in these models incorporates computational models that are claimed to implement attention mechanisms. Despite an increasing body of work that attempts to understand the role of attention mechanisms in vision transformers, their effect is largely unknown. Here, we asked if the attention mechanisms in vision transformers exhibit similar effects as those known in human visual attention. To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects. Additionally, whereas modern experimental findings reveal that human visual attention involves both feed-forward and feedback mechanisms, the purely feed-forward architecture of vision transformers suggests that attention in these models cannot have the same effects as those known in humans. To quantify these observations, we evaluated grouping performance in a family of vision transformers. Our results suggest that self-attention modules group figures in the stimuli based on similarity of visual features such as color. Also, in a singleton detection experiment as an instance of salient object detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms thought to be utilized in human visual attention. We found that generally, the transformer-based attention modules assign more salience either to distractors or the ground, the opposite of both human and computational salience. Together, our study suggests that the mechanisms in vision transformers perform perceptual organization based on feature similarity and not attention.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fcomp.2023.1178450"
    },
    {
        "id": 23059,
        "title": "Advancements in End-to-End Speech Recognition: A Comparative Study of Deep Learning Architectures and Attention Mechanisms",
        "authors": "",
        "published": "2024-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets48033"
    },
    {
        "id": 23060,
        "title": "Research on Image Super-Resolution Using Attention Mechanisms based on Super-Resolution Generative Adversarial Network",
        "authors": "Zhouli Wu",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "With the continuous advancement of technology, Super-Resolution Generative Adversarial Networks (SRGAN) have played a significant role in the field of image super-resolution, significantly enhancing the resolution of images. However, while SRGAN excels in generating details, sometimes the restored details do not always meet people's expectations. To further enhance the quality of images and make image details clearer, this paper introduces improvements to the architecture and loss functions of the SRGAN network. Specifically, this research draws inspiration from the architecture of ESRGAN, removing the original Batch Normalization layers and introducing a newly designed Residual Block. Leveraging insights from attention mechanisms, we incorporate three layers of convolutional operations and introduce attention mechanisms into these new Residual Blocks. Furthermore, to simplify the computational complexity of the model, this paper simplifies the original loss functions, consolidating the previous four losses into two. These enhancements result in a significantly improved model in capturing visual elements, making key objects in the images more prominent compared to SRGAN. Detailed experimental results demonstrate that this model, while maintaining the clarity of details, provides higher visual quality. These achievements provide valuable insights and inspiration for further research and applications in the field of image super-resolution.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v5i2.13142"
    },
    {
        "id": 23061,
        "title": "Implicit and explicit attention mechanisms for zero-shot learning",
        "authors": "Faisal Alamri, Anjan Dutta",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.03.009"
    },
    {
        "id": 23062,
        "title": "Fast-DSAGCN: Enhancing semantic segmentation with multifaceted attention mechanisms",
        "authors": "Khawaja Iftekhar Rashid, Chenhui Yang, Chenxi Huang",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127625"
    },
    {
        "id": 23063,
        "title": "Bottleneck Transformer model with Channel Self-Attention for skin lesion classification",
        "authors": "Masato Tada, Xian-Hua Han",
        "published": "2023-7-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10215720"
    },
    {
        "id": 23064,
        "title": "Vehicle Detection and Identification System: Convolutional Neural Network with Self-Attention",
        "authors": "Maisha Maliha, Vishal Pramanik",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307938"
    },
    {
        "id": 23065,
        "title": "SASTDGCN: Self-Attention Based Spatial-Temporal Double Graph Convolutional Networks for Traffic Flow Forecasting",
        "authors": "Jingjia Wan, Yu Wu",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386418"
    },
    {
        "id": 23066,
        "title": "Wave Self-Attention Mechanism for Three-Dimensional Features",
        "authors": "Hao Liu, Yuanzhi Cheng, Hui Li",
        "published": "2023-7-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceict57916.2023.10245724"
    },
    {
        "id": 23067,
        "title": "Attributed network embedding based on self-attention mechanism for recommendation method",
        "authors": "Shuo Wang, Jing Yang, Fanshu Shang",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "AbstractNetwork embedding is a technique used to learn a low-dimensional vector representation for each node in a network. This method has been proven effective in network mining tasks, especially in the area of recommendation systems. The real-world scenarios often contain rich attribute information that can be leveraged to enhance the performance of representation learning methods. Therefore, this article proposes an attribute network embedding recommendation method based on self-attention mechanism (AESR) that caters to the recommendation needs of users with little or no explicit feedback data. The proposed AESR method first models the attribute combination representation of items and then uses a self-attention mechanism to compactly embed the combination representation. By representing users as different anchor vectors, the method can efficiently learn their preferences and reconstruct them with few learning samples. This achieves accurate and fast recommendations and avoids data sparsity problems. Experimental results show that AESR can provide personalized recommendations even for users with little explicit feedback information. Moreover, the attribute extraction of documents can effectively improve recommendation accuracy on different datasets. Overall, the proposed AESR method provides a promising approach to recommendation systems that can leverage attribute information for better performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-44696-1"
    },
    {
        "id": 23068,
        "title": "Expected benefits of crowdsourcing outcomes in government: identifying and explaining some of the mechanisms",
        "authors": "Cesar Renteria",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/eg.2023.134042"
    },
    {
        "id": 23069,
        "title": "Influence of physical activity on cognitive functions  - Potential mechanisms and benefits",
        "authors": "Blanka Dwojaczny, Monika Bejtka",
        "published": "2023-2-2",
        "citations": 0,
        "abstract": "Background and Study Aim. The results of many research indicate that systematic physical activity has also positive effect on functions of the central nervous system. For example, improvement of the cognitive functions level, such as memory and learning, under the influence of systematic physical training has been demonstrated. The positive effect of physical activity on the central nervous system is especially visible and widely described with regard to elderly people, who develop many adverse remodeling changes in the structure of the brain. However, particularly interesting are the studies which show that also among young people a positive effect of physical activity on cognitive processes is observed. Currently, several hypotheses are proposed, presenting potential mechanisms underlying the beneficial effects of physical activity on the central nervous system. The first hypothesis assumes the beneficial effect of physical activity on the expression of hippocampal genes related to synaptic plasticity. The second hypothesis assumes that physical effort per se is an inducer of the secretion of the growth factors (e.g., BDNF, IGF-1), which have a trophic effect on the nervous system. In addition, the results of the latest scientific studies indicate that the positive effect of physical activity on the central nervous system may be due to the action of phospholipase (Gpld-1), released to the bloodstream from the liver under the influence of physical exercise. This work indicates that due to the influence on cognitive functions, physical activity is absolutely essential to both elderly and young people population.\r\nConclusions. It seems necessary to educate both young and elderly people that the proper level of physical activity is a key factor allowing to maintain both physical and mental health at an appropriate, desirable level.",
        "keywords": "",
        "link": "http://dx.doi.org/10.12775/jehs.2023.13.03.026"
    },
    {
        "id": 23070,
        "title": "The Future of Chemotherapy: The Mechanisms and Benefits of Exercise in Taxane-Induced Peripheral Neuropathy",
        "authors": "Sumedha Shastry, David Mizrahi, Grace Kanzawa-Lee",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "Chemotherapy-induced peripheral neuropathy (CIPN) is a dose-limiting side-effect resulting from numerous neurotoxic chemotherapies that damages the peripheral nerves, alters sensations in the hands and feet, causes burning and shooting pains, and impairs a patient’s quality of life (QoL). There are limited established interventions to help improve CIPN symptoms. There is only one pharmacological agent (Duloxetine) for treatment of CIPN; however, it only has mild benefit, signaling a critical need for alternative management options to manage patient symptoms. Multiple studies suggest therapeutic benefits of exercise in cancer care to improve physical and psychological functioning; however, the benefits regarding CIPN symptoms and physical function are less clear. This narrative review synthesizes research articles investigating the effect and mechanisms induced by different exercise programs for patients with taxane-induced peripheral neuropathy (TIPN) symptoms and function. The overall incidence, manifestations, characteristics, and mechanisms of CIPN are also discussed. While some studies in this narrative review demonstrated that exercise programs may have benefits on sensory and motor TIPN symptoms in some but not all patients, there are consistent benefits of improved QoL and physical function across most patients. This narrative review highlights the need for future research to confirm the effects of exercise for TIPN, with a focus on other important components, including the effect of exercise adherence, type, and supervision level.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/physiologia3040042"
    },
    {
        "id": 23071,
        "title": "How far can the self be extended? Automatic attention capture is triggered not only by the self-face",
        "authors": "Anna Żochowska, Michał J. Wójcik, Anna Nowicka",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "The preferential processing of self-related information is thought to be driven by its high level of familiarity. However, some behavioral studies have shown that people may exhibit a preference for initially unfamiliar stimuli that have been associated with themselves arbitrarily. One of the key questions that needs to be addressed concerns the role of early attention in the prioritization of newly acquired information associated with the self. Another question is whether both highly familiar as well as new information referring to a subjectively significant person (i.e. close-other) benefits from preferential attentional processing. We aimed to tackle both questions by investigating the neural mechanisms involved in processing extremely familiar stimuli, like one’s own face or the face of a close-other, as well as stimuli (abstract shapes) that were newly linked to each person. We used a dot-probe paradigm that allowed us to investigate the early stages of attentional prioritization. Our analysis of the N2pc component unveiled that attention was automatically captured by the self-face, a shape associated with oneself, and the face of the close person. However, a shape associated with the close-other did not elicit the same attentional response, as the N2pc was absent. Thus, both the self-face and information referring to the extended self (self-assigned shape, close-other’s face) benefit from preferential early and automatic attentional processing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpsyg.2023.1279653"
    },
    {
        "id": 23072,
        "title": "A Lung Lesion Detection Algorithm Based on YOLOv7 and Self-Attention Mechanism",
        "authors": "Junhua Luo, Shujing Wang, Qixiang Wang, Shaojun Liu",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240165"
    },
    {
        "id": 23073,
        "title": "Self-adaptive attention fusion for multimodal aspect-based sentiment analysis",
        "authors": "Ziyue Wang, Junjun Guo",
        "published": "2023",
        "citations": 0,
        "abstract": "<abstract><p>Multimodal aspect term extraction (MATE) and multimodal aspect-oriented sentiment classification (MASC) are two crucial subtasks in multimodal sentiment analysis. The use of pretrained generative models has attracted increasing attention in aspect-based sentiment analysis (ABSA). However, the inherent semantic gap between textual and visual modalities poses a challenge in transferring text-based generative pretraining models to image-text multimodal sentiment analysis tasks. To tackle this issue, this paper proposes a self-adaptive cross-modal attention fusion architecture for joint multimodal aspect-based sentiment analysis (JMABSA), which is a generative model based on an image-text selective fusion mechanism that aims to bridge the semantic gap between text and image representations and adaptively transfer a textual-based pretraining model to the multimodal JMASA task. We conducted extensive experiments on two benchmark datasets, and the experimental results show that our model significantly outperforms other state of the art approaches by a significant margin.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/mbe.2024056"
    },
    {
        "id": 23074,
        "title": "Complex-Valued Self-Supervised PolSAR Image Classification Integrating Attention Mechanism",
        "authors": "Zuzheng Kuang, Haixia Bi, Fan Li",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281737"
    },
    {
        "id": 23075,
        "title": "A Band-dependent Self-Attention Network for Hyperspectral Image Classification",
        "authors": "Jiazhen Xu, Wanting Hu",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiiip61647.2023.00042"
    },
    {
        "id": 23076,
        "title": "Cascaded feature fusion with multi-level self-attention mechanism for object detection",
        "authors": "Chuanxu Wang, Huiru Wang",
        "published": "2023-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109377"
    },
    {
        "id": 23077,
        "title": "Individual and combined benefits of different nonequilibrium proofreading mechanisms",
        "authors": "Adélaïde A. Mohr, Daniel M. Busiello, Stefano Zamuner, Paolo De Los Rios",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physrevresearch.5.043145"
    },
    {
        "id": 23078,
        "title": "Self-prioritization in working memory gating",
        "authors": "Roel van Dooren, Bryant J. Jongkees, Roberta Sellaro",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "AbstractWorking memory (WM) involves a dynamic interplay between temporary maintenance and updating of goal-relevant information. The balance between maintenance and updating is regulated by an input-gating mechanism that determines which information should enter WM (gate opening) and which should be kept out (gate closing). We investigated whether updating and gate opening/closing are differentially sensitive to the kind of information to be encoded and maintained in WM. Specifically, since the social salience of a stimulus is known to affect cognitive performance, we investigated if self-relevant information differentially impacts maintenance, updating, or gate opening/closing. Participants first learned to associate two neutral shapes with two social labels (i.e., “you” vs. “stranger”), respectively. Subsequently they performed the reference-back paradigm, a well-established WM task that disentangles WM updating, gate opening, and gate closing. Crucially, the shapes previously associated with the self or a stranger served as target stimuli in the reference-back task. We replicated the typical finding of a repetition benefit when consecutive trials require opening the gate to WM. In Study 1 (N = 45) this advantage disappeared when self-associated stimuli were recently gated into WM and immediately needed to be replaced by stranger-associated stimuli. However, this was not replicated in a larger sample (Study 2; N = 90), where a repetition benefit always occurred on consecutive gate-opening trials. Overall, our results do not provide evidence that the self-relevance of stimuli modulates component processes of WM. We discuss possible reasons for this null finding, including the importance of continuous reinstatement and task-relevance of the shape-label associations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13414-024-02869-8"
    },
    {
        "id": 23079,
        "title": "Multi-Head Self Attention for Enhanced Object Detection in the Maritime Domain",
        "authors": "Walid Messaoud, Rim Trabelsi, Adnane Cabani, Fatma Abdelkefi",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cw58918.2023.00034"
    },
    {
        "id": 23080,
        "title": "Self-Calibrated Cross Attention Network for Few-Shot Segmentation",
        "authors": "Qianxiong Xu, Wenting Zhao, Guosheng Lin, Cheng Long",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00067"
    },
    {
        "id": 23081,
        "title": "Research on Anomaly Network Detection Based on Self-Attention Mechanism",
        "authors": "Wanting Hu, Lu Cao, Qunsheng Ruan, Qingfeng Wu",
        "published": "2023-5-25",
        "citations": 1,
        "abstract": "Network traffic anomaly detection is a key step in identifying and preventing network security threats. This study aims to construct a new deep-learning-based traffic anomaly detection model through in-depth research on new feature-engineering methods, significantly improving the efficiency and accuracy of network traffic anomaly detection. The specific research work mainly includes the following two aspects: 1. In order to construct a more comprehensive dataset, this article first starts from the raw data of the classic traffic anomaly detection dataset UNSW-NB15 and combines the feature extraction standards and feature calculation methods of other classic detection datasets to re-extract and design a feature description set for the original traffic data in order to accurately and completely describe the network traffic status. We reconstructed the dataset DNTAD using the feature-processing method designed in this article and conducted evaluation experiments on it. Experiments have shown that by verifying classic machine learning algorithms, such as XGBoost, this method not only does not reduce the training performance of the algorithm but also improves its operational efficiency. 2. This article proposes a detection algorithm model based on LSTM and the recurrent neural network self-attention mechanism for important time-series information contained in the abnormal traffic datasets. With this model, through the memory mechanism of the LSTM, the time dependence of traffic features can be learned. On the basis of LSTM, a self-attention mechanism is introduced, which can weight the features at different positions in the sequence, enabling the model to better learn the direct relationship between traffic features. A series of ablation experiments were also used to demonstrate the effectiveness of each component of the model. The experimental results show that, compared to other comparative models, the model proposed in this article achieves better experimental results on the constructed dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23115059"
    },
    {
        "id": 23082,
        "title": "Hardware-efficient Softmax Approximation for Self-Attention Networks",
        "authors": "Nazim Altar Koca, Anh Tuan Do, Chip-Hong Chang",
        "published": "2023-5-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscas46773.2023.10181465"
    },
    {
        "id": 23083,
        "title": "A Self-Attention based Network for Low Resolution Multi-View Stereo",
        "authors": "Weijuan Li, Ruiming Jia",
        "published": "2023-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135325"
    },
    {
        "id": 23084,
        "title": "Data-Efficient MADDPG Based on Self-Attention for IoT Energy Management Systems",
        "authors": "Mohammed Al-Saffar, Mustafa Gül",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3322193"
    },
    {
        "id": 23085,
        "title": "Single image reflection removal via self-attention and local discrimination",
        "authors": "Yan Huang, Xinchang Lu, Jia Fu",
        "published": "2024-3-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00371-024-03333-2"
    },
    {
        "id": 23086,
        "title": "The relationship between self-forgiveness and human flourishing: Inferring the underlying psychological mechanisms",
        "authors": "",
        "published": "2023-7-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.24425/ppb.2020.132649"
    },
    {
        "id": 23087,
        "title": "Finger Vein Recognition Based on ResNet With Self-Attention",
        "authors": "Zhibo Zhang, Guanghua Chen, Weifeng Zhang, Huiyang Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3347922"
    },
    {
        "id": 23088,
        "title": "Evaluating and Improving Context Attention Distribution on Multi-Turn response generation using Self-Contained Distractions",
        "authors": "Yujie Xing, Jon Atle Gulla",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "Despite the rapid progress of open-domain generation-based conversational agents, most deployed systems treat dialogue contexts as single-turns, while systems dealing with multi-turn contexts are less studied. There is a lack of a reliable metric for evaluating multi-turn modelling, as well as an effective solution for improving it. In this paper, we focus on an essential component of multi-turn generation-based conversational agents: context attention distribution, i.e. how systems distribute their attention on dialogue’s context. For evaluation of this component, We introduce a novel attention-mechanism-based metric: DAS ratio. To improve performance on this component, we propose an optimization strategy that employs selfcontained distractions. Our experiments on the Ubuntu chatlogs dataset show that models with comparable perplexity can be distinguished by their ability on context attention distribution. Our proposed optimization strategy improves both non-hierarchical and hierarchical models on the proposed metric by about 10% from baselines.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130210"
    },
    {
        "id": 23089,
        "title": "SATNet: Upgraded LSTM Network For Mining Time Series Correlation Utilizing The Self-Attention Mechanism",
        "authors": "Xiangyu Dai, Quan Zou",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191490"
    },
    {
        "id": 23090,
        "title": "A Novel Convolution Kernel with Multi-head Self-attention",
        "authors": "Ming Gao, Huailin Zhao, Mingfang Deng",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciibms60103.2023.10347796"
    },
    {
        "id": 23091,
        "title": "Multimodal sentiment analysis based on multiple attention mechanisms",
        "authors": "Zixuan Jin, Changbo Xu, Shaozhong Cao, Yi Liu",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640912.3640957"
    },
    {
        "id": 23092,
        "title": "Explainable Multimodal Fake Posts Detection Using Feature Extraction with Attention Mechanisms",
        "authors": "Tomoaki Ohkawa, Hiroshi Yoshiura, Takayasu Yamaguchi",
        "published": "2023-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/qrs-c60940.2023.00068"
    },
    {
        "id": 23093,
        "title": "Diabetes self-management education: Benefits and challenges",
        "authors": "IbironkeCecilia Ojo, ElizabethOlufunmilayo Ojo, SimeoKayode Olubiyi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/jin.jin_105_22"
    },
    {
        "id": 23094,
        "title": "Self-Aligning Rotational Latching Mechanisms: Optimal Geometry for Mechanical Robustness",
        "authors": "Gabriel I. Fernandez, Samuel Gessow, Justin Quan, Dennis W. Hong",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "AbstractIn concurrent work, we introduced a novel robotic package delivery system latching intelligent modular mobility system (LIMMS). Each LIMMS end effector requires a small, lightweight latching mechanism for pre-manufactured containers, such as cardboard boxes. In order to effectively process a high volume of packages, aligning the latching mechanism quickly and reliably is critical. Instead of depending on highly accurate controllers for alignment, we propose a novel self-aligning rotational mechanism to increase the system’s tolerance to misalignment. The radial latching design consists of evenly spaced blades that rotate into slots cut into the box. When misaligned, the blades contact the edges of the engagement slots, generating a self-correcting force that passively centers the blades with the slot pattern. This paper introduces a mathematical framework with closed form expressions to quantify error tolerance for these mechanisms. Through our mathematical and optimization analyses, it is shown that a two-blade design can tolerate a maximum misalignment of three times the radius to the blade tips, much larger than commonly used designs with three or more blade-like contacts. Our approach can be generalized for a class of rotational latching mechanisms with any number of blades. Utilizing this theory, a design process is laid out for developing an optimal self-aligning rotational latching mechanism given desired parameters and task constraints. With this methodology, we designed, manufactured, and verified the effectiveness of both two-blade and three-blade self-aligning in practical experiments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4057073"
    },
    {
        "id": 23095,
        "title": "Image fusion with inverted residual densely connected directional differentiation and attention mechanisms",
        "authors": "Yanfeng Li, Hanyue Zhang",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3021548"
    },
    {
        "id": 23096,
        "title": "Age Estimation Based on Graph Convolutional Networks and Multi-head Attention Mechanisms",
        "authors": "Miaomiao Yang, Changwei Yao, Shijin Yan",
        "published": "2023-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciscae59047.2023.10393185"
    },
    {
        "id": 23097,
        "title": "Scale-arbitrary Infrared Super-resolution Network based on Channel Attention Mechanisms",
        "authors": "Qi Shao, Xin Zheng, Shinan Lang, Yuchen Zheng",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/prml59573.2023.10348211"
    },
    {
        "id": 23098,
        "title": "Study on the Cognitive Mechanisms of Stigmatized Discourse Production in Western Media Based on Cognitive Attention",
        "authors": "Cuicui Zang",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "Stigma is the negative interpreation of a certain individual/group by a stigmatizer for a specific purpose under the influence of public opinion. In order to break the state of being stigmatized, the key is to understand the generation processes of stigmatized discourses. This study puts forward a theoretical framework for analyzing the cognitive mechanisms and strategies of stigmatized discourse generation by combining cognitive attention with proximization theory. In order to validate this framework, this study then takes the western reports on Russia-Ukraine conflict as an example to analyze its production strategies both in terms of cognition and language. It turns out that the framework has strong explanatory power in investigating the generation processes, different attention strategies and the language expressions of the stigma. It is helpful to understand the cognitive operation behind stigmatization and select the corresponding anti-stigma discourse to de-construct these negative conceptualizations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/ijeh.v11i3.14450"
    },
    {
        "id": 23099,
        "title": "Air Quality Index Prediction Model Based on Multiple Attention Mechanisms and Hyperparameter Optimization",
        "authors": "Fan Yang, Xiaoming Jiang, Zhanfang Chen",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiecs59936.2023.10435483"
    },
    {
        "id": 23100,
        "title": "Liver Tumor Prediction with Advanced Attention Mechanisms Integrated into a Depth-Based Variant Search Algorithm",
        "authors": "P. Kalaiselvi, S. Anusuya",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.040264"
    },
    {
        "id": 23101,
        "title": "A Modified Deeplabv3+ Model Based on Polarized Self-Attention Mechanism",
        "authors": "Zheng Zhang, Rui Song",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241058"
    },
    {
        "id": 23102,
        "title": "Sequence to Sequence Load Recognition Model Based On Sparse Self-attention Transformer",
        "authors": "Zhihua Dong, Xing Guo",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigcom61073.2023.00037"
    },
    {
        "id": 23103,
        "title": "Separable Self and Mixed Attention Transformers for Efficient Object Tracking",
        "authors": "Goutam Yelluru Gopal, Maria A. Amer",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00657"
    },
    {
        "id": 23104,
        "title": "Retracted: A Deep Learning Approach for a Source Code Detection Model Using Self-Attention",
        "authors": " Complexity",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2024/9761251"
    }
]