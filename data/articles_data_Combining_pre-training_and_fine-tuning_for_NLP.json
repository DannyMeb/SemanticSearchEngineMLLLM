[
    {
        "id": 24105,
        "title": "nanoT5: Fast &amp; Simple Pre-training and Fine-tuning of T5 Models with Limited Resources",
        "authors": "Piotr Nawrot",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.11"
    },
    {
        "id": 24106,
        "title": "Pre-Training and Fine-Tuning Attention Based Encoder Decoder Improves Sea Surface Height Multi-Variate Inpainting",
        "authors": "Théo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Béréziat",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357400003660"
    },
    {
        "id": 24107,
        "title": "‘Pre-training+Fine-tuning’ Model-based 1/2 Folded Image Restoration",
        "authors": "Huijia Song, Jiacheng Wei, Xiaozhu Lin, Huainian Zhang",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isceic59030.2023.10271228"
    },
    {
        "id": 24108,
        "title": "Fine-tuning and multilingual pre-training for abstractive summarization task for the Arabic language",
        "authors": "Mram Kahla, Attila Novák, Zijian Győző Yang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33039/ami.2022.11.002"
    },
    {
        "id": 24109,
        "title": "Enhancing Fine-Tuning in Low Data Regime by Increasing Representation Entropy During Pre-Training Phase",
        "authors": "Jaeill Kim, Jungwook Shin, Wonjong Rhee",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictc58733.2023.10391861"
    },
    {
        "id": 24110,
        "title": "A Modeling Method of Pre-training and Fine-tuning for Non-uniform Indoor Environment",
        "authors": "Gang Jing, Huan Wang, Xianting Li, Chenguang Ning",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciea58696.2023.10241942"
    },
    {
        "id": 24111,
        "title": "Bridging the Gap between Pre-Training and Fine-Tuning for Commonsense Generation",
        "authors": "Haoran Yang, Yan Wang, Piji Li, Wei Bi, Wai Lam, Chen Xu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.28"
    },
    {
        "id": 24112,
        "title": "Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training",
        "authors": "Haode Zhang, Haowen Liang, Li-Ming Zhan, Xiao-Ming Wu, Albert Y.S. Lam",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.706"
    },
    {
        "id": 24113,
        "title": "Improving Pre-Training and Fine-Tuning for Few-Shot SAR Automatic Target Recognition",
        "authors": "Chao Zhang, Hongbin Dong, Baosong Deng",
        "published": "2023-3-22",
        "citations": 1,
        "abstract": "SAR-ATR (synthetic aperture radar-automatic target recognition) is a hot topic in remote sensing. This work suggests a few-shot target recognition approach (FTL) based on the concept of transfer learning to accomplish accurate target recognition of SAR images in a few-shot scenario since the classic SAR ATR method has significant data reliance. At the same time, the strategy introduces a model distillation method to improve the model’s performance further. This method is composed of three parts. First, the data engine, which uses the style conversion model and optical image data to generate image data similar to SAR style and realize cross-domain conversion, can effectively solve the problem of insufficient training data of the SAR image classification model. Second is model training, which uses SAR image data sets to pre-train the model. Here, we introduce the deep Brownian distance covariance (Deep BDC) pooling layer to optimize the image feature representation so that the model can learn the image representation by measuring the difference between the joint feature function of the embedded feature and the edge product. Third, model fine-tuning, which freezes the model structure, except the classifier, and fine-tunes it by using a small amount of novel data. The knowledge distillation approach is also introduced simultaneously to train the model repeatedly, sharpen the knowledge, and enhance model performance. According to experimental results on the MSTAR benchmark dataset, the proposed method is demonstrably better than the SOTA method in the few-shot SAR ATR issue. The recognition accuracy is about 80% in the case of 10-way 10-shot.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15061709"
    },
    {
        "id": 24114,
        "title": "Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning",
        "authors": "Ji-Rong Wen, Zi Huang, Hanwang Zhang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11633-023-1431-y"
    },
    {
        "id": 24115,
        "title": "APF-GAN: Exploring asymmetric pre-training and fine-tuning strategy for conditional generative adversarial network",
        "authors": "Yuxuan Li, Lingfeng Yang, Xiang Li",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s41095-023-0357-1"
    },
    {
        "id": 24116,
        "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection",
        "authors": "Ming Li, Jie Wu, Xionghui Wang, Chen Chen, Jie Qin, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00632"
    },
    {
        "id": 24117,
        "title": "Fine-tuning Transformer-based MT Using Syntactic Guides",
        "authors": "Jinnawat Makwisai, Prachya Boonkwan, Sasiporn Usanavasin, Natsuda Kaothanthong, Manabu Okumura",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isai-nlp60301.2023.10354857"
    },
    {
        "id": 24118,
        "title": "Robust Lane Detection Through Self Pre-Training With Masked Sequential Autoencoders and Fine-Tuning With Customized PolyLoss",
        "authors": "Ruohan Li, Yongqi Dong",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tits.2023.3305015"
    },
    {
        "id": 24119,
        "title": "SAR-HUB: Pre-Training, Fine-Tuning, and Explaining",
        "authors": "Haodong Yang, Xinyue Kang, Long Liu, Yujiang Liu, Zhongling Huang",
        "published": "2023-11-28",
        "citations": 1,
        "abstract": "Since the current remote sensing pre-trained models trained on optical images are not as effective when applied to SAR image tasks, it is crucial to create sensor-specific SAR models with generalized feature representations and to demonstrate with evidence the limitations of optical pre-trained models in downstream SAR tasks. The following aspects are the focus of this study: pre-training, fine-tuning, and explaining. First, we collect the current large-scale open-source SAR scene image classification datasets to pre-train a series of deep neural networks, including convolutional neural networks (CNNs) and vision transformers (ViT). A novel dynamic range adaptive enhancement method and a mini-batch class-balanced loss are proposed to tackle the challenges in SAR scene image classification. Second, the pre-trained models are transferred to various SAR downstream tasks compared with optical ones. Lastly, we propose a novel knowledge point interpretation method to reveal the benefits of the SAR pre-trained model with comprehensive and quantifiable explanations. This study is reproducible using open-source code and datasets, demonstrates generalization through extensive experiments on a variety of tasks, and is interpretable through qualitative and quantitative analyses. The codes and models are open source.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15235534"
    },
    {
        "id": 24120,
        "title": "FactGen: Faithful Text Generation by Factuality-aware Pre-training and Contrastive Ranking Fine-tuning",
        "authors": "ZhiBin Lan, Wei Li, Jinsong Su, Xinyan Xiao, Jiachen Liu, Wenhao Wu, Yajuan Lyu",
        "published": "2023-4-27",
        "citations": 2,
        "abstract": "Conditional text generation is supposed to generate a fluent and coherent target text that is faithful to the source text. Although pre-trained models have achieved promising results, they still suffer from the crucial factuality problem. To deal with this issue, we propose a factuality-aware pretraining-finetuning framework named FactGen, which fully considers factuality during two training stages. Specifically, at the pre-training stage, we utilize a natural language inference model to construct target texts that are entailed by the source texts, resulting in a more factually consistent pre-training objective. Then, during the fine-tuning stage, we further introduce a contrastive ranking loss to encourage the model to generate factually consistent text with higher probability. Extensive experiments on three conditional text generation tasks demonstrate the effectiveness and generality of our training framework.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1613/jair.1.14267"
    },
    {
        "id": 24121,
        "title": "Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages",
        "authors": "Aarohi Srivastava, David Chiang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.vardial-1.16"
    },
    {
        "id": 24122,
        "title": "Fine-tuning of pre-processing filters enables scalp-EEG based training of subcutaneous EEG models",
        "authors": "Lukas Lechner, Asbjoern Wulff Helge, Esben Ahrens, Martin Bachler, Bernhard Hametner, Gerhard Gritsch, Tilmann Kluge, Manfred Hartmann",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bsn58485.2023.10331106"
    },
    {
        "id": 24123,
        "title": "On-Device Constrained Self-Supervised Learning for Keyword Spotting via Quantization Aware Pre-Training and Fine-Tuning",
        "authors": "Gene-Ping Yang, Yue Gu, Sashank Macha, Qingming Tang, Yuzong Liu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447258"
    },
    {
        "id": 24124,
        "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",
        "authors": "Angus Addlesee, Weronika Sieińska, Nancie Gunson, Daniel Hernandez Garcia, Christian Dondrup, Oliver Lemon",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.sigdial-1.22"
    },
    {
        "id": 24125,
        "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
        "authors": "Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.336"
    },
    {
        "id": 24126,
        "title": "Rhythmic lyrics translation: Customizing a pre-trained language model using stacked fine-tuning",
        "authors": "Jiwon Chong, Jaehyok Chong",
        "published": "2023",
        "citations": 0,
        "abstract": "Neural machine translation (NMT) is a software that uses neural network techniques to translate text from one language to another. As NMT models are on the rise, the focus is on translating everyday mundane sentences. However, it is also necessary to start paying attention to the translation of domain-specific text, such as lyrics or poetry. For example, even one of the most famous NMT models—Google Translate—failed to give an accurate English translation of a famous Korean nursery rhyme, \"Airplane\" (비행기). To teach the model to retain specific information other than semantics, we need specific data which contains the exact information that we are attempting to teach. In the case of rhythmically accurate lyrics translation—translated lyrics that can be used to sing along to the original melody—we need corresponding data, containing lyrical and rhythmical properties, all the while being semantically accurate. However, as there is not enough data that fits our criteria, we propose a novel method we call 'stacked fine-tuning'. We fine-tuned a pre-trained model first with a dataset from the lyrics domain, and then with a smaller dataset containing the rhythmical properties, to teach the model to translate rhythmically accurate lyrics. To evaluate the effectiveness of our approach, we translated two famous Korean nursery rhymes to English and matched them to the original melody. Our stacked fine-tuning method resulted in an NMT model that could maintain the rhythmical characteristics of lyrics during translation while single fine-tuned models failed to do so.",
        "keywords": "",
        "link": "http://dx.doi.org/10.59720/22-177"
    },
    {
        "id": 24127,
        "title": "Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis",
        "authors": "Florian Lux, Ching-Yi Chen, Ngoc Thang Vu",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022897"
    },
    {
        "id": 24128,
        "title": "PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent",
        "authors": "Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Johnson, Rongrong Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.748"
    },
    {
        "id": 24129,
        "title": "Fine-Grained Sentiment Analysis with a Fine-Tuned BERT and an Improved Pre-Training BERT",
        "authors": "Jiaxi Li",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257673"
    },
    {
        "id": 24130,
        "title": "Layer-wise Fine-tuning Based Pre-trained Language Model Syntactic Knowledge Injection Methodology",
        "authors": "Hye-Lynn Kim, Sanghyun Cho, Hyuk-Chul Kwon",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.6109/jkiice.2023.27.12.1473"
    },
    {
        "id": 24131,
        "title": "Trajectory‐BERT: Pre‐training and fine‐tuning bidirectional transformers for crowd trajectory enhancement",
        "authors": "Lingyu Li, Tianyu Huang, Yihao Li, Peng Li",
        "published": "2023-5",
        "citations": 1,
        "abstract": "AbstractTo address the issue of trajectory fragments and ID switches caused by occlusion in dense crowds, we propose a space‐time trajectory encoding method and a point‐line‐group division method to construct Trajectory‐BERT in this paper. Leveraging the spatiotemporal context‐dependent features of trajectories, we introduce pre‐training and fine‐tuning Trajectory‐BERT tasks to repair occluded trajectories. Experimental results show that data augmented with Trajectory‐BERT outperforms raw annotated data on the MOTA metric and reduces ID switches in raw labeled data, demonstrating the feasibility of our method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cav.2190"
    },
    {
        "id": 24132,
        "title": "Math Function Recognition with Fine-Tuning Pre-Trained Models",
        "authors": "Fatimah Alshamari, Abdou Youssef",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "A Mathematical Function Recognition (MFR) is an important research direction for efficient downstream math tasks such as information retrieval, knowledge extraction, and question answering. The aim of this task is to identify and classify mathematical function into a predefined set of function. However, the lack of annotated data is the bottleneck in the development of an MFR automated model. We begin this paper by describing our approach to creating a labelled dataset for MFR. Then, to identify five categories of mathematical functions, we fine-tuned a set of common pre-trained models: BERT base-cased, BERT baseuncased, DistilBERT-cased, and DistilBERT-uncased. As a result, our contributions in this paper include: (1) an annotated MFR dataset that future researchers can use; and (2) SOTA results obtained by finetuning pre-trained models for the MFR task. Our experiments demonstrate that the proposed approach achieved a high-quality recognition, with an F1 score of 96.80% on a held-out test set provided by DistilBERT-cased model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijci.2023.120204"
    },
    {
        "id": 24133,
        "title": "Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords",
        "authors": "Shahriar Golchin, Mihai Surdeanu, Nazgol Tavabi, Ata Kiapour",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.2"
    },
    {
        "id": 24134,
        "title": "Comparing The Fine-Tuning and Performance of Whisper Pre-Trained Models for Turkish Speech Recognition Task",
        "authors": "Saadin Oyucu",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304891"
    },
    {
        "id": 24135,
        "title": "Prompting and Fine-tuning Pre-trained Generative Language Models",
        "authors": "Johny Moreira, Altigran da Silva, Luciano Barbosa",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "There has been an explosion of available pre-trained and fine-tuned Generative Language Models (LM). They vary in the number of parameters, architecture, training strategy, and training set size. Aligned with it, alternative strategies exist to exploit these models, such as Fine-tuning and Prompt Engineering. However, many questions may arise throughout this process: Which model to apply for a given task? Which strategies to use? Will Prompt Engineering solve all tasks? What are the computational and financial costs involved? This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5753/sbbd_estendido.2023.25636"
    },
    {
        "id": 24136,
        "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
        "authors": "Abir Betka, Zeyd Ferhat, Riyadh Barka, Selma Boutiba, Zineddine Kahhoul, Tiar Lakhdar, Ahmed Abdelali, Habiba Dahmani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.33"
    },
    {
        "id": 24137,
        "title": "MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning",
        "authors": "Zhehua Zhong, Tianyi Chen, Zhen Wang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the state-of-the-art methods on both the GLUE and ANLI benchmarks in terms of generalization and robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/520"
    },
    {
        "id": 24138,
        "title": "MUX-PLMs: Pre-training Language Models with Data Multiplexing",
        "authors": "Vishvak Murahari, Ameet Deshpande, Carlos Jimenez, Izhak Shafran, Mingqiu Wang, Yuan Cao, Karthik Narasimhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.17"
    },
    {
        "id": 24139,
        "title": "Exploring the Transfer Learning: A Comparative Study of Pre-trained Models and Fine-tuning Techniques on image dataset",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56452/6-3-664"
    },
    {
        "id": 24140,
        "title": "Foundations and Applications in Large-scale AI Models: Pre-training, Fine-tuning, and Prompt-based Learning",
        "authors": "Derek Cheng, Dhaval Patel, Linsey Pang, Sameep Mehta, Kexin Xie, Ed H. Chi, Wei Liu, Nitesh Chawla, James Bailey",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3580305.3599209"
    },
    {
        "id": 24141,
        "title": "Exploring the potential of federated learning for diffusion model: Training and fine-tuning",
        "authors": "Shuo Chen",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "Diffusion models, a state-of-the-art generative model, have drawn attention for their capacity to produce high-quality, divers, and flexible content. However, the training of these models typically necessitates large datasets, a task that can be hindered by challenges related to privacy concerns and data distribution constraints. Due to the amount of data and hardware required for large model training, all centralized training will be done by large companies or labs with computing power. Federated Learning provides a decentralized method that allows for model training across several data sources while maintaining the data's localization, reducing privacy threats. This research proposes and evaluate a novel approach for utilizing Federated Learning in the context of diffusion models. This paper investigates the feasibility of training and fine-tuning diffusion models in a federated setting, considering various data distributions and privacy constraints. This study used the Federated Averaging (FedAvg) technique to train the unconditional diffusion model as well as to fine-tune the pre-trained diffusion mode. The experimental results demonstrate that federated training of diffusion models can achieve comparable performance to centralized training methods while preserving data locality. Additionally, Federated Learning can be effectively applied to fine-tune pre-trained diffusion model, enabling adaptation to specific tasks without exposing sensitive data. Overall, this work demonstrates Federated Learning's potential as a useful tool for training and fine-tuning diffusion models in a privacy-preserving manner.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/52/20241136"
    },
    {
        "id": 24142,
        "title": "Fine-Tune it Like I'm Five: Supporting Medical Domain Experts in Training NER Models Using Cloud, LLM, and Auto Fine-Tuning",
        "authors": "Benedict Hartmann, Philippe Tamla, Florian Freund, Matthias Hemmje",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aics60730.2023.10470654"
    },
    {
        "id": 24143,
        "title": "Unveiling the Power of Pre - Trained Language Models in NLP Applications",
        "authors": "Shrinath Pai",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115202502"
    },
    {
        "id": 24144,
        "title": "An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models",
        "authors": "Jiaxing Liu, Chaofeng Sha, Xin Peng",
        "published": "2023-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ase56229.2023.00125"
    },
    {
        "id": 24145,
        "title": "On Fine-Tuning Pre-Trained Speech Models With EMA-Target Self-Supervised Loss",
        "authors": "Hejung Yang, Hong-Goo Kang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446468"
    },
    {
        "id": 24146,
        "title": "CasANGCL: pre-training and fine-tuning model based on cascaded attention network and graph contrastive learning for molecular property prediction",
        "authors": "Zixi Zheng, Yanyan Tan, Hong Wang, Shengpeng Yu, Tianyu Liu, Cheng Liang",
        "published": "2023-1-19",
        "citations": 9,
        "abstract": "Abstract\n\nMotivation\nMolecular property prediction is a significant requirement in AI-driven drug design and discovery, aiming to predict the molecular property information (e.g. toxicity) based on the mined biomolecular knowledge. Although graph neural networks have been proven powerful in predicting molecular property, unbalanced labeled data and poor generalization capability for new-synthesized molecules are always key issues that hinder further improvement of molecular encoding performance.\n\n\nResults\nWe propose a novel self-supervised representation learning scheme based on a Cascaded Attention Network and Graph Contrastive Learning (CasANGCL). We design a new graph network variant, designated as cascaded attention network, to encode local–global molecular representations. We construct a two-stage contrast predictor framework to tackle the label imbalance problem of training molecular samples, which is an integrated end-to-end learning scheme. Moreover, we utilize the information-flow scheme for training our network, which explicitly captures the edge information in the node/graph representations and obtains more fine-grained knowledge. Our model achieves an 81.9% ROC-AUC average performance on 661 tasks from seven challenging benchmarks, showing better portability and generalizations. Further visualization studies indicate our model’s better representation capacity and provide interpretability.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bib/bbac566"
    },
    {
        "id": 24147,
        "title": "Improving Fine-tuning Pre-trained Models on Small Source Code Datasets via Variational Information Bottleneck",
        "authors": "Jiaxing Liu, Chaofeng Sha, Xin Peng",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/saner56733.2023.00039"
    },
    {
        "id": 24148,
        "title": "Fine-tuning Pre-trained Language Models to Detect In-game Trash Talks",
        "authors": "Daniel Fesalbon -, Arvin De La Cruz -, Marvin Mallari -, Nelson Rodelas -",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages. The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models’ state-of-the-art performance, this study concludes pre-trained language models’ promising potential for addressing online hate speech and in-game insulting trash talk.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2024.v06i02.14927"
    },
    {
        "id": 24149,
        "title": "Combining wav2vec 2.0 Fine-Tuning and ConLearnNet for Speech Emotion Recognition",
        "authors": "Chenjing Sun, Yi Zhou, Xin Huang, Jichen Yang, Xianhua Hou",
        "published": "2024-3-17",
        "citations": 0,
        "abstract": "Speech emotion recognition poses challenges due to the varied expression of emotions through intonation and speech rate. In order to reduce the loss of emotional information during the recognition process and to enhance the extraction and classification of speech emotions and thus improve the ability of speech emotion recognition, we propose a novel approach in two folds. Firstly, a feed-forward network with skip connections (SCFFN) is introduced to fine-tune wav2vec 2.0 and extract emotion embeddings. Subsequently, ConLearnNet is employed for emotion classification. ConLearnNet comprises three steps: feature learning, contrastive learning, and classification. Feature learning transforms the input, while contrastive learning encourages similar representations for samples from the same category and discriminative representations for different categories. Experimental results on the IEMOCAP and the EMO-DB datasets demonstrate the superiority of our proposed method compared to state-of-the-art systems. We achieve a WA and UAR of 72.86% and 72.85% on IEMOCAP, and 97.20% and 96.41% on the EMO-DB, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13061103"
    },
    {
        "id": 24150,
        "title": "OdinDTA: Combining Mutual Attention and Pre-training for Drug-target Affinity Prediction",
        "authors": "Shuting Xu, Ruochen Wang",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00106"
    },
    {
        "id": 24151,
        "title": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation",
        "authors": "Sanjeev Kumar Karn, Rikhiya Ghosh, Kusuma P, Oladimeji Farri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.57"
    },
    {
        "id": 24152,
        "title": "Optimizing Amount of Training Data and Classification Accuracy for Newly Measured Motor Imagery Using Fine-Tuning",
        "authors": "Haruto Yagyu, Nobuaki Kobayashi",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ispa58351.2023.10279756"
    },
    {
        "id": 24153,
        "title": "BERT4ST:: Fine-tuning pre-trained large language model for wind power forecasting",
        "authors": "Zefeng Lai, Tangjie Wu, Xihong Fei, Qiang Ling",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enconman.2024.118331"
    },
    {
        "id": 24154,
        "title": "Two-Stage Fine-Tuning For Low-resource Englishbased Creole with Pre-Trained LLMs",
        "authors": "Shu Hui Amanda Tan, Eint Sandi Aung, Hayato YAMANA",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csde59766.2023.10487143"
    },
    {
        "id": 24155,
        "title": "Comparative Analysis of Fine-tuning Multiple Pre- Trained Convolutional Neural Network (CNN) Models for Oryza Sativa Disease Detection",
        "authors": "Roky Das, Iqbal Ahmed",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5120/ijca2023923117"
    },
    {
        "id": 24156,
        "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
        "authors": "Ting Jiang, Deqing Wang, Fuzhen Zhuang, Ruobing Xie, Feng Xia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.35"
    },
    {
        "id": 24157,
        "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
        "authors": "Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, Greg Ver Steeg",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.539"
    },
    {
        "id": 24158,
        "title": "Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-Trained Language Models",
        "authors": "Youness Moukafih, Mounir Ghogho, Kamel Smaili",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095108"
    },
    {
        "id": 24159,
        "title": "Knowledge-guided pre-training and fine-tuning: Video representation learning for action recognition",
        "authors": "Guanhong Wang, Yang Zhou, Zhanhao He, Keyu Lu, Yang Feng, Zuozhu Liu, Gaoang Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.127136"
    },
    {
        "id": 24160,
        "title": "Cross-Domain Aspect-Based Sentiment Classification with a Pre-Training and Fine-Tuning Strategy for Low-Resource Domains",
        "authors": "Chuanjun Zhao, Meiling Wu, Xinyi Yang, Xuzhuang Sun, Suge Wang, Deyu Li",
        "published": "2024-4-30",
        "citations": 0,
        "abstract": "Aspect-based sentiment classification (ABSC) is a crucial sub-task of fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspects in a sentence as positive, negative, or neutral. Most existing ABSC methods are based on supervised learning. However, these methods rely heavily on fine-grained labeled training data, which can be scarce in low-resource domains, limiting their effectiveness. To overcome this challenge, we propose a low-resource cross-domain aspect-based sentiment classification (CDABSC) approach based on a pre-training and fine-tuning strategy. This approach applies the pre-training and fine-tuning strategy to an advanced deep learning method designed for ABSC, namely the attention-based encoding graph convolutional network (AEGCN) model. Specifically, a high-resource domain is selected as the source domain, and the AEGCN model is pre-trained using a large amount of fine-grained annotated data from the source domain. The optimal parameters of the model are preserved. Subsequently, a low-resource domain is used as the target domain, and the pre-trained model parameters are used as the initial parameters of the target domain model. The target domain is fine-tuned using a small amount of annotated data to adapt the parameters to the target domain model, improving the accuracy of sentiment classification in the low-resource domain. Finally, experimental validation on two domain benchmark datasets, restaurant and laptop, demonstrates significant outperformance of our approach over the baselines in CDABSC Micro-F1.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3653299"
    },
    {
        "id": 24161,
        "title": "Fine-tuning a pre-trained ResNet50 model to detect distributed  denial of service attack",
        "authors": "Ahmad Sanmorino, Hendra Di Kesuma",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "Distributed denial-of-service (DDoS) attacks pose a significant risk to the dependability and consistency of network services. The utilization of deep learning (DL) models has displayed encouraging outcomes in the identification of DDoS attacks. Nevertheless, crafting a precise DL model necessitates an extensive volume of labeled data and substantial computational capabilities. Within this piece, we introduce a technique to enhance a pre-trained DL model for the identification of DDoS attacks. Our strategy’s efficacy is showcased on an openly accessible dataset, revealing that the fine-tuned model we propose surpasses both the initial pre-trained model and other cutting-edge approaches in performance. The suggested fine-tuned model attained 95.1% accuracy, surpassing the initial pre-trained model as well as other leading-edge techniques. Please note that the specific evaluation metrics and their values may vary depending on the implementation, hyperparameter settings, number of datasets, or dataset characteristics. The proposed approach has several advantages, including reducing the amount of labeled data required and accelerating the training process. Initiating with a pre-existing ResNet50 model can also enhance the eventual model’s accuracy, given that the pre-trained model has already acquired the ability to extract significant features from unprocessed data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/eei.v13i2.7014"
    },
    {
        "id": 24162,
        "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
        "authors": "Jungwoo Heo, Chan-yeong Lim, Ju-ho Kim, Hyun-seo Shin, Ha-Jin Yu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-605"
    },
    {
        "id": 24163,
        "title": "TRUST-SER: On The Trustworthiness Of Fine-Tuning Pre-Trained Speech Embeddings For Speech Emotion Recognition",
        "authors": "Tiantian Feng, Rajat Hebbar, Shrikanth Narayanan",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446616"
    },
    {
        "id": 24164,
        "title": "GamMa: Efficient Fine-Tuning of Pre-Trained Language Models Using Gradient Activation Mapping Masking",
        "authors": "Anchun Gui, Jinqiang Ye, Han Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191351"
    },
    {
        "id": 24165,
        "title": "Confounder balancing in adversarial domain adaptation for pre-trained large models fine-tuning",
        "authors": "Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu, Yukang Lin",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106173"
    },
    {
        "id": 24166,
        "title": "Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts",
        "authors": "Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Zhang, Jun Zhou, Defu Lian, Ying Wei",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.808"
    },
    {
        "id": 24167,
        "title": "Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning",
        "authors": "Kaijie Zhu, Xixu Hu, Jindong Wang, Xing Xie, Ge Yang",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00408"
    },
    {
        "id": 24168,
        "title": "Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation",
        "authors": "Mohammadreza Tayaranian Hosseini, Alireza Ghaffari, Marzieh S. Tahaei, Mehdi Rezagholizadeh, Masoud Asgharian, Vahid Partovi Nia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.143"
    },
    {
        "id": 24169,
        "title": "Fine Tuning GANs for Image Colorization: Effects of Training Data and Epochs",
        "authors": "Rahul Patil, Pramod Patil, Rohit Joshi, Atharv Joshi, Ninad Hastak, Akshay Joshi",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccubea58933.2023.10392275"
    },
    {
        "id": 24170,
        "title": "Breaking the Barrier Between Pre-training and Fine-tuning: A Hybrid Prompting Model for Knowledge-Based VQA",
        "authors": "Zhongfan Sun, Yongli Hu, Qingqing Gao, Huajie Jiang, Junbin Gao, Yanfeng Sun, Baocai Yin",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3612516"
    },
    {
        "id": 24171,
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": "Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.182"
    },
    {
        "id": 24172,
        "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
        "authors": "Rheeya Uppaal, Junjie Hu, Yixuan Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.717"
    },
    {
        "id": 24173,
        "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
        "authors": "Zhong Zhang, Bang Liu, Junming Shao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.95"
    },
    {
        "id": 24174,
        "title": "HiTEK Pre-processing for Speech and Text: NLP",
        "authors": "Naveenkumar T Rudrappa,  , Mallamma V Reddy, M Hanumanthappa",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17485/ijst/v16i19.296"
    },
    {
        "id": 24175,
        "title": "Recyclable Tuning for Continual Pre-training",
        "authors": "Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.723"
    },
    {
        "id": 24176,
        "title": "Fine-Tuning Grape Phytochemistry: Examining the Distinct Influence of Oak Ash and Potassium Carbonate Pre-Treatments on Essential Components",
        "authors": "Ozkan Kaya, Hava Delavar, Fadime Ates, Turhan Yilmaz, Muge Sahin, Nurhan Keskin",
        "published": "2024-1-19",
        "citations": 2,
        "abstract": "Understanding the impact of pre-treatment methods on the phytochemical composition of grapes is essential for optimizing grape quality and producing raisins with desirable characteristics. Therefore, this study meticulously analyzed the impact of two distinct pre-treatment methods, oak ash and potassium carbonate (K2CO3), on the composition of essential phytochemical components in grapes. This research encompassed phenolic compounds, anthocyanins, phenolic acids, flavonoids, and phytoalexins. This study investigates the impact of pre-treatment methods, oak ash and K2CO3, on the phytochemical composition of grapes. Significant differences were observed in anthocyanins, flavonoids, phytoalexins, and phenolic acids between the treatments. Oak ash exhibited advantages in preserving specific compounds, including higher levels of anthocyanins, flavonols, flavones, flavanones, catechins, resveratrol, pterostilbene, and viniferin, compared to K2CO3. Notably, the delphinidin-3-O-glycoside content was significantly higher in the oak ash treatment. An analysis of phenolic compounds revealed distinctions in hydroxycinnamic acids, hydroxybenzoic acids, benzaldehyde, and phenylacetaldehyde. Additionally, gallic acid, vanillic acid, trans-caffeic acid, trans-p-coumaric acid, and (-)-epicatechin were significantly more prevalent in the K2CO3 treatment, while ferulic acid and quercetin were more prevalent in the oak ash treatment. These findings underscore the pivotal role of pre-treatment methods in shaping the phytochemical content of grapes, thus holding critical implications for grape-derived products’ quality and potential health benefits.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/horticulturae10010095"
    },
    {
        "id": 24177,
        "title": "KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models for Clinical Trial Entailment Identification",
        "authors": "Weiqi Wang, Baixuan Xu, Tianqing Fang, Lirong Zhang, Yangqiu Song",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.1"
    },
    {
        "id": 24178,
        "title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained",
        "authors": "Hongliang Dai, Ziqian Zeng",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.126"
    },
    {
        "id": 24179,
        "title": "Fine-Tuning Air Pollution Models",
        "authors": "Sarah Derouin",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "InMAP estimates air pollution within cities, but its predictions are flawed for specific chemicals. Now, scientists are addressing that shortcoming.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1029/2023eo230181"
    },
    {
        "id": 24180,
        "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
        "authors": "Jun Bai, Xiaofeng Zhang, Chen Li, Hanhua Hong, Xi Xu, Chenghua Lin, Wenge Rong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.357"
    },
    {
        "id": 24181,
        "title": "Movie Box Office Prediction Based on Pre-Train and Fine-Tuning",
        "authors": "瑞 赵",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/mos.2024.131034"
    },
    {
        "id": 24182,
        "title": "Reflections of the Alp Figure in Pre-Islamic Turkish Art",
        "authors": "Yunus BERKLİ,  , Yaemin TEPE,  ",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5152/jtri.2023.23281"
    },
    {
        "id": 24183,
        "title": "Enhancing AutoNLP with Fine-Tuned BERT Models: An Evaluation of Text Representation Methods for AutoPyTorch",
        "authors": "Parisa Safikhani, David Broneske",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "Recent advancements in Automated Machine Learning (AutoML) have led to the emergence of Automated Natural Language Processing (AutoNLP), a subfield focused on automating NLP model development. Existing NLP toolkits provide various tools and modules but lack a free AutoNLP version. To this end, architecting the design decisions and tuning knobs of AutoNLP is still essential for enhancing performance in various industries and applications. Therefore, analyzing how different text representation methods affect the performance of AutoML systems is an essential starting point for investigating AutoNLP. In this paper, we present a comprehensive study on the performance of AutoPyTorch, an open-source AutoML framework with various text representation methods for binary text classification tasks. The novelty of our research lies in investigating the impact of different text representation methods on AutoPyTorch’s performance, which is an essential step toward transforming AutoPyTorch to also support AutoNLP tasks. We conduct experiments on five diverse datasets to evaluate the performance of both contextual and noncontextual text representation methods, including onehot encoding, BERT (base uncased), fine-tuned BERT, LSA, and a method with no explicit text representation. Our results reveal that, depending on the tasks, different text representation methods may be the most suitable for extracting features to build a model with AutoPyTorch. Furthermore, the results indicate that fine-tuned BERT models consistently outperform other text representation methods across all tasks. However, during the fine-tuning process, the finetuned model had the advantage of benefiting from labels. Hence, these findings support the notion that integrating fine-tuned models or a model fine-tuned on open source large dataset, including all binary text classification tasks as text representation methods in AutoPyTorch, is a reasonable step toward developing AutoPyTorch for NLP tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131603"
    },
    {
        "id": 24184,
        "title": "Fine-Tuning Pre-Trained Models for Automated Analysis of Ophthalmic Imaging in Diagnosing Eye Diseases",
        "authors": "Rabia Emhamed Al Mamlook, Ahmad Nasayreh, Hasan Gharaibeh, Mohammad Aljaidi, Qais Al-Na'amneh, Dalia Alzu'bi, Hanin Bzizi, Abdelrahman Abdeltawab",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acit58888.2023.10453701"
    },
    {
        "id": 24185,
        "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
        "authors": "Yiwen Tang, Ray Zhang, Zoey Guo, Xianzheng Ma, Bin Zhao, Zhigang Wang, Dong Wang, Xuelong Li",
        "published": "2024-3-24",
        "citations": 1,
        "abstract": "The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggregate point cloud features within spatial neighborhoods to capture fine-grained geometric information through local interactions. Extensive experiments indicate that our Point-PEFT can achieve better performance than the full fine-tuning on various downstream tasks, while using only 5% of the trainable parameters, demonstrating the efficiency and effectiveness of our approach. Code is released at https://github.com/Ivan-Tang-3D/Point-PEFT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i6.28323"
    },
    {
        "id": 24186,
        "title": "Enhancing Pre-Trained ASR System Fine-Tuning for Dysarthric Speech Recognition Using Adversarial Data Augmentation",
        "authors": "Huimeng Wang, Zengrui Jin, Mengzhe Geng, Shujie Hu, Guinan Li, Tianzi Wang, Haoning Xu, Xunying Liu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447702"
    },
    {
        "id": 24187,
        "title": "On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research",
        "authors": "Made Nindyatama Nityasya, Haryo Wibowo, Alham Fikri Aji, Genta Winata, Radityo Eko Prasojo, Phil Blunsom, Adhiguna Kuncoro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.477"
    },
    {
        "id": 24188,
        "title": "Fine-Tuning Pre-Trained Language Model for Urgency Classification on Food Safety Feedback",
        "authors": "Umamaheswari Vasanthakumar, Jia Rui Bryna Goh, Siu Cheung Hui, Kwok Yan Lam, Benjamin Er, Muhd Tarmidzi Fua'di, Kyaw Thu Aung",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciss59129.2023.10291215"
    },
    {
        "id": 24189,
        "title": "Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond",
        "authors": "Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, Hongbin Sun",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597926.3598036"
    },
    {
        "id": 24190,
        "title": "Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data",
        "authors": "Stephen Obadinma, Hongyu Guo, Xiaodan Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.19"
    },
    {
        "id": 24191,
        "title": "The Fine-Tuning Approach for Training Monitoring",
        "authors": "Daniel Boullosa, João Gustavo Claudino, Jaime Fernandez-Fernandez, Daniel Bok, Irineu Loturco, Matthew Stults-Kolehmainen, Juan García-López, Carl Foster",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Purpose: Monitoring is a fundamental part of the training process to guarantee that the programmed training loads are executed by athletes and result in the intended adaptations and enhanced performance. A number of monitoring tools have emerged during the last century in sport. These tools capture different facets (eg, psychophysiological, physical, biomechanical) of acute training bouts and chronic adaptations while presenting specific advantages and limitations. Therefore, there is a need to identify what tools are more efficient in each sport context for better monitoring of training process. Methods and Results: We present and discuss the fine-tuning approach for training monitoring, which consists of identifying and combining the best monitoring tools with experts’ knowledge in different sport settings, designed to improve (1) the control of actual training loads and (2) understanding of athletes’ training adaptations. Instead of using single-tool approaches or merely subjective decision making, the identification of the best combination of monitoring tools to assist experts’ decisions in each specific context (ie, triangulation) is necessary to better understand the link between acute and chronic adaptations and their impact on health and performance. Future studies should elaborate on the identification of the best combination of monitoring tools for each specific sport setting. Conclusion: The fine-tuning monitoring approach requires the simultaneous use of several valid and practical tools, instead of a single tool, to improve the effectiveness of monitoring practices when added to experts’ knowledge.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1123/ijspp.2023-0154"
    },
    {
        "id": 24192,
        "title": "Combining EEG and NLP Features for Predicting Students' Lecture Comprehension Using Ensemble Classification",
        "authors": "Phantharach Natnithikarat, Theerawit Wilaiprasitporn, Supavit Kongwudhikunakorn",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rivf60135.2023.10471794"
    },
    {
        "id": 24193,
        "title": "Fine-Tuning Human for LLM Projects",
        "authors": "Rehan Guha",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4574477"
    },
    {
        "id": 24194,
        "title": "A Moral Fine-Tuning Argument",
        "authors": "Martin Jakobsen",
        "published": "2023-12-24",
        "citations": 0,
        "abstract": "This paper develops Mark D. Linville’s brief description of “a sort of moral fine-tuning argument”. I develop the argument in four ways: I unpack the argument and give it a clear formulation, I unpack the theistic explanation of why a somewhat reliable moral capacity is expected, I point to the significance of not seeking to explain a perfect moral capacity, and I put the argument up against the recent work on non-theistic moral epistemology by Derek Parfit, David Enoch, and Erik Wielenberg.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rel15010031"
    },
    {
        "id": 24195,
        "title": "Gradual Language Model Adaptation Using Fine-Grained Typology",
        "authors": "Marcell Richard Fekete, Johannes Bjerva",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.sigtyp-1.19"
    },
    {
        "id": 24196,
        "title": "How Fine Tuning Affects Contextual Embeddings: A Negative Result Explanation",
        "authors": "Ha-Thanh Nguyen, Vu Tran, Minh-Phuong Nguyen, Le-Minh Nguyen, Ken Satoh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011714200003393"
    },
    {
        "id": 24197,
        "title": "RClassify: Combining NLP and ML to Classify Rules from Requirements Specifications Documents",
        "authors": "Asha Rajbhoj, Padmalata Nistala, Ajim Pathan, Piyush Kulkarni, Vinay Kulkarni",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/re57278.2023.00026"
    },
    {
        "id": 24198,
        "title": "MAE-EEG-Transformer: A transformer-based approach combining masked autoencoder and cross-individual data augmentation pre-training for EEG classification",
        "authors": "Miao Cai, Yu Zeng",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2024.106131"
    },
    {
        "id": 24199,
        "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
        "authors": "Mrigank Raman, Pratyush Maini, J Kolter, Zachary Lipton, Danish Pruthi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.576"
    },
    {
        "id": 24200,
        "title": "CSECU-DSG at SemEval-2023 Task 4: Fine-tuning DeBERTa Transformer Model with Cross-fold Training and Multi-sample Dropout for Human Values Identification",
        "authors": "Abdul Aziz, Md. Akram Hossain, Abu Nowshed Chy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.274"
    },
    {
        "id": 24201,
        "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks",
        "authors": "Chufan Shi, Yixuan Su, Cheng Yang, Yujiu Yang, Deng Cai",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.947"
    },
    {
        "id": 24202,
        "title": "GenGradAttack: Efficient and Robust Targeted Adversarial Attacks Using Genetic Algorithms and Gradient-Based Fine-Tuning",
        "authors": "Naman Agarwal, James Pope",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012314700003636"
    },
    {
        "id": 24203,
        "title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models",
        "authors": "Alireza Ghaffari, Justin Yu, Mahsa Nejad, Masoud Asgharian, Boxing Chen, Vahid Nia",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012567700003654"
    },
    {
        "id": 24204,
        "title": "SynFine: Boosting Image Segmentation Accuracy Through Synthetic Data Generation and Surgical Fine-Tuning",
        "authors": "Mehdi Mounsif, Yassine Motie, Mohamed Benabdelkrim, Florent Brondolo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011848300003411"
    },
    {
        "id": 24205,
        "title": "Multi-Exit Vision Transformer with Custom Fine-Tuning for Fine-Grained Image Recognition",
        "authors": "Tianyi Shen, Chonghan Lee, Vijaykrishnan Narayanan",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222298"
    },
    {
        "id": 24206,
        "title": "Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning",
        "authors": "Yupei Zhang, Li Pan, Qiushi Yang, Tan Li, Zhen Chen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jbhi.2024.3384407"
    },
    {
        "id": 24207,
        "title": "Fine-Tuning BERT-Based Pre-Trained Models for Arabic Dependency Parsing",
        "authors": "Sharefah Al-Ghamdi, Hend Al-Khalifa, Abdulmalik Al-Salman",
        "published": "2023-3-27",
        "citations": 5,
        "abstract": "With the advent of pre-trained language models, many natural language processing tasks in various languages have achieved great success. Although some research has been conducted on fine-tuning BERT-based models for syntactic parsing, and several Arabic pre-trained models have been developed, no attention has been paid to Arabic dependency parsing. In this study, we attempt to fill this gap and compare nine Arabic models, fine-tuning strategies, and encoding methods for dependency parsing. We evaluated three treebanks to highlight the best options and methods for fine-tuning Arabic BERT-based models to capture syntactic dependencies in the data. Our exploratory results show that the AraBERTv2 model provides the best scores for all treebanks and confirm that fine-tuning to the higher layers of pre-trained models is required. However, adding additional neural network layers to those models drops the accuracy. Additionally, we found that the treebanks have differences in the encoding techniques that give the highest scores. The analysis of the errors obtained by the test examples highlights four issues that have an important effect on the results: parse tree post-processing, contextualized embeddings, erroneous tokenization, and erroneous annotation. This study reveals a direction for future research to achieve enhanced Arabic BERT-based syntactic parsing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13074225"
    },
    {
        "id": 24208,
        "title": "Fine-Tuning Pre-Trained CodeBERT for Code Search in Smart Contract",
        "authors": "Huan JIN, Qinying LI",
        "published": "2023-6",
        "citations": 0,
        "abstract": "Smart contracts, which automatically execute on decentralized platforms like Ethereum, require high security and low gas consumption. As a result, developers have a strong demand for semantic code search tools that utilize natural language queries to efficiently search for existing code snippets. However, existing code search models face a semantic gap between code and queries, which requires a large amount of training data. In this paper, we propose a fine-tuning approach to bridge the semantic gap in code search and improve the search accuracy. We collect 80 723 different pairs of <comment, code snippet> from Etherscan.io and use these pairs to fine-tune, validate, and test the pre-trained CodeBERT model. Using the fine-tuned model, we develop a code search engine specifically for smart contracts. We evaluate the Recall@k and Mean Reciprocal Rank (MRR) of the fine-tuned CodeBERT model using different proportions of the fine-tuned data. It is encouraging that even a small amount of fine-tuned data can produce satisfactory results. In addition, we perform a comparative analysis between the fine-tuned CodeBERT model and the two state-of-the-art models. The experimental results show that the fine-tuned CodeBERT model has superior performance in terms of Recall@k and MRR. These findings highlight the effectiveness of our fine-tuning approach and its potential to significantly improve the code search accuracy. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/wujns/2023283237"
    },
    {
        "id": 24209,
        "title": "OPTIMIZING ULTRASOUND IMAGE CLASSIFICATION THROUGH TRANSFER LEARNING: FINE-TUNING STRATEGIES AND CLASSIFIER IMPACT ON PRE-TRAINED INNER-LAYERS",
        "authors": "Mohamed Bal-Ghaoui, My Hachem El Yousfi Alaoui, Abdelilah Jilbab, Abdennaser Bourouhou",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "Transfer Learning (TL) is a popular deep learning technique used in medical image analysis, especially when data is limited. It leverages pre-trained knowledge from State-Of-The-Art (SOTA) models and applies it to specific applications through Fine-Tuning (FT). However, fine-tuning large models can be time-consuming, and determining which layers to use can be challenging. This study explores different fine-tuning strategies for five SOTA models (VGG16, VGG19, ResNet50, ResNet101, and InceptionV3) pre-trained on ImageNet. It also investigates the impact of the classifier by using a linear SVM for classification. The experiments are performed on four open-access ultrasound datasets related to breast cancer, thyroid nodules cancer, and salivary glands cancer. Results are evaluated using a five-fold stratified cross-validation technique, and metrics like accuracy, precision, and recall are computed. The findings show that fine-tuning 15% of the last layers in ResNet50 and InceptionV3 achieves good results. Using SVM for classification further improves overall performance by 6% for the two best-performing models. This research provides insights into fine-tuning strategies and the importance of the classifier in transfer learning for ultrasound image classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35784/iapgos.4464"
    },
    {
        "id": 24210,
        "title": "Enhancing Security in SMS by Combining NLP Models Using Ensemble Learning for Spam Detection with Image Steganography Integration",
        "authors": "Aditya Kumar, C. Fancy",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icecaa58104.2023.10212103"
    },
    {
        "id": 24211,
        "title": "Toward training NLP models to take into account privacy leakages",
        "authors": "Gaspard Berthelier, Antoine Boutet, Antoine Richard",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386735"
    },
    {
        "id": 24212,
        "title": "Natural Language Processing (NLP)-Driven Classification of Pre-Bid Request for Information (RFI)",
        "authors": "Rabin Shrestha, Taewoo Ko, JeeHee Lee",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1061/9780784485224.008"
    },
    {
        "id": 24213,
        "title": "Fine-Tuning of Pre-Trained Deep Face Sketch Models Using Smart Switching Slime Mold Algorithm",
        "authors": "Khaled Mohammad Alhashash, Hussein Samma, Shahrel Azmin Suandi",
        "published": "2023-4-19",
        "citations": 0,
        "abstract": "There are many pre-trained deep learning-based face recognition models developed in the literature, such as FaceNet, ArcFace, VGG-Face, and DeepFace. However, performing transfer learning of these models for handling face sketch recognition is not applicable due to the challenge of limited sketch datasets (single sketch per subject). One promising solution to mitigate this issue is by using optimization algorithms, which will perform a fine-tuning and fitting of these models for the face sketch problem. Specifically, this research introduces an enhanced optimizer that will evolve these models by performing automatic weightage/fine-tuning of the generated feature vector guided by the recognition accuracy of the training data. The following are the key contributions to this work: (i) this paper introduces a novel Smart Switching Slime Mold Algorithm (S2SMA), which has been improved by embedding several search operations and control rules; (ii) the proposed S2SMA aims to fine-tune the pre-trained deep learning models in order to improve the accuracy of the face sketch recognition problem; and (iii) the proposed S2SMA makes simultaneous fine-tuning of multiple pre-trained deep learning models toward further improving the recognition accuracy of the face sketch problem. The performance of the S2SMA has been evaluated on two face sketch databases, which are XM2VTS and CUFSF, and on CEC’s 2010 large-scale benchmark. In addition, the outcomes were compared to several variations of the SMA and related optimization techniques. The numerical results demonstrated that the improved optimizer obtained a higher level of fitness value as well as better face sketch recognition accuracy. The statistical data demonstrate that S2SMA significantly outperforms other optimization techniques with a rapid convergence curve.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13085102"
    },
    {
        "id": 24214,
        "title": "Fine-Tuning and Aligning Question Answering Models for Complex Information Extraction Tasks",
        "authors": "Matthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, Maximilien Kintz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012159000003598"
    },
    {
        "id": 24215,
        "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
        "authors": "Richard Rose, Oscar Chang, Olivier Siohan",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1952"
    },
    {
        "id": 24216,
        "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
        "authors": "Hejung Yang, Hong-Goo Kang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-623"
    },
    {
        "id": 24217,
        "title": "Fine-Tuning Restricted Boltzmann Machines Using No-Boundary Jellyfish",
        "authors": "Douglas Rodrigues, Gustavo Henrique de Rosa, Kelton Pontara da Costa, Danilo Jodas, João Papa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011643400003417"
    },
    {
        "id": 24218,
        "title": "Exploring Parameter-Efficient Fine-Tuning of a Large-Scale Pre-Trained Model for scRNA-seq Cell Type Annotation",
        "authors": "Yuhang Liu, Tianhao Li, Zixuan Wang, Guiquan Zhu, Yongqing Zhang, Quan Zou",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385599"
    },
    {
        "id": 24219,
        "title": "Fine tuning a logical model of cancer cells to predict drug synergies: combining manual curation and automated parameterization",
        "authors": "Åsmund Flobak, John Zobolas, Miguel Vazquez, Tonje S. Steigedal, Liv Thommesen, Asle Grislingås, Barbara Niederdorfer, Evelina Folkesson, Martin Kuiper",
        "published": "2023-11-20",
        "citations": 1,
        "abstract": "Treatment with combinations of drugs carries great promise for personalized therapy for a variety of diseases. We have previously shown that synergistic combinations of cancer signaling inhibitors can be identified based on a logical framework, by manual model definition. We now demonstrate how automated adjustments of model topology and logic equations both can greatly reduce the workload traditionally associated with logical model optimization. Our methodology allows the exploration of larger model ensembles that all obey a set of observations, while being less restrained for parts of the model where parameterization is not guided by biological data. We benchmark the synergy prediction performance of our logical models in a dataset of 153 targeted drug combinations. We show that well-performing manual models faithfully represent measured biomarker data and that their performance can be outmatched by automated parameterization using a genetic algorithm. Whereas the predictive performance of a curated model is strongly affected by simulated curation errors, data-guided deletion of a small subset of regulatory model edges can significantly improve prediction quality. With correct topology we find evidence of some tolerance to simulated errors in the biomarker calibration data, yet performance decreases with reduced data quality. Moreover, we show that predictive logical models are valuable for proposing mechanisms underpinning observed synergies. With our framework we predict the synergy of joint inhibition of PI3K and TAK1, and further substantiate this prediction with observations in cancer cell cultures and in xenograft experiments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fsysb.2023.1252961"
    },
    {
        "id": 24220,
        "title": "Can We Use Probing to Better Understand Fine-Tuning and Knowledge Distillation of the BERT NLU?",
        "authors": "Jakub Hościłowicz, Marcin Sowański, Piotr Czubowski, Artur Janicki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011724900003393"
    },
    {
        "id": 24221,
        "title": "Reducing Bias in Pre-Trained Models by Tuning While Penalizing Change",
        "authors": "Niklas Penzel, Gideon Stein, Joachim Denzler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012345800003660"
    },
    {
        "id": 24222,
        "title": "Efficient Fine Tuning for Fashion Object Detection",
        "authors": "Benjiang Ma, Wenjin Xu",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "Pre-trained models have achieved success in object detection. However, challenges remain due to dataset noise and lack of domain-specific data, resulting in weaker zero-shot capabilities in specialized fields such as fashion imaging. We addressed this by constructing a novel clothing object detection benchmark, Garment40K, which includes more than 140,000 human images with bounding boxes and over 40,000 clothing images. Each clothing item within this dataset is accompanied by its corresponding category and textual description. The dataset covers 2 major categories, pants and tops, which are further divided into 15 fine-grained subclasses, providing a rich and high-quality clothing resource. Leveraging this dataset, we propose an efficient fine-tuning method based on the Grounding DINO framework to tackle the issue of missed and false detections of clothing targets. This method incorporates additional similarity loss constraints and adapter modules, leading to a significantly enhanced model named Improved Grounding DINO. By fine-tuning only a small number of additional adapter module parameters, we considerably reduced computational costs while achieving performance comparable to full parameter fine tuning. This allows our model to be conveniently deployed on a variety of low-cost visual sensors. Our Improved Grounding DINO demonstrates considerable performance improvements in computer vision applications in the clothing domain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23136083"
    },
    {
        "id": 24223,
        "title": "Fine-grained Text Style Transfer with Diffusion-Based Language Models",
        "authors": "Yiwei Lyu, Tiange Luo, Jiacheng Shi, Todd Hollon, Honglak Lee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.6"
    },
    {
        "id": 24224,
        "title": "A Fine-Tuning Aggregation Convolutional Neural Network Surrogate Model of Strategy Selecting Mechanism for Repeated-Encounter Bilateral Automated Negotiation",
        "authors": "Shengbo Chang, Katsuhide Fujita",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011701300003393"
    },
    {
        "id": 24225,
        "title": "CrudeBERT: Applying Economic Theory Towards Fine-Tuning Transformer-Based Sentiment Analysis Models to the Crude Oil Market",
        "authors": "Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011749600003467"
    },
    {
        "id": 24226,
        "title": "Fine-tuning fungal effector secretion",
        "authors": "Diane G. O. Saunders",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41564-023-01456-1"
    },
    {
        "id": 24227,
        "title": "Fine-tuning is not (always) overfitting artifacts",
        "authors": "Jérémie Bogaert, Emmanuel Jean, Cyril de Bodt, François-Xavier Standaert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-152"
    },
    {
        "id": 24228,
        "title": "Divine psychology and cosmic fine-tuning",
        "authors": "Miles K. Donahue",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "Abstract\nAfter briefly outlining the fine-tuning argument (FTA), I explain how it relies crucially on the claim that it is not improbable that God would design a fine-tuned universe. Against this premise stands the divine psychology objection: the contention that the probability that God would design a fine-tuned universe is inscrutable. I explore three strategies for meeting this objection: (i) denying that the FTA requires any claims about divine psychology in the first place, (ii) defining the motivation and intention to design a fine-tuned universe into the theistic hypothesis, and (iii) providing arguments that the relevant probability is not terribly low. While I reject the first two, I conclude, in line with the third, that considerations about life's objective value establish that it is not absurdly improbable that God would design a fine-tuned universe, whether one regards the FTA as an inference merely to a cosmic designer, or to theism proper. Accordingly, the divine psychology objection fails.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/s0034412524000088"
    },
    {
        "id": 24229,
        "title": "Gradient Sparsification For Masked Fine-Tuning of Transformers",
        "authors": "James O'Neill, Sourav Dutta",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191206"
    },
    {
        "id": 24230,
        "title": "ファインチューニングによる作業認識モデル学習の教師データ量削減",
        "authors": "Masato Sabanai, Katsuhiro Kusano, Shogo Shimizu, Takayuki Kodaira",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1299/jsmemsd.2023.203"
    },
    {
        "id": 24231,
        "title": "Fine tuning an atomic mesh",
        "authors": "Jiwoong Park",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "Controlling the angle in atomic meshes could result in quantum properties on demand",
        "keywords": "",
        "link": "http://dx.doi.org/10.1126/science.adj1420"
    },
    {
        "id": 24232,
        "title": "Data Augmentation and Fine Tuning of Convolutional Neural Network during Training for Person Re-Identification in Video Surveillance Systems",
        "authors": "S. Ye, R. Bohush, H. Chen, S. Ihnatsyeva, S. V. Ablameyko",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23040124"
    },
    {
        "id": 24233,
        "title": "Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization",
        "authors": "Bashar Al-Rfooh, Gheith Abandah, Rami Al-Rfou",
        "published": "2023-5-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jeeit58638.2023.10185725"
    },
    {
        "id": 24234,
        "title": "Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses",
        "authors": "David Beauchemin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.3"
    },
    {
        "id": 24235,
        "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values",
        "authors": "Arnav Arora, Lucie-aimée Kaffee, Isabelle Augenstein",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.c3nlp-1.12"
    },
    {
        "id": 24236,
        "title": "Synergistic Combinatorial Strategy for Combating Antimicrobial  Resistance (AMR) in Clinical Bacteria by Combining Antibiotics with  Plant Extracts",
        "authors": "Mathew Gideon, Zakari Ladan",
        "published": "2023-1-17",
        "citations": 1,
        "abstract": "Bacteria resistance to antibiotics used for the treatment of infections and diseases is of global concern. Medicinal plants have been used as the primary sources of plants' active ingredients or lead compounds in drug development. The combination of various antimicrobial agents to obtain a synergistic effect is considered an ideal strategy for combating bacteria resistance. In this work, a constant repetitive synergy in all combinations was achieved by adding 0.3 mL of concentrated tetraoxosulphate (vi) acid, H2SO4 in a mixture of Calotropis procera extract separately with (a) 1 mg/mL Amoxicillin, (b) 1 mg/mL Ampicillin, (c) 100 µg/mL Azithromycin and (d) 100 µg/mL Ampicillin and were heated at 110 °C for 20 minutes. Higher zones of inhibitions were observed at 16.7 mm for Salmonella  spp, 16.4 mm for Shigella spp, 16.8 mm for Staphylococcus aureus, 21.3 mm for Escherichia coli and 22.4 mm for Streptococcus spp in situations where the antibiotics alone zone of inhibition was 0 mm at the same concentration of a, b, c, and d. These increase the regular probability model of obtaining synergism in plant extracts combination with antibiotics as shown by multiple literatures from 33% to 66% at antibiotic concentration of 100 µg/mL and 100% at antibiotic concentration of 1 mg/mL. The validation process using Piliostigma reticulatum extract shows that a volume of 0.1 mL of concentrated tetraoxosulphate (vi) acid in 2 mL of the mixture was enough to induce synergism to combat bacteria resistance. This work shows a cost-effective method where the antimicrobial activity of ineffective antibiotics can be enhanced and optimized using plant extracts. It can also be explored and applied in different ways to identify novel compounds, and isolate and purify their active principles for selectivity, efficacy, safety and their development as clinical trial candidates in antiviral and anticancer research to overcome enormous health challenges.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37256/fce.4120232071"
    },
    {
        "id": 24237,
        "title": "Focusing on Fine-Tuning: Understanding the Four Pathways for Shaping Generative AI",
        "authors": "Paul Ohm",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4738261"
    },
    {
        "id": 24238,
        "title": "Fine Tuning Named Entity Extraction Models for the Fantasy Domain",
        "authors": "Aravinth Sivaganeshan, Nisansa De Silva",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mercon60487.2023.10355501"
    },
    {
        "id": 24239,
        "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations",
        "authors": "Salah Zaiem, Titouan Parcollet, Slim Essid",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1040"
    },
    {
        "id": 24240,
        "title": "FINE-TUNING SYNAPTIC STRENGTHS IN LOCAL CIRCUITS",
        "authors": "Yukiko Goda",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ibneur.2023.08.2049"
    },
    {
        "id": 24241,
        "title": "Fine-Tuning of Conditional Transformers Improves the Generation of Functionally Characterized Proteins",
        "authors": "Marco Nicolini, Dario Malchiodi, Alberto Cabri, Emanuele Cavalleri, Marco Mesiti, Alberto Paccanaro, Peter Robinson, Justin Reese, Elena Casiraghi, Giorgio Valentini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012567900003657"
    },
    {
        "id": 24242,
        "title": "Fine-Tuning ChemBERTa-2 for Aqueous Solubility Prediction",
        "authors": "Andrew SID Lang",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31031/acsr.2023.04.000578"
    },
    {
        "id": 24243,
        "title": "Zelda Rose: a tool for hassle-free training of transformer models",
        "authors": "Loïc Grobol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.6"
    },
    {
        "id": 24244,
        "title": "Dual Parameter-Efficient Fine-Tuning for Speaker Representation Via Speaker Prompt Tuning and Adapters",
        "authors": "Zhe Li, Man-Wai Mak, Helen Mei-Ling Meng",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447795"
    },
    {
        "id": 24245,
        "title": "Beyond Fine-Tuning: Efficient and Effective Fed-Tuning for Mobile/Web Users",
        "authors": "Bingyan Liu, Yifeng Cai, Hongzhe Bi, Ziqi Zhang, Ding Li, Yao Guo, Xiangqun Chen",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543507.3583212"
    },
    {
        "id": 24246,
        "title": "AAST-NLP at ArAIEval Shared Task: Tackling Persuasion technique and Disinformation Detection using Pre-Trained Language Models On Imbalanced Datasets",
        "authors": "Ahmed El-Sayed, Omar Nasr, Noureldin Elmadany",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.56"
    },
    {
        "id": 24247,
        "title": "Some Useful Things to Know When Combining IR and NLP: The Easy, the Hard and the Ugly",
        "authors": "Omar Alonso, Kenneth Church",
        "published": "2024-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3616855.3636452"
    },
    {
        "id": 24248,
        "title": "Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model",
        "authors": "Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.26"
    },
    {
        "id": 24249,
        "title": "Some Useful Things to Know When Combining IR and NLP: the Easy, the Hard and the Ugly",
        "authors": "Omar Alonso, Kenneth Church",
        "published": "2023-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583780.3615295"
    },
    {
        "id": 24250,
        "title": "FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training",
        "authors": "Yunpeng Han, Lisai Zhang, Qingcai Chen, Zhijian Chen, Zhonghua Li, Jianxin Yang, Zhao Cao",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01443"
    },
    {
        "id": 24251,
        "title": "Fine-Tuning GPT-3 for Synthetic Danish News Generation",
        "authors": "Mina Almasi, Anton Schiønning",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.4"
    },
    {
        "id": 24252,
        "title": "grafzahl: fine-tuning Transformers for\ntext data from within R",
        "authors": " Chung-hong Chan",
        "published": "2023-1-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5117/ccr2023.1.003.chan"
    },
    {
        "id": 24253,
        "title": "Whispered Tuning: Data Privacy Preservation in Fine-Tuning LLMs through Differential Privacy",
        "authors": "Tanmay Singh, Harshvardhan Aditya, Vijay K. Madisetti, Arshdeep Bahga",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4236/jsea.2024.171001"
    },
    {
        "id": 24254,
        "title": "Búsqueda avanzada en Pubmed: Afinando.",
        "authors": " Manuel Molina",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "Se establece la metodología de la búsqueda avanzada en Pubmed, mucho más específica que la búsqueda simple desde la caja de texto.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30445/rear.v15i8.1157"
    },
    {
        "id": 24255,
        "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
        "authors": "Heng-Jui Chang, Alexander H. Liu, James Glass",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-847"
    },
    {
        "id": 24256,
        "title": "Enhancing Transfer Learning Reliability via Block-Wise Fine-Tuning",
        "authors": "Basel Barakat, Qiang Huang",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00064"
    },
    {
        "id": 24257,
        "title": "Multi-modal Contrastive-Generative Pre-training for Fine-grained Skin Disease Diagnosis",
        "authors": "Liangdi Ma, Jun Zhao, Guoxin Wang, Yuchen Guo, Feng Xu",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385898"
    },
    {
        "id": 24258,
        "title": "Enhancing pre-trained contextual embeddings with triplet loss as an effective fine-tuning method for extracting clinical features from electronic health record derived mental health clinical notes",
        "authors": "Deepali Kulkarni, Abhijit Ghosh, Amey Girdhari, Shaomin Liu, L. Alexander Vance, Melissa Unruh, Joydeep Sarkar",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100045"
    },
    {
        "id": 24259,
        "title": "Recognizing Learner Handwriting Retaining Orthographic Errors for Enabling Fine-Grained Error Feedback",
        "authors": "Christian Gold, Ronja Laarmann-Quante, Torsten Zesch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.28"
    },
    {
        "id": 24260,
        "title": "Fine-Tuning BERT for Aspect Extraction in Multi-domain ABSA",
        "authors": "Arwa Akram, Aliea Sabir",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v47i9.5217"
    },
    {
        "id": 24261,
        "title": "Fine-tuning a pre-trained Transformers-based model for gene name entity recognition in biomedical text using a customized dataset: case of Desulfovibrio vulgaris Hildenborough",
        "authors": "Alain Bertrand Bomgni, Dialo Abdala, Bichar Dip Shrestha Gurung, Marcellin Julius Nkenlifack, Venkataramana Gadhamshetty, Z. Etienne Gnimpieba",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385403"
    },
    {
        "id": 24262,
        "title": "Training for Grammatical Error Correction Without Human-Annotated L2 Learners’ Corpora",
        "authors": "Mikio Oda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.38"
    },
    {
        "id": 24263,
        "title": "RETUYT-InCo at BEA 2023 Shared Task: Tuning Open-Source LLMs for Generating Teacher Responses",
        "authors": "Alexis Baladón, Ignacio Sastre, Luis Chiruzzo, Aiala Rosá",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.61"
    },
    {
        "id": 24264,
        "title": "Domain-Specific Language Model Post-Training for Indonesian Financial NLP",
        "authors": "Ni Putu Intan Maharani, Ayu Purwarianti, Yoga Yustiawan, Fauzy Caesar Rochim",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceei59426.2023.10346625"
    },
    {
        "id": 24265,
        "title": "Analyzing Pre-trained and Fine-tuned Language Models",
        "authors": "Marius Mosbach",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bigpicture-1.10"
    },
    {
        "id": 24266,
        "title": "Boosting Fine-Tuning Via Conditional Online Knowledge Transfer",
        "authors": "Zhiqiang Liu, Yuhong Li, Chengkai Huang, Yanxia Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4374629"
    },
    {
        "id": 24267,
        "title": "A deep learning seismic processing workflow through a pretraining and fine-tuning framework",
        "authors": "R. Harsuko, T. Alkhalifah",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310257"
    },
    {
        "id": 24268,
        "title": "Symmetric Fine-Tuning for Improving Few-Shot Object Detection",
        "authors": "Emmanouil Mpampis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371859"
    },
    {
        "id": 24269,
        "title": "Tangent Model Composition for Ensembling and Continual Fine-tuning",
        "authors": "Tian Yu Liu, Stefano Soatto",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01712"
    },
    {
        "id": 24270,
        "title": "Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection",
        "authors": "Yunze Xiao, Firoj Alam",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.58"
    },
    {
        "id": 24271,
        "title": "Fine-Tuning of the Quasi-Bound $$K^- pp$$ State",
        "authors": "N. V. Shevchenko",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00601-024-01901-9"
    },
    {
        "id": 24272,
        "title": "Development of pre-trained language models for clinical NLP in Spanish",
        "authors": "Claudio Aracena, Jocelyn Dunstan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-srw.5"
    },
    {
        "id": 24273,
        "title": "unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network",
        "authors": "Tarek Saier, Johan Krause, Michael Färber",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jcdl57899.2023.00020"
    },
    {
        "id": 24274,
        "title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
        "authors": "Aleksa Cvetanović, Predrag Tadić",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/telfor59449.2023.10372792"
    },
    {
        "id": 24275,
        "title": "Reckless in Tort: Interstitial Law as Doctrinal Fine-Tuning",
        "authors": "Benjamin C. Zipursky, John C. P. Goldberg",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4591130"
    },
    {
        "id": 24276,
        "title": "Instagram Text Sentiment Analysis Combining Machine Learning and NLP",
        "authors": "Chia-Pang Chan, Jun-He Yang",
        "published": "2023-7-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3606370"
    },
    {
        "id": 24277,
        "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning",
        "authors": "Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, Songfang Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.107"
    },
    {
        "id": 24278,
        "title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers",
        "authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.40"
    },
    {
        "id": 24279,
        "title": "CLIP-FG:Selecting Discriminative Image Patches by Contrastive Language-Image Pre-Training for Fine-Grained Image Classification",
        "authors": "Min Yuan, Ningning Lv, Yufei Xie, Fuxiang Lu, Kun Zhan",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10223197"
    },
    {
        "id": 24280,
        "title": "Fine-Tuning Languages: Epistemological Foundations for Ethical AI in Journalism",
        "authors": "Laurence Dierickx, Carl-Gustav Lindén",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sds57534.2023.00013"
    },
    {
        "id": 24281,
        "title": "TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models",
        "authors": "Amrit Nagarajan, Anand Raghunathan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.782"
    },
    {
        "id": 24282,
        "title": "An Extensive Analysis and Fine-Tuning of Gmapping’s Initialization Parameters",
        "authors": "",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2023.0630.10"
    },
    {
        "id": 24283,
        "title": "Fine-tuning and the Afterlife in Aquinas",
        "authors": "Mirela Oliva",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "Does the fine-tuning of the universe for life continue in the afterlife? Aquinas would answer yes. In his view, the cosmic conditions post-apocalypse are set to support the resurrected body and the sensible knowledge of God’s majesty as reflected in the renewed material creature. The renewed universe is, thus, fine-tuned for immortal human life. In the first part, I present Aquinas’ version of fine-tuning, referring to earthly life and the afterlife. I distinguish between two modes of fine-tuning: organic and cognitive. In the second part, I analyze the characteristics of the universe post-apocalypse in connection with the resurrected body in general and the qualities of the blessed body in particular.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17990/rpf/2023_79_1_0233"
    },
    {
        "id": 24284,
        "title": "Prediction of Author’s Profile basing on Fine-Tuning BERT model",
        "authors": "Bassem Bsir, Nabil Khoufi, Mounir Zrigui",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v48i1.4839"
    },
    {
        "id": 24285,
        "title": "Data-Free Backbone Fine-Tuning for Pruned Neural Networks",
        "authors": "Adrian Holzbock, Achyut Hegde, Klaus Dietmayer, Vasileios Belagiannis",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10290102"
    },
    {
        "id": 24286,
        "title": "FINE TUNING OF THE SIBELIUS HALL STAGE ACOUSTICS",
        "authors": "P LEHTO, H MOLLER, J PATYNEN, J GOMEZ BOLANOS, P LAUKKANEN, S VEHVILAINEN",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25144/16022"
    },
    {
        "id": 24287,
        "title": "Co-Incrementation: Combining Co-Training and Incremental Learning for Subject-Specific Facial Expression Recognition",
        "authors": "Jordan Gonzalez, Thibault Geoffroy, Aurelia Deshayes, Lionel Prevost",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011635200003411"
    },
    {
        "id": 24288,
        "title": "AN EXERGAME COMBINING STRENGTH, BALANCE AND COGNITIVE TRAINING WITH PELVIC FLOOR MUSCLE EXERCISES TO TREAT OLDER WOMEN WITH URGENCY URINARY INCONTINENCE: A PRE-POST PILOT STUDY",
        "authors": "S Mont-Briant, V Guimarães, E de Bruin, J de Jong, N Swinnen, M Thalmann, C Dumoulin",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cont.2023.100945"
    },
    {
        "id": 24289,
        "title": "Zero-shot language extension for dialogue state tracking via pre-trained models and multi-auxiliary-tasks fine-tuning",
        "authors": "Lu Xiang, Yang Zhao, Junnan Zhu, Yu Zhou, Chengqing Zong",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.110015"
    },
    {
        "id": 24290,
        "title": "Fine-Tuning Should Make Us More Confident that Other Universes Exist",
        "authors": "Bradford Saad",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "Abstract\nThis paper defends the view that discovering that our universe is fine-tuned should make us more confident that other universes exist. My defense exploits a distinction between ideal and non-ideal evidential support. I use that distinction in concert with a simple model to disarm the most influential objection—the this-universe objection—to the view that fine-tuning supports the existence of other universes. However, the simple model fails to capture some important features of our epistemic situation with respect to fine-tuning. To capture these features, I introduce a more sophisticated model. I then use the more sophisticated model to show that, even once those complicating factors are taken into account, fine-tuning should boost our confidence in the existence of other universes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5406/21521123.61.1.03"
    },
    {
        "id": 24291,
        "title": "T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics",
        "authors": "Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.1014"
    },
    {
        "id": 24292,
        "title": "Concept-wise Fine-tuning Matters in Preventing Negative Transfer",
        "authors": "Yunqiao Yang, Long-Kai Huang, Ying Wei",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01719"
    },
    {
        "id": 24293,
        "title": "Life, the Multiverse, and Fine-Tuning",
        "authors": "Phillip Helbig",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10701-023-00732-8"
    },
    {
        "id": 24294,
        "title": "Incremental Few-Shot Object Detection via Simple Fine-Tuning Approach",
        "authors": "Tae-Min Choi, Jong-Hwan Kim",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160283"
    },
    {
        "id": 24295,
        "title": "Low-Dose Interleukin-2 Therapy: Fine-tuning Treg in Solid Organ Transplantation?",
        "authors": "Leila Amini, Jaspal Kaeda, Olaf Weber, Petra Reinke",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "Regulatory T cells (Treg), a subset of CD4+ T cells, are potent regulators of immune reactions, which have been shown to be a promising therapeutic alternative to toxic immunosuppressive drugs. Data support the utility of Treg in managing immunopathologies, including solid organ transplant rejection, graft-versus-host disease, and autoimmune disorders. Notably, reports suggest that interleukin-2 (IL-2) is critical to survival of Treg, which constitutively express high levels of CD25, that is, the IL-2 receptor α-chain, and are exquisitely sensitive to IL-2, even at very low concentrations in contrast to effector T cells, which only upregulate IL-2 receptor α-chain on activation. This has led to the notion of using low doses of exogenous IL-2 therapeutically to modulate the immune system, specifically Treg numbers and function. Here, we summarize developments of clinical experience with low-dose IL-2 (LD-IL-2) as a therapeutic agent. So far, no clinical data are available to support the therapeutic use of LD-IL-2 therapy in the solid organ transplant setting. For the latter, fine-tuning by biotechnological approaches may be needed because of the narrow therapeutic window and off-target effects of LD-IL-2 therapy and so to realize the therapeutic potential of this molecule.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/tp.0000000000004866"
    },
    {
        "id": 24296,
        "title": "Large-Scale Insect Detection With Fine-Tuning YOLOX",
        "authors": "Thanh-Nghi Doan",
        "published": "2023-6-21",
        "citations": 1,
        "abstract": "With the aim of detecting insect pests at an early stage, there has been an increasing demand for insect pest detection and classification, particularly in large-scale setups. Therefore, the aim of this research is to introduce a new real-time pest detection technique using a deep convolutional neural network, which not only offers improved accuracy but also faster speed and less computational effort. The networks were constructed using various modern object detector models such as YOLOv4, YOLOv5, and YOLOX. Our proposed networks were evaluated on a standard large-scale insect pest dataset, IP102, as well as on our collected dataset, Insect10. The experimental results demonstrate that our system surpasses previous methods and achieves satisfactory performance with 84.84% mAP on the Insect10 dataset and 54.19% mAP on the IP102 dataset. Our system can deliver precise and real-time pest detection and identification for agricultural crops, enabling highly accurate end-to-end pest detection that can be applied in realistic farming scenarios.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15379/ijmst.v10i2.1306"
    },
    {
        "id": 24297,
        "title": "Avoiding parameter fine-tuning in mass varying neutrino models of DE?",
        "authors": "Michael Maziashvili, Vakhtang Tsintsabadze",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.astropartphys.2023.102901"
    },
    {
        "id": 24298,
        "title": "Multi-task fine-tuning for generating keyphrases in a scientific domain",
        "authors": "Anna Glazkova, Dmitry Morozov",
        "published": "2023-4-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itnt57377.2023.10139061"
    },
    {
        "id": 24299,
        "title": "Frequency extension of radio propagation model using fine-tuning",
        "authors": "Tatsuya Nagao, Takahiro Hayashi",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/comex.2023xbl0080"
    },
    {
        "id": 24300,
        "title": "$$\\cal{Y}$$-Tuning: an efficient tuning paradigm for large-scale pre-trained models via label representation learning",
        "authors": "Yitao Liu, Chenxin An, Xipeng Qiu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-3131-8"
    },
    {
        "id": 24301,
        "title": "A Performance Comparison of NLP Text Pre-Processing Techniques for Analysing Personality Based on Myers Briggs Type Indicator (MBTI)",
        "authors": "Muhammad Haziq Abdul Rauf, Elly Johana Johan, Zalilah Abd Aziz",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoco59262.2023.10397603"
    },
    {
        "id": 24302,
        "title": "Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models",
        "authors": "Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R. Varshney, Lav R. Varshney, Payel Das",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "We introduce equi-tuning, a novel fine-tuning method that transforms (potentially non-equivariant) pretrained models into group equivariant models while incurring minimum L_2 loss between the feature representations of the pretrained and the equivariant models. Large pretrained models can be equi-tuned for different groups to satisfy the needs of various downstream tasks. Equi-tuned models benefit from both group equivariance as an inductive bias and semantic priors from pretrained models. We provide applications of equi-tuning on three different tasks: image classification, compositional generalization in language, and fairness in natural language generation (NLG). We also provide a novel group-theoretic definition for fairness in NLG. The effectiveness of this definition is shown by testing it against a standard empirical method of fairness in NLG. We provide experimental results for equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and Densenet for image classification; RNNs, GRUs, and LSTMs for compositional generalization; and GPT2 for fairness in NLG. We test these models on benchmark datasets across all considered tasks to show the generality and effectiveness of the proposed method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25832"
    },
    {
        "id": 24303,
        "title": "Measuring the Instability of Fine-Tuning",
        "authors": "Yupei Du, Dong Nguyen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.342"
    },
    {
        "id": 24304,
        "title": "Role of gene–gene loops in fine-tuning cross-regulation",
        "authors": "Petra Gross",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41588-024-01728-4"
    }
]