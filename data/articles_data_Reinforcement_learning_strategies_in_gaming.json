[
    {
        "id": 7105,
        "title": "Mimicking Electronic Gaming Machine Player Behavior Using Reinforcement Learning",
        "authors": "Gaurav Jariwala, Vlado Keselj",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21428/594757db.6b6b324c"
    },
    {
        "id": 7106,
        "title": "Deep reinforcement learning based resource allocation in edge-cloud gaming",
        "authors": "Iryanto Jaya, Yusen Li, Wentong Cai",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18337-2"
    },
    {
        "id": 7107,
        "title": "Deep-Q-Based Reinforcement Learning Method to Predict Accuracy of Atari Gaming Set Classification",
        "authors": "N Gobinathan, R Ponnusamy",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452644"
    },
    {
        "id": 7108,
        "title": "Vocabulary learning strategies in extramural English gaming and their relationship with vocabulary knowledge",
        "authors": "Raees Calafato, Tobias Clausen",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/09588221.2024.2328023"
    },
    {
        "id": 7109,
        "title": "Research on Data-driven College English Teaching Model Based on Reinforcement Learning and Virtual Reality through Online Gaming",
        "authors": "Jin Yan",
        "published": "2023-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s5.197-210"
    },
    {
        "id": 7110,
        "title": "An adaptive reinforcement learning-based multimodal data fusion framework for human–robot confrontation gaming",
        "authors": "Wen Qi, Haoyu Fan, Hamid Reza Karimi, Hang Su",
        "published": "2023-7",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.04.043"
    },
    {
        "id": 7111,
        "title": "Deep Reinforcement Learning Based Optimization and Risk Control of Trading Strategies",
        "authors": "Mengrui Bao",
        "published": "2024-4-13",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) based optimization leverages advanced machine learning techniques to solve complex decision-making problems in various domains.  Deep learning in trading involves the application of sophisticated neural network architectures to analyze financial data and make predictions in stock markets, forex, commodities, and other trading domains. Deep learning algorithms, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), traders can extract valuable insights from vast amounts of historical market data, including price movements, trading volumes, and market sentiment. These models can learn complex patterns and relationships within the data, enabling them to forecast future market trends, identify potential trading opportunities, and manage risks more effectively. Deep learning in trading has shown promise in improving decision-making processes, enhancing trading strategies, and achieving higher returns, although challenges such as data scarcity, model interpretability, and overfitting remain areas of ongoing research and development. In this paper evaluated the stock market to achieve the financial gain to achieve the significant improvement in prediction with the stock trading. The data related to the stock market are optimized with stock trend data of 7,000 stocks situated in the United States. The estimated stock trade data were computed and processed with the Multiagent Q-learning model for the data detection and classification. A deep Q-network is developed to adaptively pick distinct action subsets and train the market making agent based on the inventory states. The experimental findings demonstrate the effectiveness of our suggested methodology in feature learning and superiority in accuracy improvement when compared to deep learning-based modelling of frequency trading patterns and standard signal processing approaches for stock trend prediction. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jes.1943"
    },
    {
        "id": 7112,
        "title": "Optimization of Charging Strategies for New Energy Vehicles Based on Reinforcement Learning Algorithms",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23977/jaip.2024.070112"
    },
    {
        "id": 7113,
        "title": "Efficient Dimensionality Reduction Strategies for Quantum Reinforcement Learning",
        "authors": "Eva Andrés, M. P. Cuéllar, G. Navarro",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3318173"
    },
    {
        "id": 7114,
        "title": "Research on Reinforcement Learning Explainable Strategies Based on Advantage Saliency",
        "authors": "Dandan Yan",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "Deep reinforcement learning is increasingly being used in difficult environments with sparse rewards and high-dimensional inputs, and it performs well, but its decision-making processes are largely unclear and difficult to explain to end users. Saliency map methods explain an agent's behavior by highlighting state features relevant for the agent to take an action. In this paper, we use the perturbation-based saliency map method, propose the use of advantage function to replace the existing method of calculating state saliency, realize the combination of advantage function and perturbation-based saliency map. A saliency map is generated by noting the saliency of the dependent elements of the agent's chosen action in the Atari game environment. Experimental comparisons show that our method generates more accurate explanatory saliency maps.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v3i1.6348"
    },
    {
        "id": 7115,
        "title": "Reward poisoning attacks in deep reinforcement learning based on exploration strategies",
        "authors": "Kanting Cai, Xiangbin Zhu, Zhaolong Hu",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126578"
    },
    {
        "id": 7116,
        "title": "REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES, 53-57. SI",
        "authors": "Abhishek Kumar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2316/j.2023.201-0347"
    },
    {
        "id": 7117,
        "title": "Adaptive Cyber Defense Technique Based on Multiagent Reinforcement Learning Strategies",
        "authors": "Adel Alshamrani, Abdullah Alshahrani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.032835"
    },
    {
        "id": 7118,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 7119,
        "title": "A Deep Reinforcement Learning Technique for PNPSC Net Player Strategies",
        "authors": "Edwin Michael Bearss, Mikel Petty",
        "published": "2023-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3564746.3587011"
    },
    {
        "id": 7120,
        "title": "Impact of Reinforcement Strategies on Students’ Learning Behaviors in Classroom at Primary Level",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47205/jdss.2023(4-iv)25"
    },
    {
        "id": 7121,
        "title": "Decision Support System for Optimizing Tactics and Strategies of Sports Competition Using Reinforcement Learning Algorithm",
        "authors": "Zhiyong Xu",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "This paper presented an augmented reality secure edge machine learning dynamic optimization model for detecting volleyball movement recognition using a decision support system. The proposed Edge Augmented Decision Support System Blockchain (EdgeAu-DSSBC) model aims to address the limitations of centralized machine learning models, such as high latency, network congestion, and data privacy concerns, by utilizing edge computing and dynamic optimization techniques. The proposed EdgeAu-DSSBC system consists of two main components: an augmented reality interface that allows users to interact with the system and an edge machine learning algorithm that performs the recognition task. The system uses dynamic optimization techniques to optimize the parameters of the machine learning algorithm in real time based on the feedback received from the users. The decision support system provides additional guidance to the users and helps them to make informed decisions based on the recognition results. The EdgeAu-DSSBC model was evaluated using a dataset of volleyball movement videos and compared to existing centralized and edge-based machine-learning models. The experimental results demonstrate that the EdgeAu-DSSBC exhibits improved performance with an accuracy of 99%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jes.1304"
    },
    {
        "id": 7122,
        "title": "Multisite gaming streaming optimization over virtualized 5G environment using Deep Reinforcement Learning techniques",
        "authors": "Alberto del Rio, Javier Serrano, David Jimenez, Luis M. Contreras, Federico Alvarez",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2024.110334"
    },
    {
        "id": 7123,
        "title": "Deep Reinforcement Learning Based Rendering Service Placement for Cloud Gaming in Mobile Edge Computing Systems",
        "authors": "Yongqiang Gao, Zhihan Li",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00073"
    },
    {
        "id": 7124,
        "title": "Sentiment Driven Reinforcement Learning Trading Strategies to Enhance Market Performance",
        "authors": "Rajesh Rohilla, Raaghav Raj Maiya, Ritvik Bharti",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307400"
    },
    {
        "id": 7125,
        "title": "Escape Route Strategies in Complex Emergency Situations using Deep Reinforcement Learning",
        "authors": "Tim Wächter, Jan Rexilius, Matthias König",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ie57519.2023.10179101"
    },
    {
        "id": 7126,
        "title": "Comparisons of Different Dynamic Game Reinforcement Learning Optimal Strategies for A Non-holonomic Vehicle",
        "authors": "Lingyi Xu, Zoran Gajic",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss59072.2024.10480181"
    },
    {
        "id": 7127,
        "title": "Finding individual strategies for storage units in electricity market models using deep reinforcement learning",
        "authors": "Nick Harder, Anke Weidlich, Philipp Staudt",
        "published": "2023-10-19",
        "citations": 1,
        "abstract": "AbstractModeling energy storage units realistically is challenging as their decision-making is not governed by a marginal cost pricing strategy but relies on expected electricity prices. Existing electricity market models often use centralized rule-based bidding or global optimization approaches, which may not accurately capture the competitive behavior of market participants. To address this issue, we present a novel method using multi-agent deep reinforcement learning to model individual strategies in electricity market models. We demonstrate the practical applicability of our approach using a detailed model of the German wholesale electricity market with a complete fleet of pumped hydro energy storage units represented as learning agents. We compare the results to widely used modeling approaches and demonstrate that the proposed method performs well and can accurately represent the competitive behavior of market participants. To understand the benefits of using reinforcement learning, we analyze overall profits, aggregated dispatch, and individual behavior of energy storage units. The proposed method can improve the accuracy and realism of electricity market modeling and help policymakers make informed decisions for future market designs and policies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s42162-023-00293-0"
    },
    {
        "id": 7128,
        "title": "Supervised reinforcement learning for recommending treatment strategies in sepsis",
        "authors": "Sun Haoran, Chen Qiuming, Yang Yujing",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2689783"
    },
    {
        "id": 7129,
        "title": "Learning in two dimensions and controlling in three: Generalizable drag reduction strategies for flows past circular cylinders through deep reinforcement learning",
        "authors": "Michail Chatzimanolakis, Pascal Weber, Petros Koumoutsakos",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physrevfluids.9.043902"
    },
    {
        "id": 7130,
        "title": "QUANTITATIVE STUDIES OF DEEP REINFORCEMENT LEARNING IN GAMING, ROBOTICS, AND REAL-WORLD CONTROL SYSTEMS",
        "authors": "MUHAMMAD UMAR KHAN, SOMIA MEHAK, DR. WAJIHA YASIR, SHAGUFTA ANWAR, MUHAMMAD USMAN MAJEED, HAFIZ ARSLAN RAMZAN",
        "published": "2023-8-29",
        "citations": 0,
        "abstract": "Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm with profound implications for gaming, robotics, real-world control systems, and beyond. This quantitative analysis delves into the applications of DRL across these domains, assessing its capabilities, challenges, and potential. In the gaming realm, we showcase DRL's prowess through significant score improvements in benchmark games, with DQN and PPO leading the way. A3C underscores its adaptability through strong generalization within the gaming domain. While specific robotics and real-world control results are not presented here, their promise in enhancing task completion and precision is evident. Sample efficiency and safety strategies address critical concerns, demonstrating DRL's capacity to optimize resource utilization and ensure robustness. Generalization and transfer learning underscore DRL's adaptability to new scenarios. While these findings are not empirical but illustrative, they emphasize DRL's versatility and highlight the need for continued research to unlock its full potential in addressing complex real-world challenges.",
        "keywords": "",
        "link": "http://dx.doi.org/10.61506/01.00019"
    },
    {
        "id": 7131,
        "title": "Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach",
        "authors": "Yigit Gurses, Kaan Buyukdemirci, Yildiray Yildiz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcsys.2024.3349511"
    },
    {
        "id": 7132,
        "title": "TOWARDS EFFECTIVE STRATEGIES FOR MOBILE ROBOT USING REINFORCEMENT LEARNING AND GRAPH ALGORITHMS",
        "authors": "Sofiia Shaposhnikova, Dmytro Omelian",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "Abstract. This research paper explores the use of Reinforcement Learning (RL) and traditional graph algorithms like A* for mobile robots in the field of path planning and strategy development. The paper conducts a comprehensive analysis of these algorithms by evaluating their performance in terms of efficiency, scalability, and applicability in real-world scenarios. The results of the study show that while both RL and A* algorithms have their benefits and limitations, RL algorithms have the potential to provide more effective and scalable solutions for mobile robots in real-world applications. The paper also provides ongoing research directions aimed at improving the performance of these algorithms and concludes by offering valuable insights for researchers and practitioners working in the field of mobile robots.\r\nThe purpose of this project is to evaluate the performance of these algorithms, identify their benefits and limitations, and contribute to the development of more effective and practical solutions for mobile robots in real-world applications. The results of this study will be valuable for researchers and practitioners working in the field of mobile robots, as it will provide a comprehensive analysis of the use of RL and A* algorithms and offer ongoing research directions for improving their performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15673/atbp.v15i2.2522"
    },
    {
        "id": 7133,
        "title": "SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning",
        "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf",
        "published": "2023-8",
        "citations": 4,
        "abstract": "We present a novel recommendation system designed to provide real-time treatment strategies to therapists during psychotherapy sessions. Our system utilizes a turn-level rating mechanism that forecasts the therapeutic outcome by calculating a similarity score between the profound representation of a scoring inventory and the patient's current spoken sentence. By transcribing and segmenting the continuous audio stream into patient and therapist turns, our system conducts immediate evaluation of their therapeutic working alliance. The resulting dialogue pairs, along with their computed working alliance ratings, are then utilized in a deep reinforcement learning recommendation system. In this system, the sessions are treated as users, while the topics are treated as items. To showcase the system's effectiveness, we not only evaluate its performance using an existing dataset of psychotherapy sessions but also demonstrate its practicality through a web app. Through this demo, we aim to provide a tangible and engaging experience of our recommendation system in action.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/837"
    },
    {
        "id": 7134,
        "title": "Comparison of Deep Reinforcement Learning-Based Guidance Strategies under Non-Ideal Conditions",
        "authors": "Sevket Utku Aydinli, Ali Turker Kutay",
        "published": "2023-7-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/codit58514.2023.10284472"
    },
    {
        "id": 7135,
        "title": "Comparison of WiFi Interference Mitigation Strategies in DSME Networks: Leveraging Reinforcement Learning with Expected SARSA",
        "authors": "Ivonne Andrea Mantilla Gonzalez, Volker Turau",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/meditcom58224.2023.10266605"
    },
    {
        "id": 7136,
        "title": "Advanced Deep Reinforcement Learning Strategies for Enhanced Autonomous Vehicle Navigation Systems",
        "authors": "M Dhinakaran, R.Thalapathi Rajasekaran, V. Balaji, V. Aarthi, S. Ambika",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic457434.2024.10486336"
    },
    {
        "id": 7137,
        "title": "Social Engineering Attack-Defense Strategies Based on Reinforcement Learning",
        "authors": "Rundong Yang, Kangfeng Zheng, Xiujuan Wang, Bin Wu, Chunhua Wu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.038917"
    },
    {
        "id": 7138,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 7139,
        "title": "Measuring the Level of Fidelity Required for Transfer of Learning in Simulation-Based Learning Exercises for Novice and Experienced Practitioners",
        "authors": "Amanda Jane Davies, Ghaleb Krame",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "Background An acknowledged conundrum which exists for designers of simulation-based learning exercises centres on how much fidelity is required to aid transfer of learning from the classroom to the field of application and what is the influence of the learner’s practitioner experience. This article presents the findings from a study which explored the way in which fidelity influences the sense of immersion and learning transfer experienced by novice and experienced practitioners in technology-assisted simulation-based learning exercises. Methods Two different technology-assisted simulation-based learning exercises were explored employing participant surveys and interviews. The central aim of the exercises, conducted by an Australian police agency was the development of decision-making skills for high-risk and high-stakes situations. Case one centred on public order management for highly experienced senior police and Case Two was a judgmental use-of-force simulation for novice police recruits with limited or nil operational experience. Results The findings indicate experienced practitioners did not suggest a need to actually see a replicated site of the incident with the human and environmental elements and sounds. They relied on their policing experience to build their own visualization of the situation. It is the combination of this personal visualization with the psychological factors of the simulated incident which created a realistic environment for these experienced practitioners. For the novice practitioners – they relied on the simulation exercise and environmental features to be as realistic and extensive as possible in order to experience immersion and presence. Conclusion This study supports the suggestion that measuring how much fidelity is enough or too much is complex, and a unit of measurement could reasonably be identified as determining the balance of physical and psychological fidelity, informed by the field-based experience of the participants, which will support learning transfer from the classroom (or simulated environment) to the field of operation. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781241241548"
    },
    {
        "id": 7140,
        "title": "Decision-Making Strategies for Close-Range Air Combat Based on Reinforcement Learning with Variable-Scale Actions",
        "authors": "Lixin Wang, Jin Wang, Hailiang Liu, Ting Yue",
        "published": "2023-4-26",
        "citations": 2,
        "abstract": "The current research into decision-making strategies for air combat focuses on the performance of algorithms, while the selection of actions is often ignored, and the actions are often fixed in amplitude and limited in number in order to improve the convergence efficiency, making the strategy unable to give full play to the maneuverability of the aircraft. In this paper, a decision-making strategy for close-range air combat based on reinforcement learning with variable-scale actions is proposed; the actions are the variable-scale virtual pursuit angles and speeds. Firstly, a trajectory prediction method consisting of a real-time prediction, correction, and judgment of errors is proposed. The back propagation (BP) neural network and the long and short term memory (LSTM) neural network are used as base prediction network and correction prediction network, respectively. Secondly, the past, current, and future positions of the target aircraft are used as virtual pursuit points, and they are converted into virtual pursuit angles as the track angle commands using angle guidance law. Then, the proximity policy optimization (PPO) algorithm is applied to train the agent. The simulation results show that the attacking aircraft that uses the strategy proposed in this paper has a higher win rate during air combat and the attacking aircraft’s maneuverability is fully utilized.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/aerospace10050401"
    },
    {
        "id": 7141,
        "title": "Investigating navigation strategies in the Morris Water Maze through deep reinforcement learning",
        "authors": "Andrew Liu, Alla Borisyuk",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.004"
    },
    {
        "id": 7142,
        "title": "On Effectiveness of Exploration Strategies in Deep Reinforcement Learning for Power Allocation in Multi-Carrier Wireless Systems",
        "authors": "Amna Kopic, Kenan Turbic, Haris Gacanin",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283481"
    },
    {
        "id": 7143,
        "title": "Hosting Capacity Assessment Strategies and Reinforcement Learning Methods for Coordinated Voltage Control in Electricity Distribution Networks: A Review",
        "authors": "Jude Suchithra, Duane Robinson, Amin Rajabi",
        "published": "2023-3-1",
        "citations": 5,
        "abstract": "Increasing connection rates of rooftop photovoltaic (PV) systems to electricity distribution networks has become a major concern for the distribution network service providers (DNSPs) due to the inability of existing network infrastructure to accommodate high levels of PV penetration while maintaining voltage regulation and other operational requirements. The solution to this dilemma is to undertake a hosting capacity (HC) study to identify the maximum penetration limit of rooftop PV generation and take necessary actions to enhance the HC of the network. This paper presents a comprehensive review of two topics: HC assessment strategies and reinforcement learning (RL)-based coordinated voltage control schemes. In this paper, the RL-based coordinated voltage control schemes are identified as a means to enhance the HC of electricity distribution networks. RL-based algorithms have been widely used in many power system applications in recent years due to their precise, efficient and model-free decision-making capabilities. A large portion of this paper is dedicated to reviewing RL concepts and recently published literature on RL-based coordinated voltage control schemes. A non-exhaustive classification of RL algorithms for voltage control is presented and key RL parameters for the voltage control problem are identified. Furthermore, critical challenges and risk factors of adopting RL-based methods for coordinated voltage control are discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16052371"
    },
    {
        "id": 7144,
        "title": "Efficient learning of power grid voltage control strategies via model-based deep reinforcement learning",
        "authors": "Ramij Raja Hossain, Tianzhixi Yin, Yan Du, Renke Huang, Jie Tan, Wenhao Yu, Yuan Liu, Qiuhua Huang",
        "published": "2023-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06422-w"
    },
    {
        "id": 7145,
        "title": "Research on Reinforcement-Learning-Based Truck Platooning Control Strategies in Highway On-Ramp Regions",
        "authors": "Jiajia Chen, Zheng Zhou, Yue Duan, Biao Yu",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "With the development of autonomous driving technology, truck platooning control has become a reality. Truck platooning can improve road capacity by maintaining a minor headway. Platooning systems can significantly reduce fuel consumption and emissions, especially for trucks. In this study, we designed a Platoon-MAPPO algorithm to implement truck platooning control based on multi-agent reinforcement learning for a platooning facing an on-ramp scenario on highway. A centralized training, decentralized execution algorithm was used in this paper. Each truck only computes its actions, avoiding the data computation delay problem caused by centralized computation. Each truck considers the truck status in front of and behind itself, maximizing the overall gain of the platooning and improving the global operational efficiency. In terms of performance evaluation, we used the traditional rule-based platooning following model as a benchmark. To ensure fairness, the model used the same network structure and traffic scenario as our proposed model. The simulation results show that the algorithm proposed in this paper has good performance and improves the overall efficiency of the platoon while guaranteeing traffic safety. The average energy consumption decreased by 14.8%, and the road occupancy rate decreased by 43.3%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/wevj14100273"
    },
    {
        "id": 7146,
        "title": "System of Automated Dynamic Search for Treatment Strategies Based on Reinforcement Learning Methods",
        "authors": "Irina Kashirina, Maria Demchenko, Yulia Bondarenko",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349573"
    },
    {
        "id": 7147,
        "title": "Intelligent Game Strategies in Target-Missile-Defender Engagement Using Curriculum-Based Deep Reinforcement Learning",
        "authors": "Xiaopeng Gong, Wanchun Chen, Zhongyuan Chen",
        "published": "2023-1-31",
        "citations": 3,
        "abstract": "Aiming at the attack and defense game problem in the target-missile-defender three-body confrontation scenario, intelligent game strategies based on deep reinforcement learning are proposed, including an attack strategy applicable to attacking missiles and active defense strategy applicable to a target/defender. First, based on the classical three-body adversarial research, the reinforcement learning algorithm is introduced to improve the purposefulness of the algorithm training. The action spaces the reward and punishment conditions of both attack and defense confrontation are considered in the reward function design. Through the analysis of the sign of the action space and design of the reward function in the adversarial form, the combat requirements can be satisfied in both the missile and target/defender training. Then, a curriculum-based deep reinforcement learning algorithm is applied to train the agents and a convergent game strategy is obtained. The simulation results show that the attack strategy of the missile can maneuver according to the battlefield situation and can successfully hit the target after avoiding the defender. The active defense strategy enables the less capable target/defender to achieve the effect similar to a network adversarial attack on the missile agent, shielding targets from attack against missiles with superior maneuverability on the battlefield.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/aerospace10020133"
    },
    {
        "id": 7148,
        "title": "Re-exploring control strategies in a non-Markovian open quantum system by reinforcement learning",
        "authors": "Amine Jaouadi, Etienne Mangaud, Michèle Desouter-Lecomte",
        "published": "2024-1-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physreva.109.013104"
    },
    {
        "id": 7149,
        "title": "Reinforcement Learning-Based Intelligent Control Strategies for Optimal Power Management in Advanced Power Distribution Systems: A Survey",
        "authors": "Mudhafar Al-Saadi, Maher Al-Greer, Michael Short",
        "published": "2023-2-6",
        "citations": 11,
        "abstract": "Intelligent energy management in renewable-based power distribution applications, such as microgrids, smart grids, smart buildings, and EV systems, is becoming increasingly important in the context of the transition toward the decentralization, digitalization, and decarbonization of energy networks. Arguably, many challenges can be overcome, and benefits leveraged, in this transition by the adoption of intelligent autonomous computer-based decision-making through the introduction of smart technologies, specifically artificial intelligence. Unlike other numerical or soft computing optimization methods, the control based on artificial intelligence allows the decentralized power units to collaborate in making the best decision of fulfilling the administrator’s needs, rather than only a primitive decentralization based only on the division of tasks. Among the smart approaches, reinforcement learning stands as the most relevant and successful, particularly in power distribution management applications. The reason is it does not need an accurate model for attaining an optimized solution regarding the interaction with the environment. Accordingly, there is an ongoing need to accomplish a clear, up-to-date, vision of the development level, especially with the lack of recent comprehensive detailed reviews of this vitally important research field. Therefore, this paper fulfills the need and presents a comprehensive review of the state-of-the-art successful and distinguished intelligent control strategies-based RL in optimizing the management of power flow and distribution. Wherein extensive importance is given to the classification of the literature on emerging strategies, the proposals based on RL multiagent, and the multiagent primary secondary control of managing power flow in micro and smart grids, particularly the energy storage. As a result, 126 of the most relevant, recent, and non-incremental have been reviewed and put into relevant categories. Furthermore, salient features have been identified of the major positive and negative, of each selection.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16041608"
    },
    {
        "id": 7150,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 7151,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 7152,
        "title": "Playing Various Strategies in Dominion with Deep Reinforcement Learning",
        "authors": "Jasper Gerigk, Steve Engels",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "Deck-building games, like Dominion, present an unsolved challenge for game AI research. The complexity arising from card interactions and the relative strength of strategies depending on the game configuration result in computer agents being limited to simple strategies. This paper describes the first application of recent advances in Geometric Deep Learning to deck-building games. We utilize a comprehensive multiset-based game representation and train the policy using a Soft Actor-Critic algorithm adapted to support variable-size sets of actions. The proposed model is the first successful learning-based agent that makes all decisions without relying on heuristics and supports a broader set of game configurations. It exceeds the performance of all previous learning-based approaches and is only outperformed by search-based approaches in certain game configurations. In addition, the paper presents modifications that induce agents to exhibit novel human-like play strategies. Finally, we show that learning strong strategies based on card combinations requires a reinforcement learning algorithm capable of discovering and executing a precise strategy while ignoring simpler suboptimal policies with higher immediate rewards.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aiide.v19i1.27518"
    },
    {
        "id": 7153,
        "title": "Informal Social-Emotional Learning in Gaming Affinity Spaces: Evidence From a Reddit Discussion Thread on Elden Ring",
        "authors": "André Czauderna, Sam von Gillern, Bradley Robinson",
        "published": "2024-2",
        "citations": 0,
        "abstract": "Background Over the past few years, there has been extensive discussion and research on social-emotional learning (SEL) in formal educational settings. However, there is limited research on SEL in informal learning contexts, including online sites such as gaming affinity spaces. Aim This qualitative study aimed to investigate the social-emotional competencies demonstrated by players in a Reddit discussion board focused on Elden Ring, a video game known for its challenging difficulty. Methods The study utilized theoretical qualitative thematic analysis on a focal thread at r/Eldenring. The thread had been upvoted over 18,000 times, included 2,363 comments encompassing 116,911 words, and had 997,765 readers when we extracted it for data analysis. Inductive codes were identified and grouped under preconceived themes based on the conceptual framework of the Collaborative for Academic, Social, and Emotional Learning (CASEL), which comprises five components: self-awareness, self-management, social awareness, relationship skills, and responsible decision-making. Findings The analysis revealed promising social-emotional competencies demonstrated by players across all five components of the CASEL framework. Players exhibited self-awareness by reflecting on their emotions and self-management by regulating their frustration and persevering through challenges. Social awareness was displayed by showing empathy and encouragement towards others, relationship skills by offering support and advice to fellow players, and responsible decision-making by analyzing situations, solving problems, and reflecting on gameplay experiences. Conclusion This study highlights the potential for informal SEL in gaming affinity spaces, as evidenced by players exhibiting social-emotional competencies during a Reddit discussion thread on Elden Ring. Future research should examine, among other aspects, the learning processes that facilitate the acquisition of social-emotional competencies in gaming affinity spaces and other informal learning environments, both online and offline. Additionally, special attention should be paid to the unique features of online platforms (like Reddit’s upvoting/downvoting) and their influence on users’ interactions and SEL. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231209697"
    },
    {
        "id": 7154,
        "title": "Designing Traffic Management Strategies Using Reinforcement Learning Techniques",
        "authors": "Christine Taylor, Erik Vargo, Emily Bromberg, Tyler Manderfield",
        "published": "2023-10",
        "citations": 0,
        "abstract": " The future vision for traffic flow management is one that leverages advanced automation to assist human decision-makers in the identification of potential constraints and the development of resolution strategies. What makes this problem so challenging is the inherent uncertainty associated with forecasting these constraints, leaving human decision-makers reliant on experience to devise effective traffic management initiatives to mitigate demand in excess of resource capacity. This paper proposes to employ artificial intelligence-based methods to recommend traffic management initiatives under forecast uncertainty and to do so in a real-time planning context. The proposed algorithm consists of 1) a policy network that is generated offline using an Expert Iteration algorithm, 2) a statistical model that updates the likelihood of constraint futures based on observations, and 3) a Monte Carlo tree search algorithm that explores possible combinations of traffic management initiatives to identify the recommended actions for the current decision. The skill introduced by each of the algorithmic components is assessed for a case study focused on managing arrivals into the Atlanta Hartsfield–Jackson International Airport over 92 validation days. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/1.d0339"
    },
    {
        "id": 7155,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 7156,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 7157,
        "title": "Modeling market trading strategies of the intermediary entity for microgrids: A reinforcement learning-based approach",
        "authors": "Sanaz Ghanbari, Salah Bahramara, Hêmin Golpîra",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.epsr.2023.109989"
    },
    {
        "id": 7158,
        "title": "Optimizing Forwarding Strategies in Named Data Networking Using Reinforcement Learning",
        "authors": "Zhafirah Naghmah Ahmad, Fika Triana, Revita Rachel, Ridha Muldina Negara, Ratna Mayasari, Sri Astuti, Syamsul Rizal",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icwt58823.2023.10335311"
    },
    {
        "id": 7159,
        "title": "CPRL: Change Point Detection and Reinforcement Learning to Optimize Cache Placement Strategies",
        "authors": "Javane Rostampoor, Raviraj Adve, Ali Afana, Yahia Ahmed",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcomm.2023.3341856"
    },
    {
        "id": 7160,
        "title": "Reinforcement learning for automatic detection of effective strategies for self-regulated learning",
        "authors": "Ikenna Osakwe, Guanliang Chen, Yizhou Fan, Mladen Rakovic, Xinyu Li, Shaveen Singh, Inge Molenaar, Maria Bannert, Dragan Gašević",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.caeai.2023.100181"
    },
    {
        "id": 7161,
        "title": "A multi-agent reinforcement learning framework for optimizing financial trading strategies based on TimesNet",
        "authors": "Yuling Huang, Chujin Zhou, Kai Cui, Xiaoping Lu",
        "published": "2024-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121502"
    },
    {
        "id": 7162,
        "title": "Enhancing photovoltaic parameter estimation: integration of non-linear hunting and reinforcement learning strategies with golden jackal optimizer",
        "authors": "Chappani Sankaran Sundar Ganesh, Chandrasekaran Kumar, Manoharan Premkumar, Bizuwork Derebew",
        "published": "2024-2-2",
        "citations": 3,
        "abstract": "AbstractThe advancement of Photovoltaic (PV) systems hinges on the precise optimization of their parameters. Among the numerous optimization techniques, the effectiveness of each often rests on their inherent parameters. This research introduces a new methodology, the Reinforcement Learning-based Golden Jackal Optimizer (RL-GJO). This approach uniquely combines reinforcement learning with the Golden Jackal Optimizer to enhance its efficiency and adaptability in handling various optimization problems. Furthermore, the research incorporates an advanced non-linear hunting strategy to optimize the algorithm’s performance. The proposed algorithm is first validated using 29 CEC2017 benchmark test functions and five engineering-constrained design problems. Secondly, rigorous testing on PV parameter estimation benchmark datasets, including the single-diode model, double-diode model, three-diode model, and a representative PV module, was carried out to highlight the superiority of RL-GJO. The results were compelling: the root mean square error values achieved by RL-GJO were markedly lower than those of the original algorithm and other prevalent optimization methods. The synergy between reinforcement learning and GJO in this approach facilitates faster convergence and improved solution quality. This integration not only improves the performance metrics but also ensures a more efficient optimization process, especially in complex PV scenarios. With an average Freidman’s rank test values of 1.564 for numerical and engineering design problems and 1.742 for parameter estimation problems, the proposed RL-GJO is performing better than the original GJO and other peers. The proposed RL-GJO stands out as a reliable tool for PV parameter estimation. By seamlessly combining reinforcement learning with the golden jackal optimizer, it sets a new benchmark in PV optimization, indicating a promising avenue for future research and applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-024-52670-8"
    },
    {
        "id": 7163,
        "title": "Identifying environmentally sustainable pavement management strategies via deep reinforcement learning",
        "authors": "Ali Kazemeini, Omar Swei",
        "published": "2023-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jclepro.2023.136124"
    },
    {
        "id": 7164,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 7165,
        "title": "Counselor Strategies to Reduce Online Gaming Addiction through Creative Cognitive Behavior Counseling",
        "authors": "",
        "published": "2023-1-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47750/pegegog.13.02.18"
    },
    {
        "id": 7166,
        "title": "Erratum to “Digital Games as Media for Teaching and Learning: A Template for Critical Evaluation”",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231171032"
    },
    {
        "id": 7167,
        "title": "Developing smart air purifier control strategies for better IAQ and energy efficiency using reinforcement learning",
        "authors": "Wenzhe Shang, Junjie Liu, Congcong Wang, Jiayu Li, Xilei Dai",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.buildenv.2023.110556"
    },
    {
        "id": 7168,
        "title": "Reinforcement learning of adaptive control strategies",
        "authors": "Leslie K. Held, Luc Vermeylen, David Dignath, Wim Notebaert, Ruth M. Krebs, Senne Braem",
        "published": "2024-1-12",
        "citations": 1,
        "abstract": "AbstractHumans can up- or downregulate the degree to which they rely on task information for goal-directed behaviour, a process often referred to as cognitive control. Adjustments in cognitive control are traditionally studied in response to experienced or expected task-rule conflict. However, recent theories suggest that people can also learn to adapt control settings through reinforcement. Across three preregistered task switching experiments (n = 415), we selectively rewarded correct performance on trials with either more (incongruent) or less (congruent) task-rule conflict. Results confirmed the hypothesis that people rewarded more on incongruent trials showed smaller task-rule congruency effects, thus optimally adapting their control settings to the reward scheme. Using drift diffusion modelling, we further show that this reinforcement of cognitive control may occur through conflict-dependent within-trial adjustments of response thresholds after conflict detection. Together, our findings suggest that, while people remain more efficient at learning stimulus-response associations through reinforcement, they can similarly learn cognitive control strategies through reinforcement.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s44271-024-00055-y"
    },
    {
        "id": 7169,
        "title": "A hybrid decision support system for adaptive trading strategies: Combining a rule-based expert system with a deep reinforcement learning strategy",
        "authors": "Yuhee Kwon, Zoonky Lee",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2023.114100"
    },
    {
        "id": 7170,
        "title": "Towards Task-Oriented Communication Strategies for Platooning by Deep Reinforcement Learning",
        "authors": "Xiaoyan Kui, Zhihao Zheng, Chao Zhang, Mingkun Zhang, Samson Lasaulce",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/wiopt58741.2023.10349876"
    },
    {
        "id": 7171,
        "title": "Modern General Game Playing with Reinforcement Learning",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "In computational science and robotics, game playing and artificial intelligence (AI) are research areas that have long been studied. RL systems represent a major step towards autonomous systems that comprehend the visual world at an incredibly deep level and are poised to revolutionize the field of artificial intelligence (AI). Game Play is the testbench where environmental variables can be evaluated to generate adaptive, intelligent, or responsive behavior or simulate real-world scenarios. In this survey, we will explore general gameplay (GGP) with reinforcement learning and its various applications. This survey will cover central algorithms in deep RL, including the deep Qnetwork (DQN) and general mathematical and pragmatic approaches taken in the field. The objective of the current review is to examine the history, associations, and recent advances in the field of general game playing with reinforcement learning and its subfields. We conclude by describing several current areas of research within this field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.16"
    },
    {
        "id": 7172,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 7173,
        "title": "Edge device resource optimization (EDRO): evaluating the impact of reinforcement learning and deep reinforcement learning methods",
        "authors": "Rahul P, Hitaishi Roy, Sanglagna Khakhlary",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2024.0942"
    },
    {
        "id": 7174,
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey",
        "authors": "Amjad Yousef Majid, Serge Saaybi, Vincent Francois-Lavet, R. Venkatesha Prasad, Chris Verhoeven",
        "published": "2024",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3264540"
    },
    {
        "id": 7175,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 7176,
        "title": "Digital Games as Media for Teaching and Learning: A Template for Critical Evaluation",
        "authors": "Holger Pötzsch, Therese Holt Hansen, Emil Lundedal Hammar",
        "published": "2023-3-27",
        "citations": 0,
        "abstract": "BackgroundVideogames can be useful tools for teaching and learning. To plan educational uses, potential benefits and possible problematic aspects of specific titles need to be critically assessed by teachers and school leaders prior to implementation.Theory and MethodBased on game ontological models, we identify salient areas of inquiry in games research and use these to structure a template for evaluation. This way we operationalize foundational games research and put key insights to practical use in the planning and preparation of videogame-based teaching sessions.AimsWe develop a template for the evaluation of videogames as tools for and objects of teaching and learning to facilitate critical uses of these technologies in schools and other educational settings.ResultsWe present a template for critical evaluation to facilitate the use of videogames for educational endeavors. The template distinguishes between videogames as tools for and objects of teaching and learning and is structured along the game ontological dimensions of 1) sign system, 2) rules and mechanics, 3) materiality and 4) players, and includes aspects of both representation and simulation. This way, we disentangle a complex phenomenon and make its components amendable for critical analysis and constructive intervention.Discussion and ConclusionWe offer illustrating examples for how the template can be used to assess the usability of specific titles in education and discuss advantages and disadvantages. Finally, we suggest steps for implementation and further improvement.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231166213"
    },
    {
        "id": 7177,
        "title": "Punnett Farms: Developing An Immersive Educational Game-Based Platform for Learning Genetics",
        "authors": "Henry G.H. Low, Marina Ellefson",
        "published": "2024-4",
        "citations": 0,
        "abstract": "Background Serious games (SGs) have emerged as promising tools for life science education, providing interactive learning experiences that bolster accessibility and engagement. In the context of genetics education, which emphasizes hands-on problem-solving, SGs offer a potential platform to augment learning. Aim This article presents the detailed design of a new educational SG, titled Punnett Farms, along with a pilot evaluation of the game. Designed to help students learn the fundamentals of Mendelian/molecular genetics, the game offers a colorful virtual world with embedded educational content aimed at cultivating interactive learning and engagement. Methods Development of Punnett Farms was guided by principles of educational game design models. The game was implemented in Unity and C#. A pilot study was conducted with community college students (n = 22) to assess the game’s strengths and areas for improvement. Pre-/post-intervention knowledge assessments, along with questionnaires inspired by the GEM and MEEGA+ frameworks, were used to obtain insights into participants’ interest, enjoyment, and short-term learning outcomes. Results Participants exhibited possible short-term knowledge gains after playing Punnett Farms, as reflected by improved quiz scores. In questionnaire responses, participants also reported improved content understanding, interest towards the subject, and overall enjoyment in learning genetics. Additionally, players rated the game highly for presentation and educational utility. Conclusion By integrating educational game design principles, Punnett Farms aims to provide an immersive environment that supports learning of essential Mendelian genetics topics. Results from the pilot study were positive, suggesting the game has potential to be a helpful resource for genetics learners. Future efforts will focus on continuing to improve and evaluate the game across different contexts. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231220728"
    },
    {
        "id": 7178,
        "title": "Research on task offloading optimization strategies for vehicular networks based on game theory and deep reinforcement learning",
        "authors": "Lei Wang, Wenjiang Zhou, Haitao Xu, Liang Li, Lei Cai, Xianwei Zhou",
        "published": "2023-10-16",
        "citations": 2,
        "abstract": "With the continuous development of the 6G mobile network, computing-intensive and delay-sensitive onboard applications generate task data traffic more frequently. Particularly, when multiple intelligent agents are involved in tasks, limited computational resources cannot meet the new Quality of Service (QoS) requirements. To provide a satisfactory task offloading strategy, combining Multi-Access Edge Computing (MEC) with artificial intelligence has become a potential solution. In this context, we have proposed a task offloading decision mechanism (TODM) based on cooperative game and deep reinforcement learning (DRL). A joint optimization problem is presented to minimize both the overall task processing delay (OTPD) and overall task energy consumption (OTEC). The approach considers task vehicles (TaVs) and service vehicles (SeVs) as participants in a cooperative game, jointly devising offloading strategies to achieve resource optimization. Additionally, a proximate policy optimization (PPO) algorithm is designed to ensure robustness. Simulation experiments confirm the convergence of the proposed algorithm. Compared with benchmark algorithms, the presented scheme effectively reduces delay and energy consumption while ensuring task completion.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fphy.2023.1292702"
    },
    {
        "id": 7179,
        "title": "Multi-agent reinforcement learning with graph convolutional neural networks for optimal bidding strategies of generation units in electricity markets",
        "authors": "Pegah Rokhforoz, Mina Montazeri, Olga Fink",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120010"
    },
    {
        "id": 7180,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 7181,
        "title": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners’ dilemma game",
        "authors": "Masahiko Ueda",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.amc.2022.127819"
    },
    {
        "id": 7182,
        "title": "Reinforcement learning of control strategies for reducing skin friction drag in a fully developed turbulent channel flow",
        "authors": "Takahiro Sonoda, Zhuchen Liu, Toshitaka Itoh, Yosuke Hasegawa",
        "published": "2023-4-10",
        "citations": 14,
        "abstract": "Reinforcement learning is applied to the development of control strategies in order to reduce skin friction drag in a fully developed turbulent channel flow at a low Reynolds number. Motivated by the so-called opposition control (Choi et al., J. Fluid Mech., vol. 253, 1993, pp. 509–543), in which a control input is applied so as to cancel the wall-normal velocity fluctuation on a detection plane at a certain distance from the wall, we consider wall blowing and suction as a control input, and its spatial distribution is determined by the instantaneous streamwise and wall-normal velocity fluctuations at distance 15 wall units above the wall. A deep neural network is used to express the nonlinear relationship between the sensing information and the control input, and it is trained so as to maximize the expected long-term reward, i.e. drag reduction. When only the wall-normal velocity fluctuation is measured and a linear network is used, the present framework reproduces successfully the optimal linear weight for the opposition control reported in a previous study (Chung & Talha, Phys. Fluids, vol. 23, 2011, 025102). In contrast, when a nonlinear network is used, more complex control strategies based on the instantaneous streamwise and wall-normal velocity fluctuations are obtained. Specifically, the obtained control strategies switch abruptly between strong wall blowing and suction for downwelling of a high-speed fluid towards the wall and upwelling of a low-speed fluid away from the wall, respectively. Extracting key features from the obtained policies allows us to develop novel control strategies leading to drag reduction rates as high as 37 %, which is higher than the 23 % achieved by the conventional opposition control at the same Reynolds number. Finding such an effective and nonlinear control policy is quite difficult by relying solely on human insights. The present results indicate that reinforcement learning can be a novel framework for the development of effective control strategies through systematic learning based on a large number of trials.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/jfm.2023.147"
    },
    {
        "id": 7183,
        "title": "Dispatching in Real Frontend Fabs With Industrial Grade Discrete-Event Simulations by Deep Reinforcement Learning with Evolution Strategies",
        "authors": "Patrick Stöckermann, Alessandro Immordino, Thomas Altenmüller, Georg Seidel, Martin Gebser, Pierre Tassel, Chew Wye Chan, Feifei Zhang",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408625"
    },
    {
        "id": 7184,
        "title": "Commissioning Federated Reinforcement Learning to Envision Network Security Strategies",
        "authors": "Harshit Gupta, Sahil Sharma, Aadharsh Roshan, Ayush Baranwal, Sankalp Rajendran, Ranjana Vyas, O P Vyas, Antonio Puliafito",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440912"
    },
    {
        "id": 7185,
        "title": "Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem",
        "authors": "Chupeng Su, Cong Zhang, Dan Xia, Baoan Han, Chuang Wang, Gang Chen, Longhan Xie",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.110596"
    },
    {
        "id": 7186,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 7187,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 7188,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 7189,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 7190,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 7191,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 7192,
        "title": "AIM-RL: A New Framework Supporting Reinforcement Learning Experiments",
        "authors": "Ionuţ-Cristian Pistol, Andrei Arusoaie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091100003538"
    },
    {
        "id": 7193,
        "title": "Systematic Framework for Esports Training: Informing eSports Training with the Learning Sciences",
        "authors": "Yoo Kyung Chang, Keying Chen, Ziyue Li",
        "published": "2024-2",
        "citations": 0,
        "abstract": "Intro With the rapid expansion of the esports industry, many esports training programs and academies have emerged. According to research, esports may lead to cognitive, metacognitive, academic and professional competencies if appropriately supported (Adachi & Willoughby, 2013). However, there is no systematic curriculum or guidance on how to identify and support these competencies through esports training. Methods This paper conducts a systematic review of literature from the learning sciences to provide a systematic rubric of competencies and instructional approaches to support them through esports training. The competencies that may be supported through esports are mapped along the game design principles as categorized along Mechanics, Dynamics, and Aesthetics (MDA) framework. Current training programs were analyzed to identify training approaches that can support these competencies across different contexts. Results There is a wide range of cognitive, self-regulation, social, academic, and career competencies that were found to be supported through video games. However, the development of such competencies depend on the availability of the learning opportunities and appropriate support provided to engage these opportunities, thus requiring a systematic framework for esports training. The training approaches that can be implemented to support diverse competencies from esports training, whether amateur or professional, are varied in modality, style, and structure. Conclusion The goal of this study is to inform the esports industry more largely, as well as the trainers, researchers, and designers with systematic and research-informed approaches to support future esports athletes, both professional and amateurs. Learning from the traditional field of athletic training, esports training should take more systematic approaches to design training curriculum and programs, as well as preparing the trainers. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231219938"
    },
    {
        "id": 7194,
        "title": "Simulated Learning Environments as an Interdisciplinary Option for Vocational Training: A Systematic Review",
        "authors": "Sandra Liliana Navarro-Parra, Andrés Chiappe",
        "published": "2024-4",
        "citations": 0,
        "abstract": "Background The current scenario of university education calls for training spaces that go beyond direct instruction and knowledge memorization exercises. Such requirement stems from the need to articulate very diverse knowledge and skills that the current professional must develop both in university practice and in their daily work. Given this circumstance, it is imperative to build new teaching experiences that articulate the space of practice and the knowledge that corresponds to it, as scenarios for the interdisciplinary and permanent construction of knowledge. Method To address this issue, a systematic literature review has been conducted to explore research from the last 20 years on the educational implementation of simulations in various professions. After conducting filtering and sampling processes, 101 articles were reviewed, which were read in depth and to which qualitative processes of categorization and frequency analysis were applied. Results The results of the review highlight some advantages and limitations of simulations as learning resources and suggest the importance of their deployment from an interdisciplinary perspective of teaching, especially in health sciences. Conclusion Finally, we reflect on the importance of approaching simulation-based learning from a curricular perspective, which brings learning in higher education closer to the complex realities of the world of work. In addition, the existence of great challenges and transformative effects of its implementation on university educational structures and models is highlighted. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231221904"
    },
    {
        "id": 7195,
        "title": "Teaching Reinforcement Learning Agents via Reinforcement Learning",
        "authors": "Kun Yang, Chengshuai Shi, Cong Shen",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089695"
    },
    {
        "id": 7196,
        "title": "Molecule Builder: Environment for Testing Reinforcement Learning Agents",
        "authors": "Petr Hyner, Jan Hůla, Mikoláš Janota",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012257900003595"
    },
    {
        "id": 7197,
        "title": "Automating XSS Vulnerability Testing Using Reinforcement Learning",
        "authors": "Kento Hasegawa, Seira Hidano, Kazuhide Fukushima",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011653600003405"
    },
    {
        "id": 7198,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 7199,
        "title": "Reinforcement learning strategies using Monte-Carlo to solve the blackjack problem",
        "authors": "Raghavendra Srinivasaiah, Vinai George Biju, Santosh Kumar Jankatti, Ravikumar Hodikehosahally Channegowda, Niranjana Shravanabelagola Jinachandra",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "Blackjack is a classic casino game in which the player attempts to outsmart the dealer by drawing a combination of cards with face values that add up to just under or equal to 21 but are more incredible than the hand of the dealer he manages to come up with. This study considers a simplified variation of blackjack, which has a dealer and plays no active role after the first two draws. A different game regime will be modeled for everyone to ten multiples of the conventional 52-card deck. Irrespective of the number of standard decks utilized, the game is played as a randomized discrete-time process. For determining the optimum course of action in terms of policy, we teach an agent-a decision maker-to optimize across the decision space of the game, considering the procedure as a finite Markov decision chain. To choose the most effective course of action, we mainly research Monte Carlo-based reinforcement learning approaches and compare them with q-learning, dynamic programming, and temporal difference. The performance of the distinct model-free policy iteration techniques is presented in this study, framing the game as a reinforcement learning problem.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijece.v14i1.pp904-910"
    },
    {
        "id": 7200,
        "title": "Using Deep Reinforcement Learning And Formal Verification in Safety Critical Systems: Strategies and Challenges",
        "authors": "Satyam Sharma, Muhammad Abdul Basit Ur Rahim, Shahid Hussain, Muhammad Rizwan Abid, Tairan Liu",
        "published": "2023-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/qrs-c60940.2023.00112"
    },
    {
        "id": 7201,
        "title": "Deep-reinforcement-learning-based robot motion strategies for grabbing objects from human hands",
        "authors": "Zeyuan Cai, Zhiquan Feng, Liran Zhou, Xiaohui Yang, Tao Xu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.vrih.2022.12.001"
    },
    {
        "id": 7202,
        "title": "A multi-agent virtual market model for generalization in reinforcement learning based trading strategies",
        "authors": "Fei-Fan He, Chiao-Ting Chen, Szu-Hao Huang",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.109985"
    },
    {
        "id": 7203,
        "title": "Intelligent Scheduling Based on Reinforcement Learning Approaches: Applying Advanced Q-Learning and State–Action–Reward–State–Action Reinforcement Learning Models for the Optimisation of Job Shop Scheduling Problems",
        "authors": "Atefeh Momenikorbekandi, Maysam Abbod",
        "published": "2023-11-23",
        "citations": 1,
        "abstract": "Flexible job shop scheduling problems (FJSPs) have attracted significant research interest because they can considerably increase production efficiency in terms of energy, cost and time; they are considered the main part of the manufacturing systems which frequently need to be resolved to manage the variations in production requirements. In this study, novel reinforcement learning (RL) models, including advanced Q-learning (QRL) and RL-based state–action–reward–state–action (SARSA) models, are proposed to enhance the scheduling performance of FJSPs, in order to reduce the total makespan. To more accurately depict the problem realities, two categories of simulated single-machine job shops and multi-machine job shops, as well as the scheduling of a furnace model, are used to compare the learning impact and performance of the novel RL models to other algorithms. FJSPs are challenging to resolve and are considered non-deterministic polynomial-time hardness (NP-hard) problems. Numerous algorithms have been used previously to solve FJSPs. However, because their key parameters cannot be effectively changed dynamically throughout the computation process, the effectiveness and quality of the solutions fail to meet production standards. Consequently, in this research, developed RL models are presented. The efficacy and benefits of the suggested SARSA method for solving FJSPs are shown by extensive computer testing and comparisons. As a result, this can be a competitive algorithm for FJSPs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12234752"
    },
    {
        "id": 7204,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 7205,
        "title": "The Role of Reflection in Learning with Simulation Games – A Multi-Method Quasi Experimental Research",
        "authors": "Tobias Alf, Marieke de Wijse, Friedrich Trautwein",
        "published": "2023-12",
        "citations": 0,
        "abstract": "Introduction The role of reflection in experience-based learning is discussed widely. This paper researches the effects of a reflection assignment as part of debriefing with qualitative and quantitative methodology in five simulation game-based seminars. Research Design and Methods The intervention (reflection assignment) used in this study consists of three reflective questions that are discussed in groups. We aim to find out the effects of adding this intervention to the regular in-between-debriefings (three times during the whole seminar). The methodological setup is quasi-experimental and so, consisted of a test group and a control group. Two different types of simulation games were used, one a general management game and the other a change management game. Data used in this study is gathered from three different sources: First, we used in-game-performance data from both games. Second, we used two types of questionnaires to evaluate game experience and self-reported learning (MEEGA+ and ZMS inventory). Third, the reflection notes from the treatment-teams were used for qualitative analysis. Results In several dimensions (game-success, game-experience and self-reported learning) significant differences were found between treatment and non-treatment students. Qualitative data show a deep level of reflection for treatment teams differentiating for the two simulation games evaluated. Discussion Supported by the qualitative reflection data we assume that the better results for treatment-students are routed in the repetitive reflection. The permanent circular reflection helps treatment students to understand the games better and gain deeper insights. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231194896"
    },
    {
        "id": 7206,
        "title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012082200003541"
    },
    {
        "id": 7207,
        "title": "Communication Strategies of the Online Mobile Legend Gaming Community: Source Queens Racing (SQRC) Esport",
        "authors": "M Rafly Indra Pangestu",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "This study aimed to investigate the communication patterns within the Mobile Legend Esport community in Pasuruan, employing Miles and Huberman's analysis. The findings reveal that the community primarily exhibits an interpersonal communication pattern characterized by (a) Openness, where members are transparent and accountable in their information sharing; (b) Empathy, as players consistently demonstrate solidarity and empathy towards one another; (c) Support, with players offering mutual assistance; (d) Positive feelings, evidenced by moral support and enthusiasm for fellow players; and (e) Equality, observed in the coach's equitable distribution of game knowledge to all players. These findings shed light on the dynamics of interpersonal communication within gaming communities and have implications for fostering positive relationships and support networks among Esport enthusiasts globally.\r\nHighlights:\r\n\r\nInterpersonal communication fosters solidarity and empathy within gaming communities.\r\nEquality in knowledge sharing promotes inclusive Esport participation.\r\nPositive feelings and support enhance players' experiences and performance.\r\n\r\nKeywords: Mobile Legend, Esport, Communication Patterns, Interpersonal, Pasuruan",
        "keywords": "",
        "link": "http://dx.doi.org/10.21070/ijins.v25i.967"
    },
    {
        "id": 7208,
        "title": "A comparison between process control strategies: reinforcement learning with RBFs and NMPC coupled with EKF",
        "authors": "Pedro de Azevedo Delou, Lucas Ferreira Bernardino, Bruno Didier Olivier Capron, Argimiro Resende Secchi",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43153-023-00351-w"
    },
    {
        "id": 7209,
        "title": "Optimizing Trading Strategies in Quantitative Markets Using Multi-Agent Reinforcement Learning",
        "authors": "Hengxi Zhang, Zhendong Shi, Yuanquan Hu, Wenbo Ding, Ercan E. Kuruoğlu, Xiao-Ping Zhang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446489"
    },
    {
        "id": 7210,
        "title": "Selection of urban light pollution mitigation strategies based on reinforcement learning",
        "authors": "Jinwei Gan, Shimeng Guo, Tian Jin, Tao Liu, Xiaoming Yuan, Weixuan Kong, Ruofeng Qiu, Yunfei Qi",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2683853"
    },
    {
        "id": 7211,
        "title": "Reinforcement learning strategies in cancer chemotherapy treatments: A review",
        "authors": "Chan-Yun Yang, Chamani Shiranthika, Chung-Yih Wang, Kuo-Wei Chen, Sagara Sumathipala",
        "published": "2023-2",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cmpb.2022.107280"
    },
    {
        "id": 7212,
        "title": "Evolved Prevention Strategies for 6G Networks Through Stochastic Games and Reinforcement Learning",
        "authors": "Emanuel Figetakis, Ahmed Refaey Hussein, Mehmet Ulema",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lnet.2023.3273153"
    },
    {
        "id": 7213,
        "title": "Multi-Environment Training Against Reward Poisoning Attacks on Deep Reinforcement Learning",
        "authors": "Myria Bouhaddi, Kamel Adi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012139900003555"
    },
    {
        "id": 7214,
        "title": "Deep reinforcement learning based energy management strategies for electrified vehicles: Recent advances and perspectives",
        "authors": "Hongwen He, Xiangfei Meng, Yong Wang, Amir Khajepour, Xiaowen An, Renguang Wang, Fengchun Sun",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rser.2023.114248"
    },
    {
        "id": 7215,
        "title": "Reinventing Astronomical Survey Scheduling with Reinforcement Learning: Unveiling the Potential of Self-Driving Telescope",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2246942"
    },
    {
        "id": 7216,
        "title": "Deep reinforcement learning for comprehensive route optimization in elastic optical networks using generative strategies",
        "authors": "P. N. Renjith, G. Sujatha, M. Vinoth, G. D. Vignesh, M. Ramkumar Prabhu, B. Mouleswararao",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11082-023-05501-5"
    },
    {
        "id": 7217,
        "title": "Analyzing Deep Reinforcement Learning Strategies for Enhanced Profit Generation and Risk Mitigation in Algorithm Stock Trading",
        "authors": "Prasanna Kumar R, K Venkatraman, Sumash Chandra Bandaru, Snehitha Gorantla, Swathi Gunti, Harshith Katragadda, Ashish Reddy Mandadi",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrtac59277.2023.10480823"
    },
    {
        "id": 7218,
        "title": "A Recent Publications Survey on Reinforcement Learning for Selecting Parameters of Meta-Heuristic and Machine Learning Algorithms",
        "authors": "Maria Chernigovskaya, Andrey Kharitonov, Klaus Turowski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011954300003488"
    },
    {
        "id": 7219,
        "title": "Towards prescriptive analytics of self‐regulated learning strategies: A reinforcement learning approach",
        "authors": "Ikenna Osakwe, Guanliang Chen, Yizhou Fan, Mladen Rakovic, Shaveen Singh, Lyn Lim, Joep van der Graaf, Johanna Moore, Inge Molenaar, Maria Bannert, Alex Whitelock‐Wainwright, Dragan Gašević",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "AbstractSelf‐regulated learning (SRL) is an essential skill to achieve one's learning goals. This is particularly true for online learning environments (OLEs) where the support system is often limited compared to a traditional classroom setting. Likewise, existing research has found that learners often struggle to adapt their behaviour to the self‐regulatory demands of OLEs. Even so, existing SRL analysis tools have limited utility for real‐time or individualised support of a learner's SRL strategy during a study session. Accordingly, we explore a reinforcement learning based approach to learning optimal SRL strategies for a specific learning task. Specifically, we utilise the sequences of SRL processes acted by 44 participants, and their assessment scores for a prescribed learning task, in a purpose‐built OLE to develop a long short‐term memory (LSTM) network based reward function. This is used to train a reinforcement learning agent to find the optimal sequence of SRL processes for the learning task. Our findings indicate that the developed agents were able to effectively select SRL processes so as to maximise a prescribed learning goal as measured by predicted assessment score and predicted knowledge gains. The contributions of this work will facilitate the development of a tool which can detect sub‐optimal SRL strategy in real‐time and enable individualised SRL focused scaffolding.\nPractitioner notesWhat is already known about this topic\n\nLearners often fail to adequately adapt their behavior to the self‐regulatory demands of e‐Learning environments.\nIn order to promote effective Self‐regulated learning (SRL) capabilities, researchers and educators need tools that are able to analyze and diagnose a learner's SRL strategy use.\nCurrent methods for SRL analysis are more often descriptive as opposed to prescriptive and have limited utility for real‐time analysis or support of a learner's SRL behavior.\nWhat this paper adds\n\nThis paper proposes the use of Reinforcement Learning for prescriptive analytics of SRL. We train a Reinforcement Learning agent on sequences of SRL processes acted by learners in order to learn the optimal SRL strategy for a given learning task.\nImplications for practice and/or policy\n\nOur work will facilitate the development of a tool which can detect sub‐optimal SRL strategy in real‐time and enable individualized SRL focused scaffolding.\nThe implications of our work can aid in course design by predicting the self‐regulatory load imposed by a given task.\nThe ability to model SRL strategies using Reinforcement Learning can be extended to simulate or test SRL theories.\n\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/bjet.13429"
    },
    {
        "id": 7220,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 7221,
        "title": "Can Facilitators’ Need for Control Influence Students’ Learning Experience through Simulation? - A Qualitative Study on Simulation in Nursing Education",
        "authors": "Jon Viktor Haugom, Solveig Struksnes",
        "published": "2024-2",
        "citations": 0,
        "abstract": "Background Society has an increasing demand for nurses, but the availability of clinical placements for nursing students does not keep pace with this need. As a result, the use of simulation as a supplement or replacement for clinical practice is being discussed. Simulation can be a resource-intensive learning method, making it important to consider how simulation can be organized and implemented as efficiently as possible without compromising learning outcomes. Objective To explore students' experiences with different ways of organizing simulation. Method: Qualitative design inspired by action research. A purposive sample of 24 students was selected, and data was collected through eight focus group interviews. Inductive content analysis was used for data analysis. Results Three main categories emerged: the importance of a conducive level of stress for learning, feelings of responsibility and autonomy, and constructive feedback and reflection. Conclusion The findings suggest that the presence of a facilitator in all stages of simulation may not necessarily be the most effective approach for learning. It appears that the facilitator could focus more on organizing the simulation so that all participants can actively engage, while utilizing their time to plan scenarios, establish a safe learning environment, and participate in a summary session to clarify unresolved academic questions and nursing practices ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231212352"
    },
    {
        "id": 7222,
        "title": "Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control",
        "authors": "Rohit Bokade, Xiaoning Jin, Christopher Amato",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3275883"
    },
    {
        "id": 7223,
        "title": "PenGym: Pentesting Training Framework for Reinforcement Learning Agents",
        "authors": "Thanh Nguyen, Zhi Chen, Kento Hasegawa, Kazuhide Fukushima, Razvan Beuran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012367300003648"
    },
    {
        "id": 7224,
        "title": "Research on Inertial Navigation Technology of Unmanned Aerial Vehicles with Integrated Reinforcement Learning Algorithm",
        "authors": "",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "With the continuous expansion of unmanned aerial vehicle (UAV) applications, traditional inertial navigation technology exhibits significant limitations in complex environments. In this study, we integrate improved reinforcement learning (RL) algorithms to enhance existing unmanned aerial vehicle inertial navigation technology and introduce a modulated mechanism (MM) for adjusting the state of the intelligent agent in an innovative manner [1,2]. Through interaction with the environment, the intelligent machine can learn more effective navigation strategies [3]. The ultimate goal is to provide a foundation for autonomous navigation of unmanned aerial vehicles during flight and improve navigation accuracy and robustness. We first define appropriate state representation and action space, and then design an adjustment mechanism based on the actions selected by the intelligent agent. The adjustment mechanism outputs the next state and reward value of the agent. Additionally, the adjustment mechanism calculates the error between the adjusted state and the unadjusted state. Furthermore, the intelligent agent stores the acquired experience samples containing states and reward values in a buffer and replays the experiences during each iteration to learn the dynamic characteristics of the environment. We name the improved algorithm as the DQM algorithm. Experimental results demonstrate that the intelligent agent using our proposed algorithm effectively reduces the accumulated errors of inertial navigation in dynamic environments. Although our research provides a basis for achieving autonomous navigation of unmanned aerial vehicles, there is still room for significant optimization. Further research can include testing unmanned aerial vehicles in simulated environments, testing unmanned aerial vehicles in realworld environments, optimizing the design of reward functions, improving the algorithm workflow to enhance convergence speed and performance, and enhancing the algorithm's generalization ability. It has been proven that by integrating reinforcement learning algorithms, unmanned aerial vehicles can achieve autonomous navigation, thereby improving navigation accuracy and robustness in dynamic and changing environments [4]. Therefore, this research plays an important role in promoting the development and application of unmanned aerial vehicle technology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.06"
    },
    {
        "id": 7225,
        "title": "Unlocking Autonomous Telescopes through Reinforcement Learning: An Offline Framework and Insights from a Case Study",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2217169"
    },
    {
        "id": 7226,
        "title": "Towards Self-Adaptive Resilient Swarms Using Multi-Agent Reinforcement Learning",
        "authors": "Rafael Pina, Varuna De Silva, Corentin Artaud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012462800003654"
    },
    {
        "id": 7227,
        "title": "ADAPTIVE LEARNING THROUGH AI: REINFORCEMENT LEARNING IN TEACHING MULTIPLICATION TABLES",
        "authors": "Lara Drožđek, Igor Pesek",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/inted.2024.1186"
    },
    {
        "id": 7228,
        "title": "Peculiarities of Marketing Communication Strategies in the Chain Entertainment Business (Hotel, Cinema, Gaming, Restaurant Chains)",
        "authors": "Kateryna Naumik-Gladka",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "Technological development allows expanding marketing communication tools and updating marketing approaches to promotion. Chains of hotels, cinemas, gaming business and restaurants are a part of the entertainment industry, which directs marketing efforts to create a special impression for a visitor. This approach is the key to motivate to repeat visits and to create long-term relationships with clients. The purpose of the article is to consider the existing marketing communication strategies of enterprises in the entertainment sector, to single out and systematize the methods of setting goals for the formation of effective promotion. Peculiarities of marketing communication strategies in the chain business of the entertainment services sector are studied, as well as in context of hotel, cinema, restaurant and gaming chains. The modern market of entertainment services is rapidly changing under the influence of technological and consumer trends and social and economic crisis. Therefore, issues of crisis management and interaction with consumers during crisis situations are relevant. Also the main tools of marketing communication strategies, such as advertising, public relations, social media and interactive channels are highlighted. Methods of analysis and synthesis, grouping and comparison are applied for investigation the use of digital technologies for communication with customers in marketing campaigns of enterprises in the entertainment sector. Particular attention is paid to specific aspects of marketing communications of hotel, cinema, gaming and restaurant chain business, such as brand management, personalized offers, social media marketing and outdoor advertising. The research results and recommendations can be useful for marketing and advertising managers working in the field of hotels, cinemas, gaming and hotel chain business.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54929/2786-5738-2023-7-12-03"
    },
    {
        "id": 7229,
        "title": "Optimizing IoT-enabled WSN routing strategies using whale optimization-driven multi-criterion correlation approach employs the reinforcement learning agent",
        "authors": "K. Vijayan, Pravin R. Kshirsagar, Shrikant Vijayrao Sonekar, Prasun chakrabarti, Bhuvan Unhelkar, Martin Margala",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11082-023-06269-4"
    },
    {
        "id": 7230,
        "title": "Safe Q-Learning Approaches for Human-in-Loop Reinforcement Learning",
        "authors": "Swathi Veerabathraswamy, Nirav Bhatt",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442899"
    },
    {
        "id": 7231,
        "title": "Welcome to the Jungle: A Conceptual Comparison of Reinforcement Learning Algorithms",
        "authors": "Kenneth Schröder, Alexander Kastius, Rainer Schlosser",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011626700003396"
    },
    {
        "id": 7232,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 7233,
        "title": "Machine Learning Models on MOBA Gaming: League of Legends Winner Prediction",
        "authors": "Kaan Arık",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26650/acin.1180583"
    },
    {
        "id": 7234,
        "title": "Quantum reinforcement learning",
        "authors": "Niels M. P. Neumann, Paolo B. U. L. de Heer, Frank Phillipson",
        "published": "2023-2-22",
        "citations": 1,
        "abstract": "AbstractIn this paper, we present implementations of an annealing-based and a gate-based quantum computing approach for finding the optimal policy to traverse a grid and compare them to a classical deep reinforcement learning approach. We extended these three approaches by allowing for stochastic actions instead of deterministic actions and by introducing a new learning technique called curriculum learning. With curriculum learning, we gradually increase the complexity of the environment and we find that it has a positive effect on the expected reward of a traversal. We see that the number of training steps needed for the two quantum approaches is lower than that needed for the classical approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11128-023-03867-9"
    },
    {
        "id": 7235,
        "title": "Dynamic Path Planning for Autonomous Vehicles Using Adaptive Reinforcement Learning",
        "authors": "Karim Wahdan, Nourhan Ehab, Yasmin Mansy, Amr El Mougy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363300003636"
    },
    {
        "id": 7236,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 7237,
        "title": "Optimization of a Deep Reinforcement Learning Policy for Construction Manufacturing Control",
        "authors": "Ian Flood, Xiaoyan Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091400003546"
    },
    {
        "id": 7238,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 7239,
        "title": "Networked Personalized Federated Learning Using Reinforcement Learning",
        "authors": "Francois Gauthier, Vinay Chakravarthi Gogineni, Stefan Werner",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279781"
    },
    {
        "id": 7240,
        "title": "Survey on Practical Reinforcement Learning : from Imitation Learning to Offline Reinforcement Learning",
        "authors": "Dongsu Lee, Chanin Eom, Sungwoo Choi, Sungkwan Kim, Minhae Kwon",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.11.1405"
    },
    {
        "id": 7241,
        "title": "Taming a Wicked Problem Through Virtual Pre-simulation Gaming",
        "authors": "Roseanne Sadd, Jacinda Hills",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.34074/scop.4012006"
    },
    {
        "id": 7242,
        "title": "Combining Quest-Based Learning Gamification with Agile Project Management in Higher Education",
        "authors": "Jens-Martin Loebel",
        "published": "2023-11-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gem59776.2023.10389931"
    },
    {
        "id": 7243,
        "title": "Instance Weighted Incremental Evolution Strategies for Reinforcement Learning in Dynamic Environments",
        "authors": "Zhi Wang, Chunlin Chen, Daoyi Dong",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3160173"
    },
    {
        "id": 7244,
        "title": "Advancements in Deep Reinforcement Learning and Inverse Reinforcement Learning for Robotic Manipulation: Toward Trustworthy, Interpretable, and Explainable Artificial Intelligence",
        "authors": "Recep Ozalp, Aysegul Ucar, Cuneyt Guzelis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3385426"
    },
    {
        "id": 7245,
        "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
        "authors": "Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.977"
    },
    {
        "id": 7246,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 7247,
        "title": "Multi-Fidelity Reinforcement Learning with Control Variates",
        "authors": "Sami Khairy, Prasanna Balaprakash",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-181"
    },
    {
        "id": 7248,
        "title": "Quality Metrics for Reinforcement Learning for Edge Cloud and Internet-of-Things Systems",
        "authors": "Claus Pahl, Hamid Barzegar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194800003584"
    },
    {
        "id": 7249,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 7250,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 7251,
        "title": "Subgoal Reachability in Goal Conditioned Hierarchical Reinforcement Learning",
        "authors": "Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Bartłomiej Sobieski, Tomasz Trzciński",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012326200003636"
    },
    {
        "id": 7252,
        "title": "Prior-Information Enhanced Reinforcement Learning for Energy Management Systems",
        "authors": "Th´eo Zangato, Aomar Osmani, Pegah Alizadeh",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Amidst increasing energy demands and growing environmental concerns, the promotion of sustainable and energy-efficient practices has become imperative. This paper introduces a reinforcement learning-based technique for optimizing energy consumption and its associated costs, with a focus on energy management systems. A three-step approach for the efficient management of charging cycles in energy storage units within buildings is presented combining RL with prior knowledge. A unique strategy is adopted: clustering building load curves to discern typical energy consumption patterns, embedding domain knowledge into the learning algorithm to refine the agent’s action space and predicting of future observations to make real-time decisions. We showcase the effectiveness of our method using real-world data. It enables controlled exploration and efficient training of Energy Management System (EMS) agents. When compared to the benchmark, our model reduces energy costs by up to 15%, cutting down consumption during peak periods, and demonstrating adaptability across various building consumption profiles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.140207"
    },
    {
        "id": 7253,
        "title": "Digital Gaming for Cross-Cultural Learning",
        "authors": "Simone Titus, Dick Ng'ambi",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "Although game-based learning has gained significant attention in higher education globally, it is difficult to harness its engagement and interactions to improve student success. This paper argues that the use of digital games has the potential to interrupt social practices and increase engagement and interaction, thereby fostering meaningful learning. Using a mixed-method design, a digital game was used in a sport studies programme, involving 106 participants, over a two-year period. Data were collected through surveys, focus group discussions, and reflective blog posts. Structuration theory is considered as the theoretical lens, as it purports that recursive social activities of humans are continually recreated by human agents. The paper concludes that when participants engaged in a cross-cultural game-based learning environment, the social practices acquired through their academic career were interrupted, reshaped, and reproduced into new practices. A social constructivist game-based learning model to foster interaction within multi-cultural higher education classrooms is offered.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4018/ijgbl.331995"
    },
    {
        "id": 7254,
        "title": "A Reinforcement Learning Model for Optimal Treatment Strategies in Intensive Care: Assessment of the Role of Cardiorespiratory Features",
        "authors": "Cristian Drudi, Maximiliano Mollura, Li-wei H. Lehman, Riccardo Barbieri",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ojemb.2024.3367236"
    },
    {
        "id": 7255,
        "title": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",
        "authors": "Laura L. Pullum",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) has received significant interest in recent years, primarily because of the success of deep RL in solving many challenging tasks, such as playing chess, Go, and online computer games. However, with the increasing focus on RL, applications outside gaming and simulated environments require an understanding of the robustness, stability, and resilience of RL methods. To this end, we conducted a comprehensive literature review to characterize the available literature on these three behaviors as they pertain to RL. We classified the quantitative and theoretical approaches used to indicate or measure robustness, stability, and resilience behaviors. In addition, we identified the actions or events to which the quantitative approaches attempted to be stable, robust, or resilient. Finally, we provide a decision tree that is useful for selecting metrics to quantify behavior. We believe that this is the first comprehensive review of stability, robustness, and resilience, specifically geared toward RL.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130205"
    },
    {
        "id": 7256,
        "title": "Rapid Speed Change for Quadruped Robots via Deep Reinforcement Learning",
        "authors": "Seung Gyu Roh",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364428"
    },
    {
        "id": 7257,
        "title": "Improving anti-jamming decision-making strategies for cognitive radar via multi-agent deep reinforcement learning",
        "authors": "Wen Jiang, Yihui Ren, Yanping Wang",
        "published": "2023-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dsp.2023.103952"
    },
    {
        "id": 7258,
        "title": "Mitigating an adoption barrier of reinforcement learning-based control strategies in buildings",
        "authors": "Aakash Krishna G.S., Tianyu Zhang, Omid Ardakanian, Matthew E. Taylor",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2023.112878"
    },
    {
        "id": 7259,
        "title": "Learning Quadruped Locomotion Method Integrating Meta-Learning and Reinforcement Learning",
        "authors": "Xiafu Lv, Runming He, Chuangshi Wang, Yang Ou, Hong Wang, Zhenglin Chen",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450573"
    },
    {
        "id": 7260,
        "title": "Monetization Models in the Gaming Industry: A Comparative Analysis of In-game Purchases, Subscriptions, and Free-to-play Strategies",
        "authors": "Kumar Divya -",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "This publication devotes its attention to the commercial aspects of the gaming business, with a particular emphasis on investigating the various monetization techniques that are utilised by game developers and publishers. A complete examination of in-game purchases, subscription-based services, and free-to-play tactics is the goal of this research. The analysis will compare the economic viability of these techniques, as well as their impact on player engagement and their potential to be sustained over the long run. The purpose of this study is to provide those working in the gaming business with insights into the changing landscape of gaming commerce. These insights will aid industry professionals in making educated decisions regarding the creation of revenue and the pleasure of players. The gaming industry has witnessed unprecedented growth in recent years, and as it continues to expand, the focus on commercial strategies becomes increasingly critical for developers and publishers. This journal aims to delve into the intricate world of gaming commerce, specifically scrutinizing the diverse monetization models prevalent in the industry. By thoroughly examining in-game purchases, subscription-based services, and free-to-play strategies, this research seeks to unravel the economic intricacies, assess their impact on player engagement, and evaluate their long-term sustainability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2024.v06i01.13817"
    },
    {
        "id": 7261,
        "title": "Teaching health equity through gaming",
        "authors": "Kelli A. Copenhaver, Paige S. Randall, Jennie C. De Gagne",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.teln.2023.05.014"
    },
    {
        "id": 7262,
        "title": "“I am gaming, you are gaming” - computer gaming habits and romantic relationship satisfaction",
        "authors": "",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.53453/ms.2024.1.9"
    },
    {
        "id": 7263,
        "title": "Longer duration intertrial intervals without visual stimuli have reinforcement value and increase the rate of reinforcement and punishment learning in computer-based discriminations in humans",
        "authors": "Xiaojin Ma, Blair Bracciano, Nicole Hoppas, Sydney Zimmerman, Charles L. Pickens",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.lmot.2022.101867"
    },
    {
        "id": 7264,
        "title": "Maximising Undergraduate Medical Radiation Students’ Learning Experiences Using Cloud-Based Computed Tomography (CT) Software",
        "authors": "Minh Chau, Elio Stefan Arruzza",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Background Simulation-based learning is a crucial educational tool for disciplines involving work-integrated learning and clinical practice. Though its uptake is becoming increasingly common in a range of fields, this uptake is less profound in diagnostic radiography and computed tomography. Aim This study explored whether CT simulator software may be a viable option to facilitate the development of practical clinical skills in an effective, safe and supported environment. Methods A cross-sectional mixed methods design was employed. Students in their third year of study undertook formal simulation CT learning using the Siemens SmartSimulator, prior to a six-week off-campus clinical experience. A pre- (n = 42, response rate = 39%) and post-clinical placement Likert scale survey was completed (n = 21, retention rate = 50%), as well as focus group interviews to gather qualitative data (n = 21). Thematic analysis was employed to explore how the simulator developed students’ knowledge of CT concepts and preparedness for clinical placement. Results Survey scores were high, particularly in terms of satisfaction and relevancy. Focus groups drew attention to the software’s capacity to build on foundational principles, prepare students for placement and closely emulate the clinical environment. Students highlighted the need for continual guidance and clinical relevance and maintained that interactive simulation was inferior to real-world clinical placement. Conclusion The integration of CT simulator software has the potential to increase knowledge, confidence, and student preparation for the clinical environment. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231178491"
    },
    {
        "id": 7265,
        "title": "Student Motivation in Augmented Reality-Enhanced Gamified STEM Learning Settings",
        "authors": "Jennifer Tiede, Silke Grafe, Eleni Mangina",
        "published": "2023-11-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gem59776.2023.10390444"
    },
    {
        "id": 7266,
        "title": "Learning Policies for Neural Network Architecture Optimization Using Reinforcement Learning",
        "authors": "Raghav Vadhera, Manfred Huber",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Deep learning systems tend to be very sensitive to the specific network architecture both in terms of learning ability and performance of the learned solution. This, together with the difficulty of tuning neural network architectures leads to a need for automatic network optimization. Previous work largely optimizes a network for one specific problem using architecture search, requiring significant amounts of time training different architectures during optimization. To address this and to open up the potential for transfer across tasks, this paper presents a novel approach that uses Reinforcement Learning to learn a policy for network optimization in a derived architecture embedding space that incrementally optimizes the network for the given problem. By utilizing policy learning and an abstract problem embedding, this approach brings the promise of transfer of the policy across problems and thus the potential optimization of networks for new problems without the need for excessive additional training. For an initial evaluation of the base capabilities, experiments for a standard classification problem are performed in this paper, showing the ability of the approach to optimize the architecture for a specific problem within a given rang of fully connected networks, and indicating its potential for learning effective policies to automatically improve network architectures.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32473/flairs.36.133380"
    },
    {
        "id": 7267,
        "title": "Knowledge Distillation and Reward Shaping of Deep Reinforcement Learning",
        "authors": "Yue Mei",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424889"
    },
    {
        "id": 7268,
        "title": "Foundation -- Reinforcement -- Application: Training Strategies for Material Retention",
        "authors": "Michael Mancini",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.13182/conte23-40469"
    },
    {
        "id": 7269,
        "title": "Exploring the Synergy of Prompt Engineering and Reinforcement Learning for Enhanced Control and Responsiveness in Chat GPT",
        "authors": "",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "Conversational AI systems, such as Chat GPT, have exhibited remarkable performance in generating human-like responses. However, achieving consistent control and responsiveness remains a challenge. This research paper explores the combined effects of prompt engineering and reinforcement learning techniques in enhancing control and responsiveness in Chat GPT. Our experiments demonstrate significant improvements in the model’s performance across diverse domains and tasks. We discuss the implications of these findings for various real-world applications, such as customer support, virtual assistants, content generation, and education, and provide insights into future research directions and ethical considerations in the development of more reliable, controllable, and effective conversational AI systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.02"
    },
    {
        "id": 7270,
        "title": "Design of PID Controller using Reinforcement Learning",
        "authors": "Ashis De, Barun Mazumdar, Aritra Dhabal, Saikat Bhattacharjee, Aridip Maity, Sourav Bandopadhyay",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55248/gengpi.4.1123.113004"
    },
    {
        "id": 7271,
        "title": "DRL4HFC: Deep Reinforcement Learning for Container-Based Scheduling in Hybrid Fog/Cloud System",
        "authors": "Ameni Kallel, Molka Rekik, Mahdi Khemakhem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012356800003636"
    },
    {
        "id": 7272,
        "title": "Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments",
        "authors": "Georg Kruse, Theodora-Augustina Drăgan, Robert Wille, Jeanette Miriam Lorenz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012353100003636"
    },
    {
        "id": 7273,
        "title": "Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning",
        "authors": "Zhichao Chen, Yutaka Nakamura, Hiroshi Ishiguro",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3267385"
    },
    {
        "id": 7274,
        "title": "Equivariant Reinforcement Learning for Quadrotor UAV",
        "authors": "Beomyeol Yu, Taeyoung Lee",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156379"
    },
    {
        "id": 7275,
        "title": "Environment Adversarial Reinforcement Learning",
        "authors": "John R. Cooper",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2405"
    },
    {
        "id": 7276,
        "title": "Rainfuzz: Reinforcement-Learning Driven Heat-Maps for Boosting Coverage-Guided Fuzzing",
        "authors": "Lorenzo Binosi, Luca Rullo, Mario Polino, Michele Carminati, Stefano Zanero",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011625300003411"
    },
    {
        "id": 7277,
        "title": "A Study Toward Multi-Objective Multiagent Reinforcement Learning Considering Worst Case and Fairness Among Agents",
        "authors": "Toshihiro Matsui",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011687100003393"
    },
    {
        "id": 7278,
        "title": "Reinforcement Learning for Radar Waveform Optimization",
        "authors": "Mario Coutino, Faruk Uysal",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149794"
    },
    {
        "id": 7279,
        "title": "Combining Deep Learning on Order Books with Reinforcement Learning for Profitable Trading",
        "authors": "Koti Jaddu, Paul Bilokon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4611708"
    },
    {
        "id": 7280,
        "title": "A deep actor critic reinforcement learning framework for learning to rank",
        "authors": "Vaibhav Padhye, Kailasam Lakshmanan",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126314"
    },
    {
        "id": 7281,
        "title": "Wargaming for Learning: How Educational Gaming Supports Student Learning and Perspectives",
        "authors": "Amanda M. Rosen, Lisa Kerr",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/15512169.2024.2304769"
    },
    {
        "id": 7282,
        "title": "Assessing the Impact of Immersion on Learning in Medical Students: A Pilot Study Comparing Two-Dimensional and Three-Dimensional Virtual Simulation",
        "authors": "Lucy Bray, Sebastian Spencer, Emma Pearson, Katerina Meznikova, David Hepburn",
        "published": "2023-10",
        "citations": 0,
        "abstract": "Introduction Virtual simulation within health professions education can be categorized as two-dimensional or three-dimensional, dependent upon how users interact with the scenario. A core difference between these two categories is the degree of immersion experienced by the user. The importance of immersion for knowledge-based learning is unclear. Hence, this pilot study compares two virtual simulators to determine the impact of immersion on knowledge acquisition and retention and user experience. Methods This randomized crossover trial consisted of 25 fifth-year medical students attending two consecutive teaching sessions using two-dimensional and three-dimensional virtual simulation, respectively. Multiple-choice questions, completed immediately before and after, and one-month after the sessions, were employed to determine knowledge acquisition and retention. Questionnaires, consisting of Likert-scale and open-ended questions, evaluated user experience. Quantitative data was analyzed using a Student’s t-test and qualitative data was analyzed using thematic analysis. Results Both interventions demonstrated statistically significant levels of knowledge acquisition and retention, though there was no significant difference in the extent of learning between the simulators. The two interventions offered valuable and acceptable approaches to virtual simulation, though Likert-scale responses indicated that participants significantly preferred three-dimensional virtual simulation. Free-form responses revealed themes of education and technology, with subthemes of desirability, learning, curriculum integration, fidelity, hardware and software. Conclusions The findings indicate that higher levels of immersion do not appear to offer greater educational benefit, with two-dimensional simulation possibly offering an equally valuable learning experience to three-dimensional simulation. Participants appeared to significantly prefer three-dimensional virtual simulation, though potential uses for both simulators were identified. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10468781231189287"
    },
    {
        "id": 7283,
        "title": "Multi-Domain Active Learning for Multi-Agent Reinforcement Learning",
        "authors": "Devarani Devi Ningombam",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icidea59866.2023.10295208"
    },
    {
        "id": 7284,
        "title": "Portfolio dynamic trading strategies using deep reinforcement learning",
        "authors": "Min-Yuh Day, Ching-Ying Yang, Yensen Ni",
        "published": "2023-7-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08973-5"
    },
    {
        "id": 7285,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 7286,
        "title": "Multi‐agent reinforcement learning for process control: Exploring the intersection between fields of reinforcement learning, control theory, and game theory",
        "authors": "Yue Yifei, Samavedham Lakshminarayanan",
        "published": "2023-11",
        "citations": 3,
        "abstract": "AbstractThe application of reinforcement learning (RL) in process control has garnered increasing research attention. However, much of the current literature is focused on training and deploying a single RL agent. The application of multi‐agent reinforcement learning (MARL) has not been fully explored in process control. This work aims to: (i) develop a unique RL agent configuration that is suitable in a MARL control system for multiloop control, (ii) demonstrate the efficacy of MARL systems in controlling multiloop process that even exhibit strong interactions, and (iii) conduct a comparative study of the performance of MARL systems trained with different game‐theoretic strategies. First, we propose a design of an RL agent configuration that combines the functionalities of a feedback controller and a decoupler in a control loop. Thereafter, we deploy two such agents to form a MARL system that learns how to control a two‐input, two‐output system that exhibits strong interactions. After training, the MARL system shows effective control performance on the process. With further simulations, we examine how the MARL control system performs with increasing levels of process interaction and when trained with reward function configurations based on different game‐theoretic strategies (i.e., pure cooperation and mixed strategies). The results show that the performance of the MARL system is weakly dependent on the reward function configuration for systems with weak to moderate loop interactions. The MARL system with mixed strategies appears to perform marginally better than MARL under pure cooperation in systems with very strong loop interactions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cjce.24878"
    },
    {
        "id": 7287,
        "title": "Analysing Gaming Behaviour: Insights on Personality Traits",
        "authors": "Aikaterina Chatziavgeri, Maya Satratzemi",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "Educational games have become an integral part of the educational process at various levels, with their usage rapidly increasing as innovative e-learning methods. These games have proven to be highly engaging and effective in knowledge retention. Previous research primarily focused on the impact of violent video games on behaviour and the correlation between game behaviour and personality. Traditionally personality assessment relies on psychometric questionnaires, with the Big Five Inventory (BFI) being a widely used tool. However, these approaches often have certain drawbacks as respondents tend to carefully consider their answers, prioritizing correctness over authenticity. To address these limitations, novel approaches are being developed that incorporate gaming elements to indirectly measure personality. Therefore, an intriguing question arises: Can the subconscious moves, choices, and behaviours exhibited during gameplay serve as indicators of players' personality? In this case study, we developed an educational game focused on Databases courses for university students. The game aims to capture everyday life experiences at the university such as social connections and curiosity or willingness to try new things, based on the Five-Factor Model (OCEAN). The educational content is presented in the form of a quiz with four possible answers, providing appropriate feedback based on the selected responses. The objectives were to strengthen the knowledge and comprehension of Databases subject and also to gather information about players' gaming behaviour and thus predict their scores on two personality traits: Extraversion and Openness to Experience, based on the Five-Factor Model. A total of 149 computer science students of the University of Macedonia participated in the study by playing the game and completing the BFI questionnaire. We utilized classification algorithms to develop a model to predict student’s personality. The goodness of the model was assessed using different metrics and the results showed that it is effective to model both the extraversion and openness personality dimensions using serious games instead of questionnaires. These findings can be used by educators and game designers to develop personalized educational games taking into account learner’s personality and thus provide valuable insights for future research in this domain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.34190/ecel.22.1.1874"
    },
    {
        "id": 7288,
        "title": "Efficient Deep Reinforcement Learning for Smart Buildings: Integrating Energy Storage Systems Through Advanced Energy Management Strategies",
        "authors": "Artika Farhana, Nimmati Satheesh, Ramya M, Janjhyam Venkata Naga Ramesh, Yousef A. Baker El-Ebiary",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141257"
    },
    {
        "id": 7289,
        "title": "A Procedural Generation Platform to Create Randomized Gaming Maps using 2D Model and Machine Learning",
        "authors": "Nathan Lee, John Morris",
        "published": "2023-5-27",
        "citations": 0,
        "abstract": "As a video game developer, the most difficult problem I ran into was creating a map for the game, as it was difficult to create non repetitive and original gameplay [1]. My project proposes a solution to this problem as I use Answer Set Programming to create a program to procedurally generate maps for a video game [2]. In order to test its reliability, I allowed it to generate around 10,000 maps, stored the data of each of the maps, and used the common trends I find in the data to find problems with the program and fix it in the future. In developing a video game, level creation consumes a major portion of the development total time and level procedural generation techniques can potentially mitigate this problem. This research focused on developing a VVVVVV style level generator using Answer set programming for the game Mem.experiment which was developed at the same time. VVVVVV is a 2D puzzle platformer that uses changes in direction of gravity instead of jumping for the player's vertical movement [3]. During the development of the level generator, 10,000 levels were created. I found out that the average total time it took between generations is 45 seconds, and the average time for ASP to generate a map is 12 seconds [4]. This means that the process of displaying the generation took between 2x - 3x longer than generating the ASP solution.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130916"
    },
    {
        "id": 7290,
        "title": "Comparison of Advanced Control Strategies Applied to a Multiple-Degrees-of-Freedom Wave Energy Converter: Nonlinear Model Predictive Controller versus Reinforcement Learning",
        "authors": "Ali S. Haider, Kush Bubbar, Alan McCall",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "Achieving energy maximizing control of a Wave Energy Converter (WEC) not only needs a comprehensive dynamic model of the system—including nonlinear hydrodynamic effects and nonlinear characteristics of Power Take-Off (PTO)—but to treat the entire system using an integrated approach, i.e., as a cyber–physical system considering the WEC dynamics, control strategy, and communication interface. The resulting energy-maximizing optimization formulation leads to a non-quadratic and nonstandard cost function. This article compares the (1) Nonlinear Model Predictive Controller (NMPC) and (2) Reinforcement Learning (RL) techniques as applied to a class of multiple-degrees-of-freedom nonlinear WEC–PTO systems subjected to linear as well as nonlinear hydrodynamic conditions in simulation, using the WEC-Sim™ toolbox. The results show that with an optimal choice of RL agent and hyperparameters, as well as suitable training conditions, the RL algorithm is more robust under more stringent operating requirements, for which the NMPC algorithm fails to converge. Further, RL agents are computationally efficient on real-time target machines with a significantly reduced Task Execution Time (TET).",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/jmse11112120"
    },
    {
        "id": 7291,
        "title": "Transfer Learning for Online Prediction of Virtual Reality Cloud Gaming Traffic",
        "authors": "Sampreet Vaidya, Hatem Abou-Zeid, Diwakar Krishnamurthy",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437897"
    },
    {
        "id": 7292,
        "title": "An Industrial Use-Case for Reinforcement Learning: Optimizing a Production Planning in Stochastic Conditions",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/ijml.2024.14.1.1152"
    },
    {
        "id": 7293,
        "title": "Optimizing Naval Movement Using Deep Reinforcement Learning",
        "authors": "Joseph Coble, Armon Barton, Chris Darken, Scotty Black",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00062"
    },
    {
        "id": 7294,
        "title": "Research on Image Recognition based on Reinforcement Learning",
        "authors": "Jiabin Luo, Rongzhen Luo",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166036"
    },
    {
        "id": 7295,
        "title": "Improving Intrusion Detection Systems with Multi-Agent Deep Reinforcement Learning: Enhanced Centralized and Decentralized Approaches",
        "authors": "Amani Bacha, Farah Ktata, Faten Louati",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012124600003555"
    },
    {
        "id": 7296,
        "title": "Data Augmentation Through Expert-Guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning",
        "authors": "Giorgio Angelotti, Nicolas Drougard, Caroline Chanel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011633400003393"
    },
    {
        "id": 7297,
        "title": "Stress Management Strategies in Esports: An Exploratory Online Survey on Applied Practice",
        "authors": "Oliver Leis, Matthew Watson, Laura Swettenham, Ismael Pedraza-Ramirez, Franziska Lautenbach",
        "published": "2023-1-1",
        "citations": 5,
        "abstract": "Given the competitive nature of esports (e.g., maintaining focus and adaptive coping) and the increasing interest from practitioners in addressing stress management issues, empirical evidence on stress management strategies is needed that is tailored to the unique demands of esports. To ensure that ethical and professional standards are being met, it is important to first explore the factors that practitioners perceive to negatively impact the performance of esports players and the stress management strategies that are currently being used to support these players. Therefore, an online survey of 25 practitioners was conducted with results highlighting a variety of factors that were perceived to negatively impact players’ performance such as players’ ability to cope and lack of self-confidence. In addition, stress management strategies used before and after competition most frequently included strategies such as imagery, breathing techniques, and social support. Future research directions, limitations, and practical implications are discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1123/jege.2023-0002"
    },
    {
        "id": 7298,
        "title": "Reinforcement Learning and Advanced Reinforcement Learning to Improve Autonomous Vehicle Planning",
        "authors": "Avinash. J. Agrawal, Rashmi R. Welekar, Namita Parati, Pravin R. Satav, Uma Patel Thakur, Archana V. Potnurwar",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "Planning for autonomous vehicles is a challenging process that involves navigating through dynamic and unpredictable surroundings while making judgments in real-time. Traditional planning methods sometimes rely on predetermined rules or customized heuristics, which could not generalize well to various driving conditions. In this article, we provide a unique framework to enhance autonomous vehicle planning by fusing conventional RL methods with cutting-edge reinforcement learning techniques. To handle many elements of planning issues, our system integrates cutting-edge algorithms including deep reinforcement learning, hierarchical reinforcement learning, and meta-learning. Our framework helps autonomous vehicles make decisions that are more reliable and effective by utilizing the advantages of these cutting-edge strategies.With the use of the RLTT technique, an autonomous vehicle can learn about the intentions and preferences of human drivers by inferring the underlying reward function from expert behaviour that has been seen. The autonomous car can make safer and more human-like decisions by learning from expert demonstrations about the fundamental goals and limitations of driving. Large-scale simulations and practical experiments can be carried out to gauge the effectiveness of the suggested approach. On the basis of parameters like safety, effectiveness, and human likeness, the autonomous vehicle planning system's performance can be assessed. The outcomes of these assessments can help to inform future developments and offer insightful information about the strengths and weaknesses of the strategy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i7s.7526"
    },
    {
        "id": 7299,
        "title": "Reward Design for Deep Reinforcement Learning Towards Imparting Commonsense Knowledge in Text-Based Scenario",
        "authors": "Ryota Kubo, Fumito Uwano, Manabu Ohta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012456900003636"
    },
    {
        "id": 7300,
        "title": "Deep Reinforcement Learning",
        "authors": "Moez Krichen",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10306453"
    },
    {
        "id": 7301,
        "title": "Contrastive Learning Methods for Deep Reinforcement Learning",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3312383"
    },
    {
        "id": 7302,
        "title": "A reinforcement learning algorithm for scheduling parallel processors with identical speedup functions",
        "authors": "Farid Ziaei, Mohammad Ranjbar",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2023.100485"
    },
    {
        "id": 7303,
        "title": "Severity identification for internet gaming disorder using heart rate variability reactivity for gaming cues: a deep learning approach",
        "authors": "Sung Jun Hong, Deokjong Lee, Jinsick Park, Taekyung Kim, Young-Chul Jung, Young-Min Shon, In Young Kim",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "BackgroundThe diminished executive control along with cue-reactivity has been suggested to play an important role in addiction. Hear rate variability (HRV), which is related to the autonomic nervous system, is a useful biomarker that can reflect cognitive-emotional responses to stimuli. In this study, Internet gaming disorder (IGD) subjects’ autonomic response to gaming-related cues was evaluated by measuring HRV changes in exposure to gaming situation. We investigated whether this HRV reactivity can significantly classify the categorical classification according to the severity of IGD.MethodsThe present study included 70 subjects and classified them into 4 classes (normal, mild, moderate and severe) according to their IGD severity. We measured HRV for 5 min after the start of their preferred Internet game to reflect the autonomic response upon exposure to gaming. The neural parameters of deep learning model were trained using time-frequency parameters of HRV. Using the Class Activation Mapping (CAM) algorithm, we analyzed whether the deep learning model could predict the severity classification of IGD and which areas of the time-frequency series were mainly involved.ResultsThe trained deep learning model showed an accuracy of 95.10% and F-1 scores of 0.995 (normal), 0.994 (mild), 0.995 (moderate), and 0.999 (severe) for the four classes of IGD severity classification. As a result of checking the input of the deep learning model using the CAM algorithm, the high frequency (HF)-HRV was related to the severity classification of IGD. In the case of severe IGD, low frequency (LF)-HRV as well as HF-HRV were identified as regions of interest in the deep learning model.ConclusionIn a deep learning model using the time-frequency HRV data, a significant predictor of IGD severity classification was parasympathetic tone reactivity when exposed to gaming situations. The reactivity of the sympathetic tone for the gaming situation could predict only the severe group of IGD. This study suggests that the autonomic response to the game-related cues can reflect the addiction status to the game.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpsyt.2023.1231045"
    },
    {
        "id": 7304,
        "title": "Punisher: A Deep Reinforcement Learning Model Trained by Correcting Bad Actions",
        "authors": "Jianyi Yang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424763"
    }
]