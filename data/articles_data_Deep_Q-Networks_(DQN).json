[
    {
        "id": 16271,
        "title": "Deep Q Network (DQN), Double DQN, and Dueling DQN",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_8"
    },
    {
        "id": 16272,
        "title": "Bring Color to Deep Q-Networks: Limitations and Improvements of DQN Leading to Rainbow DQN",
        "authors": "Jonas Jäger, Felix Helfenstein, Fabian Scharf",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_12"
    },
    {
        "id": 16273,
        "title": "OneR-DQN: A botnet traffic detection model based on Deep Q Network algorithm in deep reinforcement learning",
        "authors": "Xiangyu Ma, Yongxin Feng, Yuntao Zhao, Yutao Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijsn.2023.10062404"
    },
    {
        "id": 16274,
        "title": "OneR-DQN: a botnet traffic detection model based on deep Q network algorithm in deep reinforcement learning",
        "authors": "Yutao Hu, Yuntao Zhao, Yongxin Feng, Xiangyu Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijsn.2024.137334"
    },
    {
        "id": 16275,
        "title": "Elastic step DQN: A novel multi-step algorithm to alleviate overestimation in Deep Q-Networks",
        "authors": "Adrian Ly, Richard Dazeley, Peter Vamplew, Francisco Cruz, Sunil Aryal",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.127170"
    },
    {
        "id": 16276,
        "title": "DQN Based Exit Selection in Multi-Exit Deep Neural Networks for Applications Targeting Situation Awareness",
        "authors": "Abhishek Vashist, Sharan Vidash Vidya Shanmugham, Amlan Ganguly, Sai Manoj P D",
        "published": "2022-1-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce53296.2022.9730182"
    },
    {
        "id": 16277,
        "title": "Energy Efficient Power Allocation in Massive MIMO based on Parameterized Deep DQN",
        "authors": "Shruti Sharma, Wonsik Yoon",
        "published": "No Date",
        "citations": 2,
        "abstract": "Machine learning offers advanced tools for efficient management of radio resources in modern wireless networks. In this study, we leverage a multi-agent deep reinforcement learning (DRL) approach, specifically the Parameterized Deep Q-Network (DQN), to address the challenging problem of power allocation and user association in massive multiple-input multiple-output (M-MIMO) communication networks. Our approach tackles a multi-objective optimization problem aiming to maximize network utility while meeting stringent quality of service requirements in M-MIMO networks. To address the non-convex and nonlinear nature of this problem, we introduce a novel multi-agent DQN framework. This framework defines a large action space, state space, and reward functions, enabling us to learn a near-optimal policy. Simulation results demonstrate the superiority of our Parameterized Deep DQN (PD-DQN) approach when compared to traditional DQN and RL methods. Specifically, we show that our approach outperforms traditional DQN methods in terms of convergence speed and final performance. Additionally, our approach shows 72.2 % and 108.5 % improvement over DQN methods and RL method respectively in handling large-scale multi-agent problems in M-MIMO networks.",
        "link": "http://dx.doi.org/10.20944/preprints202310.0066.v1"
    },
    {
        "id": 16278,
        "title": "SSPO-DQN spark: shuffled student psychology optimization based deep Q network with spark architecture for big data classification",
        "authors": "Bhaskar Kantapalli, Babu Rao Markapudi",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11276-022-03103-9"
    },
    {
        "id": 16279,
        "title": "Intelligent Module for System Trading of Financial Markets Assets Based on an Ensemble of Deep Neural Networks and the DQN Learning Algorithm",
        "authors": "Ivan Makarov, Maria Kovaleva, Timur Fakhrutdinov, Roman Gorbachev",
        "published": "2021-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ent50460.2021.9681753"
    },
    {
        "id": 16280,
        "title": "One-pot synthesis of S-scheme MoS2/g-C3N4 heterojunction as effective visible light photocatalyst",
        "authors": "Ha Tran Huu, My Duyen Nguyen Thi, Van Phuc Nguyen, Lan Nguyen Thi, Thi Thuy Trang Phan, Quoc Dat Hoang, Huy Hoang Luc, Sung Jin Kim, Vien Vo",
        "published": "2021-7-20",
        "citations": 60,
        "abstract": "AbstractDespite pioneering as the holy grail in photocatalysts, abundant reports have demonstrated that g-C3N4 performs poor photocatalytic activity due to its high recombination rate of photo-induced charge carriers. Many efforts have been conducted to overcome this limitation in which the semiconductor–semiconductor coupling strategies toward heterojunction formation were considered as the easiest but the most effective method. Herein, a one-pot solid-state reaction of thiourea and sodium molybdate as precursors at different temperatures under N2 gas was applied for preparing composites of MoS2/g-C3N4. The physicochemical characterization of the final products determines the variation in contents of components (MoS2 and g-C3N4) via the increase of synthesis temperature. The enhanced photocatalytic activity of the MoS2/g-C3N4 composites was evaluated by the degradation of Rhodamine B in an aqueous solution under visible light. Therein, composites synthesized at 500 °C showed the best photocatalytic performance with a degradation efficiency of 90%, much higher than that of single g-C3N4. The significant improvement in photocatalytic performance is attributed to the enhancement in light-harvesting and extension in photo-induced charge carriers’ lifetime of composites which are originated from the synergic effect between the components. Besides, the photocatalytic mechanism is demonstrated to well-fit into the S-scheme pathway with apparent evidences.",
        "link": "http://dx.doi.org/10.1038/s41598-021-94129-0"
    },
    {
        "id": 16281,
        "title": "An intelligent control for reducing third-party interference in oil and gas pipeline using Deep Q-Networks (DQN)",
        "authors": " Emem Patrick Ekpo,  James Eke",
        "published": "2024-3-30",
        "citations": 0,
        "abstract": "A nation's economic survival depends on its oil and gas pipelines. They must therefore be carefully inspected in order to enhance their efficiency and prevent product losses during the transportation of petroleum products. They could, however, fail, having negative effects on the environment, the economy, and safety. Therefore, evaluating the pipe's condition and quality would be extremely important. This research work performed an intelligent control for reducing third-party interference in oil and gas pipeline using Deep Q-Networks (DQN). The learning curve shows a steady improvement, indicating that the algorithm progressively learned and improved its performance over time. This observation demonstrates the effectiveness of the DQN algorithm in adapting and optimizing control strategies. Overall, the results of the analysis indicate that the DQN algorithm holds promise for mitigating third-party interference in oil pipelines.",
        "link": "http://dx.doi.org/10.30574/gjeta.2024.18.3.0233"
    },
    {
        "id": 16282,
        "title": "An Efficient Offloading Technique using DQN for MEC-IoT Networks",
        "authors": "Prashant Shukla, Sudhakar Pandey, Deepika Agarwal",
        "published": "2023-3-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112053"
    },
    {
        "id": 16283,
        "title": "EnRoute: A DQN based Energy Efficient Routing for URLLC in Next Generation Networks",
        "authors": "Akanksha Sharma, Sharda Tripathi",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ants59832.2023.10469593"
    },
    {
        "id": 16284,
        "title": "MAT-DQN: Toward Interpretable Multi-agent Deep Reinforcement Learning for Coordinated Activities",
        "authors": "Yoshinari Motokawa, Toshiharu Sugawara",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-86380-7_45"
    },
    {
        "id": 16285,
        "title": "Reactive Power Optimization Using Feed Forward Neural Deep Reinforcement Learning Method : (Deep Reinforcement Learning DQN algorithm)",
        "authors": "Mazhar Ali, Asad Mujeeb, Hameed Ullah, Saran Zeb",
        "published": "2020-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeees48850.2020.9121492"
    },
    {
        "id": 16286,
        "title": "Multi-objective vehicle path planning based on DQN",
        "authors": "Qingyu Huo",
        "published": "2022-10-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2640707"
    },
    {
        "id": 16287,
        "title": "Deep Reinforcement Learning with DQN vs. PPO in VizDoom",
        "authors": "Anton Zakharenkov, Ilya Makarov",
        "published": "2021-11-18",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cinti53070.2021.9668479"
    },
    {
        "id": 16288,
        "title": "DQN-GNN-Based User Association Approach for Wireless Networks",
        "authors": "Ibtihal Alablani, Mohammed J. F. Alenazi",
        "published": "2023-10-14",
        "citations": 1,
        "abstract": "In the realm of advanced mobile networks, such as the fifth generation (5G) and beyond, the increasing complexity and proliferation of devices and unique applications present a substantial challenge for User Association (UA) in wireless systems. The problem of UA in wireless networks is multifaceted and requires comprehensive exploration. This paper presents a pioneering approach to the issue, integrating a Deep Q-Network (DQN) with a Graph Neural Network (GNN) to enhance user-base station association in wireless networks. This novel approach surpasses recent methodologies, including Q-learning and max average techniques, in terms of average rewards, returns, and success rate. This superiority is attributed to its capacity to encapsulate intricate relationships and spatial dependencies among users and base stations in wireless systems. The proposed methodology achieves a success rate of 95.2%, outperforming other methodologies by a margin of up to 5.9%.",
        "link": "http://dx.doi.org/10.3390/math11204286"
    },
    {
        "id": 16289,
        "title": "Real-Time Stock Market Forecasting using Ensemble Deep Learning and Rainbow DQN",
        "authors": "Raj Shah, Ashutosh Tambe, Tej Bhatt, Uday Rote",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3586788"
    },
    {
        "id": 16290,
        "title": "Double DQN in Code",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_9"
    },
    {
        "id": 16291,
        "title": "Latency-aware computation offloading and DQN-based resource allocation approaches in SDN-enabled MEC",
        "authors": "Tianyu Du, Chunlin Li, Youlong Luo",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.adhoc.2022.102950"
    },
    {
        "id": 16292,
        "title": "Active Object Detection Using Double DQN and Prioritized Experience Replay",
        "authors": "Xiaoning Han, Huaping Liu, Fuchun Sun, Dongfang Yang",
        "published": "2018-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489296"
    },
    {
        "id": 16293,
        "title": "Performance Improvement of Laser-Charged Multi-UAV Networks Based on a DQN Approach",
        "authors": "Jimin Jeon, Howon Lee",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccnc51664.2024.10454855"
    },
    {
        "id": 16294,
        "title": "Verteilungs-DQN: Die ganze Geschichte",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.007"
    },
    {
        "id": 16295,
        "title": "Simulation Results of a DQN Based AAV Testbed in Corner Environment: A Comparison Study for Normal DQN and TLS-DQN",
        "authors": "Nobuki Saito, Tetsuya Oda, Aoto Hirata, Kyohei Toyoshima, Masaharu Hirota, Leonard Barolli",
        "published": "2022",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-79728-7_16"
    },
    {
        "id": 16296,
        "title": "DQN-based Adaptive Decentralized Congestion Control in Cellular-V2X Networks",
        "authors": "Eunhwa Lee, Yeona Kim, Cheol Mun",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14801/jkiit.2023.21.11.89"
    },
    {
        "id": 16297,
        "title": "Deep Reinforcement Learning in VizDoom via DQN and Actor-Critic Agents",
        "authors": "Maria Bakhanova, Ilya Makarov",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-85030-2_12"
    },
    {
        "id": 16298,
        "title": "Power Allocation in Multi-Agent Networks via Dueling DQN Approach",
        "authors": "Zhihao Xuan, Guiyi Wei, Zhengwei Ni",
        "published": "2021-10-22",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsip52628.2021.9688607"
    },
    {
        "id": 16299,
        "title": "A Deep Q-network (DQN) Based Path Planning Method for Mobile Robots",
        "authors": "Siyu Zhou, Xin Liu, Yingfu Xu, Jifeng Guo",
        "published": "2018-8",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icinfa.2018.8812452"
    },
    {
        "id": 16300,
        "title": "DQN-Based Multi-User Power Allocation for Hybrid RF/VLC Networks",
        "authors": "Bekir Sait Ciftler, Mohamed Abdallah, Abdulmalik Alwarafy, Mounir Hamdi",
        "published": "2021-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc42927.2021.9500564"
    },
    {
        "id": 16301,
        "title": "Flexible feedback intelligent routing algorithm based on DQN for large-scale LEO networks",
        "authors": "Zongyi Luo, Shichao Jin, Tao Dong, Jie Yin",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3023513"
    },
    {
        "id": 16302,
        "title": "A Deep Reinforcement Learning Method for Mobile Robot Collision Avoidance based on Double DQN",
        "authors": "Xidi Xue, Zhan Li, Dongsheng Zhang, Yingxin Yan",
        "published": "2019-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie.2019.8781522"
    },
    {
        "id": 16303,
        "title": "Dueling-DQN Based Spectrum Sharing Between MIMO Radar and Cellular Networks",
        "authors": "Atiquzzaman Mondal, Aparajita Dutta, Sudip Biswas",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eucnc/6gsummit58263.2023.10188273"
    },
    {
        "id": 16304,
        "title": "IEDQN: Information Exchange DQN with a Centralized Coordinator for Traffic Signal Control",
        "authors": "Donghan Xie, Zhi Wang, Chunlin Chen, Daoyi Dong",
        "published": "2020-7",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206820"
    },
    {
        "id": 16305,
        "title": "Resource Allocation in NR-V2X Mode 2 Using Multi Agent DQN",
        "authors": "Insung Lee, Duk Kyung Kim",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icufn57995.2023.10200903"
    },
    {
        "id": 16306,
        "title": "Secure Relay Selection with Outdated CSI in Cooperative Wireless Vehicular Networks: A DQN Approach",
        "authors": "Esraa M. Ghourab, Lina Bariah, Sami Muhaidat, Paschalis C. Sofotasios, Mahmoud Al-Qutayri, Ernesto Damiani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Cooperative communications is a core research area in wireless vehicular networks (WVNs), thanks to its capability to mitigate fading and improve spectral efficiency. In a cooperative scenario, the performance of the system is improved by selecting the best relay for data transmission among a group of available relays. However, due to the mobility of WVNs, the best relay is often selected in practice based on outdated channel state information (CSI), which in turn affects the overall system performance. Therefore, there is a need for a robust relay selection scheme (RSS) that improves the overall achievable performance of an outdated CSI. Motivated by this and considering the advantageous features of autoregressive moving average (ARMA), in the present work we model a cooperative vehicular communication scenario with relay selection as a Markov decision process (MDP) and propose two deep Q-networks (DQNs), namely DQN-RSS and DQN-RSS-ARMA. In the proposed framework, two deep reinforcement learning (RL)-based RSS are trained based on the intercept probability, aiming to select the optimal vehicular relay from a set of multiple relays. We then compare the proposed RSS with the conventional methods and evaluate the performance of the network from the security point of view. Simulation results show that DQN-RSS and DQN-RSS-ARMA perform better than conventional approaches, and they reduce intercept probability by approximately 15\\% and 30\\%, respectively, compared to the ARMA approach.</p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21779915"
    },
    {
        "id": 16307,
        "title": "A dynamic algorithm for trust inference based on double DQN in the internet of things",
        "authors": "Xiaodong Zhuang, Xiangrong Tong",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.dcan.2022.12.010"
    },
    {
        "id": 16308,
        "title": "Retracted: Intelligent Offloading Decision and Resource Allocations Schemes Based on RNN/DQN for Reliability Assurance in Software-Defined Massive Machine-Type Communications",
        "authors": "",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1155/2023/9818236"
    },
    {
        "id": 16309,
        "title": "Applying DQN solutions in fog-based vehicular networks: Scheduling, caching, and collision control",
        "authors": "Seongjin Park, Younghwan Yoo, Chang-Woo Pyo",
        "published": "2022-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.vehcom.2021.100397"
    },
    {
        "id": 16310,
        "title": "DQN-based Coverage Maximization for Mobile Video Camera Networks",
        "authors": "Seungho Lee, Si Young Jang, Soon J. Hyun, Dongman Lee",
        "published": "2021-1-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccnc49032.2021.9369618"
    },
    {
        "id": 16311,
        "title": "MUSK-DQN: Multi-UBS Selective-K Deep Q-Network for Maximizing Energy-Efficiency",
        "authors": "Seungmin Lee, Howon Lee",
        "published": "2023-1-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccnc51644.2023.10059845"
    },
    {
        "id": 16312,
        "title": "Activation Control of Base Stations Based on Multi-agent DQN for Heterogeneous Networks",
        "authors": "Daiki Kato, Yuto Muroki, Nobuhide Nonaka, Kenichi Higuchi",
        "published": "2022-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-fall57202.2022.10013050"
    },
    {
        "id": 16313,
        "title": "Secure Relay Selection with Outdated CSI in Cooperative Wireless Vehicular Networks: A DQN Approach",
        "authors": "Esraa M. Ghourab, Lina Bariah, Sami Muhaidat, Paschalis C. Sofotasios, Mahmoud Al-Qutayri, Ernesto Damiani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Cooperative communications is a core research area in wireless vehicular networks (WVNs), thanks to its capability to mitigate fading and improve spectral efficiency. In a cooperative scenario, the performance of the system is improved by selecting the best relay for data transmission among a group of available relays. However, due to the mobility of WVNs, the best relay is often selected in practice based on outdated channel state information (CSI), which in turn affects the overall system performance. Therefore, there is a need for a robust relay selection scheme (RSS) that improves the overall achievable performance of an outdated CSI. Motivated by this and considering the advantageous features of autoregressive moving average (ARMA), in the present work we model a cooperative vehicular communication scenario with relay selection as a Markov decision process (MDP) and propose two deep Q-networks (DQNs), namely DQN-RSS and DQN-RSS-ARMA. In the proposed framework, two deep reinforcement learning (RL)-based RSS are trained based on the intercept probability, aiming to select the optimal vehicular relay from a set of multiple relays. We then compare the proposed RSS with the conventional methods and evaluate the performance of the network from the security point of view. Simulation results show that DQN-RSS and DQN-RSS-ARMA perform better than conventional approaches, and they reduce intercept probability by approximately 15\\% and 30\\%, respectively, compared to the ARMA approach.</p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21779915.v1"
    },
    {
        "id": 16314,
        "title": "Intelligent Microgrid Energy Management Method Based on Deep Q Network (DQN)",
        "authors": "Xinghui OuYang, Sixia Fan",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "Abstract\nWith the high proportion of renewable energy in microgrid, it brings more and more challenges to the energy management of microgrid. In the meantime, the rise of artificial intelligence has brought new solutions for microgrid energy management. This paper mainly studies the method of integrating deep learning with microgrid. When a variety of flexible resources are connected to microgrid, different energy sources can be coordinated and controlled by determining preferential resources, various demand control signals and real-time electricity prices. In this paper, a deep Q network (DQN) based on deep reinforcement learning algorithm is implemented and combined with a new microgrid model. The numerical results show the effectiveness, adaptability and stability of the deep reinforcement learning method in the optimization control problem of microgrid, and also show the possibility of combining artificial intelligence with power system.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2465/1/012030"
    },
    {
        "id": 16315,
        "title": "DQN-AF: Deep Q-Network based Adaptive Forwarding Strategy for Named Data Networking",
        "authors": "Ygor Amaral B. L. de Sena, Kelvin Lopes Dias, Cleber Zanchettin",
        "published": "2020-11-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/latincom50620.2020.9282301"
    },
    {
        "id": 16316,
        "title": "A DQN-based Joint Spectrum and Computing Resource Allocation Algorithm for Multi-Service MEC Networks",
        "authors": "Feifan Zhou, Jun Zheng, Luyinru Yang, Feng Yan",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9838558"
    },
    {
        "id": 16317,
        "title": "A DQN-Based Cache Strategy for Mobile Edge Networks",
        "authors": "Siyuan Sun, Junhua Zhou, Jiuxing Wen, Yifei Wei, Xiaojun Wang",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2022.020471"
    },
    {
        "id": 16318,
        "title": "A DQN-based Joint Spectrum and Computing Resource Allocation Algorithm for MEC Networks",
        "authors": "Li Yu, Jun Zheng, Yuying Wu, Feifan Zhou, Feng Yan",
        "published": "2022-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001567"
    },
    {
        "id": 16319,
        "title": "Blockchain and DQN Enabled Co-Evolutionary Routing Scheme in UAV Networks",
        "authors": "Pengcheng Zhao, Yuxin Lu, Yunkai Wei, Supeng Leng",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocomwkshps57453.2023.10225997"
    },
    {
        "id": 16320,
        "title": "Channel-Occupation-Aware Resource Allocation in LoRa Networks: a DQN-and-Optimization-Aided Approach",
        "authors": "Zhen Qin, Jinming Li, Bo Gu",
        "published": "2022-4-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771911"
    },
    {
        "id": 16321,
        "title": "A DQN-Based Joint Computing Offloading and Resource Allocation Algorithm for MEC Networks",
        "authors": "Li Yu, Shurui Jiang, Jun Zheng, Feng Yan, Shuyuan Zhao",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279106"
    },
    {
        "id": 16322,
        "title": "A Federated Learning and DQN based Cooperative Resource Allocation Algorithm for Multi-Service MEC Networks",
        "authors": "Feifan Zhou, Shurui Jiang, Jun Zheng, Feng Yan",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10183124"
    },
    {
        "id": 16323,
        "title": "Convolutional neural network-based deep Q-network (CNN-DQN) resource management in cloud radio access network",
        "authors": "Amjad Iqbal, Mau-Luen Tham, Yoong Choon Chang",
        "published": "2022-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/jcc.2022.00.025"
    },
    {
        "id": 16324,
        "title": "Energy Efficient Training Task Assignment Scheme for Mobile Distributed Deep Learning Scenario Using DQN",
        "authors": "Yutong Liu, Lianping Zhang, Yifei Wei, Zhaoying Wang",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsnt47585.2019.8962496"
    },
    {
        "id": 16325,
        "title": "Towards Mitigating Probable Road Mishaps through DQN Based Deep Reinforcement Learning",
        "authors": "Md. Farhan Sadiq, Nafisa Tasneem, Mehnaj Ahmed, Salman Md. Sultan, Sakib Hasan",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecbios51820.2021.9510732"
    },
    {
        "id": 16326,
        "title": "A Deep Q-Network Eith Experience Optimization (DQN-EO) for Atari's Space Invaders and Its Performance Evaluation",
        "authors": "Elis Kulla",
        "published": "2022-4-1",
        "citations": 0,
        "abstract": "During recent years, the deep Q-Learning is used to solve different complex problems in different fields. However, Deep Q-Learning does not have a unified method for solving certain problems because different problems require specific settings and parameters. This paper proposes a Deep Q-Network with Experience Optimization for Atari’s “Space Invaders” environment called DQN-EO. Training and testing results are presented. The performance evaluation results show that while using the proposed algorithm the agent is better at avoiding enemy bullets by 37.7% (longer lifetime) and destroying enemy ships by 14.5% (higher score).",
        "link": "http://dx.doi.org/10.4018/ijdst.296249"
    },
    {
        "id": 16327,
        "title": "Autonomous Maneuver Decision of UAV Based on Deep Reinforcement Learning: Comparison of DQN and DDPG",
        "authors": "Yu Wang, Tianjun Ren, Zilin Fan",
        "published": "2022-8-15",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033863"
    },
    {
        "id": 16328,
        "title": "Comparison of MPPT based on Deep Reinforcement Learning by DQN, DDPG and TD3",
        "authors": "Jayandi Panggabean, Nana Sutisna, Infall Syafalni, Trio Adiono",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apsipaasc58517.2023.10317341"
    },
    {
        "id": 16329,
        "title": "Deep Q-Value Neural Network (DQN) Reinforcement Learning for the Techno-Economic Optimization of a Solar-Driven Nanofluid-Assisted Desalination Technology",
        "authors": "Sina Jafari, Siamak Hoseinzadeh, Ali Sohani",
        "published": "2022-7-18",
        "citations": 7,
        "abstract": "A solar-driven desalination system, featuring a single-slope solar still is studied here. For this design, Al2O3 nanofluid is utilized, and the condition achieving the highest efficiency and cost-effectiveness is found using a reinforcement learning called a deep Q-value neural network (DQN). The results of optimization are implemented for the built experimental setup. Experimental data obtained under the climatic conditions of Tehran, Iran, are employed to compare the enhancement potential of the optimized solar still system with nanofluid (OSTSWNF) with the solar still system with water (STSWWA). The hourly fluid temperatures in the basin as well as the hourly and cumulative freshwater production (HFWP and CFWP) are discussed. A number of other parameters, including daily water production and efficiency in addition to the cost per liter (CPL) of the resulting desalinated water, are also taken into account. The results reveal that annual water production increases from 1326.8 L to 1652.4 L, representing ~25% growth. Moreover, the annual average efficiency improves by ~32%, rising from 41.6% to 54.7%. A great economic enhancement is seen as well, with the CPL decreasing by ~8%, i.e., from USD 0.0258/L to USD 0.0237/L.",
        "link": "http://dx.doi.org/10.3390/w14142254"
    },
    {
        "id": 16330,
        "title": "Ga-DQN: A Gravity-aware DQN Based UAV Path Planning Algorithm",
        "authors": "Zhicheng Xu, Qi Wang, Fuchen Kong, Hualong Yu, Shang Gao, Demin Pan",
        "published": "2022-10-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus55513.2022.9986557"
    },
    {
        "id": 16331,
        "title": "Deep Q-Network (DQN): Reinforcement Learning Based Approach for Secure Social Distancing Adherence with SARS-CoV-2 in Public Places",
        "authors": "",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "The estimates taken far and wide to deal with the SARS-CoV-2 pandemic, limiting travel, shuttering superfluous organizations and implementing all social separating arrangements, are having serious monetary consequences. a noteworthy decrease in economic action spread over the economy the world, lasting in excess of a few months, typically clear in genuine GDP. Where it is formally announced a downturn. To quicken a strong expected recuperation with rising protectionism and unilateralism. There is a requirement for individuals to come out and face the circumstance. Despite the fact that it is established that separating individuals and investigating their contacts would be inadequate to control the SARS-CoV-2 pandemic, in light of the fact that there would be an excess of deferral between the beginning of indications and seclusion. Consequently, in these sorts of conditions it is to keep people groups from infection influence and early anticipation of these tainted individuals may prompt re development the economy too. We built up a numerical model utilizing profound Deep reinforcement learning (DRL) which is poised to revolutionize the field of artificial intelligence and the use of central algorithms in deep RL, specifically the deep Q-network (DQN), trust region policy optimization (TRPO). The proposed astute checking framework can be utilized as a reciprocal apparatus to be introduced at better places and consequently screen individuals receiving the security rules. With these prudent estimations, people will have the option to win this battle against SARS-CoV-2.",
        "link": "http://dx.doi.org/10.46632/jdaai/2/3/11"
    },
    {
        "id": 16332,
        "title": "Deep Adaptive Algorithms for Local Urban Traffic Control: Deep Reinforcement Learning with DQN",
        "authors": "P V R Subba Rao, Venkata Ramana Murthy Polisetty, K Krishna Jayanth, N V Sai Manoj, Vankadari Mohith, R Prasanna Kumar",
        "published": "2024-1-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/idciot59759.2024.10467630"
    },
    {
        "id": 16333,
        "title": "Double DQN Based Computing Offloading Scheme for Fog Radio Access Networks",
        "authors": "Fan Jiang, Xiaolin Zhu, Changyin Sun",
        "published": "2021-7-28",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc52777.2021.9580391"
    },
    {
        "id": 16334,
        "title": "Joint Resource Allocation Scheme Based Multi-agent DQN for Massive MIMO-NOMA Systems",
        "authors": "Cao Yanmei, Wang Lin, Liu Jiaqing, Zhao Jun, Yan Lin, Zhang Guomei",
        "published": "2022-6-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsn55126.2022.9817605"
    },
    {
        "id": 16335,
        "title": "Energy Efficient Power Allocation in Massive MIMO Based on Parameterized Deep DQN",
        "authors": "Shruti Sharma, Wonsik Yoon",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "Machine learning offers advanced tools for efficient management of radio resources in modern wireless networks. In this study, we leverage a multi-agent deep reinforcement learning (DRL) approach, specifically the Parameterized Deep Q-Network (DQN), to address the challenging problem of power allocation and user association in massive multiple-input multiple-output (M-MIMO) communication networks. Our approach tackles a multi-objective optimization problem aiming to maximize network utility while meeting stringent quality of service requirements in M-MIMO networks. To address the non-convex and nonlinear nature of this problem, we introduce a novel multi-agent DQN framework. This framework defines a large action space, state space, and reward functions, enabling us to learn a near-optimal policy. Simulation results demonstrate the superiority of our Parameterized Deep DQN (PD-DQN) approach when compared to traditional DQN and RL methods. Specifically, we show that our approach outperforms traditional DQN methods in terms of convergence speed and final performance. Additionally, our approach shows 72.2% and 108.5% improvement over DQN methods and the RL method, respectively, in handling large-scale multi-agent problems in M-MIMO networks.",
        "link": "http://dx.doi.org/10.3390/electronics12214517"
    },
    {
        "id": 16336,
        "title": "DRL-M4MR: An intelligent multicast routing approach based on DQN deep reinforcement learning in SDN",
        "authors": "Chenwei Zhao, Miao Ye, Xingsi Xue, Jianhui Lv, Qiuxiang Jiang, Yong Wang",
        "published": "2022-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.phycom.2022.101919"
    },
    {
        "id": 16337,
        "title": "Cross DQN: Cross Deep Q Network for Ads Allocation in Feed",
        "authors": "Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, Dong Wang",
        "published": "2022-4-25",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3485447.3512109"
    },
    {
        "id": 16338,
        "title": "Deep Reinforcement Learning-Based DQN Agent Algorithm for Visual Object Tracking in a Virtual Environmental Simulation",
        "authors": "Jin-Hyeok Park, Khurshedjon Farkhodov, Suk-Hwan Lee, Ki-Ryong Kwon",
        "published": "2022-3-22",
        "citations": 11,
        "abstract": "The complexity of object tracking models among hardware applications has become a more in-demand task to accomplish with multifunctional algorithm skills in various indeterminable environment tracking conditions. Experimenting with the virtual realistic simulator brings new dependencies and requirements, which may cause problems while experimenting with runtime processing. The goal of this paper is to present an object tracking framework that differs from the most advanced tracking models by experimenting with virtual environment simulation (Aerial Informatics and Robotics Simulation—AirSim, City Environ) using one of the Deep Reinforcement Learning Models named as Deep Q-Learning algorithms. Our proposed network examines the environment using a deep reinforcement learning model to regulate activities in the virtual simulation environment and utilizes sequential pictures from the realistic VCE (Virtual City Environ) model as inputs. Subsequently, the deep reinforcement network model was pretrained using multiple sequential training image sets and fine-tuned for adaptability during runtime tracking. The experimental results were outstanding in terms of speed and accuracy. Moreover, we were unable to identify any results that could be compared to the state-of-the-art methods that use deep network-based trackers in runtime simulation platforms, since this testing experiment was conducted on the two public datasets VisDrone2019 and OTB-100, and achieved better performance among compared conventional methods.",
        "link": "http://dx.doi.org/10.3390/app12073220"
    },
    {
        "id": 16339,
        "title": "DQN-based energy-efficient routing algorithm in software-defined data centers",
        "authors": "Zan Yao, Ying Wang, Xuesong Qiu",
        "published": "2020-6",
        "citations": 6,
        "abstract": "With the rapid development of data centers in smart cities, how to reduce energy consumption and how to raise economic benefits and network performance are becoming an important research subject. In particular, data center networks do not always run at full load, which leads to significant energy consumption. In this article, we focus on the energy-efficient routing problem in software-defined network–based data center networks. For the scenario of in-band control mode of software-defined data centers, we formulate the dual optimal objective of energy-saving and the load balancing between controllers. In order to cope with a large solution space, we design the deep Q-network-based energy-efficient routing algorithm to find the energy-efficient data paths for traffic flow and control paths for switches. The simulation result reveals that the deep Q-network-based energy-efficient routing algorithm only trains part of the states and gets a good energy-saving effect and load balancing in control plane. Compared with the solver and the CERA heuristic algorithm, energy-saving effect of the deep Q-network-based energy-efficient routing algorithm is almost the same as the heuristic algorithm; however, its calculation time is reduced a lot, especially in a large number of flow scenarios; and it is more flexible to design and resolve the multi-objective optimization problem.",
        "link": "http://dx.doi.org/10.1177/1550147720935775"
    },
    {
        "id": 16340,
        "title": "Enhanced DQN in Task Offloading Across Multi-Tier Computing Networks",
        "authors": "Jiang Limin, Zhang Ke",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip60502.2023.10387060"
    },
    {
        "id": 16341,
        "title": "Traffic Signal Control with Deep Q-Learning Network (DQN) Algorithm at Isolated Intersection",
        "authors": "Fan Qi, Rui He, Longhao Yan, Junfeng Yao, Ping Wang, Xiangmo Zhao",
        "published": "2022-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033656"
    },
    {
        "id": 16342,
        "title": "Edge Caching Replacement Optimization for D2D Wireless Networks via Weighted Distributed DQN",
        "authors": "Ruibin Li, Yiwei Zhao, Chenyang Wang, Xiaofei Wang, Victor C. M. Leung, Xiuhua Li, Tarik Taleb",
        "published": "2020-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc45663.2020.9120616"
    },
    {
        "id": 16343,
        "title": "3-Lane Based Traffic Signal Control Using Sequential-Duel Deep Q-Network (SD-DQN)",
        "authors": "Sumaiya Bhumeka, Nishat Tasnim Tanah, Alifun Nahar, Tahira Alam, Salman Md. Sultan",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecbios57802.2023.10218510"
    },
    {
        "id": 16344,
        "title": "Performance Evaluation of DQN, DDQN and Dueling DQN in Heart Disease Prediction",
        "authors": "Amit Sharma, Deepika Pantola, Suneet Kumar Gupta, Divya Kumari",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smarttechcon57526.2023.10391350"
    },
    {
        "id": 16345,
        "title": "3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds",
        "authors": "Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, Jiwen Lu",
        "published": "2017-10",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.605"
    },
    {
        "id": 16346,
        "title": "Multi-DQN: An ensemble of Deep Q-learning agents for stock market forecasting",
        "authors": "Salvatore Carta, Anselmo Ferreira, Alessandro Sebastian Podda, Diego Reforgiato Recupero, Antonio Sanna",
        "published": "2021-2",
        "citations": 103,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2020.113820"
    },
    {
        "id": 16347,
        "title": "Bootstrapping a DQN Replay Memory with Synthetic Experiences",
        "authors": "Wenzel von Pilchau, Anthony Stein, Jörg Hähner",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010107904040411"
    },
    {
        "id": 16348,
        "title": "Spectrum Handoff Based on DQN Predictive Decision for Hybrid Cognitive Radio Networks",
        "authors": "Kaitian Cao, Ping Qian",
        "published": "2020-2-19",
        "citations": 9,
        "abstract": "Spectrum handoff is one of the key techniques in a cognitive radio system. In order to improve the agility and the reliability of spectrum handoffs as well as the system throughput in hybrid cognitive radio networks (HCRNs) combing interweave mode with underlay mode, a predictive (or proactive) spectrum handoff scheme based on a deep Q-network (DQN) for HCRNs is proposed in this paper. In the proposed spectrum handoff approach, spectrum handoff success rate is introduced into an optimal spectrum resource allocation model to ensure the reliability of spectrum handoff, and the closed-form expression for the spectrum handoff success rate is obtained based on the Poisson distribution. Furthermore, we exploit the transfer learning strategy to further improve the DQN learning process and finally achieve a priority sequence of target available channels for spectrum handoffs, which can maximize the overall HCRNs throughput while satisfying constraints on secondary users’ interference with primary user, limits on the spectrum handoff success rate, and the secondary users’ performance requirements. Simulation results show that the proposed spectrum handoff scheme outperforms the state-of-the-art spectrum handoff algorithms based on predictive decision in terms of the convergence rate, the handoff success rate and the system throughput.",
        "link": "http://dx.doi.org/10.3390/s20041146"
    },
    {
        "id": 16349,
        "title": "Deep Q-Network (DQN) Approach for Automatic Vehicles Applied in the Intelligent Transportation System (ITS)",
        "authors": "Vo Thanh Ha, Tran Ngoc Tu, Nguyen Trung Dung, Trinh Luong Mien, Chu Thi Thuy",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsse58758.2023.10227206"
    },
    {
        "id": 16350,
        "title": "Towards an Energy-Efficient DQN-based User Association in Sub6GHz/mmWave Integrated Networks",
        "authors": "Thi Ha Ly Dinh, Megumi Kaneko, Keisuke Wakao, Kenichi Kawamura, Takatsune Moriyama, Yasushi Takatori",
        "published": "2021-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/msn53354.2021.00098"
    },
    {
        "id": 16351,
        "title": "Intelligent Dual Active Protocol Stack Handover Based on Double DQN Deep Reinforcement Learning for 5G mmWave Networks",
        "authors": "Changsung Lee, Jaewook Jung, Jong-Moon Chung",
        "published": "2022-7",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tvt.2022.3170420"
    },
    {
        "id": 16352,
        "title": "Energy-Efficient Video Streaming in UAV-Enabled Wireless Networks: A Safe-DQN Approach",
        "authors": "Qian Zhang, Jiansong Miao, Zhicai Zhang, F. Richard Yu, Fang Fu, Tuan Wu",
        "published": "2020-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9348075"
    },
    {
        "id": 16353,
        "title": "Secure Relay Selection With Outdated CSI in Cooperative Wireless Vehicular Networks: A DQN Approach",
        "authors": "Esraa M. Ghourab, Lina Bariah, Sami Muhaidat, Paschalis C. Sofotasios, Mahmoud Al-Qutayri, Ernesto Damiani",
        "published": "2024",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3275567"
    },
    {
        "id": 16354,
        "title": "Securing UAV-to-Vehicle Communications: A Curiosity-Driven Deep Q-learning Network (C-DQN) Approach",
        "authors": "Fang Fu, Qi Jiao, F. Richard Yu, Zhicai Zhang, Jianbo Du",
        "published": "2021-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccworkshops50388.2021.9473714"
    },
    {
        "id": 16355,
        "title": "ED-DQN: An event-driven deep reinforcement learning control method for multi-zone residential buildings",
        "authors": "Qiming Fu, Zhu Li, Zhengkai Ding, Jianping Chen, Jun Luo, Yunzhe Wang, You Lu",
        "published": "2023-8",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.buildenv.2023.110546"
    },
    {
        "id": 16356,
        "title": "Energy–latency tradeoffs edge server selection and DQN-based resource allocation schemes in MEC",
        "authors": "Chunlin Li, Zewu Ke, Qiang Liu, Cong Hu, Chengwei Lu, Youlong Luo",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11276-023-03426-1"
    },
    {
        "id": 16357,
        "title": "Towards a Better Understanding of Deep Neural Networks Representations using Deep Generative Networks",
        "authors": "Jérémie Despraz, Stéphane Gomez, Héctor F. Satizábal, Carlos Andrés Peña-Reyes",
        "published": "2017",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006495102150222"
    },
    {
        "id": 16358,
        "title": "DQN-based Intelligent Application Placement with Delay-Priority in Multi MEC Systems",
        "authors": "Juan Sebastian Camargo, Estefanía Coronado, Claudia Torres-Pérez, Javier Palomares, Muhammad Shuaib Siddiqui",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eucnc/6gsummit58263.2023.10188300"
    },
    {
        "id": 16359,
        "title": "A DQN-Based Handover Management for SDN-Enabled Ultra-Dense Networks",
        "authors": "Mengting Wu, Wei Huang, Kai Sun, Haijun Zhang",
        "published": "2020-11",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-fall49728.2020.9348779"
    },
    {
        "id": 16360,
        "title": "Drone-to-drone interception path planning by Deep Q-network with Graph Neural Network based (DQN-GNN) model",
        "authors": "Jay Aljelo Saez Ting, Sutthiphong Srigrarom",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cis-ram55796.2023.10370012"
    },
    {
        "id": 16361,
        "title": "Deep-gKnock: Nonlinear group-feature selection with deep neural networks",
        "authors": "Guangyu Zhu, Tingting Zhao",
        "published": "2021-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2020.12.004"
    },
    {
        "id": 16362,
        "title": "Using DQN and Double DQN to Play Flappy Bird",
        "authors": "Kun Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2991/978-94-6463-010-7_120"
    },
    {
        "id": 16363,
        "title": "Training Deep Neural Networks",
        "authors": "Charu C. Aggarwal",
        "published": "2018",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-94463-0_3"
    },
    {
        "id": 16364,
        "title": "Deep Networks and Backpropagation",
        "authors": "",
        "published": "2021-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108955652.007"
    },
    {
        "id": 16365,
        "title": "Energy Efficient joint user association and power allocation using Parameterized Deep DQN",
        "authors": "Amna Mughees, Mohammad Tahir, Muhammad Aman Sheikh, Angela Amphawan, Kian Meng Yap, Mohamed Hadi Habaebi, Md. Rafiqul Islam",
        "published": "2023-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccce58854.2023.10246069"
    },
    {
        "id": 16366,
        "title": "Research on the Local Path Planning for Mobile Robots based on PRO-Dueling Deep Q-Network (DQN) Algorithm",
        "authors": "Yaoyu Zhang, Caihong Li, Guosheng Zhang, Ruihong Zhou, Zhenying Liang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140842"
    },
    {
        "id": 16367,
        "title": "FSPBO-DQN: SeGAN based segmentation and Fractional Student Psychology Optimization enabled Deep Q Network for skin cancer detection in IoT applications",
        "authors": "K. Suresh Kumar, N. Suganthi, Satish Muppidi, B. Santhosh Kumar",
        "published": "2022-7",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.artmed.2022.102299"
    },
    {
        "id": 16368,
        "title": "Regularization in DQN for Parameter-Varying Control Learning Tasks",
        "authors": "Dazi Li, Chengjia Lei, Qibing Jin, Min Han",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-22808-8_4"
    },
    {
        "id": 16369,
        "title": "Dynamic routing planning method for large-scale low-orbit satellite networks based on location guided and multi-agent DQN network",
        "authors": "Tao Dong, Xinyuan Deng, Zongyi Luo, Jie Yin, Bingli Guo, Yue Xu, Zhihui Liu",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3023514"
    },
    {
        "id": 16370,
        "title": "The Task Scheduling Algorithm for Fog Computing in Intelligent Production Lines Based on DQN",
        "authors": "Fulong Xu, Zhenyu Yin, Yue Li, Feiqing Zhang, Guangyuan Xu",
        "published": "2023-7-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsn57992.2023.10297319"
    }
]