[
    {
        "id": 1001,
        "title": "Research and Application of Large Language Models in HealthcareCurrent Development of Large Language Models in the Healthcare FieldA Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare: A Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare",
        "authors": "Chunfang Zhou, Qingyue Gong, Jinyang Zhu, Huidan Luan",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3644116.3644226"
    },
    {
        "id": 1002,
        "title": "Explainable Large Language Models &amp; iContracts",
        "authors": "Georgios Stathis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012607400003636"
    },
    {
        "id": 1003,
        "title": "Language Models for Everyone—Responsible and Transparent Development of Open Large Language Models",
        "authors": "Daniel Gillblad",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008051"
    },
    {
        "id": 1004,
        "title": "Applications of large language models in oncology",
        "authors": "Chiara M. Loeffler, Keno K. Bressem, Daniel Truhn",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00761-024-01481-7"
    },
    {
        "id": 1005,
        "title": "Large Language Models in Medical Education and Quality Concerns",
        "authors": "Vinaytosh Mishra",
        "published": "2023",
        "citations": 1,
        "abstract": "The world is witnessing increased digitalization in the recent past",
        "keywords": "",
        "link": "http://dx.doi.org/10.23880/jqhe-16000319"
    },
    {
        "id": 1006,
        "title": "Future of Interacting with Computers and Large Language Models",
        "authors": "Migul Jain",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231023121603"
    },
    {
        "id": 1007,
        "title": "Analyzing Declarative Deployment Code with Large Language Models",
        "authors": "Giacomo Lanciano, Manuel Stein, Volker Hilt, Tommaso Cucinotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011991200003488"
    },
    {
        "id": 1008,
        "title": "Computing Architecture for Large-Language Models (LLMs) and Large Multimodal Models (LMMs)",
        "authors": "Bor-Sung Liang",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3626184.3639692"
    },
    {
        "id": 1009,
        "title": "Efficient Use of Large Language Models for Analysis of Text Corpora",
        "authors": "David Adamczyk, Jan Hůla",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012349800003654"
    },
    {
        "id": 1010,
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": "Mutian He, Philip N. Garner",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1799"
    },
    {
        "id": 1011,
        "title": "Demonstrating Large Language Models on Robots",
        "authors": " Google DeepMind",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.024"
    },
    {
        "id": 1012,
        "title": "Are Large Language Models Intelligent? Are Humans?",
        "authors": "Olle Häggström",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008068"
    },
    {
        "id": 1013,
        "title": "Can Language Models Solve Complex Subsurface Data Integrations: Building Subsurface Copilots with Large Language Models (LLMs)",
        "authors": "T.B. Grant, J. Goldwater, E. Knudsen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202439022"
    },
    {
        "id": 1014,
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": "Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-910"
    },
    {
        "id": 1015,
        "title": "Embodied human language models vs. Large Language Models, or why Artificial Intelligence cannot explain the modal be able to",
        "authors": "Sergio Torres-Martínez",
        "published": "2024-2-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12304-024-09553-2"
    },
    {
        "id": 1016,
        "title": "Large Language Models as SocioTechnical Systems",
        "authors": "Kaustubh Dhole",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bigpicture-1.6"
    },
    {
        "id": 1017,
        "title": "Considerations for Prompting Large Language Models",
        "authors": "Brian Schulte",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamaoncol.2023.6963"
    },
    {
        "id": 1018,
        "title": "Large Language Models and Artificial Intelligence for Police Report Writing",
        "authors": "Ian T. Adams",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21428/cb6ab371.779603ee"
    },
    {
        "id": 1019,
        "title": "Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models",
        "authors": "Sharon Gumina, Travis Dalton, John Gerdes",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3585059.3611409"
    },
    {
        "id": 1020,
        "title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions About Code",
        "authors": "Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr",
        "published": "2023",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011996900003470"
    },
    {
        "id": 1021,
        "title": "Safety of Large Language Models in Addressing Depression",
        "authors": "Thomas F Heston",
        "published": "2023-12-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.50729"
    },
    {
        "id": 1022,
        "title": "Evaluating Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies",
        "authors": "Isuri Anuradha Nanomi Arachchige,  , Le An Ha, Ruslan Mitkov, Vinitar Nahar,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_013"
    },
    {
        "id": 1023,
        "title": "Large Language Models as Corporate Lobbyists",
        "authors": "John Nay",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4316615"
    },
    {
        "id": 1024,
        "title": "Large Language Models and Information Retrieval",
        "authors": "Kalyani Pakhale",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4636121"
    },
    {
        "id": 1025,
        "title": "Prompt Engineering for Large Language Models",
        "authors": "Andrew Gao",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4504303"
    },
    {
        "id": 1026,
        "title": "Large Language Models in Enterprise Modeling: Case Study and Experiences",
        "authors": "Leon Görgen, Eric Müller, Marcus Triller, Benjamin Nast, Kurt Sandkuhl",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012387000003645"
    },
    {
        "id": 1027,
        "title": "Large Language Models Cannot Meet Artificial General Intelligence Expectations",
        "authors": "Wolfgang Hofkirchner",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008067"
    },
    {
        "id": 1028,
        "title": "Privacy-Preserving Large Language Models (PPLLMs)",
        "authors": "Mohammad Raeini",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4512071"
    },
    {
        "id": 1029,
        "title": "UsingWikidata for Enhancing Compositionality in Pre-trained Language Models",
        "authors": "Meriem Beloucif,  , Mihir Bansal, Chris Biemann,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_019"
    },
    {
        "id": 1030,
        "title": "Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models",
        "authors": "Ali Goli, Amandeep Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4437617"
    },
    {
        "id": 1031,
        "title": "Trustworthiness of Children Stories Generated by Large Language Models",
        "authors": "Prabin Bhandari, Hannah Brennan",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.24"
    },
    {
        "id": 1032,
        "title": "Evaluating Unsupervised Hierarchical Topic Models Using a Labeled Dataset",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_091"
    },
    {
        "id": 1033,
        "title": "Bringing Systems Engineering Models to Large Language Models: An Integration of OPM with an LLM for Design Assistants",
        "authors": "Ramón María García Alarcia, Pietro Russo, Alfredo Renga, Alessandro Golkar",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012621900003645"
    },
    {
        "id": 1034,
        "title": "A Survey of Large Language Models in Tourism (Tourism LLMs)",
        "authors": "Shengyu Gu",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "This comprehensive survey delves into the integration and application of Large Language Models (LLMs) within the tourism sector, a domain ripe with potential for transformative AI-driven enhancements. As tourism increasingly embraces digital innovation, LLMs stand at the forefront of this evolution, offering sophisticated solutions for personalized travel experiences, multilingual communication, and the preservation of cultural heritage. This paper systematically explores the multifaceted roles of LLMs in tourism, from generating dynamic travel itineraries and culturally rich site descriptions to providing real-time assistance and multilingual support for global travelers. Through an analysis of current implementations and potential applications, we highlight both the remarkable opportunities presented by LLMs and the significant challenges, including data privacy concerns, cultural sensitivity, and the need for real-time processing capabilities. The findings underscore the imperative for a balanced approach that harnesses the capabilities of LLMs while addressing ethical considerations and ensuring inclusivity and accessibility in global tourism. This survey aims to provide a foundational understanding for researchers, practitioners, and policymakers, guiding future innovations and fostering a responsible integration of AI technologies in enhancing the global tourism experience.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/8r27cj"
    },
    {
        "id": 1035,
        "title": "Generating clickbait spoilers with an ensemble of large language models",
        "authors": "Mateusz Woźny, Mateusz Lango",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.32"
    },
    {
        "id": 1036,
        "title": "Notes towards infrastructure governance for large language models",
        "authors": "Lara Dal Molin",
        "published": "2024-2-11",
        "citations": 0,
        "abstract": "This paper draws on information infrastructures (IIs) in science and technology studies (STS), as well as on feminist STS scholarship and contemporary critical accounts of digital technologies, to build an initial mapping of the infrastructural mechanisms and implications of large language models (LLMs). Through a comparison with discriminatory machine learning (ML) systems and a case study on gender bias, I present LLMs as contested artefacts with categorising and performative capabilities. This paper suggests that generative systems do not tangibly depart from traditional, discriminative counterparts in terms of their underlying probabilistic mechanisms, and therefore both technologies can be theorised as infrastructures of categorisation. However, LLMs additionally retain performative capabilities through their linguistic outputs. Here, I outline the intuition behind this phenomenon, which I refer to as “language as infrastructure”. While traditional, discriminative systems “disappear” into larger IIs, the hype surrounding generative technologies presents an opportunity to scrutinise these artefacts, to alter their computational mechanisms and introduce governance measures]. I illustrate this thesis through Sharma’s formulation of “broken machine”, and suggest dataset curation and participatory design as governance mechanisms that can partly address downstream harms in LLMs (Barocas, et al., 2023).",
        "keywords": "",
        "link": "http://dx.doi.org/10.5210/fm.v29i2.13567"
    },
    {
        "id": 1037,
        "title": "Sharing Learning Experience Using Large Language Models",
        "authors": "Subhajit Chattopadhyay",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4554925"
    },
    {
        "id": 1038,
        "title": "AI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models",
        "authors": "Luciano Floridi",
        "published": "2023",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4358789"
    },
    {
        "id": 1039,
        "title": "Applications of Large Language Models in Pathology",
        "authors": "Jerome Cheng",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "Large language models (LLMs) are transformer-based neural networks that can provide human-like responses to questions and instructions. LLMs can generate educational material, summarize text, extract structured data from free text, create reports, write programs, and potentially assist in case sign-out. LLMs combined with vision models can assist in interpreting histopathology images. LLMs have immense potential in transforming pathology practice and education, but these models are not infallible, so any artificial intelligence generated content must be verified with reputable sources. Caution must be exercised on how these models are integrated into clinical practice, as these models can produce hallucinations and incorrect results, and an over-reliance on artificial intelligence may lead to de-skilling and automation bias. This review paper provides a brief history of LLMs and highlights several use cases for LLMs in the field of pathology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bioengineering11040342"
    },
    {
        "id": 1040,
        "title": "Embracing Large Language Models for Medical Applications: Opportunities and Challenges",
        "authors": "Mert Karabacak, Konstantinos Margetis",
        "published": "2023-5-21",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.39305"
    },
    {
        "id": 1041,
        "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks",
        "authors": "Sherzod Hakimov, David Schlangen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.894"
    },
    {
        "id": 1042,
        "title": "Large Language Models and Logical Reasoning",
        "authors": "Robert Friedman",
        "published": "2023-5-30",
        "citations": 2,
        "abstract": "In deep learning, large language models are typically trained on data from a corpus as representative of current knowledge. However, natural language is not an ideal form for the reliable communication of concepts. Instead, formal logical statements are preferable since they are subject to verifiability, reliability, and applicability. Another reason for this preference is that natural language is not designed for an efficient and reliable flow of information and knowledge, but is instead designed as an evolutionary adaptation as formed from a prior set of natural constraints. As a formally structured language, logical statements are also more interpretable. They may be informally constructed in the form of a natural language statement, but a formalized logical statement is expected to follow a stricter set of rules, such as with the use of symbols for representing the logic-based operators that connect multiple simple statements and form verifiable propositions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/encyclopedia3020049"
    },
    {
        "id": 1043,
        "title": "Exploring Text-Generating Large Language Models (LLMs) for Emotion Recognition in Affective Intelligent Agents",
        "authors": "Aaron Pico, Emilio Vivancos, Ana Garcia-Fornes, Vicente Botti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012596800003636"
    },
    {
        "id": 1044,
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": "Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.523"
    },
    {
        "id": 1045,
        "title": "Practical PCG Through Large Language Models",
        "authors": "Muhammad U Nasir, Julian Togelius",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333197"
    },
    {
        "id": 1046,
        "title": "Leveraging Fine-Tuned Large Language Models in Bioinformatics: A Research Perspective",
        "authors": "Usama Shahid",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "Bioinformatics synergizes biology, computer science, and statistics and is further propelled by the integration of deep learning and natural language processing (NLP). This analysis extensively explores the applications of fine-tuned language models within bioinformatics, providing empirical evidence and unique perspectives on the impact, challenges, and limitations in this field. The broad scope includes biomedical literature analysis, drug discovery, clinical decision support, protein structure prediction, and pharmacovigilance, among others. This analysis underscores the need to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethical implications, and validation for a reliable application of these models. Collaborative efforts between computational and experimental biologists, ethicists, and regulatory bodies are vital to establish ethical guidelines and best practices for their use.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/we7umn.2"
    },
    {
        "id": 1047,
        "title": "Large Language Models and the Future of Law",
        "authors": "Damien Charlotin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4548258"
    },
    {
        "id": 1048,
        "title": "Limitations of and Lessons from the Learning of Large Language Models",
        "authors": "Reinhard Oldenburg",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "It is argued that the Curry-Howard correspondence for classical logic implies limitations for logical reasoning that can be learned and performed by large language models. The correspondence establishes an isomorphism between proofs in logic and programs in functional typed lambda calculus. While intuitionistic logic maps to a version of lambda calculus that can be carried out in a local way, i.e., considering local parts of the code in isolation, the version of lambda calculus that corresponds to classical logic requires non-local relations – and this non-locality cannot be learned by large language models due to their restriction to investigate a relative short sequence of tokens. A possible way to go beyond this limitation is sketched as well. Implications for other areas are investigated as well.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/9fh6ad"
    },
    {
        "id": 1049,
        "title": "Employing large language models in survey research",
        "authors": "Bernard J. Jansen, Soon-gyo Jung, Joni Salminen",
        "published": "2023-9",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100020"
    },
    {
        "id": 1050,
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": "Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.397"
    },
    {
        "id": 1051,
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": "Li Yifei, Lyle Ungar, João Sedoc",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.661"
    },
    {
        "id": 1052,
        "title": "Natural language processing in the era of large language models",
        "authors": "Arkaitz Zubiaga",
        "published": "2024-1-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/frai.2023.1350306"
    },
    {
        "id": 1053,
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": "Xunjian Yin, Baizhou Huang, Xiaojun Wan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.87"
    },
    {
        "id": 1054,
        "title": "Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models",
        "authors": "Boyang Li",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607827.3616846"
    },
    {
        "id": 1055,
        "title": "Evaluation of Large Language Models Using an Indian Language LGBTI+ Lexicon",
        "authors": "Aditya Joshi, Shruta Rawat",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine the behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM’s behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs in the Indian context, especially with prevalent social perception of the LGBTI+ community.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47289/aiej20231109"
    },
    {
        "id": 1056,
        "title": "Query2doc: Query Expansion with Large Language Models",
        "authors": "Liang Wang, Nan Yang, Furu Wei",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.585"
    },
    {
        "id": 1057,
        "title": "Empowering Vision-Language Models for Reasoning Ability through Large Language Models",
        "authors": "Yueting Yang, Xintong Zhang, Jinan Xu, Wenjuan Han",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446407"
    },
    {
        "id": 1058,
        "title": "Large Language Models and Information Retrieval",
        "authors": "Kalyani Pakhale -",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "This research article explores the synergistic integration of Optical Character Recognition (OCR) technology and Large Language Models (LLMs) to advance Information Retrieval (IR) processes. In a data-centric society, efficient IR is imperative, and the combination of OCR and LLMs presents a powerful solution. OCR transforms diverse document types into machine-readable formats, while LLMs excel in language understanding and generation. The article delves into the technical intricacies of these technologies, their seamless integration, and their potential to revolutionize information retrieval. By investigating their collaborative capabilities, this research contributes to the evolving landscape of natural language processing and information retrieval systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2023.v05i06.8841"
    },
    {
        "id": 1059,
        "title": "The Future of Tourism: Examining the Potential Applications of Large Language Models",
        "authors": "Shengyu Gu",
        "published": "2024-3-5",
        "citations": 0,
        "abstract": "Large language models such as the Generative Pre-trained Transformer (GPT) have recently gained attention for their impressive natural language processing capabilities. While their potential to revolutionize various industries is still being explored, the tourism industry stands to benefit significantly from their use. In this study, we conduct an early assessment of the impact potential of GPTs on the tourism industry using a mixed-methods approach.\n\nWe first analyze the existing literature on the use of GPTs in the tourism industry and identify several potential applications such as personalized travel recommendations, language translation, and chatbots. We then collect data from various stakeholders in the tourism industry through surveys and interviews to understand their current practices and their willingness to adopt GPT-based solutions.\n\nOur results indicate that while there is a high level of awareness and interest in GPTs among tourism professionals, the adoption of these technologies is currently limited. The main barriers identified include a lack of technical expertise, concerns around data privacy and security, and the high cost of implementing GPT-based solutions. However, those who have adopted GPTs report significant benefits in terms of increased efficiency and improved customer satisfaction.\n\nTo further explore the potential of GPTs in the tourism industry, we conduct a pilot study to develop a GPT-based travel recommendation system. The system uses GPT to generate personalized travel itineraries based on user preferences and feedback. Our evaluation of the system indicates that it performs well in terms of accuracy and user satisfaction, demonstrating the potential for GPTs to provide personalized and tailored experiences to travellers.\n\nOverall, our study provides an early look at the impact potential of GPTs on the tourism industry and identifies several avenues for future research. We recommend that tourism professionals and researchers collaborate to address the current barriers to adoption and explore the full range of applications for GPTs in the industry.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/uyruwt"
    },
    {
        "id": 1060,
        "title": "Large Language Models Reasoning and Reinforcement Learning",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4656090"
    },
    {
        "id": 1061,
        "title": "Machine Advisors: Integrating Large Language Models into Democratic Assemblies",
        "authors": "Petr Špecián",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4682958"
    },
    {
        "id": 1062,
        "title": "Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs",
        "authors": "Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, Florian Matthes",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012394300003636"
    },
    {
        "id": 1063,
        "title": "Towards Developing an Agent-Based Framework for Validating the Trustworthiness of Large Language Models",
        "authors": "Johannes Bubeck, Janick Greinacher, Yannik Langer, Tobias Roth, Carsten Lanquillon",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012364000003636"
    },
    {
        "id": 1064,
        "title": "On Finetuning Large Language Models",
        "authors": "Yu Wang",
        "published": "2023-11-28",
        "citations": 2,
        "abstract": "Abstract\nA recent paper by Häffner et al. (2023, Political Analysis 31, 481–499) introduces an interpretable deep learning approach for domain-specific dictionary creation, where it is claimed that the dictionary-based approach outperforms finetuned language models in predictive accuracy while retaining interpretability. We show that the dictionary-based approach’s reported superiority over large language models, BERT specifically, is due to the fact that most of the parameters in the language models are excluded from finetuning. In this letter, we first discuss the architecture of BERT models, then explain the limitations of finetuning only the top classification layer, and lastly we report results where finetuned language models outperform the newly proposed dictionary-based approach by 27% in terms of \n\n\n\n$R^2$\n\n\n and 46% in terms of mean squared error once we allow these parameters to learn during finetuning. Researchers interested in large language models, text classification, and text regression should find our results useful. Our code and data are publicly available.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/pan.2023.36"
    },
    {
        "id": 1065,
        "title": "Shaping Learning Experience Design Using Large Language Models (LLMs)",
        "authors": "Subhajit Chattopadhyay",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4554943"
    },
    {
        "id": 1066,
        "title": "Large Language Models for Telecom",
        "authors": "Mérouane Debbah",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/fmec59375.2023.10305960"
    },
    {
        "id": 1067,
        "title": "Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models",
        "authors": "Zheyu Zhang, Han Yang, Bolei Ma, David Rügamer, Ercong Nie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.13"
    },
    {
        "id": 1068,
        "title": "Large Language Models (LLMs) for Natural Language Processing (NLP) of Oil and Gas Drilling Data",
        "authors": "Prateek Kumar, Sanjay Kathuria",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "Abstract\nIn the oil and gas industry, drilling activities spawn substantial volumes of unstructured textual data. The examination and interpretation of these data pose significant challenges. This research exploits the emerging capabilities of large language models (LLMs) with over 100 billion parameters to extract actionable insights from raw drilling data. Through fine-tuning methodologies and the use of various prompt engineering strategies, we addressed several text downstream tasks, including summarization, classification, entity recognition, and information extraction. This study delves into our methods, findings, and the novel application of LLMs for efficient and precise analysis of drilling data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/215167-ms"
    },
    {
        "id": 1069,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437725"
    },
    {
        "id": 1070,
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": "Yaoyiran Li, Anna Korhonen, Ivan Vulić",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.595"
    },
    {
        "id": 1071,
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "authors": "Alexander Rush",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-demo.27"
    },
    {
        "id": 1072,
        "title": "Probing the “Creativity” of Large Language Models: Can models produce divergent semantic association?",
        "authors": "Honghua Chen, Nai Ding",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.858"
    },
    {
        "id": 1073,
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": "Noah Lee, Na An, James Thorne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.278"
    },
    {
        "id": 1074,
        "title": "Evaluating Generative Models for Graph-to-Text Generation",
        "authors": "Shuzhou Yuan,  , Michael Färber,  ",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_133"
    },
    {
        "id": 1075,
        "title": "Copyright Violations and Large Language Models",
        "authors": "Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.458"
    },
    {
        "id": 1076,
        "title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": "Philippe J. Giabbanelli",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408017"
    },
    {
        "id": 1077,
        "title": "Augmenting interpretable models with large language models during training",
        "authors": "Chandan Singh, Armin Askari, Rich Caruana, Jianfeng Gao",
        "published": "2023-11-30",
        "citations": 2,
        "abstract": "AbstractRecent large language models (LLMs), such as ChatGPT, have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Aug-imodels, a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable prediction models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: Aug-Linear, which augments a linear model with decoupled embeddings from an LLM and Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented, interpretable counterparts. Aug-Linear can even outperform much larger models, e.g. a 6-billion parameter GPT-J model, despite having 10,000x fewer parameters and being fully transparent. We further explore Aug-imodels in a natural-language fMRI study, where they generate interesting interpretations from scientific data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41467-023-43713-1"
    },
    {
        "id": 1078,
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "authors": "Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, Sandipan Dandapat",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.418"
    },
    {
        "id": 1079,
        "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
        "authors": "Katikapalli Subramanyam Kalyan",
        "published": "2024-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100048"
    },
    {
        "id": 1080,
        "title": "Impacts and Implications of Generative AI and Large Language Models: Redefining Banking Sector",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jier.v4i2.767"
    },
    {
        "id": 1081,
        "title": "LARGE LANGUAGE MODELS (LLMS) AND CHATGPT FOR BIOMEDICINE",
        "authors": "Cecilia Arighi, Steven Brenner, Zhiyong Lu",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/9789811286421_0048"
    },
    {
        "id": 1082,
        "title": "Can large language models help augment English psycholinguistic datasets?",
        "authors": "Sean Trott",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "AbstractResearch on language and cognition relies extensively on psycholinguistic datasets or “norms”. These datasets contain judgments of lexical properties like concreteness and age of acquisition, and can be used to norm experimental stimuli, discover empirical relationships in the lexicon, and stress-test computational models. However, collecting human judgments at scale is both time-consuming and expensive. This issue of scale is compounded for multi-dimensional norms and those incorporating context. The current work asks whether large language models (LLMs) can be leveraged to augment the creation of large, psycholinguistic datasets in English. I use GPT-4 to collect multiple kinds of semantic judgments (e.g., word similarity, contextualized sensorimotor associations, iconicity) for English words and compare these judgments against the human “gold standard”. For each dataset, I find that GPT-4’s judgments are positively correlated with human judgments, in some cases rivaling or even exceeding the average inter-annotator agreement displayed by humans. I then identify several ways in which LLM-generated norms differ from human-generated norms systematically. I also perform several “substitution analyses”, which demonstrate that replacing human-generated norms with LLM-generated norms in a statistical model does not change the sign of parameter estimates (though in select cases, there are significant changes to their magnitude). I conclude by discussing the considerations and limitations associated with LLM-generated norms in general, including concerns of data contamination, the choice of LLM, external validity, construct validity, and data quality. Additionally, all of GPT-4’s judgments (over 30,000 in total) are made available online for further analysis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13428-024-02337-z"
    },
    {
        "id": 1083,
        "title": "Hallucinations and Emergence in Large Language Models",
        "authors": "Bernardo A. Huberman, Sayandev Mukherjee",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4676180"
    },
    {
        "id": 1084,
        "title": "Variability in Large Language Models’ Responses to Medical Licensing and Certification Examinations. Comment on “How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment”",
        "authors": "Richard H Epstein, Franklin Dexter",
        "published": "2023-7-13",
        "citations": 8,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/48305"
    },
    {
        "id": 1085,
        "title": "Integrating Large Language Models into Higher Education: Guidelines for Effective Implementation",
        "authors": "Karl de Fine Licht",
        "published": "2023-8-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008065"
    },
    {
        "id": 1086,
        "title": "Large language models for reducing clinicians’ documentation burden",
        "authors": "Kirk Roberts",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41591-024-02888-w"
    },
    {
        "id": 1087,
        "title": "Exploring Mathematical Spaces using Generative AI and Large Language Models",
        "authors": "Mohammad Raeini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4761694"
    },
    {
        "id": 1088,
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": "Danqing Wang, Lei Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.659"
    },
    {
        "id": 1089,
        "title": "Analysis of the Effectiveness of Large Language Models in Assessing Argumentative Writing and Generating Feedback",
        "authors": "Daisy Albuquerque da Silva, Carlos Eduardo de Mello, Ana Garcia",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012466600003636"
    },
    {
        "id": 1090,
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "authors": "Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, Lingming Zhang",
        "published": "2023-7-12",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597926.3598067"
    },
    {
        "id": 1091,
        "title": "Large Language Models are Prone to Methodological Artifacts",
        "authors": "Melanie Brucks, Olivier Toubia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4484416"
    },
    {
        "id": 1092,
        "title": "A Surgical Perspective on Large Language Models",
        "authors": "Robert Miller",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/sla.0000000000005896"
    },
    {
        "id": 1093,
        "title": "On Political Theory and Large Language Models",
        "authors": "Emma Rodman",
        "published": "2023-10-17",
        "citations": 3,
        "abstract": " Political theory as a discipline has long been skeptical of computational methods. In this paper, I argue that it is time for theory to make a perspectival shift on these methods. Specifically, we should consider integrating recently developed generative large language models like GPT-4 as tools to support our creative work as theorists. Ultimately, I suggest that political theorists should embrace this technology as a method of supporting our capacity for creativity—but that we should do so in a way that is mindful of the content and value of theorizing, the technical constraints of the models, and the ethical questions that the technology raises. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/00905917231200826"
    },
    {
        "id": 1094,
        "title": "Experimenting with Planning and Reasoning in Ad Hoc Teamwork Environments with Large Language Models",
        "authors": "Polyana Costa, Pedro Santos, José Boaro, Daniel Moraes, Júlio Duarte, Sergio Colcher",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012472600003636"
    },
    {
        "id": 1095,
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": "Iker García-Ferrero, Begoña Altuna, Javier Alvez, Itziar Gonzalez-Dios, German Rigau",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.531"
    },
    {
        "id": 1096,
        "title": "Implications of ChatGPT and Large Language Models for Environmental Policymaking",
        "authors": "Andrew Gao",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499643"
    },
    {
        "id": 1097,
        "title": "The Next Chapter: A Study of Large Language Models in Storytelling",
        "authors": "Zhuohan Xie, Trevor Cohn, Jey Han Lau",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.23"
    },
    {
        "id": 1098,
        "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
        "authors": "Masahiro Kaneko, Naoaki Okazaki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.619"
    },
    {
        "id": 1099,
        "title": "The impact of chatbots based on large language models on second language vocabulary acquisition",
        "authors": "Zhihui Zhang, Xiaomeng Huang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e25370"
    },
    {
        "id": 1100,
        "title": "Machine recognition of non-native speech: Task-specific language models versus large language models",
        "authors": "Jian Cheng, Jared C. Bernstein, Masanori Suzuki",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "Automatic speech recognition (ASR) has offered a reliable foundation for measurement of young children’s reading skills and of second-language (L2) speaking skills. This is because well-fit task-specific language models (LMs) enable recognition that supports accurate scoring of pronunciation, fluency, vocabulary, usage, and grammar (Bernstein and Cheng, 2023). ASR works well in these measurement tasks because measurement of word production, disfluencies, and pronunciation errors is not very sensitive to moderate differences in word-error-rate (WER) accuracy, and because speech-interactive tasks appropriate for reading instruction or L2 assessment elicit relatively predictable responses, for which task-specific low-perplexity ASR systems achieve sufficiently accurate speech recognition (Cheng and Townshend, 2003). In the work reported here, we compared the accuracy of two English ASR systems on a set of 718 extended spontaneous speech recordings from 77 adult non-native speakers of English speaking from six countries under uncontrolled recording conditions. A Kaldi-based ASR system with well-fit task-specific LMs achieved WER 17%, while USM, a general-purpose mSLAM recognizer with an RNN-T decoder, achieved 11% WER, which is a 34% relative improvement. The mSLAM + RNN-T technology will be briefly described and an analysis of results in three different open-response interactive speaking tasks will be presented.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1121/10.0023275"
    }
]