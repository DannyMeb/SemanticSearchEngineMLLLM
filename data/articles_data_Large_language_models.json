[
    {
        "id": 3305,
        "title": "Research and Application of Large Language Models in HealthcareCurrent Development of Large Language Models in the Healthcare FieldA Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare: A Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare",
        "authors": "Chunfang Zhou, Qingyue Gong, Jinyang Zhu, Huidan Luan",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3644116.3644226"
    },
    {
        "id": 3306,
        "title": "Explainable Large Language Models &amp; iContracts",
        "authors": "Georgios Stathis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012607400003636"
    },
    {
        "id": 3307,
        "title": "Language Models for Everyone—Responsible and Transparent Development of Open Large Language Models",
        "authors": "Daniel Gillblad",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008051"
    },
    {
        "id": 3308,
        "title": "Applications of large language models in oncology",
        "authors": "Chiara M. Loeffler, Keno K. Bressem, Daniel Truhn",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00761-024-01481-7"
    },
    {
        "id": 3309,
        "title": "Large Language Models in Medical Education and Quality Concerns",
        "authors": "Vinaytosh Mishra",
        "published": "2023",
        "citations": 1,
        "abstract": "The world is witnessing increased digitalization in the recent past",
        "keywords": "",
        "link": "http://dx.doi.org/10.23880/jqhe-16000319"
    },
    {
        "id": 3310,
        "title": "Future of Interacting with Computers and Large Language Models",
        "authors": "Migul Jain",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231023121603"
    },
    {
        "id": 3311,
        "title": "Analyzing Declarative Deployment Code with Large Language Models",
        "authors": "Giacomo Lanciano, Manuel Stein, Volker Hilt, Tommaso Cucinotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011991200003488"
    },
    {
        "id": 3312,
        "title": "Computing Architecture for Large-Language Models (LLMs) and Large Multimodal Models (LMMs)",
        "authors": "Bor-Sung Liang",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3626184.3639692"
    },
    {
        "id": 3313,
        "title": "Efficient Use of Large Language Models for Analysis of Text Corpora",
        "authors": "David Adamczyk, Jan Hůla",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012349800003654"
    },
    {
        "id": 3314,
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": "Mutian He, Philip N. Garner",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1799"
    },
    {
        "id": 3315,
        "title": "Demonstrating Large Language Models on Robots",
        "authors": " Google DeepMind",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.024"
    },
    {
        "id": 3316,
        "title": "Are Large Language Models Intelligent? Are Humans?",
        "authors": "Olle Häggström",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008068"
    },
    {
        "id": 3317,
        "title": "Can Language Models Solve Complex Subsurface Data Integrations: Building Subsurface Copilots with Large Language Models (LLMs)",
        "authors": "T.B. Grant, J. Goldwater, E. Knudsen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202439022"
    },
    {
        "id": 3318,
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": "Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-910"
    },
    {
        "id": 3319,
        "title": "Embodied human language models vs. Large Language Models, or why Artificial Intelligence cannot explain the modal be able to",
        "authors": "Sergio Torres-Martínez",
        "published": "2024-2-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12304-024-09553-2"
    },
    {
        "id": 3320,
        "title": "Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models",
        "authors": "Sharon Gumina, Travis Dalton, John Gerdes",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3585059.3611409"
    },
    {
        "id": 3321,
        "title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions About Code",
        "authors": "Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr",
        "published": "2023",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011996900003470"
    },
    {
        "id": 3322,
        "title": "Large Language Models and Artificial Intelligence for Police Report Writing",
        "authors": "Ian T. Adams",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21428/cb6ab371.779603ee"
    },
    {
        "id": 3323,
        "title": "Safety of Large Language Models in Addressing Depression",
        "authors": "Thomas F Heston",
        "published": "2023-12-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.50729"
    },
    {
        "id": 3324,
        "title": "Considerations for Prompting Large Language Models",
        "authors": "Brian Schulte",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamaoncol.2023.6963"
    },
    {
        "id": 3325,
        "title": "Large Language Models as SocioTechnical Systems",
        "authors": "Kaustubh Dhole",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bigpicture-1.6"
    },
    {
        "id": 3326,
        "title": "Evaluating Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies",
        "authors": "Isuri Anuradha Nanomi Arachchige,  , Le An Ha, Ruslan Mitkov, Vinitar Nahar,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_013"
    },
    {
        "id": 3327,
        "title": "Large Language Models in Enterprise Modeling: Case Study and Experiences",
        "authors": "Leon Görgen, Eric Müller, Marcus Triller, Benjamin Nast, Kurt Sandkuhl",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012387000003645"
    },
    {
        "id": 3328,
        "title": "Large Language Models as Corporate Lobbyists",
        "authors": "John Nay",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4316615"
    },
    {
        "id": 3329,
        "title": "Privacy-Preserving Large Language Models (PPLLMs)",
        "authors": "Mohammad Raeini",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4512071"
    },
    {
        "id": 3330,
        "title": "Large Language Models Cannot Meet Artificial General Intelligence Expectations",
        "authors": "Wolfgang Hofkirchner",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008067"
    },
    {
        "id": 3331,
        "title": "Large Language Models and Information Retrieval",
        "authors": "Kalyani Pakhale",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4636121"
    },
    {
        "id": 3332,
        "title": "Prompt Engineering for Large Language Models",
        "authors": "Andrew Gao",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4504303"
    },
    {
        "id": 3333,
        "title": "Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models",
        "authors": "Ali Goli, Amandeep Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4437617"
    },
    {
        "id": 3334,
        "title": "UsingWikidata for Enhancing Compositionality in Pre-trained Language Models",
        "authors": "Meriem Beloucif,  , Mihir Bansal, Chris Biemann,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_019"
    },
    {
        "id": 3335,
        "title": "Trustworthiness of Children Stories Generated by Large Language Models",
        "authors": "Prabin Bhandari, Hannah Brennan",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.24"
    },
    {
        "id": 3336,
        "title": "Evaluating Unsupervised Hierarchical Topic Models Using a Labeled Dataset",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_091"
    },
    {
        "id": 3337,
        "title": "Bringing Systems Engineering Models to Large Language Models: An Integration of OPM with an LLM for Design Assistants",
        "authors": "Ramón María García Alarcia, Pietro Russo, Alfredo Renga, Alessandro Golkar",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012621900003645"
    },
    {
        "id": 3338,
        "title": "A Survey of Large Language Models in Tourism (Tourism LLMs)",
        "authors": "Shengyu Gu",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "This comprehensive survey delves into the integration and application of Large Language Models (LLMs) within the tourism sector, a domain ripe with potential for transformative AI-driven enhancements. As tourism increasingly embraces digital innovation, LLMs stand at the forefront of this evolution, offering sophisticated solutions for personalized travel experiences, multilingual communication, and the preservation of cultural heritage. This paper systematically explores the multifaceted roles of LLMs in tourism, from generating dynamic travel itineraries and culturally rich site descriptions to providing real-time assistance and multilingual support for global travelers. Through an analysis of current implementations and potential applications, we highlight both the remarkable opportunities presented by LLMs and the significant challenges, including data privacy concerns, cultural sensitivity, and the need for real-time processing capabilities. The findings underscore the imperative for a balanced approach that harnesses the capabilities of LLMs while addressing ethical considerations and ensuring inclusivity and accessibility in global tourism. This survey aims to provide a foundational understanding for researchers, practitioners, and policymakers, guiding future innovations and fostering a responsible integration of AI technologies in enhancing the global tourism experience.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/8r27cj"
    },
    {
        "id": 3339,
        "title": "Notes towards infrastructure governance for large language models",
        "authors": "Lara Dal Molin",
        "published": "2024-2-11",
        "citations": 0,
        "abstract": "This paper draws on information infrastructures (IIs) in science and technology studies (STS), as well as on feminist STS scholarship and contemporary critical accounts of digital technologies, to build an initial mapping of the infrastructural mechanisms and implications of large language models (LLMs). Through a comparison with discriminatory machine learning (ML) systems and a case study on gender bias, I present LLMs as contested artefacts with categorising and performative capabilities. This paper suggests that generative systems do not tangibly depart from traditional, discriminative counterparts in terms of their underlying probabilistic mechanisms, and therefore both technologies can be theorised as infrastructures of categorisation. However, LLMs additionally retain performative capabilities through their linguistic outputs. Here, I outline the intuition behind this phenomenon, which I refer to as “language as infrastructure”. While traditional, discriminative systems “disappear” into larger IIs, the hype surrounding generative technologies presents an opportunity to scrutinise these artefacts, to alter their computational mechanisms and introduce governance measures]. I illustrate this thesis through Sharma’s formulation of “broken machine”, and suggest dataset curation and participatory design as governance mechanisms that can partly address downstream harms in LLMs (Barocas, et al., 2023).",
        "keywords": "",
        "link": "http://dx.doi.org/10.5210/fm.v29i2.13567"
    },
    {
        "id": 3340,
        "title": "Sharing Learning Experience Using Large Language Models",
        "authors": "Subhajit Chattopadhyay",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4554925"
    },
    {
        "id": 3341,
        "title": "Generating clickbait spoilers with an ensemble of large language models",
        "authors": "Mateusz Woźny, Mateusz Lango",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.32"
    },
    {
        "id": 3342,
        "title": "AI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models",
        "authors": "Luciano Floridi",
        "published": "2023",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4358789"
    },
    {
        "id": 3343,
        "title": "Large Language Models and the Future of Law",
        "authors": "Damien Charlotin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4548258"
    },
    {
        "id": 3344,
        "title": "Applications of Large Language Models in Pathology",
        "authors": "Jerome Cheng",
        "published": "2024-3-31",
        "citations": 1,
        "abstract": "Large language models (LLMs) are transformer-based neural networks that can provide human-like responses to questions and instructions. LLMs can generate educational material, summarize text, extract structured data from free text, create reports, write programs, and potentially assist in case sign-out. LLMs combined with vision models can assist in interpreting histopathology images. LLMs have immense potential in transforming pathology practice and education, but these models are not infallible, so any artificial intelligence generated content must be verified with reputable sources. Caution must be exercised on how these models are integrated into clinical practice, as these models can produce hallucinations and incorrect results, and an over-reliance on artificial intelligence may lead to de-skilling and automation bias. This review paper provides a brief history of LLMs and highlights several use cases for LLMs in the field of pathology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bioengineering11040342"
    },
    {
        "id": 3345,
        "title": "Embracing Large Language Models for Medical Applications: Opportunities and Challenges",
        "authors": "Mert Karabacak, Konstantinos Margetis",
        "published": "2023-5-21",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.39305"
    },
    {
        "id": 3346,
        "title": "Employing large language models in survey research",
        "authors": "Bernard J. Jansen, Soon-gyo Jung, Joni Salminen",
        "published": "2023-9",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100020"
    },
    {
        "id": 3347,
        "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision &amp; Language Tasks",
        "authors": "Sherzod Hakimov, David Schlangen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.894"
    },
    {
        "id": 3348,
        "title": "Large Language Models and Logical Reasoning",
        "authors": "Robert Friedman",
        "published": "2023-5-30",
        "citations": 2,
        "abstract": "In deep learning, large language models are typically trained on data from a corpus as representative of current knowledge. However, natural language is not an ideal form for the reliable communication of concepts. Instead, formal logical statements are preferable since they are subject to verifiability, reliability, and applicability. Another reason for this preference is that natural language is not designed for an efficient and reliable flow of information and knowledge, but is instead designed as an evolutionary adaptation as formed from a prior set of natural constraints. As a formally structured language, logical statements are also more interpretable. They may be informally constructed in the form of a natural language statement, but a formalized logical statement is expected to follow a stricter set of rules, such as with the use of symbols for representing the logic-based operators that connect multiple simple statements and form verifiable propositions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/encyclopedia3020049"
    },
    {
        "id": 3349,
        "title": "Practical PCG Through Large Language Models",
        "authors": "Muhammad U Nasir, Julian Togelius",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333197"
    },
    {
        "id": 3350,
        "title": "Leveraging Fine-Tuned Large Language Models in Bioinformatics: A Research Perspective",
        "authors": "Usama Shahid",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "Bioinformatics synergizes biology, computer science, and statistics and is further propelled by the integration of deep learning and natural language processing (NLP). This analysis extensively explores the applications of fine-tuned language models within bioinformatics, providing empirical evidence and unique perspectives on the impact, challenges, and limitations in this field. The broad scope includes biomedical literature analysis, drug discovery, clinical decision support, protein structure prediction, and pharmacovigilance, among others. This analysis underscores the need to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethical implications, and validation for a reliable application of these models. Collaborative efforts between computational and experimental biologists, ethicists, and regulatory bodies are vital to establish ethical guidelines and best practices for their use.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/we7umn.2"
    },
    {
        "id": 3351,
        "title": "Limitations of and Lessons from the Learning of Large Language Models",
        "authors": "Reinhard Oldenburg",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "It is argued that the Curry-Howard correspondence for classical logic implies limitations for logical reasoning that can be learned and performed by large language models. The correspondence establishes an isomorphism between proofs in logic and programs in functional typed lambda calculus. While intuitionistic logic maps to a version of lambda calculus that can be carried out in a local way, i.e., considering local parts of the code in isolation, the version of lambda calculus that corresponds to classical logic requires non-local relations – and this non-locality cannot be learned by large language models due to their restriction to investigate a relative short sequence of tokens. A possible way to go beyond this limitation is sketched as well. Implications for other areas are investigated as well.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/9fh6ad"
    },
    {
        "id": 3352,
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": "Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.523"
    },
    {
        "id": 3353,
        "title": "Exploring Text-Generating Large Language Models (LLMs) for Emotion Recognition in Affective Intelligent Agents",
        "authors": "Aaron Pico, Emilio Vivancos, Ana Garcia-Fornes, Vicente Botti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012596800003636"
    },
    {
        "id": 3354,
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": "Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.397"
    },
    {
        "id": 3355,
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": "Xunjian Yin, Baizhou Huang, Xiaojun Wan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.87"
    },
    {
        "id": 3356,
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": "Li Yifei, Lyle Ungar, João Sedoc",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.661"
    },
    {
        "id": 3357,
        "title": "Natural language processing in the era of large language models",
        "authors": "Arkaitz Zubiaga",
        "published": "2024-1-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/frai.2023.1350306"
    },
    {
        "id": 3358,
        "title": "Evaluation of Large Language Models Using an Indian Language LGBTI+ Lexicon",
        "authors": "Aditya Joshi, Shruta Rawat",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine the behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM’s behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs in the Indian context, especially with prevalent social perception of the LGBTI+ community.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47289/aiej20231109"
    },
    {
        "id": 3359,
        "title": "Query2doc: Query Expansion with Large Language Models",
        "authors": "Liang Wang, Nan Yang, Furu Wei",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.585"
    },
    {
        "id": 3360,
        "title": "Empowering Vision-Language Models for Reasoning Ability through Large Language Models",
        "authors": "Yueting Yang, Xintong Zhang, Jinan Xu, Wenjuan Han",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446407"
    },
    {
        "id": 3361,
        "title": "Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models",
        "authors": "Boyang Li",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607827.3616846"
    },
    {
        "id": 3362,
        "title": "Towards Developing an Agent-Based Framework for Validating the Trustworthiness of Large Language Models",
        "authors": "Johannes Bubeck, Janick Greinacher, Yannik Langer, Tobias Roth, Carsten Lanquillon",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012364000003636"
    },
    {
        "id": 3363,
        "title": "Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs",
        "authors": "Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, Florian Matthes",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012394300003636"
    },
    {
        "id": 3364,
        "title": "Value Aligned Large Language Models",
        "authors": "Panagiotis Angelopoulos, Kevin Lee, Sanjog Misra",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4781850"
    },
    {
        "id": 3365,
        "title": "The Future of Tourism: Examining the Potential Applications of Large Language Models",
        "authors": "Shengyu Gu",
        "published": "2024-3-5",
        "citations": 0,
        "abstract": "Large language models such as the Generative Pre-trained Transformer (GPT) have recently gained attention for their impressive natural language processing capabilities. While their potential to revolutionize various industries is still being explored, the tourism industry stands to benefit significantly from their use. In this study, we conduct an early assessment of the impact potential of GPTs on the tourism industry using a mixed-methods approach.\n\nWe first analyze the existing literature on the use of GPTs in the tourism industry and identify several potential applications such as personalized travel recommendations, language translation, and chatbots. We then collect data from various stakeholders in the tourism industry through surveys and interviews to understand their current practices and their willingness to adopt GPT-based solutions.\n\nOur results indicate that while there is a high level of awareness and interest in GPTs among tourism professionals, the adoption of these technologies is currently limited. The main barriers identified include a lack of technical expertise, concerns around data privacy and security, and the high cost of implementing GPT-based solutions. However, those who have adopted GPTs report significant benefits in terms of increased efficiency and improved customer satisfaction.\n\nTo further explore the potential of GPTs in the tourism industry, we conduct a pilot study to develop a GPT-based travel recommendation system. The system uses GPT to generate personalized travel itineraries based on user preferences and feedback. Our evaluation of the system indicates that it performs well in terms of accuracy and user satisfaction, demonstrating the potential for GPTs to provide personalized and tailored experiences to travellers.\n\nOverall, our study provides an early look at the impact potential of GPTs on the tourism industry and identifies several avenues for future research. We recommend that tourism professionals and researchers collaborate to address the current barriers to adoption and explore the full range of applications for GPTs in the industry.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/uyruwt"
    },
    {
        "id": 3366,
        "title": "Large Language Models and Information Retrieval",
        "authors": "Kalyani Pakhale -",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "This research article explores the synergistic integration of Optical Character Recognition (OCR) technology and Large Language Models (LLMs) to advance Information Retrieval (IR) processes. In a data-centric society, efficient IR is imperative, and the combination of OCR and LLMs presents a powerful solution. OCR transforms diverse document types into machine-readable formats, while LLMs excel in language understanding and generation. The article delves into the technical intricacies of these technologies, their seamless integration, and their potential to revolutionize information retrieval. By investigating their collaborative capabilities, this research contributes to the evolving landscape of natural language processing and information retrieval systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2023.v05i06.8841"
    },
    {
        "id": 3367,
        "title": "Large Language Models Reasoning and Reinforcement Learning",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4656090"
    },
    {
        "id": 3368,
        "title": "On Finetuning Large Language Models",
        "authors": "Yu Wang",
        "published": "2023-11-28",
        "citations": 2,
        "abstract": "Abstract\nA recent paper by Häffner et al. (2023, Political Analysis 31, 481–499) introduces an interpretable deep learning approach for domain-specific dictionary creation, where it is claimed that the dictionary-based approach outperforms finetuned language models in predictive accuracy while retaining interpretability. We show that the dictionary-based approach’s reported superiority over large language models, BERT specifically, is due to the fact that most of the parameters in the language models are excluded from finetuning. In this letter, we first discuss the architecture of BERT models, then explain the limitations of finetuning only the top classification layer, and lastly we report results where finetuned language models outperform the newly proposed dictionary-based approach by 27% in terms of \n\n\n\n$R^2$\n\n\n and 46% in terms of mean squared error once we allow these parameters to learn during finetuning. Researchers interested in large language models, text classification, and text regression should find our results useful. Our code and data are publicly available.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/pan.2023.36"
    },
    {
        "id": 3369,
        "title": "Shaping Learning Experience Design Using Large Language Models (LLMs)",
        "authors": "Subhajit Chattopadhyay",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4554943"
    },
    {
        "id": 3370,
        "title": "Large Language Models for Telecom",
        "authors": "Mérouane Debbah",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/fmec59375.2023.10305960"
    },
    {
        "id": 3371,
        "title": "Machine Advisors: Integrating Large Language Models into Democratic Assemblies",
        "authors": "Petr Špecián",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4682958"
    },
    {
        "id": 3372,
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": "Noah Lee, Na An, James Thorne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.278"
    },
    {
        "id": 3373,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437725"
    },
    {
        "id": 3374,
        "title": "Large Language Models (LLMs) for Natural Language Processing (NLP) of Oil and Gas Drilling Data",
        "authors": "Prateek Kumar, Sanjay Kathuria",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "Abstract\nIn the oil and gas industry, drilling activities spawn substantial volumes of unstructured textual data. The examination and interpretation of these data pose significant challenges. This research exploits the emerging capabilities of large language models (LLMs) with over 100 billion parameters to extract actionable insights from raw drilling data. Through fine-tuning methodologies and the use of various prompt engineering strategies, we addressed several text downstream tasks, including summarization, classification, entity recognition, and information extraction. This study delves into our methods, findings, and the novel application of LLMs for efficient and precise analysis of drilling data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/215167-ms"
    },
    {
        "id": 3375,
        "title": "Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models",
        "authors": "Zheyu Zhang, Han Yang, Bolei Ma, David Rügamer, Ercong Nie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.13"
    },
    {
        "id": 3376,
        "title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": "Philippe J. Giabbanelli",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408017"
    },
    {
        "id": 3377,
        "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
        "authors": "Katikapalli Subramanyam Kalyan",
        "published": "2024-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100048"
    },
    {
        "id": 3378,
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": "Yaoyiran Li, Anna Korhonen, Ivan Vulić",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.595"
    },
    {
        "id": 3379,
        "title": "Copyright Violations and Large Language Models",
        "authors": "Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.458"
    },
    {
        "id": 3380,
        "title": "Probing the “Creativity” of Large Language Models: Can models produce divergent semantic association?",
        "authors": "Honghua Chen, Nai Ding",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.858"
    },
    {
        "id": 3381,
        "title": "Augmenting interpretable models with large language models during training",
        "authors": "Chandan Singh, Armin Askari, Rich Caruana, Jianfeng Gao",
        "published": "2023-11-30",
        "citations": 2,
        "abstract": "AbstractRecent large language models (LLMs), such as ChatGPT, have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Aug-imodels, a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable prediction models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: Aug-Linear, which augments a linear model with decoupled embeddings from an LLM and Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented, interpretable counterparts. Aug-Linear can even outperform much larger models, e.g. a 6-billion parameter GPT-J model, despite having 10,000x fewer parameters and being fully transparent. We further explore Aug-imodels in a natural-language fMRI study, where they generate interesting interpretations from scientific data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41467-023-43713-1"
    },
    {
        "id": 3382,
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "authors": "Alexander Rush",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-demo.27"
    },
    {
        "id": 3383,
        "title": "Evaluating Generative Models for Graph-to-Text Generation",
        "authors": "Shuzhou Yuan,  , Michael Färber,  ",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_133"
    },
    {
        "id": 3384,
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "authors": "Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, Sandipan Dandapat",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.418"
    },
    {
        "id": 3385,
        "title": "Large Language Models are Prone to Methodological Artifacts",
        "authors": "Melanie Brucks, Olivier Toubia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4484416"
    },
    {
        "id": 3386,
        "title": "Impacts and Implications of Generative AI and Large Language Models: Redefining Banking Sector",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jier.v4i2.767"
    },
    {
        "id": 3387,
        "title": "Analysis of the Effectiveness of Large Language Models in Assessing Argumentative Writing and Generating Feedback",
        "authors": "Daisy Albuquerque da Silva, Carlos Eduardo de Mello, Ana Garcia",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012466600003636"
    },
    {
        "id": 3388,
        "title": "Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models",
        "authors": "Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, Lingming Zhang",
        "published": "2023-7-12",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597926.3598067"
    },
    {
        "id": 3389,
        "title": "The Next Chapter: A Study of Large Language Models in Storytelling",
        "authors": "Zhuohan Xie, Trevor Cohn, Jey Han Lau",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.23"
    },
    {
        "id": 3390,
        "title": "Experimenting with Planning and Reasoning in Ad Hoc Teamwork Environments with Large Language Models",
        "authors": "Polyana Costa, Pedro Santos, José Boaro, Daniel Moraes, Júlio Duarte, Sergio Colcher",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012472600003636"
    },
    {
        "id": 3391,
        "title": "LARGE LANGUAGE MODELS (LLMS) AND CHATGPT FOR BIOMEDICINE",
        "authors": "Cecilia Arighi, Steven Brenner, Zhiyong Lu",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/9789811286421_0048"
    },
    {
        "id": 3392,
        "title": "Can large language models help augment English psycholinguistic datasets?",
        "authors": "Sean Trott",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "AbstractResearch on language and cognition relies extensively on psycholinguistic datasets or “norms”. These datasets contain judgments of lexical properties like concreteness and age of acquisition, and can be used to norm experimental stimuli, discover empirical relationships in the lexicon, and stress-test computational models. However, collecting human judgments at scale is both time-consuming and expensive. This issue of scale is compounded for multi-dimensional norms and those incorporating context. The current work asks whether large language models (LLMs) can be leveraged to augment the creation of large, psycholinguistic datasets in English. I use GPT-4 to collect multiple kinds of semantic judgments (e.g., word similarity, contextualized sensorimotor associations, iconicity) for English words and compare these judgments against the human “gold standard”. For each dataset, I find that GPT-4’s judgments are positively correlated with human judgments, in some cases rivaling or even exceeding the average inter-annotator agreement displayed by humans. I then identify several ways in which LLM-generated norms differ from human-generated norms systematically. I also perform several “substitution analyses”, which demonstrate that replacing human-generated norms with LLM-generated norms in a statistical model does not change the sign of parameter estimates (though in select cases, there are significant changes to their magnitude). I conclude by discussing the considerations and limitations associated with LLM-generated norms in general, including concerns of data contamination, the choice of LLM, external validity, construct validity, and data quality. Additionally, all of GPT-4’s judgments (over 30,000 in total) are made available online for further analysis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13428-024-02337-z"
    },
    {
        "id": 3393,
        "title": "Hallucinations and Emergence in Large Language Models",
        "authors": "Bernardo A. Huberman, Sayandev Mukherjee",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4676180"
    },
    {
        "id": 3394,
        "title": "A Surgical Perspective on Large Language Models",
        "authors": "Robert Miller",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/sla.0000000000005896"
    },
    {
        "id": 3395,
        "title": "On Political Theory and Large Language Models",
        "authors": "Emma Rodman",
        "published": "2023-10-17",
        "citations": 3,
        "abstract": " Political theory as a discipline has long been skeptical of computational methods. In this paper, I argue that it is time for theory to make a perspectival shift on these methods. Specifically, we should consider integrating recently developed generative large language models like GPT-4 as tools to support our creative work as theorists. Ultimately, I suggest that political theorists should embrace this technology as a method of supporting our capacity for creativity—but that we should do so in a way that is mindful of the content and value of theorizing, the technical constraints of the models, and the ethical questions that the technology raises. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/00905917231200826"
    },
    {
        "id": 3396,
        "title": "Implications of ChatGPT and Large Language Models for Environmental Policymaking",
        "authors": "Andrew Gao",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499643"
    },
    {
        "id": 3397,
        "title": "Construction of a Japanese Financial Benchmark for Large Language Models",
        "authors": "Masanori Hirano",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4769124"
    },
    {
        "id": 3398,
        "title": "Variability in Large Language Models’ Responses to Medical Licensing and Certification Examinations. Comment on “How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment”",
        "authors": "Richard H Epstein, Franklin Dexter",
        "published": "2023-7-13",
        "citations": 8,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/48305"
    },
    {
        "id": 3399,
        "title": "Exploring Mathematical Spaces using Generative AI and Large Language Models",
        "authors": "Mohammad Raeini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4761694"
    },
    {
        "id": 3400,
        "title": "Integrating Large Language Models into Higher Education: Guidelines for Effective Implementation",
        "authors": "Karl de Fine Licht",
        "published": "2023-8-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008065"
    },
    {
        "id": 3401,
        "title": "Large language models for reducing clinicians’ documentation burden",
        "authors": "Kirk Roberts",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41591-024-02888-w"
    },
    {
        "id": 3402,
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": "Danqing Wang, Lei Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.659"
    },
    {
        "id": 3403,
        "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
        "authors": "Masahiro Kaneko, Naoaki Okazaki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.619"
    },
    {
        "id": 3404,
        "title": "The impact of chatbots based on large language models on second language vocabulary acquisition",
        "authors": "Zhihui Zhang, Xiaomeng Huang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e25370"
    },
    {
        "id": 3405,
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": "Iker García-Ferrero, Begoña Altuna, Javier Alvez, Itziar Gonzalez-Dios, German Rigau",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.531"
    },
    {
        "id": 3406,
        "title": "Machine recognition of non-native speech: Task-specific language models versus large language models",
        "authors": "Jian Cheng, Jared C. Bernstein, Masanori Suzuki",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "Automatic speech recognition (ASR) has offered a reliable foundation for measurement of young children’s reading skills and of second-language (L2) speaking skills. This is because well-fit task-specific language models (LMs) enable recognition that supports accurate scoring of pronunciation, fluency, vocabulary, usage, and grammar (Bernstein and Cheng, 2023). ASR works well in these measurement tasks because measurement of word production, disfluencies, and pronunciation errors is not very sensitive to moderate differences in word-error-rate (WER) accuracy, and because speech-interactive tasks appropriate for reading instruction or L2 assessment elicit relatively predictable responses, for which task-specific low-perplexity ASR systems achieve sufficiently accurate speech recognition (Cheng and Townshend, 2003). In the work reported here, we compared the accuracy of two English ASR systems on a set of 718 extended spontaneous speech recordings from 77 adult non-native speakers of English speaking from six countries under uncontrolled recording conditions. A Kaldi-based ASR system with well-fit task-specific LMs achieved WER 17%, while USM, a general-purpose mSLAM recognizer with an RNN-T decoder, achieved 11% WER, which is a 34% relative improvement. The mSLAM + RNN-T technology will be briefly described and an analysis of results in three different open-response interactive speaking tasks will be presented.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1121/10.0023275"
    },
    {
        "id": 3407,
        "title": "Assessing the Utility of Multimodal Large Language Models (GPT-4 Vision and Large Language and Vision Assistant) in Identifying Melanoma Across Different Skin Tones",
        "authors": "Katrina Cirone, Mohamed Akrout, Latif Abid, Amanda Oakley",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "The large language models GPT-4 Vision and Large Language and Vision Assistant are capable of understanding and accurately differentiating between benign lesions and melanoma, indicating potential incorporation into dermatologic care, medical research, and education.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/55508"
    },
    {
        "id": 3408,
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": "Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.21"
    },
    {
        "id": 3409,
        "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        "authors": "Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.189"
    },
    {
        "id": 3410,
        "title": "Microsyntactic Unit Detection usingWord Embedding Models: Experiments on Slavic Languages",
        "authors": "Iuliia Zaitova,  , Irina Stenger, Tania Avgustinova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_134"
    },
    {
        "id": 3411,
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "authors": "Jennifer Hu, Roger Levy",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.306"
    },
    {
        "id": 3412,
        "title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models",
        "authors": "Yanchen Liu, Timo Schick, Hinrich Schtze",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.sustainlp-1.2"
    },
    {
        "id": 3413,
        "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
        "authors": "Yuwei Zhang, Zihan Wang, Jingbo Shang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.858"
    },
    {
        "id": 3414,
        "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
        "authors": "Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.885"
    },
    {
        "id": 3415,
        "title": "IoT Device Classification Using Link-Level Features for Traditional Machine Learning and Large Language Models",
        "authors": "Gabriel Morales, Farhan Romit, Adam Bienek-Parrish, Patrick Jenkins, Rocky Slavin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012365700003648"
    },
    {
        "id": 3416,
        "title": "Ten Simple Rules for Crafting Effective Prompts for Large Language Models",
        "authors": "Zhicheng Lin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4565553"
    },
    {
        "id": 3417,
        "title": "Large Language Models and the Shoreline of Ophthalmology",
        "authors": "Benjamin K. Young, Peter Y. Zhao",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamaophthalmol.2023.6937"
    },
    {
        "id": 3418,
        "title": "FinDKG: Dynamic Knowledge Graph with Large Language Models for Global Finance",
        "authors": "Xiaohui Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4608445"
    },
    {
        "id": 3419,
        "title": "Economic, Societal, Legal, and Ethical Considerations for Large Language Models",
        "authors": "Jay Lofstead",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00049"
    },
    {
        "id": 3420,
        "title": "How to write effective prompts for large language models",
        "authors": "Zhicheng Lin",
        "published": "2024-3-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41562-024-01847-2"
    },
    {
        "id": 3421,
        "title": "Benchmarking medical large language models",
        "authors": "Sadra Bakhshandeh",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s44222-023-00097-7"
    },
    {
        "id": 3422,
        "title": "Challenges and Limitations of ChatGPT and Other Large Language Models Challenges",
        "authors": "Erwin Rimban",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4454441"
    },
    {
        "id": 3423,
        "title": "Contextualized Sentiment Analysis using Large Language Models",
        "authors": "Christian Breitung, Garvin Kruthof, Sebastian Müller",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4615038"
    },
    {
        "id": 3424,
        "title": "R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
        "authors": "Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.114"
    },
    {
        "id": 3425,
        "title": "Learning How to Use Large Language Models for Empirical Legal Research",
        "authors": "Edward Stiglitz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4628573"
    },
    {
        "id": 3426,
        "title": "Enterprise Large Language Models: Knowledge Characteristics, Risks and Organizational Activities",
        "authors": "Daniel E. O'Leary",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4659476"
    },
    {
        "id": 3427,
        "title": "A Framework for the Evaluation of Large Language Models",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4649866"
    },
    {
        "id": 3428,
        "title": "Study Tests Large Language Models’ Ability to Answer Clinical Questions",
        "authors": "Emily Harris",
        "published": "2023-8-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jama.2023.12553"
    },
    {
        "id": 3429,
        "title": "Professional Certification Benchmark Dataset: The First 500 Jobs for Large Language Models",
        "authors": "David Noever, Matt Ciolino",
        "published": "2023-7-22",
        "citations": 0,
        "abstract": "The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (>70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interestingly, Turbo-GPT3.5 performed well on customer service tasks, suggesting potential applications in human augmentation for chatbots in call centers and routine advice services. The models also score well on sensory and experience-based tests such as wine sommelier, beer taster, emotional quotient, and body language reader. The OpenAI model improvement from Babbage to Turbo resulted in a median 60% better-graded performance in less than a few years. This progress suggests that focusing on the latest model's shortcomings could lead to a highly performant AI capable of mastering the most demanding professional certifications. We open-source the benchmark to expand the range of testable professional skills as the models improve or gain emergent capabilities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131211"
    },
    {
        "id": 3430,
        "title": "Should ChatGPT be biased? Challenges and risks of bias in large language models",
        "authors": "Emilio Ferrara",
        "published": "2023-11-7",
        "citations": 7,
        "abstract": "As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5210/fm.v28i11.13346"
    },
    {
        "id": 3431,
        "title": "Large Language Models and Generative AI, Oh My!",
        "authors": "Michael Zyda",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mc.2024.3350290"
    },
    {
        "id": 3432,
        "title": "Large Language Models: AI's Legal Revolution",
        "authors": "Adam Allen Bent",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.58948/2331-3528.2083"
    },
    {
        "id": 3433,
        "title": "Measuring and Modifying Factual Knowledge in Large Language Models",
        "authors": "Pouya Pezeshkpour",
        "published": "2023-12-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00122"
    },
    {
        "id": 3434,
        "title": "InteraSSort : Interactive Assortment Planning Using Large Language Models",
        "authors": "Saketh reddy Karra, Theja Tulabandhula",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4647359"
    },
    {
        "id": 3435,
        "title": "Text and knowledge in the aspect of large language models",
        "authors": "Boris Valer'evich Orekhov",
        "published": "2023-4",
        "citations": 0,
        "abstract": "\n The focus of this text is on the influence of large linguistic models on the self-determination of the humanities. Large language models are able to generate plausible texts. It seems that they thus become on a par with other tools that, throughout the development of technology have freed people from routine. At the same time, for the humanities, the individualization of the generated texts is very great, and knowledge itself is closely related to its textual embodiment. If we agree that knowledge is a text, and embodied in another text, another knowledge appears before us, then humanities will have to answer the question of how a text generated by a person differs in value from the same text generated by a machine. The text of the work raises methodological and epistemological problems of the correlation of texts of natural and artificial origin if they are made in the genre of a scientific work. The difference between such artifacts is clearly visible only for some scientific disciplines, and raises questions about the rest. These issues should be resolved with the help of deep reflection, which was not so urgently needed in the last centuries of the development of the humanities, but which is now required from a humanitarian scientist. The humanitarian will have to explicitly oppose himself to large language models and prove the importance of his work compared to what a neural network can generate.\n\t",
        "keywords": "",
        "link": "http://dx.doi.org/10.7256/2585-7797.2023.4.44180"
    },
    {
        "id": 3436,
        "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
        "authors": "Hanieh Alipour, Nick Pendar, Kohinoor Roy",
        "published": "2024-3-16",
        "citations": 0,
        "abstract": "In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-theminute review of the literature. By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.1405114"
    },
    {
        "id": 3437,
        "title": "ActuaryGPT: Applications of Large Language Models to Insurance and Actuarial Work",
        "authors": "Caesar Balona",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4543652"
    },
    {
        "id": 3438,
        "title": "Large Language Models are few(1)-shot Table Reasoners",
        "authors": "Wenhu Chen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.83"
    },
    {
        "id": 3439,
        "title": "STOCK SENTIMENT ANALYSIS USING LARGE LANGUAGE MODELS",
        "authors": "",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets49682"
    },
    {
        "id": 3440,
        "title": "Prepare for truly useful large language models",
        "authors": "",
        "published": "2023-3-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41551-023-01012-6"
    },
    {
        "id": 3441,
        "title": "Generative Models For Indic Languages: Evaluating Content Generation Capabilities",
        "authors": "Savita Bhat,  , Vasudeva Varma, Niranjan Pedanekar,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_021"
    },
    {
        "id": 3442,
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": "Gustavo Gonçalves, Emma Strubell",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.161"
    },
    {
        "id": 3443,
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "authors": "Dheeraj Mekala, Jason Wolfe, Subhro Roy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.354"
    },
    {
        "id": 3444,
        "title": "The Evolution of Large Language Models in Natural Language Understanding",
        "authors": "Chinmay Shripad Kulkarni",
        "published": "2023-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.51219/jaimld/chinmay-shripad-kulkarni/28"
    },
    {
        "id": 3445,
        "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
        "authors": "Hongyi Zheng, Abulhair Saparov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.277"
    },
    {
        "id": 3446,
        "title": "Enhancing Text Summarization: Evaluating Transformer-Based Models and the Role of Large Language Models like ChatGPT",
        "authors": "Pınar Savcı, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391040"
    },
    {
        "id": 3447,
        "title": "AI as Agency Without Intelligence: on ChatGPT, Large Language Models, and Other Generative Models",
        "authors": "Luciano Floridi",
        "published": "2023-3",
        "citations": 64,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13347-023-00621-y"
    },
    {
        "id": 3448,
        "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
        "authors": "Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.807"
    },
    {
        "id": 3449,
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": "Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.398"
    },
    {
        "id": 3450,
        "title": "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
        "authors": "Paula Maddigan, Teo Susnjak",
        "published": "2023",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3274199"
    },
    {
        "id": 3451,
        "title": "Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models",
        "authors": "Shun Kiyono,  , Sho Takase, Shengzhe Li, Toshinori Sato,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_062"
    },
    {
        "id": 3452,
        "title": "Large Language Models for Code Obfuscation Evaluation of the Obfuscation Capabilities of OpenAI’s GPT-3.5 on C Source Code",
        "authors": "Patrick Kochberger, Maximilian Gramberger, Sebastian Schrittwieser, Caroline Lawitschka, Edgar Weippl",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012167000003555"
    },
    {
        "id": 3453,
        "title": "On the Construction of Database Interfaces Based on Large Language Models",
        "authors": "João Pinheiro, Wendy Victorio, Eduardo Nascimento, Antony Seabra, Yenier Izquierdo, Grettel García, Gustavo Coelho, Melissa Lemos, Luiz Leme, Antonio Furtado, Marco Casanova",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012204000003584"
    },
    {
        "id": 3454,
        "title": "Anwendung von „large language models“ in der Klinik",
        "authors": "Jasmin Zernikow, Leonhard Grassow, Jan Gröschel, Philippe Henrion, Paul J. Wetzel, Sebastian Spethmann",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00108-023-01600-3"
    },
    {
        "id": 3455,
        "title": "A clinician's guide to large language models",
        "authors": "Giovanni Briganti",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "The rapid advancement of artificial intelligence (AI) has led to the emergence of large language models (LLMs) as powerful tools for various applications, including healthcare. These large-scale machine learning models, such as GPT and LLaMA have demonstrated potential for improving patient outcomes and transforming medical practice. However, healthcare professionals without a background in data science may find it challenging to understand and utilize these models effectively. This paper aims to provide an accessible introduction to LLMs for healthcare professionals, discussing their core concepts, relevant applications in healthcare, ethical considerations, challenges, and future directions. With an overview of LLMs, we foster a more collaborative future between healthcare professionals and data scientists, ultimately driving better patient care and medical advancements.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2217/fmai-2023-0003"
    },
    {
        "id": 3456,
        "title": "Post-Deployment Regulatory Oversight for General-Purpose Large Language Models",
        "authors": "Carson Ezell, Abraham Loeb",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4658623"
    },
    {
        "id": 3457,
        "title": "Large Language Models and User Trust:  Focus on Healthcare  (Preprint)",
        "authors": "Avishek Choudhury, Zaira Chaudhry",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/56764"
    },
    {
        "id": 3458,
        "title": "Fact-checking benchmark for the Russian Large Language Models",
        "authors": "Anastasia Kozlova,  , Denis Shevelev, Alena Fenogenova",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "Modern text-generative language models are rapidly developing. They produce text of high quality and are used in many real-world applications. However, they still have several limitations, for instance, the length of the context, degeneration processes, lack of logical structure, and facts consistency. In this work, we focus on the fact-checking problem applied to the output of the generative models on classical downstream tasks, such as paraphrasing, summarization, text style transfer, etc. We define the task of internal fact-checking, set the criteria for factual consistency, and present the novel dataset for this task for the Russian language. The benchmark for internal fact-checking and several baselines are also provided. We research data augmentation approaches to extend the training set and compare classification methods on different augmented data sets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-267-277"
    },
    {
        "id": 3459,
        "title": "The influence of Large Language Models on systematic review and research dissemination",
        "authors": "Simon Baradziej",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "Watch VIDEO.\nThis presentation will delve into the transformative role of AI in scholarly communication, highlighting its potential, implications, and challenges, and further addressing the ethical considerations that come with it.\nRecent advancements in AI, specifically large language models have unlocked new possibilities for scientific exploration and communication. Large language models such as GPT-4 and LLAMA, with their remarkable text-generation capabilities, stand at the forefront of this AI revolution. In the first part of the presentation, I examine how these AI tools are reshaping the nature of systematic reviews. The ability to analyze, summarize, and generate vast amounts of text allows these models to facilitate more efficient processes, offering a valuable tool to researchers navigating through vast databases of published work.\nI would discuss how AI is engendering new developments in research methodology. Through the use of predictive modelling and advanced analytics, AI tools like GPT-4 allow for a deeper understanding of existing research and the identification of gaps in the literature, thereby promoting innovative research approaches. However, these advancements come with the need for updated ethical frameworks, a topic I would try to address also.\nThe issues related to AI use include issues of transparency and accountability, as the ”blackbox” approach to deep learning models can be uncovered; without appropriate interpretability architecture (such as with GPT-4 or LLAMA), these models can be generating inaccurate information based on their predictive capabilities.\nNevertheless, these have proved to be of use, and with a fine prompt tuning, publicly available models can be of great use to researchers. I would delve into the question of how to balance the benefits of AI tools with the need to maintain high ethical standards in research, aiming to provide possible insights into how these ethical frameworks might be updated to accommodate the new realities of AI.\nFurthermore, I’d reflect on the consequences of AI for the evaluation of research. While AI can aid in the quick assessment of a paper's relevance or novelty, questions remain about its capacity to fully evaluate the quality and significance of research. This discussion emphasizes the need for a blend of AI models with human expertise to achieve robust research evaluation for the time being (or for further training for specific use cases of the models.)\nI’d like to conclude with a reflection on the overall impact of the integration of AI LLMs on systematic reviews and research dissemination. While acknowledging the transformative potential of AI in reshaping the scientific landscape, it underscores the need for careful navigation of the associated challenges and ethical implications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/5.7240"
    },
    {
        "id": 3460,
        "title": "Biomedical Parallel Sentence Retrieval Using Large Language Models",
        "authors": "Sheema Firdous, Sadaf Abdul Rauf",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.wmt-1.26"
    },
    {
        "id": 3461,
        "title": "Large Language Models and Return Prediction in China",
        "authors": "Lin Tan, huihang wu, Xiaoyan Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4712248"
    },
    {
        "id": 3462,
        "title": "A bilingual benchmark for evaluating large language models",
        "authors": "Mohamed Alkaoud",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "This work introduces a new benchmark for the bilingual evaluation of large language models (LLMs) in English and Arabic. While LLMs have transformed various fields, their evaluation in Arabic remains limited. This work addresses this gap by proposing a novel evaluation method for LLMs in both Arabic and English, allowing for a direct comparison between the performance of the two languages. We build a new evaluation dataset based on the General Aptitude Test (GAT), a standardized test widely used for university admissions in the Arab world, that we utilize to measure the linguistic capabilities of LLMs. We conduct several experiments to examine the linguistic capabilities of ChatGPT and quantify how much better it is at English than Arabic. We also examine the effect of changing task descriptions from Arabic to English and vice-versa. In addition to that, we find that fastText can surpass ChatGPT in finding Arabic word analogies. We conclude by showing that GPT-4 Arabic linguistic capabilities are much better than ChatGPT’s Arabic capabilities and are close to ChatGPT’s English capabilities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1893"
    },
    {
        "id": 3463,
        "title": "Intrusion Detection Technology Based on Large Language Models",
        "authors": "Hsiaofan Lai",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/easct59475.2023.10393509"
    },
    {
        "id": 3464,
        "title": "Is ChatGPT taking over the language classroom?",
        "authors": "Mandy Lau",
        "published": "2024-2-16",
        "citations": 0,
        "abstract": "ChatGPT generated much dialogue on the implications of large language models (LLMs) for language teaching and learning. Since language teachers are uniquely positioned to teach metalinguistic awareness, they can support their learners’ understanding of how LLMs are shaped by language ideologies and how their outputs are indexical of social power. This awareness would help learners be more conscientious in using LLMs, deciding how to interact with them and adapt their outputs for their purposes. This article introduces LLMs as statistical systems that predict linguistic forms. It surfaces two language ideologies that have shaped their development: the belief in the separability of language from its social contexts and the belief in the value of larger text corpora. It also highlights some ideological effects including uneven language performance, text outputs that reflect biases, privacy violations, circulation of copyrighted materials, misinformation, and hallucinations. Some suggestions for mitigating these effects are offered.",
        "keywords": "",
        "link": "http://dx.doi.org/10.25071/2564-2855.36"
    },
    {
        "id": 3465,
        "title": "Considerations for Prompting Large Language Models—Reply",
        "authors": "Shan Chen, Guergana K. Savova, Danielle S. Bitterman",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamaoncol.2023.6966"
    },
    {
        "id": 3466,
        "title": "Towards Concept-Aware Large Language Models",
        "authors": "Chen Shani, Jilles Vreeken, Dafna Shahaf",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.877"
    },
    {
        "id": 3467,
        "title": "Trend Extraction and Analysis via Large Language Models",
        "authors": "Tommaso Soru, Jim Marshall",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc59802.2024.00051"
    },
    {
        "id": 3468,
        "title": "INTEGRATING LARGE LANGUAGE MODELS FOR REAL-WORLD PROBLEM MODELLING: A COMPARATIVE STUDY",
        "authors": "Carlos-Miguel Lorenzo",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/inted.2024.0871"
    },
    {
        "id": 3469,
        "title": "Large Language Models and Financial Market Sentiment",
        "authors": "Shaun Alexander Bond, Hayden Klok, Min Zhu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4584928"
    },
    {
        "id": 3470,
        "title": "AI, ML, and Large Language Models in Cybersecurity",
        "authors": "",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets49546"
    },
    {
        "id": 3471,
        "title": "Large Language Models (LLMs): Representation Matters, Low-Resource Languages and Multi-Modal Architecture",
        "authors": "Ganesh Mani, Galane Basha Namomsa",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/africon55910.2023.10293675"
    },
    {
        "id": 3472,
        "title": "LLANIME: Large Language Models for Anime Recommendations",
        "authors": "Anjali Agarwal, Sahil Sharma",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dese60595.2023.10468757"
    },
    {
        "id": 3473,
        "title": "What’s the next word in large language models?",
        "authors": "",
        "published": "2023-4-24",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s42256-023-00655-z"
    },
    {
        "id": 3474,
        "title": "Large-language models binnen de klinische chemie",
        "authors": "William van Doorn, Steef Kurstjens",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.24078/labgeneeskunde.2023.10.23776"
    },
    {
        "id": 3475,
        "title": "Lower Energy Large Language Models (LLMs)",
        "authors": "Hsiao-Ying Lin, Jeffrey Voas",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mc.2023.3278160"
    },
    {
        "id": 3476,
        "title": "ISSCC 2024 Forum 2: Energy-Efficient AI-Computing Systems for Large-Language Models",
        "authors": "",
        "published": "2024-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isscc49657.2024.10454551"
    },
    {
        "id": 3477,
        "title": "Large Language Models Can Enhance Persuasion Through Linguistic Feature Alignment",
        "authors": "Minkyu Shin, Jin Kim",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4725351"
    },
    {
        "id": 3478,
        "title": "HTMOT: Hierarchical Topic Modelling Over Time",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_092"
    },
    {
        "id": 3479,
        "title": "Call for Papers",
        "authors": " ",
        "published": "2023",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.5840/teachphil202346111"
    },
    {
        "id": 3480,
        "title": "Use of large language models might affect our cognitive skills",
        "authors": "Richard Heersmink",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41562-024-01859-y"
    },
    {
        "id": 3481,
        "title": "Data Augmentation for Fake News Detection by Combining Seq2seq and NLI",
        "authors": "Anna Glazkova,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_048"
    },
    {
        "id": 3482,
        "title": "Long-Term Memory for Large Language Models Through Topic-Based Vector Database",
        "authors": "Yi Zhang, Zhongyang Yu, Wanqi Jiang, Yufeng Shen, Jin Li",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ialp61005.2023.10337079"
    },
    {
        "id": 3483,
        "title": "FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models",
        "authors": "Xinge Ma, Jiangming Liu, Jin Wang, Xuejie Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.529"
    },
    {
        "id": 3484,
        "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
        "authors": "Sullam Jeoung, Yubin Ge, Jana Diesner",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.752"
    },
    {
        "id": 3485,
        "title": "Making Large Language Models Better Data Creators",
        "authors": "Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, Sujay Jauhar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.948"
    },
    {
        "id": 3486,
        "title": "Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content",
        "authors": "Savita Bhat, Vasudeva Varma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eval4nlp-1.8"
    },
    {
        "id": 3487,
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "authors": "Shushan Arakelyan, Rocktim Das, Yi Mao, Xiang Ren",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.1013"
    },
    {
        "id": 3488,
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": "Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.391"
    },
    {
        "id": 3489,
        "title": "Query Rewriting in Retrieval-Augmented Large Language Models",
        "authors": "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.322"
    },
    {
        "id": 3490,
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "authors": "Daman Arora, Himanshu Singh,  Mausam",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.468"
    },
    {
        "id": 3491,
        "title": "The (ab)use of Open Source Code to Train Large Language Models",
        "authors": "Ali Al-Kaswan, Maliheh Izadi",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nlbse59153.2023.00008"
    },
    {
        "id": 3492,
        "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
        "authors": "Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordás, Gerasimos Spanakis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.539"
    },
    {
        "id": 3493,
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "authors": "David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, John Canny",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.841"
    },
    {
        "id": 3494,
        "title": "Large Language Models for Multilingual Slavic Named Entity Linking",
        "authors": "Rinalds Vīksna, Inguna Skadiņa, Daiga Deksne, Roberts Rozis",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bsnlp-1.20"
    },
    {
        "id": 3495,
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": "Potsawee Manakul, Adian Liusie, Mark Gales",
        "published": "2023",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.557"
    },
    {
        "id": 3496,
        "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
        "authors": "António Farinhas, José de Souza, Andre Martins",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.733"
    },
    {
        "id": 3497,
        "title": "Non-Parametric Memory Guidance for Multi-Document Summarization",
        "authors": "Florian Baud,  , Alex Aussem,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_017"
    },
    {
        "id": 3498,
        "title": "Multilingual Continual Learning Approaches for Text Classification",
        "authors": "Karan Praharaj,  , Irina Matveeva,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_093"
    },
    {
        "id": 3499,
        "title": "Large Language Models in Healthcare: A Review",
        "authors": "Shun Zou, Jun He",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscsic60498.2023.00038"
    },
    {
        "id": 3500,
        "title": "Digital Dissonance: Large Language Models' Unbalanced Political Narrative",
        "authors": "Fabio Motoki, Valdemar Pinho Neto, Victor Rangel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4773936"
    },
    {
        "id": 3501,
        "title": "LARGE LANGUAGE MODELS FOR CIPHERS",
        "authors": "David Noever",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "This study investigates whether transformer models like ChatGPT (GPT4, MAR2023) can generalize beyond their training data by examining their performance on the novel Cipher Dataset, which scrambles token order. The dataset consists of 654 test cases, and the analysis focuses on 51 text examples and 13 algorithmic choices. Results show that the models perform well on low-difficulty ciphers like Caesar and can unscramble tokens in 77% of the cipher examples. Despite their reliance on training data, the model's ability to generalize outside of token order is surprising, especially when leveraging large-scale models with hundreds of billions of weights and a comprehensive text corpus with few examples. The original contributions of the work focus on presenting a cipher challenge dataset and then scoring historically significant ciphers for large language models to descramble. The real challenge for these generational models lies in executing the complex algorithmic steps on new cipher inputs, potentially as a novel reasoning challenge that relies less on knowledge acquisition and more on trial-and-error or out-ofbounds responses.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijaia.2023.14301"
    },
    {
        "id": 3502,
        "title": "Large language models: fast proliferation and budding international competition",
        "authors": "",
        "published": "2023-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/13567888.2023.2198430"
    },
    {
        "id": 3503,
        "title": "Cybercrime and Privacy Threats of Large Language Models",
        "authors": "Nir Kshetri",
        "published": "2023-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mitp.2023.3275489"
    },
    {
        "id": 3504,
        "title": "Harnessing Large Language Models in Nursing Care Planning: Opportunities, Challenges, and Ethical Considerations",
        "authors": "Abdulqadir  J Nashwan, Ahmad A Abujaber",
        "published": "2023-6-16",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.40542"
    }
]