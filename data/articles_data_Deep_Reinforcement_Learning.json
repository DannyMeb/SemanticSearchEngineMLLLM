[
    {
        "id": 18071,
        "title": "Reinforcement Learning and Deep Reinforcement Learning",
        "authors": "",
        "published": "2021-4-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108955652.016"
    },
    {
        "id": 18072,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0012"
    },
    {
        "id": 18073,
        "title": "Deep Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_3"
    },
    {
        "id": 18074,
        "title": "Computer Vision, Deep Learning, Deep Reinforcement Learning",
        "authors": "Farshid PirahanSiah",
        "published": "2019-11-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-uncat.clzwyuz.v1"
    },
    {
        "id": 18075,
        "title": "Overview of Deep Reinforcement Learning Methods",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "This video gives an overview of methods for deep reinforcement learning, including deep Q-learning, actor-critic methods, deep policy networks, and policy gradient optimization algorithms.",
        "link": "http://dx.doi.org/10.52843/cassyni.kfnzpy"
    },
    {
        "id": 18076,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.017"
    },
    {
        "id": 18077,
        "title": "Deep Reinforcement Learning",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_5"
    },
    {
        "id": 18078,
        "title": "Hierarchical Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_8"
    },
    {
        "id": 18079,
        "title": "Deep Reinforcement Learning for Fluid Dynamics and Control",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning based on deep learning is currently being used for impressive control of fluid dynamic systems.  This video will describe recent advances, including for mimicking the behavior of birds and fish, for turbulence closure modeling with sub-grid-scale models, and for robotic flight demonstrations.",
        "link": "http://dx.doi.org/10.52843/cassyni.kvtnvy"
    },
    {
        "id": 18080,
        "title": "Multi-Agent Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_7"
    },
    {
        "id": 18081,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_5"
    },
    {
        "id": 18082,
        "title": "Reinforcement Learning and Deep Reinforcement Learning",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_2"
    },
    {
        "id": 18083,
        "title": "Policy-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_4"
    },
    {
        "id": 18084,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 18085,
        "title": "Deep Reinforcement Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_3"
    },
    {
        "id": 18086,
        "title": "Tabular Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_2"
    },
    {
        "id": 18087,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.016"
    },
    {
        "id": 18088,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.015"
    },
    {
        "id": 18089,
        "title": "Hierarchical Reinforcement Learning",
        "authors": "Yanhua Huang",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_10"
    },
    {
        "id": 18090,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/review2"
    },
    {
        "id": 18091,
        "title": "Deep Reinforcement Learning and Its Applications",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119873747.ch1"
    },
    {
        "id": 18092,
        "title": "Deep Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 73,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-6"
    },
    {
        "id": 18093,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/review2"
    },
    {
        "id": 18094,
        "title": "Deep Reinforcement Learning Models and Techniques",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119873747.ch3"
    },
    {
        "id": 18095,
        "title": "Deep Reinforcement Learning Processor Design for Mobile Applications",
        "authors": "Juhyoung Lee, Hoi-Jun Yoo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-36793-9_1"
    },
    {
        "id": 18096,
        "title": "Deep reinforcement learning architectures",
        "authors": "Shajulin Benedict",
        "published": "2022-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/978-0-7503-4024-3ch9"
    },
    {
        "id": 18097,
        "title": "Robust Adversarial Deep Reinforcement Learning",
        "authors": "Di Wang",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Deep reinforcement learning has shown remarkable results across various tasks. However, recent studies highlight the susceptibility of DRL to targeted adversarial disruptions. Furthermore, discrepancies between simulated settings and real-world applications often make it challenging to transfer these DRL policies, particularly in situations where safety is essential. Several solutions have been proposed to address these issues to enhance DRL's robustness. This chapter delves into the significance of adversarial attack and defense strategies in machine learning, emphasizing the unique challenges in adversarial DRL settings. It also presents an overview of recent advancements, DRL foundations, adversarial Markov decision process models, and comparisons among different attacks and defenses. The chapter further evaluates the effectiveness of various attacks and the efficacy of multiple defense mechanisms using simulation data, specifically focusing on policy success rates and average rewards. Potential limitations and prospects for future research are also explored.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch005"
    },
    {
        "id": 18098,
        "title": "Deep Learning in Medical Imaging",
        "authors": "Narjes Benameur, Ramzi Mahmoudi",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "Medical image processing tools play an important role in clinical routine in helping doctors to establish whether a patient has or does not have a certain disease. To validate the diagnosis results, various clinical parameters must be defined. In this context, several algorithms and mathematical tools have been developed in the last two decades to extract accurate information from medical images or signals. Traditionally, the extraction of features using image processing from medical data are time-consuming which requires human interaction and expert validation. The segmentation of medical images, the classification of medical images, and the significance of deep learning-based algorithms in disease detection are all topics covered in this chapter.",
        "link": "http://dx.doi.org/10.5772/intechopen.111686"
    },
    {
        "id": 18099,
        "title": "Deep Reinforcement Learning",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_10"
    },
    {
        "id": 18100,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 18101,
        "title": "Challenges of Reinforcement Learning",
        "authors": "Zihan Ding, Hao Dong",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_7"
    },
    {
        "id": 18102,
        "title": "Deep Reinforcement Learning",
        "authors": "Charu Aggarwal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29642-0_11"
    },
    {
        "id": 18103,
        "title": "Deep Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1"
    },
    {
        "id": 18104,
        "title": "Multi-Agent Reinforcement Learning",
        "authors": "Huaqing Zhang, Shanghang Zhang",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_11"
    },
    {
        "id": 18105,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review1"
    },
    {
        "id": 18106,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review2"
    },
    {
        "id": 18107,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "Amar Barik",
        "published": "2020-5-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/review1"
    },
    {
        "id": 18108,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_1"
    },
    {
        "id": 18109,
        "title": "Meta-Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_9"
    },
    {
        "id": 18110,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "Amar Barik",
        "published": "2020-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/review1"
    },
    {
        "id": 18111,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review3"
    },
    {
        "id": 18112,
        "title": "Deep Reinforcement Learning",
        "authors": "Charu C. Aggarwal",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-94463-0_9"
    },
    {
        "id": 18113,
        "title": "Arena Platform for Multi-Agent Reinforcement Learning",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_17"
    },
    {
        "id": 18114,
        "title": "Taxonomy of Reinforcement Learning Algorithms",
        "authors": "Hongming Zhang, Tianyang Yu",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_3"
    },
    {
        "id": 18115,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 18116,
        "title": "Intelligent Roundabout Insertion using Deep Reinforcement Learning",
        "authors": "Alessandro Capasso, Giulio Bacchiani, Daniele Molinari",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008915003780385"
    },
    {
        "id": 18117,
        "title": "Decision letter for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-5-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/decision1"
    },
    {
        "id": 18118,
        "title": "Decision letter for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/decision1"
    },
    {
        "id": 18119,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/review1"
    },
    {
        "id": 18120,
        "title": "Policy-Based Deep Reinforcement Learning",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-18"
    },
    {
        "id": 18121,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_1"
    },
    {
        "id": 18122,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 18123,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 18124,
        "title": "Reactive Power Optimization Using Feed Forward Neural Deep Reinforcement Learning Method : (Deep Reinforcement Learning DQN algorithm)",
        "authors": "Mazhar Ali, Asad Mujeeb, Hameed Ullah, Saran Zeb",
        "published": "2020-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeees48850.2020.9121492"
    },
    {
        "id": 18125,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 18126,
        "title": "Graph Neural Networks and Reinforcement Learning: A Survey",
        "authors": "Fatemeh Fathinezhad, Peyman Adibi, Bijan Shoushtarian, Jocelyn Chanussot",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "Graph neural network (GNN) is an emerging field of research that tries to generalize deep learning architectures to work with non-Euclidean data. Nowadays, combining deep reinforcement learning (DRL) with GNN for graph-structured problems, especially in multi-agent environments, is a powerful technique in modern deep learning. From the computational point of view, multi-agent environments are inherently complex, because future rewards depend on the joint actions of multiple agents. This chapter tries to examine different types of applying GNN and DRL techniques in the most common representations of multi-agent problems and their challenges. In general, the fusion of GNN and DRL can be addressed from two different points of view. First, GNN is used to influence the DRL performance and improve its formulation. Here, GNN is applied in relational DRL structures such as multi-agent and multi-task DRL. Second, DRL is used to improve the application of GNN. From this viewpoint, DRL can be used for a variety of purposes including neural architecture search and improving the explanatory power of GNN predictions.",
        "link": "http://dx.doi.org/10.5772/intechopen.111651"
    },
    {
        "id": 18127,
        "title": "Deep reinforcement learning",
        "authors": "Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-385787-1.00011-7"
    },
    {
        "id": 18128,
        "title": "Deep Reinforcement Learning for Mobile Social Networks",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_4"
    },
    {
        "id": 18129,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 18130,
        "title": "Decentralized Multi-agent Formation Control via Deep Reinforcement Learning",
        "authors": "Aniket Gutpa, Raghava Nallanthighal",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010241302890295"
    },
    {
        "id": 18131,
        "title": "Attacks on Deep Reinforcement Learning Systems: A Tutorial",
        "authors": "Joseph Layton, Fei Hu",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003187158-6"
    },
    {
        "id": 18132,
        "title": "Basics of Reinforcement Learning",
        "authors": "Thimira Amaratunga",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6431-7_12"
    },
    {
        "id": 18133,
        "title": "Temporal Difference Learning, SARSA, and Q-Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_4"
    },
    {
        "id": 18134,
        "title": "Solving Maximal Stable Set Problem via Deep Reinforcement Learning",
        "authors": "Taiyi Wang, Jiahao Shi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010179904830489"
    },
    {
        "id": 18135,
        "title": "Multi-task Deep Reinforcement Learning for IoT Service Selection",
        "authors": "Hiroki Matsuoka, Ahmed Moustafa",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010857800003116"
    },
    {
        "id": 18136,
        "title": "Measuring Inflation within Virtual Economies using Deep Reinforcement Learning",
        "authors": "Conor Stephens, Chris Exton",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010392804440453"
    },
    {
        "id": 18137,
        "title": "Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning",
        "authors": "Yuansheng Xie, Soroush Vosoughi, Saeed Hassanpour",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956245"
    },
    {
        "id": 18138,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_1"
    },
    {
        "id": 18139,
        "title": "Using Deep Reinforcement Learning to Build Intelligent Tutoring Systems",
        "authors": "Ciprian Paduraru, Miruna Paduraru, Stefan Iordache",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011267400003266"
    },
    {
        "id": 18140,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Zihan Ding, Yanhua Huang, Hang Yuan, Hao Dong",
        "published": "2020",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_2"
    },
    {
        "id": 18141,
        "title": "Deep vs. Deep Bayesian: Faster Reinforcement Learning on a Multi-robot Competitive Experiment",
        "authors": "Jingyi Huang, Fabio Giardina, Andre Rosendo",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010601900002994"
    },
    {
        "id": 18142,
        "title": "Background: Deep Reinforcement Learning",
        "authors": "Yuecheng Li, Hongwen He",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-79206-9_2"
    },
    {
        "id": 18143,
        "title": "Deep reinforcement learning in medical imaging",
        "authors": "S. Kevin Zhou, Qiyuan Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-385124-4.00010-6"
    },
    {
        "id": 18144,
        "title": "Deep Q-Learning",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_6"
    },
    {
        "id": 18145,
        "title": "Deep vs. Deep Bayesian: Faster Reinforcement Learning on a Multi-robot Competitive Experiment",
        "authors": "Jingyi Huang, Fabio Giardina, Andre Rosendo",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010601905010506"
    },
    {
        "id": 18146,
        "title": "Sound manipulation through multi-scattering, gradient-based optimization, deep learning and reinforcement learning",
        "authors": "Feruza Amirkulova",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.606f15dd30a2e980041f24ff"
    },
    {
        "id": 18147,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2022-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119910695.ch4"
    },
    {
        "id": 18148,
        "title": "Deep Reinforcement Learning for Interference Alignment Wireless Networks",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_3"
    },
    {
        "id": 18149,
        "title": "Deep Reinforcement Learning for Pellet Eating in Agar.io",
        "authors": "Nil Ansó, Anton Wiehe, Madalina Drugan, Marco Wiering",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007360901230133"
    },
    {
        "id": 18150,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 18151,
        "title": "Deep Reinforcement Learning Versus Evolution Strategies: A Comparative Survey",
        "authors": "Amjad Majid",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Deep Reinforcement Learning (DRL) has the potential to surpass human-level control in sequential decision-making problems. Evolution Strategies (ESs) have different characteristics than DRL, yet they are promoted as a scalable alternative. </div><div>To get insights into their strengths and weaknesses, in this paper, we put the two approaches side by side. After presenting the fundamental concepts and algorithms for each of the two approaches, they are compared from the perspectives of scalability, exploration, adaptation to dynamic environments, and multi-agent learning. Then, the paper discusses hybrid algorithms, combining aspects of both DRL and ESs, and how they attempt to capitalize on the benefits of both techniques. Lastly, both approaches are compared based on the set of applications they support, showing their potential for tackling real-world problems.</div><div>This paper aims to present an overview of how DRL and ESs can be used, either independently or in unison, to solve specific learning tasks. It is intended to guide researchers to select which method suits them best and provides a bird's eye view of the overall literature in the field. Further, we also provide application scenarios and open challenges.  </div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14679504.v1"
    },
    {
        "id": 18152,
        "title": "Wide and Deep Reinforcement Learning for Grid-based Action Games",
        "authors": "Juan Montoya, Christian Borgelt",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007313200500059"
    },
    {
        "id": 18153,
        "title": "Deep Reinforcement Learning in Human Activity Recognition: A Survey",
        "authors": "Bahareh Nikpour",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Human activity recognition is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real world intelligent systems, like visual surveillance and human-computer interaction. Deep Reinforcement Learning (DRL) has recently been employed to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based human activity recognition has only been around for a short time, and it is a challenging, novel field[ of study. Therefore, to facilitate further research in this field, we have constructed a comprehensive survey on activity recognition methods that incorporate deep reinforcement learning. Towards the end of this survey, we summarize key challenges and open problems in this area that can be addressed by researchers in the future.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19172369.v1"
    },
    {
        "id": 18154,
        "title": "Experience Filtering for Robot Navigation using Deep Reinforcement Learning",
        "authors": "Phong Nguyen, Takayuki Akiyama, Hiroki Ohashi",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006671802430249"
    },
    {
        "id": 18155,
        "title": "Introduction to Deep Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_6"
    },
    {
        "id": 18156,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 18157,
        "title": "Scenario-assisted Deep Reinforcement Learning",
        "authors": "Raz Yerushalmi, Guy Amir, Achiya Elyasaf, David Harel, Guy Katz, Assaf Marron",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010904700003119"
    },
    {
        "id": 18158,
        "title": "Introduction",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_1"
    },
    {
        "id": 18159,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "Richard Schuster",
        "published": "2024-1-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v2/review1"
    },
    {
        "id": 18160,
        "title": "Applications of Deep reinforcement learning in MEMS and nanotechnology",
        "authors": "Hrishitva Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) is an artificial intelligence technique that allows agents to learn optimal behaviors through trial-and-error interactions with their environment. This paper reviews applications of DRL in the fields of micro-electro-mechanical systems (MEMS) and nanotechnology. DRL has been used to enhance the design, manufacturing, and control of micro- and nanoscale systems. Notable applications include optimizing MEMS device designs, controlling nanomaterial synthesis, enabling precise nanorobotic manipulation, automating nanofabrication, directing nanoparticle self-assembly, and optimizing MEMS/nanotechnology fabrication processes. DRL allows for greater precision, increased autonomy, and enhanced performance. However, challenges remain regarding computational complexity, data availability, and responsible AI adoption. Continued DRL research and development focused on micro- and nanoscale systems hold promise for transformative innovations in electronics, medicine, energy, and other domains.\n",
        "link": "http://dx.doi.org/10.32388/pombwn"
    },
    {
        "id": 18161,
        "title": "Introduction to Deep Learning",
        "authors": "Jingqing Zhang, Hang Yuan, Hao Dong",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_1"
    },
    {
        "id": 18162,
        "title": "IoT Device Identification Using Device Fingerprint and Deep Learning",
        "authors": "Prashant Baral, Ning Yang, Ning Weng",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "The foundation of security in IoT devices lies in their identity. However, traditional identification parameters, such as MAC address, IP address, and IMEI, are vulnerable to sniffing and spoofing attacks. To address this issue, this paper proposes a novel approach using device fingerprinting and deep learning for device identification. Device fingerprinting is generated by analyzing inter-arrival time (IAT), round trip time (RTT), or IAT/RTT outliers of packets used for communication in networks. We trained deep learning models, namely convolutional neural network (CNN) and CNN + LSTM (long short-term memory), using device fingerprints generated from TCP, UDP, ICMP packet types, ICMP packet type, and their outliers. Our results show that the CNN model performs better than the CNN + LSTM model. Specifically, the CNN model achieves an accuracy of 0.97 using the IAT device fingerprint of ICMP packet type, and 0.9648 using the IAT outlier device fingerprint of ICMP packet type on a publicly available dataset from the crawdad repository.",
        "link": "http://dx.doi.org/10.5772/intechopen.111554"
    },
    {
        "id": 18163,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "Richard Schuster",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/review2"
    },
    {
        "id": 18164,
        "title": "Deep Q-Networks",
        "authors": "Yanhua Huang",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_4"
    },
    {
        "id": 18165,
        "title": "Einstieg in Deep Reinforcement Learning",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.fm"
    },
    {
        "id": 18166,
        "title": "Deep Reinforcement Learning Controller Design for Unmanned Aerial Vehicles",
        "authors": "Ali Aboubih",
        "published": "No Date",
        "citations": 0,
        "abstract": "A Proximal Policy Optimization agent was trained to learn quadrotor dynamics, successfully selecting control outputs to stabilize the drone and track complex trajectories. The agent was trained to mimic a minimum snap trajectory. The UAV closely followed the path, maintaining desired speeds of 3.56 body lengths/second, and remaining within 0.5m of the path, in wind conditions up to 20 mph. The agent was also validated on other complex trajectories, still closely tracking them regardless of the path it was trained on. Compared to PID controllers, the RL controller had a faster response time, converging to the desired path quicker. PID tuning is high maintenance and is limited by linearization around hover state. This results in instabilities and overshoots not observed in the RL controller, as well as RL learning non-linear dynamics. However, the RL controller had noisy motor output, resulting in undesirable oscillatory behaviour not observed in PID.",
        "link": "http://dx.doi.org/10.32920/25412560.v1"
    },
    {
        "id": 18167,
        "title": "Deep Reinforcement Learning Controller Design for Unmanned Aerial Vehicles",
        "authors": "Ali Aboubih",
        "published": "No Date",
        "citations": 0,
        "abstract": "A Proximal Policy Optimization agent was trained to learn quadrotor dynamics, successfully selecting control outputs to stabilize the drone and track complex trajectories. The agent was trained to mimic a minimum snap trajectory. The UAV closely followed the path, maintaining desired speeds of 3.56 body lengths/second, and remaining within 0.5m of the path, in wind conditions up to 20 mph. The agent was also validated on other complex trajectories, still closely tracking them regardless of the path it was trained on. Compared to PID controllers, the RL controller had a faster response time, converging to the desired path quicker. PID tuning is high maintenance and is limited by linearization around hover state. This results in instabilities and overshoots not observed in the RL controller, as well as RL learning non-linear dynamics. However, the RL controller had noisy motor output, resulting in undesirable oscillatory behaviour not observed in PID.",
        "link": "http://dx.doi.org/10.32920/25412560"
    },
    {
        "id": 18168,
        "title": "Policy-Based Reinforcement Learning Approaches",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_10"
    },
    {
        "id": 18169,
        "title": "Imitation Learning",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_8"
    },
    {
        "id": 18170,
        "title": "Recommender System using Reinforcement Learning: A Survey",
        "authors": "Mehrdad Rezaei, Nasseh Tabrizi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011300300003277"
    }
]