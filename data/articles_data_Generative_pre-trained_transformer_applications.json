[
    {
        "id": 20505,
        "title": "Unveiling the Thematic Landscape of Generative Pre-trained Transformer (GPT) Through Bibliometric Analysis",
        "authors": "Carlos Alberto Gómez Cano, Verenice Sánchez Castillo, Tulio Andrés Clavijo Gallego",
        "published": "2023-4-2",
        "citations": 11,
        "abstract": "Introduction: the Generative Pre-trained Transformer (GPT) is a deep learning language model architecture developed by OpenAI.\nAim: to describe the knowledge networks (both at the theoretical and country levels) of the Generative Pre-trained Transformer (GPT) as an emerging technology.\nResults: 222 Documents were identified, of which 69 were articles, 50 were conference papers, 36 were editorials, 29 were notes, 19 were letters, 14 were reviews, 3 were conference reviews, and 2 were short surveys. In terms of the number of documents per year, 2 were found in 2019, 10 in 2020, 22 in 2021, 44 in 2022, and 144 in 2023. The year-on-year growth rate was over 100% in all years. The subject area with the highest number of documents was Computer Science with 90 documents. The most productive countries in relation to GPT were the United States with 60 documents, followed by China with 19, the United Kingdom with 18, India with 15, and Australia with 12. Co-occurrence illustrated the centrality of Artificial Intelligence, Natural Language Processing, Deep Learning, and the term Human around ChatGPT and GPT.\nConclusions: this bibliometric study aimed to describe the knowledge networks of the Generative Pre-trained Transformer (GPT) as an emerging technology. Although only 222 documents were found, this study revealed a high level of international scientific collaboration in the field. The results suggest that GPT is a highly relevant technology with a wide range of potential applications in natural language processing, artificial intelligence, and deep learning.\nMoreover, the study was able to qualitatively characterize the main thematic areas surrounding GPT, including its applications in chatbots, text generation, machine translation, sentiment analysis, and more.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56294/mr202333"
    },
    {
        "id": 20506,
        "title": "Schweizerdeutsche Telefonanrufe und Meetings mit GPT (Generative Pre-trained Transformer) automatisieren",
        "authors": "Mark Bosshard",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.38023/927d5adf-652b-4477-8fe9-4ef4bee420ff"
    },
    {
        "id": 20507,
        "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
        "authors": "Minhyeok Lee",
        "published": "2023-5-25",
        "citations": 8,
        "abstract": "In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the models’ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11112451"
    },
    {
        "id": 20508,
        "title": "Performance of generative pre-trained Transformer-4 (GPT-4) in RCOG diploma-style questions",
        "authors": "Richard Armitage",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/postmj/qgae038"
    },
    {
        "id": 20509,
        "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
        "authors": "Fahim Sufi",
        "published": "2024-2-8",
        "citations": 2,
        "abstract": "GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info15020099"
    },
    {
        "id": 20510,
        "title": "ChatGPT-A Generative Pre-Trained Transformer",
        "authors": " Manisha Rajesh Gupta",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "ChatGPT is an advanced conversational AI model developed by OpenAI. It is designed to engage in natural and coherent conversations with users, providing human-like responses to a wide range of topics and questions. It leverages deep learning technique. ChatGPT uses a combination of machine learning techniques, including deep learning and natural language processing, to understand and generate human-like text. The model has been trained on a vast amount of internet text data to ensure its ability to generate relevant and contextually accurate responses. ChatGPT has applications in customer service, virtual assistants, and other conversational interfaces, offering a powerful tool for natural language understanding and generation. It is most likely used to generate human like responses and makes the communication interactive.",
        "keywords": "",
        "link": "http://dx.doi.org/10.48175/ijarsct-15087"
    },
    {
        "id": 20511,
        "title": "Generative Pre-trained Transformer (GPT) Models for Irony Detection and Classification",
        "authors": "Mustafa Ulvi Aytekin, O. Ayhan Erdem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391005"
    },
    {
        "id": 20512,
        "title": "<i>Chatbots Attempt Physics Homework—ChatGPT: Chat Generative Pre-Trained Transformer</i>",
        "authors": "Dan MacIsaac",
        "published": "2023-4-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1119/10.0017700"
    },
    {
        "id": 20513,
        "title": "Performance exploration of Generative Pre-trained Transformer-2 for lyrics generation",
        "authors": "Yijia Hu",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "In recent years, the field of Natural Language Processing (NLP) has undergone a revolution, with text generation playing a key role in this transformation. This shift is not limited to technological areas but has also seamlessly penetrated creative domains, with a prime example being the generation of song lyrics. To be truly effective, generative models, like Generative Pre-trained Transformer (GPT)-2, require fine-tuning as a crucial step. This paper, utilizing the robustness of the widely-referenced Kaggle dataset titled \"Song Lyrics\", carefully explores the impacts of modulating three key parameters: learning rate, batch size, and sequence length. The dataset presents a compelling narrative that highlights the learning rate as the most influential determinant, directly impacting the quality and coherence of the lyrics generated. While increasing the batch size and extending sequence lengths promise enhanced model performance, it is evident that there is a saturation point beyond which further benefits are limited. Through this exploration, the paper aims to demystify the complex world of model calibration and emphasize the importance of strategic parameter selection in pursuit of lyrical excellence.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/48/20241154"
    },
    {
        "id": 20514,
        "title": "The artificial intelligence pharma era after “Chat Generative Pre-trained Transformer”",
        "authors": "Zhengwei Xie, Gangqing Hu",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "Abstract\nThe era of advanced artificial intelligence has arrived with the development of chatbots like ChatGPT (Chat Generative Pre-trained Transformer). As described by Ouyang et al. (2022), ChatGPT demonstrates an impressive ability to generate human-like responses and solve practical problems, surpassing original expectations for its capabilities. The rapid release and adoption of ChatGPT signals a new phase in AI development, powered by large language models that can be fine-tuned through human feedback. However, risks remain regarding how such powerful models may be misused. Further research is needed to ensure safe and ethical deployment of these transformative technologies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1515/mr-2023-0023"
    },
    {
        "id": 20515,
        "title": "Enhancing clinical reasoning with Chat Generative Pre-trained Transformer: a practical guide",
        "authors": "Takanobu Hirosawa, Taro Shimizu",
        "published": "2024-2-19",
        "citations": 2,
        "abstract": "Abstract\n\nObjectives\nThis study aimed to elucidate effective methodologies for utilizing the generative artificial intelligence (AI) system, namely the Chat Generative Pre-trained Transformer (ChatGPT), in improving clinical reasoning abilities among clinicians.\n\n\nMethods\nWe conducted a comprehensive exploration of the capabilities of ChatGPT, emphasizing two main areas: (1) efficient utilization of ChatGPT, with a focus on application and language selection, input methodology, and output verification; and (2) specific strategies to bolster clinical reasoning using ChatGPT, including self-learning via simulated clinical case creation and engagement with published case reports.\n\n\nResults\nEffective AI-based clinical reasoning development requires a clear delineation of both system roles and user needs. All outputs from the system necessitate rigorous verification against credible medical resources. When used in self-learning scenarios, capabilities of ChatGPT in clinical case creation notably enhanced disease comprehension.\n\n\nConclusions\nThe efficient use of generative AIs, as exemplified by ChatGPT, can impressively enhance clinical reasoning among medical professionals. Adopting these cutting-edge tools promises a bright future for continuous advancements in clinicians’ diagnostic skills, heralding a transformative era in digital healthcare.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1515/dx-2023-0116"
    },
    {
        "id": 20516,
        "title": "Chatbot Generative Pre-trained Transformer and artificial intelligence in sports physical therapy and rehabilitation",
        "authors": "Mohammad Ahsan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/sjsm.sjsm_16_23"
    },
    {
        "id": 20517,
        "title": "Generative Pre‐Trained Transformer 4 in healthcare: Challenges, opportunities, and recommendations",
        "authors": "Hassam Ali",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/med4.21"
    },
    {
        "id": 20518,
        "title": "Artificial intelligence tools in medical education beyond Chat Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Li Feng Tan, Isaac K S Ng, Desmond Teo",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/postmj/qgae014"
    },
    {
        "id": 20519,
        "title": "A Case Report on Ground-Level Alternobaric Vertigo Due to Eustachian Tube Dysfunction With the Assistance of Conversational Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Hee-Young Kim",
        "published": "2023-3-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.36830"
    },
    {
        "id": 20520,
        "title": "The Impact of Chat Generative Pre-trained Transformer (ChatGPT) on Oncology: Application, Expectations, and Future Prospects",
        "authors": "Yanxing Li, Wentao Gao, Zhenhua Luan, Zhi Zhou, Jianjun Li",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.48670"
    },
    {
        "id": 20521,
        "title": "Foot and Ankle Surgery declares use of generative artificial intelligence like Chat Generative Pre-trained Transformer (ChatGPT) for scientific publications",
        "authors": "Martinus Richter",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.fas.2023.05.002"
    },
    {
        "id": 20522,
        "title": "GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)",
        "authors": "Aivin V. Solatorio",
        "published": "2023-11-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3615894.3628499"
    },
    {
        "id": 20523,
        "title": "cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific De Novo Molecular Generation",
        "authors": "Ye Wang, Honggang Zhao, Simone Sciabola, Wenlu Wang",
        "published": "2023-5-30",
        "citations": 10,
        "abstract": "Deep generative models applied to the generation of novel compounds in small-molecule drug design have attracted a lot of attention in recent years. To design compounds that interact with specific target proteins, we propose a Generative Pre-Trained Transformer (GPT)-inspired model for de novo target-specific molecular design. By implementing different keys and values for the multi-head attention conditional on a specified target, the proposed method can generate drug-like compounds both with and without a specific target. The results show that our approach (cMolGPT) is capable of generating SMILES strings that correspond to both drug-like and active compounds. Moreover, the compounds generated from the conditional model closely match the chemical space of real target-specific molecules and cover a significant portion of novel compounds. Thus, the proposed Conditional Generative Pre-Trained Transformer (cMolGPT) is a valuable tool for de novo molecule design and has the potential to accelerate the molecular optimization cycle time.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/molecules28114430"
    },
    {
        "id": 20524,
        "title": "The application of Chat Generative Pre-trained Transformer in nursing education",
        "authors": "Jialin Liu, Fan Liu, Jinbo Fang, Siru Liu",
        "published": "2023-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.outlook.2023.102064"
    },
    {
        "id": 20525,
        "title": "The Expanding Role of ChatGPT (Chat-Generative Pre-Trained Transformer) in Neurosurgery: A Systematic Review of Literature and Conceptual Framework",
        "authors": "Alex Roman, Lubna Al-Sharif, Mohamed AL Gharyani",
        "published": "2023-8-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.43502"
    },
    {
        "id": 20526,
        "title": "Analisis Sentimen AicoGPT (Generative Pre-trained Transformer) Menggunakan TF-IDF",
        "authors": " Sri Rahayu,  Jajang Jaya Purnama,  Abdul Hamid,  Nina Kurnia Hikmawati",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "Peran artificial intelligence memudahkan mencari informasi yang tepat dan akurat bahkan penyelesaian masalah dengan model yang kompleks. Salah satu terobosan berbasis AI adalah ChatGPT oleh OpenAI pada tahun 2020, dilanjutkan dengan versi terbaru pada tahun 2023 yaitu GPT–3. Sejak saat itu, beberapa teknologi AI serupa versi mobile mulai bermunculan, salah satunya AicoGPT. Namun, kinerja dari aplikasi serupa ini belum dapat diandalkan sehingga masih perlu menganalisis tanggapan para penggunanya, apakah akan sama menakjubkannya atau tidak. Dari permasalahan tersebut, penelitian ini dibuat dengan tujuan untuk menganalisis 1443 data ulasan para pengguna aplikasi AicoGPT di Google Playstore dengan teknik analisis sentimen menggunakan TFIDF dan perbandingan klasifikasi LR dan SVM. Dari kedua ujicoba tersebut, menghasilkan akurasi terbaik dengan Algoritma SVM, yaitu sebesar 92%. Sedangkan LR menghasilkan akurasi sebesar 89%. Dari penelitian ini, dapat disimpulkan secara singkat bahwa metode TF-IDF dengan klasifikasi SVM, cocok digunakan untuk melakukan analisis sentimen dari dataset yang diteliti. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.24002/jbi.v14i02.7039"
    },
    {
        "id": 20527,
        "title": "Generative Pre-Trained Transformer for Kazakh Text Generation Tasks",
        "authors": "Gulmira Tolegen, Alymzhan Toleu, Rustam Mussabayev, Bagashar Zhumazhanov, Gulzat Ziyatbekova",
        "published": "2023-8-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/opcs59592.2023.10275765"
    },
    {
        "id": 20528,
        "title": "Chat Generative Pre-Trained Transformer (ChatGPT) usage in healthcare",
        "authors": "Yanhui Zhang, Haolong Pei, Shihan Zhen, Qian Li, Fengchao Liang",
        "published": "2023-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.gande.2023.07.002"
    },
    {
        "id": 20529,
        "title": "Role of Artificial Intelligence based Chat Generative Pre-trained Transformer (ChatGPT) in Cyber Security",
        "authors": "S. Guru Prasad, V. Ceronmani Sharmila, M.K. Badrinarayanan",
        "published": "2023-5-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaaic56838.2023.10141395"
    },
    {
        "id": 20530,
        "title": "Patient Education Materials Generated by Chat Generative Pre-trained Transformer Versus Experts",
        "authors": "Hinpetch Daungsupawong, Viroj Wiwanitkit",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/sap.0000000000003783"
    },
    {
        "id": 20531,
        "title": "Writing for Pediatric Critical Care Medicine: Engaging With Citations to References in the Chatbot Generative Pre-Trained Transformer Era",
        "authors": "Robert C. Tasker",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/pcc.0000000000003356"
    },
    {
        "id": 20532,
        "title": "Universal skepticism of ChatGPT: a review of early literature on chat generative pre-trained transformer",
        "authors": "Casey Watters, Michal K. Lemanski",
        "published": "2023-8-23",
        "citations": 4,
        "abstract": "ChatGPT, a new language model developed by OpenAI, has garnered significant attention in various fields since its release. This literature review provides an overview of early ChatGPT literature across multiple disciplines, exploring its applications, limitations, and ethical considerations. The review encompasses Scopus-indexed publications from November 2022 to April 2023 and includes 156 articles related to ChatGPT. The findings reveal a predominance of negative sentiment across disciplines, though subject-specific attitudes must be considered. The review highlights the implications of ChatGPT in many fields including healthcare, raising concerns about employment opportunities and ethical considerations. While ChatGPT holds promise for improved communication, further research is needed to address its capabilities and limitations. This literature review provides insights into early research on ChatGPT, informing future investigations and practical applications of chatbot technology, as well as development and usage of generative AI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fdata.2023.1224976"
    },
    {
        "id": 20533,
        "title": "Chat generative pre-trained transformer (ChatGPT): potential implications for rheumatology practice",
        "authors": "Arvind Nune, Karthikeyan. P. Iyengar, Ciro Manzo, Bhupen Barman, Rajesh Botchu",
        "published": "2023-5-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00296-023-05340-3"
    },
    {
        "id": 20534,
        "title": "Information Retrieval Performance in Text Generation using Knowledge from Generative Pre-trained Transformer (GPT-3)",
        "authors": "Kaira Milani Fitria",
        "published": "2023-8-1",
        "citations": 1,
        "abstract": "The rise of advanced language models like GPT-3 and text generation has witnessed remarkable progress. However, leveraging the vast amount of knowledge within these models to enhance information retrieval performance remains an area that needs to be explored. This research used Artificial Intelligence, specifically the OpenAI GPT-3 language model, to create an application to help make written content. This research investigates the impact of incorporating GPT-3's knowledge into text generation processes and evaluates its influence on information retrieval tasks. Several features in text generation generate text that requires exact information, such as specifications for a product and accurate descriptions of a job or product, which are included in the concept of information retrieval in text creation by language models. The research used the few-shot learning method in the GPT-3 language model. The generated responses are then evaluated using established information retrieval metrics such as precision, recall, and F1-score. The findings of this research reveal the effectiveness of utilizing GPT-3's knowledge in enhancing information retrieval performance. The generated responses demonstrate improved relevance to user queries, resulting in the same performance precision and recall scores compared to other paid text generator websites. Application results are testing in capabilities of retrieving some information. Application capabilities tested on other commercial text generator engines. The test results obtained BERTscore 86\\% (precision), 88\\% (recall), and 87\\% (F1-Score). ",
        "keywords": "",
        "link": "http://dx.doi.org/10.34312/jjom.v5i2.20574"
    },
    {
        "id": 20535,
        "title": "Opportunities and Considerations for the Incorporation of Artificial Intelligence into Global Neurosurgery: A Generative Pre-Trained Transformer Chatbot-Based Approach",
        "authors": "Nathan A. Shlobin, Gail Rosseau",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.wneu.2024.03.149"
    },
    {
        "id": 20536,
        "title": "Nursing Education in the Age of Chat Generative Pre-Trained Transformer: Current Roles and Future Perspective",
        "authors": "Bhavna Rani, Saumya Prakash Srivastava, Shafali Thakur",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "Abstract\nChat Generative Pre-trained Transformer (ChatGPT), an artificial intelligence-powered chatbot, has received a lot of interest from the academic community since its inception. Health-care sector and higher education has significantly advanced with the use of AI technologies. With the advent of AI technologies, such as ChatGPT, the future of nursing education is poised for significant transformation. In this article, we will explore the potential impact of ChatGPT on nursing education, discussing its benefits, challenges, and implications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/amhs.amhs_208_23"
    },
    {
        "id": 20537,
        "title": "To chat or not to chat, Generative Pre-trained Transformer?",
        "authors": "PrakashK Dubey",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/jigims.jigims_32_23"
    },
    {
        "id": 20538,
        "title": "Chat Generative Pre-Trained Transformer (ChatGPT): Comprehending its Operational Structure, AI Techniques, Working, Features and Limitations",
        "authors": "Ishita Naik, Dishita Naik, Nitin Naik",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictbig59752.2023.10456201"
    },
    {
        "id": 20539,
        "title": "Exploring the Potential and Limitations of Chat Generative Pre-trained Transformer (ChatGPT) in Generating Board-Style Dermatology Questions: A Qualitative Analysis",
        "authors": "Ibraheim Ayub, Dathan Hamann, Carsten R Hamann, Matthew J Davis",
        "published": "2023-8-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.43717"
    },
    {
        "id": 20540,
        "title": "The impact of Chat Generative Pre-trained Transformer (ChatGPT) on medical education",
        "authors": "Jonathan J Y Heng, Desmond B Teo, L F Tan",
        "published": "2023-9-21",
        "citations": 12,
        "abstract": "Abstract\nArtificial intelligence (AI) in medicine is developing rapidly. The advent of Chat Generative Pre-trained Transformer (ChatGPT) has taken the world by storm with its potential uses and efficiencies. However, technology leaders, researchers, educators, and policy makers have also sounded the alarm on its potential harms and unintended consequences. AI will increasingly find its way into medicine and is a force of both disruption and innovation. We discuss the potential benefits and limitations of this new league of technology and how medical educators have to develop skills and curricula to best harness this innovative power.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/postmj/qgad058"
    },
    {
        "id": 20541,
        "title": "Treatment Outcomes of Leiomyosarcoma Metastasis Affecting the Brachial Plexus: A Comparative Case Report Using Chat Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Aroosa Zamarud, Neelan Marianayagam, Vashisht Sekar, Steven D. Chang, Antonio Meola",
        "published": "2023-3-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.36715"
    },
    {
        "id": 20542,
        "title": "ShellGPT: Generative Pre-trained Transformer Model for Shell Language Understanding",
        "authors": "Jie Shi, Sihang Jiang, Bo Xu, Jiaqing Liang, Yanghua Xiao, Wei Wang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/issre59848.2023.00082"
    },
    {
        "id": 20543,
        "title": "Generative Pre-trained Transformer (GPT) based model with relative attention for de novo drug design",
        "authors": "Suhail Haroon, Hafsath C.A., Jereesh A.S.",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compbiolchem.2023.107911"
    },
    {
        "id": 20544,
        "title": "Role of chat-generative pre-trained transformer (ChatGPT) in anaesthesia: Merits and pitfalls",
        "authors": "Ashwini Reddy, Swati Patel, Amiya Kumar Barik, Punith Gowda",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/ija.ija_504_23"
    },
    {
        "id": 20545,
        "title": "MetaQA: Enhancing human-centered data search using Generative Pre-trained Transformer (GPT) language model and artificial intelligence",
        "authors": "Diya Li, Zhe Zhang",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "Accessing and utilizing geospatial data from various sources is essential for developing scientific research to address complex scientific and societal challenges that require interdisciplinary knowledge. The traditional keyword-based geosearch approach is insufficient due to the uncertainty inherent within spatial information and how it is presented in the data-sharing platform. For instance, the Gulf of Mexico Coastal Ocean Observing System (GCOOS) data search platform stores geoinformation and metadata in a complex tabular. Users can search for data by entering keywords or selecting data from a drop-down manual from the user interface. However, the search results provide limited information about the data product, where detailed descriptions, potential use, and relationship with other data products are still missing. Language models (LMs) have demonstrated great potential in tasks like question answering, sentiment analysis, text classification, and machine translation. However, they struggle when dealing with metadata represented in tabular format. To overcome these challenges, we developed Meta Question Answering System (MetaQA), a novel spatial data search model. MetaQA integrates end-to-end AI models with a generative pre-trained transformer (GPT) to enhance geosearch services. Using GCOOS metadata as a case study, we tested the effectiveness of MetaQA. The results revealed that MetaQA outperforms state-of-the-art question-answering models in handling tabular metadata, underlining its potential for user-inspired geosearch services.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1371/journal.pone.0293034"
    },
    {
        "id": 20546,
        "title": "#242 Designing a clinical trial with chat generative pre-trained transformer (ChatGPT) on the role of hyperthermic intraperitoneal chemotherapy (HIPEC) in ovarian cancer",
        "authors": "Esra Bilir, Ilker Kahramanoglu",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1136/ijgc-2023-esgo.541"
    },
    {
        "id": 20547,
        "title": "Applying the Diamond Model of Intrusion Analysis with Generative Pre-trained Transformer 3",
        "authors": "Sheng-Shan Chen, Tun-Wen Pai, Chin-Yu Sun",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce-taiwan58799.2023.10226923"
    },
    {
        "id": 20548,
        "title": "The Role of Generative Pre-trained Transformers (GPT) in Recreational Tourism: An Interview with ChatGPT",
        "authors": "Okan ÇOLAK",
        "published": "2023-10-31",
        "citations": 3,
        "abstract": "This study explores the potential benefits of employing generative pre-trained transformers (GPTs) in recreational tourism, the difficulties that arise when implementing such technology, the impact it has on tourist behaviour, and the ways it can be utilised in recreational tourism management. The original aspect of the study is that it is the first to give detailed information about the use of GPT in recreational tourism. ChatGPT was used as an interviewer in the study. ChatGPT is a software application that utilizes the high-powered machine learning software called Generative Pre-trained Transformer (GPT-3), developed by the OpenAI organization. Six questions were posed on the ChatGPT query screen (https://chat.openai.com/chat). The interview queries were prepared with reference to the study by Fusté-Forné and Orea-Giner (2023). The question statements in this study on the use of GPT in gastronomy tourism were changed to recreational tourism. ChatGPT's replies were tabulated and presented descriptively. Inferences and suggestions were made in line with the answers given by ChatGPT. The study showed that using GPT technology in recreational-based tourism can offer better customer interaction, decision-making, and a personalized travel experience. ChatGPT underlined that new technologies continue to evolve, and recreational-based tourism will become more personalized, informative, and immersive, ultimately enhancing the overall travel experience, and contributing to the growth and sustainability of the tourism industry. ChatGPT also noted GPT models have the potential to shape and enhance the entire travel experience for tourists, from the initial planning stages to the actual trip itself. Although GPT makes significant contributions to recreational tourism management and recreational tourists, there are deficiencies in ethical, privacy, and authenticity concerns.",
        "keywords": "",
        "link": "http://dx.doi.org/10.25307/jssr.1341967"
    },
    {
        "id": 20549,
        "title": "Performance of Generative Pre-trained Transformer-4 (GPT-4) in Membership of the Royal College of General Practitioners (MRCGP)-style examination questions",
        "authors": "Richard C Armitage",
        "published": "2024-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/postmj/qgad128"
    },
    {
        "id": 20550,
        "title": "Exploring the adoption of the metaverse and chat generative pre-trained transformer: A single-valued neutrosophic Dombi Bonferroni-based method for the selection of software development strategies",
        "authors": "Abdullah Önden, Karahan Kara, İsmail Önden, Galip Cihan Yalçın, Vladimir Simic, Dragan Pamucar",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108378"
    },
    {
        "id": 20551,
        "title": "AEJMC Advertising Division 2023 Teaching Pre-Conference Review: Innovating Data Storytelling and Visualization With Artificial Intelligence and Chat Generative Pre-trained Transformer",
        "authors": "Robin Spring, Shanshan Lou",
        "published": "2024-5",
        "citations": 0,
        "abstract": " The 26th annual Teaching Pre-Conference organized by the Advertising Division of the Association for Education in Journalism and Mass Communication focused on the topic of innovating data storytelling and visualization with AI and ChatGPT. Five prominent speakers from leading media companies and universities shared insights with advertising educators, covering the application, impact, and challenges of generative artificial intelligence (AI) in the advertising industry. The five panels also delved into effective ways of integrating generative AI tools into the classroom. Three key trends that arise from the panel presentations are discussed below. Relevant advertising AI tools and class activities are also shared in the report. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10980482241236883"
    },
    {
        "id": 20552,
        "title": "ANALISIS SENTIMEN PADA ULASAN APLIKASI CHAT  GENERATIVE PRE-TRAINED TRANSFORMER GPT MENGGUNAKAN METODE KLASIFIKASI K-NEAREST NEIGHBOR(KNN)",
        "authors": "Muhammad Nanda Fahriza, Noviana Riza",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "Klasifikasi K-Nearest Neighbor (KNN) akan dipakai dalam penelitian ini untuk melakukan analisis sentimen terhadap ulasan pengguna terhadap aplikasi Chat Generative Pre-Trained Transformer GPT. Model bahasa reguler GPT telah dipersiapkan secara luas dan dapat menghasilkan teks yang dapat dimengerti dan signifikan. Bagaimanapun, untuk memahami reaksi klien terhadap aplikasi, diperlukan pemeriksaan opini atas audit. Dalam penelitian ini, hasil analisis sentimen akan memberikan pemahaman yang lebih mendalam tentang respon pengguna terhadap aplikasi Chat Generative Pre-Trained Transformer GPT. Informasi ini dapat berguna bagi pengembang dan pemilik aplikasi untuk memahami kelebihan dan kekurangan aplikasi serta meningkatkan pengalaman pengguna secara keseluruhan. Penelitian ini ingin mengidentifikasi penelitian terkait sebelumnya, membandingkan pendekatan dan teknik yang digunakan, menganalisis efektivitas metode KNN, mengidentifikasi kelebihan dan kekurangan metode tersebut, serta memberikan rekomendasi untuk pengembangan lebih lanjut. Diharapkan hasil penelitian ini dapat memberikan pemahaman yang mendalam tentang respon pengguna terhadap aplikasi tersebut dan memberikan sumbangan bagi pengembangan dan peningkatan pengalaman pengguna.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36040/jati.v7i2.6767"
    },
    {
        "id": 20553,
        "title": "SoftGPT: Learn Goal-Oriented Soft Object Manipulation Skills by Generative Pre-Trained Heterogeneous Graph Transformer",
        "authors": "Junjia Liu, Zhihao Li, Wanyu Lin, Sylvain Calinon, Kay Chen Tan, Fei Chen",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341846"
    },
    {
        "id": 20554,
        "title": "Feel the Market: An Attempt to Identify Additional Factor in the Capital Asset Pricing Model (CAPM) Using Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Christopher Lingwei Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4521946"
    },
    {
        "id": 20555,
        "title": "Industrial‐generative pre‐trained transformer for intelligent manufacturing systems",
        "authors": "Han Wang, Min Liu, Weiming Shen",
        "published": "2023-6",
        "citations": 6,
        "abstract": "AbstractManufacturing enterprises are facing how to utilise industrial knowledge and continuously accumulating massive unlabelled data to achieve human‐cyber‐physical collaborative and autonomous intelligence. Recently, artificial intelligence‐generative content has achieved great performance in several domains and scenarios. A new concept of industrial generative pre‐trained Transformer (Industrial‐GPT) for intelligent manufacturing systems is introduced to solve various scenario tasks. It refers to pre‐training with industrial datasets, fine‐tuning with industrial scenarios, and reinforcement learning with domain knowledge. To enable Industrial‐GPT to better empower the manufacturing industry, Model as a Service is introduced to cloud computing as a new service mode, which provides a more efficient and flexible service approach by directly invoking the general model of the upper layer and customising it for specific businesses. Then, the operation mechanism of the Industrial‐GPT driven intelligent manufacturing system is described. Finally, the challenges and prospects of applying the Industrial‐GPT in the manufacturing industry are discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cim2.12078"
    },
    {
        "id": 20556,
        "title": "Microcreativity with chat generative pre-trained transformer: Learnings in virtual space",
        "authors": "Lia Machado Fiuza Fialho, Vanusa Nascimento Sabino Neves, Karla Angélica Silva do Nascimento",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "The Chat Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence technology that engages in dialogue with humans, capable of generating formulations in the form of micro-narratives that can be problematized as a learning tool in the virtual space. This research aims to understand how ChatGPT can be used in teacher training as a didactic tool to facilitate learning in the virtual space through micro-narratives. We conducted a qualitative study using an action research approach with Brazilian students from the Graduate Program at the State University of Ceará in Brazil. The research consisted of five phases: diagnosis, which involved a mixed questionnaire to assess prior knowledge about generating micro-narratives using ChatGPT; action planning, which involved developing a training plan; implementation of the action, which included practicing the use of ChatGPT to generate micro-narratives and working with them in a didactic perspective in the virtual space; evaluation, which involved sharing the micro-narratives and engaging in circular discussions about them; and learning, which involved documenting the educational possibilities and limitations of micro-narratives. The results, processed using IRaMuTeQ, showed limited prior knowledge about ChatGPT and the importance of micro-narratives for educational work, as well as their fruitful pedagogical use for learning in the virtual space through conscious utilization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3926/jotse.2338"
    },
    {
        "id": 20557,
        "title": "Pengenalan Web Ai Chatgpt (Generative Pre-Trained Transformer) Oleh Openai Di Smp Indriasana Palembang",
        "authors": "Alwin Marcellino, Dicky Ryanto Fernandes, Fionna Caroline, Nicolas Jacky Pratama Hasan, Yosefa Camilia Moniung, Muhammad Rizky Pribadi",
        "published": "2023-7-20",
        "citations": 0,
        "abstract": "ChatGPT oleh OpenAI merupakan contoh teknologi Kecerdasan Buatan (Artificial Intelligence) yang dapat berinteraksi dengan pengguna melalui chat. ChatGPT seringkali terdengar di kalangan masyarakat saat ini, maka dari itu dilakukan pengabdian mengenai ChatGPT kepada siswa-siswi SMP Indriasana Palembang. Kegiatan pengabdian ini bertujuan untuk mengenalkan siswa-siswi pada ChatGPT sebagai asisten virtual yang dapat memudahkan kehidupan sehari-hari dalam memperoleh informasi, menjelaskan cara penggunaan ChatGPT, kekurangan dan kelebihannya, serta berbagai pengawasan yang harus dilakukan guna mencegah penyalahgunaan teknologi tersebut. Kegiatan pengabdian ini dilakukan dengan metode penyampaian materi yang interaktif dan pelatihan menggunakan platform ChatGPT kepada siswa-siswi yang didampingi langsung oleh Tim Pengabdian. Penggunaan ChatGPT dalam kehidupan sehari-hari memiliki potensi besar untuk meningkatkan pelayanan dan memberikan manfaat bagi masyarakat dari berbagai kalangan usia.Kata kunci: Artificial Intelligence, Asisten Virtual, ChatGPT, OpenAI, Teknologi.",
        "keywords": "",
        "link": "http://dx.doi.org/10.53513/abdi.v3i2.8351"
    },
    {
        "id": 20558,
        "title": "Rédiger un éditorial pour SMQ en 2024 lorsque l’on est directeur de la revue, fainéant et fripon : merci Chat Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Emmanuel Stip",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7202/1109831ar"
    },
    {
        "id": 20559,
        "title": "A Program Based on Chat Generative Pre-trained Text Transformer (ChatGPT) for Enhancing EFL Majors' Descriptive Paragraph Writing Skills and Their English Grammar Use",
        "authors": "Haggag Mohamed Haggag",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21608/mfes.2023.315599"
    },
    {
        "id": 20560,
        "title": "Evolutionary Game Analysis of Artificial Intelligence Such as the Generative Pre-Trained Transformer in Future Education",
        "authors": "Yanwei You, Yuquan Chen, Yujun You, Qi Zhang, Qiang Cao",
        "published": "2023-6-9",
        "citations": 11,
        "abstract": "As an emerging research area since generative artificial intelligence (represented by Chat Generative Pre-trained Transformer (ChatGPT)) has been accessible to the public, especially in education, appropriate AI application could bring numerous benefits to education; however, its abuse has the potential to be harmful. In this paper, we aimed to explore the potential of AI in the future of education with the analytical method of evolutionary game analysis (EGA). By studying the behavior of two agents, the school and the students, EGA can be used to identify strategies that can be used to improve the effectiveness of the education model in the context of the AI era. A stable evolutionary strategy for the school and students was devised under a variety of scenarios. Additionally, we conducted a numerical analysis to further explore the impact of several key factors on the stable strategy. The results indicated that schools should adopt positive supervision to standardize the use of AI in education, and students should be more active in becoming involved in AI technology. Based on this study, we believe that the school has the ability to provide effective suggestions and practical guidelines to help students succeed academically and embrace future trends in AI education.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su15129355"
    },
    {
        "id": 20561,
        "title": "GPT (Generative Pre-trained Transformer) – A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
        "authors": "Gokul Yenduri, M Ramalingam, G Chemmalar Selvi, Y Supriya, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, G Deepti Raj, Rutvij H Jhaveri, B Prabadevi, Weizheng Wang, Athanasios V. Vasilakos, Thippa Reddy Gadekallu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3389497"
    },
    {
        "id": 20562,
        "title": "On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs",
        "authors": "Jarrod Ragsdale, Rajendra V. Boppana",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3326104"
    },
    {
        "id": 20563,
        "title": "Enhancing catalysis studies with chat generative pre-trained transformer (ChatGPT): Conversation with ChatGPT",
        "authors": "Navid Ansari, Vahid Babaei, Mohammad Mahdi Najafpour",
        "published": "2024",
        "citations": 0,
        "abstract": "This study describes the integration of generative pre-trained transformer and similar large language models in catalysis research, highlighting their potential to revolutionize understanding and innovation in oxygen-evolution reaction catalysts.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1039/d3dt04178f"
    },
    {
        "id": 20564,
        "title": "Scope of homoeopathic research based on Chat (Generative Pre-trained Transformer) GPT: An Artificial Intelligence (AI) approach.",
        "authors": "Aditya Dilipkumar Patil, Monali Thopte, Sargam Singh",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.51910/ijhdr.v23icf.1380"
    },
    {
        "id": 20565,
        "title": "GPT-LS: Generative Pre-Trained Transformer with Offline Reinforcement Learning for Logic Synthesis",
        "authors": "Chenyang Lv, Ziling Wei, Weikang Qian, Junjie Ye, Chang Feng, Zhezhi He",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccd58817.2023.00056"
    },
    {
        "id": 20566,
        "title": "The role of chat generative pre-trained transformer in facilitating decision-making and the e-learning process in higher education",
        "authors": "Khaldun G. Al-Moghrabi, Ali M. Al-Ghonmein",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "Digital technology and artificial intelligence technologies have been progressing rapidly, thus giving rise to intelligent chatbots such as chat generative pre-trained transformer (ChatGPT). These chatbots make searching for information more efficient and provide higher education institutions with assistance in decision-making. The goal of this research is to explore the capabilities of ChatGPT technology and its role in enhancing the e-learning process. Moreover, it seeks to determine whether ChatGPT can provide useful suggestions to improve the decision-making process in higher education. ChatGPT is effective in an e-learning environment for the following reasons: it facilitates personalized learning experiences, offers real-time support, and enhances decision-making by leveraging natural language processing capabilities. As suggested by the findings, ChatGPT has significant potential in higher education, as demonstrated by its ability to improve interactive participation, educational strategies, and educational outcomes. This study highlights the importance of incorporating ChatGPT into higher education settings to improve e-learning and decision-making.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/eei.v13i3.7237"
    },
    {
        "id": 20567,
        "title": "Artificial intelligence in orthopaedics: can Chat Generative Pre-trained Transformer (ChatGPT) pass Section 1 of the Fellowship of the Royal College of Surgeons (Trauma &amp; Orthopaedics) examination?",
        "authors": "Rory Cuthbert, Ashley I Simpson",
        "published": "2023-9-21",
        "citations": 7,
        "abstract": "Abstract\n\nPurpose\nChat Generative Pre-trained Transformer (ChatGPT) is a large language artificial intelligence (AI) model which generates contextually relevant text in response to questioning. After ChatGPT successfully passed the United States Medical Licensing Examinations, proponents have argued it should play an increasing role in medical service provision and education. AI in healthcare remains in its infancy, and the reliability of AI systems must be scrutinized. This study assessed whether ChatGPT could pass Section 1 of the Fellowship of the Royal College of Surgeons (FRCS) examination in Trauma and Orthopaedic Surgery.\n\n\nMethods\nThe UK and Ireland In-Training Examination (UKITE) was used as a surrogate for the FRCS. Papers 1 and 2 of UKITE 2022 were directly inputted into ChatGPT. All questions were in a single-best-answer format without wording alterations. Imaging was trialled to ensure ChatGPT utilized this information.\n\n\nResults\nChatGPT scored 35.8%: 30% lower than the FRCS pass rate and 8.2% lower than the mean score achieved by human candidates of all training levels. Subspecialty analysis demonstrated ChatGPT scored highest in basic science (53.3%) and lowest in trauma (0%). In 87 questions answered incorrectly, ChatGPT only stated it did not know the answer once and gave incorrect explanatory answers for the remaining questions.\n\n\nConclusion\nChatGPT is currently unable to exert the higher-order judgement and multilogical thinking required to pass the FRCS examination. Further, the current model fails to recognize its own limitations. ChatGPT’s deficiencies should be publicized equally as much as its successes to ensure clinicians remain aware of its fallibility.\n\n\nKey messages\n\nWhat is already known on this topic\nFollowing ChatGPT’s much-publicized success in passing the United States Medical Licensing Examinations, clinicians and medical students are using the model increasingly frequently for medical service provision and education. However ChatGPT remains in its infancy, and the model’s reliability and accuracy remain unproven.\n\n\nWhat this study adds\nThis study demonstrates ChatGPT is currently unable to exert the higher-order judgement and multilogical thinking required to pass the Fellowship of the Royal College of Surgeons (FRCS) (Trauma & Orthopaedics) examination. Further, the current model fails to recognize its own limitations when offering both direct and explanatory answers.\n\n\nHow this study might affect research, practice, or policy\nThis study highlights the need for medical students and clinicians to exert caution when employing ChatGPT as a revision tool or applying it in clinical practice, and for patients to be aware of its fallibilities when using it as a health resource. Future research questions include:\n\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/postmj/qgad053"
    },
    {
        "id": 20568,
        "title": "How We Learned to Stop Worrying and Love AI: Analyzing the Rapid Evolution of Generative Pre-Trained Transformer (GPT) and its Impacts on Law, Business, and Society",
        "authors": "Scott J. Shackelford, Lawrence J. Trautman, W. Gregory Voss",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4516154"
    },
    {
        "id": 20569,
        "title": "Blepharoptosis Consultation with Artificial Intelligence: Aesthetic Surgery Advice and Counseling from Chat Generative Pre-Trained Transformer (ChatGPT)",
        "authors": "Makoto Shiraishi, Koji Tanigawa, Yoko Tomioka, Ami Miyakuni, Yuta Moriwaki, Rui Yang, Jun Oba, Mutsumi Okazaki",
        "published": "2024-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00266-024-04002-4"
    },
    {
        "id": 20570,
        "title": "Chat Generative Pre-trained Transformer: why we should embrace this technology",
        "authors": "Martin R. Chavez, Thomas S. Butler, Patricia Rekawek, Hye Heo, Wendy L. Kinzler",
        "published": "2023-6",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ajog.2023.03.010"
    },
    {
        "id": 20571,
        "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
        "authors": "S. B. Atallah, N. R. Banda, A. Banda, N. A. Roeck",
        "published": "2023-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10151-023-02837-8"
    },
    {
        "id": 20572,
        "title": "A Program Based on Chat Generative Pre-trained Text Transformer (ChatGPT) for Enhancing EFL Majors&amp;#039; Descriptive Paragraph Writing Skills and Their English Grammar Use",
        "authors": "حجاج حجاج",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21608/mfes.2023.222875.1594"
    },
    {
        "id": 20573,
        "title": "Bridging artificial intelligence in medicine with generative pre-trained transformer (GPT) technology",
        "authors": "Ethan Waisberg, Joshua Ong, Sharif Amit Kamran, Mouayad Masalkhi, Nasif Zaman, Prithul Sarker, Andrew G. Lee, Alireza Tavakkoli",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21037/jmai-23-36"
    },
    {
        "id": 20574,
        "title": "MSR92 Can Artificial Intelligence (AI) Large Language Models (LLMS) Such as Generative Pre-Trained Transformer (GPT) Be Used to Automate Literature Reviews?",
        "authors": "I. Guerra, J. Gallinaro, K. Rtveladze, A. Lambova, E. Asenova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jval.2023.09.2151"
    },
    {
        "id": 20575,
        "title": "Investigating antiquities trafficking with generative pre-trained transformer (GPT)-3 enabled knowledge graphs: A case study",
        "authors": "Shawn Graham, Donna Yates, Ahmed El-Roby",
        "published": "2023-6-20",
        "citations": 2,
        "abstract": "Background: There is a wide variety of potential sources from which insight into the antiquities trade could be culled, from newspaper articles to auction catalogues, to court dockets, to personal archives, if it could all be systematically examined. We explore the use of a large language model, GPT-3, to semi-automate the creation of a knowledge graph of a body of scholarship concerning the antiquities trade. Methods: We give GPT-3 a prompt guiding it to identify knowledge statements around the trade. Given GPT-3’s understanding of the statistical properties of language, our prompt teaches GPT-3 to append text to each article we feed it where the appended text summarizes the knowledge in the article. The summary is in the form of a list of subject, predicate, and object relationships, representing a knowledge graph. Previously we created such lists by manually annotating the source articles. We compare the result of this automatic process with a knowledge graph created from the same sources via hand. When such knowledge graphs are projected into a multi-dimensional embedding model using a neural network (via the Ampligraph open-source Python library), the relative positioning of entities implies the probability of a connection; the direction of the positioning implies the kind of connection. Thus, we can interrogate the embedding model to discover new probable relationships. The results can generate new insight about the antiquity trade, suggesting possible avenues of research. Results: We find that our semi-automatic approach to generating the knowledge graph in the first place produces comparable results to our hand-made version, but at an enormous savings of time and a possible expansion of the amount of materials we can consider. Conclusions: These results have implications for working with other kinds of archaeological knowledge in grey literature, reports, articles, and other venues via computational means.",
        "keywords": "",
        "link": "http://dx.doi.org/10.12688/openreseurope.16003.1"
    },
    {
        "id": 20576,
        "title": "Automated ophthalmic imaging analysis in the era of Generative Pre-Trained Transformer-4",
        "authors": "Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, SharifAmit Kamran, Nasif Zaman, Prithul Sarker, AndrewG Lee, Alireza Tavakkoli",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/pajo.pajo_62_23"
    },
    {
        "id": 20577,
        "title": "Alternative Approaches to HVAC Control of Chat Generative Pre-Trained Transformer (ChatGPT) for Autonomous Building System Operations",
        "authors": "Ki Uhn Ahn, Deuk-Woo Kim, Hyun Mi Cho, Chang-U Chae",
        "published": "2023-10-24",
        "citations": 1,
        "abstract": "Artificial intelligence (AI) technology has rapidly advanced and transformed the nature of scientific inquiry. The recent release of the large language model Chat Generative Pre-Trained Transformer (ChatGPT) has attracted significant attention from the public and various industries. This study applied ChatGPT to autonomous building system operations, specifically coupling it with an EnergyPlus reference office building simulation model. The operational objective was to minimize the energy use of the building systems, including four air-handling units, two chillers, a cooling tower, and two pumps, while ensuring that indoor CO2 concentrations remain below 1000 ppm. The performance of ChatGPT in an autonomous operation was compared with control results based on a deep Q-network (DQN), which is a reinforcement learning method. The ChatGPT and DQN lowered the total energy use by 16.8% and 24.1%, respectively, compared with the baseline operation, while maintaining an indoor CO2 concentration below 1000 ppm. Notably, compared with the DQN, ChatGPT-based control does not require a learning process to develop intelligence for building control. In real-world applications, the high generalization capabilities of the ChatGPT-based control, resulting from its extensive training on vast and diverse data, could potentially make it more effective.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/buildings13112680"
    },
    {
        "id": 20578,
        "title": "Perception of Chat Generative Pre-trained Transformer (Chat-GPT) AI tool amongst MSK clinicians",
        "authors": "Karthikeyan. P. Iyengar, Mina Malak Abed Yousef, Arvind Nune, Gaurav Kant Sharma, Rajesh Botchu",
        "published": "2023-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jcot.2023.102253"
    },
    {
        "id": 20579,
        "title": "Evaluating the performance of Generative Pre-trained Transformer-4 (GPT-4) in standardizing radiology reports",
        "authors": "Amir M. Hasani, Shiva Singh, Aryan Zahergivar, Beth Ryan, Daniel Nethala, Gabriela Bravomontenegro, Neil Mendhiratta, Mark Ball, Faraz Farhadi, Ashkan Malayeri",
        "published": "2023-11-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00330-023-10384-x"
    },
    {
        "id": 20580,
        "title": "Atypical Nelson Syndrome Following Right Partial and Left Total Nephrectomy With Incidental Bilateral Total Adrenalectomy of Renal Cell Carcinoma: A Chat Generative Pre-Trained Transformer (ChatGPT)-Assisted Case Report and Literature Review",
        "authors": "Kyle Schuppe, Skyler Burke, Blake Cohoe, Kevin Chang, Raymond S Lance, Henry Mroch",
        "published": "2023-3-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.36042"
    },
    {
        "id": 20581,
        "title": "Software Defect Prediction via Generative Adversarial Networks and Pre-Trained Model",
        "authors": "Wei Song, Lu Gan, Tie Bao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.01503119"
    },
    {
        "id": 20582,
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": "Liwen Hou, David Smith",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.230"
    },
    {
        "id": 20583,
        "title": "Generative Pre-Trained Transformer-Based Reinforcement Learning for Testing Web Application Firewalls",
        "authors": "Hongliang Liang, Xiangyu Li, Da Xiao, Jie Liu, Yanjie Zhou, Aibo Wang, Jin Li",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tdsc.2023.3252523"
    },
    {
        "id": 20584,
        "title": "P-288 Chat Generative Pre-trained Transformer (ChatGPT) Proves to be an Effective Assistant for Clinical Embryologists in Laboratory Tasks: A Pilot Cross-sectional Study",
        "authors": "F Choucair, H Burjaq, A I Rahim, O Atilan, N Younis, A Al Hourani, G Raad",
        "published": "2023-6-22",
        "citations": 0,
        "abstract": "Abstract\n\nStudy question\nWhat are the capabilities of ChatGPT in troubleshooting, fact-checking and generating report templates in the in vitro fertilization (IVF) laboratory?\n\n\nSummary answer\nClinical embryologists perceived ChatGPT as accurate and comprehensive in troubleshooting, generating standard operating procedures (SOP), writing reports and fact-finding in the IVF laboratory.\n\n\nWhat is known already\nChatGPT is an artificial intelligence (AI)-driven chat robot (chatbot) with 175 billion parameters in its natural language processing model. It is remarkable for its concise, human-like answers to user inquiries. With advanced AI technology, ChatGPT provides in-depth responses, handles complex problems, and addresses intricate questions. This chatbot has received significant recognition and is anticipated to encourage users to employ it for practical applications, including in the IVF laboratory. However, the abilities of ChatGPT in executing various tasks (such as troubleshooting, generating SOPs) in the in vitro fertilization (IVF) laboratory have not been investigated yet.\n\n\nStudy design, size, duration\nThe aim of this cross-sectional study is to assess the proficiency of ChatGPT in four tasks commonly performed by embryologists: troubleshooting, designing SOPs, composing reports, and fact-checking. An anonymous online survey of clinical embryologists (n = 40) was conducted to achieve this aim. It was performed between December 2022 and January 2023.\n\n\nParticipants/materials, setting, methods\nClinical embryologists participated in a five-point Likert scale (1-Very disagree to 5-Very agree) questionnaire. Participants were presented with eight vignettes generated by ChatGPT related to the investigated four tasks. Embryologists provide ratings about both their perceived accuracy and perceived completeness of the provided answers. We then asked about their intention to incorporate ChatGPT into their daily tasks.\n\n\nMain results and the role of chance\nThe median years of experience of survey participants was 11.5 years (IQR 8-18). Among the participants, 37.5% held an undergraduate bachelor's degree, while the largest proportion, 62.5%, held graduate degrees (Masters and doctorates). Embryologists rated ChatGPT as having an accurate (mean Likert score of 3.45) and comprehensive (mean Likert score of 3.36) value for troubleshooting. They also considered it to be accurate (mean Likert score of 3.34) and comprehensive (mean Likert score of 3.26) for writing SOP templates. Furthermore, they found ChatGPT to have an accurate (mean Likert score of 3.71) and comprehensive (mean Likert score of 3.66) value for report writing. Additionally, they deemed ChatGPT to have both an accurate and comprehensive value (mean Likert score of 3.67) for verifying facts. Overall, experienced embryologists perceived an added value of ChatGPT (Likert scores ≥3). On average, they rated ChatGPT as accurate (average score of 3.54) and comprehensive (average score of 3.49) in all tasks tested. They also expressed an intention to use ChatGPT in their laboratory work (average Likert, 3.58).\n\n\nLimitations, reasons for caution\nThe embryologists who participated in the survey were highly experienced.\n\n\nWider implications of the findings\nThe model has the potential to assist clinical embryologists in resolving issues and performing administrative duties, making it a valuable resource. Embryologists may benefit from integrating ChatGPT into their educational and certification processes, as well as their daily tasks.\n\n\nTrial registration number\nNot applicable\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/humrep/dead093.646"
    },
    {
        "id": 20585,
        "title": "Improve Information Service Capabilities from Content Aggregation to Knowledge Provision with Generative Pre-trained Transformer (GPT)",
        "authors": "Li Yu, Pei Bohao, Yu Qiang, Zhang Wei",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/snpd-winter57765.2023.10223888"
    },
    {
        "id": 20586,
        "title": "Medical image Generative Pre-Trained Transformer (MI-GPT): future direction for precision medicine",
        "authors": "Xiaohui Zhang, Yan Zhong, Chentao Jin, Daoyan Hu, Mei Tian, Hong Zhang",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00259-023-06450-7"
    },
    {
        "id": 20587,
        "title": "Generative pre-trained transformers (GPT) for surface engineering",
        "authors": "Spyros Kamnis",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.surfcoat.2023.129680"
    },
    {
        "id": 20588,
        "title": "Performance of chat generative pre-trained transformer (ChatGPT) on personal review of learning in obstetrics and gynecology",
        "authors": "A Cohen, J Burns, M Gabra, A Gordon, N Deebel, R Terlecki, K Woodburn",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ajog.2024.02.051"
    },
    {
        "id": 20589,
        "title": "Revolutionizing Translation with AI: Unravelling Neural Machine Translation and Generative Pre-Trained Large Language Models",
        "authors": "Sai Cheong Siu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499768"
    },
    {
        "id": 20590,
        "title": "The GPT-Comparator: Discovering and Reporting Spatial and Topical Biases in Generative Pre-Trained Transformers",
        "authors": "Sriram Elango, Kokil Jaidka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4410154"
    },
    {
        "id": 20591,
        "title": "Chat Generative Pre-trained Transformer–written obstetrics and gynecology abstracts fool practitioners",
        "authors": "Gabriel Levin, Raanan Meyer, Amber Yasmeen, Bowen Yang, Paul-Adrien Guigue, Tomer Bar-noy, Angela Tatar, Olga Perelshtein Brezinov, Yoav Brezinov",
        "published": "2023-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ajogmf.2023.100993"
    },
    {
        "id": 20592,
        "title": "Emotion Classification using Generative Pre-trained Embedding and Machine Learning",
        "authors": "Geeta Pattun, Pradeep Kumar",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmlant59547.2023.10372980"
    },
    {
        "id": 20593,
        "title": "Comparison of Patient Education Materials Generated by Chat Generative Pre-Trained Transformer Versus Experts",
        "authors": "Ya-Ching Hung, Sara C. Chaker, Matthew Sigel, Mariam Saad, Elizabeth D. Slater",
        "published": "2023-10",
        "citations": 10,
        "abstract": "\nIntroduction\nImproving patient education materials may improve patient outcomes. This study aims to explore the possibility of generating patient education materials with the assistance of a large language model, Chat Generative Pre-Trained Transformer (ChatGPT). In addition, we compare the accuracy and readability of ChatGPT-generated materials versus expert-generated materials.\n\n\nMethods\nPatient education materials in implant-based breast reconstruction were generated by experts and ChatGPT independently. Readability and accuracy of the materials are the main outcomes. Readability of the materials was compared using Flesch-Kincaid score. Accuracy of the materials generated by ChatGPT was evaluated by 2 independent reviewers. Content errors are categorized into information errors, statistical errors, and multiple errors (errors more than 2 types).\n\n\nResults\nThe content generated by experts had higher readability. The Flesch-Kincaid score is at the 7.5 grade for expert-generated materials, whereas the content generated by ChatGPT is at the 10.5 grade (despite ChatGPT being asked to generate content at the seventh grade level). The accuracy of ChatGPT-generated content is 50%, with most errors being information errors. ChatGPT often provides information about breast reduction or breast augmentation, despite being asked specifically about breast reconstruction. Despite its limitation, ChatGPT significantly reduced the time required to generate patient education materials. Although it takes experts 1 month to generate patient education materials, ChatGPT generates materials within 30 minutes.\n\n\nConclusions\nChatGPT can be a powerful starting tool to generate patient education materials. However, its readability and accuracy still require improvements.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/sap.0000000000003634"
    },
    {
        "id": 20594,
        "title": "Generative Pre-Trained Transformer-Empowered Healthcare Conversations: Current Trends, Challenges, and Future Directions in Large Language Model-Enabled Medical Chatbots",
        "authors": "James C. L. Chow, Valerie Wong, Kay Li",
        "published": "2024-3-14",
        "citations": 1,
        "abstract": "This review explores the transformative integration of artificial intelligence (AI) and healthcare through conversational AI leveraging Natural Language Processing (NLP). Focusing on Large Language Models (LLMs), this paper navigates through various sections, commencing with an overview of AI’s significance in healthcare and the role of conversational AI. It delves into fundamental NLP techniques, emphasizing their facilitation of seamless healthcare conversations. Examining the evolution of LLMs within NLP frameworks, the paper discusses key models used in healthcare, exploring their advantages and implementation challenges. Practical applications in healthcare conversations, from patient-centric utilities like diagnosis and treatment suggestions to healthcare provider support systems, are detailed. Ethical and legal considerations, including patient privacy, ethical implications, and regulatory compliance, are addressed. The review concludes by spotlighting current challenges, envisaging future trends, and highlighting the transformative potential of LLMs and NLP in reshaping healthcare interactions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/biomedinformatics4010047"
    },
    {
        "id": 20595,
        "title": "Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers",
        "authors": "Qihao Zhu, Xinyu Zhang, Jianxi Luo",
        "published": "2023-4-1",
        "citations": 12,
        "abstract": "Abstract\nBiological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a data-driven approach to bridge the gap. This paper proposes a generative design approach based on the generative pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely generative pre-trained transformer 3 (GPT-3), is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the mapping relevancy between the domains within the generated BID concepts. The approach is evaluated and then employed in a real-world project of designing light-weighted flying cars during its conceptual design phase The results show our approach can generate BID concepts with good performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4056598"
    },
    {
        "id": 20596,
        "title": "Unveiling the Power of Pre - Trained Language Models in NLP Applications",
        "authors": "Shrinath Pai",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115202502"
    },
    {
        "id": 20597,
        "title": "Pre-Trained Generative Architectures for Question-Asking Chatbots on Technical Text: A Case Study",
        "authors": "Nikahat Mulla, Prachi Gharpure",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asiancon58793.2023.10270559"
    },
    {
        "id": 20598,
        "title": "Possibilities and Pitfalls of Generative Pre-Trained Transformers in Healthcare",
        "authors": "Tajinder Kumar, Ramesh Kait,  Ankita, Sunita Rani",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icacctech61146.2023.00016"
    },
    {
        "id": 20599,
        "title": "Biomedical generative pre-trained based transformer language model for age-related disease target discovery",
        "authors": "Diana Zagirova, Stefan Pushkov, Geoffrey Ho Duen Leung, Bonnie Hei Man Liu, Anatoly Urban, Denis Sidorenko, Aleksandr Kalashnikov, Ekaterina Kozlova, Vladimir Naumov, Frank W. Pun, Ivan V. Ozerov, Alex Aliper, Alex Zhavoronkov",
        "published": "2023-9-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18632/aging.205055"
    },
    {
        "id": 20600,
        "title": "FLUID-GPT (Fast Learning to Understand and Investigate Dynamics with a Generative Pre-Trained Transformer): Efficient Predictions of Particle Trajectories and Erosion",
        "authors": "Steve D. Yang, Zulfikhar A. Ali, Bryan M. Wong",
        "published": "2023-9-20",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acs.iecr.3c01639"
    },
    {
        "id": 20601,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 20602,
        "title": "Generative Pre-trained Transformer 4 makes cardiovascular magnetic resonance reports easy to understand",
        "authors": "Babak Salam, Dmitrij Kravchenko, Sebastian Nowak, Alois M. Sprinkart, Leonie Weinhold, Anna Odenthal, Narine Mesropyan, Leon M. Bischoff, Ulrike Attenberger, Daniel L. Kuetting, Julian A. Luetkens, Alexander Isaak",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jocmr.2024.101035"
    },
    {
        "id": 20603,
        "title": "3P Evaluating conversational generative pre-trained transformer (ChatGPT) as a tool in early breast cancer (eBC) cases",
        "authors": "F. Patanè, F. Palumbo, G. Lorenzini, I. Bargagna, P. Cinacchi, I. Albanese, D. Bilancio, F. Pantaleo, G. Acconci, G. Bianchini, E. Baldacci, M. La Commare, B. Fratini, A. Fontana",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.esmoop.2024.102258"
    },
    {
        "id": 20604,
        "title": "Prompting and Fine-tuning Pre-trained Generative Language Models",
        "authors": "Johny Moreira, Altigran da Silva, Luciano Barbosa",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "There has been an explosion of available pre-trained and fine-tuned Generative Language Models (LM). They vary in the number of parameters, architecture, training strategy, and training set size. Aligned with it, alternative strategies exist to exploit these models, such as Fine-tuning and Prompt Engineering. However, many questions may arise throughout this process: Which model to apply for a given task? Which strategies to use? Will Prompt Engineering solve all tasks? What are the computational and financial costs involved? This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5753/sbbd_estendido.2023.25636"
    },
    {
        "id": 20605,
        "title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network",
        "authors": "Rongxiang Weng, Wen Sen Cheng, Min Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.291"
    },
    {
        "id": 20606,
        "title": "Aspects of creating a corporate question-and-answer system using generative pre-trained language models",
        "authors": "Aleksei Golikov, Dmitrii Akimov, Maksim Romanovskii, Sergei Trashchenkov",
        "published": "2023-12",
        "citations": 0,
        "abstract": "\n The article describes various ways to use generative pre-trained language models to build a corporate question-and-answer system. A significant limitation of the current generative pre-trained language models is the limit on the number of input tokens, which does not allow them to work \"out of the box\" with a large number of documents or with a large document. To overcome this limitation, the paper considers the indexing of documents with subsequent search query and response generation based on two of the most popular open source solutions at the moment – the Haystack and LlamaIndex frameworks. It has been shown that using the open source Haystack framework with the best settings allows you to get more accurate answers when building a corporate question-and-answer system compared to the open source LlamaIndex framework, however, requires the use of an average of several more tokens.    The article used a comparative analysis to evaluate the effectiveness of using generative pre-trained language models in corporate question-and-answer systems using the Haystack and Llamaindex frameworks. The evaluation of the obtained results was carried out using the EM (exact match) metric. The main conclusions of the conducted research on the creation of question-answer systems using generative pre-trained language models are: 1. Using hierarchical indexing is currently extremely expensive in terms of the number of tokens used (about 160,000 tokens for hierarchical indexing versus 30,000 tokens on average for sequential indexing), since the response is generated by sequentially processing parent and child nodes. 2. Processing information using the Haystack framework with the best settings allows you to get somewhat more accurate answers than using the LlamaIndex framework (0.7 vs. 0.67 with the best settings). 3. Using the Haystack framework is more invariant with respect to the accuracy of responses in terms of the number of tokens in the chunk. 4. On average, using the Haystack framework is more expensive in terms of the number of tokens (about 4 times) than the LlamaIndex framework. 5. The \"create and refine\" and \"tree summarize\" response generation modes for the LlamaIndex framework are approximately the same in terms of the accuracy of the responses received, however, more tokens are required for the \"tree summarize\" mode.\n\t",
        "keywords": "",
        "link": "http://dx.doi.org/10.25136/2409-8698.2023.12.69353"
    },
    {
        "id": 20607,
        "title": "Manual Corpora Development for Generative Pre-trained Transformers (GPT) &amp;amp; Evaluation of GPT Model Learning Capability",
        "authors": "Prasad Nimantha Madusanka Ukwatta Hewage",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4391815"
    },
    {
        "id": 20608,
        "title": "Comparison of large-scale pre-trained models based ViT, swin transformer and ConvNeXt",
        "authors": "Jiapeng Yu",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671201"
    },
    {
        "id": 20609,
        "title": "Nothing New Under the Sun?",
        "authors": "A. G. Elrod",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "This article investigates the potential impact of generative artificial intelligence, specifically OpenAI's GPT, on the field of biblical studies, particularly biblical Hebrew. The study is divided into three main categories: (1) knowledge retrieval or language understanding, (2) generative modeling or creative problem solving, and (3) command interpretation or query parsing. Experiments are conducted using OpenAI's GPT, the ETCBC's BHSA dataset, and Text-Fabric Python libraries. Results demonstrate GPT's limitations and proficiencies in biblical Hebrew and its capacity to employ its proficiencies creatively in problem-solving scenarios involving multifaceted forms of reasoning. The study concludes that understanding the capabilities and potential trajectories of these technologies is vital for biblical Hebrew scholarship, as they already possess the capacity to disrupt established scholarly norms and democratize access to advanced tools.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7146/hn.v8i2.143114"
    },
    {
        "id": 20610,
        "title": "Photo-based Carbohydrates Counting using Pre-trained Transformer Models",
        "authors": "Ivan Contreras, Marti Guso, Aleix Beneyto, Josep Vehi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.445"
    },
    {
        "id": 20611,
        "title": "Performance of a commercially available Generative Pre-trained Transformer (GPT) in describing radiolucent lesions in panoramic radiographs and establishing differential diagnoses",
        "authors": "Thaísa Pinheiro Silva, Maria Fernanda Silva Andrade-Bortoletto, Thaís Santos Cerqueira Ocampo, Caio Alencar-Palha, Michael M. Bornstein, Christiano Oliveira-Santos, Matheus L. Oliveira",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "Abstract\nObjectives\nTo evaluate the performance of a commercially available Generative Pre-trained Transformer (GPT) in describing and establishing differential diagnoses for radiolucent lesions in panoramic radiographs.\n\nMaterials and methods\nTwenty-eight panoramic radiographs, each containing a single radiolucent lesion, were evaluated in consensus by three examiners and a commercially available ChatGPT-3.5 model. They provided descriptions regarding internal structure (radiodensity, loculation), periphery (margin type, cortication), shape, location (bone, side, region, teeth/structures), and effects on adjacent structures (effect, adjacent structure). Diagnostic impressions related to origin, behavior, and nature were also provided. The GPT program was additionally prompted to provide differential diagnoses. Keywords used by the GPT program were compared to those used by the examiners and scored as 0 (incorrect), 0.5 (partially correct), or 1 (correct). Mean score values and standard deviation were calculated for each description. Performance in establishing differential diagnoses was assessed using Rank-1, -2, and − 3.\n\nResults\nDescriptions of margination, affected bone, and origin received the highest scores: 0.93, 0.93, and 0.87, respectively. Shape, region, teeth/structures, effect, affected region, and nature received considerably lower scores ranging from 0.22 to 0.50. Rank-1, -2, and − 3 demonstrated accuracy in 25%, 57.14%, and 67.85% of cases, respectively.\n\nConclusion\nThe performance of the GPT program in describing and providing differential diagnoses for radiolucent lesions in panoramic radiographs is variable and at this stage limited in its use for clinical application.\n\nClinical relevance\nUnderstanding the potential role of GPT systems as an auxiliary tool in image interpretation is imperative to validate their clinical applicability.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00784-024-05587-5"
    },
    {
        "id": 20612,
        "title": "The Use and Misuse of Pre-Trained Generative Large Language Models in Reliability Engineering",
        "authors": "Yunwei Hu, Yavuz Goktas, David Deepak Yellamati, Catherine De Tassigny",
        "published": "2024-1-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rams51492.2024.10457630"
    },
    {
        "id": 20613,
        "title": "Program Synthesis with Generative Pre-trained Transformers and Grammar-Guided Genetic Programming Grammar",
        "authors": "Ning Tao, Anthony Ventresque, Takfarinas Saber",
        "published": "2023-10-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/la-cci58595.2023.10409384"
    },
    {
        "id": 20614,
        "title": "BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection",
        "authors": "Sihao Hu, Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He, Ling Liu",
        "published": "2023-4-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543507.3583345"
    },
    {
        "id": 20615,
        "title": "CpG Island Detection Using Modified Transformer Model with Pre–trained Embedding",
        "authors": "Md Jubaer Hossain, Mohammed Imamul Hassan Bhuiyan, Zaowad Rahabin Abdullah",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441209"
    },
    {
        "id": 20616,
        "title": "Disaster Image Classification Using Pre-trained Transformer and Contrastive Learning Models",
        "authors": "Soudabeh Taghian Dinani, Doina Caragea",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302517"
    },
    {
        "id": 20617,
        "title": "Leveraging Generative Pre-trained Language Models for Advanced Unsupervised Neural Machine Translation",
        "authors": "Nitiraj Kulkarni, Prerna Ghorpade",
        "published": "2024-3-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17148/iarjset.2024.11405"
    },
    {
        "id": 20618,
        "title": "Pre‐trained low‐light image enhancement transformer",
        "authors": "Jingyao Zhang, Shijie Hao, Yuan Rao",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "AbstractLow‐light image enhancement is a longstanding challenge in low‐level vision, as images captured in low‐light conditions often suffer from significant aesthetic quality flaws. Recent methods based on deep neural networks have made impressive progress in this area. In contrast to mainstream convolutional neural network (CNN)‐based methods, an effective solution inspired by the transformer, which has shown impressive performance in various tasks, is proposed. This solution is centred around two key components. The first is an image synthesis pipeline, and the second is a powerful transformer‐based pre‐trained model, known as the low‐light image enhancement transformer (LIET). The image synthesis pipeline includes illumination simulation and realistic noise simulation, enabling the generation of more life‐like low‐light images to overcome the issue of data scarcity. LIET combines streamlined CNN‐based encoder‐decoders with a transformer body, efficiently extracting global and local contextual features at a relatively low computational cost. The extensive experiments show that this approach is highly competitive with current state‐of‐the‐art methods. The codes have been released and are available at LIET.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.13076"
    },
    {
        "id": 20619,
        "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
        "authors": "Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.692"
    },
    {
        "id": 20620,
        "title": "Detecting Anomalous 3D Point Clouds Using Pre-Trained Feature Extractors",
        "authors": "Dario Mantegazza, Alessandro Giusti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012465600003660"
    },
    {
        "id": 20621,
        "title": "A Review for Pre-Trained Transformer-Based Time Series Forecasting Models",
        "authors": "Yunus Emre Midilli, Sergei Parshutin",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itms59786.2023.10317721"
    },
    {
        "id": 20622,
        "title": "On the effect of dropping layers of pre-trained transformer models",
        "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov",
        "published": "2023-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csl.2022.101429"
    },
    {
        "id": 20623,
        "title": "Research on Generative Pre-Trained Model Evaluation Based on Causality Analysis",
        "authors": "Hongyu Wu, Yuanfei He, Miaomiao Yang, Lixin Zhang, Tong Ling, Yifei Wang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiiip61647.2023.00057"
    },
    {
        "id": 20624,
        "title": "Performance of 4 Pre-Trained Sentence Transformer Models in the Semantic Query of a Systematic Review Dataset on Peri-Implantitis",
        "authors": "Carlo Galli, Nikolaos Donos, Elena Calciolari",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "Systematic reviews are cumbersome yet essential to the epistemic process of medical science. Finding significant reports, however, is a daunting task because the sheer volume of published literature makes the manual screening of databases time-consuming. The use of Artificial Intelligence could make literature processing faster and more efficient. Sentence transformers are groundbreaking algorithms that can generate rich semantic representations of text documents and allow for semantic queries. In the present report, we compared four freely available sentence transformer pre-trained models (all-MiniLM-L6-v2, all-MiniLM-L12-v2, all-mpnet-base-v2, and All-distilroberta-v1) on a convenience sample of 6110 articles from a published systematic review. The authors of this review manually screened the dataset and identified 24 target articles that addressed the Focused Questions (FQ) of the review. We applied the four sentence transformers to the dataset and, using the FQ as a query, performed a semantic similarity search on the dataset. The models identified similarities between the FQ and the target articles to a varying degree, and, sorting the dataset by semantic similarities using the best-performing model (all-mpnet-base-v2), the target articles could be found in the top 700 papers out of the 6110 dataset. Our data indicate that the choice of an appropriate pre-trained model could remarkably reduce the number of articles to screen and the time to completion for systematic reviews.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info15020068"
    },
    {
        "id": 20625,
        "title": "Commit-Level Software Change Intent Classification Using a Pre-Trained Transformer-Based Code Model",
        "authors": "Tjaša Heričko, Boštjan Šumak, Sašo Karakatič",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "Software evolution is driven by changes made during software development and maintenance. While source control systems effectively manage these changes at the commit level, the intent behind them are often inadequately documented, making understanding their rationale challenging. Existing commit intent classification approaches, largely reliant on commit messages, only partially capture the underlying intent, predominantly due to the messages’ inadequate content and neglect of the semantic nuances in code changes. This paper presents a novel method for extracting semantic features from commits based on modifications in the source code, where each commit is represented by one or more fine-grained conjoint code changes, e.g., file-level or hunk-level changes. To address the unstructured nature of code, the method leverages a pre-trained transformer-based code model, further trained through task-adaptive pre-training and fine-tuning on the downstream task of intent classification. This fine-tuned task-adapted pre-trained code model is then utilized to embed fine-grained conjoint changes in a commit, which are aggregated into a unified commit-level vector representation. The proposed method was evaluated using two BERT-based code models, i.e., CodeBERT and GraphCodeBERT, and various aggregation techniques on data from open-source Java software projects. The results show that the proposed method can be used to effectively extract commit embeddings as features for commit intent classification and outperform current state-of-the-art methods of code commit representation for intent categorization in terms of software maintenance activities undertaken by commits.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math12071012"
    },
    {
        "id": 20626,
        "title": "Reducing Bias in Pre-Trained Models by Tuning While Penalizing Change",
        "authors": "Niklas Penzel, Gideon Stein, Joachim Denzler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012345800003660"
    },
    {
        "id": 20627,
        "title": "P464 Could Chat Generative Pre-Trained Transformer (ChatGPT) be an AI-provided friend of the patients for frequently asked questions concerning their IBD management? An evidence- and guidelines-controlled textual analysis of AI-provided outputs",
        "authors": "A G Gravina, R Pellegrino, G Palladino, G Imperio, A Ventura, S Auletta, M Cipullo, P Ciamarra, M Dallio, A Federico",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "Abstract\n\nBackground\nArtificial intelligence (AI) is gradually and potentially entering the everyday lives of patients with Inflammatory Bowel Disease (IBD). Systems such as Chat Generative Pre-Trained Transformer (ChatGPT), based on large language models (LLM), are now within reach. It is necessary to weigh whether these LLM systems can be real generators of medical information and whether the latter is generated based on credible databases and evidence. This study evaluated and analysed whether the outputs of ChatGPT to common questions from IBD patients can provide credible and scientifically reliable outputs.\n\n\nMethods\nIBD-expert physicians retrieved a list of ten IBD frequently asked questions in their clinical practice. The ten with the highest frequency (Q1-10) were collected from the total number of questions and then input on ChatGPT (see Table) on three different days (18th, 19th, and 20th August 2023), and each output generated by the chatbot was categorised as O1, O2, and O3. The same research team evaluated the AI-generated responses by ChatGPT for each question by objectively comparing them with the available evidence (provided by meta-analyses, systematic reviews and ECCO guidelines).\n\n\nResults\nQ1 O1-3 were reliable, with a correct definition of the absence of definitive therapy for IBD. In Q2 O1-3, ChatGPT failed to clarify the lack of solid evidence for nutritional therapy in IBD. Q3 O1-3 provided corrected guidance on repeat endoscopic examinations for IBD. Q4 O1-3 did not provide reliable answers on removing topical therapy in ulcerative colitis. In Q5 O1-3 good advice was given to the IBD patient on how to plan a pregnancy. Q6 O1-3 brought out the lack of ChatGPT update (stopped in September 2021), limiting the range of new oral small molecule therapies available for the IBD patient. In Q7 O1-3 ChatGPT correctly outlined the need for individual risk stratification for endoscopic surveillance of colorectal cancer. In Q8 and Q9 O1-3 ChatGPT correctly weighed the risk of infection/cancer during biologic therapy and how heritability is only one piece of the pathogenetic puzzle of IBD. Finally, in Q10 he correctly elucidated the possibility of biological therapy preventing post-operative recurrence of Crohn's disease.\n\n\nConclusion\nChatGPT is a potentially useful complementary tool for communicating with IBD patients. It is necessary to improve the degree to which it is updated and sources on appropriate databases (e.g., Scopus, Web of Science, and MEDLINE). Ultimately, there are still significant limitations. Much of the latest findings, in fact, are excluded from ChatGPT evaluation. A portion of the outputs provided are partially incorrect or not fully detailed and not yet ready to be released to the patient without a physician filter.\n\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/ecco-jcc/jjad212.0594"
    },
    {
        "id": 20628,
        "title": "Interpreting Art by Leveraging Pre-Trained Models",
        "authors": "Niklas Penzel, Joachim Denzler",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10216010"
    },
    {
        "id": 20629,
        "title": "Integrally Pre-Trained Transformer Pyramid Networks",
        "authors": "Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, Qixiang Ye",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01785"
    },
    {
        "id": 20630,
        "title": "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment",
        "authors": "Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00272"
    },
    {
        "id": 20631,
        "title": "University of Hildesheim at SemEval-2023 Task 1: Combining Pre-trained Multimodal and Generative Models for Image Disambiguation",
        "authors": "Sebastian Diem, Chan Jong Im, Thomas Mandl",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.18"
    },
    {
        "id": 20632,
        "title": "A Developed Graphical User Interface-Based on Different Generative Pre-trained Transformers Models",
        "authors": "Ekrem Küçük, İpek Balıkçı Çiçek, Zeynep Küçükakçalı, Cihan Yetiş, Cemil Çolak",
        "published": "2024-4-5",
        "citations": 0,
        "abstract": "Objective: The article investigates the integration of advanced Generative Pretrained Transformers (GPT) models into a user-friendly Graphical User Interface (GUI). The primary objective of this work is to simplify access to complex Natural Language Processing (NLP) tasks for a diverse range of users, including those with limited technical background.\r\nMethods: The development process of the GUI was comprehensive and systematic:\r\n•\tNeeds Assessment: This stage involved understanding the requirements and expectations of potential users to ensure the GUI effectively addresses their needs.\r\n•\tPreliminary Design and Development: The initial designs were created and developed into a functional GUI, emphasizing the integration of features supporting various NLP tasks like text summarization, translation, and question-answering.\r\n•\tIterative Refinement: Continuous improvements were made based on user feedback, focusing on enhancing user experience, ease of navigation, and customization capabilities.\r\nResults: The developed GUI successfully integrated GPT models, including GPT-4 Turbo and GPT-3.5, resulting in an intuitive and adaptable interface. It demonstrated efficiency in performing various NLP tasks, thereby making these advanced language processing tools accessible to a broader audience. The GUI's design, emphasizing user-friendliness and adaptability, was particularly noted for its ability to cater to both technical and non-technical users.\r\nConclusion: In conclusion, the article illustrates the significant impact of combining advanced GPT models with a Graphical User Interface to democratize the use of NLP tools. This integration not only makes complex language processing more accessible but also marks a pivotal step in the inclusive application of AI technology across various domains. The successful implementation of the GUI highlights the potential of AI in enhancing user interaction and broadening the scope of technology usage in everyday tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56941/odutip.1413597"
    },
    {
        "id": 20633,
        "title": "Enhancing Data Representation: A Novel Text-to-Image Protocol for Advanced Visual Content Generation using Generative Pre-trained Transformers",
        "authors": "IJSREM Journal",
        "published": "2024-1-15",
        "citations": 0,
        "abstract": "The rapid advancement of text-to-image generation has led to the development of innovative protocols for creating visual content from textual descriptions. This article presents a cutting-edge text-to-image protocol designed to enhance data representation through advanced neural network architectures and natural language processing techniques. The protocol leverages state-of-the-art deep learning models to generate high-fidelity images from textual inputs, offering significant potential for applications in diverse fields such as art generation, e-commerce, and content creation. The proposed protocol demonstrates promising results in producing realistic and contextually relevant images, marking a substantial leap forward in the realm of text-to-image technology. Key Words: Text-to-Image Protocol , Data Representation Neural Network Architectures,Natural Language Processing ,Deep Learning Models ,Image Generation, E-commerce Applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55041/ijsrem28134"
    },
    {
        "id": 20634,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Roheen Qamar",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/6rmwu1"
    },
    {
        "id": 20635,
        "title": "ChatGPT (Generated Pre-Trained Transformer) As an Adjunct to Mental Health Interventions: A Commentary",
        "authors": "Villarino Resti Tito H, Villarino Maureen Lorence F",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23937/2572-4037.1510062"
    },
    {
        "id": 20636,
        "title": "Survey Paper: Automatic Title Generation for Text with RNN and Pre-trained Transformer Language Model",
        "authors": "Vishal Lodhwal, Gowri Choudhary",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "Abstract: Nowadays huge amounts of text data are available due to the evolution of the Internet. Although search engines are used to select text data, it is unfeasible to go through the entire search results of text that are related to search intent. Therefore, text summarization is the only method by which we can reduce text data without the loss of information. A new method for Title or Text generation is the Transformer language model that has been trained i.e. GPT-2, GPTNeo, Chat-GPT, and LSTM Model of RNN by which we can generate catchy titles to take readers’ attention and attract them to read a full article. This paper has discovered recent literature on all the previously mentioned topics which are related to Title or Text generators and examines several methods which propose to make use of various language models. The suggested approaches are also contrasted with one another to highlight their unique advantages, and to suggest constantly better ways.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22214/ijraset.2023.49713"
    },
    {
        "id": 20637,
        "title": "Revolutionizing generative pre-traineds: Insights and challenges in deploying ChatGPT and generative chatbots for FAQs",
        "authors": "Feriel Khennouche, Youssef Elmir, Yassine Himeur, Nabil Djebari, Abbes Amira",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123224"
    },
    {
        "id": 20638,
        "title": "Deep Interactive Volume Exploration Through Pre-Trained 3D CNN and Active Learning",
        "authors": "Marwa Salhi, Riadh Ksantini, Belhassen Zouari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011638500003417"
    },
    {
        "id": 20639,
        "title": "Generative pre-trained transformers (GPT)-based automated data mining for building energy management: Advantages, limitations and the future",
        "authors": "Chaobo Zhang, Jie Lu, Yang Zhao",
        "published": "2024-2",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enbenv.2023.06.005"
    },
    {
        "id": 20640,
        "title": "MetaAc4C: A multi-module deep learning framework for accurate prediction of N4-acetylcytidine sites based on pre-trained bidirectional encoder representation and generative adversarial networks",
        "authors": "Zutan Li, Bingbing Jin, Jingya Fang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ygeno.2023.110749"
    },
    {
        "id": 20641,
        "title": "p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models",
        "authors": "Haoyuan Wu, Xinyun Zhang, Peng Xu, Peiyu Liao, Xufeng Yao, Bei Yu",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Vision-Language models (VLMs) pre-trained on large corpora have demonstrated notable success across a range of downstream tasks. In light of the rapidly increasing size of pre-trained VLMs, parameter-efficient transfer learning (PETL) has garnered attention as a viable alternative to full fine-tuning. One such approach is the adapter, which introduces a few trainable parameters into the pre-trained models while preserving the original parameters during adaptation.\nIn this paper, we present a novel modeling framework that recasts adapter tuning after attention as a graph message passing process on attention graphs, where the projected query and value features and attention matrix constitute the node features and the graph adjacency matrix, respectively. Within this framework, tuning adapters in VLMs necessitates handling heterophilic graphs, owing to the disparity between the projected query and value space.\nTo address this challenge, we propose a new adapter architecture, p-adapter, which employs p-Laplacian message passing in Graph Neural Networks (GNNs). Specifically, the attention weights are re-normalized based on the features, and the features are then aggregated using the calibrated attention matrix, enabling the dynamic exploitation of information with varying frequencies in the heterophilic attention graphs.\nWe conduct extensive experiments on different pre-trained VLMs and multi-modal tasks, including visual question answering, visual entailment, and image captioning. The experimental results validate our method's significant superiority over other PETL methods. Our code is available at https://github.com/wuhy68/p-Adapter/.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i6.28415"
    },
    {
        "id": 20642,
        "title": "Survey on leveraging pre-trained generative adversarial networks for image editing and restoration",
        "authors": "Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, Lei Zhang",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11432-022-3679-0"
    },
    {
        "id": 20643,
        "title": "Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization",
        "authors": "Liqiang Jing, Yiren Li, Junhao Xu, Yongcan Yu, Pei Shen, Xuemeng Song",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11633-022-1372-x"
    },
    {
        "id": 20644,
        "title": "Leveraging Generative Pre-trained Transformers for the Integration of Environmental, Social, and Governance Considerations into Investment Management for Thai Stock",
        "authors": "Nuthdanai Wangpratham, Teerasit Termsaithong, Pasin Marupanthorn, Jutha Koryanyong, Chanon Chanpiwat, Carat Pathomsathit",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4715431"
    },
    {
        "id": 20645,
        "title": "Harnessing Generative Pre-Trained Transformers for Construction Accident Prediction with Saliency Visualization",
        "authors": "Byunghee Yoo, Jinwoo Kim, Seongeun Park, Changbum R. Ahn, Taekeun Oh",
        "published": "2024-1-12",
        "citations": 1,
        "abstract": "Leveraging natural language processing models using a large volume of text data in the construction safety domain offers a unique opportunity to improve understanding of safety accidents and the ability to learn from them. However, little effort has been made to date in regard to utilizing large language models for the prediction of accident types that can help to prevent and manage potential accidents. This research aims to develop a model for predicting the six types of accidents (caught-in-between, cuts, falls, struck-by, trips, and others) by employing transfer learning with a fine-tuned generative pre-trained transformer (GPT). Additionally, to enhance the interpretability of the fine-tuned GPT model, a method for saliency visualization of input text was developed to identify words that significantly impact prediction results. The models were evaluated using a comprehensive dataset comprising 15,000 actual accident records. The results indicate that the suggested model for detecting the six accident types achieves 82% accuracy. Furthermore, it was observed that the proposed saliency visualization method can identify accident precursors from unstructured free-text data of construction accident reports. These results highlight the advancement of the generalization performance of large language processing-based accident prediction models, thereby proactively preventing construction accidents.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14020664"
    },
    {
        "id": 20646,
        "title": "OC.03.3: CAN CHAT GENERATIVE PRE-TRAINED TRANSFORMER (CHATGPT) BE AN AI-PROVIDED FRIEND OF THE IBD PATIENTS FOR FREQUENTLY ASKED QUESTIONS RELATED TO THEIR DISEASE MANAGEMENT? AN EVIDENCE- AND GUIDELINESCONTROLLED TEXTUAL ANALYSIS OF AI-PROVIDED OUTPUTS",
        "authors": "A.G. Gravina, G. Palladino, R. Pellegrino, G. Imperio, A. Ventura, S. Auletta, M. Cipullo, P. Ciamarra, M. Dallio, A. Federico",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s1590-8658(24)00415-8"
    },
    {
        "id": 20647,
        "title": "DatUS: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer",
        "authors": "Sonal Kumar, Arijit Sur, Rashmi Dutta Baruah",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcds.2024.3383952"
    },
    {
        "id": 20648,
        "title": "Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models",
        "authors": "Muhammad Asad Arshed, Shahzad Mumtaz, Muhammad Ibrahim, Saeed Ahmed, Muhammad Tahir, Muhammad Shafi",
        "published": "2023-7-18",
        "citations": 7,
        "abstract": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14070415"
    },
    {
        "id": 20649,
        "title": "An Efficient Pre-Trained Classification of Brain Tumor with Convolutional Neural Networks",
        "authors": "Sangeetha. G, Vadivu. G",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.13052/rp-9788770040723.028"
    },
    {
        "id": 20650,
        "title": "Pre-Trained CNN Architecture Analysis for Transformer-Based Indonesian Image Caption Generation Model",
        "authors": "Rifqi Mulyawan, Andi Sunyoto, Alva Hendi Muhammad Muhammad",
        "published": "2023-5-5",
        "citations": 0,
        "abstract": "Classification and object recognition in image processing has significantly improved computer vision tasks. The method is often used for visual problems, especially in picture classification utilizing the Convolutional Neural Network (CNN). In the popular state-of-the-art (SOTA) task of generating a caption on an image, the implementation is often used for feature extraction of an image as an encoder. Instead of performing direct classification, these extracted features are sent from the encoder to the decoder section to generate the sequence. So, some CNN layers related to the classification task are not required. This study aims to determine which CNN pre-trained architecture or model performs best in extracting image features using a state-of-the-art Transformer model as its decoder. Unlike the original Transformerâ€™s architecture, we implemented a vector-to-sequence way instead of sequence-to-sequence for the model. Indonesian Flickr8k and Flick30k datasets were used in this research. Evaluations were carried out using several pre-trained architectures, including ResNet18, ResNet34, ResNet50, ResNet101, VGG16, Efficientnet_b0, Efficientnet_b1, and Googlenet. The qualitative model inference results and quantitative evaluation scores were analyzed in this study. The test results show that the ResNet50 architecture can produce stable sequence generation with the highest accuracy value. With some experimentation, finetuning the encoder can significantly increase the model evaluation score. As for future work, further exploration with larger datasets like Flickr30k, MS COCO 14, MS COCO 17, and other image captioning datasets in Indonesian also implementing a new Transformers-based method can be used to get a better Indonesian automatic image captioning model.Â ",
        "keywords": "",
        "link": "http://dx.doi.org/10.30630/joiv.7.2.1387"
    },
    {
        "id": 20651,
        "title": "AraBERTopic: A Neural Topic Modeling Approach for News Extraction from Arabic Facebook Pages using Pre-trained BERT Transformer Model",
        "authors": "Nassera HABBAT, Houda ANOUN, Larbi HASSOUNI",
        "published": "2023-7-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12785/ijcds/140101"
    },
    {
        "id": 20652,
        "title": "Trajectory Prediction in First-Person Video: Utilizing a Pre-Trained Bird's-Eye View Model",
        "authors": "Masashi Hatano, Ryo Hachiuma, Hideo Saito",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011683300003417"
    },
    {
        "id": 20653,
        "title": "TransVCOX: Bridging Transformer Encoder and Pre-trained VAE for Robust Cancer Multi-Omics Survival Analysis",
        "authors": "Xiaoyu Li, Wenwen Min, Jinyu Chen, Jiaxin Wu, Shunfang Wang",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385668"
    },
    {
        "id": 20654,
        "title": "Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\"",
        "authors": "Qurrota A’yuna Itsnaini, Mardhiya Hayaty, Andriyan Dwi Putra, Nidal A.M Jabari",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "Automatic Text Summarization (ATS) is one of the utilizations of technological sophistication in terms of text processing assisting humans in producing a summary or key points of a document in large quantities. We use Indonesian language as objects because there are few resources in NLP research using Indonesian language. This paper utilized PLTMs (Pre-Trained Language Models) from the transformer architecture, namely T5 (Text-to-Text Transfer Transformer) which has been completed previously with a larger dataset. Evaluation in this study was measured through comparison of the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) calculation results between the reference summary and the model summary. The experiments with the pre-trained t5-base model with fine tuning parameters of 220M for the Indonesian news dataset yielded relatively high ROUGE values, namely ROUGE-1 = 0.68, ROUGE-2 = 0.61, and ROUGE-L = 0.65. The evaluation value worked well, but the resulting model has not achieved satisfactory results because in terms of abstraction, the model did not work optimally. We also found several errors in the reference summary in the dataset used.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33096/ilkom.v15i1.1532.124-131"
    },
    {
        "id": 20655,
        "title": "Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection",
        "authors": "Feng Liu, Xiaosong Zhang, Zhiliang Peng, Zonghao Guo, Fang Wan, Xiangyang Ji, Qixiang Ye",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00628"
    },
    {
        "id": 20656,
        "title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?",
        "authors": "Jaromir Savelka, Arav Agarwal, Christopher Bogart, Yifan Song, Majd Sakr",
        "published": "2023-6-29",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3587102.3588792"
    },
    {
        "id": 20657,
        "title": "Automated data mining framework for building energy conservation aided by generative pre-trained transformers (GPT)",
        "authors": "Chaobo Zhang, Jian Zhang, Yang Zhao, Jie Lu",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2023.113877"
    },
    {
        "id": 20658,
        "title": "GS2P: a generative pre-trained learning to rank model with over-parameterization for web-scale search",
        "authors": "Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dawei Yin",
        "published": "2024-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06469-9"
    },
    {
        "id": 20659,
        "title": "Performance Evaluation of Pre-Trained Deep Learning Models for Bird Species Identification",
        "authors": "Kerolos Hany, Youssef Ayman, Ayman Atia",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10255070"
    },
    {
        "id": 20660,
        "title": "Pre-trained Deep Learning Models for UAV-based Weed Recognition",
        "authors": "Faiza Mekhalfa, Fouad Yacef, Mahmoud Belhocine",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/spa59660.2023.10274449"
    },
    {
        "id": 20661,
        "title": "DAM INFOLOW FORECASTING IN THAILAND USING A PRE-TRAINED TRANSFORMER MODEL",
        "authors": "Daiki TOGI, Kiyoharu KAJIYAMA, Shinjiro KANAE",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2208/jscejj.23-16148"
    },
    {
        "id": 20662,
        "title": "Dual attention and channel transformer based generative adversarial network for restoration of the damaged artwork",
        "authors": "Praveen Kumar, Varun Gupta, Manan Grover",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107457"
    },
    {
        "id": 20663,
        "title": "Development and evaluation of a program based on a generative pre-trained transformer model from a public natural language processing platform for efficiency enhancement in post-procedural quality control of esophageal endoscopic submucosal dissection",
        "authors": "Huaiyuan Ma, Xingbin Ma, Chunxiao Yang, Qiong Niu, Tao Gao, Chengxia Liu, Yan Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00464-023-10620-x"
    },
    {
        "id": 20664,
        "title": "HiVeGPT: Human-Machine-Augmented Intelligent Vehicles With Generative Pre-Trained Transformer",
        "authors": "Junping Zhang, Jian Pu, Jianru Xue, Ming Yang, Xin Xu, Xiao Wang, Fei-Yue Wang",
        "published": "2023-3",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tiv.2023.3256982"
    },
    {
        "id": 20665,
        "title": "Is generative pre-trained transformer artificial intelligence (Chat-GPT) a reliable tool for guidelines synthesis? A preliminary evaluation for biologic CRSwNP therapy",
        "authors": "Antonino Maniaci, Alberto Maria Saibene, Christian Calvo-Henriquez, Luigi Vaira, Thomas Radulesco, Justin Michel, Carlos Chiesa-Estomba, Leigh Sowerby, David Lobo Duro, Miguel Mayo-Yanez, Juan Maza-Solano, Jerome Rene Lechien, Ignazio La Mantia, Salvatore Cocuzza",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00405-024-08464-9"
    },
    {
        "id": 20666,
        "title": "A survey of transformer-based multimodal pre-trained modals",
        "authors": "Xue Han, Yi-Tong Wang, Jun-Lan Feng, Chao Deng, Zhan-Heng Chen, Yu-An Huang, Hui Su, Lun Hu, Peng-Wei Hu",
        "published": "2023-1",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.09.136"
    },
    {
        "id": 20667,
        "title": "AB-Gen: Antibody Library Design with Generative Pre-Trained Transformer and Deep Reinforcement Learning",
        "authors": "Xiaopeng Xu, Tiantian Xu, Juexiao Zhou, Xingyu Liao, Ruochi Zhang, Yu Wang, Lu Zhang, Xin Gao",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "Abstract\nAntibody leads must fulfill multiple desirable properties to be clinical candidates. Primarily due to the low throughput in the experimental procedure, the need for such multi-property optimization causes the bottleneck in preclinical antibody discovery and development, because addressing one issue usually causes another. We developed a reinforcement learning (RL) method, named AB-Gen, for antibody library design using a generative pre-trained transformer (GPT) as the policy network of the RL agent. We showed that this model can learn the antibody space of heavy chain complementarity determining region 3 (CDRH3) and generate sequences with similar property distributions. Besides, when using human epidermal growth factor receptor-2 (HER2) as the target, the agent model of AB-Gen was able to generate novel CDRH3 sequences that fulfill multi-property constraints. Totally, 509 generated sequences were able to pass all property filters, and three highly conserved residues were identified. The importance of these residues was further demonstrated by molecular dynamics simulations, consolidating that the agent model was capable of grasping important information in this complex optimization task. Overall, the AB-Gen method is able to design novel antibody sequences with an improved success rate than the traditional propose-then-filter approach. It has the potential to be used in practical antibody design, thus empowering the antibody discovery and development process. The source code of AB-Gen is freely available at Zenodo (https://doi.org/10.5281/zenodo.7657016) and BioCode (https://ngdc.cncb.ac.cn/biocode/tools/BT007341).",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.gpb.2023.03.004"
    },
    {
        "id": 20668,
        "title": "Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study",
        "authors": "Zekun Jiang, Ruchun Jia, Le Zhang, Kang Li",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/healthcom56612.2023.10472397"
    },
    {
        "id": 20669,
        "title": "Generative Pre-Trained Transformers for 15-Minute City Design",
        "authors": "Rutvik Deshpande, Sayjel Vijay Patel, Camiel Weijenberg, Maciej Nisztuk, Miriam Corcuera, Jianxi Luo, Qihao Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52842/conf.caadria.2023.1.595"
    },
    {
        "id": 20670,
        "title": "Chat generative pre-trained transformer’s performance on dermatology-specific questions and its implications in medical education",
        "authors": "James Behrmann, Ellen M. Hong, Shannon Meledathu, Aliza Leiter, Michael Povelaitis, Mariela Mitre",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21037/jmai-23-47"
    },
    {
        "id": 20671,
        "title": "Pre-Trained Language Models and Their Applications",
        "authors": "Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, Yu Sun",
        "published": "2023-6",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eng.2022.04.024"
    },
    {
        "id": 20672,
        "title": "Parametrized pre-trained network (PPNet): A novel shape classification method using SPHARMs for MI detection",
        "authors": "Gelareh Valizadeh, Farshid Babapour Mofrad",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120368"
    },
    {
        "id": 20673,
        "title": "Multi-label classification of chest X-ray images with pre-trained vision Transformer model",
        "authors": "Xing Suxia,  , Ju Zihan, Liu Zijiao, Wang Yu, Fan Fuqiang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.11834/jig.220284"
    },
    {
        "id": 20674,
        "title": "Discharge summary hospital course summarisation of in patient Electronic Health Record text with clinical concept guided deep pre-trained Transformer models",
        "authors": "Thomas Searle, Zina Ibrahim, James Teo, Richard J.B. Dobson",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jbi.2023.104358"
    },
    {
        "id": 20675,
        "title": "Empowering Customer Support: Using Generative AI and Pre-trained LLM's in a Chatbot Revolution",
        "authors": "Munesh Kumar B N, Rohini A, Armaan Shaik, Mohammad Basha Shaik, Mohammed Abrar",
        "published": "2024-1-15",
        "citations": 0,
        "abstract": " This paper addresses the challenge of efficiently handling a diverse array of customer queries by proposing the development of an innovative web-based customer support chatbot. The objectives encompass creating a versatile system capable of interpreting and resolving a spectrum of customer complaints, enhancing support staff efficiency, and facilitating knowledge base updates. The proposed methodology employs the MERN stack for web app development and integrates Generative AI and pre-trained Large Language Models (LLMs), specifically OpenAI's prebuilt models, for intelligent responses. The pseudo code outlines the interactions between the chatbot, support staff, and administrators, ensuring a seamless process. Results indicate the chatbot's commendable ability to accurately interpret queries, showcasing language versatility in various Indian and foreign languages. Efficient handling of varied phrasings and also accommodating grammatical errors. The use of prebuilt models is highlighted as a practical and efficient solution, reducing implementation time compared to custom models. Additionally, the implementation is recognized for its scalability, positioning it as a practical and efficient solution for evolving customer support needs in the near future. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.15680/ijircce.2024.1201020"
    },
    {
        "id": 20676,
        "title": "Brain Tumor Classification Using a Pre-Trained Auxiliary Classifying Style-Based Generative Adversarial Network",
        "authors": "M. Akshay Kumaar, Duraimurugan Samiayya, Venkatesan Rajinikanth, Durai Raj Vincent P M, Seifedine Kadry",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.9781/ijimai.2023.02.008"
    },
    {
        "id": 20677,
        "title": "Dual mode information fusion with pre-trained CNN models and transformer for video-based non-invasive anaemia detection",
        "authors": "Abhishek Kesarwani, Sunanda Das, Dakshina Ranjan Kisku, Mamata Dalui",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.105592"
    },
    {
        "id": 20678,
        "title": "Generative Pre-Trained Transformers (GPT) Artificial intelligence – Assessing the Accuracy of ChatGPT as an Adjunct for Peri-operative Care",
        "authors": "Omar Allam, Mica Williams, Mariana Almeida, David Alper, Andrew Craver, John Persing, Michael Alperovich",
        "published": "2023-10-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1097/01.gox.0000992588.09873.87"
    },
    {
        "id": 20679,
        "title": "Do Pre-trained Models Benefit Equally in Continual Learning?",
        "authors": "Kuan-Ying Lee, Yuanyi Zhong, Yu-Xiong Wang",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00642"
    },
    {
        "id": 20680,
        "title": "Efficient Transferability Assessment for Selection of Pre-trained Detectors",
        "authors": "Zhao Wang, Aoxue Li, Zhenguo Li, Qi Dou",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00170"
    },
    {
        "id": 20681,
        "title": "Optimization of binding affinities in chemical space with generative pre-trained transformer and deep reinforcement learning",
        "authors": "Xiaopeng Xu, Juexiao Zhou, Chen Zhu, Qing Zhan, Zhongxiao Li, Ruochi Zhang, Yu Wang, Xingyu Liao, Xin Gao",
        "published": "2024-2-20",
        "citations": 0,
        "abstract": "Background The key challenge in drug discovery is to discover novel compounds with desirable properties. Among the properties, binding affinity to a target is one of the prerequisites and usually evaluated by molecular docking or quantitative structure activity relationship (QSAR) models. Methods In this study, we developed SGPT-RL, which uses a generative pre-trained transformer (GPT) as the policy network of the reinforcement learning (RL) agent to optimize the binding affinity to a target. SGPT-RL was evaluated on the Moses distribution learning benchmark and two goal-directed generation tasks, with Dopamine Receptor D2 (DRD2) and Angiotensin-Converting Enzyme 2 (ACE2) as the targets. Both QSAR model and molecular docking were implemented as the optimization goals in the tasks. The popular Reinvent method was used as the baseline for comparison. Results The results on the Moses benchmark showed that SGPT-RL learned good property distributions and generated molecules with high validity and novelty. On the two goal-directed generation tasks, both SGPT-RL and Reinvent were able to generate valid molecules with improved target scores. The SGPT-RL method achieved better results than Reinvent on the ACE2 task, where molecular docking was used as the optimization goal. Further analysis shows that SGPT-RL learned conserved scaffold patterns during exploration. Conclusions The superior performance of SGPT-RL in the ACE2 task indicates that it can be applied to the virtual screening process where molecular docking is widely used as the criteria. Besides, the scaffold patterns learned by SGPT-RL during the exploration process can assist chemists to better design and discover novel lead candidates.",
        "keywords": "",
        "link": "http://dx.doi.org/10.12688/f1000research.130936.2"
    },
    {
        "id": 20682,
        "title": "Optimization of binding affinities in chemical space with generative pre-trained transformer and deep reinforcement learning",
        "authors": "Xiaopeng Xu, Juexiao Zhou, Chen Zhu, Qing Zhan, Zhongxiao Li, Ruochi Zhang, Yu Wang, Xingyu Liao, Xin Gao",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "Background: The key challenge in drug discovery is to discover novel compounds with desirable properties. Among the properties, binding affinity to a target is one of the prerequisites and usually evaluated by molecular docking or quantitative structure activity relationship (QSAR) models. Methods: In this study, we developed Simplified molecular input line entry system Generative Pre-trained Transformer with Reinforcement Learning (SGPT-RL), which uses a transformer decoder as the policy network of the reinforcement learning agent to optimize the binding affinity to a target. SGPT-RL was evaluated on the Moses distribution learning benchmark and two goal-directed generation tasks, with Dopamine Receptor D2 (DRD2) and Angiotensin-Converting Enzyme 2 (ACE2) as the targets. Both QSAR model and molecular docking were implemented as the optimization goals in the tasks. The popular Reinvent method was used as the baseline for comparison. Results: The results on Moses benchmark showed that SGPT-RL learned good property distributions and generated molecules with high validity and novelty. On the two goal-directed generation tasks, both SGPT-RL and Reinvent were able to generate valid molecules with improved target scores. The SGPT-RL method achieved better results than Reinvent on the ACE2 task, where molecular docking was used as the optimization goal. Further analysis shows that SGPT-RL learned conserved scaffold patterns during exploration. Conclusions: The superior performance of SGPT-RL in the ACE2 task indicates that it can be applied to the virtual screening process where molecular docking is widely used as the criteria. Besides, the scaffold patterns learned by SGPT-RL during the exploration process can assist chemists to better design and discover novel lead candidates.",
        "keywords": "",
        "link": "http://dx.doi.org/10.12688/f1000research.130936.1"
    },
    {
        "id": 20683,
        "title": "Window-based transformer generative adversarial network for autonomous underwater image enhancement",
        "authors": "Mehnaz Ummar, Fayaz Ali Dharejo, Basit Alawode, Taslim Mahbub, Md. Jalil Piran, Sajid Javed",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107069"
    },
    {
        "id": 20684,
        "title": "Detection of Alzheimer's Disease Stages Using Pre-Trained Deep Learning Approaches",
        "authors": "Shruti Pallawi, Dushyant Kumar Singh",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccmla58983.2023.10346730"
    },
    {
        "id": 20685,
        "title": "An Analysis of Pre-trained Models Versus Custom Deep Learning Models for Forest Fire Detection",
        "authors": "Shouthiri Partheepan, Farzad Sanati, Jahan Hassan",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itnac59571.2023.10368557"
    },
    {
        "id": 20686,
        "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
        "authors": "Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song",
        "published": "2024-3-31",
        "citations": 24,
        "abstract": "Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3617680"
    },
    {
        "id": 20687,
        "title": "Automated multi-class classification of lung diseases from CXR-images using pre-trained convolutional neural networks",
        "authors": "Sahebgoud Hanamantray Karaddi, Lakhan Dev Sharma",
        "published": "2023-1",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.118650"
    },
    {
        "id": 20688,
        "title": "Afrikaans Literary Genre Recognition using Embeddings and Pre-Trained Multilingual Language Models",
        "authors": "Eduan Kotzé, Burgert A. Senekal",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467838"
    },
    {
        "id": 20689,
        "title": "Generative Pre-training Transformer Chat (ChatGPT) in the scientific community: the train has left the station",
        "authors": "Enrico CHECCUCCI, Paolo VERRI, Daniele AMPARORE, Giovanni E. CACCIAMANI, Cristian FIORI, Alberto BREDA, Francesco PORPIGLIA",
        "published": "2023-3",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23736/s2724-6051.23.05326-0"
    },
    {
        "id": 20690,
        "title": "BiPFT: Binary Pre-trained Foundation Transformer with Low-Rank Estimation of Binarization Residual Polynomials",
        "authors": "Xingrun Xing, Li Du, Xinyuan Wang, Xianlin Zeng, Yequan Wang, Zheng Zhang, Jiajun Zhang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Pretrained foundation models offer substantial benefits for a wide range of downstream tasks, which can be one of the most potential techniques to access artificial general intelligence. However, scaling up foundation transformers for maximal task-agnostic knowledge has brought about computational challenges, especially on resource-limited devices such as mobiles. This work proposes the first Binary Pretrained Foundation Transformer (BiPFT) for natural language understanding (NLU) tasks, which remarkably saves 56 times operations and 28 times memory. In contrast to previous task-specific binary transformers, BiPFT exhibits a substantial enhancement in the learning capabilities of binary neural networks (BNNs), promoting BNNs into the era of pre-training. Benefiting from extensive pretraining data, we further propose a data-driven binarization method. Specifically, we first analyze the binarization error in self-attention operations and derive the polynomials of binarization error. To simulate full-precision self-attention, we define binarization error as binarization residual polynomials, and then introduce low-rank estimators to model these polynomials. Extensive experiments validate the effectiveness of BiPFTs, surpassing task-specific baseline by 15.4% average performance on the GLUE benchmark. BiPFT also demonstrates improved robustness to hyperparameter changes, improved optimization efficiency, and reduced reliance on downstream distillation, which consequently generalize on various NLU tasks and simplify the downstream pipeline of BNNs. Our code and pretrained models are publicly available at https://github.com/Xingrun-Xing/BiPFT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i14.29542"
    },
    {
        "id": 20691,
        "title": "TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models",
        "authors": "Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei",
        "published": "2023-6-26",
        "citations": 34,
        "abstract": "Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26538"
    },
    {
        "id": 20692,
        "title": "Offline Pre-trained Multi-agent Decision Transformer",
        "authors": "Linghui Meng, Muning Wen, Chenyang Le, Xiyun Li, Dengpeng Xing, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, Yaodong Yang, Bo Xu",
        "published": "2023-4",
        "citations": 8,
        "abstract": "AbstractOffline reinforcement learning leverages previously collected offline datasets to learn optimal policies with no necessity to access the real environment. Such a paradigm is also desirable for multi-agent reinforcement learning (MARL) tasks, given the combinatorially increased interactions among agents and with the environment. However, in MARL, the paradigm of offline pre-training with online fine-tuning has not been studied, nor even datasets or benchmarks for offline MARL research are available. In this paper, we facilitate the research by providing large-scale datasets and using them to examine the usage of the decision transformer in the context of MARL. We investigate the generalization of MARL offline pre-training in the following three aspects: 1) between single agents and multiple agents, 2) from offline pretraining to online fine tuning, and 3) to that of multiple downstream tasks with few-shot and zero-shot capabilities. We start by introducing the first offline MARL dataset with diverse quality levels based on the StarCraftII environment, and then propose the novel architecture of multi-agent decision transformer (MADT) for effective offline learning. MADT leverages the transformer’s modelling ability for sequence modelling and integrates it seamlessly with both offline and online MARL tasks. A significant benefit of MADT is that it learns generalizable policies that can transfer between different types of agents under different task scenarios. On the StarCraft II offline dataset, MADT outperforms the state-of-the-art offline reinforcement learning (RL) baselines, including BCQ and CQL. When applied to online tasks, the pre-trained MADT significantly improves sample efficiency and enjoys strong performance in both few-short and zero-shot cases. To the best of our knowledge, this is the first work that studies and demonstrates the effectiveness of offline pre-trained models in terms of sample efficiency and generalizability enhancements for MARL.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11633-022-1383-7"
    },
    {
        "id": 20693,
        "title": "GS<sup>2</sup>P: A Generative Pre-trained Learning to Rank Model with Over-parameterization for Web-Scale Search",
        "authors": "Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dawei Yin",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302487"
    },
    {
        "id": 20694,
        "title": "SGCSumm: An extractive multi-document summarization method based on pre-trained language model, submodularity, and graph convolutional neural networks",
        "authors": "Alireza Ghadimi, Hamid Beigy",
        "published": "2023-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.119308"
    },
    {
        "id": 20695,
        "title": "Milo: Attacking Deep Pre-trained Model for Programming Languages Tasks with Anti-analysis Code Obfuscation",
        "authors": "Leo Song, Steven H.H. Ding",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00084"
    },
    {
        "id": 20696,
        "title": "Lung Cancer Classification Using Image Enhancement and CNN-Based Pre-Trained Models",
        "authors": "Farid Al-Areqi, Aysun Taşyapı Çelebi, Mehmet Zeki Konyar",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asyu58738.2023.10296677"
    },
    {
        "id": 20697,
        "title": "FastPacket: Towards Pre-trained Packets Embedding based on FastText for nextgeneration NIDS",
        "authors": "",
        "published": "2023-2-27",
        "citations": 0,
        "abstract": "New Attacks are increasingly used by attackers everyday but many of them are not detected by Intrusion Detection Systems as most IDS ignore raw packet information and only care about some basic statistical information extracted from PCAP files. Using networking programs to extract fixed statistical features from packets is good, but may not enough to detect nowadays challenges. We think that it is time to utilize big data and deep learning for automatic dynamic feature extraction from packets. It is time to get inspired by deep learning pre-trained models in computer vision and natural language processing, so security deep learning solutions will have its pre-trained models on big datasets to be used in future researches. In this paper, we proposed a new approach for embedding packets based on character-level embeddings, inspired by FastText success on text data. We called this approach FastPacket. Results are measured on subsets of CIC-IDS-2017 dataset, but we expect promising results on big data pre-trained models. We suggest building pre-trained FastPacket on MAWI big dataset and make it available to community, similar to FastText. To be able to outperform currently used NIDS, to start a new era of packet-level NIDS that can better detect complex attacks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jctcsr.02.01.07"
    },
    {
        "id": 20698,
        "title": "An End-to-End Autonomous Driving Pre-trained Transformer Model for Multi-Behavior-Optimal Trajectory Generation",
        "authors": "Zelin Qian, Kun Jiang, Weitao Zhou, Junze Wen, Cheng Jing, Zhong Cao, Diange Yang",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421847"
    },
    {
        "id": 20699,
        "title": "CPT: a pre-trained unbalanced transformer for both Chinese language understanding and generation",
        "authors": "Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Hang Yan, Fei Yang, Zhe Li, Hujun Bao, Xipeng Qiu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11432-021-3536-5"
    },
    {
        "id": 20700,
        "title": "Comparative assessment of common pre-trained CNNs for vision-based surface defect detection of machined components",
        "authors": "Swarit Anand Singh, Aitha Sudheer Kumar, K.A. Desai",
        "published": "2023-5",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119623"
    },
    {
        "id": 20701,
        "title": "Robotic Applications of Pre-Trained Vision-Language Models to Various Recognition Behaviors",
        "authors": "Kento Kawaharazuka, Yoshiki Obinata, Naoaki Kanazawa, Kei Okada, Masayuki Inaba",
        "published": "2023-12-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/humanoids57100.2023.10375211"
    },
    {
        "id": 20702,
        "title": "Comparative Analysis of Pre-Trained CNN Architectures for Multi-Class Weather Classification: A Study on Deep Learning Techniques",
        "authors": "Abdelrahman Ezzeldin Nagib, Habiba Mohamed, Abdelaziz Ashraf",
        "published": "2023-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217616"
    },
    {
        "id": 20703,
        "title": "DLP-personality detection: a text-based personality detection framework with psycholinguistic features and pre-trained features",
        "authors": "Hao Lin",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17015-z"
    },
    {
        "id": 20704,
        "title": "Combining multiple pre-trained models for hate speech detection in Bengali, Marathi, and Hindi",
        "authors": "Arpan Nandi, Kamal Sarkar, Arjun Mallick, Arkadeep De",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17934-x"
    }
]