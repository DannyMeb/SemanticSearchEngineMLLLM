[
    {
        "id": 31105,
        "title": "On Selecting Optimal Hyperparameters for Reinforcement Learning Based Robotics Applications: A Practical Approach",
        "authors": "Ignacio Fidalgo, Guillermo Villate, Alberto Tellaeche, Juan VÃ¡zquez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012158200003543"
    },
    {
        "id": 31106,
        "title": "SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering",
        "authors": "Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao, Cewu Lu",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.040"
    },
    {
        "id": 31107,
        "title": "Constrained footstep planning using model-based reinforcement learning in virtual constraint-based walking",
        "authors": "Takanori Jin, Taisuke Kobayashi, Takamitsu Matsubara",
        "published": "2024-4-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01691864.2024.2336253"
    },
    {
        "id": 31108,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 31109,
        "title": "Robot Arm Movement Control by Model-based Reinforcement Learning using Machine Learning Regression Techniques and Particle Swarm Optimization",
        "authors": "Meta Mueangprasert, Pisak Chermprayong, Kittipong Boonlong",
        "published": "2023-1-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ica-symp56348.2023.10044940"
    },
    {
        "id": 31110,
        "title": "Learning to Shape by Grinding: Cutting-Surface-Aware Model-Based Reinforcement Learning",
        "authors": "Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3303721"
    },
    {
        "id": 31111,
        "title": "Retracted: Reinforcement Learning-Based Continuous Action Space Path Planning Method for Mobile Robots",
        "authors": "Journal of Robotics",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2024/9819037"
    },
    {
        "id": 31112,
        "title": "Reinforcement learning in robotic motion planning by combined experience-based planning and self-imitation learning",
        "authors": "Sha Luo, Lambert Schomaker",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104545"
    },
    {
        "id": 31113,
        "title": "Deep Reinforcement Learning based Automated UAV Path Planning Model for Worker Safety Monitoring in Construction",
        "authors": "Dohyeong Kim, Yuna Jung, Dongmin Lee",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55785/jcar.2.4.17"
    },
    {
        "id": 31114,
        "title": "Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning",
        "authors": "Laura Smith, Ilya Kostrikov, Sergey Levine",
        "published": "2023-7-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.056"
    },
    {
        "id": 31115,
        "title": "Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers",
        "authors": "Yan Wang, Gautham Vasan, A. Rupam Mahmood",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160684"
    },
    {
        "id": 31116,
        "title": "Benchmarking the Full-Order Model Optimization Based Imitation in the Humanoid Robot Reinforcement Learning Walk",
        "authors": "Ekaterina Chaikovskaya, Inna Minashina, Vladimir Litvinenko, Egor Davydenko, Dmitry Makarov, Yulia Danik, Roman Gorbachev",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406818"
    },
    {
        "id": 31117,
        "title": "Growing Robot Navigation Based on Deep Reinforcement Learning",
        "authors": "Ahmad Ataka, Andreas P. Sandiwan",
        "published": "2023-4-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccar57134.2023.10151740"
    },
    {
        "id": 31118,
        "title": "On Mobile-Agent-based Swarm Reinforcement Learning in a Heterogeneous Robotic Network",
        "authors": "Gayathri Rangu, Shivashankar B. Nair",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3610419.3610499"
    },
    {
        "id": 31119,
        "title": "Reinforcement learning methods for network-based transfer parameter selection",
        "authors": "Yue Guo, Yu Wang, I-Hsuan Yang, Katia Sycara",
        "published": "2023-8-31",
        "citations": 1,
        "abstract": "A significant challenge in self-driving technology involves the domain-specific training of prediction models on intentions of other surrounding vehicles. Separately processing domain-specific models requires substantial human resources, time, and equipment for data collection and training. For instance, substantial difficulties arise when directly applying a prediction model developed with data from China to the United States market due to complex factors such as differing driving behaviors and traffic rules. The emergence of transfer learning seems to offer solutions, enabling the reuse of models and data to enhance prediction efficiency across international markets. However, many transfer learning methods require a comparison between source and target data domains to determine what can be transferred, a process that can often be legally restricted. A specialized area of transfer learning, known as network-based transfer, could potentially provide a solution. This approach involves pre-training and fine-tuning \"student\" models using selected parameters from a \"teacher\" model. However, as networks typically have a large number of parameters, it raises questions about the most efficient methods for parameter selection to optimize transfer learning. An automatic parameter selector through reinforcement learning has been developed in this paper, named \"Automatic Transfer Selector via Reinforcement Learning\". This technique enhances the efficiency of parameter selection for transfer prediction between international self-driving markets, in contrast to manual methods. With this innovative approach, technicians are relieved from the labor-intensive task of testing each parameter combination, or enduring lengthy training periods to evaluate the impact of prediction transfer. Experiments have been conducted using a temporal convolutional neural network fully trained with the data from the Chinese market and one month's US data, focusing on improving the training efficiency of specific driving scenarios in the US. Results show that the proposed approach significantly improves the prediction transfer process. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.20517/ir.2023.23"
    },
    {
        "id": 31120,
        "title": "Comparison of Model-Based and Model-Free Reinforcement Learning for Real-World Dexterous Robotic Manipulation Tasks",
        "authors": "David Valencia, John Jia, Raymond Li, Alex Hayashi, Megan Lecchi, Reuel Terezakis, Trevor Gee, Minas Liarokapis, Bruce A. MacDonald, Henry Williams",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160983"
    },
    {
        "id": 31121,
        "title": "Robotic assembly strategy via reinforcement learning based on force and visual information",
        "authors": "Kuk-Hyun Ahn, Minwoo Na, Jae-Bok Song",
        "published": "2023-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104399"
    },
    {
        "id": 31122,
        "title": "Expert Initialized Hybrid Model-Based and Model-Free Reinforcement Learning",
        "authors": "Jeppe Langaa, Christoffer Sloth",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178306"
    },
    {
        "id": 31123,
        "title": "Handling Sparse Rewards in Reinforcement Learning Using Model Predictive Control",
        "authors": "Murad Dawood, Nils Dengler, Jorge de Heuvel, Maren Bennewitz",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161492"
    },
    {
        "id": 31124,
        "title": "A collaborative siege method of multiple unmanned vehicles based on reinforcement learning",
        "authors": "Muqing Su, Ruimin Pu, Yin Wang, Meng Yu",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "A method based on multi-agent reinforcement learning is proposed to tackle the challenges to capture escaping Target by Unmanned Ground Vehicles (UGVs). Initially, this study introduces environment and motion models tailored for cooperative UGV capture, along with clearly defined success criteria for direct capture. An attention mechanism integrated into the Soft Actor-Critic (SAC) is leveraged, directing focus towards pivotal state features pertinent to the task while effectively managing less relevant aspects. This allows capturing agents to concentrate on the whereabouts and activities of the target agent, thereby enhancing coordination and collaboration during pursuit. This focus on the target agent aids in refining the capture process and ensures precise estimation of value functions. The reduction in superfluous activities and unproductive scenarios amplifies efficiency and robustness. Furthermore, the attention weights dynamically adapt to environmental shifts. To address constrained incentives arising in scenarios with multiple vehicles capturing targets, the study introduces a revamped reward system. It divides the reward function into individual and cooperative components, thereby optimizing both global and localized incentives. By facilitating cooperative collaboration among capturing UGVs, this approach curtails the action space of the target UGV, leading to successful capture outcomes. The proposed technique demonstrates enhanced capture success compared to previous SAC algorithms. Simulation trials and comparisons with alternative learning methodologies validate the effectiveness of the algorithm and the design approach of the reward function.",
        "keywords": "",
        "link": "http://dx.doi.org/10.20517/ir.2024.03"
    },
    {
        "id": 31125,
        "title": "Sim-to-Real Model-Based and Model-Free Deep Reinforcement Learning for Tactile Pushing",
        "authors": "Max Yang, Yijiong Lin, Alex Church, John Lloyd, Dandan Zhang, David A.W. Barton, Nathan F. Lepora",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3295236"
    },
    {
        "id": 31126,
        "title": "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation",
        "authors": "Gagan Khandate, Siqi Shang, Eric Chang, Tristan Saidi, Johnson Adams, Matei Ciocarlie",
        "published": "2023-7-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.020"
    },
    {
        "id": 31127,
        "title": "Data-Efficient Task Generalization via Probabilistic Model-Based Meta Reinforcement Learning",
        "authors": "Arjun Bhardwaj, Jonas Rothfuss, Bhavya Sukhija, Yarden As, Marco Hutter, Stelian Coros, Andreas Krause",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2024.3371260"
    },
    {
        "id": 31128,
        "title": "Reinforcement Learning-Based Optimal Multiple Waypoint Navigation",
        "authors": "Christos Vlachos, Panagiotis Rousseas, Charalampos P. Bechlioulis, Kostas J. Kyriakopoulos",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160725"
    },
    {
        "id": 31129,
        "title": "Deep reinforcement learning of event-triggered communication and consensus-based control for distributed cooperative transport",
        "authors": "Kazuki Shibata, Tomohiko Jimbo, Takamitsu Matsubara",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2022.104307"
    },
    {
        "id": 31130,
        "title": "A Study on Reinforcement Learning based Control of Quadcopter with a Cable-suspended Payload",
        "authors": "Pratik Prajapati, Atul Patidar, Vineet Vashista",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3610419.3610494"
    },
    {
        "id": 31131,
        "title": "Zeroth-Order Optimization Attacks on Deep Reinforcement Learning-Based Lane Changing Algorithms for Autonomous Vehicles",
        "authors": "Dayu Zhang, Nasser Azad, Sebastian Fischmeister, Stefan Marksteiner",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012187700003543"
    },
    {
        "id": 31132,
        "title": "Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models",
        "authors": "Yi Liu, Gaurav Datta, Ellen Novoseller, Daniel S. Brown",
        "published": "2023-5-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161081"
    },
    {
        "id": 31133,
        "title": "Quadrotor Auto Trimming based on Deep Reinforcement Learning and Digital Twin",
        "authors": "Sara Taheri, Alireza Abdollahi, Amin Rezaeizadeh",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrom60803.2023.10412557"
    },
    {
        "id": 31134,
        "title": "Path Planning of Unmanned Underwater Vehicles Based on Deep Reinforcement Learning Algorithm",
        "authors": "Yu Wang, Zhenzhong Chu, Yongli Hu",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icarm58088.2023.10218902"
    },
    {
        "id": 31135,
        "title": "Deep-reinforcement learning-based route planning with obstacle avoidance for autonomous vessels",
        "authors": "Ryosuke Saga, Rinto Kozono, Yutaro Tsurumi, Yasunori Nihei",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10015-023-00909-4"
    },
    {
        "id": 31136,
        "title": "Decentralized multi-agent reinforcement learning based on best-response policies",
        "authors": "Volker Gabler, Dirk Wollherr",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "Introduction: Multi-agent systems are an interdisciplinary research field that describes the concept of multiple decisive individuals interacting with a usually partially observable environment. Given the recent advances in single-agent reinforcement learning, multi-agent reinforcement learning (RL) has gained tremendous interest in recent years. Most research studies apply a fully centralized learning scheme to ease the transfer from the single-agent domain to multi-agent systems.Methods: In contrast, we claim that a decentralized learning scheme is preferable for applications in real-world scenarios as this allows deploying a learning algorithm on an individual robot rather than deploying the algorithm to a complete fleet of robots. Therefore, this article outlines a novel actorâcritic (AC) approach tailored to cooperative MARL problems in sparsely rewarded domains. Our approach decouples the MARL problem into a set of distributed agents that model the other agents as responsive entities. In particular, we propose using two separate critics per agent to distinguish between the joint task reward and agent-based costs as commonly applied within multi-robot planning. On one hand, the agent-based critic intends to decrease agent-specific costs. On the other hand, each agent intends to optimize the joint team reward based on the joint task critic. As this critic still depends on the joint action of all agents, we outline two suitable behavior models based on Stackelberg games: a game against nature and a dyadic game against each agent. Following these behavior models, our algorithm allows fully decentralized execution and training.Results and Discussion: We evaluate our presented method using the proposed behavior models within a sparsely rewarded simulated multi-agent environment. Although our approach already outperforms the state-of-the-art learners, we conclude this article by outlining possible extensions of our algorithm that future research may build upon.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/frobt.2024.1229026"
    },
    {
        "id": 31137,
        "title": "Study on force control for robot massage with a model-based reinforcement learning algorithm",
        "authors": "Meng Xiao, Tie Zhang, Yanbiao Zou, Xiaohu Yan, Wen Wu",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11370-023-00474-6"
    },
    {
        "id": 31138,
        "title": "Online 3D Bin Packing for Novel Objects Based on Deep Reinforcement Learning",
        "authors": "Shao-Yu Chien, Ching-Chang Wong",
        "published": "2023-8-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aris59192.2023.10268559"
    },
    {
        "id": 31139,
        "title": "Research on Unmanned Surface Vehicle Collision Avoidance Based on Deep Reinforcement Learning",
        "authors": "Hua Xiao, Qingnian Zhang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489693"
    },
    {
        "id": 31140,
        "title": "Robust Grasping of a Variable Stiffness Soft Gripper in High-Speed Motion Based on Reinforcement Learning",
        "authors": "Mingzhu Zhu, Junyue Dai, Yu Feng",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1089/soro.2022.0246"
    },
    {
        "id": 31141,
        "title": "Offline Feature-Based Reinforcement Learning with Preprocessed Image Inputs for Liquid Pouring Control",
        "authors": "Stephan Pareigis, Jesus Hermosilla-Diaz, Jeeangh Reyes-Montiel, Fynn MaaÃ, Helen Haase, Maximilian Mang, Antonio Marin-Hernandez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012235700003543"
    },
    {
        "id": 31142,
        "title": "An Adaptive Agent Decision Model Based on Deep Reinforcement  Learning and Autonomous Learning",
        "authors": "",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33168/jliss.2023.0309"
    },
    {
        "id": 31143,
        "title": "Obstacle Avoidance Based on Deep Reinforcement Learning and Artificial Potential Field",
        "authors": "Haoran Han, Zhilong Xi, Jian Cheng, Maolong Lv",
        "published": "2023-4-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccar57134.2023.10151771"
    },
    {
        "id": 31144,
        "title": "Addressing Challenges in Dynamic Modeling of Stewart Platform using Reinforcement Learning-Based Control Approach",
        "authors": "Hadi Yadavari, Vahid Tavakol Aghaei, Serhat Ä°kizoÄlu",
        "published": "2024-1-12",
        "citations": 0,
        "abstract": "In this paper, we focus on enhancing the performance of the controller utilized in the Stewart platform by investigating the dynamics of the platform. Dynamic modeling is crucial for control and simulation, yet challenging for parallel robots like the Stewart platform due to closed-loop kinematics. We explore classical methods to solve its inverse dynamical model, but conventional approaches face difficulties, often resulting in simplified and inaccurate models. To overcome this limitation, we propose a novel approach by replacing the classical feedforward inverse dynamic block with a reinforcement learning (RL) agent, which, to our knowledge, has not been tried yet in the context of the Stewart platform control. Our proposed methodology utilizes a hybrid control topology that combines RL with existing classical control topologies and inverse kinematic modeling. We leverage three deep reinforcement learning (DRL) algorithms and two model-based RL algorithms to achieve improved control performance, highlighting the versatility of the proposed approach. By incorporating the learned feedforward control topology into the existing PID controller, we demonstrate enhancements in the overall control performance of the Stewart platform. Notably, our approach eliminates the need for explicit derivation and solving of the inverse dynamic model, overcoming the drawbacks associated with inaccurate and simplified models. Through several simulations and experiments, we validate the effectiveness of our reinforcement learning-based control approach for the dynamic modeling of the Stewart platform. The results highlight the potential of RL techniques in overcoming the challenges associated with dynamic modeling in parallel robot systems, promising improved control performance. This enhances accuracy and reduces the development time of control algorithms in real-world applications. Nonetheless, it requires a simulation step before practical implementations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18196/jrc.v5i1.20582"
    },
    {
        "id": 31145,
        "title": "Learning to Draw Through A Multi-Stage Environment Model Based Reinforcement Learning",
        "authors": "Ji Qiu, Peng Lu, Xujun Peng",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222280"
    },
    {
        "id": 31146,
        "title": "Reinforcement learning based non-linear feedback synchronous control technology for permanent magnet synchronous motors",
        "authors": "Jia Li",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3011006"
    },
    {
        "id": 31147,
        "title": "Uncertainty-Aware Model-Based Offline Reinforcement Learning for Automated Driving",
        "authors": "Christopher Diehl, Timo Sebastian Sievernich, Martin Kruger, Frank Hoffmann, Torsten Bertram",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3236579"
    },
    {
        "id": 31148,
        "title": "Robotic Control Mechanism Based on Deep Reinforcement Learning",
        "authors": "Zhaoyan Pan, Junchao Zhou, Qianyi Fan, Zibin Feng, Xinlan Gao, Mu Su",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscer58777.2023.00018"
    },
    {
        "id": 31149,
        "title": "Nash Equilibrium Solution Based on Safety-Guarding Reinforcement Learning in Nonzero-Sum Game",
        "authors": "Junkai Tan, Shuangsi Xue, Hui Cao, Huan Li",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icarm58088.2023.10218910"
    },
    {
        "id": 31150,
        "title": "Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach",
        "authors": "Wenxing Liu, Hanlin Niu, Wei Pan, Guido Herrmann, Joaquin Carrasco",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161062"
    },
    {
        "id": 31151,
        "title": "A SAC-Based Deep Reinforcement Learning Approach for Autonomous Underwater Vehicle Combat",
        "authors": "Kai Zhang, Yang Xu, Junjie Zhu",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489144"
    },
    {
        "id": 31152,
        "title": "Automatic Cell Rotation Method Based on Deep Reinforcement Learning",
        "authors": "Huiying Gong, Yujie Zhang, Yaowei Liu, Qili Zhao, Xin Zhao, Mingzhu Sun",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161043"
    },
    {
        "id": 31153,
        "title": "Offâpolicy correction algorithm for double Q network based on deep reinforcement learning",
        "authors": "Qingbo Zhang, Manlu Liu, Heng Wang, Weimin Qian, Xinglang Zhang",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractA deep reinforcement learning (DRL) method based on the deep deterministic policy gradient (DDPG) algorithm is proposed to address the problems of a mismatch between the needed training samples and the actual training samples during the training of intelligence, the overestimation and underestimation of the existence of Qâvalues, and the insufficient dynamism of the intelligence policy exploration. This method introduces the ActorâCritic OffâPolicy Correction (ACâOffâPOC) reinforcement learning framework and an improved double Qâvalue learning method, which enables the value function network in the target task to provide a more accurate evaluation of the policy network and converge to the optimal policy more quickly and stably to obtain higher value returns. The method is applied to multiple MuJoCo tasks on the Open AI Gym simulation platform. The experimental results show that it is better than the DDPG algorithm based solely on the different policy correction framework (ACâOffâPOC) and the conventional DRL algorithm. The value of returns and stability of the doubleâQânetwork offâpolicy correction algorithm for the deep deterministic policy gradient (DCAOPâDDPG) proposed by the authors are significantly higher than those of other DRL algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/csy2.12102"
    },
    {
        "id": 31154,
        "title": "ARiADNE: A Reinforcement learning approach using Attention-based Deep Networks for Exploration",
        "authors": "Yuhong Cao, Tianxiang Hou, Yizhuo Wang, Xian Yi, Guillaume Sartoretti",
        "published": "2023-5-29",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160565"
    },
    {
        "id": 31155,
        "title": "Disturbance-aware reinforcement learning for rejecting excessive disturbances",
        "authors": "Wenjie Lu, Manman Hu",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2022.104341"
    },
    {
        "id": 31156,
        "title": "Developing multi-agent adversarial environment using reinforcement learning and imitation learning",
        "authors": "Ziyao Han, Yupeng Liang, Kazuhiro Ohkura",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10015-023-00912-9"
    },
    {
        "id": 31157,
        "title": "Intelligent decision-making system for multiple marine autonomous surface ships based on deep reinforcement learning",
        "authors": "Wei Guan, Wenzhe Luo, Zhewen Cui",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104587"
    },
    {
        "id": 31158,
        "title": "An AR-assisted Deep Reinforcement Learning-based approach towards mutual-cognitive safe human-robot interaction",
        "authors": "Chengxi Li, Pai Zheng, Yue Yin, Yat Ming Pang, Shengzeng Huo",
        "published": "2023-4",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rcim.2022.102471"
    },
    {
        "id": 31159,
        "title": "Density estimation based soft actor-critic: deep reinforcement learning for static output feedback control with measurement noise",
        "authors": "Ran Wang, Ye Tian, Kenji Kashima",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01691864.2024.2309621"
    },
    {
        "id": 31160,
        "title": "SofaGym: An Open Platform for Reinforcement Learning Based on Soft Robot Simulations",
        "authors": "Pierre Schegg, Etienne MÃ©nager, Elie Khairallah, Damien Marchal, JÃ©rÃ©mie Dequidt, Philippe Preux, Christian Duriez",
        "published": "2023-4-1",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1089/soro.2021.0123"
    },
    {
        "id": 31161,
        "title": "Coordinated Tuning of Slurry Shield Control Parameters based on Reinforcement Learning",
        "authors": "BingJian Wu, Jing Lu, Min Hu, Li Zhou, ShenRong Jiang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrae59816.2023.10458482"
    },
    {
        "id": 31162,
        "title": "Reinforcement Learning-Based Approach to Robot Path Tracking in Nonlinear Dynamic Environments",
        "authors": "Wei Chen, Zebin Zhou",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": " To address the issue of error-prone and unstable trajectory tracking and dynamic obstacle avoidance of mobile robots in locally observable nonlinear dynamic settings, a deep reinforcement learning (RL)-based visual perception, and decision-making system is proposed. The technique creates a closed loop between the systemâs environmental perception and decision-making capabilities by combining the perceptual capabilities of convolutional neural networks with the decision-making capabilities of RL in a generic form. It achieves direct output control from the visual perception input of the environment to the action through end-to-end learning. The simulation results show that this approach is capable of meeting the demands of multi-task intelligent perception and decision making. It also more effectively addresses issues with traditional algorithms, including their tendency to fall into local optimums, oscillate in groups of similar obstacles without recognizing the path, oscillate in tight spaces and inaccessible targets close to obstacles and significantly enhance real-time and adaptability of robot trajectory tracking and dynamic obstacle avoidance. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219843623500147"
    },
    {
        "id": 31163,
        "title": "A Reinforcement Learning Method for Quadrotor Attitude Control Based on Expert Information",
        "authors": "Yalu Zhu, Shikang Lian, WenTao Zhong, Wei Meng",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cacre58689.2023.10208497"
    },
    {
        "id": 31164,
        "title": "Flattening and folding towels with a single-arm robot based on reinforcement learning",
        "authors": "Hassan Shehawy, Daniele Pareyson, Virginia Caruso, Stefano De Bernardi, Andrea Maria Zanchettin, Paolo Rocco",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104506"
    },
    {
        "id": 31165,
        "title": "A model-based approach to meta-Reinforcement Learning: Transformers and tree search",
        "authors": "Brieuc Pinon, RaphaÃ«l Jungers, Jean-Charles Delvenne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-117"
    },
    {
        "id": 31166,
        "title": "MURM: Utilization of Multi-Views for Goal-Conditioned Reinforcement Learning in Robotic Manipulation",
        "authors": "Seongwon Jang, Hyemi Jeong, Hyunseok Yang",
        "published": "2023-8-19",
        "citations": 0,
        "abstract": "We present a novel framework, multi-view unified reinforcement learning for robotic manipulation (MURM), which efficiently utilizes multiple camera views to train a goal-conditioned policy for a robot to perform complex tasks. The MURM framework consists of three main phases: (i) demo collection from an expert, (ii) representation learning, and (iii) offline reinforcement learning. In the demo collection phase, we design a scripted expert policy that uses privileged information, such as Cartesian coordinates of a target and goal, to solve the tasks. We add noise to the expert policy to provide sufficient interactive information about the environment, as well as suboptimal behavioral trajectories. We designed three tasks in a Pybullet simulation environment, including placing an object in a desired goal position and picking up various objects that are randomly positioned in the environment. In the representation learning phase, we use a vector-quantized variational autoencoder (VQVAE) to learn a more structured latent representation that makes it feasible to train for RL compared to high-dimensional raw images. We train VQVAE models for each distinct camera view and define the best viewpoint settings for training. In the offline reinforcement learning phase, we use the Implicit Q-learning (IQL) algorithm as our baseline and introduce a separated Q-functions method and dropout method that can be implemented in multi-view settings to train the goal-conditioned policy with supervised goal images. We conduct experiments in simulation and show that the single-view baseline fails to solve complex tasks, whereas MURM is successful.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/robotics12040119"
    },
    {
        "id": 31167,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 31168,
        "title": "Behavioral Cloning Based Model Generation Method for Reinforcement Learning",
        "authors": "Dengmin Xiao, Bo Wang, Zhongqi Sun, Xiao He",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450935"
    },
    {
        "id": 31169,
        "title": "Model gradient: unified model and policy learning in model-based reinforcement learning",
        "authors": "Chengxing Jia, Fuxiang Zhang, Tian Xu, Jing-Cheng Pang, Zongzhang Zhang, Yang Yu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-3150-5"
    },
    {
        "id": 31170,
        "title": "Reinforcement Learning for Solving Control Problems in Robotics",
        "authors": "Askhat Diveev, Elena Sofronova, Sergey Konstantinov, Viktoria Moiseenko",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/engproc2023033029"
    },
    {
        "id": 31171,
        "title": "Deep Reinforcement Learning based Personalized Locomotion Planning for Lower-Limb Exoskeletons",
        "authors": "Javad K. Mehr, Eddie Guo, Mojtaba Akbari, Vivian K. Mushahwar, Mahdi Tavakoli",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161559"
    },
    {
        "id": 31172,
        "title": "Intelligent Control Method of Robotic Arm Follow-Up Based on Reinforcement Learning",
        "authors": "Lu Qian, Zhenyuan Dong, Qian Lin, Peng Zhang, Xiaohua Wang",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscer58777.2023.00015"
    },
    {
        "id": 31173,
        "title": "A Secure Reinforcement Learning Dynamic Obstacle Avoidance Method Based on SSA-TD3",
        "authors": "Gen Mi, Lei Zhang, Kai Zhao, Miao Cheng",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489307"
    },
    {
        "id": 31174,
        "title": "Deep Reinforcement Learning with Pedestrian Trajectory Prediction Model for Service Robot Navigation in Crowded Environments",
        "authors": "Shih-Hao Wang, Yu-Hsiung Wu, Tzuu-Hseng S. Li",
        "published": "2023-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aris59192.2023.10268571"
    },
    {
        "id": 31175,
        "title": "Adaptive Locomotion Control of Hexapod Robot Based on Deep Reinforcement Learning and Proprioception",
        "authors": "Ruiwen Li, Lei Wang, Yiyang Chen, Ping Ma",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rcae59706.2023.10398839"
    },
    {
        "id": 31176,
        "title": "A Reinforcement-Learning-Based Adaptive Sliding Mode Controller for Robotic Manipulators",
        "authors": "Di Luo, Zhiqin Cai, Da Jiang, Xiaolu Qiu, Haijun Peng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rcae59706.2023.10398851"
    },
    {
        "id": 31177,
        "title": "Designing Automation for Pickup and Delivery Tasks in Modern Warehouses Using Multi Agent Path Finding (MAPF) and Multi Agent Reinforcement Learning (MARL) Based Approaches",
        "authors": "Shambhavi Mishra, Rajendra Kumar Dwivedi",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "A warehouse pickup and delivery problem finds its solution using multi agent path finding (MAPF) approach. Also, the problem has been used to showcase the capabilities of the multi agent reinforcement learning (MARL). The warehouse pickup and delivery work needs the agent to pick up a requested item and successfully deliver it to the intended location within the warehouse. The problem has been solved based on two approaches that include single shot and lifelong problem solution. The single shot solution has the delivery as the final goal and thus once it reaches the delivery address, it stops whereas in case of lifelong, the agent needs to deliver the item which it had picked, deliver it to the required place and then again pick up new item until requests are satisfied. The strategy used by multi agent path finding (MAPF) approach aims at constructing collision free paths to reach the delivery location but in case of multi agent reinforcement learning (MARL), the agentsâ decision making tactics (or policies) are learned which are then used to help agents decide path to be followed based on environment state and agentâs position. The results show that the lifelong conflict based search (CBS) is a better option when the agents are less in number as in that case, the re-planning will take overall less time but when the agents are large in number then this re-planning can take very long to produce conflict free paths from source to goal nodes. In this case, shared experience action critic (SEAC) which is based on multi agent reinforcement learning (MARL) approach can be more efficient choice as it takes the current environment state to give the most suitable action for that time t. For this study the agents taken for learning are homogeneous in nature that can pickup and deliver any type of requested item. We can address the same pickup and delivery problem when the agents are not all same and differ in their capabilities and the type of item they can handle.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4108/airo.3449"
    },
    {
        "id": 31178,
        "title": "Path Planning for Robotic Arm Based on Reinforcement Learning under the Train",
        "authors": "Guanhao Xie, Duo Zhao, Qichao Tang, Muhua Zhang, Wenjie Zhao, Yewen Wang",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/robio58561.2023.10354783"
    },
    {
        "id": 31179,
        "title": "Safe reinforcement learning for high-speed autonomous racing",
        "authors": "Benjamin D. Evans, Hendrik W. Jordaan, Herman A. Engelbrecht",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cogr.2023.04.002"
    },
    {
        "id": 31180,
        "title": "A  Deep Reinforcement Learning Model-based Optimization Method for Graphic Design",
        "authors": "Qi Guo, Zhen Wang",
        "published": "2024-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v48i5.5295"
    },
    {
        "id": 31181,
        "title": "A Graph-Based Spatial-Temporal Deep Reinforcement Learning Model for Edge Caching",
        "authors": "Jiacheng Hou, Amiya Nayak",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436966"
    },
    {
        "id": 31182,
        "title": "Deep Reinforcement Learning Based Backstepping Control for Underactuated AUV",
        "authors": "Hongxuan Chen, Tiechao Bo, Shufeng Wang, Guoyuan Tang",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icarm58088.2023.10218400"
    },
    {
        "id": 31183,
        "title": "Cooperative Transport by Manipulators with Uncertainty-Aware Model-Based Reinforcement Learning",
        "authors": "Takumi Aotani, Taisuke Kobayashi",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sii58957.2024.10417389"
    },
    {
        "id": 31184,
        "title": "Cherry-Picking with Reinforcement Learning",
        "authors": "Yunchu Zhang, Liyiming Ke, Abhay Deshpande, Abhishek Gupta, Siddhartha Srinivasa",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.021"
    },
    {
        "id": 31185,
        "title": "A mixed perception-based human-robot collaborative maintenance approach driven by augmented reality and online deep reinforcement learning",
        "authors": "Changchun Liu, Zequn Zhang, Dunbing Tang, Qingwei Nie, Linqi Zhang, Jiaye Song",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rcim.2023.102568"
    },
    {
        "id": 31186,
        "title": "Civic education reform based on deep reinforcement learning model",
        "authors": "Dan Peng",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "Abstract\nThe integration of artificial intelligence technology into education is an inevitable trend of scientific progress and educational reform, and how to use artificial intelligence technology and ideological and political education reform is called a key research direction in the education sector. Aiming at the problems of cold start in personalized recommendation system, lack of interpretability of recommendation results, and ignoring the implicit features of the course for better acceptance of recommendation results by learners, the BPRMF model based on deep learning is proposed to be applied to the problem of recommendation of Civics and Political Science course, which not only models learnersâ preferences and combines with course attribute features to generate recommendation rating ranking list and provide personalized recommendation service. Then the study of Civics education reform is conducted, mainly analyzing the change in teaching methods based on big data, machine learning, and deep learning technologies to promote secondary school students. The performance of the BPRMF model is evaluated in comparison with the BPRMF model under different k values. It is concluded that the accuracy rate of the BPRMF model is 8.9%~12.01% higher than UBCF and 8.07%~10.26% higher than IBCF, but with the increase of k value, the recall rate will gradually pull away from other models and optimize the recommendation system to some extent. This study is beneficial to ideological education in the implementation process to better utilize the opportunities, meet the challenges, and develop efficiently.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2478/amns.2023.2.00417"
    },
    {
        "id": 31187,
        "title": "AR3n: A Reinforcement Learning-Based Assist-as-Needed Controller for Robotic Rehabilitation",
        "authors": "Shrey Pareek, Harris J Nisar, Thenkurussi Kesavadas",
        "published": "2024",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mra.2023.3282434"
    },
    {
        "id": 31188,
        "title": "Deep Reinforcement Learning-Based Control of Stewart Platform With Parametric Simulation in ROS and Gazebo",
        "authors": "Hadi Yadavari, Vahid Tavakol Aghaei, Serhat Ä°kizoÄlu",
        "published": "2023-6-1",
        "citations": 2,
        "abstract": "AbstractThe Stewart platform is an entirely parallel robot with mechanical differences from typical serial robotic manipulators, which has a wide application area ranging from flight and driving simulators to structural test platforms. This work concentrates on learning to control a complex model of the Stewart platform using state-of-the-art deep reinforcement learning (DRL) algorithms. In this regard, to enhance the reliability of the learning performance and to have a test bed capable of mimicking the behavior of the system completely, a precisely designed simulation environment is presented. Therefore, we first design a parametric representation for the kinematics of the Stewart platform in Gazebo and robot operating system (ROS) and integrate it with a Python class to conveniently generate the structures in simulation description format (SDF). Then, to control the system, we benefit from three DRL algorithms: the asynchronous advantage actorâcritic (A3C), the deep deterministic policy gradient (DDPG), and the proximal policy optimization (PPO) to learn the control gains of a proportional integral derivative (PID) controller for a given reaching task. We chose to apply these algorithms due to the Stewart platformâs continuous action and state spaces, making them well-suited for our problem, where exact controller tuning is a crucial task. The simulation results show that the DRL algorithms can successfully learn the controller gains, resulting in satisfactory control performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4056971"
    },
    {
        "id": 31189,
        "title": "Improving reinforcement learning based moving object grasping with trajectory prediction",
        "authors": "Binzhao Xu, Taimur Hassan, Irfan Hussain",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11370-023-00491-5"
    },
    {
        "id": 31190,
        "title": "Adaptive MIMO PID Control of a Wheel-Leg Manipulator by Considering the Slippage Based on Deep Reinforcement Learning",
        "authors": "A.H. Asadi, N. Nikseresht, A.H. Korayem",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrom60803.2023.10412351"
    },
    {
        "id": 31191,
        "title": "Sample-Efficient Goal-Conditioned Reinforcement Learning via Predictive Information Bottleneck for Goal Representation Learning",
        "authors": "Qiming Zou, Einoshin Suzuki",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161213"
    },
    {
        "id": 31192,
        "title": "Deep-Reinforcement-Learning-Based Path Planning for Industrial Robots Using Distance Sensors as Observation",
        "authors": "Teham Bhuiyan, Linh KÃ¤stner, Yifan Hu, Benno Kutschank, Jens Lambrecht",
        "published": "2023-4-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccre57112.2023.10155608"
    },
    {
        "id": 31193,
        "title": "Reinforcement Learning-Based Energy Management for Plug-in Hybrid Electric Vehicles",
        "authors": "Xiaobin Li, Yuxin Zhang, Yi Peng, Wenqing Zhang, Shiyue Zhang, Xuefang Li",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eecr56827.2023.10150124"
    },
    {
        "id": 31194,
        "title": "Distributed Optimal Scheduling of Integrated Energy Systems Based on Federated Reinforcement Learning",
        "authors": "Zhiqing Sun, Yifang Chen, Yi Xuan, Yuanzhong Chen, Yinan Lou",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10488950"
    },
    {
        "id": 31195,
        "title": "Deep Reinforcement Learning for Mapless Robot Navigation Systems",
        "authors": "Iure Rosa L. Oliveira, Alexandre S. BrandÃ£o",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lars/sbr/wre59448.2023.10333043"
    },
    {
        "id": 31196,
        "title": "Model-based Reinforcement Learning: A Survey",
        "authors": "Thomas M. Moerland, Joost Broekens, Aske Plaat, Catholijn M. Jonker",
        "published": "2023",
        "citations": 97,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/2200000086"
    },
    {
        "id": 31197,
        "title": "Solving Stabilize-Avoid via Epigraph Form Optimal Control using Deep Reinforcement Learning",
        "authors": "Oswin So, Chuchu Fan",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.085"
    },
    {
        "id": 31198,
        "title": "A Temporal Action Detection Model Based on Deep Reinforcement Learning",
        "authors": "Zhaojia Han, Kan Li, Shaojie Qu",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsai61474.2023.10423353"
    },
    {
        "id": 31199,
        "title": "Reader: Model-based language-instructed reinforcement learning",
        "authors": "Nicola Dainese, Pekka Marttinen, Alexander Ilin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.1032"
    },
    {
        "id": 31200,
        "title": "Deep Reinforcement Learning Based Pushing and Grasping Model with Frequency Domain Mapping and Supervised Learning",
        "authors": "Weiliang Cao, Zhenwei Cao, Yong Song",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/oncon60463.2023.10430838"
    },
    {
        "id": 31201,
        "title": "A Reinforcement Learning Based RecommendationSystem to Improve Performance of Students in Outcome Based Education Model",
        "authors": "Mustafa Bin Tariq, Hafiz Adnan Habib",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3370852"
    },
    {
        "id": 31202,
        "title": "Motion Planning using Reinforcement Learning for Serial Manipulators",
        "authors": "Vedant V Chakravadhanula, Tejal Agarwal, Shyamanta M Hazarika",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3610419.3610420"
    },
    {
        "id": 31203,
        "title": "Efficient Policy Learning for General Robotic Tasks with Adaptive Dual-memory Hindsight Experience Replay Based on Deep Reinforcement Learning",
        "authors": "Menghua Dong, Fengkang Ying, Xiangjian Li, Huashan Liu",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrca57894.2023.10087824"
    },
    {
        "id": 31204,
        "title": "SoLo T-DIRL: Socially-Aware Dynamic Local Planner based on Trajectory-Ranked Deep Inverse Reinforcement Learning",
        "authors": "Yifan Xu, Theodor Chakhachiro, Tribhi Kathuria, Maani Ghaffari",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160536"
    },
    {
        "id": 31205,
        "title": "Reinforcement-Learning Based Robotic Assembly of Fractured Objects Using Visual and Tactile Information",
        "authors": "Xinchao Song, Nikolas Lamb, Sean Banerjee, Natasha Kholgade Banerjee",
        "published": "2023-2-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icara56516.2023.10125938"
    },
    {
        "id": 31206,
        "title": "A Novel ActorâCritic Motor Reinforcement Learning for Continuum Soft Robots",
        "authors": "Luis Pantoja-Garcia, Vicente Parra-Vega, Rodolfo Garcia-Rodriguez, Carlos Ernesto VÃ¡zquez-GarcÃ­a",
        "published": "2023-10-9",
        "citations": 1,
        "abstract": "Reinforcement learning (RL) is explored for motor control of a novel pneumatic-driven soft robot modeled after continuum media with a varying density. This model complies with closed-form Lagrangian dynamics, which fulfills the fundamental structural property of passivity, among others. Then, the question arises of how to synthesize a passivity-based RL model to control the unknown continuum soft robot dynamics to exploit its inputâoutput energy properties advantageously throughout a reward-based neural network controller. Thus, we propose a continuous-time ActorâCritic scheme for tracking tasks of the continuum 3D soft robot subject to Lipschitz disturbances. A reward-based temporal difference leads to learning with a novel discontinuous adaptive mechanism of Critic neural weights. Finally, the reward and integral of the Bellman error approximation reinforce the adaptive mechanism of Actor neural weights. Closed-loop stability is guaranteed in the sense of Lyapunov, which leads to local exponential convergence of tracking errors based on integral sliding modes. Notably, it is assumed that dynamics are unknown, yet the control is continuous and robust. A representative simulation study shows the effectiveness of our proposal for tracking tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/robotics12050141"
    },
    {
        "id": 31207,
        "title": "GAC: A deep reinforcement learning model toward user incentivization in unknown social networks",
        "authors": "Shiqing Wu, Weihua Li, Quan Bai",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.110060"
    },
    {
        "id": 31208,
        "title": "Evaluating Reliability and Economics of EV Charging Configurations and Deep Reinforcement Learning in Robotics and Autonomy",
        "authors": "Chandru Lin",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "Growing EV popularity drives companies to focus on reliable charging station designs despite challenges in maintaining reliability. A proposed 36-ported design combines uniform and non-uniform port arrangements, tested with 50-350 kW systems. Failure rates are estimated using MILHDBK217F and MILHBK-338B standards, assessing port reliability and station success rates through binomial distribution and cost analysis. This design improves voltage stability and reduces maintenance costs through enhanced port reliability. In robotics and autonomous systems, Deep Reinforcement Learning (DRL) excels but faces challenges from unsafe policies leading to hazardous decisions. This study introduces a reliability assessment framework for DRL-controlled systems, using formal neural network analysis. A two-level verification approach evaluates safety locally using reachability tools and globally by aggregating local safety metrics across tasks. Experimental validation confirms the framework's effectiveness in enhancing RAS safety.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.32388/pqujel"
    },
    {
        "id": 31209,
        "title": "Reinforcement Learning with Goal Relabeling and Dynamic Model for Robotic Tasks",
        "authors": "Kun Dong, Yongle Luo, Yuxin Wang, Shan Fang, Yu Liu, Erkang Cheng, Zhiyong Sun, Qiang Zhang, Bo Song",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rcar58764.2023.10250060"
    },
    {
        "id": 31210,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 31211,
        "title": "Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning",
        "authors": "David Brandfonbrener, Stephen Tu, Avi Singh, Stefan Welker, Chad Boodoo, Nikolai Matni, Jake Varley",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161096"
    },
    {
        "id": 31212,
        "title": "Model-based variable impedance learning control for robotic manipulation",
        "authors": "Akhil S. Anand, Jan Tommy Gravdahl, Fares J. Abu-Dakka",
        "published": "2023-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104531"
    },
    {
        "id": 31213,
        "title": "Reinforcement Learning based model for Maximizing Operator's Profit in Open-RAN",
        "authors": "Mahdi Sharara, Sahar Hoteit, VÃ©ronique VÃ¨que",
        "published": "2023-5-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/noms56928.2023.10154452"
    },
    {
        "id": 31214,
        "title": "A 3D/2D Coronary Artery Registration Method Based on Deep Reinforcement Learning",
        "authors": "Jun Yan, Zhuang Fu, Zeyu Fu, Yisheng Guan",
        "published": "2023-3-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccr56747.2023.10193905"
    },
    {
        "id": 31215,
        "title": "Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning",
        "authors": "Zhichao Chen, Yutaka Nakamura, Hiroshi Ishiguro",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3267385"
    },
    {
        "id": 31216,
        "title": "Reinforcement learning for swarm robotics: An overview of applications, algorithms and simulators",
        "authors": "Marc-AndrÄ Blais, Moulay A. Akhloufi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cogr.2023.07.004"
    },
    {
        "id": 31217,
        "title": "Simulation of Cross-border E-commerce Supply Chain Coordination Decision Model Based on Reinforcement Learning Algorithm",
        "authors": "Jiamin Liu",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnetic59568.2023.00095"
    },
    {
        "id": 31218,
        "title": "Modeling and Simulation of High Frequency and High Precision Multi Axis Cooperative Control Based on Reinforcement Learning",
        "authors": "Haiyu Wang, Zhuang Fu, Jun Yan, Zuohao Hua, Yisheng Guan",
        "published": "2023-3-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccr56747.2023.10193913"
    },
    {
        "id": 31219,
        "title": "Inverse Reinforcement Learning with Attention-based Feature Extraction from Video Demonstrations",
        "authors": "Weixin Zhang, Tao Lu, Yinghao Cai, Shuo Wang",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/robio58561.2023.10354995"
    },
    {
        "id": 31220,
        "title": "Evaluating Optimization Approaches for Deep-Reinforcement-Learning-based Navigation Agents",
        "authors": "Linh KÃ¤stner, Liam Roberts, Teham Bhuiyan, Jens Lambrecht",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cis-ram55796.2023.10370762"
    },
    {
        "id": 31221,
        "title": "Parallel Reinforcement Learning Simulation for Visual Quadrotor Navigation",
        "authors": "Jack Saunders, Sajad Saeedi, Wenbin Lil",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160675"
    },
    {
        "id": 31222,
        "title": "Deep reinforcement learning for continuous-time self-triggered control with experimental evaluation",
        "authors": "Ran Wang, Kenji Kashima",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01691864.2023.2229886"
    },
    {
        "id": 31223,
        "title": "Video Summarization Generation Model Based on Transformer and Deep Reinforcement Learning",
        "authors": "Guangli Wu, Shanshan Song, Leiting Li",
        "published": "2023-4-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccs57501.2023.10150725"
    },
    {
        "id": 31224,
        "title": "Vessel-following model for inland waterways based on deep reinforcement learning",
        "authors": "Fabian Hart, Ostap Okhrin, Martin Treiber",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.114679"
    },
    {
        "id": 31225,
        "title": "Deep Reinforcement Learning-Based Operation of Distribution Systems Using Surrogate Model",
        "authors": "Van-Hai Bui, Sina Zarrabian, Wencong Su",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10253401"
    },
    {
        "id": 31226,
        "title": "Deep Reinforcement Learning Model for Stock Portfolio Management Based on Data Fusion",
        "authors": "Haifeng Li, Mo Hai",
        "published": "2024-3-17",
        "citations": 0,
        "abstract": "AbstractDeep reinforcement learning (DRL) can be used to extract deep features that can be incorporated into reinforcement learning systems to enable improved decision-making; DRL can therefore also be used for managing stock portfolios. Traditional methods cannot fully exploit the advantages of DRL because they are generally based on real-time stock quotes, which do not have sufficient features for making comprehensive decisions. In this study, in addition to stock quotes, we introduced stock financial indices as additional stock features. Moreover, we used Markowitz mean-variance theory for determining stock correlation. A three-agent deep reinforcement learning model called Collaborative Multi-agent reinforcement learning-based stock Portfolio management System (CMPS) was designed and trained based on fused data. In CMPS, each agent was implemented with a deep Q-network to obtain the features of time-series stock data, and a self-attention network was used to combine the output of each agent. We added a risk-free asset strategy to CMPS to prevent risks and referred to this model as CMPS-Risk Free (CMPS-RF). We conducted experiments under different market conditions using the stock data of China Shanghai Stock Exchange 50 and compared our model with the state-of-the-art models. The results showed that CMPS could obtain better profits than the compared benchmark models, and CMPS-RF was able to accurately recognize the market risk and achieved the best Sharpe and Calmar ratios. The study findings are expected to aid in the development of an efficient investment-trading strategy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11582-4"
    },
    {
        "id": 31227,
        "title": "Graphic Design Optimization Method Based on Deep Reinforcement Learning Model",
        "authors": "Jiwen Zhang",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "Abstract\nThis paper used a new interior graphic modeling research based on CAD and depth enhancement teaching models. A massive database for graphic design has been established. An optimization method is proposed based on intelligent decision making, intelligent monitoring, panoramic vision, professional cooperation and intelligent planning. This system can make many systems of different dimensions share and integrate horizontally. The graphic design of CAD is introduced into 3D CAD. The Boolean method is introduced into the smooth grid instruction to obtain the smooth surface of the target surface. Combining the object of plane decomposition with other geometric shapes by form-fitting instruction achieves object control. Experiments show the effectiveness of the method. The system has good running performance, stability and safety.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2478/amns.2023.1.00309"
    },
    {
        "id": 31228,
        "title": "Robotics with Multi-Fingered Grippers and Deep Reinforcement Learning in Unity",
        "authors": "Jwu-Sheng Hu, Li-Jing Zheng",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cacs60074.2023.10326170"
    },
    {
        "id": 31229,
        "title": "Reinforcement Learning-Based Model-Free Controller for Feedback Stabilization of Robotic Systems",
        "authors": "Rupam Singh, Bharat Bhushan",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3137548"
    },
    {
        "id": 31230,
        "title": "ACapsule Q-Learning Based Reinforcement Model for Intrusion Detection System on Smart Grid",
        "authors": "Tala Talaei Khoei, Naima Kaabouch",
        "published": "2023-5-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eit57321.2023.10187374"
    },
    {
        "id": 31231,
        "title": "Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms",
        "authors": "Resul Dagdanov, Halil Durmus, Nazim Kemal Ure",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160883"
    },
    {
        "id": 31232,
        "title": "DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning",
        "authors": "I Made Aswin Nahrendra, Byeongho Yu, Hyun Myung",
        "published": "2023-5-29",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161144"
    },
    {
        "id": 31233,
        "title": "Deep Reinforcement Learning-based Task Assignment and Path Planning for Multi-agent Construction Robots",
        "authors": "Xinghui Xu, Borja Garcia de Soto",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22260/icra2023/0008"
    },
    {
        "id": 31234,
        "title": "Learning-Based Energy Consumption Model of Machining Processes Using Gaussian Process Regression",
        "authors": "Alicia Soto Bono, Alan McGibney, Susan Rea, Kritchai Witheephanich",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012180100003543"
    },
    {
        "id": 31235,
        "title": "Model Extraction Attacks Against Reinforcement Learning Based Controllers",
        "authors": "Momina Sajid, Yanning Shen, Yasser Shoukry",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383661"
    },
    {
        "id": 31236,
        "title": "Model-Based Deep Reinforcement Learning with Traffic Inference for Traffic Signal Control",
        "authors": "Hao Wang, Jinan Zhu, Bao Gu",
        "published": "2023-3-21",
        "citations": 6,
        "abstract": "In the modern world, the extremely rapid growth of traffic demand has become a major problem for urban traffic development. Continuous optimization of signal control systems is an important way to relieve traffic pressure in cities. In recent years, with the impressive development of deep reinforcement learning (DRL), some DRL approaches have started to be applied to traffic signal control. Unlike traditional signal control methods, agents trained using DRL approaches continuously receive feedback from the environment to continuously improve the policy. Since current research in the field is more focused on the performance of the agent, data efficiency during training is ignored to some extent. However, in traffic signal control tasks, the cost of trial and error is very expensive. In this paper, we propose a DRL approach based on a traffic inference model. The proposed traffic inference model is based on the future information given based on upstream intersections and data from the environment to continuously learn the changing patterns of the traffic environment in order to make inferences about changes in the traffic environment. In the proposed algorithm, the inference model interacts with the agent instead of the environment. Through comprehensive experiments based on realistic datasets, we demonstrate that our proposed algorithm is superior to other algorithms in terms of its data efficiency and stronger performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13064010"
    },
    {
        "id": 31237,
        "title": "Reinforcement learning with parameterized action space and sparse reward for UAV navigation",
        "authors": "Shiying Feng, Xiaofeng Li, Lu Ren, Shuiqing Xu",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "Autonomous navigation of unmanned aerial vehicles (UAVs) is widely used in building rescue systems. As the complexity of the task increases, traditional methods based on environment models are hard to apply. In this paper, a reinforcement learning (RL) algorithm is proposed to solve the UAV navigation problem. The UAV navigation task is modeled as a Markov Decision Process (MDP) with parameterized actions. In addition, the sparse reward problem is also taken into account. To address these issues, we develop the HER-MPDQN by combining Multi-Pass Deep Q-Network (MP-DQN) and Hindsight Experience Replay (HER). Two UAV navigation simulation environments with progressive difficulty are constructed to evaluate our method. The results show that HER-MPDQN outperforms other baselines in relatively simple tasks. Especially for complex tasks involving relay operations, only our method can achieve satisfactory performance. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.20517/ir.2023.10"
    },
    {
        "id": 31238,
        "title": "GAN-Based Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",
        "authors": "Jie Huang, Jiangshan Hao, Rongshun Juan, Randy Gomez, Keisuke Nakamura, Guangliang Li",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160939"
    },
    {
        "id": 31239,
        "title": "OCSRL: An Model-Based Reinforcement Learning Approach to Optimize Energy Consumption of Cooling Systems",
        "authors": "Keshav Kaushik, Vinayak Naik",
        "published": "2023-10-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/e-science58273.2023.10254663"
    },
    {
        "id": 31240,
        "title": "Model-Based Reinforcement Learning via Stochastic Hybrid Models",
        "authors": "Hany Abdulsamad, Jan Peters",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ojcsys.2023.3277308"
    },
    {
        "id": 31241,
        "title": "A Study and Application of Planning Model Based on Principal Component Analysis and Reinforcement Learning",
        "authors": "Yurou Xiong",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257781"
    },
    {
        "id": 31242,
        "title": "Controlling Tiltrotors Unmanned Aerial Vehicles (UAVs) with Deep Reinforcement Learning",
        "authors": "Aline Gabriel De Almeida, Esther Luna Colombini, Alexandre Da Silva SimÃµes",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lars/sbr/wre59448.2023.10333034"
    },
    {
        "id": 31243,
        "title": "Autonomous Navigation of Wheelchairs in Indoor Environments using Deep Reinforcement Learning and Computer Vision",
        "authors": "Paulo De Almeida Afonso, Paulo Roberto Ferreira",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lars/sbr/wre59448.2023.10332918"
    },
    {
        "id": 31244,
        "title": "Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control",
        "authors": "Jiayu Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160374"
    },
    {
        "id": 31245,
        "title": "Autonomous Micro-Grids: A Reinforcement Learning-Based Energy Management Model in Smart Cities",
        "authors": "Erol Ãzkan, Ä°brahim KÃ¶k, Suat ÃzdemÄ±r",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isncc58260.2023.10323891"
    },
    {
        "id": 31246,
        "title": "Model predictive controlâbased value estimation for efficient reinforcement learning",
        "authors": "Qizhen Wu, Kexin Liu, Lei Chen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mis.2024.3386204"
    },
    {
        "id": 31247,
        "title": "Transmission Control in NB-IoT With Model-Based Reinforcement Learning",
        "authors": "Juan J. Alcaraz, Fernando Losilla, Francisco-Javier Gonzalez-CastaÃ±o",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3284990"
    },
    {
        "id": 31248,
        "title": "Towards AI-Controlled Movement Restoration: Learning FES-Cycling Stimulation with Reinforcement Learning",
        "authors": "Nat Wannawas, A. Aldo Faisal",
        "published": "2023-9-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icorr58425.2023.10304767"
    },
    {
        "id": 31249,
        "title": "Application of fuzzy reinforcement learning in IoT-based robotics for autonomous navigation and collision avoidance",
        "authors": "V. Sitharamulu, D. Mahammad Rafi, Janardhan Naulegari, Hanumantha Rao Battu",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "In this study, we investigate the viability of applying fuzzy reinforcement learning (FRL) to Internet of Things-based robots for purposes of autonomous navigation and collision avoidance. The proposed approach utilises FRL, IoT, and a sensor network to give the robot the ability to learn from its environment and act in accordance with those policies. The authors used FRL to train a mobile robot with wheels to move around and avoid obstacles, and then they put the robot through its paces in a virtual world. Results showed that the FRL-based technique improved the robotâs navigation and collision avoidance performance compared to traditional rule-based approaches. The results of this study indicate that FRL may be a viable technique for enabling autonomous navigation and obstacle avoidance in IoT-based robotics.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/jifs-233860"
    },
    {
        "id": 31250,
        "title": "Digital twin for autonomous collaborative robot by using synthetic data and reinforcement learning",
        "authors": "Dongjun Kim, Minho Choi, Jumyung Um",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rcim.2023.102632"
    },
    {
        "id": 31251,
        "title": "Integrating Mechanism and Data: Reinforcement Learning Based on Multi-Fidelity Model for Data Center Cooling Control",
        "authors": "Ni Mu, Xiao Hu, Qing-Shan Jia",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450959"
    },
    {
        "id": 31252,
        "title": "A Reinforcement Learning Toolkit for Quadruped Robots With Pybullet",
        "authors": "Fuchun Liu, Suchen Zhang",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isrimt59937.2023.10428509"
    },
    {
        "id": 31253,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago SimÃ£o, Nils Jansen, Guillermo PÃ©rez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 31254,
        "title": "A UAV Cluster Goods Delivery Task Assignment Method Based on Graph Matching and Reinforcement Learning",
        "authors": "Yu Han, Xiaolei Wang, Shangyong Zhang, Jianhao Guo, Yi Zhang",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3632971.3633040"
    },
    {
        "id": 31255,
        "title": "Visual navigation for mobile robots based on deep reinforcement learning",
        "authors": "Pengfei Lian, Liang Yuan, Lihui Sun",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rcar58764.2023.10249586"
    },
    {
        "id": 31256,
        "title": "Subgoal-Driven Navigation in Dynamic Environments Using Attention-Based Deep Reinforcement Learning",
        "authors": "Jorge De Heuvel, Weixian Shi, Xiangyu Zeng, Maren Bennewitz",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406349"
    },
    {
        "id": 31257,
        "title": "Reinforcement Learning for Imbalanced Vehicle Booming Noise Classification",
        "authors": "Jenny Yang, Deepti S. Kunte, Bram Cornelis, David A. Clifton",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mlcr61158.2023.00014"
    },
    {
        "id": 31258,
        "title": "Chasing the Intruder: A Reinforcement Learning Approach for Tracking Unidentified Drones",
        "authors": "Shivam Kainth, Subham Sahoo, Rajtilak Pal, Shashi Shekhar Jha",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3610419.3610487"
    },
    {
        "id": 31259,
        "title": "Reinforcement Learning Based Energy Consolidation Model for Efficient Cloud Computing System",
        "authors": "",
        "published": "2023-1-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18576/amis/170109"
    },
    {
        "id": 31260,
        "title": "TEAMSTER: Model-based reinforcement learning for ad hoc teamwork",
        "authors": "JoÃ£o G. Ribeiro, GonÃ§alo Rodrigues, Alberto Sardinha, Francisco S. Melo",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104013"
    },
    {
        "id": 31261,
        "title": "A LARGE-SCALE PATH PLANNING ALGORITHM FOR UNDERWATER ROBOTS BASED ON DEEP REINFORCEMENT LEARNING, 204-210.",
        "authors": "Wenhui Wang, Leqing Li, Fumeng Ye, Yumin Peng, Yiming Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2316/j.2024.206-1035"
    },
    {
        "id": 31262,
        "title": "Deep Reinforcement Learning-Based Control of Bicycle Robots on Rough Terrain",
        "authors": "Xianjin Zhu, Xudong Zheng, Yang Deng, Zhang Chen, Bin Liang, Yu Liu",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccar57134.2023.10151769"
    },
    {
        "id": 31263,
        "title": "Spatio-Temporal Transformer-Based Reinforcement Learning for Robot Crowd Navigation",
        "authors": "Haodong He, Hao Fu, Qiang Wang, Shuai Zhou, Wei Liu, Yang Chen",
        "published": "2023-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/robio58561.2023.10355042"
    },
    {
        "id": 31264,
        "title": "Towards Autonomous Robotic Ultrasound Scanning Using the Reinforcement Learning-Based Volumetric Data Navigation Method",
        "authors": "Cuifeng Shen, Zhaokun Deng, Jinzhi Wang, Shuangyi Wang, Chen Chen",
        "published": "2023-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wrcsara60131.2023.10261822"
    },
    {
        "id": 31265,
        "title": "An Advisor-Based Architecture for a Sample-Efficient Training of Autonomous Navigation Agents with Reinforcement Learning",
        "authors": "Rukshan Darshana Wijesinghe, Dumindu Tissera, Mihira Kasun Vithanage, Alex Xavier, Subha Fernando, Jayathu Samarawickrama",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "Recent advancements in artificial intelligence have enabled reinforcement learning (RL) agents to exceed human-level performance in various gaming tasks. However, despite the state-of-the-art performance demonstrated by model-free RL algorithms, they suffer from high sample complexity. Hence, it is uncommon to find their applications in robotics, autonomous navigation, and self-driving, as gathering many samples is impractical in real-world hardware systems. Therefore, developing sample-efficient learning algorithms for RL agents is crucial in deploying them in real-world tasks without sacrificing performance. This paper presents an advisor-based learning algorithm, incorporating prior knowledge into the training by modifying the deep deterministic policy gradient algorithm to reduce the sample complexity. Also, we propose an effective method of employing an advisor in data collection to train autonomous navigation agents to maneuver physical platforms, minimizing the risk of collision. We analyze the performance of our methods with the support of simulation and physical experimental setups. Experiments reveal that incorporating an advisor into the training phase significantly reduces the sample complexity without compromising the agentâs performance compared to various benchmark approaches. Also, they show that the advisorâs constant involvement in the data collection process diminishes the agentâs performance, while the limited involvement makes training more effective.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/robotics12050133"
    },
    {
        "id": 31266,
        "title": "Deep reinforcement learning in mobile robotics â a concise review",
        "authors": "Rayadurga Gnana Prasuna, Sudharsana Rao Potturu",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18152-9"
    },
    {
        "id": 31267,
        "title": "Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learningâ",
        "authors": "Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.01.076"
    },
    {
        "id": 31268,
        "title": "Experimental evaluations of model-based reinforcement learning combined with MPC",
        "authors": "Qiming Zhang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nModel-based reinforcement learning has reached outstanding effects in control and decision areas with high sample efficiency compared with model-free algorithms. However, how to combine learning and planning, aiming at improving effect and reducing the cost of planning at the same time seems to be challenging in the model-based area. In this paper, several state-of-the-art model-based algorithms combined with MPC: TD-MPC, PlaNet and Dreamer, are discussed and compared. Theoretical background and features of these methods are analyzed based on the continuous control environments. This work also covers how to design model-based method for different tasks and future directions for improving performance for model-based reinforcement learning research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2646/1/012008"
    },
    {
        "id": 31269,
        "title": "Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games",
        "authors": "Yuling Yan, Gen Li, Yuxin Chen, Jianqing Fan",
        "published": "2024-4-2",
        "citations": 0,
        "abstract": " This paper makes progress toward learning Nash equilibria in two-player, zero-sum Markov games from offline data. Despite a large number of prior works tackling this problem, the state-of-the-art results suffer from the curse of multiple agents in the sense that their sample complexity bounds scale linearly with the total number of joint actions. The current paper proposes a new model-based algorithm, which provably finds an approximate Nash equilibrium with a sample complexity that scales linearly with the total number of individual actions. This work also develops a matching minimax lower bound, demonstrating the minimax optimality of the proposed algorithm for a broad regime of interest. An appealing feature of the result lies in algorithmic simplicity, which reveals the unnecessity of sophisticated variance reduction and sample splitting in achieving sample optimality. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/opre.2022.0342"
    },
    {
        "id": 31270,
        "title": "Cooperative transportation of tether-suspended payload via quadruped robots based on deep reinforcement learning",
        "authors": "Hongwu Zhu, Shunzhe Yang, Weiqi Wang, Xuchun He, Ning Ding",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/robio58561.2023.10354782"
    },
    {
        "id": 31271,
        "title": "Game confrontation model of USV based on multi-agent reinforcement learning",
        "authors": "Kai Liu, Xiao Wang, ChunLei Wang",
        "published": "2023-8-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccis59572.2023.10263075"
    },
    {
        "id": 31272,
        "title": "Mobile Robot Path Planning Using Deep reinforcement learning",
        "authors": "Ali Abedi, Reza Ghaderizadeh Anari, Hossein Mohammadi",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrom60803.2023.10412450"
    },
    {
        "id": 31273,
        "title": "Model-Free HVAC Optimizer based on Reinforcement Learning",
        "authors": "Charalampos Marantos, Christos Lamprakos, Kostas Siozios, Dimitrios Soudris",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isie51358.2023.10227979"
    },
    {
        "id": 31274,
        "title": "Model-free Maneuvering Control of Fixed-Wing UAVs Based on Deep Reinforcement Learning",
        "authors": "Wenya Wei, Zhou Fang, Yiwen Zhu",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1072"
    },
    {
        "id": 31275,
        "title": "Model-Free Optimal Control Based on Reinforcement Learning for Rotary Inverted Pendulum",
        "authors": "Eduardo Yudho, Xiaoou Li, Brisbane Ovilla-MartÃ­nez, Wen Yu",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10372059"
    },
    {
        "id": 31276,
        "title": "Mobile User Interface Adaptation Based on Usability Reward Model and Multi-Agent Reinforcement Learning",
        "authors": "Dmitry Vidmanov, Alexander Alfimtsev",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Today, reinforcement learning is one of the most effective machine learning approaches in the tasks of automatically adapting computer systems to user needs. However, implementing this technology into a digital product requires addressing a key challenge: determining the reward model in the digital environment. This paper proposes a usability reward model in multi-agent reinforcement learning. Well-known mathematical formulas used for measuring usability metrics were analyzed in detail and incorporated into the usability reward model. In the usability reward model, any neural network-based multi-agent reinforcement learning algorithm can be used as the underlying learning algorithm. This paper presents a study using independent and actor-critic reinforcement learning algorithms to investigate their impact on the usability metrics of a mobile user interface. Computational experiments and usability tests were conducted in a specially designed multi-agent environment for mobile user interfaces, enabling the implementation of various usage scenarios and real-time adaptations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/mti8040026"
    },
    {
        "id": 31277,
        "title": "Closed-Loop Reinforcement Learning Based Deep Brain Stimulation Using SpikerNet: A Computational Model",
        "authors": "Brandon S. Coventry, Edward L. Bartlett",
        "published": "2023-4-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ner52421.2023.10123797"
    },
    {
        "id": 31278,
        "title": "Obtaining emergent behaviors for swarm robotics singling with deep reinforcement learning",
        "authors": "Pilar Arques, Fidel Aznar, Mar Pujol, RamÃ³n Rizo",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01691864.2023.2194952"
    },
    {
        "id": 31279,
        "title": "LEARNEST: LEARNing Enhanced Model-based State ESTimation for Robots using Knowledge-based Neural Ordinary Differential Equations",
        "authors": "Kong Yao Chee, M. Ani Hsieh",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161211"
    },
    {
        "id": 31280,
        "title": "Potato Disease Degree Recognition Model Based on Image Processing and Contrast Learning",
        "authors": "Liang Jia",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489812"
    },
    {
        "id": 31281,
        "title": "A Learning-Based Model Predictive Control Strategy for Home Energy Management Systems",
        "authors": "Wenqi Cai, Shambhuraj Sawant, Dirk Reinhardt, Soroush Rastegarpour, SÃ©bastien Gros",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3346324"
    },
    {
        "id": 31282,
        "title": "Research on Data-driven College English Teaching Model Based on Reinforcement Learning and Virtual Reality through Online Gaming",
        "authors": "Jin Yan",
        "published": "2023-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s5.197-210"
    },
    {
        "id": 31283,
        "title": "Low Level Grasp Controller for Slippage and Deformation Prevention exploiting Deep Reinforcement Learning",
        "authors": "HIRAKJYOTI BASUMATARY, Shyamanta Hazarika, Prathamesh Kanbaskar, Atharva Shrawge",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3610419.3610483"
    },
    {
        "id": 31284,
        "title": "Reinforcement Learning with Probabilistically Safe Control Barrier Functions for Ramp Merging",
        "authors": "Soumith Udatha, Yiwei Lyu, John Dolan",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161418"
    },
    {
        "id": 31285,
        "title": "Active Predictive Coding: Brain-Inspired Reinforcement Learning for Sparse Reward Robotic Control Problems",
        "authors": "Alexander Ororbia, Ankur Mali",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160530"
    },
    {
        "id": 31286,
        "title": "W-IRL: Inverse Reinforcement Learning via Wasserstein Metric",
        "authors": "Keyu Wu, Chao Li, Fengge Wu, Junsuo Zhao",
        "published": "2023-3-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccr56747.2023.10194048"
    },
    {
        "id": 31287,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 31288,
        "title": "A Gradient-based reinforcement learning model of market equilibration",
        "authors": "ZhongzhiÂ (Lawrence) He",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jedc.2023.104670"
    },
    {
        "id": 31289,
        "title": "High-accuracy model-based reinforcement learning, a survey",
        "authors": "Aske Plaat, Walter Kosters, Mike Preuss",
        "published": "2023-9",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10462-022-10335-w"
    },
    {
        "id": 31290,
        "title": "Dyna-style Model-based reinforcement learning with Model-Free Policy Optimization",
        "authors": "Kun Dong, Yongle Luo, Yuxin Wang, Yu Liu, Chengeng Qu, Qiang Zhang, Erkang Cheng, Zhiyong Sun, Bo Song",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2024.111428"
    },
    {
        "id": 31291,
        "title": "Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems",
        "authors": "Yi Dong, Xingyu Zhao, Sen Wang, Xiaowei Huang",
        "published": "2024-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2024.3364471"
    },
    {
        "id": 31292,
        "title": "Exploration of Unknown Environment using Deep Reinforcement Learning",
        "authors": "Asad Ali, Sarah Gul, Tallat Mahmood, Anayat Ullah",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrai57502.2023.10089589"
    },
    {
        "id": 31293,
        "title": "Reinforcement Learning and Place Cell Replay in Spatial Navigation",
        "authors": "Chance Hamilton, Pablo Scleidorovich, Alfredo Weitzenfeld",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5038/lzsj6129"
    },
    {
        "id": 31294,
        "title": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning",
        "authors": "Zhongyu Li, Xue Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, Koushil Sreenath",
        "published": "2023-7-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.052"
    },
    {
        "id": 31295,
        "title": "Reinforcement learning as an innovative model-based approach: Examples from precision dosing, digital health and computational psychiatry",
        "authors": "Benjamin Ribba",
        "published": "2023-2-17",
        "citations": 3,
        "abstract": "Model-based approaches are instrumental for successful drug development and use. Anchored within pharmacological principles, through mathematical modeling they contribute to the quantification of drug response variability and enables precision dosing. Reinforcement learning (RL)âa set of computational methods addressing optimization problems as a continuous learning processâshows relevance for precision dosing with high flexibility for dosing rule adaptation and for coping with high dimensional efficacy and/or safety markers, constituting a relevant approach to take advantage of data from digital health technologies. RL can also support contributions to the successful development of digital health applications, recognized as key players of the future healthcare systems, in particular for reducing the burden of non-communicable diseases to society. RL is also pivotal in computational psychiatryâa way to characterize mental dysfunctions in terms of aberrant brain computationsâand represents an innovative modeling approach forpsychiatric indications such as depression or substance abuse disorders for which digital therapeutics are foreseen as promising modalities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fphar.2022.1094281"
    },
    {
        "id": 31296,
        "title": "Reinforcement Learning-Based Market Game Model Considering Virtual Power Plants",
        "authors": "Dehai Yang, Yong Wei, Xuelei Nie, Liqiang Zhao, Guiyang Xu",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpet59380.2023.10367670"
    },
    {
        "id": 31297,
        "title": "Autonomous Port Navigation With Ranging Sensors Using Model-Based Reinforcement Learning",
        "authors": "Siemen Herremans, Ali Anwar, Arne Troch, Ian Ravijts, Maarten Vangeneugden, Siegfried Mercelis, Peter Hellinckx",
        "published": "2023-6-11",
        "citations": 0,
        "abstract": "Abstract\nAutonomous shipping has recently gained much interest in the research community. However, little research focuses on inland - and port navigation, even though this is identified by countries such as Belgium and the Netherlands as an essential step towards a sustainable future. These environments pose unique challenges, since they can contain dynamic obstacles that do not broadcast their location, such as small vessels, kayaks or buoys. Therefore, this research proposes a navigational algorithm which can navigate an inland vessel in a wide variety of complex port scenarios using ranging sensors to observe the environment. The proposed methodology is based on a machine learning approach that has recently set benchmark results in various domains: model-based reinforcement learning. By randomizing the port environments during training, the trained model can navigate in scenarios that it never encountered during training. Furthermore, results show that our approach outperforms the commonly used dynamic window approach and a benchmark model-free reinforcement learning algorithm. This work is therefore a significant step towards vessels that can navigate autonomously in complex port scenarios.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/omae2023-104455"
    },
    {
        "id": 31298,
        "title": "Settling the sample complexity of model-based offline reinforcement learning",
        "authors": "Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, Yuting Wei",
        "published": "2024-2-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1214/23-aos2342"
    },
    {
        "id": 31299,
        "title": "SLAYO-RL: A Target-Driven Deep Reinforcement Learning Approach with SLAM and YoLo for an Enhanced Autonomous Agent",
        "authors": "JosÃ© Montes, Troy Costa Kohwalter, Esteban Clua",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lars/sbr/wre59448.2023.10332988"
    },
    {
        "id": 31300,
        "title": "Research on E-commerce Personalized Transaction Processing Model Based on Reinforcement Learning",
        "authors": "Jinling Chi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijcsyse.2023.10057564"
    },
    {
        "id": 31301,
        "title": "Class-Specific Model Pruning Method Based on Dependency Graph and Reinforcement Learning",
        "authors": "Yongxin Wang, Peinan Shao, Xinchao Wang",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ainit59027.2023.10212591"
    },
    {
        "id": 31302,
        "title": "Efficient hyperparameters optimization through model-based reinforcement learning with experience exploiting and meta-learning",
        "authors": "Xiyuan Liu, Jia Wu, Senpeng Chen",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08050-x"
    },
    {
        "id": 31303,
        "title": "Construction of an Environment Model for Auto-tuning Quantum Dot Devices Using Model-based Reinforcement Learning",
        "authors": "Chihiro Kondo, Raisei Mizokuchi, Jun Yoneda, Tetsuo Kodera",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7567/ssdm.2023.b-1-03"
    },
    {
        "id": 31304,
        "title": "Assistive standing seat based on reinforcement learning",
        "authors": "Renyu Tian, Weizhen Sun",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3598151.3598165"
    }
]