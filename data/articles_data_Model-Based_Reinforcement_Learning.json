[
    {
        "id": 17871,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_5"
    },
    {
        "id": 17872,
        "title": "Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we introduce dynamic programming, which is a cornerstone of model-based reinforcement learning. We demonstrate dynamic programming for policy iteration and value iteration, leading to the quality function and Q-learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.6fs4s9"
    },
    {
        "id": 17873,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 17874,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-4"
    },
    {
        "id": 17875,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch3"
    },
    {
        "id": 17876,
        "title": "A confidence-based reinforcement learning model for perceptual learning",
        "authors": "Matthias Guggenmos, Philipp Sterzer",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractIt is well established that learning can occur without external feedback, yet normative reinforcement learning theories have difficulties explaining such instances of learning. Recently, we reported on a confidence-based reinforcement learn-ing model for the model case of perceptual learning (Guggenmos, Wilbertz, Hebart, & Sterzer, 2016), according to which the brain capitalizes on internal monitoring processes when no external feedback is available. In the model, internal confidence prediction errors – the difference between current confidence and expected confidence – serve as teaching signals to guide learning. In the present paper, we explore an extension to this idea. The main idea is that the neural information processing pathways activated for a given sensory stimulus are subject to fluctuations, where some pathway configurations lead to higher confidence than others. Confidence prediction errors strengthen pathway configurations for which fluctuations lead to above-average confidence and weaken those that are associated with below-average con-fidence. We show through simulation that the model is capable of self-reinforced perceptual learning and can benefit from exploratory network fluctuations. In addition, by simulating different model parameters, we show that the ideal confidence-based learner should (i) exhibit high degrees of network fluctuation in the initial phase of learning, but re-duced fluctuations as learning progresses, (ii) have a high learning rate for network updates for immediate performance gains, but a low learning rate for long-term maximum performance, and (iii) be neither too under-nor too overconfident. In sum, we present a model in which confidence prediction errors strengthen favorable network fluctuations and enable learning in the absence of external feedback. The model can be readily applied to data of real-world perceptual tasks in which observers provided both choice and confidence reports.",
        "link": "http://dx.doi.org/10.1101/136903"
    },
    {
        "id": 17877,
        "title": "Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we describe Q-learning, which is one of the most popular methods in reinforcement learning. Q-learning is a type of temporal difference learning.  We discuss other TD algorithms, such as SARSA, and connections to biological learning through dopamine. Q-learning is also one of the most common frameworks for deep reinforcement learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.ss11hp"
    },
    {
        "id": 17878,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Soumya Ray, Prasad Tadepalli",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_561"
    },
    {
        "id": 17879,
        "title": "Systematic Model-based Design of a Reinforcement Learning-based Neural Adaptive Cruise Control System",
        "authors": "Or Yarom, Jannis Fritz, Florian Lange, Xiaobo Liu-Henke",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010923300003116"
    },
    {
        "id": 17880,
        "title": "What is dopamine doing in model-based reinforcement learning?",
        "authors": "Thomas Akam, Mark Walton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Experiments have implicated dopamine in model-based reinforcement learning (RL). These findings are unexpected as dopamine is thought to encode a reward prediction error (RPE), which is the key teaching signal in model-free RL. Here we examine two possible accounts for dopamine’s involvement in model-based RL: the first that dopamine neurons carry a prediction error used to update a type of predictive state representation called a successor representation, the second that two well established aspects of dopaminergic activity, RPEs and surprise signals, can together explain dopamine’s involvement in model-based RL.",
        "link": "http://dx.doi.org/10.31234/osf.io/z2fmw"
    },
    {
        "id": 17881,
        "title": "Model-Based Reinforcement Learning from PILCO to PETS",
        "authors": "Pascal Klink",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_14"
    },
    {
        "id": 17882,
        "title": "Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards",
        "authors": "Paul Krueger, Thomas Griffiths",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1191-0"
    },
    {
        "id": 17883,
        "title": "EEG-based classification of learning strategies : Model-based and model-free reinforcement learning",
        "authors": "Dongjae Kim, Charles Weston, Sang Wan Lee",
        "published": "2018-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iww-bci.2018.8311522"
    },
    {
        "id": 17884,
        "title": "Dissociation between task structure learning and performance in human model-based reinforcement learning",
        "authors": "Sabrine Hamroun, Maël Lebreton, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "The multi-step learning paradigm has become the dominant paradigm to investigate the trade-off between model-free reinforcement learning – which only leverages state-action-reward associations – and model-based reinforcement learning – which additionally builds on an explicit representation of state-transitions. Experimentally, while reward values usually have to be learned by trial-and-errors, state-transitions are customarily provided by instructions and extensive training. Accordingly, little is known about the learning strategies that are implemented in the ecological situation in which action-state-transitions are not known ex-ante. To fill this gap, we administered a new version of the two-step tasks, in which action-state-transitions have to be learned in parallel of reward values, to two cohorts of participants tested in the lab (N=30) and online (N=200). While choice patterns in the learning phase showed little sign of model-based learning, participants still accurately retrieved action-state-transitions in post-learning assessments. Together our results reveal an intriguing dissociation between knowledge and performance in human reinforcement learning: while reward values and the action-state-transitions can be concomitantly learned in the two-step paradigm, choices do not seem to benefit from model-based computations.",
        "link": "http://dx.doi.org/10.31234/osf.io/2uw85"
    },
    {
        "id": 17885,
        "title": "Iacppo: A Deep Reinforcement Learning-Based Model for Warehouse Inventory Replenishment",
        "authors": "Ran Tian, Haopeng Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4459934"
    },
    {
        "id": 17886,
        "title": "Hierarchical Model-Based Deep Reinforcement Learning For Single-Asset Trading",
        "authors": "Adrian Millea",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a hierarchical reinforcement learning (RL) architecture that employs various low-level agents to act in the trading environment, i.e. the market. The highest level agent selects among a group of specialised agents, and then the selected agent decides when to sell or buy a single asset for some period. This period can be variable according to a termination function. We hypothesized that due to different market regimes, more than one single agent is needed when trying to learn from such heterogeneous data, and instead, multiple agents will perform better, with each one specialising in a subset of the data. We use $k-means$ clustering to partition the data and train each agent with a different cluster. Partitioning the input data also helps model-based RL (MBRL), where models can be heterogeneous. We also add two simple decision-making models to the set of low-level agents, diversifying the pool of available agents and thus increasing overall behaviour flexibility. We perform multiple experiments showing the strengths of a hierarchical approach and test various prediction models at both levels. We also use a risk-based reward at the high level, which transforms the overall problem into a risk-return optimization. This type of reward shows a significant reduction in risk while minimally reducing profits. Overall, the hierarchical approach shows significant promise, especially when the pool of low-level agents is highly diverse.",
        "link": "http://dx.doi.org/10.20944/preprints202305.1532.v1"
    },
    {
        "id": 17887,
        "title": "Expert Initialized Hybrid Model-Based and Model-Free Reinforcement Learning",
        "authors": "Jeppe Langaa, Christoffer Sloth",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178306"
    },
    {
        "id": 17888,
        "title": "A Decentralized Path Planning Model Based on Deep Reinforcement Learning",
        "authors": "dong guo, Shouwen Ji, yanke yao",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4411759"
    },
    {
        "id": 17889,
        "title": "Do model-based and model-free reinforcement learning correspond to goal-directed and habitual actions, respectively? A systematic review.",
        "authors": "Xuan Yee",
        "published": "No Date",
        "citations": 0,
        "abstract": "The idea that model-based reinforcement learning (RL) corresponds to goal-directed actions and model-free RL corresponds to habitual actions is deeply entrenched in psychology. To test this hypothesis, we will first review and evaluate the evidence base from behavioral and neural studies. Behaviorally, positive correlations between behavioral parameters that index model-based/model-free RL and goal-directed/habitual actions would support the hypothesis. Neurally, overlapping neural substrates that underlie the model-based/goal-directed constructs and between the model-free/habitual constructs, as well as a dissociation between the neural substrates underlying both groups of constructs, would support the hypothesis. We will then discuss alternative classes of computational theories beyond the model-based/model-free framework that purport to describe goal-directed and habitual behaviour, and compare these theories against the model-based/model-free framework as well as against each other. Some of the alternative theories covered in this review include dichotomy-based frameworks (e.g., value-based vs. value-free), hierarchical frameworks (e.g., action sequences or active inference), and biological frameworks (e.g., actor-critic models). We then outline potential approaches to synthesize the findings and future avenues of research. Overall, we find that model-based RL maps onto certain facets of goal-directed actions but not necessarily onto other facets. On the other hand, there is much evidence suggesting that model-free RL does not track habitual actions.",
        "link": "http://dx.doi.org/10.31234/osf.io/qm3gp"
    },
    {
        "id": 17890,
        "title": "Proposal model for e-learning based on Case Based Reasoning and Reinforcement Learning",
        "authors": "Anibal Flores, Luis Alfaro, Jose Herrera",
        "published": "2019-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/edunine.2019.8875800"
    },
    {
        "id": 17891,
        "title": "Pseudo-learning effects in reinforcement learning model-based analysis: A problem of misspecification of initial preference",
        "authors": "Kentaro Katahira, Bai Yu, Takashi Nakao",
        "published": "No Date",
        "citations": 4,
        "abstract": "In this study, we investigate a methodological problem of reinforcement-learning (RL) model-based analysis of choice behavior. We show that misspecification of the initial preference of subjects can significantly affect the parameter estimates, model selection, and conclusions of an analysis. This problem can be considered to be an extension of the methodological flaw in the free-choice paradigm (FCP), which has been controversial in studies of decision making. To illustrate the problem, we conducted simulations of a hypothetical reward-based choice experiment. The simulation shows that the RL model-based analysis reports an apparent preference change if hypothetical subjects prefer one option from the beginning, even when they do not change their preferences (i.e., via learning). We discuss possible solutions for this problem.",
        "link": "http://dx.doi.org/10.31234/osf.io/a6hzq"
    },
    {
        "id": 17892,
        "title": "A deep reinforcement learning model based on deterministic policy gradient for collective neural crest cell migration",
        "authors": "George Lykotrafitis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5f5f8e69aa777f8ba5bd6177"
    },
    {
        "id": 17893,
        "title": "Model-Based or Model-Free, a Review of Approaches in Reinforcement Learning",
        "authors": "Qingyan Huang",
        "published": "2020-8",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds49703.2020.00051"
    },
    {
        "id": 17894,
        "title": "Learning of Dynamic Models",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch4"
    },
    {
        "id": 17895,
        "title": "Policy-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_4"
    },
    {
        "id": 17896,
        "title": "Model-Based Reinforcement Learning for Approximate Optimal Control",
        "authors": "Rushikesh Kamalapurkar, Patrick Walters, Joel Rosenfeld, Warren Dixon",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-78384-0_4"
    },
    {
        "id": 17897,
        "title": "Model-Free Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-5"
    },
    {
        "id": 17898,
        "title": "An Empirical Research on the Investment Strategy of Stock Market based on Deep Reinforcement Learning model",
        "authors": "Yuming Li, Pin Ni, Victor Chang",
        "published": "2019",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007722000520058"
    },
    {
        "id": 17899,
        "title": "Learning to Paint With Model-Based Deep Reinforcement Learning",
        "authors": "Zhewei Huang, Shuchang Zhou, Wen Heng",
        "published": "2019-10",
        "citations": 69,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00880"
    },
    {
        "id": 17900,
        "title": "MOOR: Model-based Offline Reinforcement learning for sustainable fishery management",
        "authors": "",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36334/modsim.2021.m2.ju"
    },
    {
        "id": 17901,
        "title": "Model-based Reinforcement Learning: A Survey",
        "authors": "Thomas M. Moerland, Joost Broekens, Aske Plaat, Catholijn M. Jonker",
        "published": "2023",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/9781638280576"
    },
    {
        "id": 17902,
        "title": "Model‐Based Reinforcement Learning",
        "authors": "Jun Liu, Milad Farsi",
        "published": "2022-12-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602"
    },
    {
        "id": 17903,
        "title": "Modeling Survival in model-based Reinforcement Learning",
        "authors": "Saeed Moazami, Peggy Doerschuk",
        "published": "2020-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai49837.2020.00009"
    },
    {
        "id": 17904,
        "title": "Piecewise Learning and Control with Stability Guarantees",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch7"
    },
    {
        "id": 17905,
        "title": "Artificial Neural Networks and Reinforcement Learning for Model-based Design of an Automated Vehicle Guidance System",
        "authors": "Or Yarom, Soeren Scherler, Marian Goellner, Xiaobo Liu-Henke",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008995407250733"
    },
    {
        "id": 17906,
        "title": "Deep Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_3"
    },
    {
        "id": 17907,
        "title": "Model abstraction for model-based reinforcement learning in the human orbitofrontal cortex",
        "authors": "Yu Takagi, Wako Yoshida, Saori Tanaka",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1271-0"
    },
    {
        "id": 17908,
        "title": "A Gradient-Based Reinforcement Learning Model of Market Equilibration",
        "authors": "Zhongzhi Lawrence He",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4294713"
    },
    {
        "id": 17909,
        "title": "Tabular Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_2"
    },
    {
        "id": 17910,
        "title": "Structured Online Learning‐Based Control of Continuous‐Time Nonlinear Systems",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch5"
    },
    {
        "id": 17911,
        "title": "Mpc-Based Model-Based Reinforcement Learning and Planning For Auv Path Following",
        "authors": "Zheng Zhang, Tianhao Chen, Dong Jiang, Zheng Fang, Guangliang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4349138"
    },
    {
        "id": 17912,
        "title": "An Adaptive Agent Decision Model Based on Deep Reinforcement  Learning and Autonomous Learning",
        "authors": "",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33168/jliss.2023.0309"
    },
    {
        "id": 17913,
        "title": "Reinforcement Twinning: From Digital Twins to Model-Based Reinforcement Learning",
        "authors": "Lorenzo Schena, Pedro Afonso  Duque Morgado Marques, Romain Poletti, Samuel Ahizi, Jan Van den Berghe, Miguel Alfonso Mendez",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4761240"
    },
    {
        "id": 17914,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396.v1"
    },
    {
        "id": 17915,
        "title": "Reinforcement Learning Based Attitude Fault-Tolerant Control of Spacecraft with Unknown System Model",
        "authors": "Shaolong Yang, Lei Jin, Jiaxuan Rao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4692007"
    },
    {
        "id": 17916,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396"
    },
    {
        "id": 17917,
        "title": "Front Matter",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.fmatter"
    },
    {
        "id": 17918,
        "title": "Learning to Draw Through A Multi-Stage Environment Model Based Reinforcement Learning",
        "authors": "Ji Qiu, Peng Lu, Xujun Peng",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222280"
    },
    {
        "id": 17919,
        "title": "An Iterative Model-Based Reinforcement Learning Utilizing Multi-Perspective Learning with Monte-Carlo Tree Search",
        "authors": "Jiao Wang, Yijian Zhang, Yingxin Ren, Yingtong Ren, Xue Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4677186"
    },
    {
        "id": 17920,
        "title": "Learning Locomotion Skills via Model-based Proximal Meta-Reinforcement Learning",
        "authors": "Qing Xiao, Zhengcai Cao, Mengchu Zhou",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2019.8914406"
    },
    {
        "id": 17921,
        "title": "Appendix",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.app1"
    },
    {
        "id": 17922,
        "title": "A Texas Hold’em decision model based on Reinforcement Learning",
        "authors": "Xiao Chuan ZHANG, Yi Li",
        "published": "2020-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc49329.2020.9164345"
    },
    {
        "id": 17923,
        "title": "Model-Based Algorithms",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_3"
    },
    {
        "id": 17924,
        "title": "Sample Complexity of Model-Based Robust Reinforcement Learning",
        "authors": "Kishan Panaganti, Dileep Kalathil",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683162"
    },
    {
        "id": 17925,
        "title": "Index",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.index"
    },
    {
        "id": 17926,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100065"
    },
    {
        "id": 17927,
        "title": "AGV Path Planning Model based on Reinforcement Learning",
        "authors": "Xiaofei Liao, Yang Wang, Yiliang Xuan, Dequan Wu",
        "published": "2020-11-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326742"
    },
    {
        "id": 17928,
        "title": "A deep reinforcement learning based model supporting object familiarization",
        "authors": "Maximilian Panzner, Philipp Cimiano",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2017.8329828"
    },
    {
        "id": 17929,
        "title": "Learning Path Recommendation Based on Knowledge Tracing Model and Reinforcement Learning",
        "authors": "Dejun Cai, Yuan Zhang, Bintao Dai",
        "published": "2019-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc47050.2019.9064104"
    },
    {
        "id": 17930,
        "title": "Model-Based Reinforcement Learning via Proximal Policy Optimization",
        "authors": "Yuewen Sun, Xin Yuan, Wenzhang Liu, Changyin Sun",
        "published": "2019-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8996875"
    },
    {
        "id": 17931,
        "title": "Python Toolbox",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch10"
    },
    {
        "id": 17932,
        "title": "News Article Name Disambiguation Model Based on Reinforcement Learning",
        "authors": "Yi Ding",
        "published": "2021-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/caibda53561.2021.00033"
    },
    {
        "id": 17933,
        "title": "DATA-EFFICIENT MODEL-BASED REINFORCEMENT LEARNING FOR ROBOT CONTROL",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2316/j.2021.206-0528"
    },
    {
        "id": 17934,
        "title": "DeepBike: A Deep Reinforcement Learning Based Model for Large-scale Online Bike Share Rebalancing",
        "authors": "Zhuoli Yin, Zhaoyu Kou, Hua Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBike share systems (BSSs), as a potentially environment-friendly mobility mode, are being deployed globally. To address spatially and temporally imbalanced bike and dock demands, BSS operators need to redistribute bikes among stations using a fleet of rebalancing vehicles in real-time. However, existing studies mainly generate BSS rebalancing solutions for small-scale BSSs or subsets of BSSs, while deploying small-size rebalancing fleets. How to produce online rebalancing solutions for large-scale BSS with multiple rebalancing vehicles to minimize customer loss is critical for system operation yet remains unsolved. To address this gap, we proposed a deep reinforcement learning based model — DeepBike — that trains deep Q-network (DQN) to learn the optimal strategy for dynamic bike share rebalancing. DeepBike uses real-time states of rebalancing vehicles, stations and predicted demands as inputs to output the long-term quality values of rebalancing actions of each rebalancing vehicle. Rebalancing vehicles could work asynchronously as each individually runs the DQN. We compared the performance of the proposed DeepBike against baseline models for dynamic bike share rebalancing based on historical trip records from Divvy BSS in Chicago, which possesses more than 500 stations and 16 rebalancing vehicles. The evaluation results show that our proposed DeepBike model was able to better reduce customer loss by 111.09% and 57.6% than the mixed integer programming and heuristic-based models, respectively, and increased overall net profits by 101.26% and 220.01%, respectively. The DeepBike model is effective for large-scale dynamic bike share rebalancing problems and has the potential to improve the operation of shared mobility systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3998473/v1"
    },
    {
        "id": 17935,
        "title": "ID-RDRL: A Deep Reinforcement Learning-based Feature Selection Intrusion Detection Model",
        "authors": "Kezhou Ren, Yifan Zeng, Zhiqin Cao, Yingchao Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNetwork assaults pose significant security concerns to network services; hence, new technical solutions must be used to enhance the efficacy of intrusion detection systems. Existing approaches pay insufficient attention to data preparation and inadequately identify unknown network threats. This paper presents a network intrusion detection model (ID-RDRL) based on RFE feature extraction and reinforcement learning. ID-RDRL filters the optimum subset of features using the RFE feature selection technique, feeds them into a neural network to extract feature information and then trains a classifier using DRL to recognize network intrusions. We utilized CSE-CIC-IDS2018 as a dataset and conducted tests to evaluate the model's performance, which is comprised of a comprehensive collection of actual network traffic. The experimental results demonstrate that the proposed ID-RDRL model can select the optimal subset of features, remove approximately 80 percent of redundant features, and learn the selected features through DRL to enhance the IDS performance for network attack identification. In a complicated network environment, it has promising application potential in IDS.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1765453/v1"
    },
    {
        "id": 17936,
        "title": "A model-based approach to meta-Reinforcement Learning: Transformers and tree search",
        "authors": "Brieuc Pinon, Raphaël Jungers, Jean-Charles Delvenne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-117"
    },
    {
        "id": 17937,
        "title": "Optimal Control",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch2"
    },
    {
        "id": 17938,
        "title": "Severe Sexual Abuse Reduces Frontoparietal Network Activity during Model-Based Reinforcement Learning Updates",
        "authors": "Allison Letkiewicz, Amy L. Cochran, Josh M. Cisler",
        "published": "No Date",
        "citations": 0,
        "abstract": "Trauma and trauma-related disorders are characterized by altered learning styles. Two learning processes that have been delineated using computational modeling are model-free and model-based reinforcement learning (RL), characterized by trial and error and goal-driven, rule-based learning, respectively. Prior research suggests that model-free RL is disrupted among individuals with a history of assaultive trauma and may contribute to altered fear responding. Currently, it is unclear whether model-based RL, which involves building abstract and nuanced representations of stimulus-outcome relationships to prospectively predict action-related outcomes, is also impaired among individuals who have experienced trauma. The present study sought to test the hypothesis of impaired model-based RL among adolescent females exposed to assaultive trauma. Participants (n=60) completed a three-arm bandit RL task during fMRI acquisition. Two computational models compared the degree to which each participant’s task behavior fit the use of a model-free versus model-based RL strategy. Overall, a greater portion of participants’ behavior was better captured by the model-based than model-free RL model. Although assaultive trauma did not predict learning strategy use, greater sexual abuse severity predicted less use of model-based compared to model-free RL. Additionally, severe sexual abuse predicted less left frontoparietal network encoding of model-based RL updates, which was not accounted for by PTSD. Given the significant impact that sexual trauma has on mental health and other aspects of functioning, it is plausible that altered model-based RL is an important route through which clinical impairment emerges.",
        "link": "http://dx.doi.org/10.31234/osf.io/fqv2s"
    },
    {
        "id": 17939,
        "title": "Behavioral Cloning Based Model Generation Method for Reinforcement Learning",
        "authors": "Dengmin Xiao, Bo Wang, Zhongqi Sun, Xiao He",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450935"
    },
    {
        "id": 17940,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 17941,
        "title": "Model-Based Indirect RL: Dynamic Programming",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_5"
    },
    {
        "id": 17942,
        "title": "Model gradient: unified model and policy learning in model-based reinforcement learning",
        "authors": "Chengxing Jia, Fuxiang Zhang, Tian Xu, Jing-Cheng Pang, Zongzhang Zhang, Yang Yu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11704-023-3150-5"
    },
    {
        "id": 17943,
        "title": "Reinforcement Learning-Based Model Reduction for Partial Differential Equations: Application to the Burgers Equation",
        "authors": "Mouhacine Benosman, Ankush Chakrabarty, Jeff Borggaard",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_11"
    },
    {
        "id": 17944,
        "title": "Safe Exploration in Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_8"
    },
    {
        "id": 17945,
        "title": "Nonlinear Systems Analysis",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch1"
    },
    {
        "id": 17946,
        "title": "Homogeneous Transformation Matrix Based Neural Network for Model Based Reinforcement Learning on Robot Manipulator",
        "authors": "Mochammad Rizky Diprasetya, Andreas Schwung",
        "published": "2022-8-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit48603.2022.10002834"
    },
    {
        "id": 17947,
        "title": "Reinforcement learning in learning automata and cellular learning automata via multiple reinforcement signals",
        "authors": "Reza Vafashoar, Mohammad Reza Meybodi",
        "published": "2019-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2019.01.021"
    },
    {
        "id": 17948,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100065-1"
    },
    {
        "id": 17949,
        "title": "Efficient hyperparameter optimization through model-based reinforcement learning",
        "authors": "Jia Wu, SenPeng Chen, XiYuan Liu",
        "published": "2020-10",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.06.064"
    },
    {
        "id": 17950,
        "title": "Efficient Neural Network Pruning Using Model-Based Reinforcement Learning",
        "authors": "Blanka Bencsik, Marton Szemenyei",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismcr56534.2022.9950598"
    },
    {
        "id": 17951,
        "title": "Model Based Human‐Robot Interaction Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch3"
    },
    {
        "id": 17952,
        "title": "Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning",
        "authors": "Florian Shkurti, Nikhil Kakodkar, Gregory Dudek",
        "published": "2018-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8463196"
    },
    {
        "id": 17953,
        "title": "Model-Free Reinforcement-Learning-Based Control Methodology for Power Electronic Converters",
        "authors": "Dajr Alfred, Dariusz Czarkowski, Jiaxin Teng",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/greentech48523.2021.00024"
    },
    {
        "id": 17954,
        "title": "Model-Based Reinforcement Learning For Robot Control",
        "authors": "Xiang Li, Weiwei Shang, Shuang Cong",
        "published": "2020-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarm49381.2020.9195341"
    },
    {
        "id": 17955,
        "title": "Incremental Learning of Planning Actions in Model-Based Reinforcement Learning",
        "authors": "Jun Hao Alvin Ng, Ronald P. A. Petrick",
        "published": "2019-8",
        "citations": 2,
        "abstract": "The soundness and optimality of a plan depends on the correctness of the domain model. Specifying complete domain models can be difficult when interactions between an agent and its environment are complex. We propose a model-based reinforcement learning (MBRL) approach to solve planning problems with unknown models. The model is learned incrementally over episodes using only experiences from the current episode which suits non-stationary environments. We introduce the novel concept of reliability as an intrinsic motivation for MBRL, and a method to learn from failure to prevent repeated instances of similar failures. Our motivation is to improve the learning efficiency and goal-directedness of MBRL. We evaluate our work with experimental results for three planning domains.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/443"
    },
    {
        "id": 17956,
        "title": "A New Paradigm to Study Social and Physical Affordances as Model-Based Reinforcement Learning",
        "authors": "Augustin Chartouny, Keivan Amini, Mehdi Khamassi, Benoît Girard",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683077"
    },
    {
        "id": 17957,
        "title": "Model-based reinforcement learning with missing data",
        "authors": "Nobuhiko Yamaguchi, Osamu Fukuda, Hiroshi Okumura",
        "published": "2020-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/candarw51189.2020.00042"
    },
    {
        "id": 17958,
        "title": "Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction",
        "authors": "Masashi Okada, Tadahiro Taniguchi",
        "published": "2021-5-30",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9560734"
    },
    {
        "id": 17959,
        "title": "Model-Based Reinforcement Learning for Learning Deterministic Policies",
        "authors": "Eiji Uchibe",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2022.2p1-b09"
    },
    {
        "id": 17960,
        "title": "A Graph-Based Spatial-Temporal Deep Reinforcement Learning Model for Edge Caching",
        "authors": "Jiacheng Hou, Amiya Nayak",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436966"
    },
    {
        "id": 17961,
        "title": "Temporal Logic Guided Safe Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_9"
    },
    {
        "id": 17962,
        "title": "A Study of Model Based and Model Free Offline Reinforcement Learning",
        "authors": "Indu Shukla, Haley R. Dozier, Althea C. Henslee",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci58124.2022.00061"
    },
    {
        "id": 17963,
        "title": "An Application to Solar Photovoltaic Systems",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch8"
    },
    {
        "id": 17964,
        "title": "A Structured Online Learning Approach to Nonlinear Tracking with Unknown Dynamics",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch6"
    },
    {
        "id": 17965,
        "title": "Addressing Sample Efficiency and Model-bias in Model-based Reinforcement Learning",
        "authors": "Akhil S Anand, Jens Erik Kveen, Fares Abu-Dakka, Esten Ingar Grøtli, Jan Tommy Gravdahl",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00009"
    },
    {
        "id": 17966,
        "title": "How to Use Your Model: Model-Based Reinforcement Learning with Model-Free Policy Optimization",
        "authors": "Kun Dong, Yongle Luo, Yuxin Wang, Yu Liu, Chengeng Qu, Qiang Zhang, Erkang Cheng, Zhiyong Sun, Song Bo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552867"
    },
    {
        "id": 17967,
        "title": "Query-Based Summarization using Reinforcement Learning and Transformer Model",
        "authors": "Yllias Chali, Asif Mahmud",
        "published": "2021-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai52525.2021.00026"
    },
    {
        "id": 17968,
        "title": "Mastering Financial Portfolio Optimization for It-Etf by Developing Model-Based Reinforcement Learning",
        "authors": "Martin Kang, Kyungmyung Jang, Gary  F. Templeton, Dong-Heon Kwak",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4656627"
    },
    {
        "id": 17969,
        "title": "MODEL-BASED SECURITY ANALYSIS OF FPGA DESIGNS THROUGH REINFORCEMENT LEARNING",
        "authors": "Michael Vetter",
        "published": "2019-11-1",
        "citations": 0,
        "abstract": "Finding potential security weaknesses in any complex IT system is an important and often challenging task best started in the early stages of the development process. We present a method that transforms this task for FPGA designs into a reinforcement learning (RL) problem. This paper introduces a method to generate a Markov Decision Process based RL model from a formal, high-level system description (formulated in the domain-specific language) of the system under review and different, quantified assumptions about the system’s security. Probabilistic transitions and the reward function can be used to model the varying resilience of different elements against attacks and the capabilities of an attacker. This information is then used to determine a plausible data exfiltration strategy. An example with multiple scenarios illustrates the workflow. A discussion of supplementary techniques like hierarchical learning and deep neural networks concludes this paper.",
        "link": "http://dx.doi.org/10.14311/ap.2019.59.0518"
    },
    {
        "id": 17970,
        "title": "Cooperative Transport by Manipulators with Uncertainty-Aware Model-Based Reinforcement Learning",
        "authors": "Takumi Aotani, Taisuke Kobayashi",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii58957.2024.10417389"
    }
]