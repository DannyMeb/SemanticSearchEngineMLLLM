[
    {
        "id": 10560,
        "title": "WITHDRAWN: DSBAL: Distributed Stacked Bidirectional Attention-based LSTM Method for Time Series Forecasting",
        "authors": "",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn by the authors due to author disagreement with the posting of the preprint. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2401424/v2"
    },
    {
        "id": 10561,
        "title": "Bca-Net: A Bidirectional Compound Attention Network for Colon Polyps Segmentation",
        "authors": "Bo Yu, Wei Shao, Zehua Ren",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4646383"
    },
    {
        "id": 10562,
        "title": "Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing",
        "authors": "Soohyeong Kim, Whanhee Cho, Minji Kim, Yong Choi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.25"
    },
    {
        "id": 10563,
        "title": "Stock Price Prediction using Bidirectional LSTM with Attention",
        "authors": "Sayan Biswas",
        "published": "2022-5-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaic53980.2022.9896969"
    },
    {
        "id": 10564,
        "title": "WITHDRAWN: DSBAL: Distributed Stacked Bidirectional Attention-based LSTM Method for Time Series Forecasting",
        "authors": "N. Prakash, Sumaiya Farzana. G",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAccurate time series forecasting is crucial to increase the performance and turnover of every business. However, It’s quite a difficult task due to the non-stationary and high level of uncertainty in the time series data. This paper proposes a new method called the Distributed Stacked Bidirectional Attention Long Short-Term Memory Neural Network (DSBAL) for time series forecasting. The DSBAL method combines the Stacked Bidirectional LSTM (SBiLSTM) and Attention mechanism in distributed computing. The proposed method consists of an SBiLSTM encoder, attention mechanism, and SBiLSTM decoder. SBiLSTM encoder is used to extract the complex features in the daily tomato supply data, in addition, the Attention mechanism is introduced to enhance the performance of SBILSTM by selecting the more appropriate sequence in the data by giving higher weightage to them. SBiLSTM decoder uses the most appropriate sequences from the attention mechanism to predict the daily tomato supply data. The entire process of the proposed method runs in distributed computing to improve efficiency, accuracy, and scalability. Our proposed method allows us to use only appropriate sequences in the data, captures complicated patterns, and addresses computational issues. To prove the efficiency of the proposed methodology, the experiments are conducted with other time series forecasting methods like RNN, LSTM, Stacked LSTM, Bidirectional LSTM, and Attention LSTM using daily tomato supply datasets in terms of SMAPE and RMSE. The results obtained from the experiment demonstrate that our proposed method is more efficient, accurate, and scalable.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2401424/v1"
    },
    {
        "id": 10565,
        "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
        "authors": "Pengfei Liu, Kun Li, Helen Meng",
        "published": "2020-10-25",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2067"
    },
    {
        "id": 10566,
        "title": "Attention Mechanism Based Bidirectional LSTM Model for Broadband Power Amplifier Linearization",
        "authors": "rina Su, Jiacheng Wang, Gaoming Xu, taijun liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this letter, a novel model for broadband power amplifier (PA)\nlinearization is proposed, namely Attention Mechanism based\nBidirectional Long Short-term Memory network (AM-BiLSTM). In order to\nverify the linearization performance of the AM-BiLSTM model, a 100MHz\nbandwidth 5G new radio (5G NR) signal is employed to test the sub-6G PA\noperating at 2.6-GHz. The experimental results show that the adjacent\nchannel power ratio (ACPR) of the PA with AM-BiLSTM can be improved by\n24dB which is 6-dB better than the generalized memory polynomial (GMP)\nand 3-dB better than the Chebyshev polynomials LSTM (CP-LSTM) in\nref[1]. Therefore, the proposed AM-BiLSTM is very effective for the\nlinearization of broadband PA.",
        "link": "http://dx.doi.org/10.22541/au.168395581.19424005/v1"
    },
    {
        "id": 10567,
        "title": "BiATNovo: A Self-Attention based Bidirectional Peptide Sequencing Method",
        "authors": "Siyu Wu, Zhongzhi Luan, Zhenxin Fu, Qunying Wang, Tiannan Guo",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractTraditional database-based peptide sequencing methods have shortcomings in discoverability and universality, while de novo sequencing is the essential way to analyze unknown proteins and discover new peptides and proteins. Most existing de novo sequencing algorithms have the problem of accumulated deviation and unbalanced output. At the same time, some algorithms could be more suitable for Data-Independent Acquisition Mass Spectrometry (DIA-MS) data. This paper designed a bidirectional peptide sequencing method to alleviate the problems of unbalanced output and deviation accumulation in the sequencing process. The self-attention mechanism was applied to de novo sequencing to increase the interaction within the peptide sequence and the interaction between the MS/MS spectra and the peptide sequence. On the DIA-MS dataset, the peptide prediction accuracy improved by an average of 15.6% compared with the state-of-the-art method. On the DDA-MS dataset, our method achieved the best performance on partial datasets, the amino acid accuracy improved by an average of 3%. At the same time, two new evaluation scores, Position-BLEU and Alignment score, were proposed to evaluate the misalignment between the predicted sequence and the reference sequence, and the partial absence of fragment ions.",
        "link": "http://dx.doi.org/10.1101/2023.05.11.540352"
    },
    {
        "id": 10568,
        "title": "An Improved NLP for Syntactic and Semantic Matching using Bidirectional LSTM and Attention Mechanism",
        "authors": "Fadya Abbas",
        "published": "2022-5-28",
        "citations": 0,
        "abstract": "Dealing with extensive amounts of textual data requires an efficient deep learning model to be adapted. However, the following reasons; the highly ambiguous and complex nature of many prosodic phrasing also enough dataset suitable for system training is always limited, cause big challenges for training the NLP models. This proposed conceptual framework aims to provide an understanding and familiarity with the elements of modern deep learning networks for NLP use. In this design, the encoder uses Bidirectional Long Short-Term Memory deep network layers, to encode the test sequences into more context-sensitive representations. Moreover, the attention mechanism is mainly used to generate a context vector that is determined from distinct alignment scores at different word positions, hence, it can focus more on a small words' subset. Hence, the attention mechanism improved the model data efficiency, and the model performance is validated using an example of data sets that show promise for a real-life application.",
        "link": "http://dx.doi.org/10.5121/csit.2022.120906"
    },
    {
        "id": 10569,
        "title": "Lightweight Video Frame Interpolation Based on Bidirectional Attention Module",
        "authors": "Yige Li, Han Yang",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscc58397.2023.10217927"
    },
    {
        "id": 10570,
        "title": "Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks",
        "authors": "Chandrakant Bothe, Sven Magg, Cornelius Weber, Stefan Wermter",
        "published": "2018-9-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2527"
    },
    {
        "id": 10571,
        "title": "Improving Mandarin Tone Recognition Using Convolutional Bidirectional Long Short-Term Memory with Attention",
        "authors": "Longfei Yang, Yanlu Xie, Jinsong Zhang",
        "published": "2018-9-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2561"
    },
    {
        "id": 10572,
        "title": "Bidirectional Cross-Selective Attention Network for Video Salient Object Detection",
        "authors": "jun zhang, Biao Zhu, Peng Zhang, Ruijian Cheng, Yuzhen Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327757"
    },
    {
        "id": 10573,
        "title": "HAR-BiNet: COVID-19 Prediction Using Hybrid Attention based Residual Bidirectional Gated Recurrent Unit",
        "authors": "S. John Joseph, Gandhiraj R",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractOne of the most disruptive emergency situations of the century, as seen globally, is the coronavirus epidemic and its quick spread. Clinical image analysis of chest computed tomography (CT)images can be useful in the prevention of the spread of this virus by providing a precise diagnosis. Detecting COVID-19 is possible with the use of artificial intelligence-assisted image analysis.Hence, a deep learning based technique is introduced in this research to forecast the COVID-19. The CT image acquired from the dataset is pre-processed using image resizing and image normalization to make the input image appropriate for the further processing. Then, the significant features will be extracted using convolutional neural network (CNN), Haralick Texture Features,and Histogram of Oriented Gradient (HOG). Using the extracted attributes the optimal best features are chosen using the proposed Chaotic Fennec Fox Optimization (CFFA) algorithm. Using the selected features, COVID-19 prediction will be devised using the proposed Hybrid Attention ResidualBiGRUNetwork (HAR-BiNet), which is designed by integrating attention module, ResNet_152 and Bidirectional Gated Recurrent Unit.The analysis of the proposed CFFA-HAR-BiNet based on accuracy, specificity, precision, recall, F1-Measure and MSE acquired the values of 96.10%, 99.71%, 96.54%, 94.70%, 96.30%, and 3.29% respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3376366/v1"
    },
    {
        "id": 10574,
        "title": "Multi-Indicator Water Quality Prediction with Attention-Assisted Bidirectional Lstm and Encoder-Decoder",
        "authors": "Jing Bi, Luyao Zhang, Haitao Yuan, Jia Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4200649"
    },
    {
        "id": 10575,
        "title": "Assessment of bidirectional transformer encoder model and attention based bidirectional LSTM language models for fake news detection",
        "authors": "Anshika Choudhary, Anuja Arora",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jretconser.2023.103545"
    },
    {
        "id": 10576,
        "title": "Attention-Based Bidirectional LSTM Network for Target Tracking",
        "authors": "Xinyu Yang, Dianfeng Qiao",
        "published": "2021-8-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icetci53161.2021.9563583"
    },
    {
        "id": 10577,
        "title": "BAM: A Bidirectional Attention Module for Masked Face Recognition",
        "authors": "M. Saad Shakeel",
        "published": "2022-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vcip56404.2022.10008847"
    },
    {
        "id": 10578,
        "title": "Bidirectional Attention for Text-Dependent Speaker Verification",
        "authors": "Xin Fang, Tian Gao, Liang Zou, Zhenhua Ling",
        "published": "2020-11-27",
        "citations": 8,
        "abstract": "Automatic speaker verification provides a flexible and effective way for biometric authentication. Previous deep learning-based methods have demonstrated promising results, whereas a few problems still require better solutions. In prior works examining speaker discriminative neural networks, the speaker representation of the target speaker is regarded as a fixed one when comparing with utterances from different speakers, and the joint information between enrollment and evaluation utterances is ignored. In this paper, we propose to combine CNN-based feature learning with a bidirectional attention mechanism to achieve better performance with only one enrollment utterance. The evaluation-enrollment joint information is exploited to provide interactive features through bidirectional attention. In addition, we introduce one individual cost function to identify the phonetic contents, which contributes to calculating the attention score more specifically. These interactive features are complementary to the constant ones, which are extracted from individual speakers separately and do not vary with the evaluation utterances. The proposed method archived a competitive equal error rate of 6.26% on the internal “DAN DAN NI HAO” benchmark dataset with 1250 utterances and outperformed various baseline methods, including the traditional i-vector/PLDA, d-vector, self-attention, and sequence-to-sequence attention models.",
        "link": "http://dx.doi.org/10.3390/s20236784"
    },
    {
        "id": 10579,
        "title": "Sentence Similarity Prediction based on Siamese CNN-Bidirectional LSTM with Self-attention",
        "authors": "Mintae Kim, Yeongtaek Oh, Wooju Kim",
        "published": "2019-3-31",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/jok.2019.46.3.241"
    },
    {
        "id": 10580,
        "title": "Brati: Bidirectional Recurrent Attention for Time Series Imputation",
        "authors": "Armando Collado-Villaverde, Pablo Muñoz, María  D. R. Moreno",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4331197"
    },
    {
        "id": 10581,
        "title": "BiEAF: An Bidirectional Enhanced Attention Flow Model for Question Answering Task",
        "authors": "Yihan Yang",
        "published": "2021-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icise-ie53922.2021.00086"
    },
    {
        "id": 10582,
        "title": "Coordination Attention Based Transformers with Bidirectional Contrastive Loss for Multimodal Speech Emotion Recognition",
        "authors": "Weiquan Fan, Xiangmin Xu, Guohua Zhou, Xiaofang Deng, Xiaofen Xing",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4647924"
    },
    {
        "id": 10583,
        "title": "Bidirectional GRU with Multi-Head Attention for Chinese NER",
        "authors": "Shuo Yan, Jianping Chai, Liyun Wu",
        "published": "2020-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec49072.2020.9141551"
    },
    {
        "id": 10584,
        "title": "Bidirectional GRU dengan Attention Mechanism pada Analisis Sentimen PLN Mobile",
        "authors": "Moh. Ainur Rohman, Suhartono Suhartono, Totok Chamidy",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "PLN Mobile adalah aplikasi ponsel customer self-service yang terintegrasi dengan Aplikasi Pengaduan dan Keluhan Pelanggan (APKT) dan Aplikasi Pelayanan Pelanggan Terpusat (AP2T). Mulai awal tahun 2021 sampai sekarang PLN menggencarkan sosialisasi PLN Mobile pada masyarakat sehingga jumlah ulasan PLN Mobile pada google playstore meningkat drastis. Untuk mengetahui kepuasan pelanggan tidak bisa hanya dengan melihat dan menganalisis dari kolom ulasan PLN Mobile di google playstore, hal ini dikarenakan data ulasan berbentuk tidak terstruktur. Untuk mengatasi masalah ini dibutuhkan teknik khusus yaitu analisis sentimen. Penelitian ini bertujuan untuk mengusulkan arsitektur analisis sentimen untuk mengatasi ketidakmampuan algoritma deep learning seperti LSTM dan GRU dalam menangkap informasi penting. Arsitektur yang diusulkan yaitu mengkombinasikan Bidirectional GRU (BiGRU) dengan attention mechanism menggunakan word2vec sebagai word embedding. Attention mechanism digunakan untuk menangkap kata yang penting sehingga arsitektur tersebut dapat memahami informasi yang penting. Kemudian, arsitektur yang diusulkan dilakukan perbandingan dengan metode CNN, CNN-GRU, CNN-LSTM, CNN-BiGRU, CNN-BiLSTM dengan menggunakan data ulasan PLN Mobile. Hasil eksperimen menunjukkan bahwa arsitektur analisis sentimen yang diusulkan memiliki akurasi dan f1-score yang lebih tinggi.",
        "link": "http://dx.doi.org/10.33633/tc.v22i2.7876"
    },
    {
        "id": 10585,
        "title": "Multi-domain Network Intrusion Detection Based on Attention-based Bidirectional LSTM",
        "authors": "Xiaoning Wang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itnec56291.2023.10081953"
    },
    {
        "id": 10586,
        "title": "Bidirectional Mechanisms rather than Alternatives: The Role of Sustained Attention in Interactive Contexts Can Only Be Understood through Joint Attention",
        "authors": "Emily A.M. Phillips, Sam V. Wass",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1159/000515869"
    },
    {
        "id": 10587,
        "title": "Attention and long-term memory: Bidirectional interactions and their effects on behavior",
        "authors": "Deborah E. Hannula",
        "published": "2018",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/bs.plm.2018.09.004"
    },
    {
        "id": 10588,
        "title": "Bangla Image Captioning With Bidirectional GRU &amp; Attention Mechanism",
        "authors": "Fatema Tuz Zohora, Zainal Abedin",
        "published": "2022-2-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciset54810.2022.9775869"
    },
    {
        "id": 10589,
        "title": "Attention-Based Bidirectional Hierarchical LSTM Networks for Text Semantic Classification",
        "authors": "Xiaoming Shi, Ran Lu",
        "published": "2019-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itme.2019.00143"
    },
    {
        "id": 10590,
        "title": "Bidirectional LSTM with Hierarchical Attention for Text Classification",
        "authors": "Jianping Li, Yimou Xu, Huaye Shi",
        "published": "2019-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaeac47372.2019.8997969"
    },
    {
        "id": 10591,
        "title": "Bidirectional Attention for SQL Generation",
        "authors": "Gao Huilin, Guo Tong, Wang Fan, Ma Chao",
        "published": "2019-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccbda.2019.8725626"
    },
    {
        "id": 10592,
        "title": "Bidirectional Multi-Stack RNNs with Attention for Machine Translation",
        "authors": "Zhiren Chen, Ziyu Qiu, Nuo Chen",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oncon60463.2023.10430889"
    },
    {
        "id": 10593,
        "title": "Accurate and Lightweight MRI Super-Resolution Via Multi-Scale Bidirectional Fusion Attention Network",
        "authors": "Ling Xu, guangyao li, Qiaochuan Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4147324"
    },
    {
        "id": 10594,
        "title": "Character relationship extraction based on BERT bidirectional-GRU neural network-attention",
        "authors": "JinPing Yu, Haibo Yang",
        "published": "2022-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp54964.2022.9778554"
    },
    {
        "id": 10595,
        "title": "Stance Detection Using Multi-Head Attention Based Bidirectional GRU",
        "authors": "Peng Jia, Yajun Du, Binyan Lyu, Ruilin Hu",
        "published": "2021-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc54389.2021.9674443"
    },
    {
        "id": 10596,
        "title": "Multi-label Aerial Image Classification using A Bidirectional Class-wise Attention Network",
        "authors": "Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu",
        "published": "2019-5",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jurse.2019.8808940"
    },
    {
        "id": 10597,
        "title": "Bidirectional Attention LSTM Networks for Non-instructive Load Monitoring",
        "authors": "Yuwei Fan, Chao Liu, Tengbo Guo, Dongxiang Jiang",
        "published": "2022-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/phm2022-london52454.2022.00076"
    },
    {
        "id": 10598,
        "title": "Human Motion Denoising Using Attention-Based Bidirectional Recurrent Neural Network",
        "authors": "Seong Uk Kim, Hanyoung Jang, Jongmin Kim",
        "published": "2019-11-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3355056.3364577"
    },
    {
        "id": 10599,
        "title": "A Bidirectional Deep-Learning-Based Spectral Attention Mechanism for Hyperspectral Data Classification",
        "authors": "Bishwas Praveen, Vineetha Menon",
        "published": "2022-1-4",
        "citations": 7,
        "abstract": "Hyperspectral remote sensing presents a unique big data research paradigm through its rich information captured across hundreds of spectral bands, which embodies vital spatial and temporal information about the underlying land cover. Deep-learning-based hyperspectral data analysis methodologies have made significant advancements over the past few years. Despite their success, most deep learning frameworks for hyperspectral data classification tend to suffer in terms of computational and classification efficacy as the data size increases. This is largely due to their equal emphasis criteria on the rich spectral information present in the data, albeit all of the spectral information not being essential for hyperspectral data analysis. On the contrary, this redundant information present in the spectral bands can deter the performance of hyperspectral data analysis techniques. Therefore, in this work, we propose a novel bidirectional spectral attention mechanism, which is computationally efficient and capable of adaptive spectral information diversification through selective emphasis on spectral bands that comprise more information and suppress the ones with lesser information. The concept of 3D-convolutions in tandem with bidirectional long short-term memory (LSTM) is used in the proposed architecture as spectral attention mechanism. A feedforward neural network (FNN)-based supervised classification is then performed to validate the performance of our proposed approach. Experimental results reveal that the proposed hyperspectral data analysis model with spectral attention mechanism outperforms other spatial- and spectral-information-extraction-based hyperspectral data analysis techniques compared.",
        "link": "http://dx.doi.org/10.3390/rs14010217"
    },
    {
        "id": 10600,
        "title": "Real-Time Monitoring for Hydraulic States Based on Convolutional Bidirectional LSTM with Attention Mechanism",
        "authors": "Kyutae Kim, Jongpil Jeong",
        "published": "2020-12-11",
        "citations": 13,
        "abstract": "By monitoring a hydraulic system using artificial intelligence, we can detect anomalous data in a manufacturing workshop. In addition, by analyzing the anomalous data, we can diagnose faults and prevent failures. However, artificial intelligence, especially deep learning, needs to learn much data, and it is often difficult to get enough data at the real manufacturing site. In this paper, we apply augmentation to increase the amount of data. In addition, we propose real-time monitoring based on a deep-learning model that uses convergence of a convolutional neural network (CNN), a bidirectional long short-term memory network (BiLSTM), and an attention mechanism. CNN extracts features from input data, and BiLSTM learns feature information. The learned information is then fed to the sigmoid classifier to find out if it is normal or abnormal. Experimental results show that the proposed model works better than other deep-learning models, such as CNN or long short-term memory (LSTM).",
        "link": "http://dx.doi.org/10.3390/s20247099"
    },
    {
        "id": 10601,
        "title": "Ship Trajectory Prediction Based on Attention in Bidirectional Recurrent Neural Networks",
        "authors": "Chao Wang, Yuhui Fu",
        "published": "2020-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isctt51595.2020.00100"
    },
    {
        "id": 10602,
        "title": "The Bidirectional Social-Cognitive Mechanisms of the Social-Attention Symptoms of Autism",
        "authors": "Peter Mundy, Jenifer Bullen",
        "published": "2022-1-31",
        "citations": 14,
        "abstract": "Differences in social attention development begin to be apparent in the 6th to 12th month of development in children with Autism Spectrum Disorder (ASD) and theoretically reflect important elements of its neurodevelopmental endophenotype. This paper examines alternative conceptual views of these early social attention symptoms and hypotheses about the mechanisms involved in their development. One model emphasizes mechanism involved in the spontaneous allocation of attention to faces, or social orienting. Alternatively, another model emphasizes mechanisms involved in the coordination of attention with other people, or joint attention, and the socially bi-directional nature of its development. This model raises the possibility that atypical responses of children to the attention or the gaze of a social partner directed toward themselves may be as important in the development of social attention symptoms as differences in the development of social orienting. Another model holds that symptoms of social attention may be important to early development, but may not impact older individuals with ASD. The alterative model is that the social attention symptoms in infancy (social orienting and joint attention), and social cognitive symptoms in childhood and adulthood share common neurodevelopmental substrates. Therefore, differences in early social attention and later social cognition constitute a developmentally continuous axis of symptom presentation in ASD. However, symptoms in older individuals may be best measured with in vivo measures of efficiency of social attention and social cognition in social interactions rather than the accuracy of response on analog tests used in measures with younger children. Finally, a third model suggests that the social attention symptoms may not truly be a symptom of ASD. Rather, they may be best conceptualized as stemming from differences domain general attention and motivation mechanisms. The alternative argued for here that infant social attention symptoms meet all the criteria of a unique dimension of the phenotype of ASD and the bi-directional phenomena involved in social attention cannot be fully explained in terms of domain general aspects of attention development.",
        "link": "http://dx.doi.org/10.3389/fpsyt.2021.752274"
    },
    {
        "id": 10603,
        "title": "Breast Cancer Classification with Electronic Medical Records Using Hierarchical Attention Bidirectional Networks",
        "authors": "Dehua Chen, Guangjun Qian, Qiao Pan",
        "published": "2018-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm.2018.8621479"
    },
    {
        "id": 10604,
        "title": "Accurate Water Quality Prediction with Attention-Based Bidirectional Lstm and Encoder-Decoder",
        "authors": "Jing Bi, Zexian Chen, Haitao Yuan, Jia Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4207678"
    },
    {
        "id": 10605,
        "title": "Attention augmentation with multi-residual in bidirectional LSTM",
        "authors": "Ye Wang, Xinxiang Zhang, Mi Lu, Han Wang, Yoonsuck Choe",
        "published": "2020-4",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2019.10.068"
    },
    {
        "id": 10606,
        "title": "Bidirectional Decoding Tacotron for Attention Based Neural Speech Synthesis",
        "authors": "Wei Zhao, Li Xu",
        "published": "2022-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3561877.3561887"
    },
    {
        "id": 10607,
        "title": "An Algorithm for Point Cloud Object Classification Combining Bidirectional Attention Mechanism and Edge Convolution",
        "authors": "Zhiheng Geng",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eiecs59936.2023.10435614"
    },
    {
        "id": 10608,
        "title": "Chinese Event Extraction Based on Attention and Semantic Features: A Bidirectional Circular Neural Network",
        "authors": "Yue Wu, Junyi Zhang",
        "published": "2018-9-26",
        "citations": 8,
        "abstract": "Chinese event extraction uses word embedding to capture similarity, but suffers when handling previously unseen or rare words. From the test, we know that characters may provide some information that we cannot obtain in words, so we propose a novel architecture for combining word representations: character–word embedding based on attention and semantic features. By using an attention mechanism, our method is able to dynamically decide how much information to use from word or character level embedding. With the semantic feature, we can obtain some more information about a word from the sentence. We evaluate different methods on the CEC Corpus, and this method is found to improve performance.",
        "link": "http://dx.doi.org/10.3390/fi10100095"
    },
    {
        "id": 10609,
        "title": "Outcome-Oriented Predictive Process Monitoring with Attention-Based Bidirectional LSTM Neural Networks",
        "authors": "Jiaojiao Wang, Dongjin Yu, Chengfei Liu, Xiaoxiao Sun",
        "published": "2019-7",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icws.2019.00065"
    },
    {
        "id": 10610,
        "title": "Grain protein function prediction based on self-attention mechanism and bidirectional LSTM",
        "authors": "Jing Liu, Xinghua Tang, Xiao Guan",
        "published": "2023-1-19",
        "citations": 1,
        "abstract": "AbstractWith the development of genome sequencing technology, using computing technology to predict grain protein function has become one of the important tasks of bioinformatics. The protein data of four grains, soybean, maize, indica and japonica are selected in this experimental dataset. In this paper, a novel neural network algorithm Chemical-SA-BiLSTM is proposed for grain protein function prediction. The Chemical-SA-BiLSTM algorithm fuses the chemical properties of proteins on the basis of amino acid sequences, and combines the self-attention mechanism with the bidirectional Long Short-Term Memory network. The experimental results show that the Chemical-SA-BiLSTM algorithm is superior to other classical neural network algorithms, and can more accurately predict the protein function, which proves the effectiveness of the Chemical-SA-BiLSTM algorithm in the prediction of grain protein function. The source code of our method is available at https://github.com/HwaTong/Chemical-SA-BiLSTM.",
        "link": "http://dx.doi.org/10.1093/bib/bbac493"
    },
    {
        "id": 10611,
        "title": "Bidirectional LSTM and Self-Attention Mechanisms based Multi-Label Sentiment Analysis",
        "authors": "Aruna A.R",
        "published": "2025",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijsse.2025.10059046"
    },
    {
        "id": 10612,
        "title": "Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction",
        "authors": "Guimin Hu, Guangming Lu, Yi Zhao",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.51"
    },
    {
        "id": 10613,
        "title": "Multimodal Joint Prediction of Traffic Spatial-Temporal Data with Graph Sparse Attention Mechanism and Bidirectional Temporal Convolutional Network",
        "authors": "Dongran Zhang, Jiangnan Yan, Kemal Polat, Adi Alhudhaif, Jun Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4687890"
    },
    {
        "id": 10614,
        "title": "Bidirectional Joint Attention Mechanism for Target Tracking Algorithm",
        "authors": "Shuxian Wang, Haibo Ge, Wenhao Li, Li'Ang Liu, Ting Zhou, Shenghua Yang",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnlp55136.2022.00047"
    },
    {
        "id": 10615,
        "title": "Multi-Head Bidirectional Attention for MRC",
        "authors": "Dagmawi Moges, Hong Qu, Mingsheng Fu",
        "published": "2019-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3373419.3373430"
    },
    {
        "id": 10616,
        "title": "Improved Network Intrusion Classification with Attention-Assisted Bidirectional Lstm and Optimized Sparse Contractive Autoencoders",
        "authors": "Jing Bi, Ziyue Guan, Haitao Yuan, Jia Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4207683"
    },
    {
        "id": 10617,
        "title": "An intelligent Chatbot using deep learning with Bidirectional RNN and attention model",
        "authors": "Manyu Dhyani, Rajiv Kumar",
        "published": "2021",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.matpr.2020.05.450"
    },
    {
        "id": 10618,
        "title": "Using Bidirectional Long Short Term Memory with Attention Layer to Estimate Driver Behavior",
        "authors": "Shokoufeh Monjezi Kouchak, Ashraf Gaffar",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2019.00059"
    },
    {
        "id": 10619,
        "title": "Similarity and Farness Based Bidirectional Neural Co-Attention for Amharic Natural Language Inference",
        "authors": "Abebawu Eshetu, Getenesh Teshome, Ribka Alemayehu",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.winlp-1.4"
    },
    {
        "id": 10620,
        "title": "Research on sEMG Gesture Recognition Based on Hybrid Dilated Convolutional Neural Network Combining Bidirectional Gated Recurrent Unit And Attention Mechanism",
        "authors": "Kai Zhang, Feng Chen",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727886"
    },
    {
        "id": 10621,
        "title": "Bidirectional Attention Network for Monocular Depth Estimation",
        "authors": "Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat Kaur Bingbing Liu",
        "published": "2021-5-30",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9560885"
    },
    {
        "id": 10622,
        "title": "Occupancy Detection for General Households by Bidirectional LSTM with Attention",
        "authors": "Hisashi Oshima, Tsuyoshi Ishizone, Kazuyuki Nakamura, Tomoyuki Higuchi",
        "published": "2022-10-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon49645.2022.9968594"
    },
    {
        "id": 10623,
        "title": "Attention-based bidirectional gated recurrent unit neural networks for well logs prediction and lithology identification",
        "authors": "Lili Zeng, Weijian Ren, Liqun Shan",
        "published": "2020-11",
        "citations": 48,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.07.026"
    },
    {
        "id": 10624,
        "title": "Retracted: Rumor Detection with Bidirectional Graph Attention Networks",
        "authors": "Security and Communication Networks",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.1155/2024/9768194"
    },
    {
        "id": 10625,
        "title": "Audio2Face: Generating Speech/Face Animation from Single Audio with Attention-Based Bidirectional LSTM Networks",
        "authors": "Guanzhong Tian, Yi Yuan, Yong Liu",
        "published": "2019-7",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmew.2019.00069"
    },
    {
        "id": 10626,
        "title": "Bidirectional LSTM and Attention for Depression Detection on Clinical Interview Transcripts",
        "authors": "Mingzheng Li, Haojie Xu, Weifeng Liu, Jiangwei Liu",
        "published": "2022-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicn56848.2022.10006532"
    },
    {
        "id": 10627,
        "title": "Attention-based bidirectional-long short-term memory for abnormal human activity detection",
        "authors": "Manoj Kumar, Anoop Kumar Patel, Mantosh Biswas, S. Shitharth",
        "published": "2023-9-2",
        "citations": 3,
        "abstract": "AbstractAbnormal human behavior must be monitored and controlled in today’s technology-driven era, since it may cause damage to society in the form of assault or web-based violence, such as direct harm to a person or the propagation of hate crimes through the internet. Several authors have attempted to address this issue, but no one has yet come up with a solution that is both practical and workable. Recently, deep learning models have become popular as a means of handling massive amounts of data but their potential to categorize the aberrant human activity remains unexplored. Using a convolutional neural network (CNN), a bidirectional long short-term memory (Bi-LSTM), and an attention mechanism to pay attention to the unique spatiotemporal characteristics of raw video streams, a deep-learning approach has been implemented in the proposed framework to detect anomalous human activity. After analyzing the video, our suggested architecture can reliably assign an abnormal human behavior to its designated category. Analytic findings comparing the suggested architecture to state-of-the-art algorithms reveal an accuracy of 98.9%, 96.04%, and 61.04% using the UCF11, UCF50, and subUCF crime datasets, respectively.",
        "link": "http://dx.doi.org/10.1038/s41598-023-41231-0"
    },
    {
        "id": 10628,
        "title": "Remote Sensing Image Registration with Bidirectional Generative Adversarial and Cross-Attention",
        "authors": "Tian Zekun, Chen Ying, Sun Wei, Gao Han",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciibms60103.2023.10347610"
    },
    {
        "id": 10629,
        "title": "TA-BLSTM: Tag Attention-based Bidirectional Long Short-Term Memory for Service Recommendation in Mashup Creation",
        "authors": "Min Shi, Yufei Tang, Jianxun Liu",
        "published": "2019-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852438"
    },
    {
        "id": 10630,
        "title": "Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection",
        "authors": "Yefei Zha, Ruobing Li, Hui Lin",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.56"
    },
    {
        "id": 10631,
        "title": "Night setback identification of district heat substations using bidirectional long short term memory with attention mechanism",
        "authors": "Fan Zhang, Chris Bales, Hasan Fleyeh",
        "published": "2021-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.energy.2021.120163"
    },
    {
        "id": 10632,
        "title": "Accurate and lightweight MRI super-resolution via multi-scale bidirectional fusion attention network",
        "authors": "Ling Xu, Guanyao Li, Qiaochuan Chen",
        "published": "2022-12-15",
        "citations": 1,
        "abstract": "High-resolution magnetic resonance (MR) imaging has attracted much attention due to its contribution to clinical diagnoses and treatment. However, because of the interference of noise and the limitation of imaging equipment, it is expensive to generate a satisfactory image. Super-resolution (SR) is a technique that enhances an imaging system’s resolution, which is effective and cost-efficient for MR imaging. In recent years, deep learning-based SR methods have made remarkable progress on natural images but not on medical images. Most existing medical images SR algorithms focus on the spatial information of a single image but ignore the temporal correlation between medical images sequence. We proposed two novel architectures for single medical image and sequential medical images, respectively. The multi-scale back-projection network (MSBPN) is constructed of several different scale back-projection units which consist of iterative up- and down-sampling layers. The multi-scale machine extracts different scale spatial information and strengthens the information fusion for a single image. Based on MSBPN, we proposed an accurate and lightweight Multi-Scale Bidirectional Fusion Attention Network(MSBFAN) that combines temporal information iteratively. That supplementary temporal information is extracted from the adjacent image sequence of the target image. The MSBFAN can effectively learn both the spatio-temporal dependencies and the iterative refinement process with only a lightweight number of parameters. Experimental results demonstrate that our MSBPN and MSBFAN are outperforming current SR methods in terms of reconstruction accuracy and parameter quantity of the model.",
        "link": "http://dx.doi.org/10.1371/journal.pone.0277862"
    },
    {
        "id": 10633,
        "title": "An efficient gene expression data classification using optimized bidirectional long short-term memory with self attention mechanism",
        "authors": "S. Jacophine Susmi",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18387-6"
    },
    {
        "id": 10634,
        "title": "A novel Multi-Layer Attention Framework for visual description prediction using bidirectional LSTM",
        "authors": "Dinesh Naik, C. D. Jaidhar",
        "published": "2022-11-12",
        "citations": 6,
        "abstract": "AbstractThe massive influx of text, images, and videos to the internet has recently increased the challenge of computer vision-based tasks in big data. Integrating visual data with natural language to generate video explanations has been a challenge for decades. However, recent experiments on image/video captioning that employ Long-Short-Term-Memory (LSTM) have piqued the interest of researchers studying its possible application in video captioning. The proposed video captioning architecture combines the bidirectional multilayer LSTM (BiLSTM) encoder and unidirectional decoder. The innovative architecture also considers temporal relations when creating superior global video representations. In contrast to the majority of prior work, the most relevant features of a video are selected and utilized specifically for captioning purposes. Existing methods utilize a single-layer attention mechanism for linking visual input with phrase meaning. This approach employs LSTMs and a multilayer attention mechanism to extract characteristics from movies, construct links between multi-modal (words and visual material) representations, and generate sentences with rich semantic coherence. In addition, we evaluated the performance of the suggested system using a benchmark dataset for video captioning. The obtained results reveal superior performance relative to state-of-the-art works in METEOR and promising performance relative to the BLEU score. In terms of quantitative performance, the proposed approach outperforms most existing methodologies.",
        "link": "http://dx.doi.org/10.1186/s40537-022-00664-6"
    },
    {
        "id": 10635,
        "title": "Vehicle Destination Prediction Using Bidirectional LSTM with Attention Mechanism",
        "authors": "Pietro Casabianca, Yu Zhang, Miguel Martínez-García, Jiafu Wan",
        "published": "2021-12-17",
        "citations": 6,
        "abstract": "Satellite navigation has become ubiquitous to plan and track travelling. Having access to a vehicle’s position enables the prediction of its destination. This opens the possibility to various benefits, such as early warnings of potential hazards, route diversions to pass traffic congestion, and optimizing fuel consumption for hybrid vehicles. Thus, reliably predicting destinations can bring benefits to the transportation industry. This paper investigates using deep learning methods for predicting a vehicle’s destination based on its journey history. With this aim, Dense Neural Networks (DNNs), Long Short-Term Memory (LSTM) networks, Bidirectional LSTM (BiLSTM), and networks with and without attention mechanisms are tested. Especially, LSTM and BiLSTM models with attention mechanism are commonly used for natural language processing and text-classification-related applications. On the other hand, this paper demonstrates the viability of these techniques in the automotive and associated industrial domain, aimed at generating industrial impact. The results of using satellite navigation data show that the BiLSTM with an attention mechanism exhibits better prediction performance destination, achieving an average accuracy of 96% against the test set (4% higher than the average accuracy of the standard BiLSTM) and consistently outperforming the other models by maintaining robustness and stability during forecasting.",
        "link": "http://dx.doi.org/10.3390/s21248443"
    },
    {
        "id": 10636,
        "title": "RONet: Real-time Range-only Indoor Localization via Stacked Bidirectional LSTM with Residual Attention",
        "authors": "Hyungtae Lim, Changgue Park, Hyun Myung",
        "published": "2019-11",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968551"
    },
    {
        "id": 10637,
        "title": "Improving flight delays prediction by developing attention-based bidirectional LSTM network",
        "authors": "Maged Mamdouh, Mostafa Ezzat, Hesham Hefny",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121747"
    },
    {
        "id": 10638,
        "title": "ABR-HIC: Attention Based Bidirectional RNN for Hierarchical Industry Classification",
        "authors": "Rongzhe Wei, Qinghua Zheng, Bo Dong, Kuanzheng Yang, Huan He, Jianfei Ruan",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9006526"
    },
    {
        "id": 10639,
        "title": "MAC Protocol Based IoT Network Intrusion Detection Using Improved Efficient Shuffle Bidirectional COOT Channel Attention Network",
        "authors": "Nanavath Kiran Singh Nayak, Budhaditya Bhattacharyya",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3299031"
    },
    {
        "id": 10640,
        "title": "Malicious URL Detection Model Based on Bidirectional Gated Recurrent Unit and Attention Mechanism",
        "authors": "Tiefeng Wu, Miao Wang, Yunfang Xi, Zhichao Zhao",
        "published": "2022-12-2",
        "citations": 4,
        "abstract": "With the rapid development of Internet technology, numerous malicious URLs have appeared, which bring a large number of security risks. Efficient detection of malicious URLs has become one of the keys for defense against cyber attacks. Deep learning methods bring new developments to the identification of malicious web pages. This paper proposes a malicious URL detection method based on a bidirectional gated recurrent unit (BiGRU) and attention mechanism. The method is based on the BiGRU model. A regularization operation called a dropout mechanism is added to the input layer to prevent the model from overfitting, and an attention mechanism is added to the middle layer to strengthen the feature learning of URLs. Finally, the deep learning network DA-BiGRU model is formed. The experimental results demonstrate that the proposed method can achieve better classification results in malicious URL detection, which has high significance for practical applications.",
        "link": "http://dx.doi.org/10.3390/app122312367"
    },
    {
        "id": 10641,
        "title": "Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition",
        "authors": "Ziping Zhao, Yu Zheng, Zixing Zhang, Haishuai Wang, Yiqin Zhao, Chao Li",
        "published": "2018-9-2",
        "citations": 37,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-1477"
    },
    {
        "id": 10642,
        "title": "Bidirectional LSTM with attention mechanism and convolutional layer for text classification",
        "authors": "Gang Liu, Jiabao Guo",
        "published": "2019-4",
        "citations": 639,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2019.01.078"
    },
    {
        "id": 10643,
        "title": "Insulator Defect Detection Method upon Fused Attention Mechanism and Bidirectional Feature Fusion",
        "authors": "Yiming Chen",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Abstract\nInsulators are important components for achieving electrical insulation and mechanical support, but they are prone to various defects in harsh operating environments, which can damage their mechanical strength and insulation performance. This article proposes the Shuffle YOLOv7 model based on the YOLOv7 algorithm for insulator defect detection, aiming to solve the weakness of low precision in traditional object detection algorithms when facing complex backgrounds and small-sized defects. To address the issue of low attention to flashover faults in traditional algorithms, the ShuffleAttention fusion attention mechanism is supplied to concentrate on both intra-channel and inter-channel deep features, and the original PANet structure is replaced with a pyramid which has a bidirectional feature fusion structure to enhance the network’s feature extraction ability. The Focal-EIOU LOSS optimization method focuses on high-quality prior boxes to improve model accuracy, and the effectiveness of the optimization method is verified through ablation experiments. These results of the experiment show that the proposed algorithm achieves varying degrees of performance improvement in terms of precision, recall, average precision, and overall loss compared to mainstream object detection algorithms in detecting insulator damage and flashover.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2632/1/012013"
    },
    {
        "id": 10644,
        "title": "Hybrid Sampling and Similarity Attention Layer in Bidirectional Long Short  Term Memory in Credit Card Fraud Detection",
        "authors": "",
        "published": "2022-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22266/ijies2022.1231.04"
    },
    {
        "id": 10645,
        "title": "An Abnormal Traffic Detection Based on Attention-Guided Bidirectional GRU",
        "authors": "Hao Li, Erlu He, Chunxu Kuang, Xiaopeng Yang, Xiangbo Wu, Zhe Jia",
        "published": "2022-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icct56141.2022.10072487"
    },
    {
        "id": 10646,
        "title": "Attend, Correct And Focus: A Bidirectional Correct Attention Network For Image-Text Matching",
        "authors": "Yang Liu, Huaqiu Wang, Fanyang Meng, Mengyuan Liu, Hong Liu",
        "published": "2021-9-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506438"
    },
    {
        "id": 10647,
        "title": "Inter-Personal Relation Extraction Model Based on Bidirectional GRU and Attention Mechanism",
        "authors": "Yuming Li, Pin Ni, Gangmin Li, Xutao Wang, Zhenjin Dai",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc47050.2019.9064050"
    },
    {
        "id": 10648,
        "title": "Attention-based bidirectional LSTM for Chinese punctuation prediction",
        "authors": "Jinliang Li, Chengfeng Yin, Zhen Jia, Tianrui Li, Min Tang",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813273238_0063"
    },
    {
        "id": 10649,
        "title": "Attention-based bidirectional LSTM for Chinese punctuation prediction",
        "authors": "Jinliang Li, Chengfeng Yin, Zhen Jia, Tianrui Li, Min Tang",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813273238_0090"
    },
    {
        "id": 10650,
        "title": "Attention mechanism based bidirectional LSTM model for broadband power amplifier linearization",
        "authors": "Rina Su, Jiacheng Wang, Gaoming Xu, Taijun Liu",
        "published": "2023-7",
        "citations": 1,
        "abstract": "AbstractIn this letter, a novel digital predistortion (DPD) model for broadband power amplifier (PA) linearization is proposed, namely Attention Mechanism based Bidirectional Long Short‐term Memory Network (AM‐BiLSTM) model. In order to verify the linearization performance of the AM‐BiLSTM model in digital predistortion process, a 100 MHz bandwidth 5G new radio (5G NR) signal is employed to test a sub‐6G PA operating at 2.6‐GHz. The experimental results show that the adjacent channel power ratio (ACPR) of the PA with AM‐BiLSTM model can be improved by 24 dB which is 6 dB and 3 dB better than the generalized memory polynomial (GMP) model and the Chebyshev polynomials LSTM (CP‐LSTM) model in ref [1], repspectively. Therefore, the proposed AM‐BiLSTM model is very effective for the DPD linearization of broadband PAs.",
        "link": "http://dx.doi.org/10.1049/ell2.12869"
    },
    {
        "id": 10651,
        "title": "Wellhead Compressor Failure Prediction Using Attention-based Bidirectional LSTMs with Data Reduction Techniques",
        "authors": "Wirasak Chomphu, Boonserm Kijsirikul",
        "published": "2020-3-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3388142.3388154"
    },
    {
        "id": 10652,
        "title": "Research and application of recommendation algorithm based on bidirectional attention model",
        "authors": "Jiahua Wan, Chengrui Ji, Yiwen Zhang",
        "published": "2021-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3508546.3508553"
    },
    {
        "id": 10653,
        "title": "Bidirectional-GRU Based on Attention Mechanism for Aspect-level Sentiment Analysis",
        "authors": "Zhai Penghua, Zhang Dingyi",
        "published": "2019-2-22",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3318299.3318368"
    },
    {
        "id": 10654,
        "title": "Sarcasm Detection Using Multi-Head Attention Based Bidirectional LSTM",
        "authors": "Avinash Kumar, Vishnu Teja Narapareddy, Veerubhotla Aditya Srikanth, Aruna Malapati, Lalita Bhanu Murthy Neti",
        "published": "2020",
        "citations": 97,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2963630"
    },
    {
        "id": 10655,
        "title": "AB-LSTM: Attention Bidirectional Long Short-Term Memory for Multivariate Time-Series Forecasting",
        "authors": "Xiaofeng Zhou, Andri Pranolo, Yingchi Mao",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2e357697.2023.10262559"
    },
    {
        "id": 10656,
        "title": "Bidirectional Transformer with Sparse Attention for Gastrointestinal Disease Recognition",
        "authors": "Xuewei Cao, Hanlun Guan",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icbaie59714.2023.10281350"
    },
    {
        "id": 10657,
        "title": "Relation extraction in Chinese using attention-based bidirectional long short-term memory networks",
        "authors": "Yanzi Zhang",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "Relation extraction is an important topic in information extraction, as it is used to create large-scale knowledge graphs for a variety of downstream applications. Its goal is to find and extract semantic links between entity pairs in natural language sentences. Deep learning has substantially advanced neural relation extraction, allowing for the autonomous learning of semantic features. We offer an effective Chinese relation extraction model that uses bidirectional LSTM (Bi-LSTM) and an attention mechanism to extract crucial semantic information from phrases without relying on domain knowledge from lexical resources or language systems in this study. The attention mechanism included into the Bi-LSTM network allows for automatic focus on key words. Two benchmark datasets were used to create and test our models: Chinese SanWen and FinRE. The experimental results show that the SanWen dataset model outperforms the FinRE dataset model, with area under the receiver operating characteristic curve values of 0.70 and 0.50, respectively. The models trained on the SanWen and FinRE datasets achieve values of 0.44 and 0.19, respectively, for the area under the precision-recall curve. In addition, the results of repeated modeling experiments indicated that our proposed method was robust and reproducible.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1509"
    },
    {
        "id": 10658,
        "title": "An Anomaly Detection Approach Based on Bidirectional Temporal Convolutional Network and Multi-Head Attention Mechanism",
        "authors": "Rui Wang, Jiayao Li",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "Anomaly detection aims at detecting the data instances that deviate from the majority of data, and it is widely used in various fields for its ability to ensure the quality of the overall data. However, traditional anomaly detection methods face the problems such as low efficiency due to high data complexity and lack of data labels. At the same time, most methods only learn the forward features of time-series data, while lacking attention to the reverse features. For these disadvantages, this paper designs an anomaly detection approach called BiTCN-MHA based on the bidirectional temporal convolutional network (BiTCN) and multi-head attention (MHA) mechanism, which learns the features of anomalous data by capturing the forward and reverse temporal features in the time-series data, as well as solves the problems of feature information overload and neuron “death” by using MHA mechanism and ELU activation function, respectively, thereby quickly and accurately detecting anomalous data. Extensive experiments on six public datasets show that compared with eight state-of-the-arts, the proposed BiTCN-MHA method can improve the precision, recall, AUC and F1-Score by about 6.10%, 10.16%, 4.06% and 8.50%, respectively, especially having better detection performance on small time-series data.",
        "link": "http://dx.doi.org/10.5755/j01.itc.53.1.34254"
    },
    {
        "id": 10659,
        "title": "Joint extraction of Chinese cybersecurity events based on bidirectional TCN and attention",
        "authors": "Yonggan Zhang, Fangjie Wan, Danyang Yang",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2646773"
    }
]