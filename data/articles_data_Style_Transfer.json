[
    {
        "id": 13371,
        "title": "Arbitrary Style Transfer with Style Enhancement and Structure Retention",
        "authors": "sijia Yang, Yun Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4305461"
    },
    {
        "id": 13372,
        "title": "Style Transfer Using AI",
        "authors": "Mourya Teja Kunuku",
        "published": "No Date",
        "citations": 0,
        "abstract": "Style Transfer Using AI",
        "link": "http://dx.doi.org/10.31219/osf.io/7detr"
    },
    {
        "id": 13373,
        "title": "Principal Style Components: Expressive Style Control and Cross-Speaker Transfer in Neural TTS",
        "authors": "Alexander Sorin, Slava Shechtman, Ron Hoory",
        "published": "2020-10-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1854"
    },
    {
        "id": 13374,
        "title": "Style transfer and evaluation: an intuitive style transfer quantitative evaluation method",
        "authors": "Yuchen Jiang, Bowen Zhang, Wanjie Zhang",
        "published": "2022-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2636921"
    },
    {
        "id": 13375,
        "title": "Neural Style Transfer",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_300367"
    },
    {
        "id": 13376,
        "title": "Neural Style Transfer with automatic style weight searching",
        "authors": "Xiang Zhou",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Neural Style Transfer (NST) has garnered significant attention in the field of computer vision. Previous research in this area has made important breakthroughs, creating various style transfer models and innovative architectural designs, and has achieved success in commercial applications. This paper presents a novel design for automatic style weight determination based on constraining the initial total loss within a healthy range and finding the optimal solution through grid search. This design aims to harness the potential of existing NST models by automating the optimization of neural style transfer hyperparameters. The paper first discusses the impact of content weights and loss weights on the generated images and validates the influence of weight ratios on image quality through experimental adjustments of weight proportions. It also confirms that the initial loss essentially defines the optimization space of the optimizer. The paper explores the significance of the initial loss and proposes a method to improve image generation quality by constraining the initial loss range.",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241242"
    },
    {
        "id": 13377,
        "title": "Hair Shading Style Transfer for Manga with cGAN",
        "authors": "Masashi Aizawa, Ryohei Orihara, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008961405870594"
    },
    {
        "id": 13378,
        "title": "Separating Style and Content for Generalized Style Transfer",
        "authors": "Yexun Zhang, Ya Zhang, Wenbin Cai",
        "published": "2018-6",
        "citations": 83,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00881"
    },
    {
        "id": 13379,
        "title": "Arbitrary Style Transfer With Style-Attentional Networks",
        "authors": "Dae Young Park, Kwang Hee Lee",
        "published": "2019-6",
        "citations": 196,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00603"
    },
    {
        "id": 13380,
        "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
        "authors": "Daxin Tan, Tan Lee",
        "published": "2021-8-30",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1129"
    },
    {
        "id": 13381,
        "title": "Appearance Constrained Topology Optimization Using Neural Style Transfer",
        "authors": "Praveen Vulimiri, Albert To",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0004695v"
    },
    {
        "id": 13382,
        "title": "Neural Artistic Style Transfer with Conditional Adversarial Network",
        "authors": "Pathirage  Nipun Deelaka",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358610"
    },
    {
        "id": 13383,
        "title": "A Comparative Analysis of Fast-style Transfer and VGG19-GramMatrix Approach to Neural Style Transfer",
        "authors": "Aryan Ratra, Aryan Agarwal, Vikrant Sharma, Satvik Vats, Sunny Singh, Vinay Kukreja",
        "published": "2023-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiss58487.2023.10250595"
    },
    {
        "id": 13384,
        "title": "Content and Style Disentanglement for Artistic Style Transfer",
        "authors": "Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, Bjorn Ommer",
        "published": "2019-10",
        "citations": 88,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00452"
    },
    {
        "id": 13385,
        "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
        "authors": "Yookyung Shin, Younggun Lee, Suhee Jo, Yeongtae Hwang, Taesu Kim",
        "published": "2022-9-18",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10131"
    },
    {
        "id": 13386,
        "title": "Improving Universal Style Transfer using Sub-style Decomposition",
        "authors": "Paraskevas Pegios, Nikolaos Passalis, Anastasios Tefas",
        "published": "2020-9-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3411408.3411411"
    },
    {
        "id": 13387,
        "title": "Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss",
        "authors": "Dino Rattcliffe, You Wang, Alex Mansbridge, Penny Karanasou, Alexis Moinet, Marius Cotescu",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10572"
    },
    {
        "id": 13388,
        "title": "How Positive Are You: Text Style Transfer using Adaptive Style Embedding",
        "authors": "Heejin Kim, Kyung-Ah Sohn",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.191"
    },
    {
        "id": 13389,
        "title": "STaDA: Style Transfer as Data Augmentation",
        "authors": "Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic",
        "published": "2019",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007353401070114"
    },
    {
        "id": 13390,
        "title": "STaDA: Style Transfer as Data Augmentation",
        "authors": "Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007353400002108"
    },
    {
        "id": 13391,
        "title": "One-shot Image Style Transfer via Pre-trained GAN Inversion",
        "authors": "Zegang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.164402981.12432631/v1"
    },
    {
        "id": 13392,
        "title": "Neural Style Transfer for Vector Graphics",
        "authors": "Ivan Jarsky, Valeria Efimova, Artyom Chebykin, Viacheslav Shalamov, Andrey Filchenkov",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012438200003660"
    },
    {
        "id": 13393,
        "title": "Supervised Learning for Makeup Style Transfer",
        "authors": "Natalia Strawa, Grzegorz Sarwas",
        "published": "2022",
        "citations": 0,
        "abstract": "This paper addresses the problem of using deep learning for makeup style transfer. For solving this problem, we propose a new supervised method. Additionally, we present a technique for creating a synthetic dataset for makeup transfer used to train our model. The obtained results were compared with six popular methods for makeup transfer using three metrics. The tests were carried out on four available data sets. The proposed method, in many respects, is competitive with the methods used in the literature. Thanks to images of faces with generated synthetic makeup, the proposed method learns to better transfer details, and the learning process is significantly accelerated.",
        "link": "http://dx.doi.org/10.24132/csrn.3201.25"
    },
    {
        "id": 13394,
        "title": "SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text Style Transfer",
        "authors": "Jicheng Li, Yang Feng, Jiao Ou",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533731"
    },
    {
        "id": 13395,
        "title": "Flexible Selecting of Style to Content Ratio in Neural Style Transfer",
        "authors": "Taehee Jeong, Anubha Mandal",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00046"
    },
    {
        "id": 13396,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 13397,
        "title": "Improving Material Translation Based on Style Image Retrieval for Neural Style Transfer",
        "authors": "Gibran Benitez-Garcia, Keiji Yanai",
        "published": "2020-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36463/idw.2020.0981"
    },
    {
        "id": 13398,
        "title": "Style Agnostic 3D Reconstruction via Adversarial Style Transfer",
        "authors": "Felix Petersen, Bastian Goldluecke, Oliver Deussen, Hilde Kuehne",
        "published": "2022-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv51458.2022.00233"
    },
    {
        "id": 13399,
        "title": "Network of Steel: Neural Font Style Transfer from Heavy Metal to Corporate Logos",
        "authors": "Aram Ter-Sarkisov",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009343906210629"
    },
    {
        "id": 13400,
        "title": "Transfer and Extraction of the Style of Handwritten Letters using Deep Learning",
        "authors": "Omar Mohammed, Gérard Bailly, Damien Pellier",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007388606770684"
    },
    {
        "id": 13401,
        "title": "Font Style Transfer Using Neural Style Transfer and Unsupervised Cross-domain Transfer",
        "authors": "Atsushi Narusawa, Wataru Shimoda, Keiji Yanai",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-21074-8_9"
    },
    {
        "id": 13402,
        "title": "On Text Style Transfer via Style-Aware Masked Language Models",
        "authors": "Sharan Narasimhan, Pooja H, Suvodip Dey, Maunendra Sankar Desarkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.25"
    },
    {
        "id": 13403,
        "title": "Image Style Transfer Method Based on Improved Style Loss Function",
        "authors": "Hanmin Ye, Wenjie Liu, Yingzhi Liu",
        "published": "2020-12-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic49862.2020.9338927"
    },
    {
        "id": 13404,
        "title": "Preserving Fine-Grained Style Consistency for Universal Image Style Transfer",
        "authors": "Yubo Zhu, Xinxiao Wu, Jialu Chen",
        "published": "2022-11-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac57282.2022.10023773"
    },
    {
        "id": 13405,
        "title": "Arbitrary style transfer via content consistency and style consistency",
        "authors": "Xiaoming Yu, Gan Zhou",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00371-023-02855-5"
    },
    {
        "id": 13406,
        "title": "Diving Deeper into Volume Style Transfer",
        "authors": "Mike Navarro",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587421.3595448"
    },
    {
        "id": 13407,
        "title": "Neural Style Transfer for 3d Meshes",
        "authors": "Hongyuan Kang, Xiao Dong, Juan Cao, Zhonggui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4363664"
    },
    {
        "id": 13408,
        "title": "Tackling Data Bias in Painting Classification with Style Transfer",
        "authors": "Mridula Vijendran, Frederick Li, Hubert P. H. Shum",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011776600003417"
    },
    {
        "id": 13409,
        "title": "Style Fader Generative Adversarial Networks for Style Degree Controllable Artistic Style Transfer",
        "authors": "Zhiwen Zuo, Lei Zhao, Shuobin Lian, Haibo Chen, Zhizhong Wang, Ailin Li, Wei Xing, Dongming Lu",
        "published": "2022-7",
        "citations": 4,
        "abstract": "Artistic style transfer is the task of synthesizing content images with learned artistic styles. Recent studies have shown the potential of Generative Adversarial Networks (GANs) for producing artistically rich stylizations. Despite the promising results, they usually fail to control the generated images' style degree, which is inflexible and limits their applicability for practical use. To address the issue, in this paper, we propose a novel method that for the first time allows adjusting the style degree for existing GAN-based artistic style transfer frameworks in real time after training. Our method introduces two novel modules into existing GAN-based artistic style transfer frameworks: a Style Scaling Injection (SSI) module and a Style Degree Interpretation (SDI) module. The SSI module accepts the value of Style Degree Factor (SDF)  as the input and outputs parameters that scale the feature activations in existing models, offering control signals to alter the style degrees of the stylizations.  And the SDI module interprets the output probabilities of a multi-scale content-style binary classifier as the style degrees, providing a mechanism to parameterize the style degree of the stylizations. Moreover, we show that after training our method can enable existing GAN-based frameworks to produce over-stylizations. The proposed method can facilitate many existing GAN-based artistic style transfer frameworks with marginal extra training overheads and modifications. Extensive qualitative evaluations on two typical GAN-based style transfer models demonstrate the effectiveness of the proposed method for gaining style degree control for them.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/693"
    },
    {
        "id": 13410,
        "title": "Applying Neural Style Transfer to Transform Images into a Different Style Domain by the NST Method",
        "authors": "A Vishnukumar, Kamalanaban E, S Gugan, Abdul Gafoor A, Mukesh T",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA group of software algorithms known as neural style transfer (NST) alter digital pictures or videos to take on the look or visual style of another image. It is a machine learning-based optimization approach. In this study, we employ software that translates a picture's style onto a second image, referred to as the content image. Images may be encrypted, and the data can then be transferred to the recipient as an encrypted picture. The purpose of picture alteration is served through the usage of deep neural networks. The NST method may be used with this paper to modify two photos in photo-editing software. The created image can be outputted in whatever size the user desires. Hence The neural style transfer.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2647275/v1"
    },
    {
        "id": 13411,
        "title": "Balancing Content and Style with Two-Stream FCNs for Style Transfer",
        "authors": "Duc Minh Vo, Trung-Nghia Le, Akihiro Sugimoto",
        "published": "2018-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv.2018.00152"
    },
    {
        "id": 13412,
        "title": "Text Style Transfer via Learning Style Instance Supported Latent Space",
        "authors": "Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, Maosong Sun",
        "published": "2020-7",
        "citations": 14,
        "abstract": "Text style transfer pursues altering the style of a sentence while remaining its main content unchanged. Due to the lack of parallel corpora, most recent work focuses on unsupervised methods and has achieved noticeable progress. Nonetheless, the intractability of completely disentangling content from style for text leads to a contradiction of content preservation and style transfer accuracy. To address this problem, we propose a style instance supported method, StyIns. Instead of representing styles with embeddings or latent variables learned from single sentences, our model leverages the generative flow technique to extract underlying stylistic properties from multiple instances of each style, which form a more discriminative and expressive latent style space. By combining such a space with the attention-based structure, our model can better maintain the content and simultaneously achieve high transfer accuracy. Furthermore, the proposed method can be flexibly extended to semi-supervised learning so as to utilize available limited paired data. Experiments on three transfer tasks, sentiment modification, formality rephrasing, and poeticness generation, show that StyIns obtains a better balance between content and style, outperforming several recent baselines.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/526"
    },
    {
        "id": 13413,
        "title": "Text Style Transfer: Leveraging a Style Classifier on Entangled Latent Representations",
        "authors": "Xiaoyan Li, Sun Sun, Yunli Wang",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.repl4nlp-1.9"
    },
    {
        "id": 13414,
        "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
        "authors": "Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang",
        "published": "2019",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1601"
    },
    {
        "id": 13415,
        "title": "QR code arbitrary style transfer algorithm based on style matching layer",
        "authors": "Hai-Sheng Li, Jingyin Chen, Huafeng Huang",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-17231-7"
    },
    {
        "id": 13416,
        "title": "PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models",
        "authors": "Tai-Yin Chiu, Danna Gurari",
        "published": "2022-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00769"
    },
    {
        "id": 13417,
        "title": "Style-ERD: Responsive and Coherent Online Motion Style Transfer",
        "authors": "Tianxin Tao, Xiaohang Zhan, Zhongquan Chen, Michiel van de Panne",
        "published": "2022-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00648"
    },
    {
        "id": 13418,
        "title": "“One-Shot” Super-Resolution via Backward Style Transfer for Fast High-Resolution Style Transfer",
        "authors": "Jikang Cheng, Zhen Han, Zhongyuan Wang, Liang Chen",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsp.2021.3098230"
    },
    {
        "id": 13419,
        "title": "Style-Aware Normalized Loss for Improving Arbitrary Style Transfer",
        "authors": "Jiaxin Cheng, Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Prem Natarajan",
        "published": "2021-6",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.00020"
    },
    {
        "id": 13420,
        "title": "In-camera, Photorealistic Style Transfer for On-set Automatic Grading",
        "authors": "Itziar Zabaleta, Marcelo Bertalmio",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5594/m001835"
    },
    {
        "id": 13421,
        "title": "Fonts Style Transfer using Conditional GAN",
        "authors": "Naho Sakao, Yoshinori Dobashi",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cw.2019.00075"
    },
    {
        "id": 13422,
        "title": "Reimagining Animation Making through Style Transfer",
        "authors": "Sujin Kim",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3610591.3616435"
    },
    {
        "id": 13423,
        "title": "Voice Conversion Using Speech-to-Speech Neuro-Style Transfer",
        "authors": "Ehab A. AlBadawy, Siwei Lyu",
        "published": "2020-10-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3056"
    },
    {
        "id": 13424,
        "title": "EI-StyleGAN: A High Quality Face Cartoon Style Transfer Model",
        "authors": "Rui Li, Fenli Fu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCartoonization of face images is a new art forms applicable to various scenes, but there are problems in the research of incomplete extraction of image details after style transfer, simple superposition between domains, and poor generation quality. Since StyleGAN has better results in the generation of artistic images, on this basis, this paper proposes a new EI-StyleGAN to construct a network model applicable to face style migration, in which the inner and outer style restriction modules are introduced to obtain different detailed features in the images of the two domains, respectively. Meanwhile, the latent space coding is obtained by image inversion before inputting into the generated model, which can perform the transformation between images between the source and target domains. The whole process adopts an incremental generation strategy to smoothly transform the generation space of the model to the target domain, and gradually generates high-quality face cartoon style transfer result image. Experiments demonstrate the effectiveness of the method in improving the image quality after style transfer and the smooth conversion between different styles.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230687/v1"
    },
    {
        "id": 13425,
        "title": "Towards Real-time G-buffer-Guided Style Transfer in Computer Games",
        "authors": "Eleftherios Ioannou, Steve Maddock",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170654648.86162009/v1"
    },
    {
        "id": 13426,
        "title": "Name your style: text-guided artistic style transfer",
        "authors": "Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, Vicky Kalogeiton",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00359"
    },
    {
        "id": 13427,
        "title": "New Image Processing: VGG Image Style Transfer with Gram Matrix Style Features",
        "authors": "Longqing Zhang, Zishang Wang, Jinwen He, Yixuan Li",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405398"
    },
    {
        "id": 13428,
        "title": "Shanghai-Style Realistic Style Watercolor Painting Style Transfer by Using RSIM Evaluation",
        "authors": "Rongrong Fu, Yuanyuan Wang, Jiajun Lin, Ruiyang Fan, Han Zhao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-6372-7_72"
    },
    {
        "id": 13429,
        "title": "Deep Style Transfer",
        "authors": "Dongdong Chen, Lu Yuan, Gang Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_863"
    },
    {
        "id": 13430,
        "title": "An Investigation of Applications of Neural Style Transfer to Forensic Footwear Comparison",
        "authors": "Gregory Stock",
        "published": "2023",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.6028/nist.ir.8460"
    },
    {
        "id": 13431,
        "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
        "authors": "Shifeng Pan, Lei He",
        "published": "2021-8-30",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-979"
    },
    {
        "id": 13432,
        "title": "Restore the Incomplete Calligraphy Based on Style Transfer",
        "authors": "Mengxi Qin, Xin Chen",
        "published": "2019-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866187"
    },
    {
        "id": 13433,
        "title": "Digital Image Art Style Transfer Algorithm and Simulation Based on Deep Learning Model",
        "authors": "Nitin Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to solve the problems of poor region delineation and boundary artifacts in Indian style migration of images, an improved Variational Autoencoder (VAE) method for dress style migration is proposed. Firstly, the Yolo v3 model is used to quickly identify the dress localization of the input image, and then the classical semantic segmentation algorithm (FCN) is used to finely delineate the desired dress style migration region twice, and finally the trained VAE model is used to generate the migrated Indian style image using a decision support system. The results show that, compared with the traditional style migration model, the improved VAE style migration model can obtain finer synthetic images for dress style migration, and can adapt to different Indian traditional styles to meet the application requirements of dress style migration scenarios. We evaluated several deep learning based models and achieved BLEU value of 0.6 on average. The transformer-based model outperformed the other models, achieving a BLEU value of up to 0.72.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1221051/v1"
    },
    {
        "id": 13434,
        "title": "Beyond Simple Text Style Transfer: Unveiling Compound Text Style Transfer with Prompt-Based Pre-Trained Language Models",
        "authors": "Shuai Ju, Chenxu Wang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447801"
    },
    {
        "id": 13435,
        "title": "Photorealistic Style Transfer for Cinema Shoots",
        "authors": "Itziar Zabaleta, Marcelo Bertalmio",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvcs.2018.8496499"
    },
    {
        "id": 13436,
        "title": "Style Mixer: Semantic‐aware Multi‐Style Transfer Network",
        "authors": "Zixuan Huang, Jinghuai Zhang, Jing Liao",
        "published": "2019-10",
        "citations": 11,
        "abstract": "AbstractRecent neural style transfer frameworks have obtained astonishing visual quality and flexibility in Single‐style Transfer (SST), but little attention has been paid to Multi‐style Transfer (MST) which refers to simultaneously transferring multiple styles to the same image. Compared to SST, MST has the potential to create more diverse and visually pleasing stylization results. In this paper, we propose the first MST framework to automatically incorporate multiple styles into one result based on regional semantics. We first improve the existing SST backbone network by introducing a novel multi‐level feature fusion module and a patch attention module to achieve better semantic correspondences and preserve richer style details. For MST, we designed a conceptually simple yet effective region‐based style fusion module to insert into the backbone. It assigns corresponding styles to content regions based on semantic matching, and then seamlessly combines multiple styles together. Comprehensive evaluations demonstrate that our framework outperforms existing works of SST and MST.",
        "link": "http://dx.doi.org/10.1111/cgf.13853"
    },
    {
        "id": 13437,
        "title": "Line Search-Based Feature Transformation for Fast, Stable, and Tunable Content-Style Control in Photorealistic Style Transfer",
        "authors": "Tai-Yin Chiu, Danna Gurari",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00033"
    },
    {
        "id": 13438,
        "title": "Arbitrary Style Transfer Based on Content Integrity and Style Consistency Enhancement",
        "authors": "Lu Kang, Guoqiang Xiao, Michael S. Lew, Song Wu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447437"
    },
    {
        "id": 13439,
        "title": "Adaptive Style Modulation for Artistic Style Transfer",
        "authors": "Yipeng Zhang, Bingliang Hu, Yingying Huang, Chi Gao, Quan Wang",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-022-11135-7"
    },
    {
        "id": 13440,
        "title": "Artistic Style Transfer using Deep Learning and Style Fusion- a Review",
        "authors": " Mohammed Mutahar,  Dr. R. Chinnaiyan",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "In recent years, after the study ‘A Neural Algorithm of Artistic Style’ published by Gatys et al. in 2016b, research on style transfer boomed drastically. Style transfer is the process of copying an art style from a ‘style image’ to the contents of the ‘content image’ and producing a ‘draft image’ that is on par with respect to quality expectations. This paper explores different techniques of achieving style transformations namely Style Fusion and Convolutional Neural Networks (CNNs). Although CNNs are the state-of-the-art architecture to tackle cognitive visual tasks, and that they clearly perform much better than most conventional algorithms, the image processing-based style fusion method comes close to the CNN in terms of image output quality and supersedes in terms of time and computation and resources complexity. The procedure of both of these methods has been discussed in detail in this paper and it was concluded that CNNs have a lot more room for improvement that can be facilitated by the availability of better and larger datasets.",
        "link": "http://dx.doi.org/10.32628/ijsrst523103146"
    },
    {
        "id": 13441,
        "title": "Style Permutation for Diversified Arbitrary Style Transfer",
        "authors": "Pan Li, Dan Zhang, Lei Zhao, Duanqing Xu, Dongming Lu",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.3034653"
    },
    {
        "id": 13442,
        "title": "Neural Style Transfer for Image-Based Garment Interchange Through Multi-Person Human Views",
        "authors": "Hajer Ghodhbani, Mohamed Neji, Adel Alimi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011694200003417"
    },
    {
        "id": 13443,
        "title": "Mask-Guided Font Style Transfer in Natural Scene",
        "authors": "Jing Yang, Siyu Xia",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902806"
    },
    {
        "id": 13444,
        "title": "Redefining Textual Dynamics for Enhanced Text Style Transfer",
        "authors": "Carlos Asanka, Conti Vatsalan, Rodolfo Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Conventional text style transfer (TST) methodologies primarily utilize style classifiers to segregate the content and stylistic elements of text for effective style transformation. Despite the pivotal role of these classifiers, their influence on TST techniques remains largely unexplored. This study embarks on a detailed exploration of the limitations inherent in style classifiers within current TST frameworks. We reveal that these classifiers often inadequately comprehend sentence syntax, leading to diminished performance in TST models. In response, we introduce the Syntax-Enhanced Style Transfer (SEST) model, a groundbreaking approach incorporating a syntax-sensitive style classifier. This classifier ensures that the extracted style representations robustly encapsulate syntax nuances, enhancing TST effectiveness. Rigorous evaluations across diverse TST benchmarks demonstrate that SEST significantly surpasses contemporary models in performance. Additionally, our case studies highlight SEST's proficiency in producing syntactically coherent sentences that aptly retain original content.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0144.v1"
    },
    {
        "id": 13445,
        "title": "FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer",
        "authors": "Pavol Harar, Lukas Herrmann, Philipp Gross, David Haselbach",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4517676"
    },
    {
        "id": 13446,
        "title": "FormalStyler: GPT based Model for Formal Style Transfer based on Formality and Meaning Preservation",
        "authors": "Mariano de Rivero, Cristhiam Tirado, Willy Ugarte",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010674300003064"
    },
    {
        "id": 13447,
        "title": "Detail-Preserving Arbitrary Style Transfer",
        "authors": "Ting Zhu, Shiguang Liu",
        "published": "2020-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme46284.2020.9102931"
    },
    {
        "id": 13448,
        "title": "Content and style transfer with generative adversarial network",
        "authors": "Wenhua Ding, Junwei Du, Lei Hou, Jinhuan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe generative adversarial network is often used for image conversion tasks such as image coloring, semantic composition,style transfer, etc.,but at this stage, the training of image generation models often depends on a large number of paired datasets, and can only achieve conversion between two image domains.When processing tasks in more than two domains,it lacks scalability and robustness.To solve the above problems,this paper proposes a Content and Style transfer model based on Generative Adversarial Network (CS-GAN).This model can fuse style features (such as monet style,cubism) and content features (such as color ,texture) of fashion items on unpaired datasets at the same time,which can realize the conversion of multiple image domains,so as to effectively complete the task of transferring the content and style of fashion items.In particular,we propose a layer consistent dynamic convolution (LCDC) method,which encodes the style image as a learnable convolution parameter,which can adaptively learn style features,and more flexibly and efficiently complete the arbitrary style transfer of fashion items.To validate the performance of our model,we conducts comparative experiments and results analysis on the public fashion dataset. Compared with other mainstream methods,this method has improved in image synthesis quality,Inception Score (IS) and Frechet Inception Dinstance score (FID) evaluation index.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2423729/v1"
    },
    {
        "id": 13449,
        "title": "Effects of dimples’ arrangement style of rough surface and jet geometry on impinging jet heat transfer",
        "authors": "Nevin Celik",
        "published": "2020-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00231-019-02714-x"
    },
    {
        "id": 13450,
        "title": "Neural Rendering-Based 3D Scene Style Transfer Method via Semantic Understanding Using a Single Style Image",
        "authors": "Jisun Park, Kyungeun Cho",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "In the rapidly emerging era of untact (“contact-free”) technologies, the requirement for three-dimensional (3D) virtual environments utilized in virtual reality (VR)/augmented reality (AR) and the metaverse has seen significant growth, owing to their extensive application across various domains. Current research focuses on the automatic transfer of the style of rendering images within a 3D virtual environment using artificial intelligence, which aims to minimize human intervention. However, the prevalent studies on rendering-based 3D environment-style transfers have certain inherent limitations. First, the training of a style transfer network dedicated to 3D virtual environments demands considerable style image data. These data must align with viewpoints that closely resemble those of the virtual environment. Second, there was noticeable inconsistency within the 3D structures. Predominant studies often neglect 3D scene geometry information instead of relying solely on 2D input image features. Finally, style adaptation fails to accommodate the unique characteristics inherent in each object. To address these issues, we propose a novel approach: a neural rendering-based 3D scene-style conversion technique. This methodology employs semantic nearest-neighbor feature matching, thereby facilitating the transfer of style within a 3D scene while considering the distinctive characteristics of each object, even when employing a single style image. The neural radiance field enables the network to comprehend the geometric information of a 3D scene in relation to its viewpoint. Subsequently, it transfers style features by employing the unique features of a single style image via semantic nearest-neighbor feature matching. In an empirical context, our proposed semantic 3D scene style transfer method was applied to 3D scene style transfers for both interior and exterior environments. This application utilizes the replica, 3DFront, and Tanks and Temples datasets for testing. The results illustrate that the proposed methodology surpasses existing style transfer techniques in terms of maintaining 3D viewpoint consistency, style uniformity, and semantic coherence.",
        "link": "http://dx.doi.org/10.3390/math11143243"
    },
    {
        "id": 13451,
        "title": "Learning Dynamic Style Kernels for Artistic Style Transfer",
        "authors": "Wenju Xu, Chengjiang Long, Yongwei Nie",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00972"
    },
    {
        "id": 13452,
        "title": "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval",
        "authors": "Nina Shvetsova, Anna Kukleva, Bernt Schiele, Hilde Kuehne",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.02009"
    },
    {
        "id": 13453,
        "title": "MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer",
        "authors": "Yazheng Yang, Zhou Zhao, Qi Liu",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3580305.3599438"
    },
    {
        "id": 13454,
        "title": "Exploiting Style Transfer-Based Task Augmentation for Cross-Domain Few-Shot Learning",
        "authors": "Shuzhen Rao, Jun Huang, Zengming Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4424628"
    },
    {
        "id": 13455,
        "title": "Illumination-Aware Style Transfer for Image Harmonization",
        "authors": "Teng Ren, Haitao Zhang",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897164"
    },
    {
        "id": 13456,
        "title": "PROSODY-VC: Voice style transfer with speech prosody",
        "authors": "Arunava Kr Kalita, Rusha Patra",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183559"
    },
    {
        "id": 13457,
        "title": "CNN-Based Image Style Transfer and Its Applications",
        "authors": "Nanhao Jin",
        "published": "2020-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds49703.2020.00081"
    },
    {
        "id": 13458,
        "title": "Unifying Human Motion Synthesis and Style Transfer with Denoising Diffusion Probabilistic Models",
        "authors": "Ziyi Chang, Edmund Findlay, Haozheng Zhang, Hubert P. H. Shum",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011631000003417"
    },
    {
        "id": 13459,
        "title": "One-shot style transfer using Wasserstein Autoencoder",
        "authors": "Hidemoto Nakada, Hideki Asoh",
        "published": "2021-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asiancon51346.2021.9544732"
    },
    {
        "id": 13460,
        "title": "Style transfer with VGG19",
        "authors": "Langtian Lang",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Style transfer is a wide-used technique in image and photograph processing, which could transfer the style of an image to a target image that has a different content. This image processing technique has been used in the algorithms of some image processing software as well as modern artistic creation. However, the intrinsic principle of style transfer and its transfer accuracy is still not clear and stable. This article discusses a new method for preprocessing image data that uses feature extraction and forming vector fields and utilizing multiple VGG19 to separately train the distinct features in images to obtain a better effect in predicting. Our model could generate more autonomous and original images that are not simply adding a style filter to the image, which can help the development of AI style transfer and painting.",
        "link": "http://dx.doi.org/10.54254/2755-2721/6/20230752"
    },
    {
        "id": 13461,
        "title": "APPAREL DESIGN USING IMAGE STYLE TRANSFER",
        "authors": "",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets35101"
    },
    {
        "id": 13462,
        "title": "Interactive Multi-Channel Network for Fast Clothing Style Transfer",
        "authors": "Qixiang Wang, Shujing Wang",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549821"
    },
    {
        "id": 13463,
        "title": "English to Thai Calligraphy Style Transfer Using Deep Learning",
        "authors": "Orawan Watchanupaporn",
        "published": "2021-1-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acie51979.2021.9381082"
    },
    {
        "id": 13464,
        "title": "Deep Style Transfer",
        "authors": "Dongdong Chen, Lu Yuan, Gang Hua",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-03243-2_863-1"
    },
    {
        "id": 13465,
        "title": "Image Style Transfer via Multi-Style Geometry Warping",
        "authors": "Ioana Alexandru, Constantin Nicula, Cristian Prodan, Răzvan-Paul Rotaru, Mihai-Lucian Voncilă, Nicolae Tarbă, Costin-Anton Boiangiu",
        "published": "2022-6-14",
        "citations": 3,
        "abstract": "Style transfer of an image has been receiving attention from the scientific community since its inception in 2015. This topic is characterized by an accelerated process of innovation; it has been defined by techniques that blend content and style, first covering only textural details, and subsequently incorporating compositional features. The results of such techniques has had a significant impact on our understanding of the inner workings of Convolutional Neural Networks. Recent research has shown an increasing interest in the geometric deformation of images, since it is a defining trait for different artists, and in various art styles, that previous methods failed to account for. However, current approaches are limited to matching class deformations in order to obtain adequate outputs. This paper solves these limitations by combining previous works in a framework that can perform geometric deformation on images using different styles from multiple artists by building an architecture that uses multiple style images and one content image as input. The proposed framework uses a combination of various other existing frameworks in order to obtain a more intriguing artistic result. The framework first detects objects of interest from various classes inside the image and assigns them a bounding box, before matching each detected object image found in a bounding box with a similar style image and performing warping on each of them on the basis of these similarities. Next, the algorithm blends back together all the warped images so they are placed in a similar position as the initial image, and style transfer is finally applied between the merged warped images and a different chosen image. We manage to obtain stylistically pleasing results that were possible to generate in a reasonable amount of time, compared to other existing methods.",
        "link": "http://dx.doi.org/10.3390/app12126055"
    },
    {
        "id": 13466,
        "title": "Caster: Cartoon style transfer via dynamic cartoon style casting",
        "authors": "Zhanjie Zhang, Jiakai Sun, Jiafu Chen, Lei Zhao, Boyan Ji, Zehua Lan, Guangyuan Li, Wei Xing, Duanqing Xu",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126654"
    },
    {
        "id": 13467,
        "title": "Chinese Character Style Transfer Based on the Fusion of Multi-scale Content and Style Features",
        "authors": "Xiaoxue Zhou, Ziying Zhang, Xin Chen, Mengxi Qin",
        "published": "2021-7-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9550311"
    },
    {
        "id": 13468,
        "title": "DualAST: Dual Style-Learning Networks for Artistic Style Transfer",
        "authors": "Haibo Chen, Lei Zhao, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo, Ailin Li, Wei Xing, Dongming Lu",
        "published": "2021-6",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.00093"
    },
    {
        "id": 13469,
        "title": "StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition",
        "authors": "Xiaolei Wu, Zhihao Hu, Lu Sheng, Dong Xu",
        "published": "2021-10",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01435"
    },
    {
        "id": 13470,
        "title": "Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer",
        "authors": "Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, Hua Wu",
        "published": "2020",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.639"
    }
]