[
    {
        "id": 23671,
        "title": "SequenceGan:  Text to Image Synthesis with Sequence Models and Gans",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div><div>Generative Adversarial Nets are one of the most popular generative frameworks. In our work, we introduce the SequenceGAN, a method that can generate images based on a given caption by supplying the conditional sequential text input to the generator and the discriminator. Unlike other conditional methods, SequenceGAN uses recurrent layers for better context understanding. We also demonstrated SequenceGANs performance by applying it to the MNIST and Flickr 8k datasets.</div><div><br></div></div><div><div></div></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14922633"
    },
    {
        "id": 23672,
        "title": "SequenceGan text to image synthesis with Seq models and GANs",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div><div>Generative Adversarial Nets are one of the most popular generative frameworks. In our work, we introduce the SequenceGAN, a method that can generate images based on a given caption by supplying the conditional sequential text input to the generator and the discriminator. Unlike other conditional methods, SequenceGAN uses recurrent layers for better context understanding. We also demonstrated SequenceGANs performance by applying it to the MNIST and Flickr 8k datasets.</div><div><br></div></div><div><div></div></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14922633.v2"
    },
    {
        "id": 23673,
        "title": "SequenceGan:  Text to Image Synthesis with Sequence Models and Gans",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div><div>Generative Adversarial Nets are one of the most popular generative frameworks. In our work, we introduce the SequenceGAN, a method that can generate images based on a given caption by supplying the conditional sequential text input to the generator and the discriminator. Unlike other conditional methods, SequenceGAN uses recurrent layers for better context understanding. We also demonstrated SequenceGANs performance by applying it to the MNIST and Flickr 8k datasets.</div><div><br></div></div><div><div></div></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14922633.v3"
    },
    {
        "id": 23674,
        "title": "SequenceGan text to image synthesis with Seq models and GANs",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div><div>Generative Adversarial Nets are one of the most popular generative frameworks. In our work, we introduce the SequenceGAN, a method that can generate images based on a given caption by supplying the conditional sequential text input to the generator and the discriminator. Unlike other conditional methods, SequenceGAN uses recurrent layers for better context understanding. We also demonstrated SequenceGANs performance by applying it to the MNIST and Flickr 8k datasets.</div><div><br></div></div><div><div></div></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14922633.v1"
    },
    {
        "id": 23675,
        "title": "Cross-view image synthesis using geometry-guided conditional GANs",
        "authors": "Krishna Regmi, Ali Borji",
        "published": "2019-10",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2019.07.008"
    },
    {
        "id": 23676,
        "title": "Cross-View Image Synthesis Using Conditional GANs",
        "authors": "Krishna Regmi, Ali Borji",
        "published": "2018-6",
        "citations": 102,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00369"
    },
    {
        "id": 23677,
        "title": "GANs for Biological Image Synthesis",
        "authors": "Anton Osokin, Anatole Chessel, Rafael E. Carazo Salas, Federico Vaggi",
        "published": "2017-10",
        "citations": 61,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.245"
    },
    {
        "id": 23678,
        "title": "Image Synthesis Using Conditional GANs for Selective Laser Melting Additive Manufacturing",
        "authors": "Andy Ramlatchan, Yaohang Li",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892033"
    },
    {
        "id": 23679,
        "title": "Medical Image Synthesis Using Generative Adversarial Networks",
        "authors": "Vishal Raner, Amit Joshi, Suraj Sawant",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-43205-7_3"
    },
    {
        "id": 23680,
        "title": "ArtGAN: Artwork synthesis with conditional categorical GANs",
        "authors": "Wei Ren Tan, Chee Seng Chan, Hernan E. Aguirre, Kiyoshi Tanaka",
        "published": "2017-9",
        "citations": 66,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2017.8296985"
    },
    {
        "id": 23681,
        "title": "Image Synthesis and Editing with Generative Adversarial Networks (GANs): A Review",
        "authors": "Wanwan Li",
        "published": "2021-7-29",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/worlds451998.2021.9514052"
    },
    {
        "id": 23682,
        "title": "Image Synthesis Using GANs and Diffusion Models",
        "authors": "Ayush Karn, Shubham Kumar, Sonu K Kushwaha, Rahul Katarya",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inc457730.2023.10263208"
    },
    {
        "id": 23683,
        "title": "GANs for Medical Image Synthesis: An Empirical Study",
        "authors": "Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande",
        "published": "2023-3-16",
        "citations": 45,
        "abstract": "Generative adversarial networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they have been trained to replicate. One recurrent theme in medical imaging, is whether GANs can also be as effective at generating workable medical data, as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study, to gauge the benefits of GANs in medical imaging. We tested various GAN architectures, from basic DCGAN to more sophisticated style-based GANs, on three medical imaging modalities and organs, namely: cardiac cine-MRI, liver CT, and RGB retina images. GANs were trained on well-known and widely utilized datasets, from which their FID scores were computed, to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images and the original data. The results reveal that GANs are far from being equal, as some are ill-suited for medical imaging applications, while others performed much better. The top-performing GANs are capable of generating realistic-looking medical images by FID standards, that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggest that no GAN is capable of reproducing the full richness of medical datasets.",
        "link": "http://dx.doi.org/10.3390/jimaging9030069"
    },
    {
        "id": 23684,
        "title": "Vit-GAN: Image-to-image Translation with Vision Transformes and Conditional GANS",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we have developed a general-purpose architecture, Vit-Gan, capable of performing\nmost of the image-to-image translation tasks from semantic image segmentation to single image depth\nperception. This paper is a follow-up paper, an extension of generator based model [1] in which the\nobtained results were very promising. This opened the possibility of further improvements with adversarial architecture. We used a unique vision transformers-based generator architecture and Conditional\nGANs(cGANs) with a Markovian Discriminator (PatchGAN) (https://github.com/YigitGunduc/vit-gan).\nIn the present work, we use images as conditioning arguments. It is observed that the obtained results\nare more realistic than the commonly used architectures.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16785751.v1"
    },
    {
        "id": 23685,
        "title": "Vit-GAN: Image-to-image Translation with Vision Transformes and Conditional GANS",
        "authors": "Yigit Gunduc",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we have developed a general-purpose architecture, Vit-Gan, capable of performing\nmost of the image-to-image translation tasks from semantic image segmentation to single image depth\nperception. This paper is a follow-up paper, an extension of generator based model [1] in which the\nobtained results were very promising. This opened the possibility of further improvements with adversarial architecture. We used a unique vision transformers-based generator architecture and Conditional\nGANs(cGANs) with a Markovian Discriminator (PatchGAN) (https://github.com/YigitGunduc/vit-gan).\nIn the present work, we use images as conditioning arguments. It is observed that the obtained results\nare more realistic than the commonly used architectures.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16785751"
    },
    {
        "id": 23686,
        "title": "Dual Attention GANs for Semantic Image Synthesis",
        "authors": "Hao Tang, Song Bai, Nicu Sebe",
        "published": "2020-10-12",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3394171.3416270"
    },
    {
        "id": 23687,
        "title": "Overview of GANs for Image Synthesis and Detection Methods",
        "authors": "Eric Tjon, Melody Moh, Teng-Sheng Moh",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-55692-1_5"
    },
    {
        "id": 23688,
        "title": "MIGAN: Malware Image Synthesis Using GANs",
        "authors": "Abhishek Singh, Debojyoti Dutta, Amit Saha",
        "published": "2019-7-17",
        "citations": 8,
        "abstract": "Majority of the advancement in Deep learning (DL) has occurred in domains such as computer vision, and natural language processing, where abundant training data is available. A major obstacle in leveraging DL techniques for malware analysis is the lack of sufficiently big, labeled datasets. In this paper, we take the first steps towards building a model which can synthesize labeled dataset of malware images using GAN. Such a model can be utilized to perform data augmentation for training a classifier. Furthermore, the model can be shared publicly for community to reap benefits of dataset without sharing the original dataset. First, we show the underlying idiosyncrasies of malware images and why existing data augmentation techniques as well as traditional GAN training fail to produce quality artificial samples. Next, we propose a new method for training GAN where we explicitly embed prior domain knowledge about the dataset into the training procedure. We show improvements in training stability and sample quality assessed on different metrics. Our experiments show substantial improvement on baselines and promise for using such a generative model for malware visualization systems.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.330110033"
    },
    {
        "id": 23689,
        "title": "Anycost GANs for Interactive Image Synthesis and Editing",
        "authors": "Ji Lin, Richard Zhang, Frieder Ganz, Song Han, Jun-Yan Zhu",
        "published": "2021-6",
        "citations": 36,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.01474"
    },
    {
        "id": 23690,
        "title": "Dual image and mask synthesis with GANs for semantic segmentation in optical coherence tomography",
        "authors": "Jason Kugelman, David Alonso-Caneiro, Scott A. Read, Stephen J. Vincent, Fred K. Chen, Michael J. Collins",
        "published": "2020-11-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dicta51227.2020.9363402"
    },
    {
        "id": 23691,
        "title": "Contrast agent-free synthesis and segmentation of ischemic heart disease images using progressive sequential causal GANs",
        "authors": "Chenchu Xu, Lei Xu, Pavlo Ohorodnyk, Mike Roth, Bo Chen, Shuo Li",
        "published": "2020-5",
        "citations": 39,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.media.2020.101668"
    },
    {
        "id": 23692,
        "title": "Collaging Class-specific GANs for Semantic Image Synthesis",
        "authors": "Yuheng Li, Yijun Li, Jingwan Lu, Eli Shechtman, Yong Jae Lee, Krishna Kumar Singh",
        "published": "2021-10",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01415"
    },
    {
        "id": 23693,
        "title": "Blended multi-class text to image synthesis GANs with RoBerTa and Mask R-CNN",
        "authors": "M Siddharth, R Aarthi",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.01.065"
    },
    {
        "id": 23694,
        "title": "Text-To-Image Synthesis Using Modified GANs",
        "authors": "Lakshmi S Hanne, R Kundana, R. Thirukkumaran, Yagna Vikas Parvatikar, K Madhura",
        "published": "2022-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/accai53970.2022.9752641"
    },
    {
        "id": 23695,
        "title": "GANs for Image Generation",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8_2"
    },
    {
        "id": 23696,
        "title": "Comparison of deep convolution and least squares GANs for diabetic retinopathy image synthesis",
        "authors": "İsa Ataş",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00521-023-08482-4"
    },
    {
        "id": 23697,
        "title": "CapGAN: Text-to-Image Synthesis Using Capsule GANs",
        "authors": "Maryam Omar, Hafeez Ur Rehman, Omar Bin Samin, Moutaz Alazab, Gianfranco Politano, Alfredo Benso",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "Text-to-image synthesis is one of the most critical and challenging problems of generative modeling. It is of substantial importance in the area of automatic learning, especially for image creation, modification, analysis and optimization. A number of works have been proposed in the past to achieve this goal; however, current methods still lack scene understanding, especially when it comes to synthesizing coherent structures in complex scenes. In this work, we propose a model called CapGAN, to synthesize images from a given single text statement to resolve the problem of global coherent structures in complex scenes. For this purpose, skip-thought vectors are used to encode the given text into vector representation. This encoded vector is used as an input for image synthesis using an adversarial process, in which two models are trained simultaneously, namely: generator (G) and discriminator (D). The model G generates fake images, while the model D tries to predict what the sample is from training data rather than generated by G. The conceptual novelty of this work lies in the integrating capsules at the discriminator level to make the model understand the orientational and relative spatial relationship between different entities of an object in an image. The inception score (IS) along with the Fréchet inception distance (FID) are used as quantitative evaluation metrics for CapGAN. IS recorded for images generated using CapGAN is 4.05 ± 0.050, which is around 34% higher than images synthesized using traditional GANs, whereas the FID score calculated for synthesized images using CapGAN is 44.38, which is ab almost 9% improvement from the previous state-of-the-art models. The experimental results clearly demonstrate the effectiveness of the proposed CapGAN model, which is exceptionally proficient in generating images with complex scenes.",
        "link": "http://dx.doi.org/10.3390/info14100552"
    },
    {
        "id": 23698,
        "title": "Guiding Gans: How to Control Non-Conditional Pre-Trained Gans for Conditional Image Generation",
        "authors": "Alejandro González, Manel Mateos, Felipe Perez-Stoppa, Ester Vidaña-Vila, Joan Navarro, Xavier Sevillano",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4560891"
    },
    {
        "id": 23699,
        "title": "Toward U-Net-based GANs for Diverse Facial Image Synthesis from Sketch",
        "authors": "Warintorn Phusomsai, Yachai Limpiyakorn",
        "published": "2020-1-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3424311.3424323"
    },
    {
        "id": 23700,
        "title": "Cross-view panorama image synthesis with progressive attention GANs",
        "authors": "Songsong Wu, Hao Tang, Xiao-Yuan Jing, Jianjun Qian, Nicu Sebe, Yan Yan, Qinghua Zhang",
        "published": "2022-11",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2022.108884"
    },
    {
        "id": 23701,
        "title": "Scaling up GANs for Text-to-Image Synthesis",
        "authors": "Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, Taesung Park",
        "published": "2023-6",
        "citations": 46,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00976"
    },
    {
        "id": 23702,
        "title": "Aerial to Street View Image Translation using Cascaded Conditional GANs",
        "authors": "Kshitij Singh, Alexia Briassouli, Mirela Popa",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010814000003124"
    },
    {
        "id": 23703,
        "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
        "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro",
        "published": "2018-6",
        "citations": 2174,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00917"
    },
    {
        "id": 23704,
        "title": "Artifygan: Animated Face Image Generation Using Gans",
        "authors": "Iqra Bismi, Priya Khandelwal, Saniya Lande, Mohammad Masum",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622124"
    },
    {
        "id": 23705,
        "title": "Feature Cycling Block for Improving Gans-Based Image Generation Performance",
        "authors": "Seung Park, Yong-Goo Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4452070"
    },
    {
        "id": 23706,
        "title": "Cellcyclegan: Spatiotemporal Microscopy Image Synthesis Of Cell Populations Using Statistical Shape Models And Conditional Gans",
        "authors": "Dennis Bahr, Dennis Eschweiler, Anuk Bhattacharyya, Daniel Moreno-Andres, Wolfram Antonin, Johannes Stegmaier",
        "published": "2021-4-13",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi48211.2021.9433896"
    },
    {
        "id": 23707,
        "title": "Medical image synthesis via conditional GANs: Application to segmenting brain tumours",
        "authors": "Mohammad Hamghalam, Amber L. Simpson",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2024.107982"
    },
    {
        "id": 23708,
        "title": "FaceVision-GAN: A 3D Model Face Reconstruction Method from a Single Image Using GANs",
        "authors": "Danilo Avola, Luigi Cinque, Gian Foresti, Marco Marini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012306200003654"
    },
    {
        "id": 23709,
        "title": "360-Degree Image Completion by Two-Stage Conditional Gans",
        "authors": "Naofumi Akimoto, Seito Kasai, Masaki Hayashi, Yoshimitsu Aoki",
        "published": "2019-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803435"
    },
    {
        "id": 23710,
        "title": "ReeGAN: MRI image edge-preserving synthesis based on GANs trained with misaligned data",
        "authors": "Xiangjiang Lu, Xiaoshuang Liang, Wenjing Liu, Xiuxia Miao, Xianglong Guan",
        "published": "2024-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11517-024-03035-w"
    },
    {
        "id": 23711,
        "title": "Cluster with GANs",
        "authors": "Yuri Feigin, Hedva Spitzer, Raja Giryes",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2022.103571"
    },
    {
        "id": 23712,
        "title": "Semantic-Fusion Gans for Semi-Supervised Satellite Image Classification",
        "authors": "Subhankar Roy, Enver Sangineto, Nicu Sebe, Begum Demir",
        "published": "2018-10",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2018.8451836"
    },
    {
        "id": 23713,
        "title": "CONDITIONAL GANS AS A SOLUTION TO IMAGE-TO-IMAGE RENDERING PROBLEMS",
        "authors": "Aleem Ali, Et. al.",
        "published": "2021-4-13",
        "citations": 0,
        "abstract": "In many existing solutions of image-to-image rendering problems, the only focus is to find the closest output of the Generative Adversarial Network (GAN). In this research article, authors propose a generative adversarial network, a solution to pixel-to-pixel rendering problems and reduced the loss function to the maximum under all interactions. For achieving the best result, we have considered the mean square loss function in the generator and binary cross for the discriminator. Our proposed model deals with not only images but also read sketches where the edges are not sharp too. We have used a facade dataset to test our proposed model.",
        "link": "http://dx.doi.org/10.17762/itii.v9i2.458"
    },
    {
        "id": 23714,
        "title": "Bipartite Graph Reasoning GANs for Person Pose and Facial Image Synthesis",
        "authors": "Hao Tang, Ling Shao, Philip H. S. Torr, Nicu Sebe",
        "published": "2023-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11263-022-01722-5"
    },
    {
        "id": 23715,
        "title": "Peer Review #3 of \"Small facial image dataset augmentation using conditional GANs based on incomplete edge feature input (v0.1)\"",
        "authors": "",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.760v0.1/reviews/3"
    },
    {
        "id": 23716,
        "title": "Peer Review #2 of \"Small facial image dataset augmentation using conditional GANs based on incomplete edge feature input (v0.2)\"",
        "authors": "",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.760v0.2/reviews/2"
    },
    {
        "id": 23717,
        "title": "Peer Review #2 of \"Small facial image dataset augmentation using conditional GANs based on incomplete edge feature input (v0.1)\"",
        "authors": "",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.760v0.1/reviews/2"
    },
    {
        "id": 23718,
        "title": "Sketch-to-Color Image with GANs",
        "authors": "Wenbo Zhang",
        "published": "2020-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itca52113.2020.00075"
    },
    {
        "id": 23719,
        "title": "Combining Text and Image Knowledge with GANs for Zero-Shot Action Recognition in Videos",
        "authors": "Kaiqiang Huang, Luis Miralles-Pechuán, Susan Mckeever",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010903100003124"
    },
    {
        "id": 23720,
        "title": "Generative Adversarial Networks (GANs)",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8_1"
    },
    {
        "id": 23721,
        "title": "Recoloring Image For Color Vision Deficiency By GANS",
        "authors": "Xiangdong Zhang, Meili Zhang, Liang Zhang, Peiyi Shen, Guangming Zhu, Ping Li",
        "published": "2019-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803463"
    },
    {
        "id": 23722,
        "title": "Reversible GANs for Memory-Efficient Image-To-Image Translation",
        "authors": "Tycho F.A. van der Ouderaa, Daniel E. Worrall",
        "published": "2019-6",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00485"
    },
    {
        "id": 23723,
        "title": "More Key Applications of GANs",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8_3"
    },
    {
        "id": 23724,
        "title": "Single-image reflection removal using conditional GANs",
        "authors": "Miran Heo, Yoonsik Choe",
        "published": "2019-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/elinfocom.2019.8706433"
    },
    {
        "id": 23725,
        "title": "Performance Analysis of Conditional GANs based Image-to-Image Translation Models for Low-Light Image Enhancement",
        "authors": "Neetu Singh, Abdul Manaf F, Mudit Rastogi, Rahul Prasad",
        "published": "2022-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsc56524.2022.10009340"
    },
    {
        "id": 23726,
        "title": "Peer Review #1 of \"Small facial image dataset augmentation using conditional GANs based on incomplete edge feature input (v0.2)\"",
        "authors": "X Ning",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.760v0.2/reviews/1"
    },
    {
        "id": 23727,
        "title": "Text-to-Image Generator using GANs",
        "authors": "Vadik Amar,  Sonu, Hatesh Shyan",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciics59993.2023.10421307"
    },
    {
        "id": 23728,
        "title": "Multi-national COVID-19 CT Image-label Pairs Synthesis via Few-Shot GANs Adaptation",
        "authors": "Jing Zhang, Yingpeng Xie, Dandan Sun, Ruidong Huang, Tianfu Wang, Baiying Lei, Kuntao Chen",
        "published": "2023-4-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230584"
    },
    {
        "id": 23729,
        "title": "An End-to-End Pipeline for Medical Image Enhancement using GANs Architecture",
        "authors": "Tirth Patel, Jaskaran Singh, Apoorv Dankar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Medical Imaging is used by radiologists for diagnostic purposes and to check for abnormalities, and these imaging techniques involve radiation. Overexposure to radiation can have an adverse impact on the human body, and using less radiation gives us a noisy output. Hence, radiologists find it difficult as there is a trade-off between the amount of radiation that can be used and the quality of the image. Moreover, noise in medical images can occur due to fluctuation of photons, a reflection of radiations from the subject, or due to instrumental vibration or faults. The proposed approach is a pipeline which starts with denoising using GANs architecture, in which two models have been trained, one for handling all kinds of noise and the second one specifically for Poisson noise. Further, post-processing methods like single-shot HDR using Retinex Filtering and Edge Enhancement using unsharp masking have been done to get a structurally more similar and enhanced denoised image.",
        "link": "http://dx.doi.org/10.31219/osf.io/8nvxy"
    },
    {
        "id": 23730,
        "title": "Peer Review #1 of \"Small facial image dataset augmentation using conditional GANs based on incomplete edge feature input (v0.1)\"",
        "authors": "X Ning",
        "published": "2021-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.760v0.1/reviews/1"
    },
    {
        "id": 23731,
        "title": "IterGANs: Iterative GANs to learn and control 3D object transformation",
        "authors": "Ysbrand Galama, Thomas Mensink",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2019.102803"
    },
    {
        "id": 23732,
        "title": "Audio-Based Emotion Recognition Enhancement Through Progressive Gans",
        "authors": "Christos Athanasiadis, Enrique Hortal, Stylianos Asteriadis",
        "published": "2020-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190959"
    },
    {
        "id": 23733,
        "title": "I-GANs for Synthetical Infrared Images Generation",
        "authors": "Mohammad Mahdi Moradi, Reza Ghaderi",
        "published": "2022-2-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mvip53647.2022.9738551"
    },
    {
        "id": 23734,
        "title": "Image Based State Estimation",
        "authors": "Nicholas Gans, Guoqiang Hu, Warren E. Dixon",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-642-27737-5_281-4"
    },
    {
        "id": 23735,
        "title": "Single Image Super-Resolution Based on Wasserstein GANs",
        "authors": "Fei Wu, Bo Wang, Dagang Cui, Linhao Li",
        "published": "2018-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8484039"
    },
    {
        "id": 23736,
        "title": "Multiple GANs guided by self-attention mechanism for automatic cardiac image segmentation",
        "authors": "Chang Yuwen, Lei Jiang, Hengfei Cui",
        "published": "2022-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2623153"
    },
    {
        "id": 23737,
        "title": "Sparse GANs for Thermal Infrared Image Generation From Optical Image",
        "authors": "Xiaoyan Qian, Miao Zhang, Feng Zhang",
        "published": "2020",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.3024576"
    },
    {
        "id": 23738,
        "title": "Novel images? Artistic image creation with science and technology protocols: GANs and CRISPR-Cas9",
        "authors": "Laura Beloff",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14236/ewic/pom2021.33"
    },
    {
        "id": 23739,
        "title": "Image Inpainting via Edge Information Guided GANs",
        "authors": "Liang Yan, Chengyang Li, Chengduan Wang",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291226"
    },
    {
        "id": 23740,
        "title": "Image steganography using texture features and GANs",
        "authors": "Jinjing Huang, Shaoyin Cheng, Songhao Lou, Fan Jiang",
        "published": "2019-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852252"
    },
    {
        "id": 23741,
        "title": "Geometric Transformations-Based Medical Image Augmentation",
        "authors": "S. Kalaivani, N. Asha, A. Gayathri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-43205-7_8"
    },
    {
        "id": 23742,
        "title": "GAN2X: Non-Lambertian Inverse Rendering of Image GANs",
        "authors": "Xingang Pan, Ayush Tewari, Lingjie Liu, Christian Theobalt",
        "published": "2022-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/3dv57658.2022.00081"
    },
    {
        "id": 23743,
        "title": "Neural Style Transfer for image within images and conditional GANs for destylization",
        "authors": " Mallika, Jagpal Singh Ubhi, Ashwani Kumar Aggarwal",
        "published": "2022-5",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2022.103483"
    },
    {
        "id": 23744,
        "title": "StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis",
        "authors": "Minguk Kang, Joonghyuk Shin, Jaesik Park",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpami.2023.3306436"
    },
    {
        "id": 23745,
        "title": "Progressive GANomaly:  anomaly detection with progressively growing GANs",
        "authors": "Djennifer K. Madzia-Madzou, Hugo J. Kuijf",
        "published": "2022-4-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2611105"
    },
    {
        "id": 23746,
        "title": "Generating large scale images using GANs",
        "authors": "Mohamed Mohsen, Mohamed Moustafa",
        "published": "2019-8-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2540489"
    },
    {
        "id": 23747,
        "title": "Forest Single-Frame Remote Sensing Image Super-Resolution Using GANs",
        "authors": "Yafeng Zhao, Shuai Zhang, Junfeng Hu",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "Generative Adversarial Networks (GANs) possess remarkable fitting capabilities and play a crucial role in the field of computer vision. Super-resolution restoration is the process of converting low-resolution images into high-resolution ones, providing more detail and information. This is of paramount importance for monitoring and managing forest resources, enabling the surveillance of vegetation, wildlife, and potential disruptive factors in forest ecosystems. In this study, we propose an image super-resolution model based on Generative Adversarial Networks. We incorporate Multi-Scale Residual Blocks (MSRB) as the core feature extraction component to obtain image features at different scales, enhancing feature extraction capabilities. We introduce a novel attention mechanism, GAM Attention, which is added to the VGG network to capture more accurate feature dependencies in both spatial and channel domains. We also employ the adaptive activation function Meta ACONC and Ghost convolution to optimize training efficiency and reduce network parameters. Our model is trained on the DIV2K and LOVEDA datasets, and experimental results indicate improvements in evaluation metrics compared to SRGAN, with a PSNR increase of 0.709/2.213 dB, SSIM increase of 0.032/0.142, and LPIPS reduction of 0.03/0.013. The model performs on par with Real-ESRGAN but offers significantly improved speed. Our model efficiently restores single-frame remote sensing images of forests while achieving results comparable to state-of-the-art methods. It overcomes issues related to image distortion and texture details, producing forest remote sensing images that closely resemble high-resolution real images and align more closely with human perception. This research has significant implications on a global scale for ecological conservation, resource management, climate change research, risk management, and decision-making processes.",
        "link": "http://dx.doi.org/10.3390/f14112188"
    },
    {
        "id": 23748,
        "title": "Efficient Multi-Objective GANs for Image Restoration",
        "authors": "Jingwen Su, Hujun Yin",
        "published": "2021-6-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9413897"
    },
    {
        "id": 23749,
        "title": "Unpaired Multi-Domain Image Generation via Regularized Conditional GANs",
        "authors": "Xudong Mao, Qing Li",
        "published": "2018-7",
        "citations": 5,
        "abstract": "In this paper, we study the problem of multi-domain image generation, the goal of which is to generate pairs of corresponding images from different domains. With the recent development in generative models, image generation has achieved great progress and has been applied to various computer vision tasks. However, multi-domain image generation may not achieve the desired performance due to the difficulty of learning the correspondence of different domain images, especially when the information of paired samples is not given. To tackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is capable of learning to generate corresponding images in the absence of paired training data. RegCGAN is based on the conditional GAN, and we introduce two regularizers to guide the model to learn the corresponding semantics of different domains. We evaluate the proposed model on several tasks for which paired training data is not given, including the generation of edges and photos, the generation of faces with different attributes, etc. The experimental results show that our model can successfully generate corresponding images for all these tasks, while outperforms the baseline methods. We also introduce an approach of applying RegCGAN to unsupervised domain adaptation.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/354"
    },
    {
        "id": 23750,
        "title": "Stable and improved generative adversarial nets (GANS): A constructive survey",
        "authors": "Guanghao Zhang, Enmei Tu, Dongshun Cui",
        "published": "2017-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2017.8296606"
    },
    {
        "id": 23751,
        "title": "Unlimited Resolution Image Generation with R2D2-GANs",
        "authors": "Marija Jegorova, Antti Ilari Karjalainen, Jose Vazquez, Timothy M. Hospedales",
        "published": "2020-10-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf38699.2020.9389260"
    },
    {
        "id": 23752,
        "title": "Changing the Image Memorability: From Basic Photo Editing to GANs",
        "authors": "Oleksii Sidorov",
        "published": "2019-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw.2019.00107"
    },
    {
        "id": 23753,
        "title": "Image-to-Image Translation Based Face Photo De-Meshing Using Gans",
        "authors": "Abdul Jabbar, Muhammad Assam, Tamara Al Shloul, Hala Hameed, Nouf Al-Kahtani, Hend Alkahtani, Nivin Ghamry, ElSayed  M. Tag El Din, Muhammad Amin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327718"
    },
    {
        "id": 23754,
        "title": "Improved Techniques for Training Single-Image GANs",
        "authors": "Tobias Hinz, Matthew Fisher, Oliver Wang, Stefan Wermter",
        "published": "2021-1",
        "citations": 62,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv48630.2021.00134"
    },
    {
        "id": 23755,
        "title": "Predictive RANSAC: Effective model fitting and tracking approach under heavy noise and outliers",
        "authors": "Yingmao Li, Nicholas R. Gans",
        "published": "2017-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2017.05.013"
    },
    {
        "id": 23756,
        "title": "Generating Videos by Traversing Image Manifolds Learned by GANs",
        "authors": "Isabela Albuquerque, Joao Monteiro, Tiago Falk",
        "published": "2018-12-3",
        "citations": 0,
        "abstract": "In this work, we introduce a two-step framework for generative modeling of temporal data. Specifically, the generative adversarial networks (GANs) setting is employed to generate synthetic scenes of moving objects. To do so, we propose a two-step training scheme within which: a generator of static frames is trained first. Afterwards, a recurrent model is trained with the goal of providing a sequence of inputs to the previously trained frames generator, thus yielding scenes which look natural. The adversarial setting is employed in both training steps. However, with the aim of avoiding known training instabilities in GANs, a multiple discriminator approach is used to train both models. Results in the studied video dataset indicate that, by employing such an approach, the recurrent part is able to learn how to coherently navigate the image manifold induced by the frames generator, thus yielding more natural-looking scenes.",
        "link": "http://dx.doi.org/10.52591/lxai201812036"
    },
    {
        "id": 23757,
        "title": "Regularization to Suppress Mode Collapse in GANs Using Image Similarity",
        "authors": "Kuniyasu Imade, Taro Kiriyama, Satoshi Yamane",
        "published": "2022-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce56475.2022.10014060"
    },
    {
        "id": 23758,
        "title": "Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs",
        "authors": "Muhammad Asif Khan, Hamid Menouar, Ridha Hamila",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivcnz61134.2023.10343548"
    },
    {
        "id": 23759,
        "title": "Multimodal Conditional Image Synthesis with Product-of-Experts GANs",
        "authors": "Xun Huang, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu",
        "published": "2022",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-19787-1_6"
    },
    {
        "id": 23760,
        "title": "Spatio-temporal generation of morphological Plant features for yield prediction before harvest from Visual Image input using Progressively Growing GANs",
        "authors": "Dhruv Sheth",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10509323.1"
    },
    {
        "id": 23761,
        "title": "Deformable GANs for Pose-Based Human Image Generation",
        "authors": "Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, Nicu Sebe",
        "published": "2018-6",
        "citations": 302,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00359"
    },
    {
        "id": 23762,
        "title": "Spatio-temporal generation of morphological Plant features for yield prediction before harvest from Visual Image input using Progressively Growing GANs",
        "authors": "Dhruv Sheth",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10509323.2"
    },
    {
        "id": 23763,
        "title": "Siamese Score: Detecting Mode Collapse for GANs",
        "authors": "Jizheng Jia, Qiyang Zhao",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisp-bmei48845.2019.8965997"
    },
    {
        "id": 23764,
        "title": "LatentSwap3D: Semantic Edits on 3D Image GANs",
        "authors": "Enis Simsar, Alessio Tonioni, Evin Pınar Örnek, Federico Tombari",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00312"
    },
    {
        "id": 23765,
        "title": "Comparing CNNs and GANs for Image Completion",
        "authors": "Rohith Saji, Sai Krishna Anand, B. R. Chandavarkar",
        "published": "2021-7-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt51525.2021.9579725"
    },
    {
        "id": 23766,
        "title": "<i>Infinite Barnacle</i>: The AI Image and Imagination in GANs from Personal Snapshots",
        "authors": "Eryk Salvaggio",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nToday’s artificial intelligence image generation tools create images from datasets. These training sets are typically images sourced from the World Wide Web. However, artists may produce their own datasets from photographs. This essay explores one such process. In it, the artist discusses training a generative adversarial network (GAN) from images of personal memories. These images are shared here not as public artworks, but as personal photographs: snapshots reproduced and newly imagined by a machine. The essay explores the distortion that AI image generation introduces to memory and imagination, connecting ideas of photography to cybernetics to expose new ways of theorizing the image in the current stage of AI. It concludes that a theory of A imagery may borrow from theories of traditional photography but must examine its distinctions.",
        "link": "http://dx.doi.org/10.1162/leon_a_02404"
    },
    {
        "id": 23767,
        "title": "Artifact-Free Thin Cloud Removal Using Gans",
        "authors": "Takahiro Toizumi, Simone Zini, Kazutoshi Sagi, Eiji Kaneko, Masato Tsukada, Raimondo Schettini",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803652"
    },
    {
        "id": 23768,
        "title": "High-quality multispectral image generation using Conditional GANs",
        "authors": "Ayush Soni, Alexander Loui, Scott Brown, Carl Salvaggio",
        "published": "2020-1-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2352/issn.2470-1173.2020.8.imawm-086"
    },
    {
        "id": 23769,
        "title": "Image Denoising with Self Operational and Convolutional Cycle-GANs",
        "authors": "Hodaka Yamanouchi, Yusuke Sao, Toshiyuki Uto",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itc-cscc58803.2023.10212738"
    },
    {
        "id": 23770,
        "title": "Conditional GANs for semantic segmentation of multispectral satellite images",
        "authors": "Vladimir V. Kniaz",
        "published": "2018-10-9",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2325601"
    }
]