[
    {
        "id": 23571,
        "title": "Transformer-Based Language Models as Psycholinguistic Subjects: Focusing on Understanding Metaphor",
        "authors": "Wonil Chung,  ",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "Metaphor is a fundamental aspect of human language and cognition, playing a crucial role in communication, comprehension, and creative expression. In light of the recent advancements demonstrated by prominent language models, a pivotal question arises: Can these expansive language models effectively discern metaphorical knowledge? The primary objective involves comparing the surprisal values estimated from neural network language models like autoregressive and bidirectional language models to the reaction times of human when exposed to both metaphorical and literal sentences. Our secondary objective involves assessing the AI's comprehension of metaphors by utilizing the sensicality ratings generated by sophisticated ChatGPT. To achieve this, we used psycholinguistic methods, and adopted the experimental materials from Lai, Currana, and Menna (2009). We found the surprisal values estimated from the autoregressive language model demonstrate metaphor processing that closely resembles that of native speakers. Furthermore, ChatGPT's processing of conventional metaphorical sentences closely resembles its approach to literal sentences, mirroring the convergence observed in native speakers' ERP response to conventional metaphorical sentences and their alignment with that of literal sentences.",
        "link": "http://dx.doi.org/10.14342/smog.2023.119.87"
    },
    {
        "id": 23572,
        "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding",
        "authors": "Young Jin Kim, Hany Hassan",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.sustainlp-1.20"
    },
    {
        "id": 23573,
        "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models",
        "authors": "Rakesh Chada, Pradeep Natarajan, Darshan Fofadiya, Prathap Ramachandra",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.44"
    },
    {
        "id": 23574,
        "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
        "authors": "Tharindu Madusanka, Iqra Zahid, Hao Li, Ian Pratt-Hartmann, Riza Batista-Navarro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.536"
    },
    {
        "id": 23575,
        "title": "Probing for Multilingual Numerical Understanding in Transformer-Based Language Models",
        "authors": "Devin Johnson, Denise Mak, Andrew Barker, Lexi Loessberg-Zahl",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.blackboxnlp-1.18"
    },
    {
        "id": 23576,
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": "Jia Lim, Hady Lauw",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.534"
    },
    {
        "id": 23577,
        "title": "LVCSR with Transformer Language Models",
        "authors": "Eugen Beck, Ralf Schlüter, Hermann Ney",
        "published": "2020-10-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1164"
    },
    {
        "id": 23578,
        "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
        "authors": "Michael Hanna, Yonatan Belinkov, Sandro Pezzelle",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.744"
    },
    {
        "id": 23579,
        "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
        "authors": "Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann",
        "published": "2020-10-25",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1963"
    },
    {
        "id": 23580,
        "title": "Advanced Language Understanding with Syntax-Enhanced Transformer",
        "authors": "Wender Rine, Rodolfo Patel, Neo Steve",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we introduce Syntax-Enhanced Transformer Model (SET), a groundbreaking approach in the realm of Transformer-based language modeling that seeks to redefine the boundaries of linguistic analysis and comprehension. SET innovatively combine (i) the well-established high-level performance, scalability, and adaptability of traditional Transformers with (ii) a sophisticated analysis of syntactic structures. This synergy is enabled by a novel attention mechanism tailored to parse syntactic nuances and a deterministic process adept at transforming linearized parse trees into meaningful linguistic representations.  Our comprehensive experiments reveal that SET significantly advance the field by surpassing existing benchmarks in sentence-level language modeling perplexity. They exhibit exceptional proficiency in tasks that require an acute awareness of syntax, setting new standards for language models in understanding complex linguistic structures. Furthermore, SET demonstrate an enhanced capability to grasp nuanced linguistic patterns that have traditionally been challenging for standard Transformer models.  However, our studies also uncover a unique aspect of SET: while they excel in sentence-level tasks, their representation of sentences as singular vectors&mdash;owing to the syntactic composition constraints intrinsic to their design&mdash;introduces certain limitations in document-level language modeling. This observation points to an intriguing area for future exploration; it suggests the potential need for an alternative or complementary memory mechanism within Transformer models, one that functions independently from, yet in harmony with, syntactic structures. Such a mechanism could be pivotal in enhancing the model's ability to comprehend and process long-form texts effectively. In conclusion, SET mark a significant stride in the journey towards more sophisticated, syntax-aware language models. They offer promising insights into the integration of deep linguistic knowledge with cutting-edge machine learning techniques, potentially opening doors to a new era of natural language understanding and processing.",
        "link": "http://dx.doi.org/10.20944/preprints202312.1673.v1"
    },
    {
        "id": 23581,
        "title": "Adapting Pretrained Transformer to Lattices for Spoken Language Understanding",
        "authors": "Chao-Wei Huang, Yun-Nung Chen",
        "published": "2019-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9003825"
    },
    {
        "id": 23582,
        "title": "Comparative Evaluation of Transformer-Based Nepali Language Models",
        "authors": "Suyogya Ratna Tamrakar, Chaklam Silpasuwanchai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLarge pre-trained transformer models using self-supervised learning have achieved state-of-the-art performances in various NLP tasks. However, for low-resource language like Nepali, pre-training of monolingual models remains a problem due to lack of training data and well-designed and balanced benchmark datasets. Furthermore, several multilingual pre-trained models such as mBERT and XLM-RoBERTa have been released, but their performance remains unknown for Nepali language. We compared Nepali monolingual pre-trained transformer models with multilingual models to determine their performance using a Nepali text classification dataset as a downstream task based on different number of classes and data sizes, taking machine learning (ML) and deep learning (DL) algorithms as baselines. Under-representation of Nepali language in mBERT resulted in overall poor performance, but, XLM-RoBERTa, which has a larger vocabulary size, produced state-of-the-art performance which is relatively similar to that of Nepali DistilBERT and DeBERTa, which outperformed all of the baseline algorithms. Bi-LSTM and SVM from the baselines also performed very well in variety of settings. Moreover, to assess the cross-language knowledge transfer for the cases when mono-lingual models are not available, we also evaluated HindiRoBERTa, a monolingual Indian language model on Nepali text dataset. This research mainly contributes to the Nepali NLP community by creation of news classification dataset with 20 classes, with over 200,000 articles and performance evaluation of various pre-trained monolingual Nepali transformers with multilingual transformers, DL and ML algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2289743/v1"
    },
    {
        "id": 23583,
        "title": "Understanding models understanding language",
        "authors": "Anders Søgaard",
        "published": "2022-10-27",
        "citations": 1,
        "abstract": "AbstractLandgrebe and Smith (Synthese 198(March):2061–2081, 2021) present an unflattering diagnosis of recent advances in what they call language-centric artificial intelligence—perhaps more widely known as natural language processing: The models that are currently employed do not have sufficient expressivity, will not generalize, and are fundamentally unable to induce linguistic semantics, they say. The diagnosis is mainly derived from an analysis of the widely used Transformer architecture. Here I address a number of misunderstandings in their analysis, and present what I take to be a more adequate analysis of the ability of Transformer models to learn natural language semantics. To avoid confusion, I distinguish between inferential and referential semantics. Landgrebe and Smith (2021)’s analysis of the Transformer architecture’s expressivity and generalization concerns inferential semantics. This part of their diagnosis is shown to rely on misunderstandings of technical properties of Transformers. Landgrebe and Smith (2021) also claim that referential semantics is unobtainable for Transformer models. In response, I present a non-technical discussion of techniques for grounding Transformer models, giving them referential semantics, even in the absence of supervision. I also present a simple thought experiment to highlight the mechanisms that would lead to referential semantics, and discuss in what sense models that are grounded in this way, can be said to understand language. Finally, I discuss the approach Landgrebe and Smith (2021) advocate for, namely manual specification of formal grammars that associate linguistic expressions with logical form.",
        "link": "http://dx.doi.org/10.1007/s11229-022-03931-4"
    },
    {
        "id": 23584,
        "title": "Pre-Trained Transformer-Based Language Models for Sundanese",
        "authors": "Wilson Wongso, Henry Lucky, Derwin Suhartono",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThe Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-907893/v1"
    },
    {
        "id": 23585,
        "title": "Classifying Drug Ratings Using User Reviews with Transformer-Based Language Models",
        "authors": "Akhil Shiju, Zhe He",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractDrugs.com provides users’ textual reviews and numeric ratings of drugs. However, text reviews may not always be consistent with the numeric ratings. Overly positive or negative rating may be misleading. In this project, to classify user ratings of drugs with their textual reviews, we built classification models using traditional machine learning and deep learning approaches. Machine learning models including Random Forest and Naive Bayesian classifiers were built using TF-IDF features as input. Also, transformer-based neural network models including BERT, BioBERT, RoBERTa, XLNet, ELECTRA, and ALBERT were built using the raw text as input. Overall, BioBERT model outperformed the other models with an overall accuracy of 87%. We further identified UMLS concepts from the postings and analyzed their semantic types in the postings stratified by the classification result. This research demonstrated that transformer-based models can be used to classify drug reviews and identify reviews that are inconsistent with the ratings.",
        "link": "http://dx.doi.org/10.1101/2021.04.15.21255573"
    },
    {
        "id": 23586,
        "title": "Single-sequence protein structure prediction using supervised transformer protein language models",
        "authors": "Wenkai Wang, Zhenling Peng, Jianyi Yang",
        "published": "No Date",
        "citations": 8,
        "abstract": "AbstractIt remains challenging for single-sequence protein structure prediction with AlphaFold2 and other deep learning methods. In this work, we introduce trRosettaX-Single, a novel algorithm for singlesequence protein structure prediction. It is built on sequence embedding from s-ESM-1b, a supervised transformer protein language model optimized from the pre-trained model ESM-1b. The sequence embedding is fed into a multi-scale network with knowledge distillation to predict inter-residue 2D geometry, including distance and orientations. The predicted 2D geometry is then used to reconstruct 3D structure models based on energy minimization. Benchmark tests show that trRosettaX-Single outperforms AlphaFold2 and RoseTTAFold on natural proteins. For instance, with single-sequence input, trRosettaX-Single generates structure models with an average TM-score ~0.5 on 77 CASP14 domains, significantly higher than AlphaFold2 (0.35) and RoseTTAFold (0.34). Further test on 101 human-designed proteins indicates that trRosettaX-Single works very well, with accuracy (average TM-score 0.77) approaching AlphaFold2 and higher than RoseTTAFold, but using much less computing resource. On 2000 designed proteins from network hallucination, trRosettaX-Single generates structure models highly consistent to the hallucinated ones. These data suggest that trRosettaX-Single may find immediate applications in de novo protein design and related studies. trRosettaX-Single is available through the trRosetta server at: http://yanglab.nankai.edu.cn/trRosetta/.",
        "link": "http://dx.doi.org/10.1101/2022.01.15.476476"
    },
    {
        "id": 23587,
        "title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation",
        "authors": "Keqi Deng, Shinji Watanabe, Jiatong Shi, Siddhant Arora",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-933"
    },
    {
        "id": 23588,
        "title": "Comparative Analysis of Transformer based Language Models",
        "authors": "Aman Pathak",
        "published": "2021-1-23",
        "citations": 3,
        "abstract": "Natural language processing (NLP) has witnessed many substantial advancements in the past three years. With the introduction of the Transformer and self-attention mechanism, language models are now able to learn better representations of the natural language. These attentionbased models have achieved exceptional state-of-the-art results on various NLP benchmarks. One of the contributing factors is the growing use of transfer learning. Models are pre-trained on unsupervised objectives using rich datasets that develop fundamental natural language abilities that are fine-tuned further on supervised data for downstream tasks. Surprisingly, current researches have led to a novel era of powerful models that no longer require finetuning. The objective of this paper is to present a comparative analysis of some of the most influential language models. The benchmarks of the study are problem-solving methodologies, model architecture, compute power, standard NLP benchmark accuracies and shortcomings.",
        "link": "http://dx.doi.org/10.5121/csit.2021.110111"
    },
    {
        "id": 23589,
        "title": "Utilization of Transformer-Based Language Models in Understanding Citizens’ Interests, Sentiments and Emotions Towards Public Services Digitalization",
        "authors": "Gloria Hristova, Nikolay Netov",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "We live in an era of digital revolution not only in the industry, but also in the public sector. User opinion is key in e-services development. Currently the most established approaches for analyzing citizens’ opinions are surveys and personal interviews. However, governments should focus not only on developing public e-services but also on implementing modern solutions for data analysis based on machine learning and artificial intelligence. The main aim of the current study is to engage state-of-the-art natural language processing technologies to develop an analytical approach for public opinion analysis. We utilize transformer-based language models to derive valuable insights into citizens’ interests and expressed sentiments and emotions towards digitalization of educational, administrative and health public services. Our research brings empirical evidence on the practical usefulness of such methods in the government domain.",
        "link": "http://dx.doi.org/10.3233/faia230711"
    },
    {
        "id": 23590,
        "title": "Improving protein secondary structure prediction by deep language models and transformer networks",
        "authors": "Tianqi Wu, Weihang Cheng, Jianlin Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProtein secondary structure prediction is useful for many applications. It can be considered a language translation problem, i.e., translating a sequence of 20 different amino acids into a sequence of secondary structure symbols (e.g., alpha helix, beta strand, and coil). Here, we develop a novel protein secondary structure predictor called TransPross based on the transformer network and attention mechanism widely used in natural language processing to directly extract the evolutionary information from the protein language (i.e., raw multiple sequence alignment (MSA) of a protein) to predict the secondary structure. The method is different from traditional methods that first generate a MSA and then calculate expert-curated statistical profiles from the MSA as input. The attention mechnism used by TransPross can effectively capture long-range residue-residue interactions in protein sequences to predict secondary structures. Benchmarked on several datasets, TransPross outperforms the state-of-art methods. Moreover, our experiment shows that the prediction accuracy of TransPross positively correlates with the depth of MSAs and it is able to achieve the average prediction accuracy (i.e., Q3 score) above 80% for hard targets with few homologous sequences in their MSAs. TransPross is freely available athttps://github.com/BioinfoMachineLearning/TransPro",
        "link": "http://dx.doi.org/10.1101/2022.11.21.517442"
    },
    {
        "id": 23591,
        "title": "Transformer-based Language Models for Semantic Search and Mobile Applications Retrieval",
        "authors": "João Coelho, António Neto, Miguel Tavares, Carlos Coutinho, João Oliveira, Ricardo Ribeiro, Fernando Batista",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010657300003064"
    },
    {
        "id": 23592,
        "title": "Sentence Bottleneck Autoencoders from Transformer Language Models",
        "authors": "Ivan Montero, Nikolaos Pappas, Noah A. Smith",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.137"
    },
    {
        "id": 23593,
        "title": "Transformer protein language models are unsupervised structure learners",
        "authors": "Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, Alexander Rives",
        "published": "No Date",
        "citations": 100,
        "abstract": "AbstractUnsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1",
        "link": "http://dx.doi.org/10.1101/2020.12.15.422761"
    },
    {
        "id": 23594,
        "title": "Evaluation of Transformer-Based Neural Language Models for Writing Feedback and Automated Essay Scoring",
        "authors": "Temesgen Abraha, Amril Nazir",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWriting remains a challenging skill for many students due to inadequate feedback tools. There is a need to develop more effective tools for supporting students’ writing skill development. This study aims to evaluate the effectiveness of transformer-based neural language models for assessing and automatically scoring argumentative essays written by 8th-12th grade English Language Learners (ELLs). The students’ English essays were assessed and scored based on six criteria, including cohesion, syntax, vocabulary, phraseology, grammar, and conventions. The models were trained on real teacher feedback from 2700 scored essays. We also compared various transformer-based neural language models to find the most effective model. Several metrics were used for evaluation, with the root mean square error (RMSE) as the primary measure. The results show that a specific model, DeBERTa-v3-large, outperforms others in most categories. In conclusion, this study suggests that transformer-based neural language models, especially when using the DeBERTa-v3-large model, hold significant promise in improving automated essay scoring and feedback, potentially leading to enhanced writing skills among English language learners.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3979085/v1"
    },
    {
        "id": 23595,
        "title": "MuLan-Methyl - Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
        "authors": "Wenhuan Zeng, Anupam Gautam, Daniel H. Huson",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractTransformer-based language models are successfully used to address massive text-related tasks. DNA methylation is an important epigenetic mechanism and its analysis provides valuable insights into gene regulation and biomarker identification. Several deep learning-based methods have been proposed to identify DNA methylation and each seeks to strike a balance between computational effort and accuracy. Here, we introduce MuLan-Methyl, a deep-learning framework for predicting DNA methylation sites, which is based on five popular transformer-based language models. The framework identifies methylation sites for three different types of DNA methylation, namely N6-adenine, N4-cytosine, and 5-hydroxymethylcytosine. Each of the employed language models is adapted to the task using the “pre-train and fine-tune” paradigm. Pre-training is performed on a custom corpus of DNA fragments and taxonomy lineages using self-supervised learning. Fine-tuning aims at predicting the DNA-methylation status of each type. The five models are used to collectively predict the DNA methylation status. We report excellent performance of MuLan-Methyl on a benchmark dataset. Moreover, we argue that the model captures characteristic differences between different species that are relevant for methylation. This work demonstrates that language models can be successfully adapted to applications in biological sequence analysis and that joint utilization of different language models improves model performance. Mulan-Methyl is open source and we provide a web server that implements the approach.Key pointsMuLan-Methyl aims at identifying three types of DNA-methylation sites.It uses an ensemble of five transformer-based language models, which were pre-trained and fine-tuned on a custom corpus.The self-attention mechanism of transformers give rise to importance scores, which can be used to extract motifs.The method performs favorably in comparison to existing methods.The implementation can be applied to chromosomal sequences to predict methylation sites.",
        "link": "http://dx.doi.org/10.1101/2023.01.04.522704"
    },
    {
        "id": 23596,
        "title": "Transformer-Based Multilingual Language Models in Cross-Lingual Plagiarism Detection",
        "authors": "Tatevik Ter-Hovhannisyan, Karen Avetisyan",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivmem57067.2022.9983968"
    },
    {
        "id": 23597,
        "title": "Whisper-Slu: Extending a Pretrained Speech-to-Text Transformer for Low Resource Spoken Language Understanding",
        "authors": "Quentin Meeus, Marie-Francine Moens, Hugo Van Hamme",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389786"
    },
    {
        "id": 23598,
        "title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models",
        "authors": "Sebastian Koch, Stefan Pasch",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386765"
    },
    {
        "id": 23599,
        "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
        "authors": "Luke Gessler, Nathan Schneider",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.17"
    },
    {
        "id": 23600,
        "title": "Learning the Language of NMR: Structure Elucidation from NMR spectra using Transformer Models",
        "authors": "Marvin Alberts, Federico Zipoli, Alain C. Vaucher",
        "published": "No Date",
        "citations": 1,
        "abstract": "The application of machine learning models in chemistry has made remarkable strides in recent years. Even though there is considerable interest in automating common proce- dure in analytical chemistry using machine learning, very few models have been adopted into everyday use. Among the analytical instruments available to chemists, Nuclear Mag- netic Resonance (NMR) spectroscopy is one of the most important, offering insights into molecular structure unobtainable with other methods. However, most processing and analysis of NMR spectra is still performed manually, making the task tedious and time consuming especially for larger quantities of spectra. We present a transformer-based machine learning model capable of predicting the molecular structure directly from the NMR spectrum. Our model is pretrained on synthetic NMR spectra, achieving a top–1 accuracy of 67.0% when predicting the structure from both the 1H and 13C spectrum. Additionally, we train a model which, given a spectrum and a set of likely compounds, selects the one corresponding to the spectrum. This model achieves a top–1 accuracy of 96.0% when trained on 1H spectra.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-8wxcz"
    },
    {
        "id": 23601,
        "title": "Warped Language Models for Noise Robust Language Understanding",
        "authors": "Mahdi Namazifar, Gokhan Tur, Dilek Hakkani-Tur",
        "published": "2021-1-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383493"
    },
    {
        "id": 23602,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the \\ac{3gpp} standard working groups. We consider training the selected models on 3GPP techincal documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23501271"
    },
    {
        "id": 23603,
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": "Mutian He, Philip N. Garner",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1799"
    },
    {
        "id": 23604,
        "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
        "authors": "Ryo Masumura, Tomohiro Tanaka, Atsushi Ando, Hosana Kamiyama, Takanobu Oba, Satoshi Kobashikawa, Yushi Aono",
        "published": "2019-9-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1534"
    },
    {
        "id": 23605,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the \\ac{3gpp} standard working groups. We consider training the selected models on 3GPP techincal documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23501271.v1"
    },
    {
        "id": 23606,
        "title": "Alignment of Dialogue Models",
        "authors": "",
        "published": "2021-1-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108610728.009"
    },
    {
        "id": 23607,
        "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
        "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.195"
    },
    {
        "id": 23608,
        "title": "Enhancing Text Summarization: Evaluating Transformer-Based Models and the Role of Large Language Models like ChatGPT",
        "authors": "Pınar Savcı, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391040"
    },
    {
        "id": 23609,
        "title": "Attacking a Transformer-Based Models for Arabic Language as Low Resources Language (LRL) Using Word-Substitution Methods",
        "authors": "Hanin Alshalan, Banafsheh Rekabdar",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00025"
    },
    {
        "id": 23610,
        "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
        "authors": "Baolin Peng, Chenguang Zhu, Michael Zeng, Jianfeng Gao",
        "published": "2021-8-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-117"
    },
    {
        "id": 23611,
        "title": "Classical Machine Learning and Transformer Models for Offensive and Abusive Language Classification on Dziri Language",
        "authors": "Mohammed Mehdi Bouchene, Kheireddine Abainia",
        "published": "2023-9-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasa59624.2023.10286654"
    },
    {
        "id": 23612,
        "title": "Transformer-based deep neural network language models for Alzheimer’s disease detection from targeted speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early diagnosis of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classification tasks with small training sets. The overall classification model is a simple classifier on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classifier achieves classification accuracy of 88.08%, which improves thestate-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of sufficiently large datasets, but also reduces the need for expert-defined features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v2"
    },
    {
        "id": 23613,
        "title": "Accurate, interpretable predictions of materials properties within transformer language models",
        "authors": "Vadim Korolev, Pavel Protsenko",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patter.2023.100803"
    },
    {
        "id": 23614,
        "title": "Transformer-Based Deep Neural Network Language Models for Alzheimer's Disease Detection from Targeted Speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early diagnosis of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classiﬁcation tasks with small training sets. The overall classiﬁcation model is a simple classiﬁer on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classiﬁer achieves classiﬁcation accuracy of 88.08%, which improves the state-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of suﬃciently large datasets, but also reduces the need for expert-deﬁned features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v1"
    },
    {
        "id": 23615,
        "title": "20.5 C-Transformer: A 2.6-18.1μJ/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models",
        "authors": "Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, Hoi-Jun Yoo",
        "published": "2024-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isscc49657.2024.10454330"
    },
    {
        "id": 23616,
        "title": "Probing for Bridging Inference in Transformer Language Models",
        "authors": "Onkar Pandit, Yufang Hou",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.327"
    },
    {
        "id": 23617,
        "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
        "authors": "Lina Conti, Guillaume Wisniewski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.641"
    },
    {
        "id": 23618,
        "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
        "authors": "William Timkey, Marten van Schijndel",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.372"
    },
    {
        "id": 23619,
        "title": "Deeper understanding of In-Context Retrieval-Augmented Language Models",
        "authors": " ",
        "published": "No Date",
        "citations": 0,
        "abstract": "<strong> Transformative Advances in Language Models through External Knowledge Integration </strong> <strong> Author: </strong> Qingqin Fang(0009–0003–5348–4264) <strong> <strong> Introduction </strong> </strong> In the dynamic field of natural language processing, the integration of external knowledge has emerged as a pivotal strategy for enhancing the performance of language models.",
        "link": "http://dx.doi.org/10.59350/ch3em-a4h27"
    },
    {
        "id": 23620,
        "title": "Understanding Large Language Models",
        "authors": "Thimira Amaratunga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/979-8-8688-0017-7"
    },
    {
        "id": 23621,
        "title": "Topic-Transformer for Document-Level Language Understanding",
        "authors": "Oumaima Hourrane, El Habib Benlahmar",
        "published": "2022-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3844/jcssp.2022.18.25"
    },
    {
        "id": 23622,
        "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization",
        "authors": "Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.627"
    },
    {
        "id": 23623,
        "title": "Transformer-based deep neural network language models for Alzheimer's disease risk assessment from targeted speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early risk assessment of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classiﬁcation tasks with small training sets. The overall classiﬁcation model is a simple classiﬁer on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classiﬁer achieves classiﬁcation accuracy of 88.08%, which improves the state-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of suﬃciently large datasets, but also reduces the need for expert-deﬁned features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v3"
    },
    {
        "id": 23624,
        "title": "Symbolic Semantic Memory in Transformer Language Models",
        "authors": "Robert Morain, Kenneth Vargas, Dan Ventura",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00166"
    },
    {
        "id": 23625,
        "title": "Detecting Sarcasm in Conversation Context Using Transformer-Based Models",
        "authors": "Adithya Avvaru, Sanath Vobilisetty, Radhika Mamidi",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.figlang-1.15"
    },
    {
        "id": 23626,
        "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
        "authors": "Laurent Sartran, Samuel Barrett, Adhiguna Kuncoro, Miloš Stanojević, Phil Blunsom, Chris Dyer",
        "published": "2022-12-22",
        "citations": 1,
        "abstract": "Abstract\nWe introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.",
        "link": "http://dx.doi.org/10.1162/tacl_a_00526"
    },
    {
        "id": 23627,
        "title": "Influence of Language Proficiency on the Readability of Review Text and Transformer-based Models for Determining Language Proficiency",
        "authors": "Salim Sazzed",
        "published": "2022-4-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3487553.3524666"
    },
    {
        "id": 23628,
        "title": "Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation",
        "authors": "Usman Naseem, Ajay Bandi, Shaina Raza, Junaid Rashid, Bharathi Raja Chakravarthi",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.bionlp-1.10"
    },
    {
        "id": 23629,
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": "Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.459"
    },
    {
        "id": 23630,
        "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
        "authors": "Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal",
        "published": "2020",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.748"
    },
    {
        "id": 23631,
        "title": "Transformer-based Natural Language Understanding and Generation",
        "authors": "Feng Zhang, Gaoyun An, Qiuqi Ruan",
        "published": "2022-10-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp56322.2022.9965301"
    },
    {
        "id": 23632,
        "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
        "authors": "Eric Sun, Jinyu Li, Zhong Meng, Yu Wu, Jian Xue, Shujie Liu, Yifan Gong",
        "published": "2021-8-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1949"
    },
    {
        "id": 23633,
        "title": "On the effect of dropping layers of pre-trained transformer models",
        "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov",
        "published": "2023-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.csl.2022.101429"
    },
    {
        "id": 23634,
        "title": "Large Language Models' Understanding of Math: Source Criticism and Extrapolation",
        "authors": "Roozbeh Yousefzadeh, Xuenan Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIt has been suggested that large language models such as GPT-4 have acquired some form of understanding beyond the correlations among the words in text including some understanding of mathematics as well. Here, we perform a critical inquiry into this claim by evaluating the mathematical understanding of the GPT-4 model. Considering that GPT-4's training set is a secret, it is not straightforward to evaluate whether the model's correct answers are based on a mathematical understanding or based on replication of proofs that the model has seen before. We specifically craft mathematical questions which their formal proofs are not readily available on the web, proofs that are more likely not seen by the GPT-4. We see that GPT-4 is unable to solve those problems despite their simplicity. It is hard to find scientific evidence suggesting that GPT-4 has acquired an understanding of even basic mathematical concepts. A straightforward way to find failure modes of GPT-4 in theorem proving is to craft questions where their formal proofs are not available on the web. Our finding suggests that GPT-4's ability is to reproduce, rephrase, and polish the mathematical proofs that it has seen before, and not in grasping mathematical concepts. We also see that GPT-4's ability to prove mathematical theorems is continuously expanding over time despite the claim that it is a fixed model. We suggest that the task of proving mathematical theorems in formal language is comparable to the methods used in search engines such as Google while predicting the next word in a sentence may be a misguided approach, a recipe that often leads to excessive extrapolation and eventual failures. Prompting the GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question whether it is valuable for machine learning or for theorem proving.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3598853/v1"
    },
    {
        "id": 23635,
        "title": "Arabic Dialect Identification and Sentiment Classification using Transformer-based Models",
        "authors": "Joseph Attieh, Fadi Hassan",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.54"
    },
    {
        "id": 23636,
        "title": "Deep Transfer Learning &amp;amp;Amp; Beyond: Transformer Language Models in Information Systems Research",
        "authors": "Ross Gruetzemacher, David Paradice",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3944942"
    },
    {
        "id": 23637,
        "title": "LVBERT: Transformer-Based Model for Latvian Language Understanding",
        "authors": "Artūrs Znotiņš, Guntis Barzdiņš",
        "published": "2020-9-15",
        "citations": 4,
        "abstract": "This paper presents LVBERT – the first publicly available monolingual language model pre-trained for Latvian. We show that LVBERT improves the state-of-the-art for three Latvian NLP tasks including Part-of-Speech tagging, Named Entity Recognition and Universal Dependency parsing. We release LVBERT to facilitate future research and downstream applications for Latvian NLP.",
        "link": "http://dx.doi.org/10.3233/faia200610"
    },
    {
        "id": 23638,
        "title": "Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models",
        "authors": "Behrad TaghiBeyglou, Frank Rudzicz",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100046"
    },
    {
        "id": 23639,
        "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
        "authors": "Chong Yu, Tao Chen, Zhongxue Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.15"
    },
    {
        "id": 23640,
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models",
        "authors": "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.765"
    },
    {
        "id": 23641,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 10,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 23642,
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": "Soo Ryu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.27"
    },
    {
        "id": 23643,
        "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
        "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.276"
    },
    {
        "id": 23644,
        "title": "Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models",
        "authors": "Taichi Iki, Akiko Aizawa",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.167"
    },
    {
        "id": 23645,
        "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
        "authors": "Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.3"
    },
    {
        "id": 23646,
        "title": "CTRAN: CNN-Transformer-based network for natural language understanding",
        "authors": "Mehrdad Rafiepour, Javad Salimi Sartakhti",
        "published": "2023-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107013"
    },
    {
        "id": 23647,
        "title": "Efficiency in Language Understanding and Generation: An Evaluation of Four Open-Source Large Language Models",
        "authors": "Siu Ming Wong, Ho Leung, Ka Yan Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study provides a comprehensive evaluation of the efficiency of Large Language Models (LLMs) in performing diverse language understanding and generation tasks. Through a systematic comparison of open-source models including GPT-Neo, Bloom, FLAN-T5, and Mistral-7B, the research explores their performance across widely recognized benchmarks such as GLUE, SuperGLUE, LAMBADA, and SQuAD. Our findings reveal significant variations in model accuracy, computational efficiency, scalability, and adaptability, underscoring the influence of model architecture and training paradigms on performance outcomes. The study identifies key factors contributing to the models' efficiency and offers insights into potential optimization strategies for enhancing their applicability in real-world NLP applications. By highlighting the strengths and limitations of current LLMs, this research contributes to the ongoing development of more effective, efficient, and adaptable language models, paving the way for future advancements in the field of natural language processing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4063228/v1"
    },
    {
        "id": 23648,
        "title": "How Relevant Are Selectional Preferences for Transformer-based Language Models?",
        "authors": "Eleni Metheniti, Tim Van de Cruys, Nabil Hathout",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.109"
    },
    {
        "id": 23649,
        "title": "A Comparison of Transformer and LSTM Encoder Decoder Models for ASR",
        "authors": "Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schluter, Hermann Ney",
        "published": "2019-12",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9004025"
    },
    {
        "id": 23650,
        "title": "Deep Transformer Language Models for Arabic Text Summarization: A Comparison Study",
        "authors": "Hasna Chouikhi, Mohammed Alsuhaibani",
        "published": "2022-11-23",
        "citations": 6,
        "abstract": "Large text documents are sometimes challenging to understand and time-consuming to extract vital information from. These issues are addressed by automatic text summarizing techniques, which condense lengthy texts while preserving their key information. Thus, the development of automatic summarization systems capable of fulfilling the ever-increasing demands of textual data becomes of utmost importance. It is even more vital with complex natural languages. This study explores five State-Of-The-Art (SOTA) Arabic deep Transformer-based Language Models (TLMs) in the task of text summarization by adapting various text summarization datasets dedicated to Arabic. A comparison against deep learning and machine learning-based baseline models has also been conducted. Experimental results reveal the superiority of TLMs, specifically the PEAGASUS family, against the baseline approaches, with an average F1-score of 90% on several benchmark datasets.",
        "link": "http://dx.doi.org/10.3390/app122311944"
    },
    {
        "id": 23651,
        "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models",
        "authors": "Dusan Varis, Ondřej Bojar",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.650"
    },
    {
        "id": 23652,
        "title": "How Are Idioms Processed Inside Transformer Language Models?",
        "authors": "Ye Tian, Isobel James, Hye Son",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.starsem-1.16"
    },
    {
        "id": 23653,
        "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
        "authors": "Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, Muhao Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.317"
    },
    {
        "id": 23654,
        "title": "A Survey of Transformer-Based Natural Language Processing Models",
        "authors": "鸣姝 赖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/airr.2023.123025"
    },
    {
        "id": 23655,
        "title": "Introduction",
        "authors": "Thimira Amaratunga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/979-8-8688-0017-7_1"
    },
    {
        "id": 23656,
        "title": "QUESTION ANSWERING SYSTEM FOR HOSPITALITY DOMAIN USING TRANSFORMER-BASED LANGUAGE MODELS",
        "authors": "Sathish Sathish Dhanasegar",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "Recent research demonstrates significant success on a wide range of Natural Language Processing (NLP) tasks by utilizing Transformer architectures. Question answering (QA) is an important aspect of the NLP task. The systems enable users to ask a question in natural language and receive an answer accordingly. Most questions in the hospitality industry are content-based, with the expected response being accurate data rather than”yes” or ”no.” Therefore, it requires the system to understand the semantics of the questions and return relevant answers. Despite several advancements in transformer-based models for QA, we are interested in evaluating how it performs with unlabeled data using a pre-trained model, which could also define-tune. This project aims to develop a Question-Answering system for the hospitality domain, in which text will have hospitality content, and the user will be able to ask a question about them. We use an Attention mechanism to train a span-based model that predicts the position of the start and end tokens in a paragraph. By using the model, the users can directly type in their questions in the interactive user interface and receive the response. The data set for this study is created using response templates from the existing dialogue system. We use the Stanford Question and Answer (SQuAD 2.0) data structure to form the dataset, which is mostly used for QA models. During phase1, we evaluate the pre-trained QA models BERT, ROBERTa, and DistilBERT to predict answers and measure the results using Exact Match(EM) and ROUGE-LF1-Score. In Phase 2 of the project, we fine-tune the QA models and their hyper-parameters by training the model with hospitality data sets, and the results are compared. The fine-tuned ROBERTa models achieved the maximum of ROUGE-L F1-Score and EM of 71.39 and 52.17, respectively, which is a relatively 4% increase in F1-Score and 8.7% increase in EM score compared to the pre-trained model. The results of this project will be used to improve the efficiency of the dialogue system in the hospitality industry.",
        "link": "http://dx.doi.org/10.26562/irjcs.2022.v0905.003"
    },
    {
        "id": 23657,
        "title": "Transformers",
        "authors": "Thimira Amaratunga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/979-8-8688-0017-7_3"
    },
    {
        "id": 23658,
        "title": "Analyzing Redundancy in Pretrained Transformer Models",
        "authors": "Fahim Dalvi, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.398"
    },
    {
        "id": 23659,
        "title": "Pre-trained transformer-based language models for Sundanese",
        "authors": "Wilson Wongso, Henry Lucky, Derwin Suhartono",
        "published": "2022-12",
        "citations": 4,
        "abstract": "AbstractThe Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",
        "link": "http://dx.doi.org/10.1186/s40537-022-00590-7"
    },
    {
        "id": 23660,
        "title": "Transformer-Based Language Models for Bulgarian",
        "authors": "Iva Marinova,  , Kiril Simov, Petya Osenova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_077"
    },
    {
        "id": 23661,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437725"
    },
    {
        "id": 23662,
        "title": "WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
        "authors": "Heting Gao, Junrui Ni, Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson",
        "published": "2022-9-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11031"
    },
    {
        "id": 23663,
        "title": "Detecting Tweets Containing Cannabidiol-Related COVID-19 Misinformation Using Transformer Language Models and FDA Warning Letters (Preprint)",
        "authors": "Jason Turner, Mehmed Kantardzic, Rachel Vickers-Smith, Andrew Brown",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nThe COVID-19 pandemic introduced yet another medical condition for online sellers of loosely regulated substances such as cannabidiol (CBD) to falsely promote sales. As a result, it has become necessary to innovate ways to identify such instances of misinformation.\n\n\nOBJECTIVE\nWe used transformer-based language models to identify COVID-19 misinformation as it relates to the sales and/or promotion of CBD, by finding tweets that are semantically similar to quotes taken from known instances of misinformation, specifically the publicly available FDA warning letters.\n\n\nMETHODS\nWe collected tweets using CBD and COVID-19 related terms. Using a previously trained model, we extracted the tweets indicating commercialization/sales of CBD, and annotated those containing COVID-19 misinformation, according to the FDA’s definitions. We encoded the collection of tweets and misinformation quotes into sentence vectors, and then calculated the cosine similarity between each quote and each tweet, so that a threshold could be established to identify tweets that are making false claims regarding CBD and COVID-19, while minimizing the instance of false-positives.\n\n\nRESULTS\nWe demonstrated that by using quotes taken from FDA warning letters of known offenses we can identify semantically similar tweets that also contain similar misinformation. By identifying a cosine distance threshold between the sentence vector of the warning letters and the sentence vector of the tweets, we can identify tweets that contain similar forms of misinformation.\n\n\nCONCLUSIONS\nOur framework shows that commercial CBD/COVID-19 misinformation can potentially be identified and consequently curbed by using transformer-based language models and known prior instances of misinformation. Our approach functions without need for labeled data, potentially reducing the time in which misinformation could be identified. Our proposed framework shows promise in being easily adapted to identify other forms of misinformation related to loosely regulated substances, such as that related to autism, dementia, and Alzheimer’s disease.\n",
        "link": "http://dx.doi.org/10.2196/preprints.38390"
    },
    {
        "id": 23664,
        "title": "Understanding Word Embeddings and Language Models",
        "authors": "Jose Manuel Gomez-Perez, Ronald Denaux, Andres Garcia-Silva",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44830-1_3"
    },
    {
        "id": 23665,
        "title": "The evolution of transformer models from unidirectional to  bidirectional in Natural Language Processing",
        "authors": "Yihang Sun",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Transformer models have revolutionized Natural Language Processing (NLP), transitioning from traditional sequential models to innovative architectures based on attention mechanisms. The shift from unidirectional to bidirectional models has been a remarkable development in NLP. This paper mainly focuses on the evolution of NLP caused by Transformer models, with the transition from unidirectional to bidirectional modeling. This paper explores how the transformer model has revolutionized NLP, and the evolution from traditional sequential models to innovative attention-driven architectures. In this paper, it mainly discusses the limitations of traditional NLP models like RNNs, LSTMs and CNN when handling lengthy text sequences and complex dependencies, highlighting how transformer models, employing self-attention mechanisms and bidirectional modeling (e.g., BERT and GPT), have significantly improved NLP tasks. It provides a thorough review of the shift from unidirectional to bidirectional transformer models, offering insights into their utilization and development. Finally, this paper concludes with a summary and outlook for the entire study.",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230794"
    },
    {
        "id": 23666,
        "title": "How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?",
        "authors": "Laura Zanella, Yannick Toussaint",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.12"
    },
    {
        "id": 23667,
        "title": "Developmental Negation Processing in Transformer Language Models",
        "authors": "Antonio Laverghetta Jr., John Licato",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-short.60"
    },
    {
        "id": 23668,
        "title": "Understanding the Attention Mechanism in Neural Network Transformer Models in Image Restoration Tasks",
        "authors": "Nikita Berezhnov, Alexander Sirota",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349626"
    },
    {
        "id": 23669,
        "title": "Leveraging Transformer-based Language Models for Enhanced Service Insight in Tourism",
        "authors": "Aleyna Er, Şuayb Talha Özçelik, Meltem Turhan Yöndem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391041"
    },
    {
        "id": 23670,
        "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
        "authors": "Przemyslaw Joniak, Akiko Aizawa",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gebnlp-1.6"
    }
]