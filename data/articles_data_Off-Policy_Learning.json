[
    {
        "id": 18371,
        "title": "Q-Learning in Code",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_5"
    },
    {
        "id": 18372,
        "title": "Off-Policy Safe Reinforcement Learning for Nonlinear Discrete-Time Systems",
        "authors": "Mayank Shekhar JHA, Bahare Kiumarsi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4559729"
    },
    {
        "id": 18373,
        "title": "Distributed off-Policy Actor-Critic Reinforcement Learning with Policy Consensus",
        "authors": "Yan Zhang, Michael M. Zavlanos",
        "published": "2019-12",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029969"
    },
    {
        "id": 18374,
        "title": "Safe Off-policy Reinforcement Learning Using Barrier Functions",
        "authors": "Zahra Marvi, Bahare Kiumarsi",
        "published": "2020-7",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147584"
    },
    {
        "id": 18375,
        "title": "Reviewing On-Policy/Off-Policy Critic Learning in the Context of Temporal Differences and Residual Learning",
        "authors": "Frederic Roettger",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_2"
    },
    {
        "id": 18376,
        "title": "When to Target Customers? Retention Management using Dynamic Off-Policy Policy Learning",
        "authors": "Ryuya Ko, Kosuke Uetake, Kohei Yata, Ryosuke Okada",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4293532"
    },
    {
        "id": 18377,
        "title": "Bounds for off-policy prediction in reinforcement learning",
        "authors": "Ajin George Joseph, Shalabh Bhatnagar",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966359"
    },
    {
        "id": 18378,
        "title": "Off-policy Learning for Remote Electrical Tilt Optimization",
        "authors": "Filippo Vannella, Jaeseong Jeong, Alexandre Proutiere",
        "published": "2020-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-fall49728.2020.9348456"
    },
    {
        "id": 18379,
        "title": "On-Policy vs. Off-Policy Deep Reinforcement Learning for Resource Allocation in Open Radio Access Network",
        "authors": "Nessrine Hammami, Kim Khoa Nguyen",
        "published": "2022-4-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771605"
    },
    {
        "id": 18380,
        "title": "On-Demand Cold Start Frequency Reduction with Off-Policy Reinforcement Learning in Serverless Computing",
        "authors": "Siddharth Agarwal, Maria Rodriguez Read, Rajkumar Buyya",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4661993"
    },
    {
        "id": 18381,
        "title": "Off-policy Imitation Learning from Visual Inputs",
        "authors": "Zhihao Cheng, Li Shen, Dacheng Tao",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161566"
    },
    {
        "id": 18382,
        "title": "Exploration with Multiple Random ε-Buffers in Off-Policy Deep Reinforcement Learning",
        "authors": " Kim,  Park",
        "published": "2019-11-1",
        "citations": 2,
        "abstract": "In terms of deep reinforcement learning (RL), exploration is highly significant in achieving better generalization. In benchmark studies, ε-greedy random actions have been used to encourage exploration and prevent over-fitting, thereby improving generalization. Deep RL with random ε-greedy policies, such as deep Q-networks (DQNs), can demonstrate efficient exploration behavior. A random ε-greedy policy exploits additional replay buffers in an environment of sparse and binary rewards, such as in the real-time online detection of network securities by verifying whether the network is “normal or anomalous.” Prior studies have illustrated that a prioritized replay memory attributed to a complex temporal difference error provides superior theoretical results. However, another implementation illustrated that in certain environments, the prioritized replay memory is not superior to the randomly-selected buffers of random ε-greedy policy. Moreover, a key challenge of hindsight experience replay inspires our objective by using additional buffers corresponding to each different goal. Therefore, we attempt to exploit multiple random ε-greedy buffers to enhance explorations for a more near-perfect generalization with one original goal in off-policy RL. We demonstrate the benefit of off-policy learning from our method through an experimental comparison of DQN and a deep deterministic policy gradient in terms of discrete action, as well as continuous control for complete symmetric environments.",
        "link": "http://dx.doi.org/10.3390/sym11111352"
    },
    {
        "id": 18383,
        "title": "Optimal Robust Control of Nonlinear Uncertain System via Off-Policy Integral Reinforcement Learning",
        "authors": "Xiaoyang Wang, Xiufen Ye",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189626"
    },
    {
        "id": 18384,
        "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples With On-Policy Experiences",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "2024-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3174051"
    },
    {
        "id": 18385,
        "title": "Viznav: A Modular Off-Policy Deep Reinforcement Learning Framework for Vision-Based Autonomous Uav Navigation in 3d Dynamic Environments",
        "authors": "Fadi AlMahamid, Katarina Grolinger",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573189"
    },
    {
        "id": 18386,
        "title": "Off-Policy Reinforcement Learning for Optimal Control of a Two Wheeled Self Balancing Robot",
        "authors": "Athira Mullachery, Shaikshavali Chitraganti",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10441833"
    },
    {
        "id": 18387,
        "title": "Identification and off-policy learning of multiple objectives using adaptive clustering",
        "authors": "Thommen George Karimpanal, Erik Wilhelm",
        "published": "2017-11",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2017.04.074"
    },
    {
        "id": 18388,
        "title": "Off-Policy Meta-Reinforcement Learning With Belief-Based Task Inference",
        "authors": "Takahisa Imagawa, Takuya Hiraoka, Yoshimasa Tsuruoka",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3170582"
    },
    {
        "id": 18389,
        "title": "Generalized Gradient Emphasis Learning for Off-Policy Evaluation and Control with Function Approximation",
        "authors": "Jiaqing Cao, Quan Liu, Lan Wu, Qiming Fu, Shan Zhong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nEmphatic temporal-difference (TD) learning (Sutton et al. 2016) is a pioneering off-policy reinforcement learning method involving the use of the followon trace. The recently proposed Gradient Emphasis Learning (GEM, Zhang et al. 2020) algorithm is used to fix the problems of unbounded variance and large emphasis approximation error introduced by the followon trace from the perspective of stochastic approximation. In this paper, we rethink GEM and introduce a novel generalized GEM(β) algorithm to learn the true emphasis. The key to the construction of the generalized GEM(β) algorithm is introducing a tunable hyper-parameter β that is not necessarily the same as the discount factor γ to the GEM operator. We then apply the emphasis estimated by the proposed GEM(β) algorithm to the value estimation gradient and the policy gradient, respectively, yielding the corresponding emphatic TD variant for off-policy evaluation and actor-critic algorithm for off-policy control. Finally, we demonstrate empirically the advantage of the proposed algorithms across a range of problems, for both off-policy evaluation and off-policy control, and for both linear and nonlinear function approximation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2115364/v1"
    },
    {
        "id": 18390,
        "title": "Multi-player H<sub>∞</sub> Differential Game using On-Policy and Off-Policy Reinforcement Learning",
        "authors": "Peiliang An, Mushuang Liu, Yan Wan, Frank L. Lewis",
        "published": "2020-10-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icca51439.2020.9264554"
    },
    {
        "id": 18391,
        "title": "Off-Policy Game Reinforcement Learning",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_7"
    },
    {
        "id": 18392,
        "title": "Pessimistic Reward Models for Off-Policy Learning in Recommendation",
        "authors": "Olivier Jeunen, Bart Goethals",
        "published": "2021-9-13",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3460231.3474247"
    },
    {
        "id": 18393,
        "title": "Off-policy reinforcement learning for distributed output synchronization of linear multi-agent systems",
        "authors": "Bahare Kiumarsi, Frank L. Lewis",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280830"
    },
    {
        "id": 18394,
        "title": "H<sub>∞</sub> Optimal Distributed Tracking Control of Network Distributed Systems over Directed Networks via Off-Policy Reinforcement Learning",
        "authors": "Gulnihal Kucuksayacigil",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178321"
    },
    {
        "id": 18395,
        "title": "Off-Policy Q-Learning for Infinite Horizon LQR Problem with Unknown Dynamics",
        "authors": "Xinxing Li, Zhihong Peng, Li Liang",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie.2018.8433684"
    },
    {
        "id": 18396,
        "title": "Off-policy reinforcement learning for robust control of discrete-time uncertain linear systems",
        "authors": "Yongliang Yang, Zhishan Guo, Donald Wunsch, Yixin Yin",
        "published": "2017-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2017.8027737"
    },
    {
        "id": 18397,
        "title": "Optimal tracking control for discrete-time systems by model-free off-policy Q-learning approach",
        "authors": "Jinna Li, Decheng Yuan, Zhengtao Ding",
        "published": "2017-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ascc.2017.8287094"
    },
    {
        "id": 18398,
        "title": "OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching",
        "authors": "Hana Hoshino, Kei Ota, Asako Kanezaki, Rio Yokota",
        "published": "2022-5-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811660"
    },
    {
        "id": 18399,
        "title": "Off-policy and on-policy reinforcement learning with the Tsetlin machine",
        "authors": "Saeed Rahimi Gorji, Ole-Christoffer Granmo",
        "published": "2023-4",
        "citations": 2,
        "abstract": "AbstractThe Tsetlin Machine is a recent supervised learning algorithm that has obtained competitive accuracy- and resource usage results across several benchmarks. It has been used for convolution, classification, and regression, producing interpretable rules in propositional logic. In this paper, we introduce the first framework for reinforcement learning based on the Tsetlin Machine. Our framework integrates the value iteration algorithm with the regression Tsetlin Machine as the value function approximator. To obtain accurate off-policy state-value estimation, we propose a modified Tsetlin Machine feedback mechanism that adapts to the dynamic nature of value iteration. In particular, we show that the Tsetlin Machine is able to unlearn and recover from the misleading experiences that often occur at the beginning of training. A key challenge that we address is mapping the intrinsically continuous nature of state-value learning to the propositional Tsetlin Machine architecture, leveraging probabilistic updates. While accurate off-policy, this mechanism learns significantly slower than neural networks on-policy. However, by introducing multi-step temporal-difference learning in combination with high-frequency propositional logic patterns, we are able to close the performance gap. Several gridworld instances document that our framework can outperform comparable neural network models, despite being based on simple one-level AND-rules in propositional logic. Finally, we propose how the class of models learnt by our Tsetlin Machine for the gridworld problem can be translated into a more understandable graph structure. The graph structure captures the state-value function approximation and the corresponding policy found by the Tsetlin Machine.",
        "link": "http://dx.doi.org/10.1007/s10489-022-04297-3"
    },
    {
        "id": 18400,
        "title": "Off-Policy Reinforcement Learning for Optimal Preview Tracking Control of Linear Discrete-Time systems with unknown dynamics",
        "authors": "Chao-Ran Wang, Huai-Ning Wu",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623077"
    },
    {
        "id": 18401,
        "title": "Three-Dimensional Waypoint Navigation of Multicopters by Attitude and Throttle Commands using Off-Policy Reinforcement Learning",
        "authors": "Francesco d'Apolito, Christoph Sulzbachner",
        "published": "2022-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas54217.2022.9836078"
    },
    {
        "id": 18402,
        "title": "A perspective on off-policy evaluation in reinforcement learning",
        "authors": "Lihong Li",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11704-019-9901-7"
    },
    {
        "id": 18403,
        "title": "Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning",
        "authors": "Archit Sharma, Michael Ahn, Sergey Levine, Vikash Kumar, Karol Hausman, Shixiang Gu",
        "published": "2020-7-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2020.xvi.053"
    },
    {
        "id": 18404,
        "title": "Timing-Aware Resilience of Data-driven Off-policy Reinforcement Learning for Discrete-Time Systems",
        "authors": "Lijing Zhai, Filippos Fotiadis, Kyriakos G. Vamvoudakis, Jérôme Hugues",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155865"
    },
    {
        "id": 18405,
        "title": "An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem",
        "authors": "Victor G. Lopez, Matthias A. Müller",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10384256"
    },
    {
        "id": 18406,
        "title": "Imitation Learning-Based Performance-Power Trade-Off Uncore Frequency Scaling Policy for Multicore System",
        "authors": "Baonan Xiao, Jianfeng Yang, Xianxian Qi",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "As the importance of uncore components, such as shared cache slices and memory controllers, increases in processor architecture, the percentage of uncore power consumption in the overall power consumption of multicore processors rises significantly. To maximize the power efficiency of a multicore processor system, we investigate the uncore frequency scaling (UFS) policy and propose a novel imitation learning-based uncore frequency control policy. This policy performs online learning based on the DAgger algorithm and converts the annotation cost of online aggregation data into fine-tuning of the expert model. This design optimizes the online learning efficiency and improves the generality of the UFS policy on unseen loads. On the other hand, we shift our policy optimization target to Performance Per Watt (PPW), i.e., the power efficiency of the processor, to avoid saving a percentage of power while losing a larger percentage of performance. The experimental results show that our proposed policy outperforms the current advanced UFS policy in the benchmark test sequence of SPEC CPU2017. Our policy has a maximum improvement of about 10% relative to the performance-first policies. In the unseen processor load, the tuning decision made by our policy after collecting 50 aggregation data can maintain the processor stably near the optimal power efficiency state.",
        "link": "http://dx.doi.org/10.3390/s23031449"
    },
    {
        "id": 18407,
        "title": "Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes",
        "authors": "Andrew Bennett, Nathan Kallus",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": " In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived assuming a perfect Markov decision process (MDP) model. In “Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes,” A. Bennett and N. Kallus tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, they consider estimating the value of a given target policy in an unknown POMDP, given observations of trajectories generated by a different and unknown policy, which may depend on the unobserved states. They consider both when the target policy value can be identified the observed data and, given identification, how best to estimate it. Both these problems are addressed by extending the framework of proximal causal inference to POMDP settings, using sequences of so-called bridge functions. This results in a novel framework for off-policy evaluation in POMDPs that they term proximal reinforcement learning, which they validate in various empirical settings. ",
        "link": "http://dx.doi.org/10.1287/opre.2021.0781"
    },
    {
        "id": 18408,
        "title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates",
        "authors": "Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine",
        "published": "2017-5",
        "citations": 701,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2017.7989385"
    },
    {
        "id": 18409,
        "title": "Output Feedback H<sub>∞</sub>Control of Unknown Discrete-time Linear Systems: Off-policy Reinforcement Learning",
        "authors": "Pouria Tooranjipour, Bahare Kiumarsi",
        "published": "2021-12-14",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683057"
    },
    {
        "id": 18410,
        "title": "Off-policy evaluation for tabular reinforcement learning with synthetic trajectories",
        "authors": "Weiwei Wang, Yuqiang Li, Xianyi Wu",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11222-023-10351-y"
    },
    {
        "id": 18411,
        "title": "Quasi-Stochastic Approximation and Off-Policy Reinforcement Learning",
        "authors": "Andrey Bernstein, Yue Chen, Marcello Colombino, Emiliano Dall'Anese, Prashant Mehta, Sean Meyn",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029247"
    },
    {
        "id": 18412,
        "title": "An N-step Look Ahead Algorithm Using Mixed (On and Off) Policy Reinforcement Learning",
        "authors": "Vivek Kuchibhotla, P Harshitha, Shobhit Goyal",
        "published": "2020-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciss49785.2020.9315959"
    },
    {
        "id": 18413,
        "title": "High-Value Prioritized Experience Replay for Off-Policy Reinforcement Learning",
        "authors": "Xi Cao, Huaiyu Wan, Youfang Lin, Sheng Han",
        "published": "2019-11",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00215"
    },
    {
        "id": 18414,
        "title": "A MULTI-AGENT APPROACH TO POMDPS USING OFF-POLICY REINFORCEMENT LEARNING AND GENETIC ALGORITHMS",
        "authors": "Samuel Obadan, Zenghui Wang",
        "published": "2020-9-27",
        "citations": 5,
        "abstract": "This paper introduces novel concepts for accelerating learning in an off-policy reinforcement learning algorithm for Partially Observable Markov Decision Processes (POMDP) by leveraging multiple agents frame work. Reinforcement learning (RL) algorithm is considerably a slow but elegant approach to learning in an unknown environment. Although the action-value (Q-learning) is faster than the state-value, the rate of convergence to an optimal policy or maximum cumulative reward remains a constraint. Consequently, in an attempt to optimize the learning phase of an RL problem within POMD environment, we present two multi-agent learning paradigms: the multi-agent off-policy reinforcement learning and an ingenious GA (genetic Algorithm) approach for multi-agent offline learning using feedforward neural networks. At the end of the trainings (episodes and epochs) for reinforcement learning and genetic algorithm respectively, we compare the convergence rate for both algorithms with respect to creating the underlying MDPs for POMDP problems. Finally, we demonstrate the impact of layered resampling of Monte CarloвЂ™s particle filter for improving the belief state estimation accuracy with respect to ground truth within POMDP domains. Initial empirical results suggest practicable solutions.",
        "link": "http://dx.doi.org/10.47839/ijc.19.3.1887"
    },
    {
        "id": 18415,
        "title": "Reliable Off-Policy Evaluation for Reinforcement Learning",
        "authors": "Jie Wang, Rui Gao, Hongyuan Zha",
        "published": "2024-3",
        "citations": 0,
        "abstract": " Off-policy evaluation is an important topic in reinforcement learning, which estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. It is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. Here we leverage methodologies from (Wasserstein) distributionally robust optimization to provide robust and optimistic cumulative reward estimates. With proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. We also generalize those results to batch reinforcement learning. ",
        "link": "http://dx.doi.org/10.1287/opre.2022.2382"
    },
    {
        "id": 18416,
        "title": "Adaptive Optimal Control for Stochastic Multiplayer Differential Games Using On-Policy and Off-Policy Reinforcement Learning",
        "authors": "Mushuang Liu, Yan Wan, Frank L. Lewis, Victor G. Lopez",
        "published": "2020-12",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.2969215"
    },
    {
        "id": 18417,
        "title": "Optimal Control for Multi-agent Systems Using Off-Policy Reinforcement Learning",
        "authors": "Hao Wang, Zhiru Chen, Jun Wang, Lijun Lu, Mingzhe Li",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccr55715.2022.10053883"
    },
    {
        "id": 18418,
        "title": "Output-feedback Quadratic Tracking Control of Continuous-time Systems by Using Off-policy Reinforcement Learning with Neural Networks Observer",
        "authors": "Qingqing Meng, Yunjian Peng",
        "published": "2020-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc49329.2020.9164737"
    },
    {
        "id": 18419,
        "title": "Off-policy Learning over Heterogeneous Information for Recommendation",
        "authors": "Xiangmeng Wang, Qian Li, Dianer Yu, Guandong Xu",
        "published": "2022-4-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3485447.3512072"
    },
    {
        "id": 18420,
        "title": "Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning",
        "authors": "Yan Yin, Zhiyu Chen, Gang Liu, Jiasong Yin, Jianwei Guo",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123202"
    },
    {
        "id": 18421,
        "title": "Off-policy temporal difference learning with distribution adaptation in fast mixing chains",
        "authors": "Arash Givchi, Maziar Palhang",
        "published": "2018-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00500-017-2490-1"
    },
    {
        "id": 18422,
        "title": "Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods",
        "authors": "Boyao Li, Tao Lu, Jiayi Li, Ning Lu, Yinghao Cai, Shuo Wang",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio49542.2019.8961529"
    },
    {
        "id": 18423,
        "title": "Policy learning and policy failure: definitions, dimensions and intersections1",
        "authors": "Claire A. Dunlop",
        "published": "2020-1-15",
        "citations": 0,
        "abstract": "This chapter provides an overview of policy learning and policy failure, both of which are classic topics of policy studies. The links between the two literatures appear obvious, yet there are very few studies that address how one can learn from failure, learn to limit failure, and fail to learn. This book offers a rare attempt to bring these two literatures together. The chapter then begins by defining policy learning and failure before organising the main studies in these fields along the key dimensions of processes, products, and analytical levels. Learning and failure studies are beginning to offer analysis in and for the policy process that concentrates on the prescriptive techniques that can help on the ground. Intellectual endeavours on the design implications of learning and failure are still in their infancy, but two streams of activity are making headway. For learning, analysis of international organisations makes particularly strong offerings on how governments should learn. Different instruments and methods for cross-national learning include: benchmarking, peer review, checklists, facilitated coordination, and extrapolation. Meanwhile, the prescriptive turn in failure studies is less concerned with how not to fail and more focused on its inverse — how to succeed in policy making.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.003.0001"
    },
    {
        "id": 18424,
        "title": "Off-Policy Reinforcement-Learning Algorithm to Solve Minimax Games on Graphs",
        "authors": "Victor G. Lopez, Kyriakos G. Vamvoudakis, Yan Wan, Frank L. Lewis",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029772"
    },
    {
        "id": 18425,
        "title": "Unified Intrinsically Motivated Exploration for Off-Policy Learning in Continuous Action Spaces",
        "authors": "Baturay Saglam, Furkan B. Mutlu, Onat Dalmaz, Suleyman S. Kozat",
        "published": "2022-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu55565.2022.9864795"
    },
    {
        "id": 18426,
        "title": "Towards Off-Policy Learning for Ranking Policies with Logged Feedback",
        "authors": "Teng Xiao, Suhang Wang",
        "published": "2022-6-28",
        "citations": 3,
        "abstract": "Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20849"
    },
    {
        "id": 18427,
        "title": "Off-Policy Learning-to-Bid with AuctionGym",
        "authors": "Olivier Jeunen, Sean Murphy, Ben Allison",
        "published": "2023-8-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3580305.3599877"
    },
    {
        "id": 18428,
        "title": "Multi-Horizon Learning in Procedurally-Generated Environments for Off-Policy Reinforcement Learning (Student Abstract)",
        "authors": "Raja Farrukh Ali, Kevin Duong, Nasik Muhammad Nafi, William Hsu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Value estimates at multiple timescales can help create advanced discounting functions and allow agents to form more effective predictive models of their environment. In this work, we investigate learning over multiple horizons concurrently for off-policy reinforcement learning by using an advantage-based action selection method and introducing architectural improvements. Our proposed agent learns over multiple horizons simultaneously, while using either exponential or hyperbolic discounting functions. We implement our approach on Rainbow, a value-based off-policy algorithm, and test on Procgen, a collection of procedurally-generated environments, to demonstrate the effectiveness of this approach, specifically to evaluate the agent's performance in previously unseen scenarios.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26935"
    },
    {
        "id": 18429,
        "title": "An Improved Trust-Region Method for Off-Policy Deep Reinforcement Learning",
        "authors": "Hepeng Li, Xiangnan Zhong, Haibo He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191837"
    },
    {
        "id": 18430,
        "title": "Distributed Gradient Temporal Difference Off-policy Learning With Eligibility Traces: Weak Convergence",
        "authors": "Miloš S. Stanković, Marko Beko, Srdjan S. Stanković",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2020.12.2184"
    },
    {
        "id": 18431,
        "title": "A Selective Portfolio Management Algorithm with Off-Policy Reinforcement Learning Using Dirichlet Distribution",
        "authors": "Hyunjun Yang, Hyeonjun Park, Kyungjae Lee",
        "published": "2022-11-23",
        "citations": 0,
        "abstract": "Existing methods in portfolio management deterministically produce an optimal portfolio. However, according to modern portfolio theory, there exists a trade-off between a portfolio’s expected returns and risks. Therefore, the optimal portfolio does not exist definitively, but several exist, and using only one deterministic portfolio is disadvantageous for risk management. We proposed Dirichlet Distribution Trader (DDT), an algorithm that calculates multiple optimal portfolios by taking Dirichlet Distribution as a policy. The DDT algorithm makes several optimal portfolios according to risk levels. In addition, by obtaining the pi value from the distribution and applying importance sampling to off-policy learning, the sample is used efficiently. Furthermore, the architecture of our model is scalable because the feed-forward of information between portfolio stocks occurs independently. This means that even if untrained stocks are added to the portfolio, the optimal weight can be adjusted. We also conducted three experiments. In the scalability experiment, it was shown that the DDT extended model, which is trained with only three stocks, had little difference in performance from the DDT model that learned all the stocks in the portfolio. In an experiment comparing the off-policy algorithm and the on-policy algorithm, it was shown that the off-policy algorithm had good performance regardless of the stock price trend. In an experiment comparing investment results according to risk level, it was shown that a higher return or a better Sharpe ratio could be obtained through risk control.",
        "link": "http://dx.doi.org/10.3390/axioms11120664"
    },
    {
        "id": 18432,
        "title": "Model-Free Solution to the Discrete-Time Coupled Riccati Equation Using Off-Policy Reinforcement Learning",
        "authors": "Lu Li, Liming Wang, Yongliang Yang, Jie Dong, Yixin Yin, Shusen Cheng",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865951"
    },
    {
        "id": 18433,
        "title": "On the Reuse Bias in Off-Policy Reinforcement Learning",
        "authors": "Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Dong Yan, Jun Zhu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS --- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses, we present a novel yet simple Bias-Regularized Importance Sampling (BIRIS) framework along with practical algorithms, which can alleviate the negative impact of the Reuse Bias, and show that our BIRIS can significantly reduce the Reuse Bias empirically. Moreover, extensive experimental results show that our BIRIS-based methods can significantly improve the sample efficiency on a series of continuous control tasks in MuJoCo.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/502"
    },
    {
        "id": 18434,
        "title": "Off-Policy Deep Reinforcement Learning by Bootstrapping the Covariate Shift",
        "authors": "Carles Gelada, Marc G. Bellemare",
        "published": "2019-7-17",
        "citations": 6,
        "abstract": "In this paper we revisit the method of off-policy corrections for reinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this method, online updates to the value function are reweighted to avoid divergence issues typical of off-policy learning. While Hallak et al.’s solution is appealing, it cannot easily be transferred to nonlinear function approximation. First, it requires a projection step onto the probability simplex; second, even though the operator describing the expected behavior of the off-policy learning algorithm is convergent, it is not known to be a contraction mapping, and hence, may be more unstable in practice. We address these two issues by introducing a discount factor into COP-TD. We analyze the behavior of discounted COP-TD and find it better behaved from a theoretical perspective. We also propose an alternative soft normalization penalty that can be minimized online and obviates the need for an explicit projection step. We complement our analysis with an empirical evaluation of the two techniques in an off-policy setting on the game Pong from the Atari domain where we find discounted COP-TD to be better behaved in practice than the soft normalization penalty. Finally, we perform a more extensive evaluation of discounted COP-TD in 5 games of the Atari domain, where we find performance gains for our approach.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33013647"
    },
    {
        "id": 18435,
        "title": "Solving Combinatorial Problems through Off-Policy Reinforcement Learning Methods",
        "authors": "Muhammad Affan, Junaid Jawaid, Syed Umaid Ahmed, Ali Isfand yar Manek, Riaz Uddin",
        "published": "2020-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecce49384.2020.9179423"
    },
    {
        "id": 18436,
        "title": "A Continuous Off-Policy Reinforcement Learning Scheme for Optimal Motion Planning in Simply-Connected Workspaces",
        "authors": "Panagiotis Rousseas, Charalampos P. Bechlioulis, Kostas J. Kyriakopoulos",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161189"
    },
    {
        "id": 18437,
        "title": "Chaining Value Functions for Off-Policy Learning",
        "authors": "Simon Schmitt, John Shawe-Taylor, Hado   van Hasselt",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "To accumulate knowledge and improve its policy of behaviour, a reinforcement learning agent can learn `off-policy' about policies that differ from the policy used to generate its experience. This is important to learn counterfactuals, or because the experience was generated out of its own control. However, off-policy learning is non-trivial, and standard reinforcement-learning algorithms can be unstable and divergent.\n\nIn this paper we discuss a novel family of off-policy prediction algorithms which are convergent by construction. The idea is to first learn on-policy about the data-generating behaviour, and then bootstrap an off-policy value estimate on this on-policy estimate, thereby constructing a value estimate that is partially off-policy. This process can be repeated to build a chain of value functions, each time bootstrapping a new estimate on the previous estimate in the chain. Each step in the chain is stable and hence the complete algorithm is guaranteed to be stable. Under mild conditions this comes arbitrarily close to the off-policy TD solution when we increase the length of the chain. Hence it can compute the solution even in cases where off-policy TD diverges. \n\nWe prove that the proposed scheme is convergent and corresponds to an iterative decomposition of the inverse key matrix. Furthermore it can be interpreted as estimating a novel objective -- that we call a `k-step expedition' -- of following the target policy for finitely many steps before continuing indefinitely with the behaviour policy. Empirically we evaluate the idea on challenging MDPs such as Baird's counter example and observe favourable results.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20792"
    },
    {
        "id": 18438,
        "title": "An Approach for DC Motor Speed Control with Off-Policy Reinforcement Learning Method",
        "authors": "Sevilay TÜFENKÇİ, Gürkan KAVURAN, Celaleddin YEROĞLU",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "In the literature, interest in automatic control systems that do not require human intervention and perform at the desired level increases day by day. In this study, a Twin Delay Deep Deterministic Policy Gradient (TD3), a reinforcement learning algorithm, automatically controls a DC motor system. A reinforcement learning method is an approach that learns what should be done to reach the goal and observes the results that come out with the interaction of both itself and the environment. The proposed method aims to adjust the voltage value applied to the input of the DC motor in order to reach output with single input and single output structure to the desired speed.",
        "link": "http://dx.doi.org/10.17694/bajece.1114868"
    },
    {
        "id": 18439,
        "title": "Policy failures, policy learning and institutional change: the case of Australian health insurance policy change",
        "authors": "Adrian Kay",
        "published": "2020-1-15",
        "citations": 0,
        "abstract": "This chapter studies the connections between repeated assessments of policy failure, the catalysts of deinstitutionalisation, and subsequent opportunities for system-wide policy learning and reform. Selected evidence from the reform trajectory of Australian health insurance policy from the mid-1970s to late-1990s is used to explore these possible relationships. Here, failure delegitimised health policy institutions, making them increasingly vulnerable and giving them weak learning capacity to reform in anything but a suboptimal way. The result is a cycle of failure and dysfunctional learning. The Australian health insurance case allows one to catalogue at least one pattern of the relationships between policy failure, deinstitutionalisation, and learning. Three core analytical arguments underpin this pattern. First, policy failures create opportunities for learning at a system-wide level, only after institutions have been eroded and exhausted by repeated failure. Second, this first claim holds in both the expert and political inquiry dimensions of policy failure. Third, learning processes are related to the particular sequence of deinstitutionalisation processes; in particular, initial deinstitutionalisation in the expert domain creates the conditions for political learning processes.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.003.0006"
    },
    {
        "id": 18440,
        "title": "16. Passing off",
        "authors": "Abbe Brown, Smita Kheria, Jane Cornwell, Marta Iljadica",
        "published": "2023-8-14",
        "citations": 0,
        "abstract": "This chapter examines the action of passing off, that is, the means by which one trader may prevent another from misleading customers by misrepresenting (or ‘passing off’) goods or services as emanating from the former party. It analyses the key elements of goodwill, misrepresentation, and damage, as well as considering extended passing off by reference to multiple examples of groups of producers seeking to protect the goodwill associated with their products. It concludes with discussion of key issues regarding the future of passing off, in particular in relation to the internet and its possible development as a law against unfair competition.",
        "link": "http://dx.doi.org/10.1093/he/9780192855916.003.0016"
    },
    {
        "id": 18441,
        "title": "Policy myopia as a source of policy failure: adaptation and policy learning under deep uncertainty",
        "authors": "Sreeja Nair, Michael Howlett",
        "published": "2020-1-15",
        "citations": 0,
        "abstract": "This chapter introduces the idea of ‘policy myopia’ as a pressing source of failure in policy making and explores the possibility of developing policies that learn to help mitigate its impacts. It notes that while the problem of bounded rationality and short-term uncertainty is widely acknowledged as the central existential condition for all policy making, the long-term problem of an uncertain, and sometimes unknowable, future is rarely acknowledged. As uncertainty deepens, so too does the probability of policy failure. In these circumstances, actors need to design policies with flexibility and adaptation built in. Where policy solutions are robust over a range of possible scenarios and over time, one can say policies have learning capacity organised into them. Yet, in cases of radical uncertainty, learning may not be possible (or indeed preferable) at all. This reminder of the limits of learning is important. Updating one's belief systems assumes that a certain amount of knowledge exists in the first place.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.003.0007"
    },
    {
        "id": 18442,
        "title": "Learning With Options That Terminate Off-Policy",
        "authors": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann Nowé",
        "published": "2018-4-29",
        "citations": 3,
        "abstract": "\n      \n        A temporally abstract action, or an option, is specified by a policy and a termination condition: the policy guides the option behavior, and the termination condition roughly determines its length. Generally, learning with longer options (like learning with multi-step returns) is known to be more efficient. However, if the option set for the task is not ideal, and cannot express the primitive optimal policy well, shorter options offer more flexibility and can yield a better solution. Thus, the termination condition puts learning efficiency at odds with solution quality. We propose to resolve this dilemma by decoupling the behavior and target terminations, just like it is done with policies in off-policy learning. To this end, we give a new algorithm, Q(beta), that learns the solution with respect to any termination condition, regardless of how the options actually terminate. We derive Q(beta) by casting learning with options into a common framework with well-studied multi-step off policy learning. We validate our algorithm empirically, and show that it holds up to its motivating claims.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.11740"
    },
    {
        "id": 18443,
        "title": "Towards Robust Off-Policy Learning for Runtime Uncertainty",
        "authors": "Da Xu, Yuting Ye, Chuanwei Ruan, Bo Yang",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "Off-policy learning plays a pivotal role in optimizing and evaluating policies prior to the online deployment. However, during the real-time serving, we observe varieties of interventions and constraints that cause inconsistency between the online and offline setting, which we summarize and term as runtime uncertainty. Such uncertainty cannot be learned from the logged data due to its abnormality and rareness nature. To assert a certain level of robustness, we perturb the off-policy estimators along an adversarial direction in view of the runtime uncertainty. It allows the resulting estimators to be robust not only to observed but also unexpected runtime uncertainties. Leveraging this idea, we bring runtime-uncertainty robustness to three major off-policy learning methods: the inverse propensity score method, reward-model method, and doubly robust method. We theoretically justify the robustness of our methods to runtime uncertainty, and demonstrate their effectiveness using both the simulation and the real-world online experiments.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i9.21249"
    },
    {
        "id": 18444,
        "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
        "authors": "Bowen Zheng, Ran Cheng",
        "published": "2023-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3583131.3590512"
    },
    {
        "id": 18445,
        "title": "A Parallel Framework of Adaptive Dynamic Programming Algorithm With Off-Policy Learning",
        "authors": "Changyin Sun, Xiaofeng Li, Yuewen Sun",
        "published": "2021-8",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3015767"
    },
    {
        "id": 18446,
        "title": "Policy Learning and Policy Failure",
        "authors": "",
        "published": "2020-1-15",
        "citations": 0,
        "abstract": "First published as a special issue of Policy & Politics, this updated volume explores policy failures and the valuable opportunities for learning that they offer. The book begins with an overview of policy learning and policy failure. The links between the two appear obvious, yet there are very few studies that address how one can learn from failure, learn to limit failure, and fail to learn. The book attempts to bring the two together. In doing so, it explores how dysfunctional forms of policy learning impact policy failure at the meso-level. The book expands on this by demonstrating how different learning processes generated by actors at the meso-level mediate the extent to which policy transfer is a success or failure. It re-assesses some of the literature on policy transfer and policy diffusion, in light of ideas as to what constitutes failure, partial failure, or limited success. This is followed by an examination of situations in which the incentives of partisanship can encourage a government to actively seek to exacerbate an existing policy failure rather than to repair it. The book studies the connections between repeated assessments of policy failure and subsequent opportunities for system-wide policy learning and reform. Finally, it introduces the idea of ‘policy myopia’ as a pressing source of failure in policy making and explores the possibility of developing policies that learn to help mitigate its impacts.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.001.0001"
    },
    {
        "id": 18447,
        "title": "Off-Policy Integral Reinforcement Learning for Semi-Global Constrained Output Regulation of Continuous-Time Linear Systems",
        "authors": "Yongliang Yang, Xianzhong Chen, Yixin Yin, Donald C. Wunsch",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489343"
    },
    {
        "id": 18448,
        "title": "Lipschitzness is all you need to tame off-policy generative adversarial imitation learning",
        "authors": "Lionel Blondé, Pablo Strasser, Alexandros Kalousis",
        "published": "2022-4",
        "citations": 5,
        "abstract": "AbstractDespite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is asine qua noncondition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward shaping methods, which makes the base method it is plugged into provably more robust, as shown in several additional theoretical guarantees. We then discuss these through a fine-grained lens and share our insights. Crucially, the guarantees derived and reported in this work are valid foranyreward satisfying the Lipschitzness condition, nothing is specific to imitation. As such, these may be of independent interest.",
        "link": "http://dx.doi.org/10.1007/s10994-022-06144-5"
    },
    {
        "id": 18449,
        "title": "Overcoming the failure of ‘silicon somewheres’: learning in policy transfer processes",
        "authors": "Sarah Giest",
        "published": "2020-1-15",
        "citations": 0,
        "abstract": "This chapter discusses the impact of different types of learning on the success and failure of the transfer of the famous Silicon Valley Model (SVM) of innovation. Working with the idea of ‘adaptive learning’, it underlines the importance of understanding the learning process, and critically, the depth of learning that underpins policy transfer. Policy transfer is ‘a process in which knowledge about policies, administrative arrangements, institutions and ideas in one political setting (past or present) is used in the development of policies, administrative arrangements, institutions and ideas in another political setting’. Thus, knowledge exchange is highly dependent on the setting it occurs in as well as on the individuals involved in the process. There are different degrees of transfer: copying, emulation, combinations, and inspiration. These categories move from direct and complete transfer to searching for inspiration to create policy change. The chapter looks at four cases to demonstrate how different learning processes generated by actors at the meso-level, mainly networks of stakeholders and experts, mediate the extent to which policy transfer is a success or failure.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.003.0003"
    },
    {
        "id": 18450,
        "title": "On-policy and off-policy Q-learning strategies for spacecraft systems: An approach for time-varying discrete-time without controllability assumption of augmented system",
        "authors": "Hoang Nguyen, Hoang Bach Dang, Phuong Nam Dao",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ast.2024.108972"
    },
    {
        "id": 18451,
        "title": "Lessons on off-policy methods from a notification component of a chatbot",
        "authors": "Scott Rome, Tianwen Chen, Michael Kreisel, Ding Zhou",
        "published": "2021-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-021-05978-9"
    },
    {
        "id": 18452,
        "title": "The experimentation–accountability trade-off in innovation and industrial policy: are learning networks the solution?",
        "authors": "Slavo Radosevic, Despina Kanellou, George Tsekouras",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "Abstract\nThe exact nature of industrial/innovation (I/I) policy challenges and the best way to address them are unknown ex ante. This requires a degree of experimentation, which can be problematic in the context of an accountable public administration and leaves the question of how to reconcile the experimental nature of I/I policy with the need for public accountability, a crucial but unresolved issue. The trade-off between experimentation and accountability requires a governance model that will allow continuous feedback loops among the various stakeholders and ongoing evaluation of and adjustments to activities as programmes are implemented. We propose an ‘action learning’ approach, incorporating the governance mechanism of ‘learning networks’ to handle the problems of implementing experimental governance of new and untried I/I policies. We resolve the issue of accountability by drawing on the literature on network governance in public policy. By integrating control and learning dimensions of accountability, this approach enables us to resolve conceptually and empirically trade-offs between the need for experimentation and accountability in I/I policy.",
        "link": "http://dx.doi.org/10.1093/scipol/scad013"
    },
    {
        "id": 18453,
        "title": "Decentralized Zero-sum Games for Nonlinear Systems Based on Off-policy Learning Scheme",
        "authors": "Hao Luo, Chaoxu Mu, Lifu Yu, Ke Wang",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9550253"
    },
    {
        "id": 18454,
        "title": "Efficiently Breaking the Curse of Horizon in Off-Policy Evaluation with Double Reinforcement Learning",
        "authors": "Nathan Kallus, Masatoshi Uehara",
        "published": "2022-11",
        "citations": 12,
        "abstract": " Demystifying the Curse of Horizon in Offline Reinforcement Learning in Order to Break It  Offline reinforcement learning (RL), where we evaluate and learn new policies using existing off-policy data, is crucial in applications where experimentation is challenging and simulation unreliable, such as medicine. It is also notoriously difficult because the similarity (density ratio) between observed trajectories and those generated by any new policy diminishes exponentially as the horizon grows, known as the curse of horizon, which severely limits the application of offline RL whenever horizons are moderately long or even infinite. In “Efficiently Breaking the Curse of Horizon in Off-Policy Evaluation with Double Reinforcement Learning,” Kallus and Uehara set out to understand these limits and when they can be broken. They precisely characterize the curse by deriving the semiparametric efficiency lower bounds for the policy-value estimation problem in different models. On the one hand, this shows why the curse necessarily plagues standard estimators: they work even in non-Markov models and therefore must be limited by the corresponding bound. On the other hand, greater efficiency is possible in certain Markovian models, and they give the first estimator achieving these much lower efficiency bounds in infinite-horizon Markov decision processes. ",
        "link": "http://dx.doi.org/10.1287/opre.2021.2249"
    },
    {
        "id": 18455,
        "title": "Partial Off-policy Learning: Balance Accuracy and Diversity for Human-Oriented Image Captioning",
        "authors": "Jiahe Shi, Yali Li, Shengjin Wang",
        "published": "2021-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00219"
    },
    {
        "id": 18456,
        "title": "Fast Link Scheduling in Wireless Networks Using Regularized Off-Policy Reinforcement Learning",
        "authors": "Sagnik Bhattacharya, Ayan Banerjee, Subrahmanya Swamy Peruru, Kothapalli Venkata Srinivas",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lnet.2023.3264486"
    },
    {
        "id": 18457,
        "title": "Constrained Off-policy Learning over Heterogeneous Information for Fairness-aware Recommendation",
        "authors": "Xiangmeng Wang, Qian Li, Dianer Yu, Qing Li, Guandong Xu",
        "published": "2023-10-26",
        "citations": 1,
        "abstract": "\n            Fairness-aware recommendation eliminates discrimination issues to build trustworthy recommendation systems. Existing fairness-aware approaches ignore accounting for rich user and item attributes and thus cannot capture the impact of attributes on affecting recommendation fairness. These real-world attributes severely cause unfair recommendations by favoring items with popular attributes, leading to item exposure unfairness in recommendations. Moreover, existing approaches mostly mitigate unfairness for static recommendation models, e.g., collaborative filtering. Static models can not handle dynamic user interactions with the system that reflect users’ preferences shift through time. Thus, static models are limited in their ability to adapt to user behavior shifts to gain long-run user satisfaction. As user and item attributes are largely involved in modern recommenders and user interactions are naturally dynamic, it is essential to develop a novel method that eliminates unfairness caused by attributes meanwhile embrace the dynamic modeling of user behavior shifts. In this paper, we propose\n            Constrained Off-policy Learning over Heterogeneous Information for Fairness-aware Recommendation (Fair-HINpolicy)\n            , which uses recent advances in context-aware off-policy learning to produce fairness-aware recommendations with rich attributes from a Heterogeneous Information Network. In particular, we formulate the off-policy learning as a Constrained Markov Decision Process (CMDP) by dynamically constraining the fairness of item exposure at each iteration. We also design an attentive action sampling to reduce the search space for off-policy learning. Our solution adaptively receives HIN-augmented corrections for counterfactual risk minimization, and ultimately yields an effective policy that maximizes long-term user satisfaction. We extensively evaluate our method through simulations on large-scale real-world datasets, obtaining favorable results compared with state-of-the-art methods.\n          ",
        "link": "http://dx.doi.org/10.1145/3629172"
    },
    {
        "id": 18458,
        "title": "Pathologies of policy learning: what are they and how do they contribute to policy failure?1",
        "authors": "Claire A. Dunlop",
        "published": "2020-1-15",
        "citations": 1,
        "abstract": "This chapter explores how dysfunctional forms of policy learning impact policy failure at the meso-level. Using the long-running policy failure of the management of bovine tuberculosis (BTB) in England, analysis focuses on negative lessons generated by the interactions of an epistemic community of scientific experts and civil servants charged with balancing the competing interest actors to craft a workable policy. The chapter then outlines the capacity challenges faced by decision-makers engaged in epistemic learning and the ways in which advisory relationships can go wrong and learning can degenerate. These degenerations are understood as rooted in failures in government's organisational capacities. Empirically, the analysis of BTB policy in England finds that epistemic learning degenerated as a result of weaknesses in the government's analytical and communicative capacities. The chapter concludes with some reflections on the value of learning theories as a conceptual lens for policy failure.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447352006.003.0002"
    },
    {
        "id": 18459,
        "title": "An Off-Policy Trust Region Policy Optimization Method With Monotonic Improvement Guarantee for Deep Reinforcement Learning",
        "authors": "Wenjia Meng, Qian Zheng, Yue Shi, Gang Pan",
        "published": "2022-5",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3044196"
    },
    {
        "id": 18460,
        "title": "$H_{\\infty}$ Tracking Control of Unknown Discrete- Time Linear Systems via Output-Data-Driven Off-policy Q-learning Algorithm",
        "authors": "Kun Zhang, Xuantong Liu, Lei Zhang, Qian Chen, Yunjian Peng",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901898"
    },
    {
        "id": 18461,
        "title": "Off-policy Q-learning-based Output Feedback Fault-tolerant Tracking Control of Industrial Processes",
        "authors": "Linzhu Jia, Limin Wang, Ridong Zhang, Furong Gao",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/safeprocess58597.2023.10295876"
    },
    {
        "id": 18462,
        "title": "Towards Off-policy Evaluation as a Prerequisite for Real-world Reinforcement Learning in Building Control",
        "authors": "Bingqing Chen, Ming Jin, Zhe Wang, Tianzhen Hong, Mario Bergés",
        "published": "2020-11-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427871"
    },
    {
        "id": 18463,
        "title": "H∞ Control for Discrete-time Linear Systems by Integrating Off-policy Q-learning and Zero-sum Game",
        "authors": "Jinna Li, Zhengtao Ding, Chunyu Yang, Hong Niu",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icca.2018.8444362"
    },
    {
        "id": 18464,
        "title": "Off-Policy Q-Learning for Anti-Interference Control of Multi-Player Systems",
        "authors": "Jinna Li, Zhenfei Xiao, Tianyou Chai, Frank. L. Lewis, Sarangapani Jagannathan",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2020.12.2180"
    },
    {
        "id": 18465,
        "title": "Off-policy Q-learning-based Tracking Control for Stochastic Linear Discrete-Time Systems",
        "authors": "Xuantong Liu, Lei Zhang, Yunjian Peng",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccr55715.2022.10053863"
    },
    {
        "id": 18466,
        "title": "Robust hierarchical games of linear discrete-time systems based on off-policy model-free reinforcement learning",
        "authors": "Xiao Ma, Yuan Yuan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2024.106711"
    },
    {
        "id": 18467,
        "title": "Batch Reinforcement Learning With a Nonparametric Off-Policy Policy Gradient",
        "authors": "Samuele Tosatto, Joao Carvalho, Jan Peters",
        "published": "2022-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpami.2021.3088063"
    },
    {
        "id": 18468,
        "title": "Celebrating Robustness in Efficient Off-Policy Meta-Reinforcement Learning",
        "authors": "Ziyi Liu, Zongyuan Li, Qianqian Cao, Yuan Wan, Xian Guo",
        "published": "2022-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rcar54675.2022.9872291"
    },
    {
        "id": 18469,
        "title": "An incremental off-policy search in a model-free Markov decision process using a single sample path",
        "authors": "Ajin George Joseph, Shalabh Bhatnagar",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-018-5697-1"
    },
    {
        "id": 18470,
        "title": "Ad-load Balancing via Off-policy Learning in a Content Marketplace",
        "authors": "Hitesh Sagtani, Madan Gopal Jhawar, Rishabh Mehrotra, Olivier Jeunen",
        "published": "2024-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3616855.3635846"
    }
]