[
    {
        "id": 9001,
        "title": "Unveiling the Thematic Landscape of Generative Pre-trained Transformer (GPT) Through Bibliometric Analysis",
        "authors": "Carlos Alberto Gómez Cano, Verenice Sánchez Castillo, Tulio Andrés Clavijo Gallego",
        "published": "2023-4-2",
        "citations": 9,
        "abstract": "Introduction: the Generative Pre-trained Transformer (GPT) is a deep learning language model architecture developed by OpenAI.\nAim: to describe the knowledge networks (both at the theoretical and country levels) of the Generative Pre-trained Transformer (GPT) as an emerging technology.\nResults: 222 Documents were identified, of which 69 were articles, 50 were conference papers, 36 were editorials, 29 were notes, 19 were letters, 14 were reviews, 3 were conference reviews, and 2 were short surveys. In terms of the number of documents per year, 2 were found in 2019, 10 in 2020, 22 in 2021, 44 in 2022, and 144 in 2023. The year-on-year growth rate was over 100% in all years. The subject area with the highest number of documents was Computer Science with 90 documents. The most productive countries in relation to GPT were the United States with 60 documents, followed by China with 19, the United Kingdom with 18, India with 15, and Australia with 12. Co-occurrence illustrated the centrality of Artificial Intelligence, Natural Language Processing, Deep Learning, and the term Human around ChatGPT and GPT.\nConclusions: this bibliometric study aimed to describe the knowledge networks of the Generative Pre-trained Transformer (GPT) as an emerging technology. Although only 222 documents were found, this study revealed a high level of international scientific collaboration in the field. The results suggest that GPT is a highly relevant technology with a wide range of potential applications in natural language processing, artificial intelligence, and deep learning.\nMoreover, the study was able to qualitatively characterize the main thematic areas surrounding GPT, including its applications in chatbots, text generation, machine translation, sentiment analysis, and more.",
        "link": "http://dx.doi.org/10.56294/mr202333"
    },
    {
        "id": 9002,
        "title": "Schweizerdeutsche Telefonanrufe und Meetings mit GPT (Generative Pre-trained Transformer) automatisieren",
        "authors": "Mark Bosshard",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.38023/927d5adf-652b-4477-8fe9-4ef4bee420ff"
    },
    {
        "id": 9003,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Priyanshu Priya",
        "published": "2023-12-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ejpn8g"
    },
    {
        "id": 9004,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Hao Yu",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/16y6ju"
    },
    {
        "id": 9005,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Sunitha Sabbu",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/z8vi7n"
    },
    {
        "id": 9006,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Utkarsh Sharma",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/c87pxf"
    },
    {
        "id": 9007,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Abdul Jaleel",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/s142n1"
    },
    {
        "id": 9008,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Enrico Ferrari",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/03oa0c"
    },
    {
        "id": 9009,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Ismail Dergaa",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hnu7u9"
    },
    {
        "id": 9010,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Deepali Bajaj",
        "published": "2023-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jsq5fe"
    },
    {
        "id": 9011,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Everton Gomede",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/d2sppz"
    },
    {
        "id": 9012,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Otilia Manta",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/yvujxa"
    },
    {
        "id": 9013,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Yousef Sharrab",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/dznrwf"
    },
    {
        "id": 9014,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Rashmita Khilar",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/4dbqec"
    },
    {
        "id": 9015,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Vedanarayanan.v Venugopal",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/9ws8op"
    },
    {
        "id": 9016,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Gilad Shamir",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/687rre"
    },
    {
        "id": 9017,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Anatoliy V. Zavdoveev",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/kqjoq5"
    },
    {
        "id": 9018,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Joaquín Gayoso-Cabada",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hlv4qx"
    },
    {
        "id": 9019,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Rodrigo Pérez Fernández",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/pr6zic"
    },
    {
        "id": 9020,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Ashfia Jannat Keya",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fpmtsi"
    },
    {
        "id": 9021,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "G. N. Vivekananda",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/4ehkp5"
    },
    {
        "id": 9022,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Harisu Abdullahi Shehu",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ric72y"
    },
    {
        "id": 9023,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "豐緒 王",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/bidlef"
    },
    {
        "id": 9024,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Jac Ka Lok Leung",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/656efp"
    },
    {
        "id": 9025,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Juan Carlos Rincon Acuña",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jumydu"
    },
    {
        "id": 9026,
        "title": "Performance of generative pre-trained Transformer-4 (GPT-4) in RCOG diploma-style questions",
        "authors": "Richard Armitage",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/postmj/qgae038"
    },
    {
        "id": 9027,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Juan Carlos Rincon Acuña",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/es9m9a"
    },
    {
        "id": 9028,
        "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
        "authors": "Fahim Sufi",
        "published": "2024-2-8",
        "citations": 1,
        "abstract": "GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.",
        "link": "http://dx.doi.org/10.3390/info15020099"
    },
    {
        "id": 9029,
        "title": "The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models",
        "authors": "Deepu Kurian, Amin Alizadeh, Courtney M Peebles",
        "published": "No Date",
        "citations": 0,
        "abstract": "The impact of generative artificial intelligence (AI) on education and human development is currently unknown and may have substantial ethical implications in these fields. In this commentary, we discuss the nature of Generative Pre-trained Transformer (GPT) models that use deep learning to produce human-like text. This is an effort to understand how GPT models can support students, educators, and human development professionals to enhance learning while assisting in developing their professions. We conclude by outlining the need for policy decisions and ethical considerations on incorporating these technologies into educational and human development settings.\n",
        "link": "http://dx.doi.org/10.32388/mzqcd3"
    },
    {
        "id": 9030,
        "title": "Generative Pre-trained Transformer (GPT) Models for Irony Detection and Classification",
        "authors": "Mustafa Ulvi Aytekin, O. Ayhan Erdem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391005"
    },
    {
        "id": 9031,
        "title": "Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective",
        "authors": "ChatGPT Generative Pre-trained Transformer, Alex Zhavoronkov",
        "published": "2022-12-21",
        "citations": 101,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18632/oncoscience.571"
    },
    {
        "id": 9032,
        "title": "FLUID-GPT (Fast Learning to Understand and Investigate Dynamics with a Generative Pre-Trained Transformer): Efficient Predictions of Particle Trajectories and Erosion",
        "authors": "Steve Yang, Zulfikhar Ali, Bryan Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "The deleterious impact of erosion due to high-velocity particle impingement adversely affects a variety of engineering/industrial systems, resulting in irreversible mechanical wear of materials/components. Brute force computational fluid dynamics (CFD) calculations are commonly used to predict surface erosion by directly solving the Navier Stokes equations for the fluid and particle dynamics; however, these numerical approaches often require significant computational resources. In contrast, recent data-driven approaches using machine learning (ML) have shown immense promise for more efficient and accurate predictions to sidestep the computationally demanding CFD calculations. To this end, we have developed FLUID-GPT (Fast Learning to Understand and Investigate Dynamics with a Generative Pre-Trained Transformer), a new hybrid ML architecture for accurately predicting particle trajectories and erosion on an industrial-scale steam header geometry. Our FLUID-GPT approach utilizes a Generative Pre-Trained Transformer 2 (GPT-2) with a Convolutional Neural Network (CNN) for the first time to predict surface erosion using only information from five initial conditions: particle size, main-inlet speed, main-inlet pressure, sub-inlet speed, and sub-inlet pressure. Compared to the Long- and Short-Term Memory (LSTM) ML techniques used in previous work, our FLUID-GPT model is much more accurate (a 54% decrease in mean squared error) and efficient (70% less training time). Our work demonstrates that FLUID-GPT is an accurate and efficient ML approach for predicting time-series trajectories and their subsequent spatial erosion patterns in these complex dynamic systems.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-ppk9s"
    },
    {
        "id": 9033,
        "title": "The Role of Generative Pre-trained Transformers (GPT) in Recreational Tourism: An Interview with ChatGPT",
        "authors": "Okan ÇOLAK",
        "published": "2023-10-31",
        "citations": 3,
        "abstract": "This study explores the potential benefits of employing generative pre-trained transformers (GPTs) in recreational tourism, the difficulties that arise when implementing such technology, the impact it has on tourist behaviour, and the ways it can be utilised in recreational tourism management. The original aspect of the study is that it is the first to give detailed information about the use of GPT in recreational tourism. ChatGPT was used as an interviewer in the study. ChatGPT is a software application that utilizes the high-powered machine learning software called Generative Pre-trained Transformer (GPT-3), developed by the OpenAI organization. Six questions were posed on the ChatGPT query screen (https://chat.openai.com/chat). The interview queries were prepared with reference to the study by Fusté-Forné and Orea-Giner (2023). The question statements in this study on the use of GPT in gastronomy tourism were changed to recreational tourism. ChatGPT's replies were tabulated and presented descriptively. Inferences and suggestions were made in line with the answers given by ChatGPT. The study showed that using GPT technology in recreational-based tourism can offer better customer interaction, decision-making, and a personalized travel experience. ChatGPT underlined that new technologies continue to evolve, and recreational-based tourism will become more personalized, informative, and immersive, ultimately enhancing the overall travel experience, and contributing to the growth and sustainability of the tourism industry. ChatGPT also noted GPT models have the potential to shape and enhance the entire travel experience for tourists, from the initial planning stages to the actual trip itself. Although GPT makes significant contributions to recreational tourism management and recreational tourists, there are deficiencies in ethical, privacy, and authenticity concerns.",
        "link": "http://dx.doi.org/10.25307/jssr.1341967"
    },
    {
        "id": 9034,
        "title": "GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)",
        "authors": "Aivin V. Solatorio",
        "published": "2023-11-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3615894.3628499"
    },
    {
        "id": 9035,
        "title": "Information Retrieval Performance in Text Generation using Knowledge from Generative Pre-trained Transformer (GPT-3)",
        "authors": "Kaira Milani Fitria",
        "published": "2023-8-1",
        "citations": 1,
        "abstract": "The rise of advanced language models like GPT-3 and text generation has witnessed remarkable progress. However, leveraging the vast amount of knowledge within these models to enhance information retrieval performance remains an area that needs to be explored. This research used Artificial Intelligence, specifically the OpenAI GPT-3 language model, to create an application to help make written content. This research investigates the impact of incorporating GPT-3's knowledge into text generation processes and evaluates its influence on information retrieval tasks. Several features in text generation generate text that requires exact information, such as specifications for a product and accurate descriptions of a job or product, which are included in the concept of information retrieval in text creation by language models. The research used the few-shot learning method in the GPT-3 language model. The generated responses are then evaluated using established information retrieval metrics such as precision, recall, and F1-score. The findings of this research reveal the effectiveness of utilizing GPT-3's knowledge in enhancing information retrieval performance. The generated responses demonstrate improved relevance to user queries, resulting in the same performance precision and recall scores compared to other paid text generator websites. Application results are testing in capabilities of retrieving some information. Application capabilities tested on other commercial text generator engines. The test results obtained BERTscore 86\\% (precision), 88\\% (recall), and 87\\% (F1-Score). ",
        "link": "http://dx.doi.org/10.34312/jjom.v5i2.20574"
    },
    {
        "id": 9036,
        "title": "Generative Pre-trained Transformer (GPT) based model with relative attention for de novo drug design",
        "authors": "Suhail Haroon, Hafsath C.A., Jereesh A.S.",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiolchem.2023.107911"
    },
    {
        "id": 9037,
        "title": "MetaQA: Enhancing human-centered data search using Generative Pre-trained Transformer (GPT) language model and artificial intelligence",
        "authors": "Diya Li, Zhe Zhang",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "Accessing and utilizing geospatial data from various sources is essential for developing scientific research to address complex scientific and societal challenges that require interdisciplinary knowledge. The traditional keyword-based geosearch approach is insufficient due to the uncertainty inherent within spatial information and how it is presented in the data-sharing platform. For instance, the Gulf of Mexico Coastal Ocean Observing System (GCOOS) data search platform stores geoinformation and metadata in a complex tabular. Users can search for data by entering keywords or selecting data from a drop-down manual from the user interface. However, the search results provide limited information about the data product, where detailed descriptions, potential use, and relationship with other data products are still missing. Language models (LMs) have demonstrated great potential in tasks like question answering, sentiment analysis, text classification, and machine translation. However, they struggle when dealing with metadata represented in tabular format. To overcome these challenges, we developed Meta Question Answering System (MetaQA), a novel spatial data search model. MetaQA integrates end-to-end AI models with a generative pre-trained transformer (GPT) to enhance geosearch services. Using GCOOS metadata as a case study, we tested the effectiveness of MetaQA. The results revealed that MetaQA outperforms state-of-the-art question-answering models in handling tabular metadata, underlining its potential for user-inspired geosearch services.",
        "link": "http://dx.doi.org/10.1371/journal.pone.0293034"
    },
    {
        "id": 9038,
        "title": "Performance of Generative Pre-trained Transformer-4 (GPT-4) in Membership of the Royal College of General Practitioners (MRCGP)-style examination questions",
        "authors": "Richard C Armitage",
        "published": "2024-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/postmj/qgad128"
    },
    {
        "id": 9039,
        "title": "ANALISIS SENTIMEN PADA ULASAN APLIKASI CHAT  GENERATIVE PRE-TRAINED TRANSFORMER GPT MENGGUNAKAN METODE KLASIFIKASI K-NEAREST NEIGHBOR(KNN)",
        "authors": "Muhammad Nanda Fahriza, Noviana Riza",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "Klasifikasi K-Nearest Neighbor (KNN) akan dipakai dalam penelitian ini untuk melakukan analisis sentimen terhadap ulasan pengguna terhadap aplikasi Chat Generative Pre-Trained Transformer GPT. Model bahasa reguler GPT telah dipersiapkan secara luas dan dapat menghasilkan teks yang dapat dimengerti dan signifikan. Bagaimanapun, untuk memahami reaksi klien terhadap aplikasi, diperlukan pemeriksaan opini atas audit. Dalam penelitian ini, hasil analisis sentimen akan memberikan pemahaman yang lebih mendalam tentang respon pengguna terhadap aplikasi Chat Generative Pre-Trained Transformer GPT. Informasi ini dapat berguna bagi pengembang dan pemilik aplikasi untuk memahami kelebihan dan kekurangan aplikasi serta meningkatkan pengalaman pengguna secara keseluruhan. Penelitian ini ingin mengidentifikasi penelitian terkait sebelumnya, membandingkan pendekatan dan teknik yang digunakan, menganalisis efektivitas metode KNN, mengidentifikasi kelebihan dan kekurangan metode tersebut, serta memberikan rekomendasi untuk pengembangan lebih lanjut. Diharapkan hasil penelitian ini dapat memberikan pemahaman yang mendalam tentang respon pengguna terhadap aplikasi tersebut dan memberikan sumbangan bagi pengembangan dan peningkatan pengalaman pengguna.",
        "link": "http://dx.doi.org/10.36040/jati.v7i2.6767"
    },
    {
        "id": 9040,
        "title": "Generative Pre-Trained Transformers (GPT-3) Pertain To AI In The Law",
        "authors": "Lance Eliot",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3974887"
    },
    {
        "id": 9041,
        "title": "Manual Corpora Development for Generative Pre-trained Transformers (GPT) &amp;amp; Evaluation of GPT Model Learning Capability",
        "authors": "Prasad Nimantha Madusanka Ukwatta Hewage",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4391815"
    },
    {
        "id": 9042,
        "title": "Feel the Market: An Attempt to Identify Additional Factor in the Capital Asset Pricing Model (CAPM) Using Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT)",
        "authors": "Christopher Lingwei Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4521946"
    },
    {
        "id": 9043,
        "title": "Using Generative Pre-Trained Transformers (GPT) for Supervised Content Encoding: An Application in Corresponding Experiments",
        "authors": "Alexander Churchill, Shamitha Pichika, Chengxin Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Supervised content encoding applies a given codebook to a larger non-numerical dataset and is central to empirical research in public administration. Not only is it a key analytical approach for qualitative studies, but the method also allows researchers to measure constructs using non-numerical data, which can then be applied to quantitative description and causal inference. Despite its utility, supervised content encoding faces challenges including high cost and low reproducibility. In this report, we test if large language models (LLM), specifically generative pre-trained transformers (GPT), can solve these problems. Using email messages collected from a national corresponding experiment in the U.S. nursing home market as an example, we demonstrate that although we found some disparities between GPT and human coding results, the disagreement is acceptable for certain research design, which makes GPT encoding a potential substitute for human encoders. Practical suggestions for encoding with GPT are provided at the end of the letter.",
        "link": "http://dx.doi.org/10.31235/osf.io/6fpgj"
    },
    {
        "id": 9044,
        "title": "Using Generative Pre-Trained Transformers (GPT) for Electricity Price Trend Forecasting in the Spanish Market",
        "authors": "Alberto Menéndez Medina, José Antonio Heredia Álvaro",
        "published": "No Date",
        "citations": 0,
        "abstract": "Our research proposes two different training and modelling approaches of Generative Pre-trained Transformers (GPT) with specialized news feeds specific to the Spanish market: in-context example prompts and fine-tuned GPT models. Our findings indicate that integrating GPT insights into electricity price trend forecasting can result in more precise predictions and a deeper understanding of market dynamics. Through our research, we aim to provide insights to understand the capabilities of GPT solutions, and how those can be used to enhance prediction accuracy, ultimately supporting informed decision-making for stakeholders across the Spanish electricity market and companies whose margins heavily depend on electricity costs and price volatility.",
        "link": "http://dx.doi.org/10.20944/preprints202403.1197.v1"
    },
    {
        "id": 9045,
        "title": "Generative pre-trained transformers (GPT) for surface engineering",
        "authors": "Spyros Kamnis",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.surfcoat.2023.129680"
    },
    {
        "id": 9046,
        "title": "Bridging artificial intelligence in medicine with generative pre-trained transformer (GPT) technology",
        "authors": "Ethan Waisberg, Joshua Ong, Sharif Amit Kamran, Mouayad Masalkhi, Nasif Zaman, Prithul Sarker, Andrew G. Lee, Alireza Tavakkoli",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21037/jmai-23-36"
    },
    {
        "id": 9047,
        "title": "Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery",
        "authors": "Diane M. Korngiebel, Sean D. Mooney",
        "published": "2021-6-3",
        "citations": 114,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41746-021-00464-x"
    },
    {
        "id": 9048,
        "title": "Scope of homoeopathic research based on Chat (Generative Pre-trained Transformer) GPT: An Artificial Intelligence (AI) approach.",
        "authors": "Aditya Dilipkumar Patil, Monali Thopte, Sargam Singh",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51910/ijhdr.v23icf.1380"
    },
    {
        "id": 9049,
        "title": "How We Learned to Stop Worrying and Love AI: Analyzing the Rapid Evolution of Generative Pre-Trained Transformer (GPT) and its Impacts on Law, Business, and Society",
        "authors": "Scott J. Shackelford, Lawrence J. Trautman, W. Gregory Voss",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4516154"
    },
    {
        "id": 9050,
        "title": "GPT-LS: Generative Pre-Trained Transformer with Offline Reinforcement Learning for Logic Synthesis",
        "authors": "Chenyang Lv, Ziling Wei, Weikang Qian, Junjie Ye, Chang Feng, Zhezhi He",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccd58817.2023.00056"
    },
    {
        "id": 9051,
        "title": "Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions",
        "authors": "Deniz Nazarova",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47454-5_16"
    },
    {
        "id": 9052,
        "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
        "authors": "S. B. Atallah, N. R. Banda, A. Banda, N. A. Roeck",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10151-023-02837-8"
    },
    {
        "id": 9053,
        "title": "The GPT-Comparator: Discovering and Reporting Spatial and Topical Biases in Generative Pre-Trained Transformers",
        "authors": "Sriram Elango, Kokil Jaidka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4410154"
    },
    {
        "id": 9054,
        "title": "MSR92 Can Artificial Intelligence (AI) Large Language Models (LLMS) Such as Generative Pre-Trained Transformer (GPT) Be Used to Automate Literature Reviews?",
        "authors": "I. Guerra, J. Gallinaro, K. Rtveladze, A. Lambova, E. Asenova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jval.2023.09.2151"
    },
    {
        "id": 9055,
        "title": "Investigating antiquities trafficking with generative pre-trained transformer (GPT)-3 enabled knowledge graphs: A case study",
        "authors": "Shawn Graham, Donna Yates, Ahmed El-Roby",
        "published": "2023-6-20",
        "citations": 2,
        "abstract": "Background: There is a wide variety of potential sources from which insight into the antiquities trade could be culled, from newspaper articles to auction catalogues, to court dockets, to personal archives, if it could all be systematically examined. We explore the use of a large language model, GPT-3, to semi-automate the creation of a knowledge graph of a body of scholarship concerning the antiquities trade. Methods: We give GPT-3 a prompt guiding it to identify knowledge statements around the trade. Given GPT-3’s understanding of the statistical properties of language, our prompt teaches GPT-3 to append text to each article we feed it where the appended text summarizes the knowledge in the article. The summary is in the form of a list of subject, predicate, and object relationships, representing a knowledge graph. Previously we created such lists by manually annotating the source articles. We compare the result of this automatic process with a knowledge graph created from the same sources via hand. When such knowledge graphs are projected into a multi-dimensional embedding model using a neural network (via the Ampligraph open-source Python library), the relative positioning of entities implies the probability of a connection; the direction of the positioning implies the kind of connection. Thus, we can interrogate the embedding model to discover new probable relationships. The results can generate new insight about the antiquity trade, suggesting possible avenues of research. Results: We find that our semi-automatic approach to generating the knowledge graph in the first place produces comparable results to our hand-made version, but at an enormous savings of time and a possible expansion of the amount of materials we can consider. Conclusions: These results have implications for working with other kinds of archaeological knowledge in grey literature, reports, articles, and other venues via computational means.",
        "link": "http://dx.doi.org/10.12688/openreseurope.16003.1"
    },
    {
        "id": 9056,
        "title": "Perception of Chat Generative Pre-trained Transformer (Chat-GPT) AI tool amongst MSK clinicians",
        "authors": "Karthikeyan. P. Iyengar, Mina Malak Abed Yousef, Arvind Nune, Gaurav Kant Sharma, Rajesh Botchu",
        "published": "2023-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jcot.2023.102253"
    },
    {
        "id": 9057,
        "title": "Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening",
        "authors": "Ozan Unlu, Jiyeon Shin, Charlotte J Mailly, Michael F Oates, Michela R Tucci, Matthew Varugheese, Kavishwar Wagholikar, Fei Wang, Benjamin M Scirica, Alexander J Blood, Samuel J Aronson",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTBackgroundSubject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-intensive and error-prone task, demanding significant time and resources. With the advent of large language models (LLMs) and related technologies, a paradigm shift in natural language processing capabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This study aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained Transformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a clinical trial.MethodsThe Co-Operative Program for Implementation of Optimal Therapy in Heart Failure (COPILOT-HF) trial aims to recruit patients with symptomatic heart failure. As part of the screening process, a list of potentially eligible patients is created through an electronic health record (EHR) query. Currently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 exclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine patient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the structured assessments completed by the study staff and clinical notes for the past two years and developed a workflow of clinical note-based question answering system powered by RAG architecture and GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion Exclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a validation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of patients’ charts to answer the eligibility questions and determine the “gold standard” answers. We calculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each question and screening method. We also performed bootstrapping to calculate the confidence intervals for each statistic.ResultsBoth RECTIFIER and study staff answers closely aligned with the expert clinician answers across criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and 91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to determine the inclusion criteria of “symptomatic heart failure” with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility for the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), respectively.ConclusionGPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical trial screening. When incorporating new tools such as RECTIFIER, it is important to consider the potential hazards of automating the screening process and set up appropriate mitigation strategies such as final clinician review before patient engagement.",
        "link": "http://dx.doi.org/10.1101/2024.02.08.24302376"
    },
    {
        "id": 9058,
        "title": "GENERATIVE PRE-TRAINED TRANSFORMER 3",
        "authors": "Олександр Іванович ГОЛУБЕНКО, Олександр Олександрович ПІДМОГИЛЬНИЙ",
        "published": "2022-12-30",
        "citations": 0,
        "abstract": "GPT (Generative Pre-training Transformer) — це тип штучного інтелекту (AI), який використовує алгоритми машинного навчання для створення тексту природною мовою. Перша версія GPT, випущена в 2018 році, стала революційним досягненням у сфері ШІ та обробки природної мови (NLP). Однак він також мав деякі обмеження та проблеми, які були розглянуті в наступних версіях моделі.\r\nОднією з головних проблем першої версії GPT була відсутність контролю над контентом, який вона генерувала. Модель було навчено на великому наборі даних тексту, створеного людиною, і вона змогла створити зв’язний і, здавалося б, людиноподібний текст на широкий спектр тем. Однак він часто створював текст, який був упередженим, образливим або іншим чином недоречним, оскільки він не міг повністю зрозуміти контекст або значення використаних слів.\r\nІншою проблемою першої версії GPT була її нездатність виконувати складніші завдання NLP, такі як переклад або конспектування. Хоча він міг створити зв’язний текст, він не міг зрозуміти значення чи структуру тексту так, як це може зробити людина.\r\nПодальші версії GPT, такі як GPT-2 і GPT-3, вирішували ці проблеми та додавали нові можливості, такі як здатність виконувати складніші завдання NLP і генерувати більш зв’язний і відповідний контексту текст. Однак вони все ще мають обмеження і можуть давати необ’єктивні або невідповідні результати, якщо не використовувати їх відповідально.",
        "link": "http://dx.doi.org/10.53920/its-2022-2-2"
    },
    {
        "id": 9059,
        "title": "Evaluating the performance of Generative Pre-trained Transformer-4 (GPT-4) in standardizing radiology reports",
        "authors": "Amir M. Hasani, Shiva Singh, Aryan Zahergivar, Beth Ryan, Daniel Nethala, Gabriela Bravomontenegro, Neil Mendhiratta, Mark Ball, Faraz Farhadi, Ashkan Malayeri",
        "published": "2023-11-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00330-023-10384-x"
    },
    {
        "id": 9060,
        "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
        "authors": "Minhyeok Lee",
        "published": "2023-5-25",
        "citations": 7,
        "abstract": "In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the models’ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.",
        "link": "http://dx.doi.org/10.3390/math11112451"
    },
    {
        "id": 9061,
        "title": "Medical image Generative Pre-Trained Transformer (MI-GPT): future direction for precision medicine",
        "authors": "Xiaohui Zhang, Yan Zhong, Chentao Jin, Daoyan Hu, Mei Tian, Hong Zhang",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00259-023-06450-7"
    },
    {
        "id": 9062,
        "title": "Improve Information Service Capabilities from Content Aggregation to Knowledge Provision with Generative Pre-trained Transformer (GPT)",
        "authors": "Li Yu, Pei Bohao, Yu Qiang, Zhang Wei",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/snpd-winter57765.2023.10223888"
    },
    {
        "id": 9063,
        "title": "ChatGPT-A Generative Pre-Trained Transformer",
        "authors": " Manisha Rajesh Gupta",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "ChatGPT is an advanced conversational AI model developed by OpenAI. It is designed to engage in natural and coherent conversations with users, providing human-like responses to a wide range of topics and questions. It leverages deep learning technique. ChatGPT uses a combination of machine learning techniques, including deep learning and natural language processing, to understand and generate human-like text. The model has been trained on a vast amount of internet text data to ensure its ability to generate relevant and contextually accurate responses. ChatGPT has applications in customer service, virtual assistants, and other conversational interfaces, offering a powerful tool for natural language understanding and generation. It is most likely used to generate human like responses and makes the communication interactive.",
        "link": "http://dx.doi.org/10.48175/ijarsct-15087"
    },
    {
        "id": 9064,
        "title": "Plagiarism in the age of massive Generative Pre-trained Transformers (GPT-3)",
        "authors": "N Dehouche",
        "published": "2021-3-25",
        "citations": 89,
        "abstract": "As if 2020 was not a peculiar enough year, its fifth month saw the relatively quiet publication of a preprint describing the most powerful natural language processing (NLP) system to date—GPT-3 (Generative Pre-trained Transformer-3)—created by the Silicon Valley research firm OpenAI. Though the software implementation of GPT-3 is still in its initial beta release phase, and its full capabilities are still unknown as of the time of this writing, it has been shown that this artificial intelligence can comprehend prompts in natural language, on virtually any topic, and generate relevant original text content that is indistinguishable from human writing. Moreover, access to these capabilities, in a limited yet worrisome enough extent, is available to the general public. This paper presents examples of original content generated by the author using GPT-3. These examples illustrate some of the capabilities of GPT-3 in comprehending prompts in natural language and generating convincing content in response. I use these examples to raise specific fundamental questions pertaining to the intellectual property of this content and the potential use of GPT-3 to facilitate plagiarism. The goal is to instigate a sense of urgency, as well as a sense of present tardiness on the part of the academic community in addressing these questions.",
        "link": "http://dx.doi.org/10.3354/esep00195"
    },
    {
        "id": 9065,
        "title": "Implementation of Generative Pre-Trained Transformer 3 Classify-Text in Determining Thesis Supervisor",
        "authors": "Yoga Handoko Agustin",
        "published": "2022-10-3",
        "citations": 0,
        "abstract": "One of the requirements for graduating from the undergraduate level for universities in Indonesia is writing a final project or thesis. In order to graduate, of course, it is greatly influenced by the desire and strong spirit of the students and also the guidance of the supervisor. In determining the supervising lecturer, special attention must be paid to the field. Usually the selection of lecturers for thesis supervisors is determined by the study program through a meeting of lecturers in order to determine which lecturers are considered according to the title of the student and in accordance with the research of the supervisor. However, this method is a bit inconvenient and also quite time-consuming considering the number of students is more than a hundred and continues to grow every year. In this study, the thesis supervisor was classified based on the title proposed by the student. The methodology that will be used in this research is the Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology whose stages are: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and deployment, as well as using Generative Pre-Technology. trained Transformers 3 (GPT-3)",
        "link": "http://dx.doi.org/10.33395/sinkron.v7i4.11757"
    },
    {
        "id": 9066,
        "title": "<i>Chatbots Attempt Physics Homework—ChatGPT: Chat Generative Pre-Trained Transformer</i>",
        "authors": "Dan MacIsaac",
        "published": "2023-4-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1119/10.0017700"
    },
    {
        "id": 9067,
        "title": "Performance exploration of Generative Pre-trained Transformer-2 for lyrics generation",
        "authors": "Yijia Hu",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "In recent years, the field of Natural Language Processing (NLP) has undergone a revolution, with text generation playing a key role in this transformation. This shift is not limited to technological areas but has also seamlessly penetrated creative domains, with a prime example being the generation of song lyrics. To be truly effective, generative models, like Generative Pre-trained Transformer (GPT)-2, require fine-tuning as a crucial step. This paper, utilizing the robustness of the widely-referenced Kaggle dataset titled \"Song Lyrics\", carefully explores the impacts of modulating three key parameters: learning rate, batch size, and sequence length. The dataset presents a compelling narrative that highlights the learning rate as the most influential determinant, directly impacting the quality and coherence of the lyrics generated. While increasing the batch size and extending sequence lengths promise enhanced model performance, it is evident that there is a saturation point beyond which further benefits are limited. Through this exploration, the paper aims to demystify the complex world of model calibration and emphasize the importance of strategic parameter selection in pursuit of lyrical excellence.",
        "link": "http://dx.doi.org/10.54254/2755-2721/48/20241154"
    },
    {
        "id": 9068,
        "title": "Medical text prediction and suggestion using generative pre-trained transformer models with dental medical notes",
        "authors": "Joseph Sirriani, Emre Sezgin, Daniel Claman, Simon L Linwood",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractBackgroundGenerative pre-trained transformer (GPT) models are one of the latest large pre-trained natural language processing (NLP) models, which enables model training with limited datasets, and reduces dependency on large datasets which are scarce and costly to establish and maintain. There is a rising interest to explore the use of GPT models in healthcare.ObjectiveWe investigate the performance of GPT-2 and GPT-Neo models for medical text prediction using 374,787 free-text dental notes.MethodsWe fine-tune pre-trained GPT-2 and GPT-Neo models for next word prediction on a dataset of over 374,000 manually written sections of dental clinical notes. Each model was trained on 80% of the dataset, validated on 10%, and tested on the remaining 10%. We report model performance in terms of next word prediction accuracy and loss. Additionally, we analyze the performance of the models on different types of prediction tokens for categories. We annotate each token in 100 randomly sampled notes by category (e.g. Names, Abbreviations, Clinical Terms, Punctuation, etc.) and compare the performance of each model by token category.ResultsModels present acceptable accuracy scores (GPT-2: 76%, GPT-Neo: 53%), and the GPT-2 model also performs better in manual evaluations, especially for names, abbreviations, and punctuation. The results suggest that pre-trained models have the potential to assist medical charting in the future. We share the lessons learned, insights, and suggestions for future implementations.ConclusionThe results suggest that pre-trained models have the potential to assist medical charting in the future. Our study presented one of the first implementations of the GPT model used with medical notes.",
        "link": "http://dx.doi.org/10.1101/2022.04.29.22274513"
    },
    {
        "id": 9069,
        "title": "FLUID-GPT (Fast Learning to Understand and Investigate Dynamics with a Generative Pre-Trained Transformer): Efficient Predictions of Particle Trajectories and Erosion",
        "authors": "Steve D. Yang, Zulfikhar A. Ali, Bryan M. Wong",
        "published": "2023-9-20",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/acs.iecr.3c01639"
    },
    {
        "id": 9070,
        "title": "The artificial intelligence pharma era after “Chat Generative Pre-trained Transformer”",
        "authors": "Zhengwei Xie, Gangqing Hu",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "Abstract\nThe era of advanced artificial intelligence has arrived with the development of chatbots like ChatGPT (Chat Generative Pre-trained Transformer). As described by Ouyang et al. (2022), ChatGPT demonstrates an impressive ability to generate human-like responses and solve practical problems, surpassing original expectations for its capabilities. The rapid release and adoption of ChatGPT signals a new phase in AI development, powered by large language models that can be fine-tuned through human feedback. However, risks remain regarding how such powerful models may be misused. Further research is needed to ensure safe and ethical deployment of these transformative technologies.",
        "link": "http://dx.doi.org/10.1515/mr-2023-0023"
    },
    {
        "id": 9071,
        "title": "Generative Pre-Trained Transformer for Cardiac Abnormality Detection",
        "authors": "Pierre Louis Gaudilliere, Halla Sigurthorsdottir, Clementine Aguet, Jerome Van Zaen, Mathieu Lemay, Ricard Delgado-Gonzalo",
        "published": "2021-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/cinc53138.2021.9662835"
    },
    {
        "id": 9072,
        "title": "Enhancing clinical reasoning with Chat Generative Pre-trained Transformer: a practical guide",
        "authors": "Takanobu Hirosawa, Taro Shimizu",
        "published": "2024-2-19",
        "citations": 2,
        "abstract": "Abstract\n\nObjectives\nThis study aimed to elucidate effective methodologies for utilizing the generative artificial intelligence (AI) system, namely the Chat Generative Pre-trained Transformer (ChatGPT), in improving clinical reasoning abilities among clinicians.\n\n\nMethods\nWe conducted a comprehensive exploration of the capabilities of ChatGPT, emphasizing two main areas: (1) efficient utilization of ChatGPT, with a focus on application and language selection, input methodology, and output verification; and (2) specific strategies to bolster clinical reasoning using ChatGPT, including self-learning via simulated clinical case creation and engagement with published case reports.\n\n\nResults\nEffective AI-based clinical reasoning development requires a clear delineation of both system roles and user needs. All outputs from the system necessitate rigorous verification against credible medical resources. When used in self-learning scenarios, capabilities of ChatGPT in clinical case creation notably enhanced disease comprehension.\n\n\nConclusions\nThe efficient use of generative AIs, as exemplified by ChatGPT, can impressively enhance clinical reasoning among medical professionals. Adopting these cutting-edge tools promises a bright future for continuous advancements in clinicians’ diagnostic skills, heralding a transformative era in digital healthcare.\n",
        "link": "http://dx.doi.org/10.1515/dx-2023-0116"
    },
    {
        "id": 9073,
        "title": "Chatbot Generative Pre-trained Transformer and artificial intelligence in sports physical therapy and rehabilitation",
        "authors": "Mohammad Ahsan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4103/sjsm.sjsm_16_23"
    },
    {
        "id": 9074,
        "title": "Generative Pre‐Trained Transformer 4 in healthcare: Challenges, opportunities, and recommendations",
        "authors": "Hassam Ali",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/med4.21"
    },
    {
        "id": 9075,
        "title": "The Impact of Chat Generative Pre-trained Transformer (ChatGPT) on Oncology: Application, Expectations, and Future Prospects",
        "authors": "Yanxing Li, Wentao Gao, Zhenhua Luan, Zhi Zhou, Jianjun Li",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7759/cureus.48670"
    },
    {
        "id": 9076,
        "title": "Generative Pre-Trained Transformer for Design Concept Generation: An Exploration",
        "authors": "Q. Zhu, J. Luo",
        "published": "2022-5",
        "citations": 17,
        "abstract": "AbstractNovel concepts are essential for design innovation and can be generated with the aid of data stimuli and computers. However, current generative design algorithms focus on diagrammatic or spatial concepts that are either too abstract to understand or too detailed for early phase design exploration. This paper explores the uses of generative pre-trained transformers (GPT) for natural language design concept generation. Our experiments involve the use of GPT-2 and GPT-3 for different creative reasonings in design tasks. Both show reasonably good performance for verbal design concept generation.",
        "link": "http://dx.doi.org/10.1017/pds.2022.185"
    },
    {
        "id": 9077,
        "title": "Artificial intelligence tools in medical education beyond Chat Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Li Feng Tan, Isaac K S Ng, Desmond Teo",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/postmj/qgae014"
    },
    {
        "id": 9078,
        "title": "A Case Report on Ground-Level Alternobaric Vertigo Due to Eustachian Tube Dysfunction With the Assistance of Conversational Generative Pre-trained Transformer (ChatGPT)",
        "authors": "Hee-Young Kim",
        "published": "2023-3-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7759/cureus.36830"
    },
    {
        "id": 9079,
        "title": "Foot and Ankle Surgery declares use of generative artificial intelligence like Chat Generative Pre-trained Transformer (ChatGPT) for scientific publications",
        "authors": "Martinus Richter",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.fas.2023.05.002"
    },
    {
        "id": 9080,
        "title": "The application of Chat Generative Pre-trained Transformer in nursing education",
        "authors": "Jialin Liu, Fan Liu, Jinbo Fang, Siru Liu",
        "published": "2023-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.outlook.2023.102064"
    },
    {
        "id": 9081,
        "title": "cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific De Novo Molecular Generation",
        "authors": "Ye Wang, Honggang Zhao, Simone Sciabola, Wenlu Wang",
        "published": "2023-5-30",
        "citations": 9,
        "abstract": "Deep generative models applied to the generation of novel compounds in small-molecule drug design have attracted a lot of attention in recent years. To design compounds that interact with specific target proteins, we propose a Generative Pre-Trained Transformer (GPT)-inspired model for de novo target-specific molecular design. By implementing different keys and values for the multi-head attention conditional on a specified target, the proposed method can generate drug-like compounds both with and without a specific target. The results show that our approach (cMolGPT) is capable of generating SMILES strings that correspond to both drug-like and active compounds. Moreover, the compounds generated from the conditional model closely match the chemical space of real target-specific molecules and cover a significant portion of novel compounds. Thus, the proposed Conditional Generative Pre-Trained Transformer (cMolGPT) is a valuable tool for de novo molecule design and has the potential to accelerate the molecular optimization cycle time.",
        "link": "http://dx.doi.org/10.3390/molecules28114430"
    },
    {
        "id": 9082,
        "title": "Performance of a commercially available Generative Pre-trained Transformer (GPT) in describing radiolucent lesions in panoramic radiographs and establishing differential diagnoses",
        "authors": "Thaísa Pinheiro Silva, Maria Fernanda Silva Andrade-Bortoletto, Thaís Santos Cerqueira Ocampo, Caio Alencar-Palha, Michael M. Bornstein, Christiano Oliveira-Santos, Matheus L. Oliveira",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "Abstract\nObjectives\nTo evaluate the performance of a commercially available Generative Pre-trained Transformer (GPT) in describing and establishing differential diagnoses for radiolucent lesions in panoramic radiographs.\n\nMaterials and methods\nTwenty-eight panoramic radiographs, each containing a single radiolucent lesion, were evaluated in consensus by three examiners and a commercially available ChatGPT-3.5 model. They provided descriptions regarding internal structure (radiodensity, loculation), periphery (margin type, cortication), shape, location (bone, side, region, teeth/structures), and effects on adjacent structures (effect, adjacent structure). Diagnostic impressions related to origin, behavior, and nature were also provided. The GPT program was additionally prompted to provide differential diagnoses. Keywords used by the GPT program were compared to those used by the examiners and scored as 0 (incorrect), 0.5 (partially correct), or 1 (correct). Mean score values and standard deviation were calculated for each description. Performance in establishing differential diagnoses was assessed using Rank-1, -2, and − 3.\n\nResults\nDescriptions of margination, affected bone, and origin received the highest scores: 0.93, 0.93, and 0.87, respectively. Shape, region, teeth/structures, effect, affected region, and nature received considerably lower scores ranging from 0.22 to 0.50. Rank-1, -2, and − 3 demonstrated accuracy in 25%, 57.14%, and 67.85% of cases, respectively.\n\nConclusion\nThe performance of the GPT program in describing and providing differential diagnoses for radiolucent lesions in panoramic radiographs is variable and at this stage limited in its use for clinical application.\n\nClinical relevance\nUnderstanding the potential role of GPT systems as an auxiliary tool in image interpretation is imperative to validate their clinical applicability.\n",
        "link": "http://dx.doi.org/10.1007/s00784-024-05587-5"
    },
    {
        "id": 9083,
        "title": "Generative pre-trained transformers (GPT)-based automated data mining for building energy management: Advantages, limitations and the future",
        "authors": "Chaobo Zhang, Jie Lu, Yang Zhao",
        "published": "2024-2",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbenv.2023.06.005"
    },
    {
        "id": 9084,
        "title": "The Expanding Role of ChatGPT (Chat-Generative Pre-Trained Transformer) in Neurosurgery: A Systematic Review of Literature and Conceptual Framework",
        "authors": "Alex Roman, Lubna Al-Sharif, Mohamed AL Gharyani",
        "published": "2023-8-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7759/cureus.43502"
    },
    {
        "id": 9085,
        "title": "AB-Gen: Antibody Library Design with Generative Pre-trained Transformer and Deep Reinforcement Learning",
        "authors": "Xiaopeng Xu, Tiantian Xu, Juexiao Zhou, Xingyu Liao, Ruochi Zhang, Yu Wang, Lu Zhang, Xin Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAntibody leads must fulfill multiple desirable properties to be clinical candidates. Primarily due to the low throughput in the experimental procedure, the need for such multi-property optimization causes the bottleneck in preclinical antibody discovery and development, because addressing one issue usually causes another. We developed a reinforcement learning (RL) method, named AB-Gen, for antibody library design using a generative pre-trained Transformer (GPT) as the policy network of the RL agent. We showed that this model can learn the antibody space of heavy chain complementarity determining region 3 (CDRH3) and generate sequences with similar property distributions. Besides, when using HER2 as the target, the agent model of AB-Gen was able to generate novel CDRH3 sequences that fulfill multi-property constraints. 509 generated sequences were able to pass all property filters and three highly conserved residues were identified. The importance of these residues was further demonstrated by molecular dynamics simulations, which consolidated that the agent model was capable of grasping important information in this complex optimization task. Overall, the AB-Gen method is able to design novel antibody sequences with an improved success rate than the traditional propose-then-filter approach. It has the potential to be used in practical antibody design, thus empowering the antibody discovery and development process.",
        "link": "http://dx.doi.org/10.1101/2023.03.17.533102"
    },
    {
        "id": 9086,
        "title": "Pre-Trained Multi-Modal Transformer for Pet Emotion Detection",
        "authors": "Run Guo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011961500003612"
    },
    {
        "id": 9087,
        "title": "Generative Pre-Trained Transformer for Kazakh Text Generation Tasks",
        "authors": "Gulmira Tolegen, Alymzhan Toleu, Rustam Mussabayev, Bagashar Zhumazhanov, Gulzat Ziyatbekova",
        "published": "2023-8-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/opcs59592.2023.10275765"
    },
    {
        "id": 9088,
        "title": "Analisis Sentimen AicoGPT (Generative Pre-trained Transformer) Menggunakan TF-IDF",
        "authors": " Sri Rahayu,  Jajang Jaya Purnama,  Abdul Hamid,  Nina Kurnia Hikmawati",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "Peran artificial intelligence memudahkan mencari informasi yang tepat dan akurat bahkan penyelesaian masalah dengan model yang kompleks. Salah satu terobosan berbasis AI adalah ChatGPT oleh OpenAI pada tahun 2020, dilanjutkan dengan versi terbaru pada tahun 2023 yaitu GPT–3. Sejak saat itu, beberapa teknologi AI serupa versi mobile mulai bermunculan, salah satunya AicoGPT. Namun, kinerja dari aplikasi serupa ini belum dapat diandalkan sehingga masih perlu menganalisis tanggapan para penggunanya, apakah akan sama menakjubkannya atau tidak. Dari permasalahan tersebut, penelitian ini dibuat dengan tujuan untuk menganalisis 1443 data ulasan para pengguna aplikasi AicoGPT di Google Playstore dengan teknik analisis sentimen menggunakan TFIDF dan perbandingan klasifikasi LR dan SVM. Dari kedua ujicoba tersebut, menghasilkan akurasi terbaik dengan Algoritma SVM, yaitu sebesar 92%. Sedangkan LR menghasilkan akurasi sebesar 89%. Dari penelitian ini, dapat disimpulkan secara singkat bahwa metode TF-IDF dengan klasifikasi SVM, cocok digunakan untuk melakukan analisis sentimen dari dataset yang diteliti. ",
        "link": "http://dx.doi.org/10.24002/jbi.v14i02.7039"
    },
    {
        "id": 9089,
        "title": "PixieGPT: Design and Implementation of a Generative Pre-Trained Transformer for Universities of Bangladesh",
        "authors": "Hasan Mahmood Aminul Islam, Mehedi Hasan, Sumiaya Ahmed, Ariful Islam Fardin, Mehedi Hasan Nabil",
        "published": "No Date",
        "citations": 0,
        "abstract": "In a densely populated country like Bangladesh, universities grapple with the challenge of efficiently addressing myriad queries from a large student body, leading to a heightened workload for university stakeholders. To tackle these challenges, we introduce PixieGPT, a tailor-made Generative Pre-Trained Transformer for Bangladeshi universities. PixieGPT significantly mitigates workload by adeptly handling common university-related queries, thereby enhancing user experience. The hierarchical structure plays a crucial role in managing diverse queries from thousands of students about the university system. The solution introduces a modular hierarchical knowledge base (KB) with simpler complexities, addressing the intricacies of efficiently managing large volumes of queries. PixieGPT is designed in a modular way so that the solution is also adaptable for the implementation of other universities worldwide based on the requirements of a particular administrative system. The modular nature facilitates easy adaptation with minor changes based on specific university requirements, ensuring a seamless integration process. This paper delves into the intricacies of PixieGPT&#039;s design, emphasizing its pivotal role in mitigating workload challenges for university stakeholders in Bangladesh. The incorporation of BERT for Natural Language Understanding(NLU) and GPT models for Natural Language Generation(NLG) enhances PixieGPT&#039;s capabilities, contributing to the scalability and efficiency of the system. The presented use case underscores the practical benefits of PixieGPT, positioning it as a promising solution for universities globally with similar operational frameworks.",
        "link": "http://dx.doi.org/10.20944/preprints202402.1083.v1"
    },
    {
        "id": 9090,
        "title": "Transliteration based Generative Pre-trained Transformer 2 Model for Tamil Text Summarization",
        "authors": "C.R. Dhivyaa, K. Nithya, T. Janani, K. Sathis Kumar, N. Prashanth",
        "published": "2022-1-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci54379.2022.9740991"
    },
    {
        "id": 9091,
        "title": "AEJMC Advertising Division 2023 Teaching Pre-Conference Review: Innovating Data Storytelling and Visualization With Artificial Intelligence and Chat Generative Pre-trained Transformer",
        "authors": "Robin Spring, Shanshan Lou",
        "published": "2024-3-4",
        "citations": 0,
        "abstract": " The 26th annual Teaching Pre-Conference organized by the Advertising Division of the Association for Education in Journalism and Mass Communication focused on the topic of innovating data storytelling and visualization with AI and ChatGPT. Five prominent speakers from leading media companies and universities shared insights with advertising educators, covering the application, impact, and challenges of generative artificial intelligence (AI) in the advertising industry. The five panels also delved into effective ways of integrating generative AI tools into the classroom. Three key trends that arise from the panel presentations are discussed below. Relevant advertising AI tools and class activities are also shared in the report. ",
        "link": "http://dx.doi.org/10.1177/10980482241236883"
    },
    {
        "id": 9092,
        "title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?",
        "authors": "Jaromir Savelka, Arav Agarwal, Christopher Bogart, Yifan Song, Majd Sakr",
        "published": "2023-6-29",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587102.3588792"
    },
    {
        "id": 9093,
        "title": "Automated data mining framework for building energy conservation aided by generative pre-trained transformers (GPT)",
        "authors": "Chaobo Zhang, Jian Zhang, Yang Zhao, Jie Lu",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2023.113877"
    },
    {
        "id": 9094,
        "title": "Role of Artificial Intelligence based Chat Generative Pre-trained Transformer (ChatGPT) in Cyber Security",
        "authors": "S. Guru Prasad, V. Ceronmani Sharmila, M.K. Badrinarayanan",
        "published": "2023-5-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaaic56838.2023.10141395"
    },
    {
        "id": 9095,
        "title": "Patient Education Materials Generated by Chat Generative Pre-trained Transformer Versus Experts",
        "authors": "Hinpetch Daungsupawong, Viroj Wiwanitkit",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1097/sap.0000000000003783"
    },
    {
        "id": 9096,
        "title": "Chat Generative Pre-Trained Transformer (ChatGPT) usage in healthcare",
        "authors": "Yanhui Zhang, Haolong Pei, Shihan Zhen, Qian Li, Fengchao Liang",
        "published": "2023-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.gande.2023.07.002"
    },
    {
        "id": 9097,
        "title": "Writing for Pediatric Critical Care Medicine: Engaging With Citations to References in the Chatbot Generative Pre-Trained Transformer Era",
        "authors": "Robert C. Tasker",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1097/pcc.0000000000003356"
    },
    {
        "id": 9098,
        "title": "Nursing Education in the Age of Chat Generative Pre-Trained Transformer: Current Roles and Future Perspective",
        "authors": "Bhavna Rani, Saumya Prakash Srivastava, Shafali Thakur",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "Abstract\nChat Generative Pre-trained Transformer (ChatGPT), an artificial intelligence-powered chatbot, has received a lot of interest from the academic community since its inception. Health-care sector and higher education has significantly advanced with the use of AI technologies. With the advent of AI technologies, such as ChatGPT, the future of nursing education is poised for significant transformation. In this article, we will explore the potential impact of ChatGPT on nursing education, discussing its benefits, challenges, and implications.",
        "link": "http://dx.doi.org/10.4103/amhs.amhs_208_23"
    },
    {
        "id": 9099,
        "title": "To chat or not to chat, Generative Pre-trained Transformer?",
        "authors": "PrakashK Dubey",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4103/jigims.jigims_32_23"
    },
    {
        "id": 9100,
        "title": "Universal skepticism of ChatGPT: a review of early literature on chat generative pre-trained transformer",
        "authors": "Casey Watters, Michal K. Lemanski",
        "published": "2023-8-23",
        "citations": 3,
        "abstract": "ChatGPT, a new language model developed by OpenAI, has garnered significant attention in various fields since its release. This literature review provides an overview of early ChatGPT literature across multiple disciplines, exploring its applications, limitations, and ethical considerations. The review encompasses Scopus-indexed publications from November 2022 to April 2023 and includes 156 articles related to ChatGPT. The findings reveal a predominance of negative sentiment across disciplines, though subject-specific attitudes must be considered. The review highlights the implications of ChatGPT in many fields including healthcare, raising concerns about employment opportunities and ethical considerations. While ChatGPT holds promise for improved communication, further research is needed to address its capabilities and limitations. This literature review provides insights into early research on ChatGPT, informing future investigations and practical applications of chatbot technology, as well as development and usage of generative AI.",
        "link": "http://dx.doi.org/10.3389/fdata.2023.1224976"
    }
]