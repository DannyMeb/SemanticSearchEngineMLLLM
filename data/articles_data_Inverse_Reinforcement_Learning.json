[
    {
        "id": 18271,
        "title": "Inverse Reinforcement Learning for Healthcare Applications: A  Survey",
        "authors": "Mohamed-Amine Chadi, Hajar Mousannif",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010729200003101"
    },
    {
        "id": 18272,
        "title": "Estimation of Reward Function Maximizing Learning Efficiency in Inverse Reinforcement Learning",
        "authors": "Yuki Kitazato, Sachiyo Arai",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006729502760283"
    },
    {
        "id": 18273,
        "title": "Inverse-Inverse Reinforcement Learning. How to Hide Strategy from an Adversarial Inverse Reinforcement Learner",
        "authors": "Kunal Pattanayak, Vikram Krishnamurthy, Christopher Berry",
        "published": "2022-12-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992959"
    },
    {
        "id": 18274,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 18275,
        "title": "Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning",
        "authors": "Yuansheng Xie, Soroush Vosoughi, Saeed Hassanpour",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956245"
    },
    {
        "id": 18276,
        "title": "Contextual Action with Multiple Policies Inverse Reinforcement Learning for Behavior Simulation",
        "authors": "Nahum Alvarez, Itsuki Noda",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007684908870894"
    },
    {
        "id": 18277,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 18278,
        "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations",
        "authors": "Igor Halperin, Jiayu Liu, Xiao Zhang, Xiao Zhang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4002715"
    },
    {
        "id": 18279,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 18280,
        "title": "Inverse Reinforcement Learning",
        "authors": "Pieter Abbeel, Andrew Y. Ng",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_142"
    },
    {
        "id": 18281,
        "title": "Decision letter: Neural computations underlying inverse reinforcement learning in the human brain",
        "authors": "",
        "published": "2017-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.29718.016"
    },
    {
        "id": 18282,
        "title": "Area Coverage for Swarm Robots Via Inverse Reinforcement Learning",
        "authors": "Mingxuan Chen, Ping Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4592186"
    },
    {
        "id": 18283,
        "title": "Inverse Reinforcement Learning for Optimal Control Systems",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_6"
    },
    {
        "id": 18284,
        "title": "Merging Reinforcement Learning and Inverse Reinforcement Learning via Auxiliary Reward System",
        "authors": "Wadhah Zeyad Tareq, Mehmet Fatih Amasyali",
        "published": "2022-2-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic54071.2022.9722629"
    },
    {
        "id": 18285,
        "title": "Inverse Reinforcement Learning for Marketing",
        "authors": "Igor Halperin",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3087057"
    },
    {
        "id": 18286,
        "title": "Inverse Reinforcement Learning for Multiplayer Non-Zero-Sum Games",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_8"
    },
    {
        "id": 18287,
        "title": "Inverse Reinforcement Learning Schemes for Continuous-Time Deterministic Systems",
        "authors": "Hamed Jabbari Asl, Eiji Uchibe",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4386207"
    },
    {
        "id": 18288,
        "title": "Inverse Reinforcement Learning for Two-Player Zero-Sum Games",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_7"
    },
    {
        "id": 18289,
        "title": "Learning from Explanations With Maximum Likelihood Inverse Reinforcement Learning",
        "authors": "Silvia Tulli, Francisco S. Melo, Ana Paiva, Mohamed Chetouani",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nOur research effort takes inspiration from human social learning mechanisms to focus on situations in which an expert guides a learner through explanations. The proposed approach incorporates explanations into maximum likelihood inverse reinforcement learning. We computationally evaluate explanations against other teaching signals (reward, demonstration and explanation) in three navigational scenarios. The generated explanations are also evaluated in a user study with 150 participants. The user study investigates participants' preferences between the different types of teaching signals and the impact of contextual situations, i.e., distance from the task's goal, on their preferences. Our simulations' results show that explanations lead to better performance compared to reward and demonstration signals, and that explanations are preferred by human teachers in situations where the goal is far from the learner.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1439366/v1"
    },
    {
        "id": 18290,
        "title": "Background on Integral and Inverse Reinforcement Learning for Feedback Control",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_2"
    },
    {
        "id": 18291,
        "title": "Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2",
        "authors": "Alexander Peysakhovich",
        "published": "2019-1-27",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3306618.3314259"
    },
    {
        "id": 18292,
        "title": "Neuroevolution-based Inverse Reinforcement Learning",
        "authors": "Karan K. Budhraja, Tim Oates",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2017.7969297"
    },
    {
        "id": 18293,
        "title": "Linear inverse reinforcement learning in continuous time and space",
        "authors": "Rushikesh Kamalapurkar",
        "published": "2018-6",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431430"
    },
    {
        "id": 18294,
        "title": "Inverse Reinforcement Learning and Imitation Learning",
        "authors": "Matthew F. Dixon, Igor Halperin, Paul Bilokon",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41068-1_11"
    },
    {
        "id": 18295,
        "title": "Machining sequence learning via inverse reinforcement learning",
        "authors": "Yasutomo Sugisawa, Keigo Takasugi, Naoki Asakawa",
        "published": "2022-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.precisioneng.2021.09.017"
    },
    {
        "id": 18296,
        "title": "Online Observer-Based Inverse Reinforcement Learning",
        "authors": "Ryan Self, Kevin Coleman, He Bai, Rushikesh Kamalapurkar",
        "published": "2021-5-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9482906"
    },
    {
        "id": 18297,
        "title": "Using Inverse Reinforcement Learning to Predict Goal-directed Shifts of Attention",
        "authors": "Gregory Zelinsky",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1086-0"
    },
    {
        "id": 18298,
        "title": "Comfortable Driving by using Deep Inverse Reinforcement Learning",
        "authors": "Daiko Kishikawa, Sachiyo Arai",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/agents.2019.8929214"
    },
    {
        "id": 18299,
        "title": "Online inverse reinforcement learning for systems with disturbances",
        "authors": "Ryan Self, Moad Abudia, Rushikesh Kamalapurkar",
        "published": "2020-7",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147344"
    },
    {
        "id": 18300,
        "title": "Investigation and Imitation of Human Captains’ Maneuver Using Inverse Reinforcement Learning",
        "authors": "Takefumi Higaki, Hirotada Hashimoto, Hitoshi Yoshioka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutomatic collision avoidance is of significant importance to prevent maritime collisions. Although many studies have been conducted in recent years, autonomous system has not completely replaced human captains since it is still difficult to imitate their complicated decisions. Thus, the present paper tries to investigate and imitate experienced captains’ maneuver using maximum entropy inverse reinforcement learning (MaxEnt IRL). We firstly verify that MaxEnt IRL can reproduce appropriate reward function from demonstrative trajectories. Afterwards, we conduct an experiment on a simulator where well-experienced captains maneuver in congested sea and estimate reward from the trajectories. Searching the route which maximizes the obtained reward, finally, we demonstrate the optimized route can avoid collision against multiple ships in compliance with the International Regulations for Preventing Collisions at Sea (COLREGs).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1844861/v1"
    },
    {
        "id": 18301,
        "title": "Learning and Adapting Behavior of Autonomous Vehicles through Inverse Reinforcement Learning",
        "authors": "Rainer Trauth, Marc Kaufeld, Maximilian Geisslinger, Johannes Betz",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186668"
    },
    {
        "id": 18302,
        "title": "Inverse reinforcement learning for dexterous hand manipulation",
        "authors": "Jedrzej Orbik, Alejandro Agostini, Dongheui Lee",
        "published": "2021-8-23",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl49984.2021.9515637"
    },
    {
        "id": 18303,
        "title": "Learning Spatial Search using Submodular Inverse Reinforcement Learning",
        "authors": "Ji-Jie Wu, Kuo-Shih Tseng",
        "published": "2020-11-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssrr50563.2020.9292603"
    },
    {
        "id": 18304,
        "title": "Visual Navigation Using Inverse Reinforcement Learning and an Extreme Learning Machine",
        "authors": "Qiang Fang, Wenzhuo Zhang, Xitong Wang",
        "published": "2021-8-18",
        "citations": 1,
        "abstract": "In this paper, we focus on the challenges of training efficiency, the designation of reward functions, and generalization in reinforcement learning for visual navigation and propose a regularized extreme learning machine-based inverse reinforcement learning approach (RELM-IRL) to improve the navigation performance. Our contributions are mainly three-fold: First, a framework combining extreme learning machine with inverse reinforcement learning is presented. This framework can improve the sample efficiency and obtain the reward function directly from the image information observed by the agent and improve the generation for the new target and the new environment. Second, the extreme learning machine is regularized by multi-response sparse regression and the leave-one-out method, which can further improve the generalization ability. Simulation experiments in the AI-THOR environment showed that the proposed approach outperformed previous end-to-end approaches, thus, demonstrating the effectiveness and efficiency of our approach.",
        "link": "http://dx.doi.org/10.3390/electronics10161997"
    },
    {
        "id": 18305,
        "title": "Spectra to Structure: Deep Reinforcement Learning for Molecular Inverse Problem",
        "authors": "Bhuvanesh Sridharan, Sarvesh Mehta, Yashaswi Pathak, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 1,
        "abstract": "Spectroscopy is the study of how matter interacts with electromagnetic radiations of specific frequencies that has led to several monumental discoveries in science. The spectra of any particular molecule is highly information-rich, yet the inverse relation from the spectra to the molecular structure is still an unsolved problem. Nuclear Magnetic Resonance (NMR) spectroscopy is one such critical tool in the tool-set for scientists to characterise any chemical sample. In this work, a novel framework is proposed that attempts to solve this inverse problem by navigating the chemical space to find the correct structure that resulted in the target spectra. The proposed framework uses a combination of online Monte- Carlo-Tree-Search (MCTS) and a set of offline trained Graph Convolution Networks to build a molecule iteratively from scratch. Our method is able to predict the correct structure of the molecule ∼80% of the time in its top 3 guesses. We believe that the proposed framework is a significant step in solving the inverse design problem of NMR spectra to molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-4hc7k"
    },
    {
        "id": 18306,
        "title": "Learning Tasks in Intelligent Environments via Inverse Reinforcement Learning",
        "authors": "Syed Ihtesham Hussain Shah, Antonio Coronato",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ie51775.2021.9486594"
    },
    {
        "id": 18307,
        "title": "Robo-Advising: Enhancing Investment with Inverse Optimization and Deep Reinforcement Learning",
        "authors": "Haoran Wang, Shi Yu",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00063"
    },
    {
        "id": 18308,
        "title": "Inverse Reinforcement Learning with Learning and Leveraging Demonstrators’ Varying Expertise Levels",
        "authors": "Somtochukwu Oguchienti, Mahsa Ghasemi",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313475"
    },
    {
        "id": 18309,
        "title": "Inverse Reinforcement Learning for Text Summarization",
        "authors": "Yu Fu, Deyi Xiong, Yue Dong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.436"
    },
    {
        "id": 18310,
        "title": "Inverse Reinforcement Learning Control for Building Energy Management",
        "authors": "Sourav Dey, Thibault Marzullo, Gregor Henze",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4330892"
    },
    {
        "id": 18311,
        "title": "Author response: Neural computations underlying inverse reinforcement learning in the human brain",
        "authors": "Sven Collette, Wolfgang M Pauli, Peter Bossaerts, John O'Doherty",
        "published": "2017-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.29718.017"
    },
    {
        "id": 18312,
        "title": "Reinforcement Learning Based Dynamic Inverse Attitude Control of Near-space Vehicle",
        "authors": "Yaohua Shen, Mou Chen",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189285"
    },
    {
        "id": 18313,
        "title": "Deep Inverse Reinforcement Learning for Sepsis Treatment",
        "authors": "Chao Yu, Guoqi Ren, Jiming Liu",
        "published": "2019-6",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi.2019.8904645"
    },
    {
        "id": 18314,
        "title": "Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach",
        "authors": "Muhammad Fahad, Zhuo Chen, Yi Guo",
        "published": "2018-10",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593438"
    },
    {
        "id": 18315,
        "title": "Inverse Reinforcement Learning based Bayesian Goal Inference Method for Early Nuclear Proliferation Detection",
        "authors": "Dennis Thomas, Zachary Weems, Richard Overstreet, Benjamin Wilson",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2203121"
    },
    {
        "id": 18316,
        "title": "Equilibrium Inverse Reinforcement Learning for Ride-hailing Vehicle Network",
        "authors": "Takuma Oda",
        "published": "2021-4-19",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3442381.3449935"
    },
    {
        "id": 18317,
        "title": "Risk-sensitive Inverse Reinforcement Learning via Coherent Risk Models",
        "authors": "Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, Marco Pavone",
        "published": "2017-7-12",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2017.xiii.069"
    },
    {
        "id": 18318,
        "title": "Online inverse reinforcement learning for nonlinear systems",
        "authors": "Ryan Self, Michael Harlan, Rushikesh Kamalapurkar",
        "published": "2019-8",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta.2019.8920458"
    },
    {
        "id": 18319,
        "title": "Clustered Autoencoded Variational Inverse Reinforcement Learning",
        "authors": "Yuling Max Chen",
        "published": "2022-1-1",
        "citations": 0,
        "abstract": "Abstract\nVariational Auto-Encoder (VAE) is a handy and computationally friendly Bayesian tool for Inverse Reinforcement Learning (IRL) problems, with the native setting of absent reward functions in a Markov Decision Process (MDP). However, recent works mainly deal with single reward, which turn out to be insufficient for complex dynamic environments with multiple demonstrators of various characteristics (hence multiple reward functions). This paper extends the dimensionality of reward (from ℝ to ℝ\nK\n) by incorporating a latent embedding and clustering step on top of a scalable Bayesian IRL model, which enhances her applicability to multi-reward scenarios. We introduce our method, Clustered Autoencoded Variational Inverse Reinforcement Learning (CAVIRL), which is able to approximate multiple posterior reward functions and learn the corresponding policies for experts of various characteristics and skills. As a by-product, the proposed model also thrives to determine the number of clusters K on her own, as opposed to the competing multi-reward imitation learning models that require K to be prespecified. We trained the proposed model within a grid world with multiple types of players, where we achieved 100% correctness in determining the number of players’ types and 80%-83.9% match between the model-learned policies and the players’ demonstrations from the data.",
        "link": "http://dx.doi.org/10.1515/stat-2022-0109"
    },
    {
        "id": 18320,
        "title": "Demonstration-Efficient Inverse Reinforcement Learning in Procedurally Generated Environments",
        "authors": "Alessandro Sestini, Alexander Kuhnle, Andrew D. Bagdanov",
        "published": "2021-8-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619084"
    },
    {
        "id": 18321,
        "title": "First-Person Activity Forecasting with Online Inverse Reinforcement Learning",
        "authors": "Nicholas Rhinehart, Kris M. Kitani",
        "published": "2017-10",
        "citations": 65,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.399"
    },
    {
        "id": 18322,
        "title": "Online Data-Driven Inverse Reinforcement Learning for Deterministic Systems",
        "authors": "Hamed Jabbari Asl, Eiji Uchibe",
        "published": "2022-12-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci51031.2022.10022226"
    },
    {
        "id": 18323,
        "title": "Apprenticeship learning of flight trajectories prediction with inverse reinforcement learning",
        "authors": "Christos Spatharis, Konstantinos Blekas, George A. Vouros",
        "published": "2020-9-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3411408.3411427"
    },
    {
        "id": 18324,
        "title": "A Universal Approach to Nanophotonic Inverse Design through Reinforcement Learning",
        "authors": "Marco Butz, Alexander Leifhelm, Marlon Becker, Benjamin Risse, Carsten Schuck",
        "published": "2023",
        "citations": 0,
        "abstract": "We present a novel method to perform universal black-box optimization of pixel-discrete nanophotonic devices based on reinforcement learning. We demonstrate the capabilities of our method for a silicon-on-insulator waveguide-mode converter with > 95% conversion efficiency.",
        "link": "http://dx.doi.org/10.1364/cleo_si.2023.sth4g.3"
    },
    {
        "id": 18325,
        "title": "Objective Weight Interval Estimation Using Adversarial Inverse Reinforcement Learning",
        "authors": "Naoya Takayama, Sachiyo Arai",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3281593"
    },
    {
        "id": 18326,
        "title": "Inverse Reinforcement Learning for Generalized Labeled Multi-Bernoulli Multi-Target Tracking",
        "authors": "Ryan W. Thomas, Jordan D. Larson",
        "published": "2021-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero50100.2021.9438310"
    },
    {
        "id": 18327,
        "title": "Modeling Driver Behavior using Adversarial Inverse Reinforcement Learning",
        "authors": "Moritz Sackmann, Henrik Bey, Ulrich Hofmann, Jorn Thielecke",
        "published": "2022-6-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827292"
    },
    {
        "id": 18328,
        "title": "Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations",
        "authors": "Mostafa Hussein, Momotaz Begum, Marek Petrik",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793867"
    },
    {
        "id": 18329,
        "title": "Inverse Reinforcement Learning: A Control Lyapunov Approach",
        "authors": "Samuel Tesfazgi, Armin Lederer, Sandra Hirche",
        "published": "2021-12-14",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683494"
    },
    {
        "id": 18330,
        "title": "Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling",
        "authors": "QiJie Zou, Haoyu Li, Rubo Zhang",
        "published": "2018-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500666"
    },
    {
        "id": 18331,
        "title": "Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning",
        "authors": "Mehmet F. Ozkan, Abishek J. Rocque, Yao Ma",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.11.283"
    },
    {
        "id": 18332,
        "title": "Integral Reinforcement Learning for Optimal Regulation",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_3"
    },
    {
        "id": 18333,
        "title": "Integral Reinforcement Learning for Optimal Tracking",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_4"
    },
    {
        "id": 18334,
        "title": "Integral Reinforcement Learning for Zero-Sum Games",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_5"
    },
    {
        "id": 18335,
        "title": "From inverse optimal control to inverse reinforcement learning: A historical review",
        "authors": "Nematollah Ab Azar, Aref Shahmansoorian, Mohsen Davoudi",
        "published": "2020",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.arcontrol.2020.06.001"
    },
    {
        "id": 18336,
        "title": "Learning Reward Models for Cooperative Trajectory Planning with Inverse Reinforcement Learning and Monte Carlo Tree Search",
        "authors": "Karl Kurzer, Matthias Bitzer, J. Marius Zollner",
        "published": "2022-6-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827031"
    },
    {
        "id": 18337,
        "title": "Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning",
        "authors": "Florian Shkurti, Nikhil Kakodkar, Gregory Dudek",
        "published": "2018-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8463196"
    },
    {
        "id": 18338,
        "title": "Inverse Reinforcement Learning for Identification of Linear-Quadratic Zero-Sum Differential Games",
        "authors": "Emin Martirosyan, Ming Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4103314"
    },
    {
        "id": 18339,
        "title": "Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning",
        "authors": "Zhichao Chen, Yutaka Nakamura, Hiroshi Ishiguro",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lra.2023.3267385"
    },
    {
        "id": 18340,
        "title": "Joint Path Planning and Power Allocation of a Cellular-Connected Uav Using Apprenticeship Learning via Deep Inverse Reinforcement Learning",
        "authors": "Alireza Shamsoshoaraa, Fatemeh Lotfi, Sajad Mousavi, Fatemeh Afghah, Ismail Guvenc",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4511065"
    },
    {
        "id": 18341,
        "title": "Inverse reinforcement learning in contextual MDPs",
        "authors": "Stav Belogolovsky, Philip Korsunsky, Shie Mannor, Chen Tessler, Tom Zahavy",
        "published": "2021-9",
        "citations": 4,
        "abstract": "AbstractWe consider the task of Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). In this setting, contexts, which define the reward and transition kernel, are sampled from a distribution. In addition, although the reward is a function of the context, it is not provided to the agent. Instead, the agent observes demonstrations from an optimal policy. The goal is to learn the reward mapping, such that the agent will act optimally even when encountering previously unseen contexts, also known as zero-shot transfer. We formulate this problem as a non-differential convex optimization problem and propose a novel algorithm to compute its subgradients. Based on this scheme, we analyze several methods both theoretically, where we compare the sample complexity and scalability, and empirically. Most importantly, we show both theoretically and empirically that our algorithms perform zero-shot transfer (generalize to new and unseen contexts). Specifically, we present empirical experiments in a dynamic treatment regime, where the goal is to learn a reward function which explains the behavior of expert physicians based on recorded data of them treating patients diagnosed with sepsis.",
        "link": "http://dx.doi.org/10.1007/s10994-021-05984-x"
    },
    {
        "id": 18342,
        "title": "Ordinal Inverse Reinforcement Learning Applied to Robot Learning with Small Data",
        "authors": "Adrià Colomé, Carme Torras",
        "published": "2022-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981731"
    },
    {
        "id": 18343,
        "title": "Inverse reinforcement learning through logic constraint inference",
        "authors": "Mattijs Baert, Sam Leroux, Pieter Simoens",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-023-06311-2"
    },
    {
        "id": 18344,
        "title": "An inverse reinforcement learning algorithm for semi-Markov decision processes",
        "authors": "Chuanfang Tan, Yanjie Li, Yuhu Cheng",
        "published": "2017-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280816"
    },
    {
        "id": 18345,
        "title": "Multi-Objective Inverse Reinforcement Learning via Non-Negative Matrix Factorization",
        "authors": "Daiko Kishikawa, Sachiyo Arai",
        "published": "2021-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiai-aai53430.2021.00078"
    },
    {
        "id": 18346,
        "title": "Identification of Animal Behavioral Strategies by Inverse Reinforcement Learning",
        "authors": "Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki Tsukada, Shunji Nakano, Ikue Mori, Shin Ishii",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractAnimals are able to reach a desired state in an environment by controlling various behavioral patterns. Identification of the behavioral strategy used for this control is important for understanding animals’ decision-making and is fundamental to dissect information processing done by the nervous system. However, methods for quantifying such behavioral strategies have not been fully established. In this study, we developed an inverse reinforcement-learning (IRL) framework to identify an animal’s behavioral strategy from behavioral time-series data. As a particular target, we applied this framework toC. elegansthermotactic behavior; after cultivation at a constant temperature with or without food, the fed and starved worms prefer and avoid from the cultivation temperature on a thermal gradient, respectively. Our IRL approach revealed that the fed worms used both absolute and temporal derivative of temperature and that their strategy comprised mixture of two strategies: directed migration (DM) and isothermal migration (IM). The DM is a strategy that the worms efficiently reach to specific temperature, which explained thermotactic behaviors of the fed worms. The IM is a strategy that the worms track along a constant temperature, which reflects isothermal tracking well observed in previous studies. We also showed the neural basis underlying the strategies, by applying our method to thermosensory neuron-deficient worms. In contrast to fed animals, the strategy of starved animals indicated that they escaped the cultivation temperature using only absolute, but not temporal derivative of temperature. Thus, our IRL-based approach is capable of identifying animal strategies from behavioral time-series data and will be applicable to wide range of behavioral studies, including decision-making of other organisms.Author SummaryUnderstanding animal decision-making has been a fundamental problem in neuroscience and behavioral ecology. Many studies analyze actions that represent decision-making in behavioral tasks, in which rewards are artificially designed with specific objectives. However, it is impossible to extend this artificially designed experiment to a natural environment, because in a natural environment, the rewards for freely-behaving animals cannot be clearly defined. To this end, we must reverse the current paradigm so that rewards are identified from behavioral data. Here, we propose a new reverse-engineering approach (inverse reinforcement learning) that can estimate a behavioral strategy from time-series data of freely-behaving animals. By applying this technique with thermotaxis inC. elegans, we successfully identified the reward-based behavioral strategy.",
        "link": "http://dx.doi.org/10.1101/129007"
    },
    {
        "id": 18347,
        "title": "Meta-Cognition. An Inverse-Inverse Reinforcement Learning Approach for Cognitive Radars",
        "authors": "Kunal Pattanayak, Vikram Krishnamurthy, Christopher Berry",
        "published": "2022-7-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/fusion49751.2022.9841368"
    },
    {
        "id": 18348,
        "title": "Inverse Reinforcement Learning Approach for Elicitation of Preferences in Multi-objective Sequential Optimization",
        "authors": "Akiko Ikenaga, Sachiyo Arai",
        "published": "2018-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/agents.2018.8460075"
    },
    {
        "id": 18349,
        "title": "Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control",
        "authors": "Jiayu Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160374"
    },
    {
        "id": 18350,
        "title": "Future Trajectory Prediction via RNN and Maximum Margin Inverse Reinforcement Learning",
        "authors": "Dooseop Choi, Taeg-Hyun An, Kyounghwan Ahn, Jeongdan Choi",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00026"
    },
    {
        "id": 18351,
        "title": "Reinforcement Learning for Robots with special reference to the Inverse kinematics solutions",
        "authors": "Priya Shukla, G. C. Nandi",
        "published": "2018-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocomtech.2018.8722399"
    },
    {
        "id": 18352,
        "title": "Improved reward estimation for efficient robot navigation using inverse reinforcement learning",
        "authors": "Olimpiya Saha, Prithviraj Dasgupta",
        "published": "2017-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ahs.2017.8046385"
    },
    {
        "id": 18353,
        "title": "Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning",
        "authors": "Sayan Ghosh, Shashank Srivastava",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.125"
    },
    {
        "id": 18354,
        "title": "Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning",
        "authors": "Changxi You, Jianbo Lu, Dimitar Filev, Panagiotis Tsiotras",
        "published": "2019-4",
        "citations": 164,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.robot.2019.01.003"
    },
    {
        "id": 18355,
        "title": "Gradient-based minimization for multi-expert Inverse Reinforcement Learning",
        "authors": "Davide Tateo, Matteo Pirotta, Marcello Restelli, Andrea Bonarini",
        "published": "2017-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280919"
    },
    {
        "id": 18356,
        "title": "Meta-Adversarial Inverse Reinforcement Learning for Decision-making Tasks",
        "authors": "Pin Wang, Hanhan Li, Ching-Yao Chan",
        "published": "2021-5-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561330"
    },
    {
        "id": 18357,
        "title": "Bridging the Gap Between Imitation Learning and Inverse Reinforcement Learning",
        "authors": "Bilal Piot, Matthieu Geist, Olivier Pietquin",
        "published": "2017-8",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2543000"
    },
    {
        "id": 18358,
        "title": "Forward and inverse reinforcement learning sharing network weights and hyperparameters",
        "authors": "Eiji Uchibe, Kenji Doya",
        "published": "2021-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2021.08.017"
    },
    {
        "id": 18359,
        "title": "FP-IRL: Fokker-Planck-based Inverse Reinforcement Learning --- A Physics-Constrained Approach to Modeling Markov Decision Processes",
        "authors": "Chengyang Huang, Siddhartha Srivastava, Xun Huan, Krishna Garikipati",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.64c26778632e9539aa87d9f7"
    },
    {
        "id": 18360,
        "title": "Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient",
        "authors": "Ming Zhan, Jingjing Fan, Jianying Guo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3305453"
    },
    {
        "id": 18361,
        "title": "An inverse reinforcement learning framework with the Q-learning mechanism for the metaheuristic algorithm",
        "authors": "Fuqing Zhao, Qiaoyun Wang, Ling Wang",
        "published": "2023-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110368"
    },
    {
        "id": 18362,
        "title": "Objective-aware Traffic Simulation via Inverse Reinforcement Learning",
        "authors": "Guanjie Zheng, Hanyang Liu, Kai Xu, Zhenhui Li",
        "published": "2021-8",
        "citations": 2,
        "abstract": "Traffic simulators act as an essential component in the operating and planning of transportation systems. Conventional traffic simulators usually employ a calibrated physical car-following model to describe vehicles' behaviors and their interactions with traffic environment. However, there is no universal physical model that can accurately predict the pattern of vehicle's behaviors in different situations. A fixed physical model tends to be less effective in a complicated environment given the non-stationary nature of traffic dynamics. In this paper, we formulate traffic simulation as an inverse reinforcement learning problem, and propose a parameter sharing adversarial inverse reinforcement learning model for dynamics-robust simulation learning. Our proposed model is able to imitate a vehicle's trajectories in the real world while simultaneously recovering the reward function that reveals the vehicle's true objective which is invariant to different dynamics. Extensive experiments on synthetic and real-world datasets show the superior performance of our approach compared to state-of-the-art methods and its robustness to variant dynamics of traffic.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/519"
    },
    {
        "id": 18363,
        "title": "Driver Behavior Modeling via Inverse Reinforcement Learning Based on Particle Swarm Optimization",
        "authors": "Zeng-Jie Liu, Huai-Ning Wu",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327174"
    },
    {
        "id": 18364,
        "title": "Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning",
        "authors": "Jared Town, Zachary Morrison, Rushikesh Kamalapurkar",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156188"
    },
    {
        "id": 18365,
        "title": "Option compatible reward inverse reinforcement learning",
        "authors": "Rakhoon Hwang, Hanjin Lee, Hyung Ju Hwang",
        "published": "2022-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2022.01.016"
    },
    {
        "id": 18366,
        "title": "Misspecification in Inverse Reinforcement Learning",
        "authors": "Joar Skalse, Alessandro Abate",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function R from a policy pi. To do this, we need a model of how pi relates to R. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function R. We also introduce a framework for reasoning about misspecification in IRL, together with formal tools that can be used to easily derive the misspecification robustness of new IRL models.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i12.26766"
    },
    {
        "id": 18367,
        "title": "Recent Advancements in Inverse Reinforcement Learning",
        "authors": "Alberto Maria Metelli",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Inverse reinforcement learning (IRL) has seen significant advancements in recent years. This class of approaches aims to efficiently learn the underlying reward function that rationalizes the behavior exhibited by expert agents, often represented by humans. In contrast to mere behavioral cloning, the reconstruction of a reward function yields appealing implications, as it allows for more effective interpretability of the expert’s decisions and provides a transferable specification of the expert’s objectives for application in even different environments. Unlike the well-understood field of reinforcement learning (RL) from a theoretical perspective, IRL still grapples with limited understanding, significantly constraining its applicability. A fundamental challenge in IRL is the inherent ambiguity in selecting a reward function, given the existence of multiple candidate functions, all explaining the expert’s behavior.\n\nIn this talk, I will survey three of my papers that have made notable contributions to the IRL field: “Provably Efficient Learning of Transferable Rewards”, “Towards Theoretical Understanding of Inverse Reinforcement Learning”, and “Inverse Reinforcement Learning with Sub-optimal Experts\".\n\nThe central innovation introduced by the first paper is a novel formulation of the IRL problem that overcomes the issue of ambiguity. IRL is reframed as the problem of learning the feasible reward set, which is the set of all rewards that can explain the expert’s behavior. This approach postpones the selection of the reward function, thereby circumventing the ambiguity issues. Furthermore, the feasible reward set exhibits convenient geometric properties that enable the development of efficient algorithms for its computation. \n\nBuilding on this novel formulation of IRL, the second paper addresses the problem of efficiently learning the feasible reward set when the environment and the expert’s policy are not known in advance. It introduces a novel way to assess the dissimilarity between feasible reward sets based on the Hausdorff distance and presents a new PAC (probabilistic approximately correct) framework. The most significant contribution of this paper is the introduction of the first sample complexity lower bound, which highlights the challenges inherent in the IRL problem. Deriving this lower bound necessitated the development of novel technical tools. The paper also demonstrates that when a generative model of the environment is available, a uniform sampling strategy achieves a sample complexity that matches the lower bound, up to logarithmic factors.\n\nFinally, in the third paper, the IRL problem in the presence of sub-optimal experts is investigated. Specifically, the paper assumes the availability of multiple sub-optimal experts, in addition to the expert agent, which provides additional demonstrations, associated with a known quantification of the maximum amount of sub-optimality. The paper shows that this richer information mitigates the ambiguity problem, significantly reducing the size of the feasible reward set while retaining its favorable geometric properties. Furthermore, the paper explores the associated statistical problem and derives novel lower bounds for sample complexity, along with almost matching algorithms. These selected papers represent notable advancements in IRL, contributing to the establishment of a solid theoretical foundation for IRL and extending the framework to accommodate scenarios with sub-optimal experts.",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30296"
    },
    {
        "id": 18368,
        "title": "Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning",
        "authors": "Kentaro Nishi, Masamichi Shimosaka",
        "published": "2020-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9197126"
    },
    {
        "id": 18369,
        "title": "Interactive Teaching Algorithms for Inverse Reinforcement Learning",
        "authors": "Parameswaran Kamalaruban, Rati Devidze, Volkan Cevher, Adish Singla",
        "published": "2019-8",
        "citations": 10,
        "abstract": "We study the problem of inverse reinforcement learning (IRL) with the added twist that the learner is assisted by a helpful teacher. More formally, we tackle the following algorithmic question: How could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process? We present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner's current policy. In particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner's dynamics and a blackbox setting where the teacher has minimal knowledge. Then, we study a sequential variant of the popular MCE-IRL learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. Extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/374"
    },
    {
        "id": 18370,
        "title": "Inverse reinforcement learning control for building energy management",
        "authors": "Sourav Dey, Thibault Marzullo, Gregor Henze",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2023.112941"
    }
]