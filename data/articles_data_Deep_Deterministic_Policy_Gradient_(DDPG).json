[
    {
        "id": 16671,
        "title": "Deterministic Policy Gradient and the DDPG",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_13"
    },
    {
        "id": 16672,
        "title": "Deep Deterministic Policy Gradient (DDPG) Agent-Based Sliding Mode Control for Quadrotor Attitudes",
        "authors": "Wenjun Hu, Yueneng Yang, Zhiyang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "A novel reinforcement learning deep deterministic policy gradient agent-based sliding mode control (DDPG-SMC) approach is proposed to suppress the chattering phenomenon in the attitude control for quadrotors, in the presence of external disturbances. First, the attitude dynamics model of the studied quadrotor is derived and the attitude control problem is described by formulas. Second, a sliding mode controller including its sliding mode surface and reaching law is selected for the nonlinear dynamic system, and the stability of the designed SMC system is supported by Lyapunov stability theorem. Third, a reinforcement learning (RL) agent based on deep deterministic policy gradient (DDPG) is trained to adjust the switching control gain adaptively. During the training process, the input signals of agent are the actual and desired attitude angles, and the output action is the time-varying control gain. Finally, the above trained agent is applied to the SMC as a parameter regulator, to implement the adaptive adjustment of the switching control gain related to reaching law, and the simulation results verify the robustness and effectiveness of the proposed DDPG-SMC method.",
        "link": "http://dx.doi.org/10.20944/preprints202401.1213.v1"
    },
    {
        "id": 16673,
        "title": "Learning Optimal Trajectory Generation for Low-Cost Redundant Manipulator using Deep Deterministic Policy Gradient(DDPG)",
        "authors": "Seunghyeon Lee, Seongho Jin, Seonghyeon Hwang, Inho Lee",
        "published": "2022-3-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7746/jkros.2022.17.1.058"
    },
    {
        "id": 16674,
        "title": "Development of a Deep Deterministic Policy Gradient (DDPG) Algorithm for Suturing Task Automation",
        "authors": "Antonella Imperato, Marco Caianiello, Fanny Ficuciello",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406967"
    },
    {
        "id": 16675,
        "title": "Deep Deterministic Policy Gradient (DDPG) Agent-Based Sliding Mode Control for Quadrotor Attitudes",
        "authors": "Wenjun Hu, Yueneng Yang, Zhiyang Liu",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "A novel reinforcement deep learning deterministic policy gradient agent-based sliding mode control (DDPG-SMC) approach is proposed to suppress the chattering phenomenon in attitude control for quadrotors, in the presence of external disturbances. First, the attitude dynamics model of the quadrotor under study is derived, and the attitude control problem is described using formulas. Second, a sliding mode controller, including its sliding mode surface and reaching law, is chosen for the nonlinear dynamic system. The stability of the designed SMC system is validated through the Lyapunov stability theorem. Third, a reinforcement learning (RL) agent based on deep deterministic policy gradient (DDPG) is trained to adaptively adjust the switching control gain. During the training process, the input signals for the agent are the actual and desired attitude angles, while the output action is the time-varying control gain. Finally, the trained agent mentioned above is utilized in the SMC as a parameter regulator to facilitate the adaptive adjustment of the switching control gain associated with the reaching law. The simulation results validate the robustness and effectiveness of the proposed DDPG-SMC method.",
        "link": "http://dx.doi.org/10.3390/drones8030095"
    },
    {
        "id": 16676,
        "title": "Using Deep Deterministic Policy Gradient (DDPG) to Train a Double-Jointed Arm to Reach Target Locations in the Unity ML-Agents Reacher Environment",
        "authors": "Oluwaseyi Awoga",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3885007"
    },
    {
        "id": 16677,
        "title": "DDPG-Edge-Cloud: A Deep-Deterministic Policy Gradient based Multi-Resource Allocation in Edge-Cloud System",
        "authors": "Arslan Qadeer, Myung J. Lee",
        "published": "2022-2-21",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic54071.2022.9722676"
    },
    {
        "id": 16678,
        "title": "Reinforcement Learning Approach with Deep Deterministic Policy Gradient DDPG-Controlled Virtual Synchronous Generator for an Islanded Microgrid",
        "authors": "Mohamed A. Afifi, Mostafa I. Marei, Ahmed M.I. Mohamad",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mepcon58725.2023.10462333"
    },
    {
        "id": 16679,
        "title": "Scheduling of AGVs in Automated Container Terminal Based on the Deep Deterministic Policy Gradient (DDPG) Using the Convolutional Neural Network (CNN)",
        "authors": "Chun Chen, Zhi-Hua Hu, Lei Wang",
        "published": "2021-12-16",
        "citations": 16,
        "abstract": "In order to improve the horizontal transportation efficiency of the terminal Automated Guided Vehicles (AGVs), it is necessary to focus on coordinating the time and space synchronization operation of the loading and unloading of equipment, the transportation of equipment during the operation, and the reduction in the completion time of the task. Traditional scheduling methods limited dynamic response capabilities and were not suitable for handling dynamic terminal operating environments. Therefore, this paper discusses how to use delivery task information and AGVs spatiotemporal information to dynamically schedule AGVs, minimizes the delay time of tasks and AGVs travel time, and proposes a deep reinforcement learning algorithm framework. The framework combines the benefits of real-time response and flexibility of the Convolutional Neural Network (CNN) and the Deep Deterministic Policy Gradient (DDPG) algorithm, and can dynamically adjust AGVs scheduling strategies according to the input spatiotemporal state information. In the framework, firstly, the AGVs scheduling process is defined as a Markov decision process, which analyzes the system’s spatiotemporal state information in detail, introduces assignment heuristic rules, and rewards the reshaping mechanism in order to realize the decoupling of the model and the AGVs dynamic scheduling problem. Then, a multi-channel matrix is built to characterize space–time state information, the CNN is used to generalize and approximate the action value functions of different state information, and the DDPG algorithm is used to achieve the best AGV and container matching in the decision stage. The proposed model and algorithm frame are applied to experiments with different cases. The scheduling performance of the adaptive genetic algorithm and rolling horizon approach is compared. The results show that, compared with a single scheduling rule, the proposed algorithm improves the average performance of task completion time, task delay time, AGVs travel time and task delay rate by 15.63%, 56.16%, 16.36% and 30.22%, respectively; compared with AGA and RHPA, it reduces the tasks completion time by approximately 3.10% and 2.40%.",
        "link": "http://dx.doi.org/10.3390/jmse9121439"
    },
    {
        "id": 16680,
        "title": "A Self-Adaptive Vibration Reduction Method Based on Deep Deterministic Policy Gradient (DDPG) Reinforcement Learning Algorithm",
        "authors": "Xin Jin, Hongbao Ma, Jian Tang, Yihua Kang",
        "published": "2022-9-27",
        "citations": 0,
        "abstract": "Although many adaptive techniques for active vibration reduction have been designed to achieve optimal performance in practical applications, few are related to reinforcement learning (RL). To explore the best performance of the active vibration reduction system (AVRS) without prior knowledge, a self-adaptive parameter regulation method based on the DDPG algorithm was examined in this study. The DDPG algorithm is unsuitable for a random environment and prone to reward-hacking. To solve this problem, a reward function optimization method based on the integral area of the decibel (dB) value between transfer functions was investigated. Simulation and graphical experimental results show that the optimized DDPG algorithm can automatically track and maintain optimal control performance of the AVRS.",
        "link": "http://dx.doi.org/10.3390/app12199703"
    },
    {
        "id": 16681,
        "title": "Deep Deterministic Policy Gradient (DDPG)-Based Resource Allocation Scheme for NOMA Vehicular Communications",
        "authors": "Yi-Han Xu, Cheng-Cheng Yang, Min Hua, Wen Zhou",
        "published": "2020",
        "citations": 56,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.2968595"
    },
    {
        "id": 16682,
        "title": "Enhancing Longitudinal Velocity Control With Attention Mechanism-Based Deep Deterministic Policy Gradient (DDPG) for Safety and Comfort",
        "authors": "Fahmida Islam, John E. Ball, Christopher T. Goodin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3368435"
    },
    {
        "id": 16683,
        "title": "Deep Deterministic Policy Gradient (DDPG)-based Photovoltaic Power Forecasting",
        "authors": "Gangqiang Li, Yuxiang Zhu, Jinfeng Gao, Fang Liu, Yu He, Yafei Guo, Rongquan Zhang",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itme56794.2022.00046"
    },
    {
        "id": 16684,
        "title": "Adversarial Sample Generation using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) and Classification for IEEE 802.11 using the Deep Deterministic Policy Gradient (DDPG)",
        "authors": "D. Sudaroli Vijayakumar, Sannasi Ganapathy",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "One of today's most promising developments is wireless networking, as it enables people across the globe to stay connected. As the wireless networks' transmission medium is open, there are potential issues in safeguarding the privacy of the information. Though several security protocols exist in the literature for the preservation of information, most cases fail with a simple spoof attack. So, intrusion detection systems are vital in wireless networks as they help in the identification of harmful traffic. One of the challenges that exist in wireless intrusion detection systems (WIDS) is finding a balance between accuracy and false alarm rate. The purpose of this study is to provide a practical classification scheme for newer forms of attack. The AWID dataset is used in the experiment, which proposes a feature selection strategy using a combination of Elastic Net and recursive feature elimination. The best feature subset is obtained with 22 features, and a deep deterministic policy gradient learning algorithm is then used to classify attacks based on those features. Samples are generated using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) to evaluate classification outcomes using adversarial samples. The meta-analysis reveals improved results in terms of feature production (22 features), classification accuracy (98.75% for testing samples and 85.24% for adversarial samples), and false alarm rates (0.35%). ",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i8.7946"
    },
    {
        "id": 16685,
        "title": "Modeling and Control of Robotic Fish Based on Deep Deterministic Policy Gradientmodeling and Control of Robotic Fish Based on Deep Deterministic Policy Gradient",
        "authors": "Prof. Dong Xu, Fan Qiao, Kaiyang Lu, Yupeng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4630936"
    },
    {
        "id": 16686,
        "title": "Deep Deterministic Policy Gradient (DDPG)-Based Energy Harvesting Wireless Communications",
        "authors": "Chengrun Qiu, Yang Hu, Yan Chen, Bing Zeng",
        "published": "2019-10",
        "citations": 192,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jiot.2019.2921159"
    },
    {
        "id": 16687,
        "title": "Improving Deep Deterministic Policy Gradient with Compact Experience Replay",
        "authors": "Daniel Neves, Lucila Ishitani, Zenilton Patrocínio",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nExperience Replay (ER) improves data efficiency in Deep Reinforcement Learning by allowing the agent to revisit past experiences that could contribute to the current policy learning. A recent method called COMPact Experience Replay (COMPER) seeks to improve ER by reducing the required number of experiences for agent training regarding the total accumulated rewards in the long run. This method can approximate good policies on Atari 2600 games on the Arcade Learning Environment (ALE) from a considerably smaller number of frame observations to achieve similar results obtained by DQN-based methods in the literature, which generally demand millions of video frames. Therefore, we conduct a detailed analysis of its components to verify and understand how they operate and how they could improve data efficiency on the Deep Deterministic Policy Gradient (DDPG) algorithm applied to physical control problems in Mujoco's environments. We present an extension of DDPG, named COMPER-DDPG, which uses components from COMPER and achieves better results than the original method in almost all environments used in the experiments with only 50,000 iterations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3991723/v1"
    },
    {
        "id": 16688,
        "title": "Developmentally Synthesizing Earthworm-Like Locomotion Gaits with Bayesian-Augmented Deep Deterministic Policy Gradients (DDPG)",
        "authors": "Sayyed Jaffar Ali Raza, Apan Dastider, Mingjie Lin",
        "published": "2020-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/case48305.2020.9216782"
    },
    {
        "id": 16689,
        "title": "A deep reinforcement learning model based on deterministic policy gradient for collective neural crest cell migration",
        "authors": "George Lykotrafitis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5f5f8e69aa777f8ba5bd6177"
    },
    {
        "id": 16690,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784.v2"
    },
    {
        "id": 16691,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784"
    },
    {
        "id": 16692,
        "title": "Optimizing UAV Computation Offloading via MEC with Deep Deterministic Policy Gradient",
        "authors": "Muhammad Usman Hadi, Ahmed Bashir Abbasi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Mobile edge computing (MEC) seems to be highly efficient to process the\ngenerated data from IoT devices by providing computational resources\nlocating in close range to network edge. MEC can be promising in\nreduction of latency and consumption of energy from data transmissions\nfrom offloading computational tasks from IoT devices to nearby edge\nservers. In this paper, a computation offloading optimization algorithm\nis proposed which is based on deep deterministic policy gradient for\nrealistic Aurelia X6 Pro unmanned aerial vehicle (UAV)-assisted MEC\nsystems. The proposed algorithm optimizes the offloading decision for\nUAVs by taking task characteristics and the communication environment\ninto consideration. The simulation yields outcomes indicating that the\nsuggested algorithm can considerably enhance the competency of MEC\nsystems.",
        "link": "http://dx.doi.org/10.22541/au.168414484.46468157/v1"
    },
    {
        "id": 16693,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784.v1"
    },
    {
        "id": 16694,
        "title": "Mutual Deep Deterministic Policy Gradient Learning",
        "authors": "Zhou Sun",
        "published": "2022-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdicn55575.2022.00099"
    },
    {
        "id": 16695,
        "title": "Deep Deterministic Policy Gradient Artificial Intelligence for Radar Applications",
        "authors": "Taylor J. Reininger, Graeme E. Smith",
        "published": "2022-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2248738.2022.9764211"
    },
    {
        "id": 16696,
        "title": "Reinforcement Learning with Deep Deterministic Policy Gradient",
        "authors": "Haining Tan",
        "published": "2021-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/caibda53561.2021.00025"
    },
    {
        "id": 16697,
        "title": "SiamTD: Siamese twin delayed deep deterministic policy gradient for rubust Object Tracking",
        "authors": "Yuzhu Wu, Baojie Fan",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727975"
    },
    {
        "id": 16698,
        "title": "Deep Deterministic Policy Gradient for Traffic Signal Control of Single Intersection",
        "authors": "Hali Pang, Weilong Gao",
        "published": "2019-6",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2019.8832406"
    },
    {
        "id": 16699,
        "title": "Residential Demand Response Strategy Based on Deep Deterministic Policy Gradient",
        "authors": "Chunyu Deng, Kehe Wu",
        "published": "2021-4-9",
        "citations": 3,
        "abstract": "With the continuous improvement of the power system and the deepening of electricity market reform, the trend of users’ active participation in power distribution is more and more significant. Demand response has become the promising focus of smart grid research. Providing reasonable incentive strategies for power grid companies and demand response strategies for customers plays a crucial role in maximizing the benefits of different participants. To meet different expectations of multiple agents in the same environment, deep reinforcement learning was adopted. The generative model of residential demand response strategy under different incentive policies can be trained iteratively through real-time interactions with the environmental conditions. In this paper, a novel optimization model of residential demand response strategy, based on a deep deterministic policy gradient (DDPG) algorithm, was proposed. The proposed work was validated with the actual electricity consumption data of a certain area in China. The results showed that the DDPG model could optimize residential demand response strategy under certain incentive policies. In addition, the overall goal of peak load-cutting and valley filling can be achieved, which reflects promising prospects of the electricity market.",
        "link": "http://dx.doi.org/10.3390/pr9040660"
    },
    {
        "id": 16700,
        "title": "Optimal Speed Limit Control for Network Mobility and Safety: A Twin-Delayed Deep Deterministic Policy Gradient Approach",
        "authors": "Fatima Afifah, Zhaomiao Guo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760709"
    },
    {
        "id": 16701,
        "title": "Deep Reinforcement Learning Automatic Landing Control of Fixed-Wing Aircraft Using Deep Deterministic Policy Gradient",
        "authors": "Chi Tang, Ying-Chih Lai",
        "published": "2020-9",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas48674.2020.9213987"
    },
    {
        "id": 16702,
        "title": "Research on Portfolio Optimization Models Using Deep Deterministic Policy Gradient",
        "authors": "Li Wei, Zhang Weiwei",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icris52159.2020.00174"
    },
    {
        "id": 16703,
        "title": "A Task-Oriented Hybrid Routing Approach Based on Deep Deterministic Policy Gradient",
        "authors": "Zongxuan Sha, Ru Huo, Chuang Sun, Shuo Wang, Tao Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4402925"
    },
    {
        "id": 16704,
        "title": "Energy Efficient Double Critic Deep Deterministic Policy Gradient Framework for Fog Computing",
        "authors": "Bhargavi Krishnamurthy, Sajjan G Shiva",
        "published": "2022-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiiot54504.2022.9817157"
    },
    {
        "id": 16705,
        "title": "Non-Cooperative Coupled Microgrid-Transportation Coordination System: A Deep Deterministic Policy Gradient-Based Approach",
        "authors": "Shiyao Zhang, Xingzheng Zhu, Shengyu Zhang, James J.Q. Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4678589"
    },
    {
        "id": 16706,
        "title": "Intelligent path planning of mobile robot based on Deep Deterministic Policy Gradient",
        "authors": "Hui Gong, Peng Wang, Cui Ni, Nuo Cheng, Hua Wang",
        "published": "No Date",
        "citations": 6,
        "abstract": "Abstract\nDeep Deterministic Policy Gradient (DDPG) is a deep reinforcement learning algorithm that is widely used in the path planning of mobile robots. It solves the continuous action space problem and can ensure the continuity of mobile robot motion using the Actor-Critic framework, which has great potential in the field of mobile robot path planning. However, because the Critic network always selects the maximum Q value to evaluate the actions of mobile robot, there is the problem of inaccurate Q value estimation. In addition, DDPG adopts a random uniform sampling method, which can’t efficiently use the more important sample data, resulting in slow convergence speed during the training of the path planning model and easily falling into local optimum. In this paper, a dueling network is introduced based on DDPG to improve the estimation accuracy of the Q value, and the reward function is optimized to increase the immediate reward, to direct the mobile robot to move faster toward the target point. To further improve the efficiency of experience replay, a single experience pool is separated into two by comprehensively considering the influence of average reward and TD-error on the importance of samples, and a dynamic adaptive sampling mechanism is adopted to sample the two experience pools separately. Finally, experiments were carried out in the simulation environment created with the ROS system and the Gazebo platform. The results of the experiments show that the proposed path planning algorithm has a fast convergence speed and high stability, and the success rate can reach 100% and 93% in the environment without obstacles and with obstacles, respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2201974/v1"
    },
    {
        "id": 16707,
        "title": "Deep deterministic policy gradient-based combustion optimization method for coal-fired boiler",
        "authors": "Xuqi Bao, Yiguo Li, Chuanpeng Zhu",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549318"
    },
    {
        "id": 16708,
        "title": "Controlling bicycle using deep deterministic policy gradient algorithm",
        "authors": "Le Pham Tuyen, TaeChoong Chung",
        "published": "2017-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/urai.2017.7992765"
    },
    {
        "id": 16709,
        "title": "MTMA-DDPG: A Deep Deterministic Policy Gradient Reinforcement Learning for Multi-task Multi-agent Environments",
        "authors": "Karim Hamadeh, Julia El Zini, Joudi Hajar, Mariette Awad",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-08333-4_22"
    },
    {
        "id": 16710,
        "title": "SFNAS-DDPG: A Biomass-Based Energy Hub Dynamic Scheduling Approach via Connecting Supervised Federated Neural Architecture Search and Deep Deterministic Policy Gradient",
        "authors": "Amirhossein Dolatabadi, Hussein Abdeltawab, Yasser Abdel-Rady I. Mohamed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3352032"
    },
    {
        "id": 16711,
        "title": "Deep Deterministic Policy Gradient in Acoustic to Articulatory Inversion",
        "authors": "Farzane Abdoli, Hamid Sheikhzadeh, Vahid Pourahmadi",
        "published": "2022-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke57176.2022.9959976"
    },
    {
        "id": 16712,
        "title": "APPLICATION DEVELOPMENT FOR MUSIC RECOMMENDATION SYSTEM USING DEEP DETERMINISTIC POLICY GRADIENT",
        "authors": "",
        "published": "2021-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33965/icwi_ac2021_202109c027"
    },
    {
        "id": 16713,
        "title": "Hierarchical Demand Response Considering Dynamic Competing Interaction Based on Multi-Agent Deep Deterministic Policy Gradient",
        "authors": "J. H. Zheng, W.H. Wang, Zhigang Li, Q.H. Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4521920"
    },
    {
        "id": 16714,
        "title": "COLREGs-compliant dynamic collision avoidance algorithm based on deep deterministic policy gradient",
        "authors": "",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56042/ijms.v50i11.66794"
    },
    {
        "id": 16715,
        "title": "Deep Deterministic Policy Gradient for Nested Parallel Negotiation",
        "authors": "Ryota Arakawa, Katsuhide Fujita",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wi-iat59888.2023.00032"
    },
    {
        "id": 16716,
        "title": "Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient",
        "authors": "Ming Zhan, Jingjing Fan, Jianying Guo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3305453"
    },
    {
        "id": 16717,
        "title": "Deep Reinforcement Learning with Robust Deep Deterministic Policy Gradient",
        "authors": "Teckchai Tiong, Ismail Saad, Kenneth Tze Kin Teo, Herwansyah bin Lago",
        "published": "2020-11-28",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecie50279.2020.9309539"
    },
    {
        "id": 16718,
        "title": "Local Path Planning with Turnabouts for Mobile Robot by Deep Deterministic Policy Gradient",
        "authors": "Tomoaki Nakamura, Masato Kobayashi, Naoki Motoi",
        "published": "2023-3-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icm54990.2023.10101921"
    },
    {
        "id": 16719,
        "title": "Deep Deterministic Policy Gradient Based Dynamic Power Control for Self-Powered Ultra-Dense Networks",
        "authors": "Han Li, Tiejun Lv, Xuewei Zhang",
        "published": "2018-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocomw.2018.8644157"
    },
    {
        "id": 16720,
        "title": "Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge",
        "authors": "Bolun Zhang, Nguyen Van Huynh",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436824"
    },
    {
        "id": 16721,
        "title": "A multi-critic deep deterministic policy gradient UAV path planning",
        "authors": "Runjia Wu, Fangqing Gu, Jie Huang",
        "published": "2020-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cis52066.2020.00010"
    },
    {
        "id": 16722,
        "title": "Multi-AUV Charging Navigation Trajectory Planning Based on Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Jiaming Yu, Hao Sun, Qinglin Sun",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240061"
    },
    {
        "id": 16723,
        "title": "Optimal Coordination of Distributed Energy Resources Using Deep Deterministic Policy Gradient",
        "authors": "Avijit Das, Di Wu",
        "published": "2022-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eesat55007.2022.9998046"
    },
    {
        "id": 16724,
        "title": "Deep Deterministic Policy Gradient Based Computation Offloading in Wireless-Powered MEC Networks",
        "authors": "Ruoqi Liu, Xuanlin Liu, Sihua Wang, Changchuan Yin",
        "published": "2020-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps50303.2020.9367589"
    },
    {
        "id": 16725,
        "title": "Inverted Pendulum Control using Twin Delayed Deep Deterministic Policy Gradient with a Novel Reward Function",
        "authors": "Manas Shil, G. N. Pillai",
        "published": "2022-2-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/delcon54057.2022.9752797"
    },
    {
        "id": 16726,
        "title": "Experiment of Cooperative Transportation using Multi-Robots by Multi-agent Deep Deterministic Policy Gradient",
        "authors": "Kazuhi Murata, Kenta Miyazaki, Nobutomo Matsunaga",
        "published": "2022-5-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ascc56756.2022.9828156"
    },
    {
        "id": 16727,
        "title": "Limited Log-Distance Path Loss Model Path Loss Exponent Estimation using Deep Deterministic Policy Gradient",
        "authors": "David P. Grabowsky, James M. Conrad, Aidan F. Browne",
        "published": "2021-3-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon45413.2021.9401929"
    },
    {
        "id": 16728,
        "title": "Compensation Control of UAV Based on Deep Deterministic Policy Gradient",
        "authors": "Zijun Xu, Juntong Qi, Mingming Wang, Chong Wu, Guang Yang",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902417"
    },
    {
        "id": 16729,
        "title": "A Manipulator Control Method Based on Deep Deterministic Policy Gradient with Parameter Noise",
        "authors": "Haifei Zhang, Xu Jian, Liting Lei, Fang Wu, Lanmei Qian, Jianlin Qiu",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nFocusing on the motion control problem of two link manipulator, a manipulator control approach based on deep deterministic policy gradient with parameter noise is proposed. Firstly, the manipulator simulation environment is built. And then the three deep reinforcement learning models named the deep deterministic policy gradient (DDPG), asynchronous advantage actor-critical (A3C) and distributed proximal policy optimization (DPPO) are established for training according to the target setting, state variables and reward & punishment mechanism of the environment model. Finally the motion control of two link manipulator is realized. After comparing and analyzing the three models, the DDPG approach based on parameter noise is proposed for further research to improve its applicability, so as to cut down the debugging time of the manipulator model and reach the goal smoothly. The experimental results indicate that the DDPG approach based on parameter noise can control the motion of two link manipulator effectively. The convergence speed of the control model is significantly promoted and the stability after convergence is improved. In comparison with the traditional control approach, the DDPG control approach based on parameter noise has higher efficiency and stronger applicability.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1016003/v1"
    },
    {
        "id": 16730,
        "title": "Continuous Control of a Robot Manipulator Using Deep Deterministic Policy Gradient",
        "authors": "Maithili Shetty, Brunda Vishishta, Shrinidhi Choragi, Karpagavalli Subramanian, Koshy George",
        "published": "2021-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc54714.2021.9703155"
    },
    {
        "id": 16731,
        "title": "Network Architecture Reasoning Via Deep Deterministic Policy Gradient",
        "authors": "Huidong Liu, Fang Du, Xiaofen Tang, Hao Liu, Zhenhua Yu",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme46284.2020.9102834"
    },
    {
        "id": 16732,
        "title": "Autonomous Handover Parameter Optimisation for 5g Cellular Networks Using Deep Deterministic Policy Gradient",
        "authors": "Chiew Foong Kwong, Chenhao Shi, Qianyu Liu, Sen Yang, David Chieng, Pushpendu Kar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374897"
    },
    {
        "id": 16733,
        "title": "Deep Deterministic Policy Gradient for Magnetic Levitation Control",
        "authors": "Sarawan Wongsa, Nitis Kowkasai",
        "published": "2020-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecti-con49241.2020.9158096"
    },
    {
        "id": 16734,
        "title": "Unmanned Aerial Vehicles Control Study Using Deep Deterministic Policy Gradient",
        "authors": "Dan Sun, Dong Gao, Jianhua Zheng, Peng Han",
        "published": "2018-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gncc42960.2018.9018682"
    },
    {
        "id": 16735,
        "title": "A Deep Deterministic Policy Gradient-based Strategy for Stocks Portfolio Management",
        "authors": "Huanming Zhang, Zhengyong Jiang, Jionglong Su",
        "published": "2021-3-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icbda51983.2021.9403049"
    },
    {
        "id": 16736,
        "title": "Continuous Control for Automated Lane Change Behavior Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Pin Wang, Hanhan Li, Ching-Yao Chan",
        "published": "2019-6",
        "citations": 40,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2019.8813903"
    },
    {
        "id": 16737,
        "title": "Improvement of PMSM Control Using Reinforcement Learning Deep Deterministic Policy Gradient Agent",
        "authors": "Marcel Nicola, Claudiu-Ionel Nicola",
        "published": "2021-10-27",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ee53374.2021.9628371"
    },
    {
        "id": 16738,
        "title": "Enhanced Fingerprint Image Compression using Deep Deterministic Policy Gradient",
        "authors": "Abdelhak Ouanane, Mohamed Riad Yagoubi, Amina Serir, Nacereddine Djelal",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeat60471.2023.10425859"
    },
    {
        "id": 16739,
        "title": "Twin-Delayed Deep Deterministic Policy Gradient Algorithm for Portfolio Selection",
        "authors": "Nicholas Baard, Terence L. van Zyl",
        "published": "2022-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cifer52523.2022.9776067"
    },
    {
        "id": 16740,
        "title": "Smart Noise Jamming Power Adjustment Using Exploratory Deep Deterministic Policy Gradient",
        "authors": "Yujie Zhang, Weibo Huo, Cui Zhang, Jifang Pei, Yin Zhang, Yulin Huang",
        "published": "2023-5-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149662"
    },
    {
        "id": 16741,
        "title": "Quadrotor Attitude Control Based on Deep Deterministic Policy Gradient with Hindsight Experience Replay",
        "authors": "Sangmin Lee, Seong-Hun Kim, Hanna Lee, Youdan Kim",
        "published": "2022-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ascc56756.2022.9828254"
    },
    {
        "id": 16742,
        "title": "Deep Deterministic Policy Gradient for Throughput Maximization in Energy Harvesting NOMA-Cognitive Radio Network",
        "authors": "Lav Garg, Saikat Majumder, Sumit Chakravarty",
        "published": "2023-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iconat57137.2023.10080443"
    },
    {
        "id": 16743,
        "title": "Synthesized Prioritized Data Pruning based Deep Deterministic Policy Gradient Algorithm Improvement",
        "authors": "Hui Xiang, Jun Cheng, Qieshi Zhang, Jianming Liu",
        "published": "2018-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icinfa.2018.8812510"
    },
    {
        "id": 16744,
        "title": "Power Control in Device-to-Device Communications using Deep Deterministic Policy Gradient Method",
        "authors": "Ranjeet Kumar, Saikat Majumder",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112096"
    },
    {
        "id": 16745,
        "title": "Intrinsic Motivation for Deep Deterministic Policy Gradient in Multi-Agent Environments",
        "authors": "Xiaoge Cao, Tao Lu, Yinghao Cai",
        "published": "2020-11-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327573"
    },
    {
        "id": 16746,
        "title": "A Novel Vehicle Platoon Following Controller Based on Deep Deterministic Policy Gradient Algorithms",
        "authors": "Guan Wang, Jianming Hu, Yusen Huo, Zuo Zhang",
        "published": "2018-7-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784481523.008"
    },
    {
        "id": 16747,
        "title": "Power Allocation for Full-Duplex Communication Systems Based on Deep Deterministic Policy Gradient",
        "authors": "Jin Qu, Congliang Zhu, Shengli Liu, Guanding Yu, Rui Yin",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps50303.2020.9367558"
    },
    {
        "id": 16748,
        "title": "Hybrid Energy Storage Control Method For DC Microgrid Based On Deep Deterministic Policy Gradient",
        "authors": "Shengji Tan, Min Ding, Zili Tao, Danyun Li, Zhijian Fang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450954"
    },
    {
        "id": 16749,
        "title": "Deep Deterministic Gradient Policy (DDGP) Reinforcement Learning Assisted Degradation-Aware Control of Solid-State Transformer",
        "authors": "Moinul Shahidul Haque, Seungdeog Choi",
        "published": "2021-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec42165.2021.9487287"
    },
    {
        "id": 16750,
        "title": "Enhancing Twin Delayed Deep Deterministic Policy Gradient with Cross-Entropy Method",
        "authors": "Hieu Trung Nguyen, Khang Tran, Ngoc Hoang Luong",
        "published": "2021-12-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nics54270.2021.9701549"
    },
    {
        "id": 16751,
        "title": "Multi-Agent Deep Deterministic Policy Gradient for Traffic Signal Control on Urban Road Network",
        "authors": "Shuyang Li",
        "published": "2020-8",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeeca49918.2020.9213523"
    },
    {
        "id": 16752,
        "title": "Dynamic Prioritization and Adaptive Scheduling Using Deep Deterministic Policy Gradient for Deploying Microservice-Based VNFs",
        "authors": "Swarna B. Chetty, Hamed Ahmadi, Avishek Nag",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278718"
    },
    {
        "id": 16753,
        "title": "The real-time optimization of active distribution system based on deep deterministic policy gradient",
        "authors": " Jin-Xia Gong,  Guangyin Mei,  Yan-Min Liu",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/cp.2019.0545"
    },
    {
        "id": 16754,
        "title": "Deep Deterministic Policy Gradient-based intelligent control scheme design for DC-DC circuit",
        "authors": "Ligong Zhang, Xinhui Zhu, Chenyang Bai, Junshan Li",
        "published": "2021-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icamechs54019.2021.9661500"
    },
    {
        "id": 16755,
        "title": "Hybrid Formation Control for Multi-Robot Hunters Based on Multi-Agent  Deep Deterministic Policy Gradient",
        "authors": "Oussama Hamed, Mohamed Hamlich",
        "published": "2021-12-21",
        "citations": 0,
        "abstract": "The cooperation between mobile robots is one of the most important topics of interest to researchers, especially in the many areas in which it can be applied. Hunting a moving target with random behavior is an application that requires robust cooperation between several robots in the multi-robot system. This paper proposed a hybrid formation control for hunting a dynamic target which is based on wolves’ hunting behavior in order to search and capture the prey quickly and avoid its escape and Multi Agent Deep Deterministic Policy Gradient (MADDPG) to plan an optimal accessible path to the desired position. The validity and the effectiveness of the proposed formation control are demonstrated with simulation results.",
        "link": "http://dx.doi.org/10.13164/mendel.2021.2.023"
    },
    {
        "id": 16756,
        "title": "Research on Automatic Lane Changing Method for Electric Vehicles Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060104"
    },
    {
        "id": 16757,
        "title": "Deep Deterministic Policy Gradient Algorithm Based on Convolutional Block Attention for Autonomous Driving",
        "authors": "Yanliang Jin, Qianhong Liu, Liquan Shen, Leiji Zhu",
        "published": "2021-6-12",
        "citations": 1,
        "abstract": "The research on autonomous driving based on deep reinforcement learning algorithms is a research hotspot. Traditional autonomous driving requires human involvement, and the autonomous driving algorithms based on supervised learning must be trained in advance using human experience. To deal with autonomous driving problems, this paper proposes an improved end-to-end deep deterministic policy gradient (DDPG) algorithm based on the convolutional block attention mechanism, and it is called multi-input attention prioritized deep deterministic policy gradient algorithm (MAPDDPG). Both the actor network and the critic network of the model have the same structure with symmetry. Meanwhile, the attention mechanism is introduced to help the vehicles focus on useful environmental information. The experiments are conducted in the open racing car simulator (TORCS)and the results of five experiment runs on the test tracks are averaged to obtain the final result. Compared with the state-of-the-art algorithm, the maximum reward increases from 62,207 to 116,347, and the average speed increases from 135 km/h to 193 km/h, while the number of success episodes to complete a circle increases from 96 to 147. Also, the variance of the distance from the vehicle to the center of the road is compared, and the result indicates that the variance of the DDPG is 0.6 m while that of the MAPDDPG is only 0.2 m. The above results indicate that the proposed MAPDDPG achieves excellent performance.",
        "link": "http://dx.doi.org/10.3390/sym13061061"
    },
    {
        "id": 16758,
        "title": "A Dosing Strategy Model of Deep Deterministic Policy Gradient Algorithm for Sepsis",
        "authors": "Tianlai Lin, Xinjue Zhang, Jianbing Gong, Rundong Tan, Xiang Xu, Lijun Wang, Yingxia Pan, junhui Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackgroundA growing body of research indicates that the use of computerized decision support systems can better guide disease treatment and save resources. An artificial intelligence (AI) clinical decision system may be a useful tool to predict how different dose combinations affect survival outcomes in patients with sepsis. We boldly use Deep Deterministic Policy Gradient (DDPG) algorithm to improve the sepsis AI clinical decision system developed by MIT and IMPERIAL College London (ICL) team. We believe that although the system has been verified by data, due to the limitations of the core algorithm, it cannot meet the needs of clinical use, it could also be that the model of training is not stable. The purpose of this study was to establish a more effective AI clinician decision-making system to guide sepsis treatment.MethodsWe chose the same data model as the MIT team for experiments. these datasets are from the publicly available Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-III v1.4) database. We utilized hospitalization information for 38,600 adult patients over 15 years of age。we adopted DDPG (a strategy-based reinforcement learning method) to build an artificial intelligence clinician model.ResultsCompared with the sepsis AI clinician decision model developed by MIT based on DNQ algorithm, the sepsis AI clinician decision model developed by DDPG algorithm had a faster convergence rate, was closer to the clinician decision, and had a higher survival rate.ConclusionsThe AI clinician decision system based on DDPG algorithm can better help the ICU (Intensive Care Unit ) clinicians to cope with the changes in the condition of sepsis patients, reduce the workload, and provide a reference dosage.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1391838/v1"
    },
    {
        "id": 16759,
        "title": "Path planning based on improved Deep Deterministic Policy Gradient algorithm",
        "authors": "Yandong Liu, Wenzhi Zhang, Fumin Chen, Jianliang Li",
        "published": "2019-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itnec.2019.8729369"
    },
    {
        "id": 16760,
        "title": "A Multi-Agent Deep Deterministic Policy Gradient Method for Multi-Zone HVAC Control",
        "authors": "Xuebo Liu, Yingying Wu, Bo Liu, Hongyu Wu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10252541"
    },
    {
        "id": 16761,
        "title": "Control Method for PEMFC Using Improved Deep Deterministic Policy Gradient Algorithm",
        "authors": "Jiawen Li, Yaping Li, Tao Yu",
        "published": "2021-9-30",
        "citations": 1,
        "abstract": "A data-driven PEMFC output voltage control method is proposed. Moreover, an Improved deep deterministic policy gradient algorithm is proposed for this method. The algorithm introduces three techniques: Clipped multiple Q-learning, policy delay update, and policy smoothing to improve the robustness of the control policy. In this algorithm, the hydrogen controller is treated as an agent, which is pre-trained to fully interact with the environment and obtain the optimal control policy. The effectiveness of the proposed algorithm is demonstrated experimentally.",
        "link": "http://dx.doi.org/10.3389/fenrg.2021.753064"
    },
    {
        "id": 16762,
        "title": "Developing Flight Control Policy Using Deep Deterministic Policy Gradient",
        "authors": "Antonios Tsourdos, Ir. Adhi Dharma Permana, Dewi H. Budiarti, Hyo-Sang Shin, Chang-Hun Lee",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icares.2019.8914343"
    },
    {
        "id": 16763,
        "title": "Duplicated Replay Buffer for Asynchronous Deep Deterministic Policy Gradient",
        "authors": "Seyed Mohammad Seyed Motehayeri, Vahid Baghi, Ehsan Maani Miandoab, Ali Moeini",
        "published": "2021-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csicc52343.2021.9420550"
    },
    {
        "id": 16764,
        "title": "A Concept of Unbiased Deep Deterministic Policy Gradient for Better Convergence in Bipedal Walker",
        "authors": "Timur Ishuov, Zhenis Otarbay, Michele Folgheraiter",
        "published": "2022-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sist54437.2022.9945743"
    },
    {
        "id": 16765,
        "title": "Real-Time Optimal Dispatch of Microgrid Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Weidong Chen, Ning Wu, Yanlu Huang",
        "published": "2021-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdidm53834.2021.00012"
    },
    {
        "id": 16766,
        "title": "Target tracking strategy using deep deterministic policy gradient",
        "authors": "Shixun You, Ming Diao, Lipeng Gao, Fulong Zhang, Huan Wang",
        "published": "2020-10",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2020.106490"
    },
    {
        "id": 16767,
        "title": "Software-Defined Networking Application with Deep Deterministic Policy Gradient",
        "authors": "Joseph Nathanael Witanto, Hyotaek Lim",
        "published": "2019-1-16",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3307363.3307404"
    },
    {
        "id": 16768,
        "title": "Mitigation of Flooding in Stormwater Systems Utilizing Imperfect Forecasting and Sensor Data with Deep Deterministic Policy Gradient Reinforcement Learning",
        "authors": "Sami Saliba, Benjamin Bowes, Stephen Adams, Peter Beling, Jonathan Goodall",
        "published": "No Date",
        "citations": 1,
        "abstract": "Climate change and development have increased urban flooding, requiring modernization of stormwater infrastructure. Retrofitting standard passive systems with controllable valves/pumps is promising, but requires real-time control (RTC). One method of automating RTC is reinforcement learning (RL), a general technique for sequential optimization and control in uncertain environments. The notion is that an RL algorithm can use inputs of real-time flood data and rainfall forecasts to learn a policy for controlling the stormwater infrastructure to minimize measures of flooding. In real-world conditions, rainfall forecasts and other state information, are subject to noise and uncertainty. To account for these characteristics of the problem data, we implemented Deep Deterministic Policy Gradient (DDPG), an RL algorithm that is distinguished by its capability to handle noise in the input data. DDPG implementations were trained and tested against a passive flood control policy. Three primary cases were studied: (i) perfect data, (ii) imperfect rainfall forecasts, and (iii) imperfect water level and forecast data. Rainfall episodes (100) that caused flooding in the passive system were selected from 10 years of observations in Norfolk, Virginia, USA; 85 randomly selected episodes were used for training and the remaining 15 unseen episodes served as test cases. Compared to the passive system, all RL implementations reduced flooding volume by 70.5% on average, and performed within a range of 5%. This suggests that DDPG is robust to noisy input data, which is essential knowledge to advance the real-world applicability of RL for stormwater RTC.",
        "link": "http://dx.doi.org/10.20944/preprints202010.0413.v1"
    },
    {
        "id": 16769,
        "title": "Autonomous Vehicle Driving via Deep Deterministic Policy Gradient",
        "authors": "Wenhui Huang, Francesco Braghin, Stefano Arrigoni",
        "published": "2019-8-18",
        "citations": 3,
        "abstract": "Abstract\nAutonomous driving has became one of the most hot trends in artificial intelligence area in recent years thanks to the machine learning algorithms. However, most of the autonomous driving studies are still limited to discrete action space. In this study, we propose to implement Deep Deterministic Policy Gradient algorithm for learning driving behavior over the continuous actions. For this purpose, a driving simulator is employed which interfaces with IPG CarMker software where the virtual environment and dynamical vehicle model can be built. “Human-in-the-loop” is performed in order to gather the data and a neural network which is implemented in Behavior Layer is trained to recognize two different scenarios-forward driving and stop. Based on the scenario the agent is dealing with, the actions are learnt and suggested from the DDPG algorithm. The experimental results show that DDPG algorithm is able to learn the optimal policy with continuous actions reliably for both scenarios.",
        "link": "http://dx.doi.org/10.1115/detc2019-97884"
    },
    {
        "id": 16770,
        "title": "Path Planning of Humanoid Arm Based on Deep Deterministic Policy Gradient",
        "authors": "Shuhuan Wen, Jianhua Chen, Shen Wang, Hong Zhang, Xueheng Hu",
        "published": "2018-12",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio.2018.8665248"
    }
]