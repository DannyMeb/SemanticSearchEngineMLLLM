[
    {
        "id": 22171,
        "title": "Parallel phonetically aware DNNs and LSTM-RNNS for frame-by-frame discriminative modeling of spoken language identification",
        "authors": "Ryo Masumura, Taichi Asami, Hirokazu Masataki, Yushi Aono",
        "published": "2017-3",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2017.7953160"
    },
    {
        "id": 22172,
        "title": "Supporting Business Process Modeling Using RNNs for Label Classification",
        "authors": "Philip Hake, Manuel Zapp, Peter Fettke, Peter Loos",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-59569-6_35"
    },
    {
        "id": 22173,
        "title": "Text Summarization in Assamese Language using Sequence to Sequence RNNs",
        "authors": "Pritom Jyoti Goutom,  , Nomi Baruah",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17485/ijst/v16isp2.5429"
    },
    {
        "id": 22174,
        "title": "Code-switched Language Models Using Dual RNNs and Same-Source Pretraining",
        "authors": "Saurabh Garg, Tanmay Parekh, Preethi Jyothi",
        "published": "2018",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d18-1346"
    },
    {
        "id": 22175,
        "title": "Learning Context-free Languages with Nondeterministic Stack RNNs",
        "authors": "Brian DuSell, David Chiang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.conll-1.41"
    },
    {
        "id": 22176,
        "title": "Mimicking Word Embeddings using Subword RNNs",
        "authors": "Yuval Pinter, Robert Guthrie, Jacob Eisenstein",
        "published": "2017",
        "citations": 54,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1010"
    },
    {
        "id": 22177,
        "title": "Diagonal rnns in symbolic music modeling",
        "authors": "Y. Cem Subakan, Paris Smaragdis",
        "published": "2017-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/waspaa.2017.8170054"
    },
    {
        "id": 22178,
        "title": "Hierarchical RNNs for Waveform-Level Speech Synthesis",
        "authors": "Qingyun Dou, Moquan Wan, Gilles Degottex, Zhiyi Ma, Mark J.F. Gales",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt.2018.8639588"
    },
    {
        "id": 22179,
        "title": "Time-Aware Multi-Scale RNNs for Time Series Modeling",
        "authors": "Zipeng Chen, Qianli Ma, Zhenxi Lin",
        "published": "2021-8",
        "citations": 10,
        "abstract": "Multi-scale information is crucial for modeling time series. Although most existing methods consider multiple scales in the time-series data, they assume all kinds of scales are equally important for each sample, making them unable to capture the dynamic temporal patterns of time series. To this end, we propose Time-Aware Multi-Scale Recurrent Neural Networks (TAMS-RNNs), which disentangle representations of different scales and adaptively select the most important scale for each sample at each time step. First, the hidden state of the RNN is disentangled into multiple independently updated small hidden states, which use different update frequencies to model time-series multi-scale information. Then, at each time step, the temporal context information is used to modulate the features of different scales, selecting the most important time-series scale. Therefore, the proposed model can capture the multi-scale information for each time series at each time step adaptively. Extensive experiments demonstrate that the model outperforms state-of-the-art methods on multivariate time series classification and human motion prediction tasks. Furthermore, visualized analysis on music genre recognition verifies the effectiveness of the model.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/315"
    },
    {
        "id": 22180,
        "title": "Applying RNNs Architecture by Jointly Learning Segmentation and Stemming for Myanmar Language",
        "authors": "Yadanar Oo, Khin Mar Soe",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce46687.2019.9015315"
    },
    {
        "id": 22181,
        "title": "On-Device End-to-end Speech Recognition with Multi-Step Parallel Rnns",
        "authors": "Yoonho Boo, Jinhwan Park, Lukas Lee, Wonyong Sung",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt.2018.8639662"
    },
    {
        "id": 22182,
        "title": "Comparison of Static Neural Network with External Memory and RNNs for Deterministic Context Free Language Learning",
        "authors": "Ying Ma, Jose Principe",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489240"
    },
    {
        "id": 22183,
        "title": "Exploring the Syntactic Abilities of RNNs with Multi-task Learning",
        "authors": "Émile Enguehard, Yoav Goldberg, Tal Linzen",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/k17-1003"
    },
    {
        "id": 22184,
        "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image\n            Caption Generator?",
        "authors": "Marc Tanti, Albert Gatt, Kenneth Camilleri",
        "published": "2017",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-3506"
    },
    {
        "id": 22185,
        "title": "Financial Forecasting With α-RNNs: A Time Series Modeling Approach",
        "authors": "Matthew Dixon, Justin London",
        "published": "2021-2-11",
        "citations": 10,
        "abstract": "The era of modern financial data modeling seeks machine learning techniques which are suitable for noisy and non-stationary big data. We demonstrate how a general class of exponential smoothed recurrent neural networks (α-RNNs) are well suited to modeling dynamical systems arising in big data applications such as high frequency and algorithmic trading. Application of exponentially smoothed RNNs to minute level Bitcoin prices and CME futures tick data, highlight the efficacy of exponential smoothing for multi-step time series forecasting. Our α-RNNs are also compared with more complex, “black-box”, architectures such as GRUs and LSTMs and shown to provide comparable performance, but with far fewer model parameters and network complexity.",
        "link": "http://dx.doi.org/10.3389/fams.2020.551138"
    },
    {
        "id": 22186,
        "title": "Actionable Email Intent Modeling With Reparametrized RNNs",
        "authors": "Chu-Cheng Lin, Dongyeop Kang, Michael Gamon, Patrick Pantel",
        "published": "2018-4-26",
        "citations": 3,
        "abstract": "\n      \n        Emails in the workplace are often intentional calls to action for its recipients. We propose to annotate these emails for what action its recipient will take. We argue that our approach of action-based annotation is more scalable and theory-agnostic than traditional speech-act-based email intent annotation, while still carrying important semantic and pragmatic information. We show that our action-based annotation scheme achieves good inter-annotator agreement. We also show that we can leverage threaded messages from other domains, which exhibit comparable intents in their conversation, with domain adaptive RAINBOW (Recurrently AttentIve Neural Bag-Of-Words). On a collection of datasets consisting of IRC, Reddit, and email, our reparametrized RNNs outperform common multitask/multidomain approaches on several speech act related tasks. We also experiment with a minimally supervised scenario of email recipient action classification, and find the reparametrized RNNs learn a useful representation.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.11931"
    },
    {
        "id": 22187,
        "title": "RNNs can generate bounded hierarchical languages with optimal memory",
        "authors": "John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, Christopher D. Manning",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.156"
    },
    {
        "id": 22188,
        "title": "A comprehensive study of deep bidirectional LSTM RNNS for acoustic modeling in speech recognition",
        "authors": "Albert Zeyer, Patrick Doetsch, Paul Voigtlaender, Ralf Schluter, Hermann Ney",
        "published": "2017-3",
        "citations": 70,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2017.7952599"
    },
    {
        "id": 22189,
        "title": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition",
        "authors": "Gail Weiss, Yoav Goldberg, Eran Yahav",
        "published": "2018",
        "citations": 61,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p18-2117"
    },
    {
        "id": 22190,
        "title": "Finetuning Pretrained Transformers into RNNs",
        "authors": "Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.830"
    },
    {
        "id": 22191,
        "title": "Unified Modeling Language-Geoframe Modeling Language",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_101435"
    },
    {
        "id": 22192,
        "title": "Quantifying Long Range Dependence in Language and User Behavior to improve RNNs",
        "authors": "Francois Belletti, Minmin Chen, Ed H. Chi",
        "published": "2019-7-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3292500.3330944"
    },
    {
        "id": 22193,
        "title": "Visualizing memorization in RNNs",
        "authors": "Andreas Madsen",
        "published": "2019-3-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23915/distill.00016"
    },
    {
        "id": 22194,
        "title": "Modeling asynchronous event sequences with RNNs",
        "authors": "Stephen Wu, Sijia Liu, Sunghwan Sohn, Sungrim Moon, Chung-il Wi, Young Juhn, Hongfang Liu",
        "published": "2018-7",
        "citations": 37,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2018.05.016"
    },
    {
        "id": 22195,
        "title": "The Comparison of Word Embedding Techniques in RNNs for Vulnerability Detection",
        "authors": "Hai Nguyen, Songpon Teerakanok, Atsuo Inomata, Tetsutaro Uehara",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010232301090120"
    },
    {
        "id": 22196,
        "title": "Sparse RNNs can support high-capacity classification",
        "authors": "Denis Turcu, L. F. Abbott",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractFeedforward network models performing classification tasks rely on highly convergent output units that collect the information passed on by preceding layers. Although convergent output-unit like neurons may exist in some biological neural circuits, notably the cerebellar cortex, neocortical circuits do not exhibit any obvious candidates for this role; instead they are highly recurrent. We investigate whether a sparsely connected recurrent neural network (RNN) can perform classification in a distributed manner without ever bringing all of the relevant information to a single convergence site. Our model is based on a sparse RNN that performs classification dynamically. Specifically, the interconnections of the RNN are trained to resonantly amplify the magnitude of responses to some external inputs but not others. The amplified and non-amplified responses then form the basis for binary classification. Furthermore, the network acts as an evidence accumulator and maintains its decision even after the input is turned off. Despite highly sparse connectivity, learned recurrent connections allow input information to flow to every neuron of the RNN, providing the basis for distributed computation. In this arrangement, the minimum number of synapses per neuron required to reach maximum memory capacity scales only logarithmically with network size. The model is robust to various types of noise, works with different activation and loss functions and with both backpropagation- and Hebbian-based learning rules. The RNN can also be constructed with a split excitation-inhibition architecture with little reduction in performance.",
        "link": "http://dx.doi.org/10.1101/2022.05.18.492540"
    },
    {
        "id": 22197,
        "title": "An Introduction to Recurrent Neural Networks (RNNs)",
        "authors": " ",
        "published": "No Date",
        "citations": 0,
        "abstract": "Understanding how RNNs work and its applications <strong> <strong> Author </strong> </strong> Wenyi Pi https://orcid.org/0009-0002-2884-2771 <strong> Introduction </strong> In the ever-evolving landscape of artificial intelligence (AI), bridging the gap between humans and machines has seen remarkable progress. Researchers and enthusiasts alike have tirelessly worked across numerous aspects of this field, bringing about amazing advancements.",
        "link": "http://dx.doi.org/10.59350/gjsvh-zbk81"
    },
    {
        "id": 22198,
        "title": "Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?",
        "authors": "Kevin Blin,  , Andrei Kucharavy,  ",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-072-4_021"
    },
    {
        "id": 22199,
        "title": "Molecular language models: RNNs or transformer?",
        "authors": "Yangyang Chen, Zixu Wang, Xiangxiang Zeng, Yayang Li, Pengyong Li, Xiucai Ye, Tetsuya Sakurai",
        "published": "2023-7-17",
        "citations": 1,
        "abstract": "Abstract\nLanguage models have shown the capacity to learn complex molecular distributions. In the field of molecular generation, they are designed to explore the distribution of molecules, and previous studies have demonstrated their ability to learn molecule sequences. In the early times, recurrent neural networks (RNNs) were widely used for feature extraction from sequence data and have been used for various molecule generation tasks. In recent years, the attention mechanism for sequence data has become popular. It captures the underlying relationships between words and is widely applied to language models. The Transformer-Layer, a model based on a self-attentive mechanism, also shines the same as the RNN-based model. In this research, we investigated the difference between RNNs and the Transformer-Layer to learn a more complex distribution of molecules. For this purpose, we experimented with three different generative tasks: the distributions of molecules with elevated scores of penalized LogP, multimodal distributions of molecules and the largest molecules in PubChem. We evaluated the models on molecular properties, basic metrics, Tanimoto similarity, etc. In addition, we applied two different representations of the molecule, SMILES and SELFIES. The results show that the two language models can learn complex molecular distributions and SMILES-based representation has better performance than SELFIES. The choice between RNNs and the Transformer-Layer needs to be based on the characteristics of dataset. RNNs work better on data focus on local features and decreases with multidistribution data, while the Transformer-Layer is more suitable when meeting molecular with larger weights and focusing on global features.",
        "link": "http://dx.doi.org/10.1093/bfgp/elad012"
    },
    {
        "id": 22200,
        "title": "Learn Computer Vision Using OpenCV",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2"
    },
    {
        "id": 22201,
        "title": "Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles",
        "authors": "Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob Zwart, Michael Steinbach, Vipin Kumar",
        "published": "2019-5-6",
        "citations": 104,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1137/1.9781611975673.63"
    },
    {
        "id": 22202,
        "title": "5. Deep Learning: RNNs and LSTMs",
        "authors": "",
        "published": "2020-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683924715-006"
    },
    {
        "id": 22203,
        "title": "Attentional Parallel RNNs for Generating Punctuation in Transcribed Speech",
        "authors": "Alp Öktem, Mireia Farrús, Leo Wanner",
        "published": "2017",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-68456-7_11"
    },
    {
        "id": 22204,
        "title": "Cognitive Model Discovery via Disentangled RNNs",
        "authors": "Kevin J. Miller, Maria Eckstein, Matthew M. Botvinick, Zeb Kurth-Nelson",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractComputational cognitive models are a fundamental tool in behavioral neuroscience. They instantiate in software precise hypotheses about the cognitive mechanisms underlying a particular behavior. Constructing these models is typically a difficult iterative process that requires both inspiration from the literature and the creativity of an individual researcher. Here, we adopt an alternative approach to learn parsimonious cognitive models directly from data. We fit behavior data using a recurrent neural network that is penalized for carrying information forward in time, leading to sparse, interpretable representations and dynamics. When fitting synthetic behavioral data from known cognitive models, our method recovers the underlying form of those models. When fit to laboratory data from rats performing a reward learning task, our method recovers simple and interpretable models that make testable predictions about neural mechanisms.",
        "link": "http://dx.doi.org/10.1101/2023.06.23.546250"
    },
    {
        "id": 22205,
        "title": "Unified Modeling Language",
        "authors": "Janis Osis, Uldis Donins",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805476-5.00001-0"
    },
    {
        "id": 22206,
        "title": "Topological Unified Modeling Language",
        "authors": "Janis Osis, Uldis Donins",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805476-5.00004-6"
    },
    {
        "id": 22207,
        "title": "Adjusting Unified Modeling Language",
        "authors": "Janis Osis, Uldis Donins",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805476-5.00003-4"
    },
    {
        "id": 22208,
        "title": "CDS Risk Premia Forecasting with Multi-Featured Deep Rnns: An Application on Br[I]Cs Countries",
        "authors": "Yasin Kutuk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4360006"
    },
    {
        "id": 22209,
        "title": "Top-down signaling dynamically mediates information processing in biologically inspired RNNs",
        "authors": "Tomas Gallo Aquino, Robert Kim, Nuttida Rungratsameetaweemana",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractRecent studies have proposed employing biologically plausible recurrent neural networks (RNNs) to explore flexible decision making processes in the brain. However, the mechanisms underlying the integration of bottom-up factors (such as incoming sensory signals) and top-down factors (such as task instructions and selective attention) remain poorly understood, both within the context of these models and the brain. To address this question, we trained biologically inspired RNNs on complex cognitive tasks that require adaptive integration of these factors. By performing extensive dynamical systems analyses, we show that our RNN model is capable of seamlessly incorporating top-down signals with sensory signals to perform the complex tasks. Furthermore, through comprehensive local connectivity analyses, we identified important inhibitory feedback signals that efficiently modulate the bottom-up sensory coding in a task-driven manner. Finally, we introduced an anatomical constraint where a specific subgroup of neurons receives the sensory input signal, effectively creating a designated sensory area within the RNN. Through this constraint, we show that these “sensory” neurons possess the remarkable ability to multiplex and dynamically combine both bottom-up and top-down information. These findings are consistent with recent experimental results highlighting that such integration is a key factor in facilitating flexible decision making. Overall, our work provides a framework for generating testable hypotheses for the hierarchical encoding of task-relevant information.",
        "link": "http://dx.doi.org/10.1101/2023.10.17.562828"
    },
    {
        "id": 22210,
        "title": "Inference Skipping for More Efficient Real-Time Speech Enhancement With Parallel RNNs",
        "authors": "Xiaohuai Le, Tong Lei, Kai Chen, Jing Lu",
        "published": "2022",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taslp.2022.3190738"
    },
    {
        "id": 22211,
        "title": "Recurrent Neural Networks (RNNs)",
        "authors": "Taweh Beysolow II",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-2734-3_6"
    },
    {
        "id": 22212,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v3/review1"
    },
    {
        "id": 22213,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v1/review3"
    },
    {
        "id": 22214,
        "title": "Neural Sequence Modeling in Physical Language Understanding",
        "authors": "Avi Bleiweiss",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008071104640472"
    },
    {
        "id": 22215,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v2/review2"
    },
    {
        "id": 22216,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v2/review1"
    },
    {
        "id": 22217,
        "title": "Unified Modeling Language",
        "authors": " ",
        "published": "2020-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/nhhvtt"
    },
    {
        "id": 22218,
        "title": "Language competition modeling and language policy evaluation",
        "authors": "Torsten Templin",
        "published": "2020-7-1",
        "citations": 3,
        "abstract": "Abstract\nIn this paper, we present a framework for the analysis of effects of language policies on the competition between\n                    languages. At the core of this framework is a language competition model that takes into account four pivotal factors for the\n                    evolution of the linguistic composition of a society: intergenerational language transmission, formal language education, adult\n                    language learning and migration. In contrast to the majority of models available in the literature, our model operates with\n                    parameters that can be estimated from empirical socio-linguistic data. It allows the reconstruction of past and simulate future\n                    dynamics. Language policies can be modeled as changes in model parameters. Therefore, projections derived from the model can be\n                    utilized to compare the effects of different policy options. We use Basque and Spanish within the Basque Autonomous Community in\n                    Spain to illustrate the application of the model.",
        "link": "http://dx.doi.org/10.1075/lplp.00055.tem"
    },
    {
        "id": 22219,
        "title": "OpenCV with Python",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_2"
    },
    {
        "id": 22220,
        "title": "An Interactive Graphical Visualization Approach to CNNs and RNNs",
        "authors": "Akhil Vyas, Prasad Calyam",
        "published": "2020-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aipr50011.2020.9425299"
    },
    {
        "id": 22221,
        "title": "Unleashing the Melodic Potential: Music Generation with Char RNNs",
        "authors": "Devang Gangal, Yash Kadam",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425396"
    },
    {
        "id": 22222,
        "title": "IML: Towards an Instructional Modeling Language",
        "authors": "Eric Rapos, Matthew Stephan",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007485204190427"
    },
    {
        "id": 22223,
        "title": "Chapter 5: Deep Learning: RNNs and LSTMs",
        "authors": "",
        "published": "2020-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683924654-006"
    },
    {
        "id": 22224,
        "title": "Analysis of Gradient Vanishing of RNNs and Performance Comparison",
        "authors": "Seol-Hyun Noh",
        "published": "2021-10-25",
        "citations": 36,
        "abstract": "A recurrent neural network (RNN) combines variable-length input data with a hidden state that depends on previous time steps to generate output data. RNNs have been widely used in time-series data analysis, and various RNN algorithms have been proposed, such as the standard RNN, long short-term memory (LSTM), and gated recurrent units (GRUs). In particular, it has been experimentally proven that LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. The learning ability is a measure of the effectiveness of gradient of error information that would be backpropagated. This study provided a theoretical and experimental basis for the result that LSTM and GRU have more efficient gradient descent than the standard RNN by analyzing and experimenting the gradient vanishing of the standard RNN, LSTM, and GRU. As a result, LSTM and GRU are robust to the degradation of gradient descent even when LSTM and GRU learn long-range input data, which means that the learning ability of LSTM and GRU is greater than standard RNN when learning long-range input data. Therefore, LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. In addition, it was verified whether the experimental results of river-level prediction models, solar power generation prediction models, and speech signal models using the standard RNN, LSTM, and GRUs are consistent with the analysis results of gradient vanishing.",
        "link": "http://dx.doi.org/10.3390/info12110442"
    },
    {
        "id": 22225,
        "title": "Neural Language Modeling for Molecule Generation",
        "authors": "Sanjar Adilov",
        "published": "No Date",
        "citations": 0,
        "abstract": "Generative neural networks have shown promising results in <i>de novo</i> drug design. Recent studies suggest that one of the efficient ways to produce novel molecules matching target properties is to model SMILES sequences using deep learning in a way similar to language modeling in natural language processing. In this paper, we present a survey of various machine learning methods for SMILES-based language modeling and propose our benchmarking results on a standardized subset of ChEMBL database.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14700831"
    },
    {
        "id": 22226,
        "title": "Deepcover: Defining Coverage Criteria for Test Suite Evaluation and Error Prediction Using State Machines Extraction from Rnns",
        "authors": "Pouria Golshanrad, Fathiyeh Faghih",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4382943"
    },
    {
        "id": 22227,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "Hongqiao Peng",
        "published": "2020-6-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v2/review3"
    },
    {
        "id": 22228,
        "title": "IML: Towards an Instructional Modeling Language",
        "authors": "Eric Rapos, Matthew Stephan",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007485204170425"
    },
    {
        "id": 22229,
        "title": "Object Detection and Recognition",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_5"
    },
    {
        "id": 22230,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": " Oveis Abedinia",
        "published": "2020-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v1/review2"
    },
    {
        "id": 22231,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "Hongqiao Peng",
        "published": "2020-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v3/review2"
    },
    {
        "id": 22232,
        "title": "Image Manipulation and Segmentation",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_4"
    },
    {
        "id": 22233,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "Hongqiao Peng",
        "published": "2020-3-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v1/review1"
    },
    {
        "id": 22234,
        "title": "Review for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "George Tsekouras",
        "published": "2020-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v1/review4"
    },
    {
        "id": 22235,
        "title": "Artificial Intelligence and Computer Vision",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_1"
    },
    {
        "id": 22236,
        "title": "Decision letter for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v3/decision1"
    },
    {
        "id": 22237,
        "title": "Toward a Modeling Language Prototype for Modeling the Behavior of Wireless Body Area Networks Communication Protocols",
        "authors": "Bethaina Touijer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011992700003464"
    },
    {
        "id": 22238,
        "title": "Software Designing With Unified Modeling Language Driven Approaches",
        "authors": "Janis Osis, Uldis Donins",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805476-5.00002-2"
    },
    {
        "id": 22239,
        "title": "Hierarchical Graph-Rnns for Action Detection of Multiple Activities",
        "authors": "Sovan Biswas, Yaser Souri, Juergen Gall",
        "published": "2019-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803650"
    },
    {
        "id": 22240,
        "title": "Neural Language Modeling for Molecule Generation",
        "authors": "Sanjar Adilov",
        "published": "No Date",
        "citations": 0,
        "abstract": "Generative neural networks have shown promising results in de novo drug design. Recent studies suggest that one of the efficient ways to produce novel molecules matching target properties is to model SMILES sequences using deep learning in a way similar to language modeling in natural language processing. In this paper, we present a survey of various machine learning methods for SMILES-based language modeling and propose our benchmarking results on a standardized subset of ChEMBL database.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14700831.v1"
    },
    {
        "id": 22241,
        "title": "TranspLanMeta: A Metamodel for TranspLan Modeling Language",
        "authors": "Deniz Cetinkaya, Mahmood Hosseini",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010202201470154"
    },
    {
        "id": 22242,
        "title": "On a Metasemantic Protocol for Modeling Language Extension",
        "authors": "Ed Seidewitz",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009181604650472"
    },
    {
        "id": 22243,
        "title": "Modeling Second Language Acquisition 1",
        "authors": "Susan M. Gass",
        "published": "2017-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315173252-1"
    },
    {
        "id": 22244,
        "title": "Deep Learning for Computer Vision",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_3"
    },
    {
        "id": 22245,
        "title": "Decision letter for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-4-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v1/decision1"
    },
    {
        "id": 22246,
        "title": "Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis",
        "authors": "Yusuke Ijima, Nobukatsu Hojo, Ryo Masumura, Taichi Asami",
        "published": "2017-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-521"
    },
    {
        "id": 22247,
        "title": "A domain specific language for distributed modeling",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46354/i3m.2023.emss.015"
    },
    {
        "id": 22248,
        "title": "Predictive Modeling of Longitudinal Data for Alzheimer’s Disease Diagnosis Using RNNs",
        "authors": "Maryamossadat Aghili, Solale Tabarestani, Malek Adjouadi, Ehsan Adeli",
        "published": "2018",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-00320-3_14"
    },
    {
        "id": 22249,
        "title": "Better Together: Jointly Using Masked Latent Semantic Modeling and Masked Language Modeling for Sample Efficient Pre-training",
        "authors": "Gábor Berend",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.26"
    },
    {
        "id": 22250,
        "title": "To Bilingualism and Beyond! Modeling Bilingualism Requires Looking Beyond Language: A Commentary on “Computational Modeling of Bilingual Language Learning: Current Models and Future Directions”",
        "authors": "Viorica Marian",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/lang.12530"
    },
    {
        "id": 22251,
        "title": "Ontology-Driven Conceptual Modeling for Early Warning Systems: Redesigning the Situation Modeling Language",
        "authors": "João L. R. Moreira, Luís Ferreira Pires, Marten van Sinderen, Patricia Dockhorn Costa",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006208904670477"
    },
    {
        "id": 22252,
        "title": "From neural network to psychophysics of time: Exploring emergent properties of RNNs using novel Hamiltonian formalism",
        "authors": "Rakesh Sengupta, Anindya Pattanayak, Raju Surampudi Bapi",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe stability analysis of dynamical neural network systems generally follows the route of finding a suitable Liapunov function after the fashion Hopfield’s famous paper on content addressable memory network or by finding conditions that make divergent solutions impossible. For the current work we focused on biological recurrent neural networks (bRNNs) that require transient external inputs (Cohen-Grossberg networks). In the current work we have proposed a general method to construct Liapunov functions for recurrent neural network with the help of a physically meaningful Hamiltonian function. This construct allows us to explore the emergent properties of the recurrent network (e.g., parameter configuration needed for winner-take-all competition in a leaky accumulator design) beyond that available in standard stability analysis, while also comparing well with standard stability analysis (ordinary differential equation approach) as a special case of the general stability constraint derived from the Hamiltonian formulation. We also show that the Cohen-Grossberg Liapunov function can be derived naturally from the Hamiltonian formalism. A strength of the construct comes from its usability as a predictor for behavior in psychophysical experiments involving numerosity and temporal duration judgements.",
        "link": "http://dx.doi.org/10.1101/125849"
    },
    {
        "id": 22253,
        "title": "RNNs for Classification of Driving Behaviour",
        "authors": "Dimitris Mantzekis, Michalis Savelonas, Stavros Karkanis, Evaggelos Spyrou",
        "published": "2019-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisa.2019.8900693"
    },
    {
        "id": 22254,
        "title": "Advanced Abnormality Recognition in IoT Networks Using LSTM-RNNs for Dynamic Security Enhancement",
        "authors": "Sakshi Pandey",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsai59793.2023.10420960"
    },
    {
        "id": 22255,
        "title": "Spatiotemporal Modeling Language Extension",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_101326"
    },
    {
        "id": 22256,
        "title": "Spatial Modeling Language Extension",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_101273"
    },
    {
        "id": 22257,
        "title": "Language Modeling",
        "authors": "Yoav Goldberg",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-02165-7_9"
    },
    {
        "id": 22258,
        "title": "Motion Analysis and Object Tracking",
        "authors": "Sunila Gollapudi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4261-2_6"
    },
    {
        "id": 22259,
        "title": "Decision letter for \"Featuring periodic correlations via dual granularity inputs structured &lt;scp&gt;RNNs&lt;/scp&gt; ensemble load forecaster\"",
        "authors": "",
        "published": "2020-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12571/v2/decision1"
    },
    {
        "id": 22260,
        "title": "Ensembles of Rnns Negatively Correlated Through Time Using Bptt and Rtrl",
        "authors": "Ali Rodan, Omar Al-Kadi, Abdel-Karim Al-Tamimi, Yasir Javed",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706199"
    },
    {
        "id": 22261,
        "title": "Deep learning: RNNs and LSTM",
        "authors": "Robert DiPietro, Gregory D. Hager",
        "published": "2020",
        "citations": 65,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-816176-0.00026-0"
    },
    {
        "id": 22262,
        "title": "Recurrent Neural Networks (RNNs) for Predictive Analytics",
        "authors": "Phillip A. Laplante, Satish Mahadevan Srinivasan",
        "published": "2023-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003278177-10"
    },
    {
        "id": 22263,
        "title": "Recurrent Neural Networks (RNNs) to improve EEG-based person identification",
        "authors": "Youssef Mohamed, Ahmed M. Anter, Ahmed B. Zaky",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217750"
    },
    {
        "id": 22264,
        "title": "CNNs and RNNs in aspect-level sentiment analysis and comparison",
        "authors": "Yijun Qi",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Aspect-based sentiment analysis (ABSA), also known as fine-grained sentiment analysis, may offer a precise polarity for each aspect in statement aspect. CNN and RNN neural network models, the most fundamental and widely used deep neural network models, have been created by researchers using a variety of methodologies to offer reliable findings in ABSA. This paper will sort out the development process of CNN and RNN models in completing various tasks in aspect-level sentiment analysis and find out the current SOTA model. In addition, by comparing the representatives of the two models in the same environment, the advantages and disadvantages of the two models are analysed to provide guidance for choosing different baseline models. The results show that the CNN-based model has the advantages of high accuracy and fast training speed, but the design is more complicated, and the pooling layer is prone to lose position or sequence features. The RNN-LSTM based model is easier to capture sequence data, has high reliability, and has a relatively simple design structure. However, it requires significantly longer training time, the model is complex, and the computational cost is high. Considering the characteristics of the two most basic neural models, new tasks require researchers to propose better hybrid neural network models.",
        "link": "http://dx.doi.org/10.54254/2755-2721/6/20230417"
    },
    {
        "id": 22265,
        "title": "Learning from Others: Daily COVID-19 Cases Prediction in India using Ensembles of LSTM-RNNs",
        "authors": "Debasrita Chakraborty, Debayan Goswami, Susmita Ghosh, Jonathan H. Chan, Ashish Ghosh",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work was presented at the 10th Joint Symposium on Computational Intelligence (JSCI10), organized by the IEEE-CIS Thailand Chapter, that aims to support research students and young researchers, to create a place enabling participants to share and discuss on their research prior to publishing their works. The event was open to all researchers who want to broaden their knowledge in the field of computational intelligence.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14718618"
    },
    {
        "id": 22266,
        "title": "Learning from Others: Daily COVID-19 Cases Prediction in India using Ensembles of LSTM-RNNs",
        "authors": "Debasrita Chakraborty, Debayan Goswami, Susmita Ghosh, Jonathan H. Chan, Ashish Ghosh",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work was presented at the 10th Joint Symposium on Computational Intelligence (JSCI10), organized by the IEEE-CIS Thailand Chapter, that aims to support research students and young researchers, to create a place enabling participants to share and discuss on their research prior to publishing their works. The event was open to all researchers who want to broaden their knowledge in the field of computational intelligence.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14718618.v1"
    },
    {
        "id": 22267,
        "title": "Proxy Model Explanations for Time Series RNNs",
        "authors": "Zach Wood-Doughty, Isabel Cachola, Mark Dredze",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00117"
    },
    {
        "id": 22268,
        "title": "Ecologically pre-trained RNNs explain suboptimal animal decisions",
        "authors": "Manuel Molano-Mazon, Yuxiu Shao, Daniel Duque, Guangyu Robert Yang, Srdjan Ostojic, Jaime de la Rocha",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractThe strategies found by animals facing a new task are determined both by individual experience and by structural priors evolved to leverage the statistics of natural environments. Rats can quickly learn to capitalize on the trial sequence correlations of two-alternative forced choice (2AFC) tasks after correct trials, but consistently deviate from optimal behavior after error trials, when they waive the accumulated evidence. To understand this outcome-dependent gating, we first show that Recurrent Neural Networks (RNNs) trained in the same 2AFC task outperform rats as they can readily learn to use across-trial information both after correct and error trials. We hypothesize that, while RNNs can optimize their behavior in the 2AFC task without any a priori restrictions, rats’ strategy is constrained by a structural prior adapted to a natural environment in which rewarded and non-rewarded actions provide largely asymmetric information. When pre-training RNNs in a more ecological task with more than two possible choices, networks develop a strategy by which they gate off the across-trial evidence after errors, mimicking rats’ behavior. Population analyses show that the pre-trained networks form an accurate representation of the sequence statistics independently of the outcome in the previous trial. After error trials, gating is implemented by a change in the network dynamics which temporarily decouples the categorization of the stimulus from the across-trial accumulated evidence. Our results suggest that the suboptimal behavior observed in rats reflects the influence of a structural prior that reacts to errors by isolating the network decision dynamics from the context, ultimately constraining the performance in a 2AFC laboratory task.",
        "link": "http://dx.doi.org/10.1101/2021.05.15.444287"
    },
    {
        "id": 22269,
        "title": "Using recurrent neural networks (RNNs) as planners for bio-inspired robotic motion",
        "authors": "Ayesha Khan, Fumin Zhang",
        "published": "2017-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta.2017.8062594"
    },
    {
        "id": 22270,
        "title": "WITHDRAWN: Finite-time stability of almost anti-periodic solutions of Clifford-valued RNNs with time-varying delays and $D$ operator on time scales",
        "authors": "",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn by the authors while they make corrections to the work. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1674283/v2"
    }
]