[
    {
        "id": 32905,
        "title": "Multi-AUV Charging Navigation Trajectory Planning Based on Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Jiaming Yu, Hao Sun, Qinglin Sun",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240061"
    },
    {
        "id": 32906,
        "title": "Deep reinforcement learning for PMSG wind turbine control via twin delayed deep deterministic policy gradient (TD3)",
        "authors": "Darkhan Zholtayev, Matteo Rubagotti, Ton Duc Do",
        "published": "2024-4-7",
        "citations": 0,
        "abstract": "AbstractThis article proposes the use of a deep reinforcement learning method—and precisely a variant of the deep deterministic policy gradient (DDPG) method known as twin delayed DDPG, or TD3—for maximum power point tracking in wind energy conversion systems that use permanent magnet synchronous generators (PMSGs). An overview of the TD3 algorithm is provided, together with a detailed description of its implementation and training for the considered application. Simulation results are provided, also including a comparison with a model‐based control method based on feedback linearization and linear‐quadratic regulation. The proposed TD3‐based controller achieves a satisfactory control performance and is more robust to PMSG parameter variations as compared to the presented model‐based method. To the best of the authors' knowledge, this article presents for the first time an approach for generating both speed and current control loops using DRL for wind energy conversion systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/oca.3129"
    },
    {
        "id": 32907,
        "title": "Twin delayed deep deterministic policy gradient-based intelligent computation offloading for IoT",
        "authors": "Siguang Chen, Bei Tang, Kun Wang",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dcan.2022.06.008"
    },
    {
        "id": 32908,
        "title": "Swap Softmax Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Chaohu Liu, Yunbo Zhao",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isas59543.2023.10164333"
    },
    {
        "id": 32909,
        "title": "Integrated Multi-Energy Scheduling Strategy for Smart Community based on Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Mengfei Wen, Yujie Wang, Jie Chen, Ren Zhu",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/segre58867.2023.00055"
    },
    {
        "id": 32910,
        "title": "Twin-Delayed Deep Deterministic Policy Gradient for altitude control of a flying-wing aircraft with an uncertain aerodynamic model",
        "authors": "Willem Völker, Yifei Li, Erik-Jan Van Kampen",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2678"
    },
    {
        "id": 32911,
        "title": "Cooperative motion planning and control of a group of autonomous underwater vehicles using twin-delayed deep deterministic policy gradient",
        "authors": "Behnaz Hadi, Alireza Khosravi, Pouria Sarhadi",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apor.2024.103977"
    },
    {
        "id": 32912,
        "title": "Twin delayed deep deterministic policy gradient based energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles considering predicted terrain information",
        "authors": "Fazhan Tao, Zhigao Fu, Huixian Gong, Baofeng Ji, Yao Zhou",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.129173"
    },
    {
        "id": 32913,
        "title": "Twin Delayed Deep Deterministic Policy Gradient (TD3) Based Virtual Inertia Control for Inverter-Interfacing DGs in Microgrids",
        "authors": "Osarodion Emmanuel Egbomwan, Shichao Liu, Hicham Chaoui",
        "published": "2023-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jsyst.2022.3222262"
    },
    {
        "id": 32914,
        "title": "Novel task decomposed multi-agent twin delayed deep deterministic policy gradient algorithm for multi-UAV autonomous path planning",
        "authors": "Yatong Zhou, Xiaoran Kong, Kuo-Ping Lin, Liangyu Liu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2024.111462"
    },
    {
        "id": 32915,
        "title": "Adaptive Cruise Control Using Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Himanshu Kumar Bishen, K.V. Shihabudheen, P.P. Muhammed Shanir",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icepe57949.2023.10201488"
    },
    {
        "id": 32916,
        "title": "A novel data-driven energy management strategy for fuel cell hybrid electric bus based on improved twin delayed deep deterministic policy gradient algorithm",
        "authors": "Ruchen Huang, Hongwen He",
        "published": "2024-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ijhydene.2023.04.335"
    },
    {
        "id": 32917,
        "title": "Fault‐resilient control of parallel PV inverters using multi‐agent twin‐delayed deep deterministic policy gradient approach",
        "authors": "Azra Malik, Ahteshamul Haque, V. S. Bharath Kurukuru, Saad Mekhilef",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "SummaryGrid‐tied photovoltaic system (GTPS) are widely favored due to their inherent benefits. The interface of parallel power electronic converters in these systems is rapidly increasing. The inverter in GTPS has a very important role in power conversion, transfer, and control. However, their challenging working conditions contribute to susceptibility to power switching device failures. Traditional fault diagnosis and tolerant approaches exhibit limitations in control efficiency, model dependency, and slow recovery from fault. This study proposes a multi‐agent twin delayed deep deterministic policy gradient (MATD3PG) configuration for intelligent parallel inverter control, fault diagnosis, and fault‐tolerant operation in GTPS structure. By leveraging the multi‐agent reinforcement learning (RL) framework, an optimal control of the parallel inverter can be achieved, encompassing fault‐tolerant operation using MATLAB Simulink/PLECS. The major advantage the given technique offers is that it carries out optimum fault‐tolerant operation without causing the system derating. Experimental findings demonstrate that the proposed fault‐tolerant model based on RL traditional methods is able to ensure continuous supply to the connected loads even during fault events. Further, the transition time from fault occurrence to recovery is found to be 6 ms, which is quite less compared with the fault‐tolerant techniques presented in literature. Through real‐time fault diagnosis‐based results, the proposed approach ensures precise tracking of reference currents, quicker response times, uninterrupted supply, and smooth transition to the post‐fault operation mode.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cta.3911"
    },
    {
        "id": 32918,
        "title": "Optimal demand response based dynamic pricing strategy via Multi-Agent Federated Twin Delayed Deep Deterministic policy gradient algorithm",
        "authors": "Haining Ma, Huifeng Zhang, Ding Tian, Dong Yue, Gerhard P. Hancke",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108012"
    },
    {
        "id": 32919,
        "title": "Twin delayed deep deterministic reinforcement learning application in vehicle electrical suspension control",
        "authors": "Nong Zhang, Shilei Zhou, Daoyu Shen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijvp.2023.10055649"
    },
    {
        "id": 32920,
        "title": "Three-Dimensional Path Planning of UAVs in a Complex Dynamic Environment Based on Environment Exploration Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Danyang Zhang, Xiongwei Li, Guoquan Ren, Jiangyi Yao, Kaiyan Chen, Xi Li",
        "published": "2023-7-5",
        "citations": 2,
        "abstract": "Unmanned Aerial Vehicle (UAV) path planning research refers to the UAV automatically planning an optimal path to the destination under the corresponding environment, while avoiding collision with obstacles in this process. In order to solve the problem of 3D path planning of UAV in a dynamic environment, a heuristic dynamic reward function is designed to guide the UAV. We propose the Environment Exploration Twin Delayed Deep Deterministic Policy Gradient (EE-TD3) algorithm, which combines the symmetrical 3D environment exploration coding mechanism on the basis of TD3 algorithm. The EE-TD3 algorithm model can effectively avoid collisions, improve the training efficiency, and achieve faster convergence speed. Finally, the performance of the EE-TD3 algorithm and other deep reinforcement learning algorithms was tested in the simulation environment. The results show that the EE-TD3 algorithm is better than other algorithms in solving the 3D path planning problem of UAV.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15071371"
    },
    {
        "id": 32921,
        "title": "Twin delayed deep deterministic policy gradient for free-electron laser online optimization",
        "authors": "M Cai, Z H Zhu, K Q Zhang, C Feng, L J Tu, D Gu, Z T Zhao",
        "published": "2023-1-1",
        "citations": 1,
        "abstract": "Abstract\nX-ray free-electron lasers (FEL) have contributed to many frontier applications of nanoscale science which benefit from its extraordinary properties. During FEL commissioning, the beam status optimization especially orbits correction is particularly significant for FEL amplification. For example, the deviation between beam orbit and the magnetic center of undulator can affect the interaction between the electron beam and the FEL pulse. Usually, FEL commissioning requires a lot of effort for multi-dimensional parameters optimization in a time-varying system. Therefore, advanced algorithms are needed to facilitate the commissioning procedure. In this paper, we propose an online method to optimize the FEL power and transverse coherence by using a twin delayed deep deterministic policy gradient (TD3) algorithm. The algorithm exhibits more stable learning convergence and improves learning performance because the overestimation bias of policy gradient methods is suppressed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2420/1/012027"
    },
    {
        "id": 32922,
        "title": "Twin-Delayed Deep Deterministic Policy Gradient Algorithm to Control a Boost Converter in a DC Microgrid",
        "authors": "Rifqi Firmansyah Muktiadji, Makbul A. M. Ramli, Ahmad H. Milyani",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": "A stable output voltage of a boost converter is vital for the appropriate functioning of connected devices and loads in a DC microgrid. Variations in load demands and source uncertainties can damage equipment and disrupt operations. In this study, a modified twin-delayed deep deterministic policy gradient (TD3) algorithm is proposed to regulate the output voltage of a boost converter in a DC microgrid. TD3 optimizes PI controller gains, which ensure system stability by employing a non-negative, fully connected layer. To achieve optimal gains, multi-deep reinforcement learning agents are trained. The agents utilize the error signal to obtain the desired output voltage. Furthermore, a new reward function used in the TD3 algorithm is introduced. The proposed controller is tested under load variations and input voltage uncertainties. Simulation and experimental results demonstrate that TD3 outperforms PSO, GA, and the conventional PI. TD3 exhibits less steady-state error, reduced overshoots, fast response times, fast recovery times, and a small voltage deviation. These findings confirm TD3’s superiority and its potential application in DC microgrid voltage control. It can be used by engineers and researchers to design DC microgrids.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13020433"
    },
    {
        "id": 32923,
        "title": "Decentralized multi-agent control of a three-tank hybrid system based on twin delayed deep deterministic policy gradient reinforcement learning algorithm",
        "authors": "N. Rajasekhar, T. K. Radhakrishnan, N. Samsudeen",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40435-023-01227-0"
    },
    {
        "id": 32924,
        "title": "Twin delayed deep deterministic reinforcement learning application in vehicle electrical suspension control",
        "authors": "Daoyu Shen, Shilei Zhou, Nong Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijvp.2023.133852"
    },
    {
        "id": 32925,
        "title": "Value activation for bias alleviation: Generalized-activated deep double deterministic policy gradients",
        "authors": "Jiafei Lyu, Yu Yang, Jiangpeng Yan, Xiu Li",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.10.085"
    },
    {
        "id": 32926,
        "title": "Virtual Synchronous Generator Control Using Twin Delayed Deep Deterministic Policy Gradient Method",
        "authors": "Oroghene Oboreh-Snapps, Buxin She, Shah Fahad, Haotian Chen, Jonathan Kimball, Fangxing Li, Hantao Cui, Rui Bo",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tec.2023.3309955"
    },
    {
        "id": 32927,
        "title": "Path Planning of Unmanned Aerial Vehicle in Complex Environments Based on State-Detection Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Danyang Zhang, Zhaolong Xuan, Yang Zhang, Jiangyi Yao, Xi Li, Xiongwei Li",
        "published": "2023-1-13",
        "citations": 2,
        "abstract": "This paper investigates the path planning problem of an unmanned aerial vehicle (UAV) for completing a raid mission through ultra-low altitude flight in complex environments. The UAV needs to avoid radar detection areas, low-altitude static obstacles, and low-altitude dynamic obstacles during the flight process. Due to the uncertainty of low-altitude dynamic obstacle movement, this can slow down the convergence of existing algorithm models and also reduce the mission success rate of UAVs. In order to solve this problem, this paper designs a state detection method to encode the environmental state of the UAV’s direction of travel and compress the environmental state space. In considering the continuity of the state space and action space, the SD-TD3 algorithm is proposed in combination with the double-delayed deep deterministic policy gradient algorithm (TD3), which can accelerate the training convergence speed and improve the obstacle avoidance capability of the algorithm model. Further, to address the sparse reward problem of traditional reinforcement learning, a heuristic dynamic reward function is designed to give real-time rewards and guide the UAV to complete the task. The simulation results show that the training results of the SD-TD3 algorithm converge faster than the TD3 algorithm, and the actual results of the converged model are better.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/machines11010108"
    },
    {
        "id": 32928,
        "title": "Path Planning Method for Manipulators Based on Improved Twin Delayed Deep Deterministic Policy Gradient and RRT*",
        "authors": "Ronggui Cai, Xiao Li",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "This paper proposes a path planning framework that combines the experience replay mechanism from deep reinforcement learning (DRL) and rapidly exploring random tree star (RRT*), employing the DRL-RRT* as the path planning method for the manipulator. The iteration of the RRT* is conducted independently in path planning, resulting in a tortuous path and making it challenging to find an optimal path. The setting of reward functions in policy learning based on DRL is very complex and has poor universality, making it difficult to complete the task in complex path planning. Aiming at the insufficient exploration of the current deterministic policy gradient DRL algorithm twin delayed deep deterministic policy gradient (TD3), a stochastic policy was combined with TD3, and the performance was verified on the simulation platform. Furthermore, the improved TD3 was integrated with RRT* for performance analysis in two-dimensional (2D) and three-dimensional (3D) path planning environments. Finally, a six-degree-of-freedom manipulator was used to conduct simulation and experimental research on the manipulator.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14072765"
    },
    {
        "id": 32929,
        "title": "Twin-delayed deep deterministic policy gradient algorithm for the energy management of microgrids",
        "authors": "David Domínguez-Barbero, Javier García-González, Miguel Á. Sanz-Bobi",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106693"
    },
    {
        "id": 32930,
        "title": "Real-Time Energy Scheduling Applying the Twin Delayed Deep Deterministic Policy Gradient and Data Clustering",
        "authors": "Ioannis Zenginis, John Vardakas, Nikolaos E. Koltsaklis, Christos Verikoukis",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jsyst.2023.3326978"
    },
    {
        "id": 32931,
        "title": "Local 2-D Path Planning of Unmanned Underwater Vehicles in Continuous Action Space Based on the Twin-Delayed Deep Deterministic Policy Gradient",
        "authors": "Zhenzhong Chu, Yu Wang, Daqi Zhu",
        "published": "2024-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2023.3348827"
    },
    {
        "id": 32932,
        "title": "Fractional-Order Control Method Based on Twin-Delayed Deep Deterministic Policy Gradient Algorithm",
        "authors": "Guangxin Jiao, Zhengcai An, Shuyi Shao, Dong Sun",
        "published": "2024-2-6",
        "citations": 0,
        "abstract": "In this paper, a fractional-order control method based on the twin-delayed deep deterministic policy gradient (TD3) algorithm in reinforcement learning is proposed. A fractional-order disturbance observer is designed to estimate the disturbances, and the radial basis function network is selected to approximate system uncertainties in the system. Then, a fractional-order sliding-mode controller is constructed to control the system, and the parameters of the controller are tuned using the TD3 algorithm, which can optimize the control effect. The results show that the fractional-order control method based on the TD3 algorithm can not only improve the closed-loop system performance under different operating conditions but also enhance the signal tracking capability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fractalfract8020099"
    },
    {
        "id": 32933,
        "title": "Twin Delayed Deterministic Policy Gradient Based Cooperative Platoon Longitudinal Control Strategy",
        "authors": "Fa Tao Zhou, Yong Fu Li, Long Wang Huang, Xin Huang",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3616901.3616947"
    },
    {
        "id": 32934,
        "title": "The application of machine learning based energy management strategy in multi-mode plug-in hybrid electric vehicle, part I: Twin Delayed Deep Deterministic Policy Gradient algorithm design for hybrid mode",
        "authors": "Changcheng Wu, Jiageng Ruan, Hanghang Cui, Bin Zhang, Tongyang Li, Kaixuan Zhang",
        "published": "2023-1",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2022.125084"
    },
    {
        "id": 32935,
        "title": "Parameter-Free Reduction of the Estimation Bias in Deep Reinforcement Learning for Deterministic Policy Gradients",
        "authors": "Baturay Saglam, Furkan Burak Mutlu, Dogan Can Cicek, Suleyman Serdar Kozat",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "AbstractApproximation of the value functions in value-based deep reinforcement learning induces overestimation bias, resulting in suboptimal policies. We show that when the reinforcement signals received by the agents have a high variance, deep actor-critic approaches that overcome the overestimation bias lead to a substantial underestimation bias. We first address the detrimental issues in the existing approaches that aim to overcome such underestimation error. Then, through extensive statistical analysis, we introduce a novel, parameter-free Deep Q-learning variant to reduce this underestimation bias in deterministic policy gradients. By sampling the weights of a linear combination of two approximate critics from a highly shrunk estimation bias interval, our Q-value update rule is not affected by the variance of the rewards received by the agents throughout learning. We test the performance of the introduced improvement on a set of MuJoCo and Box2D continuous control tasks and demonstrate that it outperforms the existing approaches and improves the baseline actor-critic algorithm in most of the environments tested.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11461-y"
    },
    {
        "id": 32936,
        "title": "Cooperative control of velocity and heading for unmanned surface vessel based on twin delayed deep deterministic policy gradient with an integral compensator",
        "authors": "Yibai Wang, Shulong Zhao, Qingling Wang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.115943"
    },
    {
        "id": 32937,
        "title": "D2PG: Deep Deterministic Policy Gradient-Based Vehicular Edge Caching Scheme for Digital Twin-Based Vehicular Networks",
        "authors": "Chauhan Harshvardhan Singh, Babbar Himanshi, Rani Shalli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23940/ijpe.23.05.p7.350358"
    },
    {
        "id": 32938,
        "title": "Intelligent Trajectory Tracking Linear Active Disturbance Rejection Control of a Powered Parafoil Based on Twin Delayed Deep Deterministic Policy Gradient Algorithm Optimization",
        "authors": "Yuemin Zheng, Zelin Fei, Jin Tao, Qinglin Sun, Hao Sun, Zengqiang Chen, Mingwei Sun",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "Powered parafoils, known for their impressive load-bearing capacity and extended endurance, have garnered significant interest. However, the parafoil system is a highly complex nonlinear system. It primarily relies on the steering gear to change flight direction and utilizes a thrust motor for climbing. However, achieving precise trajectory tracking control presents a challenge due to the interdependence of direction and altitude control. Furthermore, underactuation and wind disturbances bring additional difficulties for trajectory tracking control. Consequently, realizing trajectory tracking control for powered parafoils holds immense significance. In this paper, we propose a trajectory tracking method based on Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm-optimized Linear Active Disturbance Rejection Control (LADRC). Our method addresses the underactuation issue by incorporating a guiding law while utilizing two LADRC methods to achieve decoupling and compensate for disturbances. Moreover, we employ the TD3 algorithm to dynamically adjust controller parameters, thus enhancing the controller performance. The simulation results demonstrate the effectiveness of our proposed method as a trajectory tracking control approach. Additionally, since the control process is not reliant on system-specific models, our method can also provide guidance for trajectory tracking control in other aircraft.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132312555"
    },
    {
        "id": 32939,
        "title": "Adaptive Active Disturbance Rejection Load Frequency Control for Power System with Renewable Energies Using the Lyapunov Reward-Based Twin Delayed Deep Deterministic Policy Gradient Algorithm",
        "authors": "Yuemin Zheng, Jin Tao, Qinglin Sun, Hao Sun, Zengqiang Chen, Mingwei Sun",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "The substitution of renewable energy sources (RESs) for conventional fossil fuels in electricity generation is essential in addressing environmental pollution and resource depletion. However, the integration of RESs in the load frequency control (LFC) of power systems can have a negative impact on frequency deviation response, resulting in a decline in power quality. Moreover, load disturbances can also affect the stability of frequency deviation. Hence, this paper presents an LFC method that utilizes the Lyapunov reward-based twin delayed deep deterministic policy gradient (LTD3) algorithm to optimize the linear active disturbance rejection control (LADRC). With the advantages of being model-free and mitigating unknown disturbances, LADRC can regulate load disturbances and renewable energy deviations. Additionally, the LTD3 algorithm, based on the Lyapunov reward function, is employed to optimize controller parameters in real-time, resulting in enhanced control performance. Finally, the LADRC-LTD3 is evaluated using a power system containing two areas, comprising thermal, hydro, and gas power plants in each area, as well as RESs such as a noise-based wind turbine and photovoltaic (PV) system. A comparative analysis is conducted between the performance of the proposed controller and other control techniques, such as integral controller (IC), fractional-order proportional integral derivative (FOPID) controller, I-TD, ID-T, and TD3-optimized LADRC. The results indicate that the proposed method effectively addresses the LFC problem.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su151914452"
    },
    {
        "id": 32940,
        "title": "Memory-Enhanced Twin Delayed Deep Deterministic Policy Gradient (ME-TD3)-Based Unmanned Combat Aerial Vehicle Trajectory Planning for Avoiding Radar Detection Threats in Dynamic and Unknown Environments",
        "authors": "Jiantao Li, Tianxian Zhang, Kai Liu",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "Unmanned combat aerial vehicle (UCAV) trajectory planning to avoid radar detection threats is a complicated optimization problem that has been widely studied. The rapid changes in Radar Cross Sections (RCSs), the unknown cruise trajectory of airborne radar, and the uncertain distribution of radars exacerbate the complexity of this problem. In this paper, we propose a novel UCAV trajectory planning method based on deep reinforcement learning (DRL) technology to overcome the adverse impacts caused by the dynamics and randomness of environments. A predictive control model is constructed to describe the dynamic characteristics of the UCAV trajectory planning problem in detail. To improve the UCAV’s predictive ability, we propose a memory-enhanced twin delayed deep deterministic policy gradient (ME-TD3) algorithm that uses an attention mechanism to effectively extract environmental patterns from historical information. The simulation results show that the proposed method can successfully train UCAVs to carry out trajectory planning tasks in dynamic and unknown environments. Furthermore, the ME-TD3 algorithm outperforms other classical DRL algorithms in UCAV trajectory planning, exhibiting superior performance and adaptability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15235494"
    },
    {
        "id": 32941,
        "title": "A Novel Integral Reinforcement Learning-Based Control Method Assisted by Twin Delayed Deep Deterministic Policy Gradient for Solid Oxide Fuel Cell in DC Microgrid",
        "authors": "Yulin Liu, Tianhao Qie, Yang Yu, Yuxuan Wang, Tat Kei Chau, Xinan Zhang, Ujjal Manandhar, Sinan Li, Herbert H. C. Iu, Tyrone Fernando",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tste.2022.3224179"
    },
    {
        "id": 32942,
        "title": "A twin delayed deep deterministic policy gradient-based energy management strategy for a battery-ultracapacitor electric vehicle considering driving condition recognition with learning vector quantization neural network",
        "authors": "Rui Liu, Chun Wang, Aihua Tang, Yongzhi Zhang, Quanqing Yu",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.est.2023.108147"
    },
    {
        "id": 32943,
        "title": "TD3LVSL: A lane-level variable speed limit approach based on twin delayed deep deterministic policy gradient in a connected automated vehicle environment",
        "authors": "Wenqi Lu, Ziwei Yi, Yuanli Gu, Yikang Rui, Bin Ran",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.trc.2023.104221"
    },
    {
        "id": 32944,
        "title": "A theoretical demonstration for reinforcement learning of PI control dynamics for optimal speed control of DC motors by using Twin Delay Deep Deterministic Policy Gradient Algorithm",
        "authors": "Sevilay Tufenkci, Baris Baykant Alagoz, Gurkan Kavuran, Celaleddin Yeroglu, Norbert Herencsar, Shibendu Mahata",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.119192"
    },
    {
        "id": 32945,
        "title": "Multi‐dimensional resource management with deep deterministic policy gradient for digital twin‐enabled Industrial Internet of Things in 6 generation",
        "authors": "Yue Hu, Ning Cao, Hao Lu, Yunzhe Jiang, Yinqiu Liu, Xiaoming He",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractIn the era of sixth generation mobile networks (6G), industrial big data is rapidly generated due to the increasing data‐driven applications in the Industrial Internet of Things (IIoT). Effectively processing such data, for example, knowledge learning, on resource‐limited IIoT devices becomes a challenge. To this end, we introduce a cloud‐edge‐end collaboration architecture, in which computing, communication, and storage resources are flexibly coordinated to alleviate the issue of resource constraints. To achieve better performance in hyper‐connected experience, real‐time communication, and sustainable computing, we construct a novel architecture combining digital twin (DT)‐IIoT with edge networks. In addition, considering the energy consumption and delay issues in distributed learning, we propose a deep reinforcement learning‐based method called deep deterministic policy gradient with double actors and double critics (D4PG) to manage the multi‐dimensional resources, that is, CPU cycles, DT models, and communication bandwidths, enhancing the exploration ability and improving the inaccurate value estimation of agents in continuous action spaces. In addition, we introduce a synchronization threshold for distributed learning framework to avoid the synchronization latency caused by stragglers. Extensive experimental results prove that the proposed architecture can efficiently conduct knowledge learning, and the intelligent scheme can also improve system efficiency by managing multi‐dimensional resources.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/ett.4962"
    },
    {
        "id": 32946,
        "title": "Deep Deterministic Policy Gradient for Nested Parallel Negotiation",
        "authors": "Ryota Arakawa, Katsuhide Fujita",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wi-iat59888.2023.00032"
    },
    {
        "id": 32947,
        "title": "Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient",
        "authors": "Ming Zhan, Jingjing Fan, Jianying Guo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3305453"
    },
    {
        "id": 32948,
        "title": "Delayed Deep Deterministic Policy Gradient-Based Energy Management Strategy for Overall Energy Consumption Optimization of Dual Motor Electrified Powertrain",
        "authors": "Jiageng Ruan, Changcheng Wu, Hanghang Cui, Weihan Li, Dirk Uwe Sauer",
        "published": "2023-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2023.3265073"
    },
    {
        "id": 32949,
        "title": "Local Path Planning with Turnabouts for Mobile Robot by Deep Deterministic Policy Gradient",
        "authors": "Tomoaki Nakamura, Masato Kobayashi, Naoki Motoi",
        "published": "2023-3-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icm54990.2023.10101921"
    },
    {
        "id": 32950,
        "title": "Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge",
        "authors": "Bolun Zhang, Nguyen Van Huynh",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436824"
    },
    {
        "id": 32951,
        "title": "Deep Reinforcement Learning: Policy Gradients for US Equities Trading",
        "authors": "Miquel Noguer i Alonso, Himanshu Agrawal, David Pacheco Aznar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4645453"
    },
    {
        "id": 32952,
        "title": "Development of a Deep Deterministic Policy Gradient (DDPG) Algorithm for Suturing Task Automation",
        "authors": "Antonella Imperato, Marco Caianiello, Fanny Ficuciello",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406967"
    },
    {
        "id": 32953,
        "title": "Deep Deterministic Policy Gradient (DDPG) Agent-Based Sliding Mode Control for Quadrotor Attitudes",
        "authors": "Wenjun Hu, Yueneng Yang, Zhiyang Liu",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "A novel reinforcement deep learning deterministic policy gradient agent-based sliding mode control (DDPG-SMC) approach is proposed to suppress the chattering phenomenon in attitude control for quadrotors, in the presence of external disturbances. First, the attitude dynamics model of the quadrotor under study is derived, and the attitude control problem is described using formulas. Second, a sliding mode controller, including its sliding mode surface and reaching law, is chosen for the nonlinear dynamic system. The stability of the designed SMC system is validated through the Lyapunov stability theorem. Third, a reinforcement learning (RL) agent based on deep deterministic policy gradient (DDPG) is trained to adaptively adjust the switching control gain. During the training process, the input signals for the agent are the actual and desired attitude angles, while the output action is the time-varying control gain. Finally, the trained agent mentioned above is utilized in the SMC as a parameter regulator to facilitate the adaptive adjustment of the switching control gain associated with the reaching law. The simulation results validate the robustness and effectiveness of the proposed DDPG-SMC method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/drones8030095"
    },
    {
        "id": 32954,
        "title": "Power Control in Device-to-Device Communications using Deep Deterministic Policy Gradient Method",
        "authors": "Ranjeet Kumar, Saikat Majumder",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112096"
    },
    {
        "id": 32955,
        "title": "Enhanced Fingerprint Image Compression using Deep Deterministic Policy Gradient",
        "authors": "Abdelhak Ouanane, Mohamed Riad Yagoubi, Amina Serir, Nacereddine Djelal",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceeat60471.2023.10425859"
    },
    {
        "id": 32956,
        "title": "Smart Noise Jamming Power Adjustment Using Exploratory Deep Deterministic Policy Gradient",
        "authors": "Yujie Zhang, Weibo Huo, Cui Zhang, Jifang Pei, Yin Zhang, Yulin Huang",
        "published": "2023-5-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149662"
    },
    {
        "id": 32957,
        "title": "Deep Deterministic Policy Gradient for Throughput Maximization in Energy Harvesting NOMA-Cognitive Radio Network",
        "authors": "Lav Garg, Saikat Majumder, Sumit Chakravarty",
        "published": "2023-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iconat57137.2023.10080443"
    },
    {
        "id": 32958,
        "title": "Integrating Multi-Agent Deep Deterministic Policy Gradient and Go-Explore for Enhanced Reward Optimization",
        "authors": "Muchen Liu",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "The field of Multi-Agent Reinforcement Learning (MARL) continues to advance with the development of new and effective methods. This research is centered on two prominent approaches within this field: Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Go-Explore. The study explores the synergistic potential of combining these two methodologies to enhance rewards for individual agents as well as for agent groups. In the course of this research, MADDPG is introduced into the experimental environment, providing agents with both actor networks (policy networks) and critic networks (Q networks) to implement the actor-critic model. Additionally, each individual agent is equipped with a Go-Explore network, empowering them to conduct deeper explorations of the environment and accumulate rewards at an accelerated rate, often resulting in higher overall rewards. This novel approach emphasizes achieving a balance between individual and collaborative rewards, offering a promising avenue for optimizing multi-agent systems. The results of this study demonstrate that the combined method exhibits notable advantages in certain scenarios. Specifically, it showcases a higher rate of reward accumulation and improved overall performance. This research contributes to the MARL domain by highlighting the potential of combining MADDPG and Go-Explore to enhance the efficiency and effectiveness of multi-agent systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/znrt8d63"
    },
    {
        "id": 32959,
        "title": "Dynamic Prioritization and Adaptive Scheduling Using Deep Deterministic Policy Gradient for Deploying Microservice-Based VNFs",
        "authors": "Swarna B. Chetty, Hamed Ahmadi, Avishek Nag",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278718"
    },
    {
        "id": 32960,
        "title": "Hybrid Energy Storage Control Method For DC Microgrid Based On Deep Deterministic Policy Gradient",
        "authors": "Shengji Tan, Min Ding, Zili Tao, Danyun Li, Zhijian Fang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450954"
    },
    {
        "id": 32961,
        "title": "Research on Automatic Lane Changing Method for Electric Vehicles Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060104"
    },
    {
        "id": 32962,
        "title": "A Multi-Agent Deep Deterministic Policy Gradient Method for Multi-Zone HVAC Control",
        "authors": "Xuebo Liu, Yingying Wu, Bo Liu, Hongyu Wu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10252541"
    },
    {
        "id": 32963,
        "title": "Power Allocation in Cell-Free mmWave Massive MIMO: Using Deep Deterministic Policy Gradient",
        "authors": "Yu Zhao, Fengming Zhang, Yangjun Gao, Chaoqi Fu",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wccct56755.2023.10052585"
    },
    {
        "id": 32964,
        "title": "Deep Deterministic Policy Gradient based Dynamic Virtual Network Embedding Algorithm",
        "authors": "Yue Zong, Han Xu, Zhaoyang Zhang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640912.3640982"
    },
    {
        "id": 32965,
        "title": "Retracted: An Automatic Driving Control Method Based on Deep Deterministic Policy Gradient",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9896258"
    },
    {
        "id": 32966,
        "title": "Comparative Study for Deep Deterministic Policy Gradient and Soft Actor Critic Using an Inverted Pendulum System",
        "authors": "Aditya Shelke, Devang Vyas, Abhishek Srivastava",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370289"
    },
    {
        "id": 32967,
        "title": "A Control Method of Robotic Arm Based on Improved Deep Deterministic Policy Gradient",
        "authors": "Yanpeng Shao, Haibo Zhou, Shuaishuai Zhao, Xiaoyan Fan, Jiayi Jiang",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icma57826.2023.10215662"
    },
    {
        "id": 32968,
        "title": "Attitude Control of Rotary Steering Drilling Stabilized Platform Based on Improved Deep Deterministic Policy Gradient",
        "authors": "Aiqing Huo, Kun Zhang, Shuhan Zhang",
        "published": "2024-2-14",
        "citations": 2,
        "abstract": "Summary\nThe rotary steerable drilling system is an advanced drilling technology, with stabilized platform toolface attitude control being a critical component. Due to a multitude of downhole interference factors, coupled with nonlinearities and uncertainties, challenges arise in model establishment and attitude control. Furthermore, considering that stabilized platform toolface attitude determines the drilling direction of the entire drill bit, the effectiveness of toolface attitude control will directly impact the precision and success of drilling tool guidance. In this paper, a mathematical model and a friction model of the stabilized platform are established, and an improved deep deterministic policy gradient (I_DDPG) attitude control method is proposed to address the friction nonlinearity problem existing in the rotary steering drilling stabilized platform. A prioritized experience replay based on temporal difference (TD) error and policy gradient is introduced to improve sample usage, and high similarity samples are pruned to prevent overfitting. Furthermore, SumTree structure is adopted to sort samples for reducing computational effort, and a double critic network is used to alleviate the overestimated value. Numerical simulation results illustrate that the stabilized platform attitude control system based on I_DDPG can achieve high control accuracy with both strong anti-interference capability and good robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/217992-pa"
    },
    {
        "id": 32969,
        "title": "Simulated Annealing-Deep Deterministic Policy Gradient Algorithm For Quadrotor Attitude Control",
        "authors": "Taha Yacine Trad, Kheireddine Choutri, Mohand Lagha, Raouf Fareh, Maamar Bettayeb",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aset56582.2023.10180752"
    },
    {
        "id": 32970,
        "title": "Optimization of an MPC Based Motion Cueing Algorithm with Deep Deterministic Policy Gradient",
        "authors": "Yi Liang, Dongsu Wu, Rongjun Fu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccasit58768.2023.10351597"
    },
    {
        "id": 32971,
        "title": "Power flow rebalancing in optimal scheduling of smart distribution systems based on Deep Deterministic Policy Gradient",
        "authors": "Xinyu Ai, Junfeng Cai, Jingrui Zhang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135884"
    },
    {
        "id": 32972,
        "title": "A path following controller for deep-sea mining vehicles considering slip control and random resistance based on improved deep deterministic policy gradient",
        "authors": "Qihang Chen, Jianmin Yang, Jinghang Mao, Zhixuan Liang, Changyu Lu, Pengfei Sun",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.114069"
    },
    {
        "id": 32973,
        "title": "Research on Wargame Decision-Making Method Based on Multi-Agent Deep Deterministic Policy Gradient",
        "authors": "Sheng Yu, Wei Zhu, Yong Wang",
        "published": "2023-4-4",
        "citations": 0,
        "abstract": "Wargames are essential simulators for various war scenarios. However, the increasing pace of warfare has rendered traditional wargame decision-making methods inadequate. To address this challenge, wargame-assisted decision-making methods that leverage artificial intelligence techniques, notably reinforcement learning, have emerged as a promising solution. The current wargame environment is beset by a large decision space and sparse rewards, presenting obstacles to optimizing decision-making methods. To overcome these hurdles, a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) based wargame decision-making method is presented. The Partially Observable Markov Decision Process (POMDP), joint action-value function, and the Gumbel-Softmax estimator are applied to optimize MADDPG in order to adapt to the wargame environment. Furthermore, a wargame decision-making method based on the improved MADDPG algorithm is proposed. Using supervised learning in the proposed approach, the training efficiency is improved and the space for manipulation before the reinforcement learning phase is reduced. In addition, a policy gradient estimator is incorporated to reduce the action space and to obtain the global optimal solution. Furthermore, an additional reward function is designed to address the sparse reward problem. The experimental results demonstrate that our proposed wargame decision-making method outperforms the pre-optimization algorithm and other algorithms based on the AC framework in the wargame environment. Our approach offers a promising solution to the challenging problem of decision-making in wargame scenarios, particularly given the increasing speed and complexity of modern warfare.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13074569"
    },
    {
        "id": 32974,
        "title": "A Task-Oriented Hybrid Routing Approach based on Deep Deterministic Policy Gradient",
        "authors": "Zongxuan Sha, Ru Huo, Chuang Sun, Shuo Wang, Tao Huang",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comcom.2023.07.040"
    },
    {
        "id": 32975,
        "title": "Design of missile longitudinal controller based on deep deterministic policy gradient learning algorithm",
        "authors": "Wanchao Zhang, Guangshan Chen, Hao Ni, TingShuai Tong, YiWen Zhou",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291719"
    },
    {
        "id": 32976,
        "title": "UAV Coverage Path Planning with Quantum-based Recurrent Deep Deterministic Policy Gradient",
        "authors": " Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2023.3347219"
    },
    {
        "id": 32977,
        "title": "Trust Metric-Based Anomaly Detection via Deep Deterministic Policy Gradient Reinforcement Learning Framework",
        "authors": "Shruthi N, Siddesh G K",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "Addressing real-time network security issues is paramount due to the rapidly expanding IoT jargon. The erratic rise in usage of inadequately secured IoT- based sensory devices like wearables of mobile users, autonomous vehicles, smartphones and appliances by a larger user community is fuelling the need for a trustable, super-performant security framework. An efficient anomaly detection system would aim to address the anomaly detection problem by devising a competent attack detection model. This paper delves into the Deep Deterministic Policy Gradient (DDPG) approach, a promising Reinforcement Learning platform to combat noisy sensor samples which are instigated by alarming network attacks. The authors propose an enhanced DDPG approach based on trust metrics and belief networks, referred to as Deep Deterministic Policy Gradient Belief Network (DDPG-BN). This deep-learning-based approach is projected as an algorithm to provide “Deep-Defense” to the plethora of network attacks. Confidence interval is chosen as the trust metric to decide on the termination of sensor sample collection. Once an enlisted attack is detected, the collection of samples from the particular sensor will automatically cease. The evaluations and results of the experiments highlight a better detection accuracy of 98.37% compared to its counterpart conventional DDPG implementation of 97.46%. The paper also covers the work based on a contemporary Deep Reinforcement Learning (DRL) algorithm, the Actor Critic (AC). The proposed deep learning binary classification model is validated using the NSL-KDD dataset and the performance is compared to a few deep learning implementations as well.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijcnc.2023.15601"
    },
    {
        "id": 32978,
        "title": "A Switching Strategy for Run-to-Run Control Using Deep Deterministic Policy Gradient Algorithm and Integral Controller",
        "authors": "Zhu Ma, Tianhong Pan",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450404"
    },
    {
        "id": 32979,
        "title": "A Dual-Critic Deep Deterministic Policy Gradient Approach for Task Offloading in Edge-Fog-Cloud Environment",
        "authors": "Moshira A. Ebrahim, Gamal A. Ebrahim, Hoda K. Mohamed",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icca59364.2023.10401692"
    },
    {
        "id": 32980,
        "title": "Deep Deterministic Policy Gradient With Compatible Critic Network",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023-8",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3117790"
    },
    {
        "id": 32981,
        "title": "Optimization control of the double‐capacity water tank‐level system using the deep deterministic policy gradient algorithm",
        "authors": "Likun Ye, Pei Jiang",
        "published": "2023-11",
        "citations": 0,
        "abstract": "AbstractProcess control systems are subject to external factors such as changes in working conditions and perturbation interference, which can significantly affect the system's stability and overall performance. The application and promotion of intelligent control algorithms with self‐learning, self‐optimization, and self‐adaption characteristics have thus become a challenging yet meaningful research topic. In this article, we propose a novel approach that incorporates the deep deterministic policy gradient (DDPG) algorithm into the control of double‐capacity water tanklevel system. Specifically, we introduce a fully connected layer on the observer side of the critic network to enhance its expression capability and processing efficiency, allowing for the extraction of important features for water‐level control. Additionally, we optimize the node parameters of the neural network and use the RELU activation function to ensure the network's ability to continuously observe and learn from the external water tank environment while avoiding the issue of vanishing gradients. We enhance the system's feedback regulation ability by adding the PID controller output to the observer input based on the liquid level deviation and height. This integration with the DDPG control method effectively leverages the benefits of both, resulting in improved robustness and adaptability of the system. Experimental results show that our proposed model outperforms traditional control methods in terms of convergence, tracking, anti‐disturbance and robustness performances, highlighting its effectiveness in improving the stability and precision of double‐capacity water tank systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/eng2.12668"
    },
    {
        "id": 32982,
        "title": "5G communication resource allocation strategy for mobile edge computing based on deep deterministic policy gradient",
        "authors": "Jun He",
        "published": "2023-3",
        "citations": 2,
        "abstract": "AbstractDistributed base station deployment, limited server resources and dynamically changing end users in mobile edge networks make the design of computing offloading schemes extremely challenging. Considering the advantages of deep reinforcement learning (DRL) in dealing with dynamic complex problems, this paper designs an optimal computing offloading and resource allocation strategy. Firstly, the authors consider a multi‐user mobile edge network scenario consisting of Macro‐cell Base Station (MBS), Small‐cell Base Station (SBS) and multiple terminal devices, the communication overhead and calculation overhead generated are formulated and described in detail. Besides, combined with the deterministic delay of tasks, the optimization objective of this paper is clarified to comprehensively consider system energy consumption. Then, a learning algorithm based on Deep Deterministic Policy Gradient (DDPG) is proposed to minimize system energy consumption. Finally, simulation experiments show that the authors’ proposed DDPG algorithm can effectively optimize the target value, and the total system energy consumption is only 15.6 J, which is better than other compared algorithms. It is also proved that the proposed algorithm has excellent communication resource allocation ability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/tje2.12250"
    },
    {
        "id": 32983,
        "title": "Multi-PET Cooperative Autonomous Navigation Based on Multi-agent Deep Deterministic Policy Gradient",
        "authors": "Yichuan Huang, Yuhui Song, Zeyang Liu, Zhanhua Pan, Jisong Zhu, Zhaoxia Jing",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135975"
    },
    {
        "id": 32984,
        "title": "Reinforcement Learning Approach with Deep Deterministic Policy Gradient DDPG-Controlled Virtual Synchronous Generator for an Islanded Microgrid",
        "authors": "Mohamed A. Afifi, Mostafa I. Marei, Ahmed M.I. Mohamad",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mepcon58725.2023.10462333"
    },
    {
        "id": 32985,
        "title": "An enhanced deep deterministic policy gradient algorithm for intelligent control of robotic arms",
        "authors": "Ruyi Dong, Junjie Du, Yanan Liu, Ali Asghar Heidari, Huiling Chen",
        "published": "2023-1-23",
        "citations": 3,
        "abstract": "Aiming at the poor robustness and adaptability of traditional control methods for different situations, the deep deterministic policy gradient (DDPG) algorithm is improved by designing a hybrid function that includes different rewards superimposed on each other. In addition, the experience replay mechanism of DDPG is also improved by combining priority sampling and uniform sampling to accelerate the DDPG’s convergence. Finally, it is verified in the simulation environment that the improved DDPG algorithm can achieve accurate control of the robot arm motion. The experimental results show that the improved DDPG algorithm can converge in a shorter time, and the average success rate in the robotic arm end-reaching task is as high as 91.27%. Compared with the original DDPG algorithm, it has more robust environmental adaptability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fninf.2023.1096053"
    },
    {
        "id": 32986,
        "title": "Deep deterministic policy gradient with constraints for gait optimisation of biped robots",
        "authors": "Xingyang Liu, Haina Rong, Ferrante Neri, Peng Yue, Gexiang Zhang",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "In this paper, we propose a novel Reinforcement Learning (RL) algorithm for robotic motion control, that is, a constrained Deep Deterministic Policy Gradient (DDPG) deviation learning strategy to assist biped robots in walking safely and accurately. The previous research on this topic highlighted the limitations in the controller’s ability to accurately track foot placement on discrete terrains and the lack of consideration for safety concerns. In this study, we address these challenges by focusing on ensuring the overall system’s safety. To begin with, we tackle the inverse kinematics problem by introducing constraints to the damping least squares method. This enhancement not only addresses singularity issues but also guarantees safe ranges for joint angles, thus ensuring the stability and reliability of the system. Based on this, we propose the adoption of the constrained DDPG method to correct controller deviations. In constrained DDPG, we incorporate a constraint layer into the Actor network, incorporating joint deviations as state inputs. By conducting offline training within the range of safe angles, it serves as a deviation corrector. Lastly, we validate the effectiveness of our proposed approach by conducting dynamic simulations using the CRANE biped robot. Through comprehensive assessments, including singularity analysis, constraint effectiveness evaluation, and walking experiments on discrete terrains, we demonstrate the superiority and practicality of our approach in enhancing walking performance while ensuring safety. Overall, our research contributes to the advancement of biped robot locomotion by addressing gait optimisation from multiple perspectives, including singularity handling, safety constraints, and deviation learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/ica-230724"
    },
    {
        "id": 32987,
        "title": "Optimization of Robotic Arm Grasping through Fractional-Order Deep Deterministic Policy Gradient Algorithm",
        "authors": "Hui Geng, Qi Hu, Zhe Wang",
        "published": "2023-11-1",
        "citations": 1,
        "abstract": "Abstract\nWith the rapid development of robotics technology, robotic arm grasping has gained significant attention in the fields of automation and artificial intelligence. In this study, we propose a fractional-order deep deterministic policy gradient (DDPG) algorithm for optimizing robotic arm grasping tasks. Traditional machine learning algorithms face challenges in handling continuous action spaces, while the DDPG algorithm effectively addresses this issue. In this research, we first review the background and challenges of robotic arm grasping and provide an overview of the application of traditional reinforcement learning algorithms in grasping tasks. Subsequently, we introduce the principles and fundamental ideas of the DDPG algorithm in detail, discussing its potential for optimizing robotic arm grasping. To further enhance the performance of robotic arm grasping, we propose an improved approach based on fractional-order control. Fractional-order control exhibits unique advantages in environmental dynamics modeling and grasp posture optimization, enhancing the robustness and adaptability of robotic arm grasping. Through a series of experiments, we validate the effectiveness and superiority of the fractional-order DDPG algorithm in robotic arm grasping tasks. Our algorithm achieves significant improvements in grasping success rate and stability compared to traditional methods. The experimental results demonstrate that the fractional-order DDPG algorithm is better equipped to handle control challenges in continuous action spaces and optimize the performance of robotic arm grasping tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2637/1/012006"
    },
    {
        "id": 32988,
        "title": "Energy Management for Hybrid Energy Storage System in Electric Based on Deep Deterministic Policy Gradient",
        "authors": "Shuai Xia, Chun Wang",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "In this paper, an intelligent control system design scheme based on deep deterministic policy gradient (DDPG) algorithm is proposed for the complex continuous action space problem in the hybrid energy storage system of electric vehicles. Firstly, the basic principle and internal logic of DDPG algorithm are introduced, including key elements such as Actor-Critic architecture, experience playback, target network, reward signal, policy gradient and value function update. Then, how to apply the DDPG algorithm to the industrial control system is described in detail. The Actor network learns the optimal strategy, the Critic network evaluates the value of the state-action pair, and uses the experience playback and the target network to improve the system stability and performance. Finally, the effect of the intelligent control system based on DDPG algorithm in complex environment is verified by simulation experiments. The results show that the system can effectively optimize the control strategy, improve the response speed and stability of the system, and has a good engineering application prospect.",
        "keywords": "",
        "link": "http://dx.doi.org/10.62051/ijcsit.v2n1.22"
    },
    {
        "id": 32989,
        "title": "A Deep Deterministic Policy Gradient Algorithm Based Controller with Adjustable Learning Rate for DC-AC Inverters",
        "authors": "Jian Ye, Sen Mei, Huanyu Guo, Di Zhao, Xinan Zhang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/peas58692.2023.10395502"
    },
    {
        "id": 32990,
        "title": "DDRCN: Deep Deterministic Policy Gradient Recommendation Framework Fused with Deep Cross Networks",
        "authors": "Tianhan Gao, Shen Gao, Jun Xu, Qihui Zhao",
        "published": "2023-2-16",
        "citations": 1,
        "abstract": "As an essential branch of artificial intelligence, recommendation systems have gradually penetrated people’s daily lives. It is the active recommendation of goods or services of potential interest to users based on their preferences. Many recommendation methods have been proposed in both industry and academia. However, there are some limitations of previous recommendation methods: (1) Most of them do not consider the cross-correlation between data. (2) Many treat the recommendation process as a one-time act and do not consider the continuity of the recommendation system. To overcome these limitations, we propose a recommendation framework based on deep reinforcement learning techniques, known as DDRCN: a deep deterministic policy gradient recommendation framework incorporating deep cross networks. We use a Deep network and a Cross network to fit the cross relationships between the data, to obtain a representation of the user interaction data. The Actor-Critic network is designed to simulate the continuous interaction behavior of users through a greedy strategy. A deep deterministic policy gradient network is also used to train the recommendation model. Finally, we conduct experiments with two publicly available datasets and find that our proposed recommendation framework outperforms the baseline approach in the recall and ranking phases of recommendations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13042555"
    },
    {
        "id": 32991,
        "title": "Enhancing Crane Handling Safety: A Deep Deterministic Policy Gradient Approach to Collision-Free Path Planning",
        "authors": "Rafaela Iovanovichi Machado, Matheus Dos Santos Machado, Silvia Silva Da Costa Botelho",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indin51400.2023.10218087"
    },
    {
        "id": 32992,
        "title": "Deep deterministic policy gradient reinforcement learning for collision-free navigation of mobile robots in unknown environments",
        "authors": "Taner Yılmaz, Ömür Aydoğmus",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5505/fujece.2023.85047"
    },
    {
        "id": 32993,
        "title": "Deep deterministic policy gradient based multi-UAV control for moving convoy tracking",
        "authors": "Armaan Garg, Shashi Shekhar Jha",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107099"
    },
    {
        "id": 32994,
        "title": "WSEE Optimization of Cell-Free mMIMO Uplink Using Deep Deterministic Policy Gradient",
        "authors": "Abhinav Kumar, Venkatesh Tentu, Dheeraj Naidu Amudala, Rohit Budhiraja",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcomm.2022.3209867"
    },
    {
        "id": 32995,
        "title": "Deep-Deterministic Policy Gradient Based Multi-Resource Allocation in Edge-Cloud System: A Distributed Approach",
        "authors": "Arslan Qadeer, Myung Jong Lee",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3249153"
    },
    {
        "id": 32996,
        "title": "Wide-angle monostatic radar cross section enhanced metasurface based on deep deterministic policy gradient",
        "authors": "Yilin Jiang, Chengyue Yan, Yuxuan Pan, Jinxin Li, Yuxi Tian",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/elex.20.20230315"
    },
    {
        "id": 32997,
        "title": "Exploration- and Exploitation-Driven Deep Deterministic Policy Gradient for Active SLAM in Unknown Indoor Environments",
        "authors": "Shengmin Zhao, Seung-Hoon Hwang",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "This study proposes a solution for Active Simultaneous Localization and Mapping (Active SLAM) of robots in unknown indoor environments using a combination of Deep Deterministic Policy Gradient (DDPG) path planning and the Cartographer algorithm. To enhance the convergence speed of the DDPG network and minimize collisions with obstacles, we devised a unique reward function that integrates exploration and exploitation strategies. The exploration strategy allows the robot to achieve the shortest running time and movement trajectory, enabling efficient traversal of unmapped environments. Moreover, the exploitation strategy introduces active closed loops to enhance map accuracy. We conducted experiments using the simulation platform Gazebo to validate our proposed model. The experimental results demonstrate that our model surpasses other Active SLAM methods in exploring and mapping unknown environments, achieving significant grid completeness of 98.7%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13050999"
    },
    {
        "id": 32998,
        "title": "Hyperspectral image classification based on deep deterministic policy gradient",
        "authors": "Jian Zhou, Qianqian Cheng, Yu Su, Yuhe Qiu, Hu He, Jiawei Huang, Shuijie Wang",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2668126"
    },
    {
        "id": 32999,
        "title": "Current-constraint speed regulation for PMSM based on port-controlled Hamiltonian realization and deep deterministic policy gradient",
        "authors": "Min Wang, Yanhong Liu, Qi Wang, Patrick Wheeler",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/elex.20.20230516"
    },
    {
        "id": 33000,
        "title": "Perception Enhanced Deep Deterministic Policy Gradient for Autonomous Driving in Complex Scenarios",
        "authors": "Lyuchao Liao, Hankun Xiao, Pengqi Xing, Zhenhua Gan, Youpeng He, Jiajun Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmes.2024.047452"
    },
    {
        "id": 33001,
        "title": "Multi-Agent Collaborative Target Search Based on the Multi-Agent Deep Deterministic Policy Gradient with Emotional Intrinsic Motivation",
        "authors": "Xiaoping Zhang, Yuanpeng Zheng, Li Wang, Arsen Abdulali, Fumiya Iida",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Multi-agent collaborative target search is one of the main challenges in the multi-agent field, and deep reinforcement learning (DRL) is a good way to learn such a task. However, DRL always faces the problem of sparse reward, which to some extent reduces its efficiency in task learning. Introducing intrinsic motivation has proved to be a useful way to make the sparse reward in DRL. So, based on the multi-agent deep deterministic policy gradient (MADDPG) structure, a new MADDPG algorithm with the emotional intrinsic motivation name MADDPG-E is proposed in this paper for the multi-agent collaborative target search. In MADDPG-E, a new emotional intrinsic motivation module with three emotions, joy, sadness, and fear, is designed. The three emotions are defined by corresponding psychological knowledge to the multi-agent embodied situations in an environment. An emotional steady-state variable function H is then designed to help judge the goodness of the emotions. Based on H, an emotion-based intrinsic reward function is finally proposed. With the designed emotional intrinsic motivation module, the multi-agent system always tries to make itself joy, which means it always learns to search the target. To show the effectiveness of the proposed MADDPG-E algorithm, two kinds of simulation experiments with a determined initial position and random initial position, respectively, are carried out, and comparisons are performed with MADDPG as well as MADDPG-ICM (MADDPG with an intrinsic curiosity module). The results show that with the designed emotional intrinsic motivation module, MADDPG-E has a higher learning speed and better learning stability, and the advantage is more obvious when facing complex situations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132111951"
    },
    {
        "id": 33002,
        "title": "DEEP DETERMINISTIC POLICY GRADIENT AND GRAPH CONVOLUTIONAL NETWORKS FOR TOPOLOGY OPTIMIZATION OF BRACED STEEL FRAMES",
        "authors": "Chi-tathon KUPWIWAT, Yuichi IWAGOE, Kazuki HAYASHI, Makoto OHSAKI",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3130/aijjse.69b.0_129"
    },
    {
        "id": 33003,
        "title": "Optimal Scheduling of Wind-Photovoltaic-Energy Storage System Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Qingquan Ye, Xuguang Wu, Liyuan Chen, Xingda Wen, Qidai Lin, Yizhi Shi, Hongru Liu",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135686"
    },
    {
        "id": 33004,
        "title": "Combining multi-agent deep deterministic policy gradient and rerouting technique to improve traffic network performance under mixed traffic conditions",
        "authors": "Hung Tuan Trinh, Sang-Hoon Bae, Duy Quang Tran",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": " In the future, mixed traffic flow will include two types of vehicles: connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs). CAVs emerge as new solutions to disrupt the traditional transportation system. This new solution shares real-time data with each other and the roadside units (RSU) for network management. Reinforcement learning (RL) is a promising approach for traffic signal management in complex urban areas by leveraging information gathered from CAVs. In particular, coordinating signal management at many intersections is a critical challenge in multi-agent reinforcement learning (MARL). According to this vision, we propose an approach that combines an actor–critic network–based multi-agent deep deterministic policy gradient (MADDPG) model and a rerouting technique (RT) to increase traffic performance in vehicular networks. This algorithm overcomes the inherent non-stationary of Q-learning and the high variance of policy gradient (PG) algorithms. Based on centralized learning with decentralized execution, the MADDPG model employs one actor and one critic for each agent. The actor network uses local information to execute actions, while the critic network is trained with extra information, including the states and actions of other agents. Through a centralized learning process, agents can coordinate with each other, diminishing the influence of an unstable environment. Unlike previous studies, we not only manage traffic light systems but also consider the effect of platooning vehicles on increasing throughput. Experimental results show that our model outperforms other models in terms of traffic performance in different scenarios. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/00375497241237831"
    },
    {
        "id": 33005,
        "title": "Optimizing <scp>UAV</scp> computation offloading via <scp>MEC</scp> with deep deterministic policy gradient",
        "authors": "Ahmed Bashir Abbasi, Muhammad Usman Hadi",
        "published": "2024-1",
        "citations": 0,
        "abstract": "AbstractMobile edge computing (MEC) seems to be highly efficient to process the generated data from IoT devices by providing computational resources locating in close range to network edge. MEC can be promising in reduction of latency and consumption of energy from data transmissions from offloading computational tasks from IoT devices to nearby edge servers. In the context of the growing IoT ecosystem, there is an increasing need for efficient data processing and communication strategies. There is a demand of bridging the gap in current research with novel optimization algorithms tailored for UAV‐assisted MEC systems, shedding light on the necessity of efficient computation offloading in meeting the demands of the IoT era. In this article, a computation offloading optimization algorithm is proposed which is based on deep deterministic policy gradient for realistic Aurelia X6 Pro unmanned aerial vehicle (UAV)‐assisted MEC systems. The proposed algorithm optimizes the offloading decision for UAVs by taking task characteristics and the communication environment into consideration. To demonstrate the effectiveness of the proposed algorithm, comprehensive simulations were conducted, and the results indicate substantial improvements in MEC systems' competency. Our research not only showcases the feasibility of deep deterministic policy gradient in UAV‐assisted MEC systems but also highlights the importance of developing efficient computation offloading strategies for the evolving landscape of IoT and edge computing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/ett.4874"
    },
    {
        "id": 33006,
        "title": "Reward adaptive wind power tracking control based on deep deterministic policy gradient",
        "authors": "Peng Chen, Dezhi Han",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2023.121519"
    },
    {
        "id": 33007,
        "title": "Integration of design and control for renewable energy systems with an application to anaerobic digestion: A deep deterministic policy gradient framework",
        "authors": "Tannia A. Mendiola-Rodriguez, Luis A. Ricardez-Sandoval",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.127212"
    },
    {
        "id": 33008,
        "title": "Joint Optimization of Trajectory and Task Offloading Based on Deep Deterministic Policy Gradient in UAV-Assisted MEC",
        "authors": "Qisheng Tan, Jielin Fu",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icct59356.2023.10419420"
    },
    {
        "id": 33009,
        "title": "Efficient Massive-Device Orchestration Through Reinforcement Learning With Boosted Deep Deterministic Policy Gradient",
        "authors": "Haowei Shi, Jiadao Zou, Qingxue Zhang",
        "published": "2024-2-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2023.3301795"
    },
    {
        "id": 33010,
        "title": "Online convex optimization with switching cost and delayed gradients",
        "authors": "Spandan Senapati, Rahul Vaze",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.peva.2023.102371"
    },
    {
        "id": 33011,
        "title": "Energy Efficiency Resource Management for D2D-NOMA Enabled Network: A Dinkelbach Combined Twin Delayed Deterministic Policy Gradient Approach",
        "authors": "Xue Wang, Haotian Shi, Yanqi Li, Zhihong Qian, Zhu Han",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2023.3267452"
    },
    {
        "id": 33012,
        "title": "Deep Deterministic Policy Gradient and Active Disturbance Rejection Controller based coordinated control for gearshift manipulator of driving robot",
        "authors": "Gang Chen, Zhifeng Chen, Liangmo Wang, Weigong Zhang",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105586"
    },
    {
        "id": 33013,
        "title": "A perspective on the use of deep deterministic policy gradient reinforcement learning for retention time modeling in reversed-phase liquid chromatography",
        "authors": "Alexander Kensert, Gert Desmet, Deirdre Cabooter",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chroma.2023.464570"
    },
    {
        "id": 33014,
        "title": "Hardware-in-the-loop Testing of a Deep Deterministic Policy Gradient Algorithm as a Microgrid Secondary Controller",
        "authors": "Pedro I. N. Barbalho, Denis V. Coury, Vinicius A. Lacerda, Ricardo A. S. Fernandes",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isgteurope56780.2023.10407700"
    },
    {
        "id": 33015,
        "title": "Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient",
        "authors": "Maheed H. Ahmed, Abdullah AboHussien, Aly El-Shafei, Ahmed M. Darwish, Ahmed H. Abdel-Gawad",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106593"
    },
    {
        "id": 33016,
        "title": "Enhancing Longitudinal Velocity Control With Attention Mechanism-Based Deep Deterministic Policy Gradient (DDPG) for Safety and Comfort",
        "authors": "Fahmida Islam, John E. Ball, Christopher T. Goodin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3368435"
    },
    {
        "id": 33017,
        "title": "Bidirectional Obstacle Avoidance Enhancement‐Deep Deterministic Policy Gradient: A Novel Algorithm for Mobile‐Robot Path Planning in Unknown Dynamic Environments",
        "authors": "Junxiao Xue, Shiwen Zhang, Yafei Lu, Xiaoran Yan, Yuanxun Zheng",
        "published": "2024-2-6",
        "citations": 0,
        "abstract": "Real‐time path planning in unknown dynamic environments is a significant challenge for mobile robots. Many researchers have attempted to solve this problem by introducing deep reinforcement learning, which trains agents through interaction with their environments. A method called BOAE‐DDPG, which combines the novel bidirectional obstacle avoidance enhancement (BOAE) mechanism with the deep deterministic policy gradient (DDPG) algorithm, is proposed to enhance the learning ability of obstacle avoidance. Inspired by the analysis of the reaction advantage in dynamic psychology, the BOAE mechanism focuses on obstacle‐avoidance reactions from the state and action. The cross‐attention mechanism is incorporated to enhance the attention to valuable obstacle‐avoidance information. Meanwhile, the obstacle‐avoidance behavioral advantage is separately estimated using the modified dueling network. Based on the learning goals of the mobile robot, new assistive reward factors are incorporated into the reward function to promote learning and convergence. The proposed method is validated through several experiments conducted using the simulation platform Gazebo. The results show that the proposed method is suitable for path planning tasks in unknown environments and has an excellent obstacle‐avoidance learning capability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/aisy.202300444"
    },
    {
        "id": 33018,
        "title": "Price-taker Bidding and Pricing Strategy Using Deep Deterministic Policy Gradient Algorithm with Transformer Neural Networks",
        "authors": "Jiao Shu, Ningkai Tang, Wenteng Kuang, Tianyu Chen, Jixiang Lu, Wei Wang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135772"
    },
    {
        "id": 33019,
        "title": "Modeling Interactions of Autonomous/Manual Vehicles and Pedestrians with a Multi-Agent Deep Deterministic Policy Gradient",
        "authors": "Weichao Hu, Hongzhang Mu, Yanyan Chen, Yixin Liu, Xiaosong Li",
        "published": "2023-4-3",
        "citations": 0,
        "abstract": "This article focuses on the development of a stable pedestrian crash avoidance mitigation system for autonomous vehicles (AVs). Previous works have only used simple AV–pedestrian models, which do not reflect the actual interaction and risk status of intelligent intersections with manual vehicles. The paper presents a model that simulates the interaction between automatic driving vehicles and pedestrians on unsignalized crosswalks using the multi-agent deep deterministic policy gradient (MADDPG) algorithm. The MADDPG algorithm optimizes the PCAM strategy through the continuous interaction of multiple independent agents and effectively captures the inherent uncertainty in continuous learning and human behavior. Experimental results show that the MADDPG model can fully mitigate collisions in different scenarios and outperforms the DDPG and DRL algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su15076156"
    },
    {
        "id": 33020,
        "title": "Hierarchical path planner combining probabilistic roadmap and deep deterministic policy gradient for unmanned ground vehicles with non-holonomic constraints",
        "authors": "Jie Fan, Xudong Zhang, Kun Zheng, Yuan Zou, Nana Zhou",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2024.106821"
    },
    {
        "id": 33021,
        "title": "Visual interpretation of deep deterministic policy gradient models for energy consumption prediction",
        "authors": "Huixue Wang, Yunzhe Wang, You Lu, Qiming Fu, Jianping Chen",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jobe.2023.107847"
    },
    {
        "id": 33022,
        "title": "An actor-critic algorithm with policy gradients to solve the job shop scheduling problem using deep double recurrent agents",
        "authors": "Marta Monaci, Valerio Agasucci, Giorgio Grani",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejor.2023.07.037"
    },
    {
        "id": 33023,
        "title": "Adaptive Optimization Design of Building Energy System for Smart Elderly Care Community Based on Deep Deterministic Policy Gradient",
        "authors": "Chunmei Liu, Zhe Xue",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "In smart elderly care communities, optimizing the design of building energy systems is crucial for improving the quality of life and health of the elderly. This study pioneers an innovative adaptive optimization design methodology for building energy systems by harnessing the cutting-edge capabilities of deep reinforcement learning. This avant-garde method initially involves modeling a myriad of energy equipment embedded within the energy ecosystem of smart elderly care community buildings, thereby extracting their energy computation formulae. In a groundbreaking progression, this study ingeniously employs the actor–critic (AC) algorithm to refine the deep deterministic policy gradient (DDPG) algorithm. The enhanced DDPG algorithm is then adeptly wielded to perform adaptive optimization of the operational states within the energy system of a smart retirement community building, signifying a trailblazing approach in this realm. Simulation experiments indicate that the proposed method has better stability and convergence compared to traditional deep Q-learning algorithms. When the environmental interaction coefficient and learning ratio is 4, the improved DDPG algorithm under the AC framework can converge after 60 iterations. The stable reward value in the convergence state is −996. When the scheduling cycle of the energy system is between 0:00 and 8:00, the photovoltaic output of the system optimized by the DDPG algorithm is 0. The wind power output fluctuates within 50 kW. This study realizes efficient operation, energy saving, and emission reduction in building energy systems in smart elderly care communities and provides new ideas and methods for research in this field. It also provides an important reference for the design and operation of building energy systems in smart elderly care communities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/pr11072155"
    },
    {
        "id": 33024,
        "title": "Intelligent Array Antenna in Complex Environment Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Tong Wang, Jinshan Deng, Kaiqi Cao, Hongwei Gao, Cheng Jin",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/apcap59480.2023.10469688"
    },
    {
        "id": 33025,
        "title": "Multi-Objective Optimization of Vehicle-Following Control for\n                    Connected Electric Vehicles Based on Deep Deterministic Policy\n                    Gradient",
        "authors": "Yulin Zhang, Yue Wu, Wei He, Yang Gao, Hui Peng, Heng Li",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "<div>Eco-driving plays an increasingly important role in intelligent transportation\n                    systems, where the vehicle-following economy and safety are receiving increasing\n                    attention in recent years. In this context, this article proposes a novel deep\n                    deterministic policy gradient (DDPG)-based driving control strategy for\n                    connected electric vehicles (CEVs) under vehicle-following scenarios. Three\n                    original contributions make this article distinctive from existing studies.\n                    First, a multi-objective optimization problem including driving safety,\n                    passenger comfort, and the driving economy for the following vehicle is\n                    established, in which the battery capacity degradation cost is first considered\n                    in the vehicle-following problem. Second, a DDPG-based driving control strategy\n                    is proposed where a penalty is introduced into the multi-objective optimization\n                    reward function to accelerate the convergence process. Third, the coupling\n                    relationship of the three objectives is carefully studied. Different weighting\n                    factors are tested and analyzed to balance the three objectives. Detailed\n                    discussion and comparison under different driving cycles validate the\n                    superiority of the proposed method, e.g., a 16–31% reduction of battery capacity\n                    degradation cost with better safety and comfort, compared with existing\n                    vehicle-following strategies. This work makes a potential contribution to the\n                    artificial intelligence application of intelligent transportation systems.</div>",
        "keywords": "",
        "link": "http://dx.doi.org/10.4271/14-13-01-0005"
    },
    {
        "id": 33026,
        "title": "Autonomous handover parameter optimisation for 5G cellular networks using deep deterministic policy gradient",
        "authors": "Chiew Foong Kwong, Chenhao Shi, Qianyu Liu, Sen Yang, David Chieng, Pushpendu Kar",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122871"
    },
    {
        "id": 33027,
        "title": "A Method for Task Offloading based on Multi-agent Deep Deterministic Policy Gradient(MADDPG) and Minimum Cost Maximum Flow(MCMF) in Internet of Vehicles(IoVs)",
        "authors": "Zhiping Ouyang, Liang Li, Bo Wang",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640771.3641824"
    },
    {
        "id": 33028,
        "title": "A Novel Approach to Light Detection and Ranging Sensor Placement for\n                    Autonomous Driving Vehicles Using Deep Deterministic Policy Gradient\n                    Algorithm",
        "authors": "Felix Berens, Jordan Ambs, Stefan Elser, Markus Reischl",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "<div>This article presents a novel approach to optimize the placement of light\n                    detection and ranging (LiDAR) sensors in autonomous driving vehicles using\n                    machine learning. As autonomous driving technology advances, LiDAR sensors play\n                    a crucial role in providing accurate collision data for environmental\n                    perception. The proposed method employs the deep deterministic policy gradient\n                    (DDPG) algorithm, which takes the vehicle’s surface geometry as input and\n                    generates optimized 3D sensor positions with predicted high visibility. Through\n                    extensive experiments on various vehicle shapes and a rectangular cuboid, the\n                    effectiveness and adaptability of the proposed method are demonstrated.\n                    Importantly, the trained network can efficiently evaluate new vehicle shapes\n                    without the need for re-optimization, representing a significant improvement\n                    over classical methods such as genetic algorithms. By leveraging machine\n                    learning techniques, this research streamlines the sensor placement optimization\n                    process, enhancing the perception capabilities of autonomous driving vehicles.\n                    The optimized sensor configurations obtained from the DDPG algorithm lead to\n                    safer and more reliable autonomous driving systems, contributing to the\n                    advancement and widespread adoption of autonomous driving technology.</div>",
        "keywords": "",
        "link": "http://dx.doi.org/10.4271/12-07-03-0019"
    },
    {
        "id": 33029,
        "title": "Lithium-Plating Suppressed and Deep Deterministic Policy Gradient-Based Energy Management Strategy",
        "authors": "Dongyang Zhang, Shen Li, Zhongwei Deng, Xiaolin Tang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tte.2023.3288034"
    },
    {
        "id": 33030,
        "title": "A Hybrid Optimization Algorithm for Efficient Virtual Machine Migration and Task Scheduling Using a Cloud-Based Adaptive Multi-Agent Deep Deterministic Policy Gradient Technique",
        "authors": "Et al. Gurpreet Singh Panesar",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "This To achieve optimal system performance in the quickly developing field of cloud computing, efficient resource management—which includes accurate job scheduling and optimized Virtual Machine (VM) migration—is essential. The Adaptive Multi-Agent System with Deep Deterministic Policy Gradient (AMS-DDPG) Algorithm is used in this study to propose a cutting-edge hybrid optimization algorithm for effective virtual machine migration and task scheduling. An sophisticated combination of the War Strategy Optimization (WSO) and Rat Swarm Optimizer (RSO) algorithms, the Iterative Concept of War and Rat Swarm (ICWRS) algorithm is the foundation of this technique. Notably, ICWRS optimizes the system with an amazing 93% accuracy, especially for load balancing, job scheduling, and virtual machine migration. The VM migration and task scheduling flexibility and efficiency are greatly improved by the AMS-DDPG technology, which uses a powerful combination of deterministic policy gradient and deep reinforcement learning. By assuring the best possible resource allocation, the Adaptive Multi-Agent System method enhances decision-making even more. Performance in cloud-based virtualized systems is significantly enhanced by our hybrid method, which combines deep learning and multi-agent coordination. Extensive tests that include a detailed comparison with conventional techniques verify the effectiveness of the suggested strategy. As a consequence, our hybrid optimization approach is successful. The findings show significant improvements in system efficiency, shorter job completion times, and optimum resource utilization. Cloud-based systems have unrealized potential for synergistic optimization, as shown by the integration of ICWRS inside the AMS-DDPG framework. Enabling a high-performing and sustainable cloud computing infrastructure that can adapt to the changing needs of modern computing paradigms is made possible by this strategic resource allocation, which is attained via careful computational utilization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10.8570"
    },
    {
        "id": 33031,
        "title": "Power Allocation Based on Multi-Agent Deep Deterministic Policy Gradient for Underwater Acoustic Communication Networks",
        "authors": "Xuan Geng, Xinyu Hui",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "This paper proposes a reinforcement learning-based power allocation for underwater acoustic communication networks (UACNs). The objective function is formulated as maximizing channel capacity under constraints of maximum power and minimum channel capacity. To solve this problem, a multi-agent deep deterministic policy gradient (MADDPG) approach is introduced, where each transmitter node is considered as an agent. Given the definition of a Markov decision process (MDP) model for this problem, the agents learn to collaboratively maximize the channel capacity by deep deterministic policy gradient (DDPG) learning. Specifically, the power allocation of each agent is obtained by a centralized training and distributed execution (CTDE) method. Simulation results show the sum rate achieved by the proposed algorithm approximates that of the fractional programming (FP) algorithm and improves by at least 5% compared with the DQN (deep Q-learning network) -based power allocation algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13020295"
    },
    {
        "id": 33032,
        "title": "Multi-fidelity optimization of a quiet propeller based on deep deterministic policy gradient and transfer learning",
        "authors": "Xin Geng, Peiqing Liu, Tianxiang Hu, Qiulin Qu, Jiahua Dai, Changhao Lyu, Yunsong Ge, Rinie A.D. Akkermans",
        "published": "2023-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ast.2023.108288"
    },
    {
        "id": 33033,
        "title": "Optimal antisynchronization control for unknown multiagent systems with deep deterministic policy gradient approach",
        "authors": "Cuijuan Zhang, Lianghao Ji, Shasha Yang, Huaqing Li",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2022.12.008"
    },
    {
        "id": 33034,
        "title": "Adversarial Sample Generation using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) and Classification for IEEE 802.11 using the Deep Deterministic Policy Gradient (DDPG)",
        "authors": "D. Sudaroli Vijayakumar, Sannasi Ganapathy",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "One of today's most promising developments is wireless networking, as it enables people across the globe to stay connected. As the wireless networks' transmission medium is open, there are potential issues in safeguarding the privacy of the information. Though several security protocols exist in the literature for the preservation of information, most cases fail with a simple spoof attack. So, intrusion detection systems are vital in wireless networks as they help in the identification of harmful traffic. One of the challenges that exist in wireless intrusion detection systems (WIDS) is finding a balance between accuracy and false alarm rate. The purpose of this study is to provide a practical classification scheme for newer forms of attack. The AWID dataset is used in the experiment, which proposes a feature selection strategy using a combination of Elastic Net and recursive feature elimination. The best feature subset is obtained with 22 features, and a deep deterministic policy gradient learning algorithm is then used to classify attacks based on those features. Samples are generated using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) to evaluate classification outcomes using adversarial samples. The meta-analysis reveals improved results in terms of feature production (22 features), classification accuracy (98.75% for testing samples and 85.24% for adversarial samples), and false alarm rates (0.35%). ",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i8.7946"
    },
    {
        "id": 33035,
        "title": "Deep Deterministic Policy Gradient Reinforcement Learning Based Adaptive PID Load Frequency Control of an AC Micro-Grid",
        "authors": "Kamran Sabahi, Mohsin Jamil, Yaser Shokri-Kalandaragh, Mehdi Tavan, Yogendra Arya",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icjece.2024.3353670"
    },
    {
        "id": 33036,
        "title": "Improved Delayed Detached Eddy Simulation of Co-Flow Jet Flow Control in Extreme Adverse Pressure Gradients",
        "authors": "Brendan McBreen, Yan Ren, Gecheng Zha",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-0063"
    },
    {
        "id": 33037,
        "title": "Decision Model of Ship Intelligent Collision Avoidance Based on Automatic Information System Data and Generic Adversary Imitation Learning-Deep Deterministic Policy Gradient",
        "authors": "Jiao Liu, Guoyou Shi, Kaige Zhu, Jiahui Shi, Yuchuang Wang",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583788.3583790"
    },
    {
        "id": 33038,
        "title": "Modified deep deterministic policy gradient based on active disturbance rejection control for hypersonic vehicles",
        "authors": "Li Xu, Ji Yuehui, Song Yu, Liu Junjie, Gao Qiang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09302-5"
    },
    {
        "id": 33039,
        "title": "Robotic-Arm-Based Force Control by Deep Deterministic Policy Gradient in Neurosurgical Practice",
        "authors": "Ibai Inziarte-Hidalgo, Erik Gorospe, Ekaitz Zulueta, Jose Manuel Lopez-Guede, Unai Fernandez-Gamiz, Saioa Etxebarria",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": "This research continues the previous work “Robotic-Arm-Based Force Control in Neurosurgical Practice”. In that study, authors acquired an optimal control arm speed shape for neurological surgery which minimized a cost function that uses an adaptive scheme to determine the brain tissue force. At the end, the authors proposed the use of reinforcement learning, more specifically Deep Deterministic Policy Gradient (DDPG), to create an agent that could obtain the optimal solution through self-training. In this article, that proposal is carried out by creating an environment, agent (actor and critic), and reward function, that obtain a solution for our problem. However, we have drawn conclusions for potential future enhancements. Additionally, we analyzed the results and identified mistakes that can be improved upon in the future, such as exploring the use of varying desired distances of retraction to enhance training.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11194133"
    },
    {
        "id": 33040,
        "title": "Robotic Arm Trajectory Planning Method Using Deep Deterministic Policy Gradient With Hierarchical Memory Structure",
        "authors": "Di Zhao, Zhenyu Ding, Wenjie Li, Sen Zhao, Yuhong Du",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3340684"
    },
    {
        "id": 33041,
        "title": "Deep deterministic policy gradient and graph attention network for geometry optimization of latticed shells",
        "authors": "Chi-tathon Kupwiwat, Kazuki Hayashi, Makoto Ohsaki",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-04565-w"
    },
    {
        "id": 33042,
        "title": "The effects investigation of data-driven fitting cycle and deep deterministic policy gradient algorithm on energy management strategy of dual-motor electric bus",
        "authors": "Kaixuan Zhang, Jiageng Ruan, Tongyang Li, Hanghang Cui, Changcheng Wu",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.126760"
    },
    {
        "id": 33043,
        "title": "A model predictive control trajectory tracking lateral controller for autonomous vehicles combined with deep deterministic policy gradient",
        "authors": "Zhaokang Xie, Xiaoci Huang, Suyun Luo, Ruoping Zhang, Fang Ma",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": " To solve the problem of trajectory tracking lateral control in autonomous driving technology, a model predictive control (MPC) controller trajectory tracking lateral control method combined with a deep deterministic policy gradient algorithm (DDPG) is proposed in this paper. This method inputs the real-time state of the vehicle into DDPG to achieve real-time automatic optimization of the prediction time domain and control time domain parameters of the MPC controller, and then affects the specific performance of the MPC controller in trajectory tracking lateral control. Specifically, the state space, action space, and reward function of DDPG are defined, and the automatic driving trajectory tracking lateral controller is designed in combination with the vehicle dynamics model. To reduce the exploration space of DDPG and improve the training efficiency of the entire model, the technique of advantage-disadvantage experience separation and extraction is introduced. Finally, the proposed method was trained and verified in various scenarios, and compared with two other lateral control methods for autonomous driving. The results showed that the learning and training time of the trajectory tracking lateral control method based on DDPG-MPC was shorter than that of the DDPG-based method, and the evaluation indicators in the trajectory tracking control process were better than those of the DDPG-based method and original MPC-based method. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/01423312231197854"
    },
    {
        "id": 33044,
        "title": "Online Convex Optimization with Switching Cost and Delayed Gradients",
        "authors": "Spandan Senapati, Rahul Vaze",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "\n            We consider the online convex optimization (OCO) problem with quadratic and linear switching cost when at time t only gradient information for functions f\n            T\n            , T < t is available. For L-smooth and µ-strongly convex objective functions, we propose an algorithm (OMGD) with a competitive ratio of at most 4(L+5)+ 16(L+5)/µ for the quadratic switching cost, and also show the bound to be order-wise tight in terms of L, µ. In addition, we show that the competitive ratio of any online algorithm is at least maxΩ(L),Ω( L/√µ ) when the switching cost is quadratic. For the linear switching cost, the competitive ratio of the OMGD algorithm is shown to depend on both the path length and the squared path length of the problem instance, in addition to L, µ, and is shown to be order-wise, the best competitive ratio any online algorithm can achieve.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3649477.3649490"
    },
    {
        "id": 33045,
        "title": "Enabling coordination in energy communities: A Digital Twin model",
        "authors": "Adela Bâra, Simona-Vasilica Oprea",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enpol.2023.113910"
    },
    {
        "id": 33046,
        "title": "Autonomous Driving of Mobile Robots in Dynamic Environments Based on Deep Deterministic Policy Gradient: Reward Shaping and Hindsight Experience Replay",
        "authors": "Minjae Park, Chaneun Park, Nam Kyu Kwon",
        "published": "2024-1-13",
        "citations": 0,
        "abstract": "In this paper, we propose a reinforcement learning-based end-to-end learning method for the autonomous driving of a mobile robot in a dynamic environment with obstacles. Applying two additional techniques for reinforcement learning simultaneously helps the mobile robot in finding an optimal policy to reach the destination without collisions. First, the multifunctional reward-shaping technique guides the agent toward the goal by utilizing information about the destination and obstacles. Next, employing the hindsight experience replay technique to address the experience imbalance caused by the sparse reward problem assists the agent in finding the optimal policy. We validated the proposed technique in both simulation and real-world environments. To assess the effectiveness of the proposed method, we compared experiments for five different cases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/biomimetics9010051"
    },
    {
        "id": 33047,
        "title": "Deep Deterministic Policy Gradient Virtual Coupling control for the coordination and manoeuvring of heterogeneous uncertain nonlinear High-Speed Trains",
        "authors": "Giacomo Basile, Dario Giuseppe Lui, Alberto Petrillo, Stefania Santini",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108120"
    },
    {
        "id": 33048,
        "title": "Novel deep deterministic policy gradient technique for automated micro-grid energy management in rural and islanded areas",
        "authors": "Lilia Tightiz, L. Minh Dang, Joon Yoo",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aej.2023.09.066"
    },
    {
        "id": 33049,
        "title": "Twin Delayed DRL Approach for Resource Allocation in Multi-User NOMA Systems",
        "authors": "Ayman Rabee, Imad Barhumi",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aict59525.2023.10313195"
    },
    {
        "id": 33050,
        "title": "Time-attenuating Twin Delayed DDPG for Quadrotor Tracking Control",
        "authors": "Boyuan Deng, Jian Sun, Zhuo Li, Gang Wang",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241100"
    },
    {
        "id": 33051,
        "title": "Multilayer Deep Deterministic Policy Gradient for Static Safety and Stability Analysis of Novel Power Systems",
        "authors": "Yun Long, Youfei Lu, Hongwei Zhao, Renbo Wu, Tao Bao, Jun Liu",
        "published": "2023-4-21",
        "citations": 2,
        "abstract": "More and more renewable energy sources are integrated into novel power systems. The randomness and fluctuation of such renewable energy sources bring challenges to the static stability and safety analysis of novel power systems. In this work, a multilayer deep deterministic policy gradient is proposed to address the fluctuation of renewable energy sources. The proposed method is stacked with multilayer deep reinforcement learning methods that can be continuously updated online. The proposed multilayer deep deterministic policy gradient is compared with other deep learning algorithms. The feasibility, effectiveness, and superiority of the proposed method are verified by numerical simulations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/4295384"
    },
    {
        "id": 33052,
        "title": "Perfusion gradients promote delayed perihaematomal oedema in intracerebral haemorrhage",
        "authors": "Enrico Fainardi, Giorgio Busto, Elisa Scola, Ilaria Casetta, Katsuhiro Mizutani, Arturo Consoli, Gregoire Boulouis, Alessandro Padovani, Andrea Morotti",
        "published": "2023-5-2",
        "citations": 0,
        "abstract": "Abstract\nPerihaematomal oedema is a potential therapeutic target to improve outcome of patients with intracerebral haemorrhage, but its pathophysiology remains poorly elucidated. We investigated the longitudinal changes of cerebral perfusion and their influence on perihaematomal oedema development in 150 patients with intracerebral haemorrhage who underwent computed tomography perfusion within 6 h from onset, at 24 h and at 7 days. Perfusion parameters were measured in haemorrhagic core, perihaematomal rim, surrounding normal appearing and contralateral brain tissue. Computed tomography perfusion parameters gradually improved from the core to the periphery in each time interval with an early increase at 24 h followed by a delayed decline at 7 days compared with admission values (P &lt; 0.001). Multivariable linear regression analysis showed that haematoma volume and cerebral blood flow gradient between normal appearing and perihaematomal rim were independently associated with absolute perihaematomal oedema volume in the different time points (within 6 h, B = 0.128, P = 0.032; at 24 h, B = 0.133, P = 0.016; at 7 days, B = 0.218, P &lt; 0.001). In a secondary analysis with relative perihaematomal oedema as the outcome of interest, cerebral blood flow gradient between normal appearing and perihaematomal rim was an independent predictor of perihaematomal oedema only at 7 days (B = 0.239, P = 0.002). Our findings raise the intriguing hypothesis that perfusion gradients promote perihaematomal oedema development in the subacute phase after intracerebral haemorrhage.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/braincomms/fcad133"
    },
    {
        "id": 33053,
        "title": "Deep Reinforcement Learning for On-Demand Intelligent Routing in Deterministic Networks",
        "authors": "Ying Liu, Jianhui Yin, Weiting Zhang, Shanghan Xie",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436769"
    },
    {
        "id": 33054,
        "title": "Retracted: Semigroup of Finite-State Deterministic Intuitionistic Fuzzy Automata with Application in Fault Diagnosis of an Aircraft Twin-Spool Turbofan Engine",
        "authors": "",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9815863"
    },
    {
        "id": 33055,
        "title": "Research on the Deep Deterministic Policy Algorithm Based on the First-Order Inverted Pendulum",
        "authors": "Hailin Hu, Yuhui Chen, Tao Wang, Fu Feng, Weijin Chen",
        "published": "2023-6-27",
        "citations": 1,
        "abstract": "With the mature development of artificial intelligence technology, the application of intelligent control algorithms in control systems has become a trend to meet the high-performance requirements of modern society. This paper proposes a deep deterministic policy gradient (DDPG) controller design method based on deep reinforcement learning to improve system control performance. Firstly, the optimal control policy of the DDPG algorithm is derived from the Markov decision process and the Actor–Critic algorithm. Secondly, in order to avoid local optima in traditional control systems, the capacity and the settlement method of the DDPG experience pool are adjusted to absorb positive experience to accelerate convergence and to complete efficient training. In response, and to solve the overestimation of the Q value in DDPG, the overall structure of the Critic network is changed to shorten the convergence period of DDPG at low learning rates. Finally, a first-order inverted pendulum control system was constructed in a simulation environment to verify the control effectiveness of PID, DDPG, and improved DDPG. The simulation results reveal that the improved DDPG controller has a faster response to disturbances, smaller displacement, and angular displacement of the first-order inverted pendulum. The simulation further proves that the improved DDPG algorithm has better stability and convergence and stronger anti-interference ability and stability recovery. This control method provides a certain reference for the application of reinforcement learning in traditional control systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13137594"
    },
    {
        "id": 33056,
        "title": "Enhanced Deep Deterministic Policy Gradient Algorithm Using Grey Wolf Optimizer for Continuous Control Tasks",
        "authors": "Ebrahim Hamid Hasan Sumiea, Said Jadid Abdulkadir, Mohammed Gamal Ragab, Safwan Mahmood Al-Selwi, Suliamn Mohamed Fati, Alawi AlQushaibi, Hitham Alhussian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3341507"
    },
    {
        "id": 33057,
        "title": "Water management scheme based on prioritized deep deterministic policy gradient for proton exchange membrane fuel cells",
        "authors": "De Xiang, Yijun Cheng, Qingxian Li, Qiong Wang, Liangjiang Liu",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "AbstractTo effectively tackle the intricate and dynamic challenges encountered in proton exchange membrane fuel cells (PEMFCs), this paper introduces a model-free reinforcement learning approach to address its water management issue. Recognizing the limitations of conventional reinforcement learning methods such as Q-learning in handling the continuous actions and nonlinearity inherent in PEMFCs water management, we propose a prioritized deep deterministic policy gradient (DDPG) method. This method, rooted in the Actor-Critic framework, leverages double neural networks and prioritized experience replay to enable adaptive water management and balance. Additionally, we establish a PEMFCs water management platform and implement the prioritized DDPG method using \"Tianshou\", a modularized Python library for deep reinforcement learning. Through experimentation, the effectiveness of our proposed method is verified. This study contributes to advancing the understanding and management of water dynamics in PEMFCs, offering a promising avenue for enhancing their performance and reliability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42452-024-05789-2"
    },
    {
        "id": 33058,
        "title": "A Multi-Policy Deep Reinforcement Learning Approach for Multi-Objective Joint Routing and Scheduling in Deterministic Networks",
        "authors": "Sijin Yang, Lei Zhuang, Jianhui Zhang, Julong Lan, Bingkui Li",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2024.3358403"
    },
    {
        "id": 33059,
        "title": "Satellite Edge Computing With Collaborative Computation Offloading: An Intelligent Deep Deterministic Policy Gradient Approach",
        "authors": "Hangyu Zhang, Rongke Liu, Aryan Kaushik, Xiangqiang Gao",
        "published": "2023-5-15",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2022.3233383"
    },
    {
        "id": 33060,
        "title": "Microbiome convergence and deterministic community assembly along successional biocrust gradients on potash salt heaps",
        "authors": "Juliette A Ohan, Roberto Siani, Julia K Kurth, Veronika Sommer, Karin Glaser, Ulf Karsten, Michael Schloter, Stefanie Schulz",
        "published": "2023-7-21",
        "citations": 0,
        "abstract": "Abstract\nPotash mining, typically performed for agricultural fertilizer production, can create piles of residual salt waste that are ecologically detrimental and difficult to revegetate. Biological soil crusts (biocrusts) have been found growing on and around these heaps, suggesting resilience to the hypersaline environment. We set out to understand the community dynamics of biocrust formation by examining two succesionary salinity gradients at historical mining sites using a high throughput amplicon sequencing. Bare heaps were distinct, with little overlap between sites, and were characterized by high salinity, low nutrient availability, and specialized, low diversity microbial communities, dominated by Halobacteria, Chloroflexia, and Deinococci. ‘Initial’ stages of biocrust development were dominated by site-specific Cyanobacteria, with significant overlap between sites. Established biocrusts were the most diverse, with large proportions of Alphaproteobacteria, Anaerolineae, and Planctomycetacia. Along the salinity gradient at both sites, salinity decreased, pH decreased, and nutrients and Chlorophyll a increased. Microbiomes between sites converged during succession and community assembly process analysis revealed biocrusts at both sites were dominated by deterministic, niche-based processes; indicating a high degree of phylogenetic turnover. We posit early cyanobacterial colonization is essential for biocrust initiation, and facilitates later establishment of plant and other higher-level biota.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/femsec/fiad081"
    },
    {
        "id": 33061,
        "title": "Optimization of reward shaping function based on genetic algorithm applied to a cross validated deep deterministic policy gradient in a powered landing guidance problem",
        "authors": "Larasmoyo Nugroho, Rika Andiarti, Rini Akmeliawati, Ali Türker Kutay, Diva Kartika Larasati, Sastra Kusuma Wijaya",
        "published": "2023-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105798"
    },
    {
        "id": 33062,
        "title": "Policy Gradients for Probabilistic Constrained Reinforcement Learning",
        "authors": "Weiqin Chen, Dharmashankar Subramanian, Santiago Paternain",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089763"
    },
    {
        "id": 33063,
        "title": "Prevalence of Deep Vein Thrombosis in Hip Fracture in Geriatric Patients with Delayed Hospital Admission",
        "authors": "Shubhranshu Choudhary, Raja Babar Bashir Rather, Altaf Ahmad Kawoosa, Jaspreet Singh",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/mr231105173415"
    },
    {
        "id": 33064,
        "title": "Saliency-Driven Hand Gesture Recognition Incorporating Histogram of Oriented Gradients (HOG) and Deep Learning",
        "authors": "Farzaneh Jafari, Anup Basu",
        "published": "2023-9-11",
        "citations": 1,
        "abstract": "Hand gesture recognition is a vital means of communication to convey information between humans and machines. We propose a novel model for hand gesture recognition based on computer vision methods and compare results based on images with complex scenes. While extracting skin color information is an efficient method to determine hand regions, complicated image backgrounds adversely affect recognizing the exact area of the hand shape. Some valuable features like saliency maps, histogram of oriented gradients (HOG), Canny edge detection, and skin color help us maximize the accuracy of hand shape recognition. Considering these features, we proposed an efficient hand posture detection model that improves the test accuracy results to over 99% on the NUS Hand Posture Dataset II and more than 97% on the hand gesture dataset with different challenging backgrounds. In addition, we added noise to around 60% of our datasets. Replicating our experiment, we achieved more than 98% and nearly 97% accuracy on NUS and hand gesture datasets, respectively. Experiments illustrate that the saliency method with HOG has stable performance for a wide range of images with complex backgrounds having varied hand colors and sizes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23187790"
    },
    {
        "id": 33065,
        "title": "EDGE-Net: Efficient Deep-Learning Gradients Extraction Network",
        "authors": "Nasrin Akbari, Amirali Baniasadi",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Deep Convolutional Neural Networks (CNNs) have achieved impressive performance in edge detection tasks, but their large number of parameters often leads to high memory and energy costs for implementation on lightweight devices. In this paper, we propose a new architecture, called Efficient Deep-learning Gradients Extraction Network (EDGE-Net), that integrates the advantages of Depthwise Separable Convolutions and deformable convolutional networks (DeformableConvNet) to address these inefficiencies. By carefully selecting proper components and utilizing network pruning techniques, our proposed EDGE-Net achieves state-of-the-art accuracy in edge detection while significantly reducing complexity. Experimental results on BSDS500 and NYUDv2 datasets demonstrate that EDGE-Net outperforms current lightweight edge detectors with only 500k parameters, without relying on pre-trained weights.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijaia.2023.14207"
    },
    {
        "id": 33066,
        "title": "Single-Family Zoning and Race: Evidence From the Twin Cities",
        "authors": "Salim Furth, MaryJo Webster",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/10511482.2023.2186750"
    },
    {
        "id": 33067,
        "title": "Improving generalization capability of deep learning-based nuclei instance segmentation by non-deterministic train time and deterministic test time stain normalization",
        "authors": "Amirreza Mahbod, Georg Dorffner, Isabella Ellinger, Ramona Woitek, Sepideh Hatamikia",
        "published": "2024-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csbj.2023.12.042"
    },
    {
        "id": 33068,
        "title": "Non-Deterministic Factors Affect Competition Between Thermophilic Autotrophs from Deep-Sea Hydrothermal Vents",
        "authors": "Briana Kubik, James Holden",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "Hydrothermal vents provide windows into the rocky subseafloor on Earth and serve as terrestrial analog sites for extraterrestrial environments. By studying patterns of community assembly in hydrothermal vents and using geochemical models, we can better understand how the deep-sea biosphere contributes to local and global biogeochemical cycling and gather valuable information about how similar communities may arise on Earth and beyond Earth. One prevailing thought is that vent microbial community assembly is driven by deterministic factors such as the thermodynamic favorability of redox reactions. We hypothesized that subsurface microbial communities may also be significantly influenced by other factors, such as differential cell yields, varying optimal growth temperatures, and stochasticity.\nAt Axial Seamount in the Pacific Ocean, H2-consuming methanogens of the genera Methanocaldococcus (Topt 82°C) and Methanothermococcus (Topt 65°C) and H2-consuming sulfur reducers of the genus Desulfurobacterium (Topt 72°C) are the most abundant autotrophs that grow optimally at or above 65°C (Fortunato et al. 2017). At one low-temperature hydrothermal vent site, Marker 113, methanogens are the predominant thermophilic autotrophs while at another site, Marker 33, thermophilic autotrophic sulfur reducers predominate. There is no apparent geochemical or thermodynamic explanation for the differences in community composition. In this study, we performed a series of co-culture competition experiments using Methanocaldococcus jannaschii, Methanothermococcus thermolithotrophicus, and Desulfurobacterium thermolithotrophum HR11 as representative methanogens and sulfur reducers common to hydrothermal vents to explain the variations in community composition between thermophilic autotrophs.\nM. jannaschii increases its cell yield (cells produced per mole of CH4 produced) when grown on very low H2 concentrations as part of a growth rate-growth yield tradeoff (Topçuoğlu et al. 2019). This increase in cell yield could provide methanogens with a competitive growth advantage over H2-consuming sulfur reducers, who otherwise catalyze a more thermodynamically favorable growth reaction. Competition co-culture experiments were conducted between M. jannaschii and D. thermolithotrophum at 72°C and between M. thermolithotrophicus and D. thermolithotrophum at 65°C, both at 1:1 ratios and initial aqueous H2 concentrations of 1.2 mM (high H2) and 85 μM (low H2) to determine the effects of temperature and H2 availability on autotroph competition. For both methanogens, the growth rate, maximum cell concentration, and total CH4 produced decreased when they were grown in co-culture, at low H2, or both relative to monocultures grown with high H2. The methanogen cell yields generally increased in co-culture and at low H2. At both experimental temperatures, the growth rate of D. thermolithotrophum remained unchanged in co-culture and at low H2 relative to monocultures but the maximum cell concentration decreased in co-culture relative to monocultures at both H2 concentrations. However, at low H2, both in mono- and co-culture, there was no detectable H2S produced by the sulfur reducer suggesting a significant shift in growth yield. At both temperatures and H2 concentrations, the sulfur reducer reached higher cell concentrations than the methanogens.\nStochasticity or vent fluid chemistry could lead to early colonization of a vent by methanogens followed by niche exclusion of autotrophic sulfur reducers due to a numerical advantage of the methanogens. Therefore, competitive co-culture experiments were run as before at high H2 with varying initial methanogen:sulfur reducer ratios. At 72°C, D. thermolithotrophum reached the same maximum cell concentration and produced the same amount of H2S in monoculture and co-culture even when the methanogens initially outnumbered the sulfur reducer up to 10,000-fold. M. jannaschii reached a lower maximum cell concentration and produced less CH4 in all co-cultures relative to growth in monoculture. At 65°C, D. thermolithotrophum reached the same maximum cell concentrations and produced the same amount of H2S in monoculture and co-culture when the methanogens initially outnumbered the sulfur reducers up to 100-fold. However, when the methanogens initially outnumbered the sulfur reducers 1,000-fold, M. thermolithotrophicus grew as well as in monoculture and the maximum cell concentration and amount of H2S produced by D. thermolithotrophum was significantly lower than in monoculture and the other co-culture conditions.\nIn conclusion, both methanogens and sulfur reducers shift their redox reactions away from CH4 and H2S production, respectively, and towards biomass production when H2 is limiting. This should be accounted for in thermodynamic predictive models. Furthermore, a combination of growth temperatures lower than the optimum of sulfur reducers and high initial methanogen cell concentrations relative to sulfur reducers can lead to a long-term predominance of methanogens over autotrophic sulfur reducers in vent environments through niche exclusion.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3897/aca.6.e108248"
    },
    {
        "id": 33069,
        "title": "Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model",
        "authors": "Yingkai Sha, Ryan A. Sobash, David John Gagne",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Abstract\nAn ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1–24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overconfident but produces meaningful ensemble spreads that can distinguish good and bad forecasts. The quality of CGAN outputs is also evaluated. Results show that the CGAN outputs behave similarly to a numerical ensemble; they preserved the inter-variable correlations and the contribution of influential predictors as in the original HRRR forecasts. This work provides a novel approach to post-process CAM output using neural networks that can be applied to severe weather prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1175/aies-d-23-0094.1"
    },
    {
        "id": 33070,
        "title": "Correction: Deterministic and Probabilistic Analysis of a Simple Markov Model: How Different Could They Be?",
        "authors": "Howard Thom",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40258-023-00828-2"
    },
    {
        "id": 33071,
        "title": "Twin Delayed DDPG based Dynamic Power Allocation for Mobility in IoRT",
        "authors": "Homayun Kabir, Mau-Luen Tham, Yoong Choon Chang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.24138/jcomss-2022-0141"
    },
    {
        "id": 33072,
        "title": "Deep Deterministic Policy Gradient Algorithm Based Reinforcement Learning Controller for Single-Inductor Multiple-Output DC–DC Converter",
        "authors": "Jian Ye, Huanyu Guo, Benfei Wang, Xinan Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpel.2024.3350181"
    },
    {
        "id": 33073,
        "title": "Optimization of Profile Control and Oil Displacement Scheme Parameters Based on Deep Deterministic Policy Gradient",
        "authors": "Chaodong Tan, Chunqiu Wang, Jinjie Tian, HuiZhao Niu, Qi Wei, Xiongying Zhang",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acsomega.3c02003"
    },
    {
        "id": 33074,
        "title": "Incentive-based demand response under incomplete information based on the deep deterministic policy gradient",
        "authors": "Siyu Ma, Hui Liu, Ni Wang, Lidong Huang, Hui Hwang Goh",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2023.121838"
    },
    {
        "id": 33075,
        "title": "Hierarchical Rewarding Deep Deterministic Policy Gradient Strategy for Energy Management of Hybrid Electric Vehicles",
        "authors": "Jinhai Wang, Changqing Du, Fuwu Yan, Quan Zhou, Hongming Xu",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tte.2023.3263927"
    },
    {
        "id": 33076,
        "title": "Histogram of Oriented Gradients meet deep learning: A novel multi-task deep network for 2D surgical image semantic segmentation",
        "authors": "Binod Bhattarai, Ronast Subedi, Rebati Raman Gaire, Eduard Vazquez, Danail Stoyanov",
        "published": "2023-4",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.media.2023.102747"
    },
    {
        "id": 33077,
        "title": "Interpreting forces as deep learning gradients improves quality of predicted protein structures",
        "authors": "Jonathan Edward King, David Ryan Koes",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bpj.2023.12.011"
    },
    {
        "id": 33078,
        "title": "Advanced deep deterministic policy gradient based energy management strategy design for dual-motor four-wheel-drive electric vehicle",
        "authors": "Hanghang Cui, Jiageng Ruan, Changcheng Wu, Kaixuan Zhang, Tongyang Li",
        "published": "2023-1",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mechmachtheory.2022.105119"
    },
    {
        "id": 33079,
        "title": "Combining Q-learning and Deterministic Policy Gradient for Learning-Based MPC",
        "authors": "Katrine Seel, Sébastien Gros, Jan Tommy Gravdahl",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383562"
    },
    {
        "id": 33080,
        "title": "Learning Complicated Manipulation Skills Via Deterministic Policy with Limited Demonstrations",
        "authors": "Haofeng Liu, Jiayi Tan, Yiwen Chen, Marcelo H Ang",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406318"
    },
    {
        "id": 33081,
        "title": "A Painless Deterministic Policy Gradient Method for Learning-based MPC",
        "authors": "Akhil S Anand, Dirk Reinhardt, Shambhuraj Sawant, Jan Tommy Gravdahl, Sebastien Gros",
        "published": "2023-6-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178119"
    },
    {
        "id": 33082,
        "title": "Deep Learning-Empowered Digital Twin Using Acoustic Signal for Welding Quality Inspection",
        "authors": "Tao Ji, Norzalilah Mohamad Nor",
        "published": "2023-2-28",
        "citations": 7,
        "abstract": "Weld site inspection is a research area of interest in the manufacturing industry. In this study, a digital twin system for welding robots to examine various weld flaws that might happen during welding using the acoustics of the weld site is presented. Additionally, a wavelet filtering technique is implemented to remove the acoustic signal originating from machine noise. Then, an SeCNN-LSTM model is applied to recognize and categorize weld acoustic signals according to the traits of strong acoustic signal time sequences. The model verification accuracy was found to be 91%. In addition, using numerous indicators, the model was compared with seven other models, namely, CNN-SVM, CNN-LSTM, CNN-GRU, BiLSTM, GRU, CNN-BiLSTM, and LSTM. A deep learning model, and acoustic signal filtering and preprocessing techniques are integrated into the proposed digital twin system. The goal of this work was to propose a systematic on-site weld flaw detection approach encompassing data processing, system modeling, and identification methods. In addition, our proposed method could serve as a resource for pertinent research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23052643"
    },
    {
        "id": 33083,
        "title": "Comments to ‘Stability of traveling waves for deterministic and stochastic delayed reaction-diffusion equation based on phase shift’",
        "authors": "Christian H.S. Hamster",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cnsns.2024.107832"
    },
    {
        "id": 33084,
        "title": "Twin Flames in Deep Earth Volatiles: Heterogeneity from Accretion and Differential Stirring of Mantle Reservoirs",
        "authors": "Rita Parai, Samuel Patzkowsky, Kelsey Woody, Xinmu Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7185/gold2023.20719"
    },
    {
        "id": 33085,
        "title": "enDRTS: Deep Reinforcement Learning Based Deterministic Scheduling for Chain Flows in TSN",
        "authors": "Dong Yang, Kai Gong, Weiting Zhang, Kuo Guo, Jia Chen",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nana60121.2023.00047"
    },
    {
        "id": 33086,
        "title": "Housing Affordability Crisis and Delayed Fertility: Evidence from the USA",
        "authors": "Irakli Japaridze, Nagham Sayour",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11113-024-09865-8"
    },
    {
        "id": 33087,
        "title": "Deep Learning with Histogram of Oriented Gradients- based Computer-Aided Diagnosis for Breast Cancer Detection and Classification",
        "authors": "Anitha Ponraj, R.Aroul Canessane",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsmdi57622.2023.00099"
    },
    {
        "id": 33088,
        "title": "A Framework for Digital Twin-Based Deterministic Communication in Satellite Time Sensitive Networks",
        "authors": "Yinzhi Lu, Guofeng Zhao, Chuan Xu, Muhammad Imran, Keping Yu, Joel J.P.C. Rodrigues",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279611"
    },
    {
        "id": 33089,
        "title": "LOW THERMAL GRADIENTS AND DEEP TECTONIC BURIAL IN THE EASTERN GREAT BASIN FROM LRCM DATA",
        "authors": "Naomi Rodgers,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1130/abs/2023am-390490"
    },
    {
        "id": 33090,
        "title": "Research on the Optimal Strategy of Open Data Subjects in the Deterministic Data Security Policy Environment of Urban Governance System",
        "authors": "Anye Liu, Yitong Tan",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1061/9780784485217.090"
    },
    {
        "id": 33091,
        "title": "Implementing blockchain and deep learning in the development of an educational digital twin",
        "authors": "Narendra K. Dewangan, Preeti Chandrakar",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-09501-1"
    },
    {
        "id": 33092,
        "title": "Quadrotor Auto Trimming based on Deep Reinforcement Learning and Digital Twin",
        "authors": "Sara Taheri, Alireza Abdollahi, Amin Rezaeizadeh",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrom60803.2023.10412557"
    },
    {
        "id": 33093,
        "title": "SCMACDnet: multilevel fusion-based deep twin capsule network for change detection",
        "authors": "N. Venugopal",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00530-023-01063-4"
    },
    {
        "id": 33094,
        "title": "Policy gradients using variational quantum circuits",
        "authors": "André Sequeira, Luis Paulo Santos, Luis Soares Barbosa",
        "published": "2023-6",
        "citations": 3,
        "abstract": "AbstractVariational quantum circuits are being used as versatile quantum machine learning models. Some empirical results exhibit an advantage in supervised and generative learning tasks. However, when applied to reinforcement learning, less is known. In this work, we considered a variational quantum circuit composed of a low-depth hardware-efficient ansatz as the parameterized policy of a reinforcement learning agent. We show that an 𝜖-approximation of the policy gradient can be obtained using a logarithmic number of samples concerning the total number of parameters. We empirically verify that such quantum models behave similarly to typical classical neural networks used in standard benchmarking environments and quantum control, using only a fraction of the parameters. Moreover, we study the barren plateau phenomenon in quantum policy gradients using the Fisher information matrix spectrum.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42484-023-00101-8"
    },
    {
        "id": 33095,
        "title": "Adaptive Weighted Combination Approach for Wind Power Forecast Based on Deep Deterministic Policy Gradient Method",
        "authors": "Menglin Li, Ming Yang, Yixiao Yu, Mohammad Shahidehpour, Fushuan Wen",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpwrs.2023.3294839"
    },
    {
        "id": 33096,
        "title": "SFNAS-DDPG: A Biomass-Based Energy Hub Dynamic Scheduling Approach via Connecting Supervised Federated Neural Architecture Search and Deep Deterministic Policy Gradient",
        "authors": "Amirhossein Dolatabadi, Hussein Abdeltawab, Yasser Abdel-Rady I. Mohamed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3352032"
    },
    {
        "id": 33097,
        "title": "UAV's air combat decision-making based on deep deterministic policy gradient and prediction",
        "authors": "Yongfeng LI, Yongxi LYU, Jingping SHI, Weihua LI",
        "published": "2023-2",
        "citations": 0,
        "abstract": "To solve the enemy uncertain manipulation problem during a UAV's autonomous air combat maneuver decision-making, this paper proposes an autonomous air combat maneuver decision-making method that combines target maneuver command prediction with the deep deterministic policy algorithm. The situation data of both sides of air combat are effectively fused and processed, the UAV's six-degree-of-freedom model and maneuver library are built. In air combat, the target generates its corresponding maneuver library instructions through the deep Q network algorithm; at the same time, the UAV on our side gives the target maneuver prediction results through the probabilistic neural network. A deep deterministic policy gradient reinforcement learning method that considers both the situation information of two aircraft and the prediction results of enemy aircraft is proposed, so that the UAV can choose the appropriate maneuver decision according to the current air combat situation. The simulation results show that the method can effectively use the air combat situation information and target maneuver prediction information so that it can improve the effectiveness of the reinforcement learning method for UAV's autonomous air combat decision-making on the premise of ensuring convergence.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/jnwpu/20234110056"
    },
    {
        "id": 33098,
        "title": "Attitude Control of Stabilized Platform Based on Deep Deterministic Policy Gradient with Disturbance Observer",
        "authors": "Aiqing Huo, Xue Jiang, Shuhan Zhang",
        "published": "2023-11-3",
        "citations": 1,
        "abstract": "A rotary steerable drilling system is an advanced drilling technology, with stabilized platform tool face attitude control being a critical component. Due to a multitude of downhole interference factors, coupled with nonlinearities and uncertainties, challenges arise in model establishment and attitude control. Furthermore, considering that stabilized platform tool face attitude determines the drilling direction of the entire drill bit, the effectiveness of tool face attitude control and nonlinear disturbances, such as friction interference, will directly impact the precision and success of drilling tool guidance. In this study, a mathematical model and a friction model of the stabilized platform are established, and a Disturbance-Observer-Based Deep Deterministic Policy Gradient (DDPG_DOB) control algorithm is proposed to address the friction nonlinearity problem existing in the rotary steering drilling stabilized platform. The numerical simulation results illustrate that the stabilized platform attitude control system based on DDPG_DOB can effectively suppress friction interference, improve non-linear hysteresis, and demonstrate strong anti-interference capability and good robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132112022"
    },
    {
        "id": 33099,
        "title": "The twin global crises of climate change and water require the same thing: Accelerated action",
        "authors": "Jeff Camkin, Susana Neto",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/wwp2.12107"
    },
    {
        "id": 33100,
        "title": "Ultrasound Brain Tomography: Comparison of Deep Learning and Deterministic Methods",
        "authors": "Manuchehr Soleimani, Tomasz Rymarczyk, Grzegorz Kłosowski",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tim.2023.3330229"
    },
    {
        "id": 33101,
        "title": "Deep Learning-Based Crowdsourced Image Localization in Digital Twin Models for Enhanced Infrastructure Management",
        "authors": "Jinwu Xiao, Dahyun Oh, Kyubyung Kang",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1061/9780784485231.067"
    },
    {
        "id": 33102,
        "title": "Estimating categorical counterfactuals via deep twin networks",
        "authors": "Athanasios Vlontzos, Bernhard Kainz, Ciarán M. Gilligan-Lee",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s42256-023-00611-x"
    },
    {
        "id": 33103,
        "title": "Fiscal policy and the twin deficits: structural changes matter",
        "authors": "Wongi Kim",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractThis paper empirically and theoretically investigates the relationship between budget balances and external balances, the so-called twin deficit hypothesis. Using the US post-World War II data, I estimate a time-varying structural vector autoregressive model to evaluate the effects of structural breaks on this relationship. The empirical results reveal that the relationship is significantly time varying: (1) an increase in government spending and the consequent budget deficits tend to cause trade deficits in the Bretton Woods era; (2) in contrast, an increase in government spending tends to induce trade surpluses in the post-Bretton Woods era; and (3) with the exceptions of the 1980s and 2010s, government spending shocks cause trade deficits under a floating exchange regime. Using the open economy New Keynesian model with rule-of-thumb consumers, I find that a shift in exchange rate regimes helps in understanding empirical results (1) and (2). Moreover, slowly adjusted taxes inform our comprehension of exceptions in the 1980s, whereas zero lower bound aids our explanation of exceptions in the 2010s.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/s1365100523000147"
    },
    {
        "id": 33104,
        "title": "Millimeter-Wave Sparse Channel Estimator Based-on Twin Support Vector Regression in Deep Multipath Environments",
        "authors": "Anis Charrada",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic_aset58101.2023.10151172"
    }
]