[
    {
        "id": 1505,
        "title": "Enhancing Time Series Classification with Self-Supervised Learning",
        "authors": "Ali Ismail-Fawaz, Maxime Devanne, Jonathan Weber, Germain Forestier",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011611300003393"
    },
    {
        "id": 1506,
        "title": "Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator",
        "authors": "Shaoxiang Dang, Tetsuya Matsumoto, Yoshinori Takeuchi, Hiroaki Kudo",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-85"
    },
    {
        "id": 1507,
        "title": "Speech Recognition for Indigenous Language Using Self-Supervised Learning and Natural Language Processing",
        "authors": "Satoshi Tamura, Tomohiro Hattori, Yusuke Kato, Naoki Noguchi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012396300003654"
    },
    {
        "id": 1508,
        "title": "A study on a semi-supervised learning using self-supervised learning",
        "authors": "Daehak Kim",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7465/jkdi.2023.34.6.967"
    },
    {
        "id": 1509,
        "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
        "authors": "Chutong Meng, Junyi Ao, Tom Ko, Mingxuan Wang, Haizhou Li",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1390"
    },
    {
        "id": 1510,
        "title": "Very High-Resolution Satellite Image Registration Based on Self-supervised Deep Learning",
        "authors": "Taeheon Kim, Jaewon Hur, Youkyung Han",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7848/ksgpc.2023.41.4.217"
    },
    {
        "id": 1511,
        "title": "Combining Self-Supervised and Supervised Learning with Noisy Labels",
        "authors": "Yongqi Zhang, Hui Zhang, Quanming Yao, Jun Wan",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10221957"
    },
    {
        "id": 1512,
        "title": "Seismic data denoising by combining self-supervised and supervised learning",
        "authors": "Yen Sun, Paul Williamson",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1190/image2023-3909418.1"
    },
    {
        "id": 1513,
        "title": "SELF-SUPERVISED LEARNING FOR IMPROVED SAS TARGET RECOGNITION",
        "authors": "BW SHEFFIELD",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25144/15919"
    },
    {
        "id": 1514,
        "title": "Self-supervised learning of deep visual representations",
        "authors": " Mathilde Caron",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48556/sif.1024.21.171"
    },
    {
        "id": 1515,
        "title": "Speech Emotion Recognition Using Transfer Learning and Self-Supervised Speech Representation Learning",
        "authors": "Babak Nasersharif, Marziye Azad",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334799"
    },
    {
        "id": 1516,
        "title": "Leveraging Unsupervised and Self-Supervised Learning for Video Anomaly Detection",
        "authors": "Devashish Lohani, Carlos Crispim-Junior, Quentin Barthélemy, Sarah Bertrand, Lionel Robinault, Laure Rodet",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011663600003417"
    },
    {
        "id": 1517,
        "title": "Extending Self-Distilled Self-Supervised Learning For Semi-Supervised Speaker Verification",
        "authors": "Jeong-Hwan Choi, Jehyun Kyung, Ju-Seok Seong, Ye-Rin Jeoung, Joon-Hyuk Chang",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389802"
    },
    {
        "id": 1518,
        "title": "IPCL: Iterative Pseudo-Supervised Contrastive Learning to Improve Self-Supervised Feature Representation",
        "authors": "Sonal Kumar, Anirudh Phukan, Arijit Sur",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447607"
    },
    {
        "id": 1519,
        "title": "Self-supervised Pre-training and Semi-supervised Learning for Extractive Dialog Summarization",
        "authors": "Yingying Zhuang, Jiecheng Song, Narayanan Sadagopan, Anurag Beniwal",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543873.3587680"
    },
    {
        "id": 1520,
        "title": "Self-Supervised Adversarial Variational Learning",
        "authors": "Fei Ye, Adrian. G. Bors",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.110156"
    },
    {
        "id": 1521,
        "title": "Dual Contrastive Learning for Self-Supervised ECG Mapping to Emotions and Glucose Levels",
        "authors": "Noy Lalzary, Lior Wolf",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sensors56945.2023.10325116"
    },
    {
        "id": 1522,
        "title": "Self-Supervised Skill Learning for Semi-Supervised Long-Horizon Instruction Following",
        "authors": "Benhui Zhuang, Chunhong Zhang, Zheng Hu",
        "published": "2023-3-28",
        "citations": 0,
        "abstract": "Language as an abstraction for hierarchical agents is promising to solve compositional long-time horizon decision-making tasks. The learning of the agent poses significant challenges, as it typically requires plenty of trajectories annotated with languages. This paper addresses the challenge of learning such an agent under the scarcity of language annotations. One approach for leveraging unannotated data is to generate pseudo-labels for unannotated trajectories using sparse seed annotations. However, as the scenes of the environment and tasks assigned to the agent are diverse, the inference of language instructions is sometimes incorrect, causing the policy to learn to ground incorrect instructions to actions. In this work, we propose a self-supervised language-conditioned hierarchical skill policy (SLHSP) which utilizes unannotated data to learn reusable and general task-related skills to facilitate learning from sparse annotations. We demonstrate that the SLHSP that learned with less than 10% of annotated trajectories has a comparable performance to one that learned with 100% of annotated data. Our approach to the challenging ALFRED benchmark leads to a notable improvement in the success rate over a strong baseline also optimized for sparsely annotated data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12071587"
    },
    {
        "id": 1523,
        "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
        "authors": "Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-359"
    },
    {
        "id": 1524,
        "title": "Self-Supervised Temporal Graph Learning based on Multi-Head Self-Attention Weighted Neighborhood Sequences",
        "authors": "Yulong Cao",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bdai59165.2023.10256426"
    },
    {
        "id": 1525,
        "title": "Korean Text to Gloss: Self-Supervised Learning approach",
        "authors": "Thanh-Vu Dang, JinYoung Kim, Gwang-Hyun Yu, Ji Yong Kim, Young Hwan Park, ChilWoo Lee,  ",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "Natural Language Processing (NLP) has grown tremendously in recent years. Typically, bilingual, and multilingual translation models have been deployed widely in machine translation and gained vast attention from the research community. On the contrary, few studies have focused on translating between spoken and sign languages, especially non-English languages. Prior works on Sign Language Translation (SLT) have shown that a mid-level sign gloss representation enhances translation performance. Therefore, this study presents a new large-scale Korean sign language dataset, the Museum-Commentary Korean Sign Gloss (MCKSG) dataset, including 3828 pairs of Korean sentences and their corresponding sign glosses used in Museum-Commentary contexts. In addition, we propose a translation framework based on self-supervised learning, where the pretext task is a text-to-text from a Korean sentence to its back-translation versions, then the pre-trained network will be fine-tuned on the MCKSG dataset. Using self-supervised learning help to overcome the drawback of a shortage of sign language data. Through experimental results, our proposed model outperforms a baseline BERT model by 6.22%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30693/smj.2023.12.1.32"
    },
    {
        "id": 1526,
        "title": "IMPROVING SELF-SUPERVISED LEARNING FOR MULTI-LABEL CLASSIFICATION USING MIX-BASED AUGMENTATIONS",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.35741/issn.0258-2724.58.1.66"
    },
    {
        "id": 1527,
        "title": "A Review on Self-Supervised Learning",
        "authors": "Athul Raj, Srinjoy Dutta",
        "published": "2023-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31871/ijntr.9.1.7"
    },
    {
        "id": 1528,
        "title": "MSTDKD: a framework of using multiple self-supervised methods for semi-supervised learning",
        "authors": "JiaBin Liu, XuanMing Zhang, Jun Hu",
        "published": "2023-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2661030"
    },
    {
        "id": 1529,
        "title": "Self-Distilled Self-supervised Representation Learning",
        "authors": "Jiho Jang, Seonhoon Kim, Kiyoon Yoo, Chaerin Kong, Jangho Kim, Nojun Kwak",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00285"
    },
    {
        "id": 1530,
        "title": "Image-Based Vehicle Classification by Synergizing Features from Supervised and Self-Supervised Learning Paradigms",
        "authors": "Shihan Ma, Jidong J. Yang",
        "published": "2023-2-1",
        "citations": 3,
        "abstract": "This paper introduces a novel approach to leveraging features learned from both supervised and self-supervised paradigms, to improve image classification tasks, specifically for vehicle classification. Two state-of-the-art self-supervised learning methods, DINO and data2vec, were evaluated and compared for their representation learning of vehicle images. The former contrasts local and global views while the latter uses masked prediction on multiple layered representations. In the latter case, supervised learning is employed to finetune a pretrained YOLOR object detector for detecting vehicle wheels, from which definitive wheel positional features are retrieved. The representations learned from these self-supervised learning methods were combined with the wheel positional features for the vehicle classification task. Particularly, a random wheel masking strategy was utilized to finetune the previously learned representations in harmony with the wheel positional features during the training of the classifier. Our experiments show that the data2vec-distilled representations, which are consistent with our wheel masking strategy, outperformed the DINO counterpart, resulting in a celebrated Top-1 classification accuracy of 97.2% for classifying the 13 vehicle classes defined by the Federal Highway Administration.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/eng4010027"
    },
    {
        "id": 1531,
        "title": "Goal-Conditioned Flexible Object Manipulation by Self-Supervised Learning from Play",
        "authors": "Keigo Ishii, Shun Hiramatsu, Yuta Nomura, Shingo Murata",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364471"
    },
    {
        "id": 1532,
        "title": "EMS2L: Enhanced Multi-Task Self-Supervised Learning for 3D Skeleton Representation Learning",
        "authors": "Lilang Lin, Jiaying Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1561/116.00000022"
    },
    {
        "id": 1533,
        "title": "Renewable energy forecasting: A self-supervised learning-based transformer variant",
        "authors": "Jiarui Liu, Yuchen Fu",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.128730"
    },
    {
        "id": 1534,
        "title": "Self-Supervised Adversarial Imitation Learning",
        "authors": "Juarez Monteiro, Nathan Gavenski, Felipe Meneguzzi, Rodrigo C. Barros",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191197"
    },
    {
        "id": 1535,
        "title": "Self-Supervised Learning of Free-Hand Sketches with Bézier Curve Features",
        "authors": "Taner Gülez, Mustafa Sert",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ism59092.2023.00030"
    },
    {
        "id": 1536,
        "title": "Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR",
        "authors": "Shogo Miwa, Atsuhiko Kai",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2463"
    },
    {
        "id": 1537,
        "title": "Semi-supervised learning made simple with self-supervised clustering",
        "authors": "Enrico Fini, Pietro Astolfi, Karteek Alahari, Xavier Alameda-Pineda, Julien Mairal, Moin Nabi, Elisa Ricci",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00311"
    },
    {
        "id": 1538,
        "title": "Self-supervised and supervised deep learning for PET image reconstruction",
        "authors": "Andrew J. Reader",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0203321"
    },
    {
        "id": 1539,
        "title": "Automatic Lung Segmentation in Chest X-Ray Images Using Self-Supervised Learning",
        "authors": "Peilin Li, Siyu Xia",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240997"
    },
    {
        "id": 1540,
        "title": "ViewMix: Augmentation for Robust Representation in Self-Supervised Learning",
        "authors": "Arjon Das, Xin Zhong",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3353133"
    },
    {
        "id": 1541,
        "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations",
        "authors": "Calum Heggan, Tim Hospedales, Sam Budgett, Mehrdad Yaghoobi",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1064"
    },
    {
        "id": 1542,
        "title": "Self-Supervised Learning for Anomalous Sound Detection",
        "authors": "Kevin Wilkinghoff",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447156"
    },
    {
        "id": 1543,
        "title": "Global Self-Supervised Graph Learning for Recommendation",
        "authors": "Xinyue Liu",
        "published": "2024-2-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eebda60612.2024.10486045"
    },
    {
        "id": 1544,
        "title": "CycleCL: Self-supervised Learning for Periodic Videos",
        "authors": "Matteo Destro, Michael Gygli",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00284"
    },
    {
        "id": 1545,
        "title": "Self-Supervised Training for Bearing Fault Diagnosis via Momentum Contrast Learning",
        "authors": "Kai Wang, Chun Liu, Liang Xu",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451088"
    },
    {
        "id": 1546,
        "title": "Evolutionary Augmentation Policy Optimization for Self-Supervised Learning",
        "authors": "Noah Barrett, Zahra Sadeghi, Stan Matwin",
        "published": "2023",
        "citations": 0,
        "abstract": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54364/aaiml.2023.1167"
    },
    {
        "id": 1547,
        "title": "Deep Learning-Based Self-Supervised Transfer Learning for Medical Image Classification",
        "authors": "M. Z. Shaikh, Samender Singh, Neeraj Varshney, Birendra Kumar Saraswat",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdt61202.2024.10488985"
    },
    {
        "id": 1548,
        "title": "SSL2 Self-Supervised Learning meets semi-supervised learning: multiple clerosis segmentation in 7T-MRI from large-scale 3T-MRI",
        "authors": "Jiacheng Wang, Hao Li, Han Liu, Dewei Hu, Daiwei Lu, Keejin Yoon, Kelsey Barter, Francesca Bagnato, Ipek Oguz",
        "published": "2023-4-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2654522"
    },
    {
        "id": 1549,
        "title": "Review on self supervised learning in medical image analysis",
        "authors": "Nitu Kumari, Sonali Agrawal",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cict59886.2023.10455714"
    },
    {
        "id": 1550,
        "title": "Investigating self-supervised learning for Skin Lesion Classification",
        "authors": "Takumi Morita, Xian-Hua Han",
        "published": "2023-7-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10215580"
    },
    {
        "id": 1551,
        "title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition",
        "authors": "Li Fu, Siqi Li, Qingtao Li, Fangzhu Li, Liping Deng, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1609"
    },
    {
        "id": 1552,
        "title": "Comparison between supervised and self-supervised deep learning for SEM image denoising",
        "authors": "Tomoyuki Okuda, Jun Chen, Takahiro Motoyoshi, Ryou Yumiba, Masayoshi Ishikawa, Yasutaka Toyoda",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2660673"
    },
    {
        "id": 1553,
        "title": "Exploring self-supervised learning in Multiview captcha recognition",
        "authors": "Mukhtar Opeyemi Yusuf, Divya Srivastava, Riti Kushwaha",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440750"
    },
    {
        "id": 1554,
        "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
        "authors": "Ziyang Ma, Zhisheng Zheng, Changli Tang, Yujin Wang, Xie Chen",
        "published": "2023-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-822"
    },
    {
        "id": 1555,
        "title": "Adding Distance Information to Self-Supervised Learning for rich Representations",
        "authors": "Yeji Kim, Bai-Sun Kong",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222633"
    },
    {
        "id": 1556,
        "title": "Adopting Self-Supervised Learning into Unsupervised Video Summarization through Restorative Score.",
        "authors": "Mehryar Abbasi, Parvaneh Saeedi",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222350"
    },
    {
        "id": 1557,
        "title": "Biased Self-supervised Learning for ASR",
        "authors": "Florian L. Kreyssig, Yangyang Shi, Jinxi Guo, Leda Sari, Abdel-rahman Mohamed, Philip C. Woodland",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2499"
    },
    {
        "id": 1558,
        "title": "Phase retrieval network for multi-functional holography based on self-supervised learning",
        "authors": "Jialei Xie, Lei Jin",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3023658"
    },
    {
        "id": 1559,
        "title": "Road Condition Anomaly Detection using Self-Supervised Learning from Audio",
        "authors": "U-Ju Gim",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421899"
    },
    {
        "id": 1560,
        "title": "Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features",
        "authors": "Zheng Gao, Chen Feng, Ioannis Patras",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00179"
    },
    {
        "id": 1561,
        "title": "Self-supervised Learning with Temporary Exact Solutions: Linear Projection",
        "authors": "Evrim Ozmermer, Qiang Li",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indin51400.2023.10217918"
    },
    {
        "id": 1562,
        "title": "Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection",
        "authors": "Yuankun Xie, Haonan Cheng, Yutian Wang, Long Ye",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1383"
    },
    {
        "id": 1563,
        "title": "Class-Agnostic Self-Supervised Learning for Image Angle Classification",
        "authors": "Hyeonseok Kim, Yeejin Lee",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10317040"
    },
    {
        "id": 1564,
        "title": "Boosting Ultrasonic Image Classification via Self-Supervised Representation Learning",
        "authors": "Yajie Hou, Qingbing Sang",
        "published": "2023-3-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccr56747.2023.10194197"
    },
    {
        "id": 1565,
        "title": "ASBERT: ASR-Specific Self-Supervised Learning with Self-Training",
        "authors": "Hyung Yong Kim, Byeong-Yeol Kim, Seung Woo Yoo, Youshin Lim, Yunkyu Lim, Hanbin Lee",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10023214"
    },
    {
        "id": 1566,
        "title": "Augmentation Strategies for Self-Supervised Representation Learning from Electrocardiograms",
        "authors": "Matilda Andersson, Mattias Nilsson, Gabrielle Flood, Kalle Åström",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289960"
    },
    {
        "id": 1567,
        "title": "Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning",
        "authors": "Srijan Das, Michael Ryoo",
        "published": "2023-7-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10216260"
    },
    {
        "id": 1568,
        "title": "Language-Aware Multilingual Machine Translation with Self-Supervised Learning",
        "authors": "Haoran Xu, Jean Maillard, Vedanuj Goswami",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.38"
    },
    {
        "id": 1569,
        "title": "Graph Neural Collaborative Filtering Algorithm Based on Self-Supervised Learning and Degree Centrality",
        "authors": "Hao Wang, Chunlong Yao",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3613330.3613341"
    },
    {
        "id": 1570,
        "title": "Multi-View Self-Supervised Learning For Multivariate Variable-Channel Time Series",
        "authors": "Thea Brüsch, Mikkel N. Schmidt, Tommy S. Alstrøm",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mlsp55844.2023.10285993"
    },
    {
        "id": 1571,
        "title": "Multimodal self-supervised learning for semantic analysis of PolSAR imagery",
        "authors": "Yanxin Dong, Ronny Hänsch",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10283301"
    },
    {
        "id": 1572,
        "title": "Robust Hypergraph-Augmented Graph Contrastive Learning for Graph Self-Supervised Learning",
        "authors": "Zeming Wang, Xiaoyang Li, Rui Wang, Changwen Zheng",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3590003.3590053"
    },
    {
        "id": 1573,
        "title": "Self-Supervised and Few-Shot Contrastive Learning Frameworks for Text Clustering",
        "authors": "Haoxiang Shi, Tetsuya Sakai",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3302913"
    },
    {
        "id": 1574,
        "title": "Applying Self-Supervised Learning to Image Quality Assessment in Chest CT Imaging",
        "authors": "Eléonore Pouget, Véronique Dedieu",
        "published": "2024-3-29",
        "citations": 0,
        "abstract": "Many new reconstruction techniques have been deployed to allow low-dose CT examinations. Such reconstruction techniques exhibit nonlinear properties, which strengthen the need for a task-based measure of image quality. The Hotelling observer (HO) is the optimal linear observer and provides a lower bound of the Bayesian ideal observer detection performance. However, its computational complexity impedes its widespread practical usage. To address this issue, we proposed a self-supervised learning (SSL)-based model observer to provide accurate estimates of HO performance in very low-dose chest CT images. Our approach involved a two-stage model combining a convolutional denoising auto-encoder (CDAE) for feature extraction and dimensionality reduction and a support vector machine for classification. To evaluate this approach, we conducted signal detection tasks employing chest CT images with different noise structures generated by computer-based simulations. We compared this approach with two supervised learning-based methods: a single-layer neural network (SLNN) and a convolutional neural network (CNN). The results showed that the CDAE-based model was able to achieve similar detection performance to the HO. In addition, it outperformed both SLNN and CNN when a reduced number of training images was considered. The proposed approach holds promise for optimizing low-dose CT protocols across scanner platforms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bioengineering11040335"
    },
    {
        "id": 1575,
        "title": "Self-supervised boundary offline reinforcement learning",
        "authors": "Jiahao Shen",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3026355"
    },
    {
        "id": 1576,
        "title": "Representation Uncertainty in Self-Supervised Learning as Variational Inference",
        "authors": "Hiroki Nakamura, Masashi Okada, Tadahiro Taniguchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01511"
    },
    {
        "id": 1577,
        "title": "Feature Decoupling in Self-supervised Representation Learning for Open Set Recognition",
        "authors": "Jingyun Jia, Philip K. Chan",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191920"
    },
    {
        "id": 1578,
        "title": "Utilizing Self-Supervised Learning Features and Adapter Fine-Tuning for Enhancing Speech Emotion Recognition",
        "authors": "Tangxun Li, Junjie Hou",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mlbdbi60823.2023.10482145"
    },
    {
        "id": 1579,
        "title": "Mixup Feature: A Pretext Task Self-Supervised Learning Method for Enhanced Visual Feature Learning",
        "authors": "Jiashu Xu, Sergii Stirenko",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3301561"
    },
    {
        "id": 1580,
        "title": "Self-Supervised Learning of Depth Maps for Autonomous Cars",
        "authors": "Andrei-Sebastian Petrescu, Constantin-Cristian Damian, Daniela Coltuc",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10290114"
    },
    {
        "id": 1581,
        "title": "Self-Supervised Learning for Scanned Halftone Classification with Novel Augmentation Techniques",
        "authors": "Jing-Ming Guo, Sankarasrinivasan Seshathiri",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222434"
    },
    {
        "id": 1582,
        "title": "Self-supervised learning in different domains",
        "authors": "A. Kamaleeva, I. Bikmullina",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0175305"
    },
    {
        "id": 1583,
        "title": "Arabic Speech Recognition based on Self Supervised Learning",
        "authors": "Hiba Adreese Younis, Yusra Faisal Mohammad",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dese60595.2023.10469031"
    },
    {
        "id": 1584,
        "title": "Meta-learning-based recommendation method for self-supervised hybrid comparison learning",
        "authors": "Huixin Jiang, Lingyu Yan, Donghua Liu, Xiang Wan",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3013361"
    },
    {
        "id": 1585,
        "title": "TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning",
        "authors": "Jiexi Liu, Songcan Chen",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Learning universal time series representations applicable to various types of downstream tasks is challenging but valuable in real applications. Recently, researchers have attempted to leverage the success of self-supervised contrastive learning (SSCL) in Computer Vision(CV) and Natural Language Processing(NLP) to tackle time series representation. Nevertheless, due to the special temporal characteristics, relying solely on empirical guidance from other domains may be ineffective for time series and difficult to adapt to multiple downstream tasks. To this end, we review three parts involved in SSCL including 1) designing augmentation methods for positive pairs, 2) constructing (hard) negative pairs, and 3) designing SSCL loss. For 1) and 2), we find that unsuitable positive and negative pair construction may introduce inappropriate inductive biases, which neither preserve temporal properties nor provide sufficient discriminative features. For 3), just exploring segment- or instance-level semantics information is not enough for learning universal representation. To remedy the above issues, we propose a novel self-supervised framework named TimesURL. Specifically, we first introduce a frequency-temporal-based augmentation to keep the temporal property unchanged. And then, we construct double Universums as a special kind of hard negative to guide better contrastive learning. Additionally, we introduce time reconstruction as a joint optimization objective with contrastive learning to capture both segment-level and instance-level information. As a result, TimesURL can learn high-quality universal representations and achieve state-of-the-art performance in 6 different downstream tasks, including short- and long-term forecasting, imputation, classification, anomaly detection and transfer learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i12.29299"
    },
    {
        "id": 1586,
        "title": "HNSSL: Hard Negative-Based Self-Supervised Learning",
        "authors": "Wentao Zhu, Jingya Liu, Yufang Huang",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00506"
    },
    {
        "id": 1587,
        "title": "A Hierarchical Vision Transformer Using Overlapping Patch and Self-Supervised Learning",
        "authors": "Yaxin Ma, Ming Li, Jun Chang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191916"
    },
    {
        "id": 1588,
        "title": "ViewCLR: Learning Self-supervised Video Representation for Unseen Viewpoints",
        "authors": "Srijan Das, Michael S. Ryoo",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00553"
    },
    {
        "id": 1589,
        "title": "Detecting Fake Audio of Arabic Speakers Using Self-Supervised Deep Learning",
        "authors": "Zaynab M. Almutairi, Hebah Elgibreen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3286864"
    },
    {
        "id": 1590,
        "title": "Comparative Analysis of Self-Supervised and Supervised Deep Learning Models for Ocular Disease Recognition",
        "authors": "Vijayalakshmi S, Kavitha K R, Mugilan R S, Naveen Sivaa S",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itechsecom59882.2023.10435254"
    },
    {
        "id": 1591,
        "title": "Learning by Sorting: Self-supervised Learning with Group Ordering Constraints",
        "authors": "Nina Shvetsova, Felix Petersen, Anna Kukleva, Bernt Schiele, Hilde Kuehne",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01508"
    },
    {
        "id": 1592,
        "title": "Topology-Aware Debiased Self-Supervised Graph Learning for Recommendation",
        "authors": "Lei Han, Hui Yan, Zhicheng Qiao",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394615"
    },
    {
        "id": 1593,
        "title": "Self-Supervised Contrastive Learning for Radar-Based Human Activity Recognition",
        "authors": "Mohammad Mahbubur Rahman, Sevgi Zubeyde Gurbuz",
        "published": "2023-5-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149770"
    },
    {
        "id": 1594,
        "title": "Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
        "authors": "Sandipana Dowerah, Ajinkya Kulkarni, Romain Serizel, Denis Jouvet",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1890"
    },
    {
        "id": 1595,
        "title": "Structure-aware protein self-supervised learning",
        "authors": "Can (Sam) Chen, Jingbo Zhou, Fan Wang, Xue Liu, Dejing Dou",
        "published": "2023-4-3",
        "citations": 9,
        "abstract": "Abstract\n\nMotivation\nProtein representation learning methods have shown great potential to many downstream tasks in biological applications. A few recent studies have demonstrated that the self-supervised learning is a promising solution to addressing insufficient labels of proteins, which is a major obstacle to effective protein representation learning. However, existing protein representation learning is usually pretrained on protein sequences without considering the important protein structural information.\n\n\nResults\nIn this work, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a graph neural network model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specifically, we identify the relation between the sequential information in the protein language model and the structural information in the specially designed graph neural network model via a novel pseudo bi-level optimization scheme. We conduct experiments on three downstream tasks: the binary classification into membrane/non-membrane proteins, the location classification into 10 cellular compartments, and the enzyme-catalyzed reaction classification into 384 EC numbers, and these experiments verify the effectiveness of our proposed method.\n\n\nAvailability and implementation\nThe Alphafold2 database is available in https://alphafold.ebi.ac.uk/. The PDB files are available in https://www.rcsb.org/. The downstream tasks are available in https://github.com/phermosilla/IEConv\\_proteins/tree/master/Datasets. The code of the proposed method is available in https://github.com/GGchen1997/STEPS_Bioinformatics.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btad189"
    },
    {
        "id": 1596,
        "title": "Retracted: Magnetic Tile Surface Defect Detection Methodology Based on Self-Attention and Self-Supervised Learning",
        "authors": "",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9890718"
    },
    {
        "id": 1597,
        "title": "Graph Convolution Recommendation Algorithm Combined with Self-Supervised Learning",
        "authors": "Liu Guihong, Wang Ziyi",
        "published": "2023-7-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpics58376.2023.10235695"
    },
    {
        "id": 1598,
        "title": "Multi-Modal Self-Supervised Learning for Recommendation",
        "authors": "Wei Wei, Chao Huang, Lianghao Xia, Chuxu Zhang",
        "published": "2023-4-30",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543507.3583206"
    },
    {
        "id": 1599,
        "title": "Visual Kinematics Representation via Self-Supervised Object Tracking for Deep Reinforcement Learning",
        "authors": "Yixing Lan, Xin Xu, Qiang Fang, Yujun Zeng",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10452094"
    },
    {
        "id": 1600,
        "title": "Improved contrastive learning model via identification of false‐negatives in self‐supervised learning",
        "authors": "Joonsun Auh, Changsik Cho, Seon‐tae Kim",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "AbstractSelf‐supervised learning is a method that learns the data representation through unlabeled data. It is efficient because it learns from large‐scale unlabeled data and through continuous research, performance comparable to supervised learning has been reached. Contrastive learning, a type of self‐supervised learning algorithm, utilizes data similarity to perform instance‐level learning within an embedding space. However, it suffers from the problem of false‐negatives, which are the misclassification of data class during training the data representation. They result in loss of information and deteriorate the performance of the model. This study employed cosine similarity and temperature simultaneously to identify false‐negatives and mitigate their impact to improve the performance of the contrastive learning model. The proposed method exhibited a performance improvement of up to 2.7% compared with the existing algorithm on the CIFAR‐100 dataset. Improved performance on other datasets such as CIFAR‐10 and ImageNet was also observed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4218/etrij.2023-0285"
    },
    {
        "id": 1601,
        "title": "Application of self-supervised learning in natural language processing",
        "authors": "Ye Zhang",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "Self-supervised learning uses the label-free data learning model and has a significant impact on the NLP task. It reduces data annotation costs and improves performance. The main applications include pre-training models such as BERT and GPT, contrast learning, and pseudo-supervised and semi-supervised methods. It has been successfully applied in text classification, emotion analysis and other fields. Future research directions include mixed unsupervised learning, cross-modal learning and improving interpretability of models while focusing on ethical social issues.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/urpv6i8g3j"
    },
    {
        "id": 1602,
        "title": "Self-Supervised Contrastive Learning In Spiking Neural Networks",
        "authors": "Yeganeh Bahariasl, Saeed Reza Kheradpisheh",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mvip62238.2024.10491173"
    },
    {
        "id": 1603,
        "title": "Self-Supervised Learning Recommendation Algorithm Based on Meta-Paths",
        "authors": "Youyuan She, Caixiao Ouyang",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aict61584.2023.10452672"
    },
    {
        "id": 1604,
        "title": "Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching",
        "authors": "Dongliang Cao, Florian Bernard",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01701"
    },
    {
        "id": 1605,
        "title": "Discriminative Spatiotemporal Alignment for Self-Supervised Video Correspondence Learning",
        "authors": "Qiaoqiao Wei, Hui Zhang, Jun-Hai Yong",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00316"
    },
    {
        "id": 1606,
        "title": "Generative Adversarial Networks for Self-Supervised Transfer Learning in Medical Image Classification",
        "authors": "Mahesh T R, Meena Krishna",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425480"
    },
    {
        "id": 1607,
        "title": "Self-Supervised Learning Based Target-Aware Session Recommendation Algorithm",
        "authors": "Jianlong Feng, Caixiao Ouyang",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aict61584.2023.10452698"
    },
    {
        "id": 1608,
        "title": "Self-Supervised Learning for Electroencephalography",
        "authors": "Mohammad H. Rafiei, Lynne V. Gauthier, Hojjat Adeli, Daniel Takabi",
        "published": "2024-2",
        "citations": 50,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3190448"
    },
    {
        "id": 1609,
        "title": "Self-Supervised Learning for Point Clouds through Multi-crop Mutual Prediction",
        "authors": "Changyu Zeng, Wei Wang, Jimin Xiao, Anh Nguyen, Yutao Yue",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/prml59573.2023.10348385"
    },
    {
        "id": 1610,
        "title": "Self-supervised Learning on Graphs at Node and Graph Level",
        "authors": "Yilin Ding, Zhen Liu, Hao Hao",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3616901.3616926"
    },
    {
        "id": 1611,
        "title": "Self-Supervised Embodied Learning for Semantic Segmentation",
        "authors": "Juan Wang, Xinzhu Liu, Dawei Zhao, Bin Dai, Huaping Liu",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364408"
    },
    {
        "id": 1612,
        "title": "Maximizing model generalization for machine condition monitoring with Self-Supervised Learning and Federated Learning",
        "authors": "Matthew Russell, Peng Wang",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jmsy.2023.09.008"
    },
    {
        "id": 1613,
        "title": "SIGNAL DISCRIMINATION IN CZTS GAMMA DETECTORS USING SELF-SUPERVISED DEEP LEARNING MODELS",
        "authors": "Steven Neher",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/1989826"
    },
    {
        "id": 1614,
        "title": "Collaborative Self-Supervised Transductive Few-Shot Learning for Remote Sensing Scene Classification",
        "authors": "Haiyan Han, Yangchao Huang, Zhe Wang",
        "published": "2023-9-11",
        "citations": 2,
        "abstract": "With the advent of deep learning and the accessibility of massive data, scene classification algorithms based on deep learning have been extensively researched and have achieved exciting developments. However, the success of deep models often relies on a large amount of annotated remote sensing data. Additionally, deep models are typically trained and tested on the same set of classes, leading to compromised generalization performance when encountering new classes. This is where few-shot learning aims to enable models to quickly generalize to new classes with only a few reference samples. In this paper, we propose a novel collaborative self-supervised transductive few-shot learning (CS2TFSL) algorithm for remote sensing scene classification. In our approach, we construct two distinct self-supervised auxiliary tasks to jointly train the feature extractor, aiming to obtain a powerful representation. Subsequently, the feature extractor’s parameters are frozen, requiring no further training, and transferred to the inference stage. During testing, we employ transductive inference to enhance the associative information between the support and query sets by leveraging additional sample information in the data. Extensive comparisons with state-of-the-art few-shot scene classification algorithms on the WHU-RS19 and NWPU-RESISC45 datasets demonstrate the effectiveness of the proposed CS2TFSL. More specifically, CS2TFSL ranks first in the settings of five-way one-shot and five-way five-shot. Additionally, detailed ablation experiments are conducted to analyze the CS2TFSL. The experimental results reveal significant and promising performance improvements in few-shot scene classification through the combination of self-supervised learning and direct transductive inference.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12183846"
    },
    {
        "id": 1615,
        "title": "Research on Domain Adaptive Methods Based on Self-Supervised Learning",
        "authors": "Tianyi Jia, Yunna Lu",
        "published": "2023-7-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpics58376.2023.10235647"
    },
    {
        "id": 1616,
        "title": "Self-Supervised Learning for InSAR Phase and Coherence Estimation",
        "authors": "Francescopaolo Sica, Pavan Muguda Sanjeevamurthy, Michael Schmitt",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281641"
    },
    {
        "id": 1617,
        "title": "Image classification model based on self-supervised deep learning",
        "authors": "Jiankun Xiao",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3004961"
    },
    {
        "id": 1618,
        "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
        "authors": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe",
        "published": "2023-8-20",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1176"
    },
    {
        "id": 1619,
        "title": "Semiconductor Fab Scheduling With Self-Supervised And Reinforcement Learning",
        "authors": "Pierre Tassel, Benjamin Kovács, Martin Gebser, Konstantin Schekotihin, Patrick Stöckermann, Georg Seidel",
        "published": "2023-12-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10407747"
    },
    {
        "id": 1620,
        "title": "Self-Driving Car Using Supervised Learning",
        "authors": "Ryan Collins, Himanshu Kumar Maurya, Raj S.R. Ragul",
        "published": "2023-2-27",
        "citations": 0,
        "abstract": "In the growing field of autonomous driving, minimizing human errors lead to fatal accidents. Many companies and research groups have been working for years to achieve a fully autonomous vehicle. Self-driving cars are inevitably the future. The system to be implemented is to train and directly translate images from three cameras to steering commands. The proposed method is expected to work on local roads with or without lane markings. With just the images generated and the human steering angle as the training signal, the proposed method is expected to automatically recognize important road characteristics. A simulator (Udacity) is used to generate data for the proposed model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4028/p-5vztxi"
    },
    {
        "id": 1621,
        "title": "Continual Self-Supervised Learning in Earth Observation with Embedding Regularization",
        "authors": "Hamna Moieez, Valerio Marsocci, Simone Scardapane",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10283121"
    },
    {
        "id": 1622,
        "title": "Self-Supervised Learning Using Noisy-Latent Augmentation",
        "authors": "Rasa Khosrowshahli, Shahryar Rahnamayan, Azam Asilian Bidgoli, Masoud Makrehchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394370"
    },
    {
        "id": 1623,
        "title": "Grouped Contrastive Learning of Self-Supervised Sentence Representation",
        "authors": "Qian Wang, Weiqi Zhang, Tianyi Lei, Dezhong Peng",
        "published": "2023-8-31",
        "citations": 1,
        "abstract": "This paper proposes a method called Grouped Contrastive Learning of self-supervised Sentence Representation (GCLSR), which can learn an effective and meaningful representation of sentences. Previous works maximize the similarity between two vectors to be the objective of contrastive learning, suffering from the high-dimensionality of the vectors. In addition, most previous works have adopted discrete data augmentation to obtain positive samples and have directly employed a contrastive framework from computer vision to perform contrastive training, which could hamper contrastive training because text data are discrete and sparse compared with image data. To solve these issues, we design a novel framework of contrastive learning, i.e., GCLSR, which divides the high-dimensional feature vector into several groups and respectively computes the groups’ contrastive losses to make use of more local information, eventually obtaining a more fine-grained sentence representation. In addition, in GCLSR, we design a new self-attention mechanism and both a continuous and a partial-word vector augmentation (PWVA). For the discrete and sparse text data, the use of self-attention could help the model focus on the informative words by measuring the importance of every word in a sentence. By using the PWVA, GCLSR can obtain high-quality positive samples used for contrastive learning. Experimental results demonstrate that our proposed GCLSR achieves an encouraging result on the challenging datasets of the semantic textual similarity (STS) task and transfer task.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13179873"
    },
    {
        "id": 1624,
        "title": "SAD: self-supervised avionic diagnostics",
        "authors": "Attiano Purpura-Pontoniere, Maksim Bobrov, Tarun Bhattacharya",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2664732"
    },
    {
        "id": 1625,
        "title": "Federated Learning-assisted Self-supervised CNN for Monkeypox Diagnosis",
        "authors": "Nusrat Jahan, Garima Bajwa, Thangarajah Akilan",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wnyispw60588.2023.10349384"
    },
    {
        "id": 1626,
        "title": "Self-supervised visual representation learning on food images",
        "authors": "Andrew W. Peng, Jiangpeng He, Fengqing Zhu",
        "published": "2023-1-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2352/ei.2023.35.7.image-269"
    },
    {
        "id": 1627,
        "title": "Back to the present: self-supervised learning across cortical layers",
        "authors": "Kevin Kermani Nejad, Loreen Hertäg, Paul Anastasiades, Rui Ponte Costa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1711-0"
    },
    {
        "id": 1628,
        "title": "Multi-Task Self-Supervised Learning Based Tibetan-Chinese Speech-to-Speech Translation",
        "authors": "Rouhe Liu, Yue Zhao, Xiaona Xu",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ialp61005.2023.10337040"
    },
    {
        "id": 1629,
        "title": "Pareto Graph Self-Supervised Learning",
        "authors": "Zhengyu Chen, Teng Xiao, Donglin Wang, Min Zhang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447557"
    },
    {
        "id": 1630,
        "title": "Self supervised learning based emotion recognition using physiological signals",
        "authors": "Min Zhang, YanLi Cui",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "IntroductionThe significant role of emotional recognition in the field of human-machine interaction has garnered the attention of many researchers. Emotion recognition based on physiological signals can objectively reflect the most authentic emotional states of humans. However, existing labeled Electroencephalogram (EEG) datasets are often of small scale.MethodsIn practical scenarios, a large number of unlabeled EEG signals are easier to obtain. Therefore, this paper adopts self-supervised learning methods to study emotion recognition based on EEG. Specifically, experiments employ three pre-defined tasks to define pseudo-labels and extract features from the inherent structure of the data.Results and discussionExperimental results indicate that self-supervised learning methods have the capability to learn effective feature representations for downstream tasks without any manual labels.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnhum.2024.1334721"
    },
    {
        "id": 1631,
        "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
        "authors": "Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe",
        "published": "2023-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2051"
    },
    {
        "id": 1632,
        "title": "Self-Supervised Deep Learning—The Next Frontier",
        "authors": "T. Y. Alvin Liu, Neslihan Dilruba Koseoglu, Craig Jones",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1001/jamaophthalmol.2023.6650"
    },
    {
        "id": 1633,
        "title": "Efficient Ensemble via Rotation-Based Self- Supervised Learning Technique and Multi-Input Multi-Output Network",
        "authors": "Jaehoon Park",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3373692"
    },
    {
        "id": 1634,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 1635,
        "title": "Identification of hickory nuts with different oxidation levels by integrating self-supervised and supervised learning",
        "authors": "Haoyu Kang, Dan Dai, Jian Zheng, Zile Liang, Siwei Chen, Lizhong Ding",
        "published": "2023-3-8",
        "citations": 0,
        "abstract": "The hickory (Carya cathayensis) nuts are considered as a traditional nut in Asia due to nutritional components such as phenols and steroids, amino acids and minerals, and especially high levels of unsaturated fatty acids. However, the edible quality of hickory nuts is rapidly deteriorated by oxidative rancidity. Deeper Masked autoencoders (DEEPMAE) with a unique structure for automatically extracting some features that could be scaleable from local to global for image classification, has been considered to be a state-of-the-art computer vision technique for grading tasks. This paper aims to present a novel and accurate method for grading hickory nuts with different oxidation levels. Owing to the use of self-supervised and supervised processes, this method is able to predict images of hickory nuts with different oxidation levels effectively, i.e., DEEPMAE can predict the oxidation level of nuts. The proposed DEEPMAE model was constructed from Vision Transformer (VIT) architecture which was followed by Masked autoencoders(MAE). This model was trained and tested on image datasets containing four classes, and the differences between these classes were mainly caused by varying levels of oxidation over time. The DEEPMAE model was able to achieve an overall classification accuracy of 96.14% on the validation set and 96.42% on the test set. The results on the suggested model demonstrated that the application of the DEEPMAE model might be a promising method for grading hickory nuts with different levels of oxidation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fsufs.2023.1144998"
    },
    {
        "id": 1636,
        "title": "Enhancing breast cancer classification via histopathological image analysis: Leveraging self-supervised contrastive learning and transfer learning",
        "authors": "Faisal Bin Ashraf, S.M. Maksudul Alam, Shahriar M. Sakib",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e24094"
    },
    {
        "id": 1637,
        "title": "Self-Supervised Monocular Depth Estimation With Isometric-Self-Sample-Based Learning",
        "authors": "Geonho Cha, Ho-Deok Jang, Dongyoon Wee",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2022.3221871"
    },
    {
        "id": 1638,
        "title": "FireRisk: A Remote Sensing Dataset for Fire Risk Assessment with Benchmarks Using Supervised and Self-supervised Learning",
        "authors": "Shuchang Shen, Sachith Seneviratne, Xinye Wanyan, Michael Kirley",
        "published": "2023-11-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dicta60407.2023.00034"
    },
    {
        "id": 1639,
        "title": "Coupling Self-Supervised and Supervised Contrastive Learning for Multiple Classification of Cervical Cytological Whole Slide Images",
        "authors": "Lang Wang, Peng Jiang, Wensi Duan, Dehua Cao, Baochuan Pang, Juan Liu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446708"
    },
    {
        "id": 1640,
        "title": "Self-mentoring: A new deep learning pipeline to train a self-supervised U-net for few-shot learning of bio-artificial capsule segmentation",
        "authors": "Arnaud Deleruyelle, Cristian Versari, John Klein",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2022.106454"
    },
    {
        "id": 1641,
        "title": "Accelerating Self-Supervised Learning via Efficient Training Strategies",
        "authors": "Mustafa Taha Kocyigit, Timothy M. Hospedales, Hakan Bilen",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00561"
    },
    {
        "id": 1642,
        "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
        "authors": "Kaiqi Fu, Shaojun Gao, Shuju Shi, Xiaohai Tian, Wei Li, Zejun Ma",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-587"
    },
    {
        "id": 1643,
        "title": "3D-CSL: Self-Supervised 3D Context Similarity Learning for Near-Duplicate Video Retrieval",
        "authors": "Rui Deng, Qian Wu, Yuke Li",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222915"
    },
    {
        "id": 1644,
        "title": "Self-supervised deep learning uncovers the semantic landscape of drug-induced latent mitochondrial phenotypes",
        "authors": "Zichen Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bpj.2023.11.1104"
    },
    {
        "id": 1645,
        "title": "Self-supervised Learning of Semantic Correspondence Using Web Videos",
        "authors": "Donghyeon Kwon, Minsu Cho, Suha Kwak",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00214"
    },
    {
        "id": 1646,
        "title": "Classification of Crops through Self-Supervised Decomposition for Transfer Learning",
        "authors": "J. Jayanth, H. K. Ravikiran, K. M. Madhu",
        "published": "2023-10-17",
        "citations": 1,
        "abstract": "The 2S-DT (Self-Supervised Decomposition for Transfer Learning) model, created for crop categorization using remotely sensed data, is a unique method introduced in this paper. It deals with the difficulty of incorrectly identifying crops with comparable phenology patterns, a problem that frequently arises in agricultural remote sensing. Two datasets from Nanajangudu taluk in the Mysore district, which has a widely varied irrigated agriculture system, are used to assess the model. Using self-supervised learning, the 2S-DT model addresses the misclassification issue that frequently occurs when working with unlabeled classes, especially in high-resolution images. It uses class decomposition (CD) layer and a downstream learning approach. Using the model’s learning and the particulars of each geographical context, this layer improves the information’s arrangement. Our model architecture’s foundation is ResNet, a well-known deep learning framework. Each residual block in our ResNet architecture is made up of two 3x3 convolutional layers. Each convolutional layer is followed by batch normalization and Rectified Linear Unit (ReLU) activation functions, which improve the model’s capacity for learning. We utilized a 7x7 convolutional layer with 64 filters and a stride of 2 for Conv1 in ResNet18, resulting in an output size of 112x112x64. Conv2, which consists of Res2a and Res2b, generated an output with the dimensions 48x48x64. Conv3, which included Res3a and Res3b, produced an output with the dimensions 28x28x128. These architectural selections were made with our experimental needs in mind. The 2S-DT model’s newly added features make it easier to identify classes and update weights, improving the stability of the features’ spatial and spectral data. Extensive tests performed on two datasets show the model’s viability. Overall accuracy has improved significantly, with the 2S-DT model surpassing comparable models like TVSM, 3DCAE, and GAN Model by obtaining 95.65% accuracy for dataset 1 and 88.91% accuracy for dataset 2.",
        "keywords": "",
        "link": "http://dx.doi.org/10.25081/jaa.2023.v9.8566"
    },
    {
        "id": 1647,
        "title": "Sudowoodo: Contrastive Self-supervised Learning for Multi-purpose Data Integration and Preparation",
        "authors": "Runhui Wang, Yuliang Li, Jin Wang",
        "published": "2023-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icde55515.2023.00391"
    },
    {
        "id": 1648,
        "title": "Self supervised learning and the poverty of the stimulus",
        "authors": "Csaba Veres, Jennifer Sampson",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.datak.2023.102208"
    },
    {
        "id": 1649,
        "title": "Self-Supervised Representation Learning for Diagnosis of Cardiac Abnormalities on Echocardiograms",
        "authors": "Ramkumar Krishnamoorthy, Ajay Agrawal, Puneet Agarwal",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470471"
    },
    {
        "id": 1650,
        "title": "Self-supervised latent feature learning for partial point clouds recognition",
        "authors": "Ziyu Zhang, Feipeng Da",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.10.009"
    },
    {
        "id": 1651,
        "title": "Levels of disability in the older population of England: An unsupervised and self-supervised learning approach",
        "authors": "Marjan Qazvini",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdmw60847.2023.00088"
    },
    {
        "id": 1652,
        "title": "Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
        "authors": "Minchuan Chen, Chenfeng Miao, Jun Ma, Shaojun Wang, Jing Xiao",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1623"
    },
    {
        "id": 1653,
        "title": "Self-Supervised Learning for Context-Independent DfD Network using Multi-View Rank Supervision",
        "authors": "Nao Mishima, Akihito Seki, Shinsaku Hiura",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222834"
    },
    {
        "id": 1654,
        "title": "More Synergy, Less Redundancy: Exploiting Joint Mutual Information for Self-Supervised Learning",
        "authors": "Salman Mohamadi, Gianfranco Doretto, Donald A. Adjeroh",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222547"
    },
    {
        "id": 1655,
        "title": "Self-Supervised Spatio-Temporal Representation Learning of Satellite Image Time Series",
        "authors": "Iris Dumeur, Silvia Valero, Jordi Inglada",
        "published": "2023-7-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281412"
    },
    {
        "id": 1656,
        "title": "Efficient Few-Shot Classification Using Self-Supervised Learning and Class Factor Analysis",
        "authors": "Youngjae Lee, Hyeyoung Park",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic60209.2024.10463486"
    },
    {
        "id": 1657,
        "title": "Learning self-supervised molecular representations for drug–drug interaction prediction",
        "authors": "Rogia Kpanou, Patrick Dallaire, Elsa Rousseau, Jacques Corbeil",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "AbstractDrug–drug interactions (DDI) are a critical concern in healthcare due to their potential to cause adverse effects and compromise patient safety. Supervised machine learning models for DDI prediction need to be optimized to learn abstract, transferable features, and generalize to larger chemical spaces, primarily due to the scarcity of high-quality labeled DDI data. Inspired by recent advances in computer vision, we present SMR–DDI, a self-supervised framework that leverages contrastive learning to embed drugs into a scaffold-based feature space. Molecular scaffolds represent the core structural motifs that drive pharmacological activities, making them valuable for learning informative representations. Specifically, we pre-trained SMR–DDI on a large-scale unlabeled molecular dataset. We generated augmented views for each molecule via SMILES enumeration and optimized the embedding process through contrastive loss minimization between views. This enables the model to capture relevant and robust molecular features while reducing noise. We then transfer the learned representations for the downstream prediction of DDI. Experiments show that the new feature space has comparable expressivity to state-of-the-art molecular representations and achieved competitive DDI prediction results while training on less data. Additional investigations also revealed that pre-training on more extensive and diverse unlabeled molecular datasets improved the model’s capability to embed molecules more effectively. Our results highlight contrastive learning as a promising approach for DDI prediction that can identify potentially hazardous drug combinations using only structural information.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12859-024-05643-7"
    },
    {
        "id": 1658,
        "title": "To Compress or Not to Compress—Self-Supervised Learning and Information Theory: A Review",
        "authors": "Ravid Shwartz Ziv, Yann LeCun",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory has shaped deep neural networks, particularly the information bottleneck principle. This principle optimizes the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. This framework includes multiple encoders and decoders, suggesting that all existing work on self-supervised learning can be seen as specific instances. We aim to unify these approaches to understand their underlying principles better and address the main challenge: many works present different frameworks with differing theories that may seem contradictory. By weaving existing research into a cohesive narrative, we delve into contemporary self-supervised methodologies, spotlight potential research areas, and highlight inherent challenges. Moreover, we discuss how to estimate information-theoretic quantities and their associated empirical problems. Overall, this paper provides a comprehensive review of the intersection of information theory, self-supervised learning, and deep neural networks, aiming for a better understanding through our proposed unified approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/e26030252"
    },
    {
        "id": 1659,
        "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
        "authors": "Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, Yuzong Liu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2362"
    },
    {
        "id": 1660,
        "title": "CCC-WAV2VEC 2.0: Clustering AIDED Cross Contrastive Self-Supervised Learning of Speech Representations",
        "authors": "Vasista Sai Lodagala, Sreyan Ghosh, S. Umesh",
        "published": "2023-1-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022552"
    },
    {
        "id": 1661,
        "title": "Cross-Architecture Relational Consistency for Point Cloud Self-Supervised Learning",
        "authors": "Hongyu Li, Yifei Zhang, Dongbao Yang",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00103"
    },
    {
        "id": 1662,
        "title": "Federated Self-Supervised Learning for Intrusion Detection",
        "authors": "Bruno H. Meyer, Aurora T.R. Pozo, Michele Nogueira, Wagner M. Nunan Zola",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371956"
    },
    {
        "id": 1663,
        "title": "Unbiased and augmentation-free self-supervised graph representation learning",
        "authors": "Ruyue Liu, Rong Yin, Yong Liu, Weiping Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2024.110274"
    },
    {
        "id": 1664,
        "title": "Self-supervised learning for neural-network-based perturbative fiber nonlinearity compensation",
        "authors": "D. Tang, Y. Jiang, Z. Wu, Y. Qiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.2044"
    },
    {
        "id": 1665,
        "title": "Context-Aware Self-Supervised Learning of Whole Slide Images",
        "authors": "Milan Aryal, Nasim Yahya Soltani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2024.3365779"
    },
    {
        "id": 1666,
        "title": "Leveraging Self-Supervised Transfer Learning for Robust Medical Image Classification",
        "authors": "Surendra Yadav, Rakesh Kumar Dwivedi, Gobi N",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470710"
    },
    {
        "id": 1667,
        "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
        "authors": "Salman Mohamadi, Gianfranco Doretto, Donald A. Adjeroh",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00282"
    },
    {
        "id": 1668,
        "title": "Generative and contrastive based self-supervised learning model for histopathology image analysis",
        "authors": "Hongbo Chu, Fang Li, Yonghong He, Tian Guan",
        "published": "2023-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3587716.3587774"
    },
    {
        "id": 1669,
        "title": "Research on image classification mechanism based on self-supervised learning",
        "authors": "Wen Zhou, Jingyi Yang, Hanyu Meng, Yi Shi, Le Luo",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2675246"
    },
    {
        "id": 1670,
        "title": "Self-Supervised Learning of Action Affordances as Interaction Modes",
        "authors": "Liquan Wang, Nikita Dvornik, Rafael Dubeau, Mayank Mittal, Animesh Garg",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161371"
    },
    {
        "id": 1671,
        "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
        "authors": "Kavan Fatehi, Ayse Kucukyilmaz",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2001"
    },
    {
        "id": 1672,
        "title": "Modular Self-Supervised Learning for Hand Surgical Diagnosis",
        "authors": "Léo Déchaumet, Younés Bennani, Joseph Karkazan, Abir Barbara, Charles Dacheux, Thomas Gregory",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191165"
    },
    {
        "id": 1673,
        "title": "Seismic Data Random Noise Attenuation Using Visible Blind Spot Self-Supervised Learning",
        "authors": "Zitai Xu, Bangyu Wu, Hui Yang",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10283058"
    },
    {
        "id": 1674,
        "title": "Self-Supervised Learning Based on Similar Users for Sequential Recommendation",
        "authors": "Xiaomei Shu, Jun He, Feihu Huang, Jian Peng",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10393897"
    },
    {
        "id": 1675,
        "title": "Exploring Novel Self-Supervised Learning Techniques for Image Reconstruction Tasks",
        "authors": " Rupesh Devidas Sushir",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "Image reconstruction tasks, such as super-resolution, inpainting, and denoising, play a crucial role in various computer vision applications. Traditional methods often rely heavily on large labeled datasets for training, which can be costly and time-consuming to acquire. Self-supervised learning has emerged as a promising alternative, aiming to reduce this dependency by leveraging the inherent structures within the data itself. In this paper, we explore novel self-supervised learning techniques tailored specifically for image reconstruction tasks. We propose approaches that exploit the inherent relationships between low and high-resolution images, utilize context-aware information for inpainting, and incorporate generative adversarial networks for denoising. Through extensive experimentation, we demonstrate the efficacy of our methods in achieving competitive performance compared to supervised approaches while significantly reducing the need for labeled data. Our findings pave the way for more efficient and scalable solutions in image reconstruction, offering practical benefits across a wide range of applications",
        "keywords": "",
        "link": "http://dx.doi.org/10.48175/ijarsct-15608"
    },
    {
        "id": 1676,
        "title": "Automating seismic‐well tie via self‐supervised learning",
        "authors": "Haibin Di, Aria Abubakar",
        "published": "2023-5",
        "citations": 1,
        "abstract": "AbstractAs an essential process in subsurface interpretation, seismic‐well tie aims at calibrating the well measurements in depth domain with the seismic records in time domain for reliable reservoir mapping and modelling. Such a process usually requires massive manual efforts, primarily extracting source wavelets to synthesize seismograms from well measurements and stretching/squeezing synthetic seismograms to match actual seismic signals, and thus becomes time‐consuming and labour intensive with the amount of wells expanding in a field. This paper formulates the seismic‐well tie problem from the perspective of computer vision and, considering the lack of manually prepared training labels in most of real projects, proposes automating it by a self‐supervised workflow of two key components. The first component is to extract time‐variant wavelets in the target interval using a dual‐task autoencoder, which is optimized by maximizing the spectrum similarity between the extracted wavelet and the actual seismic. The second component is to estimate the corresponding time‐shift between a pair of the synthetic seismogram and the actual seismic using a flow net, which is trained by a pseudo dataset derived from the actual well measurements. More specifically, the data generation consists with defining one‐dimensional time‐shift curves to warp the original less accurate time‐depth relationships and then synthesizing the corresponding seismograms based on the convolutional model at all available wells. When turning to the stage of inference at a target well, the proposed workflow automatically extracts a representative wavelet, generates the corresponding synthetic seismogram from the original time‐depth relationship and estimates the time‐shift curve necessary for revising the original time‐depth relationship that would lead to an optimal tying with the actual seismic at the well. The proposed workflow is tested on two field datasets, and both results demonstrate improved tying over traditional methods such as statistical wavelet extraction and dynamic time warping.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/1365-2478.13333"
    },
    {
        "id": 1677,
        "title": "Hard-Negatives Focused Self-Supervised Learning",
        "authors": "Yiming Pan, Hua Cheng, Yiquan Fang, Yufei Liu, Taotao Liu",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cyberc58899.2023.00022"
    },
    {
        "id": 1678,
        "title": "Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition",
        "authors": "Bulat Khaertdinov, Stylianos Asteriadis",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcb57857.2023.10448965"
    },
    {
        "id": 1679,
        "title": "Generalized self-supervised contrastive learning with bregman divergence for image recognition",
        "authors": "Zhiyuan Li, Anca Ralescu",
        "published": "2023-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.05.020"
    },
    {
        "id": 1680,
        "title": "Deep semi-supervised and self-supervised learning for diabetic retinopathy detection",
        "authors": "José Miguel Arrieta, Oscar Julian Perdomo Charry, Fabio A. González",
        "published": "2023-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2669723"
    },
    {
        "id": 1681,
        "title": "Self-supervised learning enhanced ultrasound video thyroid nodule tracking",
        "authors": "Ningtao Liu, Aaron Fenster, David Tessier, Shuiping Gou, Jaron Chong",
        "published": "2023-4-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2654280"
    },
    {
        "id": 1682,
        "title": "PCB defect detection with self-supervised learning of local image patches",
        "authors": "Naifu Yao, Yongqiang Zhao, Seong G. Kong, Yang Guo",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.measurement.2023.113611"
    },
    {
        "id": 1683,
        "title": "AtmoDist: Self-supervised representation learning for atmospheric dynamics",
        "authors": "Sebastian Hoffmann, Christian Lessig",
        "published": "2023",
        "citations": 0,
        "abstract": "Abstract\nRepresentation learning has proven to be a powerful methodology in a wide variety of machine-learning applications. For atmospheric dynamics, however, it has so far not been considered, arguably due to the lack of large-scale, labeled datasets that could be used for training. In this work, we show how to sidestep the difficulty and introduce a self-supervised learning task that is applicable to a wide variety of unlabeled atmospheric datasets. Specifically, we train a neural network on the simple yet intricate task of predicting the temporal distance between atmospheric fields from distinct but nearby times. We demonstrate that training with this task on the ERA5 reanalysis dataset leads to internal representations that capture intrinsic aspects of atmospheric dynamics. For example, when employed as a loss function in other machine-learning applications, the derived AtmoDist distance leads to improved results compared to the \n\n\n$ {\\mathrm{\\ell}}_2 $\n\n-loss. For downscaling one obtains higher resolution fields that match the true statistics more closely than previous approaches and for the interpolation of missing or occluded data the AtmoDist distance leads to results that contain more realistic fine-scale features. Since it is obtained from observational data, AtmoDist also provides a novel perspective on atmospheric predictability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/eds.2023.1"
    },
    {
        "id": 1684,
        "title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank",
        "authors": "Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, Jingbo Shang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.719"
    },
    {
        "id": 1685,
        "title": "Diffraction denoising using self‐supervised learning",
        "authors": "Magdalena Markovic, Reza Malehmir, Alireza Malehmir",
        "published": "2023-9",
        "citations": 1,
        "abstract": "AbstractDiffraction wavefield contains valuable information on subsurface composition through velocity extraction and sometimes anisotropy estimation. It can also be used for the delineation of geological features, such as faults, fractures and mineral deposits. Diffraction recognition is, therefore, crucial for improved interpretation of seismic data. To date, many workflows for diffraction denoising, including deep‐learning applications, have been provided, however, with a major focus on sedimentary settings or for ground‐penetrating radar data. In this study, we have developed a workflow for a self‐supervised learning technique, an autoencoder, for diffraction denoising on synthetic seismic, ground‐penetrating radar and hardrock seismic datasets. The autoencoder provides promising results especially for the ground‐penetrating radar data. Depending on the target of the studies, diffraction signals can be tackled using the autoencoder both as the signal and/or noise when, for example, a reflection is a target. The real hardrock seismic data required additional pre‐ and post‐autoencoder image processing steps to improve automatic delineation of the diffraction. Here, we also coupled the autoencoder with Hough transform and pixel edge detection filters. Along inlines and crosslines, diffraction signals have sometimes a similar character as the reflection and may spatially be correlated making the denoising workflow unsuccessful. Coupled with additional image processing steps, we successfully isolated diffraction that is generated from a known volcanogenic massive sulphide deposit. These encouraging results suggest that the self‐supervised learning techniques such as the autoencoder can be used also for seismic mineral exploration purposes and are worthy to be implemented as additional tools for data processing and target detections.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/1365-2478.13340"
    },
    {
        "id": 1686,
        "title": "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory",
        "authors": "Rui Li, Zhiwei Xie, Haihua Xu, Yizhou Peng, Hexin Liu, Hao Huang, Eng Siong Chng",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1702"
    },
    {
        "id": 1687,
        "title": "Self-Supervised Learning for Place Representation Generalization across Appearance Changes",
        "authors": "Mohamed Adel Musallam, Vincent Gaudillière, Djamila Aouada",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00728"
    },
    {
        "id": 1688,
        "title": "Self-supervised comparative learning based improved multiple instance learning for whole slide image classification",
        "authors": "Luhan Yao, Hongyu Wang, Yingguang Hao",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3644116.3644360"
    },
    {
        "id": 1689,
        "title": "Embodied Self-Supervised Learning (EMSSL) with Sampling and Training Coordination for Robot Arm Inverse Kinematic Model Learning",
        "authors": "Qu Weiming, Liu Tianlin, Wu Xihong, Luo Dingsheng",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364338"
    },
    {
        "id": 1690,
        "title": "Evaluate AMR Graph Similarity via Self-supervised Learning",
        "authors": "Ziyi Shou, Fangzhen Lin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.892"
    },
    {
        "id": 1691,
        "title": "LFM Signal Sources Classification Based on Self-supervised Learning",
        "authors": "Tianqi Yang, Siya Mi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2528/pierl23073102"
    },
    {
        "id": 1692,
        "title": "Self-Supervised Facial Motion Representation Learning via Contrastive Subclips",
        "authors": "Zheng Sun, Shad A. Torrie, Andrew W. Sumsion, Dah-Jye Lee",
        "published": "2023-3-13",
        "citations": 0,
        "abstract": "Facial motion representation learning has become an exciting research topic, since biometric technologies are becoming more common in our daily lives. One of its applications is identity verification. After recording a dynamic facial motion video for enrollment, the user needs to show a matched facial appearance and make a facial motion the same as the enrollment for authentication. Some recent research papers have discussed the benefits of this new biometric technology and reported promising results for both static and dynamic facial motion verification tasks. Our work extends the existing approaches and introduces compound facial actions, which contain more than one dominant facial action in one utterance. We propose a new self-supervised pretraining method called contrastive subclips that improves the model performance with these more complex and secure facial motions. The experimental results show that the contrastive subclips method improves upon the baseline approaches, and the model performance for test data can reach 89.7% average precision.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12061369"
    },
    {
        "id": 1693,
        "title": "Randomly shuffled convolution for self-supervised representation learning",
        "authors": "Youngjin Oh, Minkyu Jeon, Dohwan Ko, Hyunwoo J. Kim",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2022.11.022"
    },
    {
        "id": 1694,
        "title": "Bootstrap Predictive Coding: Investigating a Non-Contrastive Self-Supervised Learning Approach",
        "authors": "Yumnah Mohamied, Peter Bell",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447173"
    },
    {
        "id": 1695,
        "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
        "authors": "Minhyeok Lee",
        "published": "2023-5-25",
        "citations": 8,
        "abstract": "In this paper, we present a rigorous mathematical examination of generative pre-trained transformer (GPT) models and their autoregressive self-supervised learning mechanisms. We begin by defining natural language space and knowledge space, which are two key concepts for understanding the dimensionality reduction process in GPT-based large language models (LLMs). By exploring projection functions and their inverses, we establish a framework for analyzing the language generation capabilities of these models. We then investigate the GPT representation space, examining its implications for the models’ approximation properties. Finally, we discuss the limitations and challenges of GPT models and their learning mechanisms, considering trade-offs between complexity and generalization, as well as the implications of incomplete inverse projection functions. Our findings demonstrate that GPT models possess the capability to encode knowledge into low-dimensional vectors through their autoregressive self-supervised learning mechanism. This comprehensive analysis provides a solid mathematical foundation for future advancements in GPT-based LLMs, promising advancements in natural language processing tasks such as language translation, text summarization, and question answering due to improved understanding and optimization of model training and performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11112451"
    },
    {
        "id": 1696,
        "title": "Motif Masking-based Self-Supervised Learning For Molecule Graph Representation Learning*",
        "authors": "Yasu Wu, Changlong Fu, Manwen Yang, Haoran Duan, Cheng Xie",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icebe59045.2023.00040"
    },
    {
        "id": 1697,
        "title": "A Comprehensive Study on Self-Supervised Distillation for Speaker Representation Learning",
        "authors": "Zhengyang Chen, Yao Qian, Bing Han, Yanmin Qian, Michael Zeng",
        "published": "2023-1-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022470"
    },
    {
        "id": 1698,
        "title": "Channel-Aware Self-Supervised Learning for EEG-based BCI",
        "authors": "Sangmin Jo, Jaehyun Jeon, Seungwoo Jeong, Heung-Il Suk",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bci57258.2023.10078451"
    },
    {
        "id": 1699,
        "title": "Automated Self-Supervised Learning for Recommendation",
        "authors": "Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao",
        "published": "2023-4-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543507.3583336"
    },
    {
        "id": 1700,
        "title": "Self-Supervised Representation Learning for Geographical Data—A Systematic Literature Review",
        "authors": "Padraig Corcoran, Irena Spasić",
        "published": "2023-2-12",
        "citations": 0,
        "abstract": "Self-supervised representation learning (SSRL) concerns the problem of learning a useful data representation without the requirement for labelled or annotated data. This representation can, in turn, be used to support solutions to downstream machine learning problems. SSRL has been demonstrated to be a useful tool in the field of geographical information science (GIS). In this article, we systematically review the existing research literature in this space to answer the following five research questions. What types of representations were learnt? What SSRL models were used? What downstream problems were the representations used to solve? What machine learning models were used to solve these problems? Finally, does using a learnt representation improve the overall performance?",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/ijgi12020064"
    },
    {
        "id": 1701,
        "title": "Strawberry Pests and Diseases Recognition with Self-Supervised Learning",
        "authors": "Aung Si Min Htet, Hyo Jong Lee",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csce60160.2023.00321"
    },
    {
        "id": 1702,
        "title": "Self-supervised Cross-stage Regional Contrastive Learning for Object Detection",
        "authors": "Junkai Yan, Lingxiao Yang, Yipeng Gao, Wei-Shi Zheng",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00183"
    },
    {
        "id": 1703,
        "title": "Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos",
        "authors": "Rui Qian, Shuangrui Ding, Xian Liu, Dahua Lin",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01529"
    },
    {
        "id": 1704,
        "title": "Giga-SSL: Self-Supervised Learning for Gigapixel Images",
        "authors": "Tristan Lazard, Marvin Lerousseau, Etienne Decencière, Thomas Walter",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00453"
    }
]