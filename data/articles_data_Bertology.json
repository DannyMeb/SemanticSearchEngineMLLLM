[
    {
        "id": 10660,
        "title": "BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology",
        "authors": "Luke Gessler, Nathan Schneider",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.blackboxnlp-1.43"
    },
    {
        "id": 10661,
        "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models",
        "authors": "Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani",
        "published": "No Date",
        "citations": 110,
        "abstract": "AbstractTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available athttps://github.com/salesforce/provis.",
        "link": "http://dx.doi.org/10.1101/2020.06.26.174417"
    },
    {
        "id": 10662,
        "title": "The ambiguity of BERTology: what do large language models represent?",
        "authors": "Tommi Buder-Gröndahl",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "AbstractThe field of “BERTology” aims to locate linguistic representations in large language models (LLMs). These have commonly been interpreted as representing structural descriptions (SDs) familiar from theoretical linguistics, such as abstract phrase-structures. However, it is unclear how such claims should be interpreted in the first place. This paper identifies six possible readings of “linguistic representation” from philosophical and linguistic literature, concluding that none has a straight-forward application to BERTology. In philosophy, representations are typically analyzed as cognitive vehicles individuated by intentional content. This clashes with a prevalent mentalist interpretation of linguistics, which treats SDs as (narrow) properties of cognitive vehicles themselves. I further distinguish between three readings of both kinds, and discuss challenges each brings for BERTology. In particular, some readings would make it trivially false to assign representations of SDs to LLMs, while others would make it trivially true. I illustrate this with the concrete case study of structural probing: a dominant model-interpretation technique. To improve the present situation, I propose that BERTology should adopt a more “LLM-first” approach instead of relying on pre-existing linguistic theories developed for orthogonal purposes.",
        "link": "http://dx.doi.org/10.1007/s11229-023-04435-5"
    },
    {
        "id": 10663,
        "title": "Chinese text classification by combining Chinese-BERTology-wwm and GCN",
        "authors": "Xue Xu, Yu Chang, Jianye An, Yongqiang Du",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "Text classification is an important and classic application in natural language processing (NLP). Recent studies have shown that graph neural networks (GNNs) are effective in tasks with rich structural relationships and serve as effective transductive learning approaches. Text representation learning methods based on large-scale pretraining can learn implicit but rich semantic information from text. However, few studies have comprehensively utilized the contextual semantic and structural information for Chinese text classification. Moreover, the existing GNN methods for text classification did not consider the applicability of their graph construction methods to long or short texts. In this work, we propose Chinese-BERTology-wwm-GCN, a framework that combines Chinese bidirectional encoder representations from transformers (BERT) series models with whole word masking (Chinese-BERTology-wwm) and the graph convolutional network (GCN) for Chinese text classification. When building text graph, we use documents and words as nodes to construct a heterogeneous graph for the entire corpus. Specifically, we use the term frequency-inverse document frequency (TF-IDF) to construct the word-document edge weights. For long text corpora, we propose an improved pointwise mutual information (PMI*) measure for words according to their word co-occurrence distances to represent the weights of word-word edges. For short text corpora, the co-occurrence information between words is often limited. Therefore, we utilize cosine similarity to represent the word-word edge weights. During the training stage, we effectively combine the cross-entropy and hinge losses and use them to jointly train Chinese-BERTology-wwm and GCN. Experiments show that our proposed framework significantly outperforms the baselines on three Chinese benchmark datasets and achieves good performance even with few labeled training sets.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1544"
    },
    {
        "id": 10664,
        "title": "How “BERTology\" Changed the State-of-the-Art also for Italian NLP",
        "authors": "Fabio Tamburini",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.aaccademia.8920"
    },
    {
        "id": 10665,
        "title": "An approach of data augmentation to improve the performance of BERTology models for Vietnamese hate speech detection",
        "authors": "Son T. Luu, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-16968-5"
    },
    {
        "id": 10666,
        "title": "A Primer in BERTology: What We Know About How BERT Works",
        "authors": "Anna Rogers, Olga Kovaleva, Anna Rumshisky",
        "published": "2020-12",
        "citations": 350,
        "abstract": " Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research. ",
        "link": "http://dx.doi.org/10.1162/tacl_a_00349"
    },
    {
        "id": 10667,
        "title": "UniBO @ KIPoS: Fine-tuning the Italian “BERTology\" for PoS-tagging Spoken Data",
        "authors": "Fabio Tamburini",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.aaccademia.7768"
    },
    {
        "id": 10668,
        "title": "Framing and BERTology: A Data-Centric Approach to Integration of Linguistic Features into Transformer-Based Pre-trained Language Models",
        "authors": "Hayastan Avetisyan, Parisa Safikhani, David Broneske",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47718-8_6"
    },
    {
        "id": 10669,
        "title": "Multi-stage transfer learning with BERTology-based language models for question answering system in vietnamese",
        "authors": "Kiet Van Nguyen, Phong Nguyen-Thuan Do, Nhat Duy Nguyen, Anh Gia-Tuan Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13042-022-01735-z"
    },
    {
        "id": 10670,
        "title": "Error Investigation of Pre-trained BERTology Models on Vietnamese Natural Language Inference",
        "authors": "Tin Van Huynh, Huy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-8234-7_14"
    }
]