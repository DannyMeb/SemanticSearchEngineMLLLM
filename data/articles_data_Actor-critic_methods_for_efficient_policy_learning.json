[
    {
        "id": 32305,
        "title": "Reinforcement Learning Maximized-Actor-Critic(MAC) Method Based on Policy-Gradient",
        "authors": "Jung-Hyun Kim, Yong-Hoon Choi, Min-Suk Kim",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.9717/kmms.2024.27.3.455"
    },
    {
        "id": 32306,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 32307,
        "title": "Efficient Bimanual Handover and Rearrangement via Symmetry-Aware Actor-Critic Learning",
        "authors": "Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, Yi Wu",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160739"
    },
    {
        "id": 32308,
        "title": "Boosting On-Policy Actor–Critic With Shallow Updates in Critic",
        "authors": "Luntong Li, Yuanheng Zhu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3378913"
    },
    {
        "id": 32309,
        "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples With On-Policy Experiences",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "2024-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3174051"
    },
    {
        "id": 32310,
        "title": "Efficient Multi-Agent Exploration with Mutual-Guided Actor-Critic",
        "authors": "Renlong Chen, Ying Tan",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254169"
    },
    {
        "id": 32311,
        "title": "Novel Methods Inspired by Reinforcement Learning Actor-Critic Mechanism for Eye-in-Hand Calibration in Robotics",
        "authors": "Chenxing Li, Yinlong Liu, Yingbai Hu, Fabian Schreier, Jan Seyler, Shahram Eivazi",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364560"
    },
    {
        "id": 32312,
        "title": "Multi-actor mechanism for actor-critic reinforcement learning",
        "authors": "Lin Li, Yuze Li, Wei Wei, Yujia Zhang, Jiye Liang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119494"
    },
    {
        "id": 32313,
        "title": "Optimizing Advantage Actor-Critic with Policy Gradient and Deep Q-learning to Maximize Profit in Forex Trading Prediction",
        "authors": "Abdillah Baradja, Rahmat Gernowo, Adi Wibowo",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ice-smartech59237.2023.10461955"
    },
    {
        "id": 32314,
        "title": "A deep actor critic reinforcement learning framework for learning to rank",
        "authors": "Vaibhav Padhye, Kailasam Lakshmanan",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126314"
    },
    {
        "id": 32315,
        "title": "Beam Selection for Energy-Efficient mmWave Network Using Advantage Actor Critic Learning",
        "authors": "Ycaro Dantas, Pedro Enrique Iturria-Rivera, Hao Zhou, Majid Bavand, Medhat Elsayed, Raimundas Gaigalas, Melike Erol-Kantarci",
        "published": "2023-5-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279804"
    },
    {
        "id": 32316,
        "title": "Q-LEARNING, POLICY ITERATION AND ACTOR-CRITIC REINFORCEMENT LEARNING COMBINED WITH METAHEURISTIC ALGORITHMS IN SERVO SYSTEM CONTROL",
        "authors": "Iuliu Alexandru Zamfirache, Radu-Emil Precup, Emil M. Petriu",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "This paper carries out the performance analysis of three control system structures and approaches, which combine Reinforcement Learning (RL) and Metaheuristic Algorithms (MAs) as representative optimization algorithms. In the first approach, the Gravitational Search Algorithm (GSA) is employed to initialize the parameters (weights and biases) of the Neural Networks (NNs) involved in Deep Q-Learning by replacing the traditional way of initializing the NNs based on random generated values. In the second approach, the Grey Wolf Optimizer (GWO) algorithm is employed to train the policy NN in Policy Iteration RL-based control. In the third approach, the GWO algorithm is employed as a critic in an Actor-Critic framework, and used to evaluate the performance of the actor NN. The goal of this paper is to analyze all three RL-based control approaches, aiming to determine which one represents the best fit for solving the proposed control optimization problem. The performance analysis is based on non-parametric statistical tests conducted on the data obtained from real-time experimental results specific to nonlinear servo system position control.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22190/fume231011044z"
    },
    {
        "id": 32317,
        "title": "Exponential TD Learning: A Risk-Sensitive Actor-Critic Reinforcement Learning Algorithm",
        "authors": "Erfaun Noorani, Christos N. Mavridis, John S. Baras",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156626"
    },
    {
        "id": 32318,
        "title": "Efficient Difficulty Level Balancing in Match-3 Puzzle Games: A Comparative Study of Proximal Policy Optimization and Soft Actor-Critic Algorithms",
        "authors": "Byounggwon Kim, Jungyoon Kim",
        "published": "2023-10-30",
        "citations": 1,
        "abstract": "Match-3 puzzle games have garnered significant popularity across all age groups due to their simplicity, non-violent nature, and concise gameplay. However, the development of captivating and well-balanced stages in match-3 puzzle games remains a challenging task for game developers. This study aims to identify the optimal algorithm for reinforcement learning to streamline the level balancing verification process in match-3 games by comparison with Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO) algorithms. By training the agent with these two algorithms, the paper investigated which approach yields more efficient and effective difficulty level balancing test results. After the comparative analysis of cumulative rewards and entropy, the findings illustrate that the SAC algorithm is the optimal choice for creating an efficient agent capable of handling difficulty level balancing for stages in a match-3 puzzle game. This is because the superior learning performance and higher stability demonstrated by the SAC algorithm are more important in terms of stage difficulty balancing in match-3 gameplay. This study expects to contribute to the development of improved level balancing techniques in match-3 puzzle games besides enhancing the overall gaming experience for players.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12214456"
    },
    {
        "id": 32319,
        "title": "Dynamic Pricing Based on Demand Response Using Actor–Critic Agent Reinforcement Learning",
        "authors": "Ahmed Ismail, Mustafa Baysal",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "Eco-friendly technologies for sustainable energy development require the efficient utilization of energy resources. Real-time pricing (RTP), also known as dynamic pricing, offers advantages over other pricing systems by enabling demand response (DR) actions. However, existing methods for determining and controlling DR have limitations in managing an increasing demand and predicting future pricing. This paper presents a novel approach to address the limitations of existing methods for determining and controlling demand response (DR) in the context of dynamic pricing systems for sustainable energy development. By leveraging actor–critic agent reinforcement learning (RL) techniques, a dynamic pricing DR model is proposed for efficient energy management. The model’s learning framework was trained using DR and real-time pricing data extracted from the Australian Energy Market Operator (AEMO) spanning a period of 17 years. The efficacy of the RL-based dynamic pricing approach was evaluated through two predicting cases: actual-predicted demand and actual-predicted price. Initially, long short-term memory (LSTM) models were employed to predict price and demand, and the results were subsequently enhanced using the deep RL model. Remarkably, the proposed approach achieved an impressive accuracy of 99% for every 30 min future price prediction. The results demonstrated the efficiency of the proposed RL-based model in accurately predicting both demand and price for effective energy management.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16145469"
    },
    {
        "id": 32320,
        "title": "Meta attention for Off-Policy Actor-Critic",
        "authors": "Jiateng Huang, Wanrong Huang, Long Lan, Dan Wu",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.024"
    },
    {
        "id": 32321,
        "title": "Communication-Efficient Multi-Agent Actor-Critic Framework for Distributed Optimization of Resource Allocation in V2X Networks",
        "authors": "Nessrine Hammami, Kim Khoa Nguyen",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278982"
    },
    {
        "id": 32322,
        "title": "Pursuit-Evasion Game Based on Fuzzy Actor-Critic Learning with Obstacle in Continuous Environment",
        "authors": "Penglin Hu, Quan Pan, Zheng Tan",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451878"
    },
    {
        "id": 32323,
        "title": "An Empirical Study of On-Policy and Off-Policy Actor-Critic Algorithms in the Context of Exploration-Exploitation Dilemma",
        "authors": "Supriya Seshagiri, Prema K V",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icetci58599.2023.10331400"
    },
    {
        "id": 32324,
        "title": "Schedule Extra Train(s) into Existing Timetable Using Actor-Critic Reinforcement Learning",
        "authors": "Jin Liu, Ronghui Liu",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422338"
    },
    {
        "id": 32325,
        "title": "Reinforcement Learning for Continuous-Time Optimal Execution: Actor-Critic Algorithm and Error Analysis",
        "authors": "Boyu Wang, Xuefeng Gao, Lingfei Li",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4378950"
    },
    {
        "id": 32326,
        "title": "Stochastic Integrated Actor–Critic for Deep Reinforcement Learning",
        "authors": "Jiaohao Zheng, Mehmet Necip Kurt, Xiaodong Wang",
        "published": "2024",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3212273"
    },
    {
        "id": 32327,
        "title": "Robust Active Simultaneous Localization and Mapping Based on Bayesian Actor-Critic Reinforcement Learning",
        "authors": "Bryan Pedraza, Dimah Dera",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00035"
    },
    {
        "id": 32328,
        "title": "Stable and Safe Reinforcement Learning via a Barrier-Lyapunov Actor-Critic Approach",
        "authors": "Liqun Zhao, Konstantinos Gatsis, Antonis Papachristodoulou",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383742"
    },
    {
        "id": 32329,
        "title": "Recurrent Soft Actor Critic Reinforcement Learning for Demand Response Problems",
        "authors": "Ulrich Ludolfinger, Daniel Zinsmeister, Vedran S. Perić, Thomas Hamacher, Sascha Hauke, Maren Martens",
        "published": "2023-6-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202844"
    },
    {
        "id": 32330,
        "title": "Learn to Race: Sequential Actor-Critic Reinforcement Learning for Autonomous Racing",
        "authors": "Ran Liu, Weichao Zhuang, Feifan Tong, Guodong Yin",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iavvc57316.2023.10328086"
    },
    {
        "id": 32331,
        "title": "Symmetric actor–critic deep reinforcement learning for cascade quadrotor flight control",
        "authors": "Haoran Han, Jian Cheng, Zhilong Xi, Maolong Lv",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126789"
    },
    {
        "id": 32332,
        "title": "Robust Actor-Critic With Relative Entropy Regulating Actor",
        "authors": "Yuhu Cheng, Longyang Huang, C. L. Philip Chen, Xuesong Wang",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3155483"
    },
    {
        "id": 32333,
        "title": "SOAC: Supervised Off-Policy Actor-Critic for Recommender Systems",
        "authors": "Shiqing Wu, Guandong Xu, Xianzhi Wang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00185"
    },
    {
        "id": 32334,
        "title": "Time-Efficient Weapon-Target Assignment by Actor-Critic Reinforcement",
        "authors": "Muhyun Byun, Hyungho Na, IL-Chul Moon",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394214"
    },
    {
        "id": 32335,
        "title": "Actor–critic learning based PID control for robotic manipulators",
        "authors": "Hamed Rahimi Nohooji, Abolfazl Zaraki, Holger Voos",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.111153"
    },
    {
        "id": 32336,
        "title": "Transformer Model Based Soft Actor-Critic Learning for HEMS",
        "authors": "Ulrich Ludolfinger, Vedran S. Perić, Thomas Hamacher, Sascha Hauke, Maren Martens",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powercon58120.2023.10331287"
    },
    {
        "id": 32337,
        "title": "Stimuli Generation for IC Design Verification using Reinforcement Learning with an Actor-Critic Model",
        "authors": "S.L. Tweehuysen, G. L. A. Adriaans, M. Gomony",
        "published": "2023-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ets56758.2023.10174129"
    },
    {
        "id": 32338,
        "title": "Multi-step actor-critic framework for reinforcement learning in continuous control",
        "authors": "",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23952/jano.5.2023.2.01"
    },
    {
        "id": 32339,
        "title": "ACRE: Actor Critic Reinforcement Learning for Failure-Aware Edge Computing Migrations",
        "authors": "Marie Siew, Shikhar Sharma, Carlee Joe-Wong",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089694"
    },
    {
        "id": 32340,
        "title": "An advanced actor critic deep reinforcement learning technique for gamification of WiFi environment",
        "authors": "Vandana Shakya, Jaytrilok Choudhary, Dhirendra Pratap Singh",
        "published": "2023-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11276-023-03582-4"
    },
    {
        "id": 32341,
        "title": "Bi-level Multi-Agent Actor-Critic Methods with ransformers",
        "authors": "Tianjiao Wan, Haibo Mi, Zijian Gao, Yuanzhao Zhai, Bo Ding, Dawei Feng",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jcc59055.2023.00007"
    },
    {
        "id": 32342,
        "title": "Improving Exploration in Actor–Critic With Weakly Pessimistic Value Estimation and Optimistic Policy Optimization",
        "authors": "Fan Li, Mingsheng Fu, Wenyu Chen, Fan Zhang, Haixian Zhang, Hong Qu, Zhang Yi",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3215596"
    },
    {
        "id": 32343,
        "title": "Comparative Study for Deep Deterministic Policy Gradient and Soft Actor Critic Using an Inverted Pendulum System",
        "authors": "Aditya Shelke, Devang Vyas, Abhishek Srivastava",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370289"
    },
    {
        "id": 32344,
        "title": "Soft Actor-Critic Reinforcement Learning-Based Optimization for Analog Circuit Sizing",
        "authors": "Sejin Park, Youngchang Choi, Seokhyeong Kang",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isocc59558.2023.10396499"
    },
    {
        "id": 32345,
        "title": "Hybrid Soft Actor-Critic and Incremental Dual Heuristic Programming Reinforcement Learning for Fault-Tolerant Flight Control",
        "authors": "Casper Teirlinck, Erik-Jan Van Kampen",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2406"
    },
    {
        "id": 32346,
        "title": "Multi-Alpha Soft Actor-Critic: Overcoming Stochastic Biases in Maximum Entropy Reinforcement Learning",
        "authors": "Conor Igoe, Swapnil Pande, Siddarth Venkatraman, Jeff Schneider",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161395"
    },
    {
        "id": 32347,
        "title": "Distributional Safety Critic for Stochastic Latent Actor-Critic",
        "authors": "Thiago S. Miranda, Heder S. Bernardino",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "When employing reinforcement learning techniques in real-world applications, one may desire to constrain the agent by limiting actions that lead to potential damage, harm, or unwanted scenarios. Particularly, recent approaches focus on developing safe behavior under partial observability conditions. In this vein, we develop a method that combines distributional reinforcement learning techniques with methods used to facilitate learning in partially observable environments, called distributional safe stochastic latent actor-critic (DS-SLAC). We evaluate the DS-SLAC performance on four Safety-Gym tasks and DS-SLAC obtained results better than those reached by state-of-the-art algorithms in two of the evaluated environments while being able to develop a safe policy in three of them. Lastly, we also identify the main challenges of performing distributional reinforcement learning in the safety-constrained partially observable setting.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5753/eniac.2023.234620"
    },
    {
        "id": 32348,
        "title": "Deep Reinforcement Learning Object Tracking Based on Actor-Double Critic Network",
        "authors": "Jing Xin, Jianglei Zhou, Xinhong Hei, Pengyu Yue, Jia Zhao",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26599/air.2023.9150013"
    },
    {
        "id": 32349,
        "title": "Group Random Access Control Scheme Based on Asynchronous Advantage Actor Critic",
        "authors": "Su Kim, Han-Seung Jang",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.2.258"
    },
    {
        "id": 32350,
        "title": "MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks",
        "authors": "Raffaele Galliera, Alessandro Morelli, Roberto Fronteddu, Niranjan Suri",
        "published": "2023-5-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/noms56928.2023.10154210"
    },
    {
        "id": 32351,
        "title": "Soft Actor-Critic Learning-Based Joint Computing, Pushing, and Caching Framework in MEC Networks",
        "authors": "Xiangyu Gao, Yaping Sun, Hao Chen, Xiaodong Xu, Shuguang Cui",
        "published": "2023-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437459"
    },
    {
        "id": 32352,
        "title": "Multiple Agents Dispatch via Batch Synchronous Actor Critic in Autonomous Mobility on Demand Systems",
        "authors": "Jiyao Li, Vicki Allan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012351700003636"
    },
    {
        "id": 32353,
        "title": "Deep Reinforcement Learning Based on Actor-Critic for Task Offloading in Vehicle Edge Computing",
        "authors": "Bingxin Wang, Lei Liu, Jie Wang",
        "published": "2023-6-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bmsb58369.2023.10211217"
    },
    {
        "id": 32354,
        "title": "Design of Adaptive Learning Control of Fixed-Wing UAV Based on Actor-Critic",
        "authors": "Fei Kong, Zhengen Zhao, Lei Cheng",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10239872"
    },
    {
        "id": 32355,
        "title": "Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks",
        "authors": "Imed Ghnaya, Mohamed Mosbah, Hasnaâ Aniss, Toufik Ahmed",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/noms56928.2023.10154436"
    },
    {
        "id": 32356,
        "title": "Stability Guaranteed Actor-Critic Learning for Robots in Continuous Time",
        "authors": "Luis Pantoja-Garcia, Vicente Parra-Vega, Rodolfo Garcia-Rodriguez",
        "published": "2023-10-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cce60043.2023.10332814"
    },
    {
        "id": 32357,
        "title": "Improved Soft Actor-Critic: Reducing Bias and Estimation Error for Fast Learning",
        "authors": "Manas Shil, G. N. Pillai, M. K. Gupta",
        "published": "2023-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sceecs57921.2023.10063058"
    },
    {
        "id": 32358,
        "title": "A Novel Actor—Critic Motor Reinforcement Learning for Continuum Soft Robots",
        "authors": "Luis Pantoja-Garcia, Vicente Parra-Vega, Rodolfo Garcia-Rodriguez, Carlos Ernesto Vázquez-García",
        "published": "2023-10-9",
        "citations": 1,
        "abstract": "Reinforcement learning (RL) is explored for motor control of a novel pneumatic-driven soft robot modeled after continuum media with a varying density. This model complies with closed-form Lagrangian dynamics, which fulfills the fundamental structural property of passivity, among others. Then, the question arises of how to synthesize a passivity-based RL model to control the unknown continuum soft robot dynamics to exploit its input–output energy properties advantageously throughout a reward-based neural network controller. Thus, we propose a continuous-time Actor–Critic scheme for tracking tasks of the continuum 3D soft robot subject to Lipschitz disturbances. A reward-based temporal difference leads to learning with a novel discontinuous adaptive mechanism of Critic neural weights. Finally, the reward and integral of the Bellman error approximation reinforce the adaptive mechanism of Actor neural weights. Closed-loop stability is guaranteed in the sense of Lyapunov, which leads to local exponential convergence of tracking errors based on integral sliding modes. Notably, it is assumed that dynamics are unknown, yet the control is continuous and robust. A representative simulation study shows the effectiveness of our proposal for tracking tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/robotics12050141"
    },
    {
        "id": 32359,
        "title": "An actor-critic framework based on deep reinforcement learning for addressing flexible job shop scheduling problems",
        "authors": "Cong Zhao, Na Deng",
        "published": "2023",
        "citations": 0,
        "abstract": "<abstract><p>With the rise of Industry 4.0, manufacturing is shifting towards customization and flexibility, presenting new challenges to meet rapidly evolving market and customer needs. To address these challenges, this paper suggests a novel approach to address flexible job shop scheduling problems (FJSPs) through reinforcement learning (RL). This method utilizes an actor-critic architecture that merges value-based and policy-based approaches. The actor generates deterministic policies, while the critic evaluates policies and guides the actor to achieve the most optimal policy. To construct the Markov decision process, a comprehensive feature set was utilized to accurately represent the system's state, and eight sets of actions were designed, inspired by traditional scheduling rules. The formulation of rewards indirectly measures the effectiveness of actions, promoting strategies that minimize job completion times and enhance adherence to scheduling constraints. The experimental evaluation conducted a thorough assessment of the proposed reinforcement learning framework through simulations on standard FJSP benchmarks, comparing the proposed method against several well-known heuristic scheduling rules, related RL algorithms and intelligent algorithms. The results indicate that the proposed method consistently outperforms traditional approaches and exhibits exceptional adaptability and efficiency, particularly in large-scale datasets.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/mbe.2024062"
    },
    {
        "id": 32360,
        "title": "Optimizing Waterflooding in Multi-Agent Systems through Decentralized Actor-Centralized Critic Reinforcement Learning",
        "authors": "R. Abdalla, M. Stundner Gönczi, O. Toumi, D. Nikolaev, R. Manasipov, M. Stundner",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202439064"
    },
    {
        "id": 32361,
        "title": "On the sample complexity of actor-critic method for reinforcement learning with function approximation",
        "authors": "Harshat Kumar, Alec Koppel, Alejandro Ribeiro",
        "published": "2023-7",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06303-2"
    },
    {
        "id": 32362,
        "title": "Dynamic pricing of product and delivery time in multi-variant production using an actor critic reinforcement learning",
        "authors": "Florian Stamer, Gisela Lanza",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cirp.2023.04.019"
    },
    {
        "id": 32363,
        "title": "Bioinspired actor-critic algorithm for reinforcement learning interpretation with Levy–Brown hybrid exploration strategy",
        "authors": "Xiao Wang, Dazi Li",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127291"
    },
    {
        "id": 32364,
        "title": "Efficient Optimization of Actor-Critic Learning for Constrained Resource Orchestration in RAN with Network Slicing",
        "authors": "Hafiza Kanwal Janjua, Ignacio de Miguel, Ramón J. Durán Barroso, Óscar González de Dios, Juan Carlos Aguado, Noemí Merayo Álvarez, Patricia Fernández, Rubén M. Lorenzo",
        "published": "2023-3-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icin56760.2023.10073489"
    },
    {
        "id": 32365,
        "title": "Learning Stall Recovery Policies using a Soft Actor-Critic Algorithm with Smooth Reward Functions",
        "authors": "Junqiu Wang, Jianmei Tan, Peng Lin, Chenguang Xing, Bo Liu",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/robio58561.2023.10354940"
    },
    {
        "id": 32366,
        "title": "Online Service Function Chain Deployment Method Based on Advantage Actor-Critic Learning",
        "authors": "Yuying Wang, Zhuo Chen",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsins60115.2023.10455499"
    },
    {
        "id": 32367,
        "title": "Statistical arbitrage trading across electricity markets using advantage actor–critic methods",
        "authors": "Sumeyra Demir, Koen Kok, Nikolaos G. Paterakis",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101023"
    },
    {
        "id": 32368,
        "title": "Optimal and Adaptive Engine Switch Control for a Parallel Hybrid Electric Vehicle Using a Computationally Efficient Actor-Critic Method",
        "authors": "Tong Liu, Kaige Tan, Wenyao Zhu, Lei Feng",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aim46323.2023.10196276"
    },
    {
        "id": 32369,
        "title": "Spatial Transform Soft Actor-Critic for Robot Grasping Skill Learning",
        "authors": "Zihao Sun, Xianfeng Yuan, Yong Song, Xiaolong Xu, Bao Pang, Qingyang Xu",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451703"
    },
    {
        "id": 32370,
        "title": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis",
        "authors": "Philip Gorinski, Matthieu Zimmer, Gerasimos Lampouras, Derrick Goh Xin Deik, Ignacio Iacobacci",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.28"
    },
    {
        "id": 32371,
        "title": "Improving Actor–Critic Reinforcement Learning via Hamiltonian Monte Carlo Method",
        "authors": "Duo Xu, Faramarz Fekri",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3215614"
    },
    {
        "id": 32372,
        "title": "A World Model for Actor–Critic in Reinforcement Learning",
        "authors": "A. I. Panov, L. A. Ugadiarov",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1134/s1054661823030379"
    },
    {
        "id": 32373,
        "title": "Multi-agent off-policy actor-critic algorithm for distributed multi-task reinforcement learning",
        "authors": "Miloš S. Stanković, Marko Beko, Nemanja Ilić, Srdjan S. Stanković",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejcon.2023.100853"
    },
    {
        "id": 32374,
        "title": "A Fast Decentralized Scheduling Method of Cooperative Localization Based on Actor-Critic Deep Reinforcement Learning",
        "authors": "Xinyue Di, Yalin Guan, Weijia Yu, Heyun Lin",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicse58435.2023.10211593"
    },
    {
        "id": 32375,
        "title": "Actor-Critic Methods in Stock Trading : A Comparative Study",
        "authors": "Firdaous Khemlichi, Houda Elyousfi Elfilali, Hiba Chougrad, Safae Elhaj Ben Ali, Youness Idrissi Khamlichi",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10253277"
    },
    {
        "id": 32376,
        "title": "Soft Actor-Critic and Risk Assessment-Based Reinforcement Learning Method for Ship Path Planning",
        "authors": "Jue Wang, Bin Ji, Qian Fu",
        "published": "2024-4-12",
        "citations": 0,
        "abstract": "Ship path planning is one of the most important themes in waterway transportation, which is deemed as the cleanest mode of transportation due to its environmentally friendly and energy-efficient nature. A path-planning method that combines the soft actor-critic (SAC) and navigation risk assessment is proposed to address ship path planning in complex water environments. Specifically, a continuous environment model is established based on the Markov decision process (MDP), which considers the characteristics of the ship path-planning problem. To enhance the algorithm’s performance, an information detection strategy for restricted navigation areas is employed to improve state space, converting absolute bearing into relative bearing. Additionally, a risk penalty based on the navigation risk assessment model is introduced to ensure path safety while imposing potential energy rewards regarding navigation distance and turning angle. Finally, experimental results obtained from a navigation simulation environment verify the robustness of the proposed method. The results also demonstrate that the proposed algorithm achieves a smaller path length and sum of turning angles with safety and fuel economy improvement compared with traditional methods such as RRT (rapidly exploring random tree) and DQN (deep Q-network).",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su16083239"
    },
    {
        "id": 32377,
        "title": "Soft Actor-Criticの改良による出力抑制と頑健化",
        "authors": "Taisuke KOBAYASHI",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1299/jsmermd.2023.2a2-e14"
    },
    {
        "id": 32378,
        "title": "Damping Injection Learning for Robots with Actor-Critic Driven by Integral Sliding Manifolds",
        "authors": "Alejandro Tevera-Ruiz, Rodolfo Garcia-Rodriguez, Vicente Parra-Vega",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cce60043.2023.10332851"
    },
    {
        "id": 32379,
        "title": "Application and Evaluation of Soft-Actor Critic Reinforcement Learning in Constrained Trajectory Planning for 6DOF Robotic Manipulators",
        "authors": "Wanqing Xia, Yuqian Lu, Xun Xu",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/m2vip58386.2023.10413400"
    },
    {
        "id": 32380,
        "title": "Bayesian Strategy Networks Based Soft Actor-Critic Learning",
        "authors": "Qin Yang, Ramviyas Parasuraman",
        "published": "2024-6-30",
        "citations": 0,
        "abstract": "A strategy refers to the rules that the agent chooses the available actions to achieve goals. Adopting reasonable strategies is challenging but crucial for an intelligent agent with limited resources working in hazardous, unstructured, and dynamic environments to improve the system’s utility, decrease the overall cost, and increase mission success probability. This article proposes a novel hierarchical strategy decomposition approach based on Bayesian chaining to separate an intricate policy into several simple sub-policies and organize their relationships as Bayesian strategy networks (BSN). We integrate this approach into the state-of-the-art DRL method—soft actor-critic (SAC), and build the corresponding Bayesian soft actor-critic (BSAC) model by organizing several sub-policies as a joint policy. Our method achieves the state-of-the-art performance on the standard continuous control benchmarks in the OpenAI Gym environment. The results demonstrate that the promising potential of the BSAC method significantly improves training efficiency. Furthermore, we extend the topic to the Multi-Agent systems (MAS), discussing the potential research fields and directions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3643862"
    },
    {
        "id": 32381,
        "title": "Mild Policy Evaluation for Offline Actor–Critic",
        "authors": "Longyang Huang, Botao Dong, Jinhui Lu, Weidong Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3309906"
    },
    {
        "id": 32382,
        "title": "Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient",
        "authors": "Maheed H. Ahmed, Abdullah AboHussien, Aly El-Shafei, Ahmed M. Darwish, Ahmed H. Abdel-Gawad",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106593"
    },
    {
        "id": 32383,
        "title": "Optimal synchronized control of nonlinear coupled harmonic oscillators based on actor–critic reinforcement learning",
        "authors": "Zhiyang Gu, Chengli Fan, Dengxiu Yu, Zhen Wang",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11071-023-08957-y"
    },
    {
        "id": 32384,
        "title": "QC_SANE: Robust Control in DRL Using Quantile Critic With Spiking Actor and Normalized Ensemble",
        "authors": "Surbhi Gupta, Gaurav Singal, Deepak Garg, Sarangapani Jagannathan",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3129525"
    },
    {
        "id": 32385,
        "title": "Actor-Critic TD3-based Deep Reinforcement Learning for Energy Management Strategy of HEV",
        "authors": "Ozan Yazar, Serdar Coskun, Lin Li, Fengqi Zhang, Cong Huang",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/hora58378.2023.10156727"
    },
    {
        "id": 32386,
        "title": "Accelerating Fuzzy Actor–Critic Learning via Suboptimal Knowledge for a Multi-Agent Tracking Problem",
        "authors": "Xiao Wang, Zhe Ma, Lei Mao, Kewu Sun, Xuhui Huang, Changchao Fan, Jiake Li",
        "published": "2023-4-13",
        "citations": 3,
        "abstract": "Multi-agent differential games usually include tracking policies and escaping policies. To obtain the proper policies in unknown environments, agents can learn through reinforcement learning. This typically requires a large amount of interaction with the environment, which is time-consuming and inefficient. However, if one can obtain an estimated model based on some prior knowledge, the control policy can be obtained based on suboptimal knowledge. Although there exists an error between the estimated model and the environment, the suboptimal guided policy will avoid unnecessary exploration; thus, the learning process can be significantly accelerated. Facing the problem of tracking policy optimization for multiple pursuers, this study proposed a new form of fuzzy actor–critic learning algorithm based on suboptimal knowledge (SK-FACL). In the SK-FACL, the information about the environment that can be obtained is abstracted as an estimated model, and the suboptimal guided policy is calculated based on the Apollonius circle. The guided policy is combined with the fuzzy actor–critic learning algorithm, improving the learning efficiency. Considering the ground game of two pursuers and one evader, the experimental results verified the advantages of the SK-FACL in reducing tracking error, adapting model error and adapting to sudden changes made by the evader compared with pure knowledge control and the pure fuzzy actor–critic learning algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12081852"
    },
    {
        "id": 32387,
        "title": "Variance-Reduced Deep Actor-Critic with an Optimally Sub-Sampled Actor Recursion",
        "authors": "Lakshmi Mandal, Raghuram Bharadwaj Diddigi, Shalabh Bhatnagar",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2024.3379109"
    },
    {
        "id": 32388,
        "title": "Autonomous Control of Lift System based on Actor-Critic Learning for Air Cushion Vehicle",
        "authors": "Hua Zhou, Yuanhui Wang, Jiyang E, Xiaole Wang",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/oceanslimerick52467.2023.10244666"
    },
    {
        "id": 32389,
        "title": "Actor-Critic Methods for IRS Design in Correlated Channel Environments: A Closer Look Into the Neural Tangent Kernel of the Critic",
        "authors": "Spilios Evmorfos, Athina P. Petropulu, H. Vincent Poor",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsp.2023.3322830"
    },
    {
        "id": 32390,
        "title": "Fixed-Time Sliding Mode Control for Air-Floating Robot Using Actor–Critic Learning Structure",
        "authors": "Weilun Zhang, Li Li, Zhijie Shao, Xingguo Xu, Guangcheng Ma, Hongwei Xia",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iecon51785.2023.10311643"
    },
    {
        "id": 32391,
        "title": "Asymmetric Actor-Critic with Approximate Information State",
        "authors": "Amit Sinha, Aditya Mahajan",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383636"
    },
    {
        "id": 32392,
        "title": "Soft Actor Critic Swing Up of a Real Inverted Pendulum on a Cart",
        "authors": "Raniero Humberto Calderon",
        "published": "2023",
        "citations": 0,
        "abstract": "The inverted pendulum, is a classical experiment widely used as a benchmark for research in control systems, due to its challenging dynamics. In this paper, Deep Reinforcement Learning is used to control a real inverted pendulum on a cart. The Soft Actor Critic algorithm with automatic entropy tuning is used to train an agent capable of acting as a controller. The agent is trained on real data collected on an episodic basis and learns to carry out the swing up control task successfully.",
        "keywords": "",
        "link": "http://dx.doi.org/10.53375/icmame.2023.403"
    },
    {
        "id": 32393,
        "title": "Dynamic Spectrum Sharing Based on Federated Learning and Multi-Agent Actor-Critic Reinforcement Learning",
        "authors": "Tongtong Yang, Wensheng Zhang, Yulian Bo, Jian Sun, Cheng-Xiang Wang",
        "published": "2023-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10182572"
    },
    {
        "id": 32394,
        "title": "An efficient and adaptive design of reinforcement learning environment to solve job shop scheduling problem with soft actor-critic algorithm",
        "authors": "Jinghua Si, Xinyu Li, Liang Gao, Peigen Li",
        "published": "2024-3-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/00207543.2024.2335663"
    },
    {
        "id": 32395,
        "title": "QVDDPG: QV Learning with Balanced Constraint in Actor-Critic Framework",
        "authors": "Jiao Huang, Jifeng Hu, Luheng Yang, Zhihang Ren, Hechang Chen, Bo Yang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191805"
    },
    {
        "id": 32396,
        "title": "A Graph-Based Soft Actor Critic Approach in Multi-Agent Reinforcement Learning",
        "authors": "Wei Pan, Cheng Liu",
        "published": "2023-2-9",
        "citations": 1,
        "abstract": "Multi-Agent Reinforcement Learning (MARL) is widely used to solve various real-world problems. In MARL, the environment contains multiple agents. A good grasp of the environment can guide agents to learn cooperative strategies. In Centralized Training Decentralized Execution (CTDE), a centralized critic is used to guide cooperative strategies learning. However, having multiple agents in the environment leads to the curse of dimensionality and influence of other agents’ strategies, resulting in difficulties for centralized critics to learn good cooperative strategies. We propose a graph-based approach to overcome the above problems. It uses a graph neural network, which uses partial observations of agents as input, and information between agents is aggregated by graph methods to extract information about the whole environment. In this way, agents can improve their understanding of the overall state of the environment and other agents in the environment while avoiding dimensional explosion. Then we combine a dual critic dynamic decomposition method with soft actor-critic to train policy. The former uses individual and global rewards for learning, avoiding the influence of other agents’ strategies, and the latter help to learn an optional policy better. We call this approach Multi-Agent Graph-based soft Actor-Critic (MAGAC). We compare our proposed method with several classical MARL algorithms under the Multi-agent Particle Environment (MPE). The experimental results show that our method can achieve a faster learning speed while learning better policy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15837/ijccc.2023.1.5062"
    },
    {
        "id": 32397,
        "title": "Fleet planning under demand and fuel price uncertainty using actor–critic reinforcement learning",
        "authors": "Izaak L. Geursen, Bruno F. Santos, Neil Yorke-Smith",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jairtraman.2023.102397"
    },
    {
        "id": 32398,
        "title": "Reinforcement learning-based adaptive motion control for autonomous vehicles via actor-critic structure",
        "authors": "Honghai Wang, Liangfen Wei, Xianchao Wang, Shuping He",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/dcdss.2024021"
    },
    {
        "id": 32399,
        "title": "Distillation and Ordinary Federated Learning Actor-Critic Algorithms in Heterogeneous UAV-Aided Networks",
        "authors": "Maedeh Nasr-Azadani, Jamshid Abouei, Konstantinos N. Plataniotis",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3273123"
    },
    {
        "id": 32400,
        "title": "Actor Critic Agents for Wind Farm Control",
        "authors": "Claire Bizon Monroc, Ana Bušić, Donatien Dubuc, Jiamin Zhu",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156453"
    },
    {
        "id": 32401,
        "title": "Experimental Validation of an Actor-Critic Model Predictive Force Controller for Robot-Environment Interaction Tasks",
        "authors": "Alessandro Pozzi, Luca Puricelli, Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio, Francesco Braghin, Loris Roveda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012160700003543"
    },
    {
        "id": 32402,
        "title": "Scheduling single-satellite observation and transmission tasks by using hybrid Actor-Critic reinforcement learning",
        "authors": "Zhijiang Wen, Lu Li, Jiakai Song, Shengyu Zhang, Haiying Hu",
        "published": "2023-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asr.2022.10.024"
    },
    {
        "id": 32403,
        "title": "Enhancing 5G Network Slicing: Slice Isolation Via Actor-Critic Reinforcement Learning with Optimal Graph Features",
        "authors": "Amir Javadpour, Forough Ja'fari, Tarik Taleb, Chafika Benzaïd",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437687"
    },
    {
        "id": 32404,
        "title": "A soft actor-critic reinforcement learning algorithm for network intrusion detection",
        "authors": "Zhengfa Li, Chuanhe Huang, Shuhua Deng, Wanyu Qiu, Xieping Gao",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cose.2023.103502"
    },
    {
        "id": 32405,
        "title": "Learning Open-Loop Saccadic Control of a 3D Biomimetic Eye Using the Actor-Critic Algorithm",
        "authors": "Henrique Granado, Reza Javanmard Alitappeh, Akhil John, A. John Van Opstal, Alexandre Bernardino",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341913"
    },
    {
        "id": 32406,
        "title": "Relative motion guidance for near-rectilinear lunar orbits with path constraints via actor-critic reinforcement learning",
        "authors": "Andrea Scorsoglio, Roberto Furfaro, Richard Linares, Mauro Massari",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asr.2022.08.002"
    },
    {
        "id": 32407,
        "title": "An Actor-Critic Hierarchical Reinforcement Learning Model for Course Recommendation",
        "authors": "Kun Liang, Guoqiang Zhang, Jinhui Guo, Wentao Li",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "Online learning platforms provide diverse course resources, but this often results in the issue of information overload. Learners always want to learn courses that are appropriate for their knowledge level and preferences quickly and accurately. Effective course recommendation plays a key role in helping learners select appropriate courses and improving the efficiency of online learning. However, when a user is enrolled in multiple courses, existing course recommendation methods face the challenge of accurately recommending the target course that is most relevant to the user because of the noise courses. In this paper, we propose a novel reinforcement learning model named Actor-Critic Hierarchical Reinforcement Learning (ACHRL). The model incorporates the actor-critic method to construct the profile reviser. This can remove noise courses and make personalized course recommendations effectively. Furthermore, we propose a policy gradient based on the temporal difference error to reduce the variance in the training process, to speed up the convergence of the model, and to improve the accuracy of the recommendation. We evaluate the proposed model using two real datasets, and the experimental results show that the proposed model significantly outperforms the existing recommendation models (improving 3.77% to 13.66% in terms of HR@5).",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12244939"
    },
    {
        "id": 32408,
        "title": "Quadruped Robot Get Bionic Learning Method Based on Intelligent Memory Soft Actor-Critic",
        "authors": "Chunyang Li, Xiaoqing Zhu, Xiaogang Ruan, Borui Nan, Lanyue Bi, Xiaoyu Zhu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327405"
    },
    {
        "id": 32409,
        "title": "Deep reinforcement learning-based model-free path planning and collision avoidance for UAVs: A soft actor–critic with hindsight experience replay approach",
        "authors": "Myoung Hoon Lee, Jun Moon",
        "published": "2023-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.icte.2022.06.004"
    },
    {
        "id": 32410,
        "title": "SMONAC: Supervised Multiobjective Negative Actor–Critic for Sequential Recommendation",
        "authors": "Fei Zhou, Biao Luo, Zhengke Wu, Tingwen Huang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3317353"
    },
    {
        "id": 32411,
        "title": "An actor-critic algorithm with policy gradients to solve the job shop scheduling problem using deep double recurrent agents",
        "authors": "Marta Monaci, Valerio Agasucci, Giorgio Grani",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejor.2023.07.037"
    },
    {
        "id": 32412,
        "title": "Density estimation based soft actor-critic: deep reinforcement learning for static output feedback control with measurement noise",
        "authors": "Ran Wang, Ye Tian, Kenji Kashima",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01691864.2024.2309621"
    },
    {
        "id": 32413,
        "title": "Integrating short-term stochastic production planning updating with mining fleet management in industrial mining complexes: an actor-critic reinforcement learning approach",
        "authors": "Joao Pedro de Carvalho, Roussos Dimitrakopoulos",
        "published": "2023-10",
        "citations": 2,
        "abstract": "AbstractShort-term production planning in industrial mining complexes involves defining daily, weekly or monthly decisions that aim to achieve production targets established by long-term planning. Operational requirements must be considered when defining fleet allocation and production scheduling decisions. Thus, this paper presents an actor-critic reinforcement learning (RL) method to make mining equipment allocation and production scheduling decisions that maximize the profitability of a mining operation. Two RL agents are proposed. The first agent allocates shovels to mining fronts by considering some operational requirements. The second agent defines the processing destination and the number of trucks required for transportation. A simulator of mining complex operations is proposed to forecast the material flow from the mining fronts to the destinations. This simulator provides new states and rewards to the RL agents, so shovel allocation and production scheduling decisions can be improved. Additionally, as the mining complex operates, sensors collect ore quality data, which are used to update the uncertainty associated with the orebody models. The improvement in material supply characterization allows the RL agents to make more informed decisions. A case study applied at a copper mining complex highlights the method’s ability to make informed decisions while collecting new data. The results show a 47% improvement in cash flow by adapting the shovel and truck allocation and material destination compared to a base case with predefined fleet assignments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-04774-3"
    },
    {
        "id": 32414,
        "title": "Reinforcement learning for automatic quadrilateral mesh generation: A soft actor–critic approach",
        "authors": "Jie Pan, Jingwei Huang, Gengdong Cheng, Yong Zeng",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.10.022"
    },
    {
        "id": 32415,
        "title": "Optimal navigation for AGVs: A soft actor–critic-based reinforcement learning approach with composite auxiliary rewards",
        "authors": "Haisen Guo, Zhigang Ren, Jialun Lai, Zongze Wu, Shengli Xie",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106613"
    },
    {
        "id": 32416,
        "title": "Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control With Action Constraints",
        "authors": "Kazumi Kasaura, Shuwa Miura, Tadashi Kozuno, Ryo Yonetani, Kenta Hoshino, Yohei Hosoe",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3284378"
    },
    {
        "id": 32417,
        "title": "An Improved Actor-Critic Reinforcement Learning with Neural Architecture Search for the Optimal Control Strategy of a Multi-Carrier Energy System",
        "authors": "Amirhossein Dolatabadi, Hussein Abdeltawab, Yasser Abdel-Rady I. Mohamed",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gtd49768.2023.00086"
    },
    {
        "id": 32418,
        "title": "Dual Variable Actor-Critic for Adaptive Safe Reinforcement Learning",
        "authors": "Junseo Lee, Jaeseok Heo, Dohyeong Kim, Gunmin Lee, Songhwai Oh",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341973"
    },
    {
        "id": 32419,
        "title": "Actor-Critic Learning of Variable Damping Injection for Quadrotor Attitude Robust Control",
        "authors": "Alejandro Tevera-Ruiz, Sergio E. Urzúa-Correa, Vicente Parra-Vega, Anand Sanchez-Orta, Rodolfo Garcia-Rodriguez",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cce60043.2023.10332878"
    },
    {
        "id": 32420,
        "title": "Comparing actor-critic deep reinforcement learning controllers for enhanced performance on a ball-and-plate system",
        "authors": "Daniel Udekwe, Ore-ofe Ajayi, Osichinaka Ubadike, Kumater Ter, Emmanuel Okafor",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.123055"
    },
    {
        "id": 32421,
        "title": "A Robust Mean-Field Actor-Critic Reinforcement Learning Against Adversarial Perturbations on Agent States",
        "authors": "Ziyuan Zhou, Guanjun Liu, Mengchu Zhou",
        "published": "2024",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3278715"
    },
    {
        "id": 32422,
        "title": "Joint Recurrent Actor-Critic Model for Partially Observable Control",
        "authors": "Han Xu, Jun Yang, Bin Liang",
        "published": "2023-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icac57885.2023.10275190"
    },
    {
        "id": 32423,
        "title": "Load Frequency Control of Two-area Power System Using an Actor-Critic Reinforcement Learning Method-based Adaptive PID Controller",
        "authors": "Rasananda Muduli, Nikhil Nair, Suraj Kulkarni, Manav Singhal, Debashisha Jena, Tukaram Moger",
        "published": "2023-8-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sefet57834.2023.10245225"
    },
    {
        "id": 32424,
        "title": "Federated Multiagent Actor–Critic Learning Task Offloading in Intelligent Logistics",
        "authors": "Qiqi Li, Yaping Cui, Tao Song, Linjiang Zheng",
        "published": "2023-7-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2023.3244783"
    },
    {
        "id": 32425,
        "title": "FUSION SPARSE AND SHAPING REWARD FUNCTION IN SOFT ACTOR-CRITIC DEEP REINFORCEMENT LEARNING FOR MOBILE ROBOT NAVIGATION",
        "authors": "Mohamad Hafiz Abu Bakar, Abu Ubaidah Shamsudin, Zubair Adil Soomro, Satoshi Tadokoro, C. J. Salaan",
        "published": "2024-1-15",
        "citations": 0,
        "abstract": "Nowadays, the advancement in autonomous robots is the latest influenced by the development of a world surrounded by new technologies. Deep Reinforcement Learning (DRL) allows systems to operate automatically, so the robot will learn the next movement based on the interaction with the environment. Moreover, since robots require continuous action, Soft Actor Critic Deep Reinforcement Learning (SAC DRL) is considered the latest DRL approach solution. SAC is used because its ability to control continuous action to produce more accurate movements. SAC fundamental is robust against unpredictability, but some weaknesses have been identified, particularly in the exploration process for accuracy learning with faster maturity. To address this issue, the study identified a solution using a reward function appropriate for the system to guide in the learning process. This research proposes several types of reward functions based on sparse and shaping reward in SAC method to investigate the effectiveness of mobile robot learning. Finally, the experiment shows that using fusion sparse and shaping rewards in the SAC DRL successfully navigates to the target position and can also increase accuracy based on the average error result of 4.99%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11113/jurnalteknologi.v86.20147"
    },
    {
        "id": 32426,
        "title": "A double Actor-Critic learning system embedding improved Monte Carlo tree search",
        "authors": "Hongjun Zhu, Yong Xie, Suijun Zheng",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09513-4"
    },
    {
        "id": 32427,
        "title": "Test Point Selection Using Deep Graph Convolutional Networks and Advantage Actor Critic (A2C) Reinforcement Learning",
        "authors": "Shaoqi Wei, Kohei Shiotani, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Hiroshi Takahashi, Gang Wang",
        "published": "2023-6-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itc-cscc58803.2023.10212888"
    },
    {
        "id": 32428,
        "title": "Actor Critic-based Multi Objective Reinforcement Learning for Multi Access Edge Computing",
        "authors": "Vishal Khot, Vallisha M, Sharan S Pai, Chandra Shekar R K, Kayarvizhy N",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.0150241"
    },
    {
        "id": 32429,
        "title": "Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines",
        "authors": "Michael Schenk, Elías F Combarro, Michele Grossi, Verena Kain, Kevin Shing Bruce Li, Mircea-Marian Popa, Sofia Vallecorsa",
        "published": "2024-4-1",
        "citations": 1,
        "abstract": "Abstract\nFree energy-based reinforcement learning (FERL) with clamped quantum Boltzmann machines (QBM) was shown to significantly improve the learning efficiency compared to classical Q-learning with the restriction, however, to discrete state-action space environments. In this paper, the FERL approach is extended to multi-dimensional continuous state-action space environments to open the doors for a broader range of real-world applications. First, free energy-based Q-learning is studied for discrete action spaces, but continuous state spaces and the impact of experience replay on sample efficiency is assessed. In a second step, a hybrid actor-critic (A-C) scheme for continuous state-action spaces is developed based on the deep deterministic policy gradient algorithm combining a classical actor network with a QBM-based critic. The results obtained with quantum annealing (QA), both simulated and with D-Wave QA hardware, are discussed, and the performance is compared to classical reinforcement learning methods. The environments used throughout represent existing particle accelerator beam lines at the European Organisation for Nuclear Research. Among others, the hybrid A-C agent is evaluated on the actual electron beam line of the Advanced Wakefield Experiment (AWAKE).",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/2058-9565/ad261b"
    },
    {
        "id": 32430,
        "title": "FastAct: A Lightweight Actor Compression Framework for Fast Policy Learning",
        "authors": "Hongjie Zhang, Haoming Ma, Zhenyu Chen",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191108"
    },
    {
        "id": 32431,
        "title": "A Strategy-Oriented Bayesian Soft Actor-Critic Model",
        "authors": "Qin Yang, Ramviyas Parasuraman",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.03.071"
    },
    {
        "id": 32432,
        "title": "Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383350"
    },
    {
        "id": 32433,
        "title": "A Multi-Agent Actor-Critic Based Approach Applied to the Snake Game",
        "authors": "Di Yuan, Zirui Luo, Xinyi Yao, Jing Xue",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241182"
    },
    {
        "id": 32434,
        "title": "Deep Deterministic Policy Gradient With Compatible Critic Network",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023-8",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3117790"
    },
    {
        "id": 32435,
        "title": "Task Offloading Optimization Based on Actor-Critic Algorithm in Vehicle Edge Computing",
        "authors": "Bingxin Wang, Lei Liu, Jie Wang",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10183035"
    },
    {
        "id": 32436,
        "title": "Design of Observer-Based Control With Residual Generator Using Actor–Critic Reinforcement Learning",
        "authors": "Lu Qian, Xingwei Zhao, Peifeng Liu, Zhenwei Zhang, Yaqiong Lv",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3215671"
    },
    {
        "id": 32437,
        "title": "Actor–critic learning-based adaptive fuzzy controller for stepper motor",
        "authors": "R Ganesh Babu, C Chellaswamy, T S Geetha",
        "published": "2023-11",
        "citations": 0,
        "abstract": " This paper presents a combination of both the Takagi–Sugeno–Kang fuzzy controller and the advantage of reinforcement learning for the reduction in the effect of external disturbances and system uncertainties. A neural network is used for the implementation of a critic network; the parameters are updated using the Lyapunov criteria for avoiding local minima problems. The fewer learning parameters used helps online tuning of the controller by the critic network based on the reward function. MATLAB/Simulink is used for simulation of the proposed system under different conditions in the investigation of the performance of the proposed controller. The actor parameters are updated based on the change in the reward function error and a control function. Five different conditions, namely, no-load condition, sudden load-torque changes, system parameter uncertainty, sudden phase interruption, and the impact of noise have been studied. The proposed method was found to meet the high-speed response with the tracking error of ±2.23% for the tracking reference trajectory and tracking error of ±2.5% under constant load-torque disturbance. The test results were compared with two benchmark controllers for the verification of the effectiveness of the proposed controller. Simulation results showed the proposed method providing an adaptive and precise speed response. Therefore, it is suitable for non-linear and uncertain applications. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/01423312231156970"
    },
    {
        "id": 32438,
        "title": "Adaptive actor-critic learning-based robust appointed-time attitude tracking control for uncertain rigid spacecrafts with performance and input constraints",
        "authors": "Zhi-Gang Zhou, Di Zhou, Xinwei Chen, Xiao-Ning Shi",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asr.2022.04.061"
    },
    {
        "id": 32439,
        "title": "Development of Advanced Control Strategy Based on Soft Actor-Critic Algorithm",
        "authors": "Michal Hlavatý, Alena Kozáková",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/comsci59259.2023.10315824"
    },
    {
        "id": 32440,
        "title": "Soft Actor-Critic-Based Service Migration in Multiuser MEC Systems",
        "authors": "Xinyu Zhang, Shuang Ren",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiecc60864.2023.10456675"
    },
    {
        "id": 32441,
        "title": "Health-considered energy management strategy for fuel cell hybrid electric vehicle based on improved soft actor critic algorithm adopted with Beta policy",
        "authors": "Weiqi Chen, Jiankun Peng, Jun Chen, Jiaxuan Zhou, Zhongbao Wei, Chunye Ma",
        "published": "2023-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enconman.2023.117362"
    },
    {
        "id": 32442,
        "title": "Time-Constrained Actor-Critic Reinforcement Learning for Concurrent Order Dispatch in On-demand Delivery",
        "authors": "Shuai Wang, Baoshen Guo, Yi Ding, Guang Wang, Suining He, Desheng Zhang, Tian He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmc.2023.3342815"
    },
    {
        "id": 32443,
        "title": "Design of Safe Optimal Guidance With Obstacle Avoidance Using Control Barrier Function-Based Actor–Critic Reinforcement Learning",
        "authors": "Chi Peng, Xiaoma Liu, Jianjun Ma",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2023.3288826"
    },
    {
        "id": 32444,
        "title": "Hierarchical Multiagent Formation Control Scheme via Actor-Critic Learning",
        "authors": "Chaoxu Mu, Jiangwen Peng, Changyin Sun",
        "published": "2023-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3153028"
    },
    {
        "id": 32445,
        "title": "Equivariant Graph-Representation-Based Actor–Critic Reinforcement Learning for Nanoparticle Design",
        "authors": "Jonas Elsborg, Arghya Bhowmik",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acs.jcim.3c00394"
    },
    {
        "id": 32446,
        "title": "Actor–Critic or Critic–Actor? A Tale of Two Time Scales",
        "authors": "Shalabh Bhatnagar, Vivek S. Borkar, Soumyajit Guin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcsys.2023.3288931"
    },
    {
        "id": 32447,
        "title": "SEGAC: Sample Efficient Generalized Actor Critic for the Stochastic On-Time Arrival Problem",
        "authors": "Hongliang Guo, Zhi He, Wenda Sheng, Zhiguang Cao, Yingjie Zhou, Weinan Gao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tits.2024.3361445"
    },
    {
        "id": 32448,
        "title": "An Actor-Critic Framework for Online Control With Environment Stability Guarantee",
        "authors": "Pavel Osinenko, Grigory Yaremenko, Georgiy Malaniya, Anton Bolychev",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3306070"
    },
    {
        "id": 32449,
        "title": "Enhancing Robustness in Multi-Agent Actor-Critic with Double Actors",
        "authors": "Xue Han, Peng Cui, Ya Zhang",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/yac59482.2023.10401809"
    },
    {
        "id": 32450,
        "title": "Actor-critic reinforcement learning leads decision-making in energy systems optimization—steam injection optimization",
        "authors": "Ramez Abdalla, Wolfgang Hollstein, Carlos Paz Carvajal, Philip Jaeger",
        "published": "2023-8",
        "citations": 1,
        "abstract": "AbstractSteam injection is a popular technique to enhance oil recovery in mature oil fields. However, the conventional approach of using a constant steam rate over an extended period can lead to sub-optimal performance due to the complex nature of the problem and reservoir heterogeneity. To address this issue, the Markov decision process can be employed to formulate the problem for reinforcement learning (RL) applications. The RL agent is trained to optimize the steam injection rate by interacting with a reservoir simulation model and receives rewards for each action. The agent’s policy and value functions are updated through continuous interaction with the environment until convergence is achieved, leading to a more efficient steam injection strategy for enhancing oil recovery. In this study, an actor-critic RL architecture was employed to train the agent to find the optimal strategy (i.e., policy). The environment was represented by a reservoir simulation model, and the agent’s actions were based on the observed state. The policy function gave a probability distribution of the actions that the agent could take, while the value function determined the expected yield for an agent starting from a given state. The agent interacted with the environment for several episodes until convergence was achieved. The improvement in net present value (NPV) achieved by the agent was a significant indication of the effectiveness of the RL-based approach. The NPV reflects the economic benefits of the optimized steam injection strategy. The agent was able to achieve this improvement by finding the optimal policies. One of the key advantages of the optimal policy was the decrease in total field heat losses. This is a critical factor in the efficiency of the steam injection process. Heat loss can reduce the efficiency of the process and lead to lower oil recovery rates. By minimizing heat loss, the agent was able to optimize the steam injection process and increase oil recovery rates. The optimal policy had four regions characterized by slight changes in a stable injection rate to increase the average reservoir pressure, increasing the injection rate to a maximum value, steeply decreasing the injection rate, and slightly changing the injection rate to maintain the average reservoir temperature. These regions reflect the different phases of the steam injection process and demonstrate the complexity of the problem. Overall, the results of this study demonstrate the effectiveness of RL in optimizing steam injection in mature oil fields. The use of RL can help address the complexity of the problem and improve the efficiency of the oil recovery process. This study provides a framework for future research in this area and highlights the potential of RL for addressing other complex problems in the energy industry.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08537-6"
    },
    {
        "id": 32451,
        "title": "Actor-Critic Based DRL Algorithm for Task Offloading Performance Optimization in Vehicle Edge Computing",
        "authors": "Bingxin Wang, Lei Liu, Jie Wang",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10183228"
    },
    {
        "id": 32452,
        "title": "Offline-Online Actor-Critic for Partially Observable Markov Decision Process",
        "authors": "Wang Xuesong, Hou Diyuan, Cheng Yuhu",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csis-iac60628.2023.10364052"
    },
    {
        "id": 32453,
        "title": "Lexicographic Actor-Critic Deep Reinforcement Learning for Urban Autonomous Driving",
        "authors": "Hengrui Zhang, Youfang Lin, Sheng Han, Kai Lv",
        "published": "2023-4",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2022.3226579"
    },
    {
        "id": 32454,
        "title": "Energy management strategy for fuel cell vehicles via soft actor-critic-based deep reinforcement learning considering powertrain thermal and durability characteristics",
        "authors": "Yuanzhi Zhang, Caizhi Zhang, Ruijia Fan, Chenghao Deng, Song Wan, Hicham Chaoui",
        "published": "2023-5",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enconman.2023.116921"
    },
    {
        "id": 32455,
        "title": "HMAAC: Hierarchical Multi-Agent Actor-Critic for Aerial Search with Explicit Coordination Modeling",
        "authors": "Chuanneng Sun, Songjun Huang, Dario Pompili",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161019"
    },
    {
        "id": 32456,
        "title": "Actor-Critic Based Back-off Algorithm for Massive Machine-Type Communication",
        "authors": "Xin Gao, Zhihong Qian, Mingtong Xie, Xue Wang",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283695"
    },
    {
        "id": 32457,
        "title": "Robust Reward-Free Actor–Critic for Cooperative Multiagent Reinforcement Learning",
        "authors": "Qifeng Lin, Qing Ling",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3302131"
    },
    {
        "id": 32458,
        "title": "Improved 1vs1 Air Combat Model With Self-Play Soft Actor-Critic and Sparse Rewards",
        "authors": "HoSeong Jung, Yong-Duk Kim",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316918"
    },
    {
        "id": 32459,
        "title": "Cache allocation Algorithm of 5G Core Network Slicing Based on Soft Migration Actor Critic*",
        "authors": "Chenglin Xu, Guohui Zhu, Qianwen Yang",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnlp58431.2023.00070"
    },
    {
        "id": 32460,
        "title": "Soft Actor-Critic-Based Grid Dispatching with Distributed Training",
        "authors": "Long Zhao, Fei Li, Yuan Zhou, Wenbin Fan",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/miccis58901.2023.00007"
    },
    {
        "id": 32461,
        "title": "Hardware-in-the-Loop Soft Robotic Testing Framework Using an Actor-Critic Deep Reinforcement Learning Algorithm",
        "authors": "Jesus Marquez, Charles Sullivan, Ryan M. Price, Robert C. Roberts",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3301215"
    },
    {
        "id": 32462,
        "title": "Learning Locomotion for Quadruped Robots via Distributional Ensemble Actor-Critic",
        "authors": "Sicen Li, Yiming Pang, Panju Bai, Jiawei Li, Zhaojin Liu, Shihao Hu, Liquan Wang, Gang Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2024.3349934"
    },
    {
        "id": 32463,
        "title": "Multi Actor-Critic PPO: A Novel Reinforcement Learning Method for Intelligent Task and Charging Scheduling in Electric Freight Vehicles Management",
        "authors": "Donghe Li, Chunlin Hu, Qingyu Yang, Shitao Chen",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421834"
    },
    {
        "id": 32464,
        "title": "EarlGAN: An enhanced actor–critic reinforcement learning agent-driven GAN for de novo drug design",
        "authors": "Huidong Tang, Chen Li, Shuai Jiang, Huachong Yu, Sayaka Kamei, Yoshihiro Yamanishi, Yasuhiko Morimoto",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.10.001"
    },
    {
        "id": 32465,
        "title": "Locating algorithm of steel stock area with asynchronous advantage actor-critic reinforcement learning",
        "authors": "Young-in Cho, Byeongseop Kim, Hee-Chang Yoon, Jong Hun Woo",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "Abstract\nIn the steel stockyard of the shipyard, the sorting work to relocate the steel plates already stacked to retrieve the target steel plate on the fabrication schedule is labor-consuming work requiring the operation of overhead cranes. To reduce the sorting work, there is a need for a method of stacking the steel plates in order of fabrication schedules when the steel plates arrive at the shipyard from the steel-making companies. However, the conventional optimization algorithm and heuristics have limitations in determining the optimal stacking location of steel plates because the real-world stacking problems in shipyards have vast solution space in addition to the uncertainty in the arrival order of steel plates. In this study, reinforcement learning is applied to the development of a real-time stacking algorithm for steel plates considering the fabrication schedule. Markov decision process suitable for the stacking problem is defined, and the optimal stacking policy is learned using an asynchronous advantage actor-critic algorithm. The learned policy is tested on several problems by varying the number of steel plates. The test results indicate that the proposed method is effective for minimizing the use of cranes compared with other metaheuristics and heuristics for stacking problems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/jcde/qwae002"
    },
    {
        "id": 32466,
        "title": "Multi-Microgrid Collaborative Optimization Scheduling Using an Improved Multi-Agent Soft Actor-Critic Algorithm",
        "authors": "Jiankai Gao, Yang Li, Bin Wang, Haibo Wu",
        "published": "2023-4-5",
        "citations": 3,
        "abstract": "The implementation of a multi-microgrid (MMG) system with multiple renewable energy sources enables the facilitation of electricity trading. To tackle the energy management problem of an MMG system, which consists of multiple renewable energy microgrids belonging to different operating entities, this paper proposes an MMG collaborative optimization scheduling model based on a multi-agent centralized training distributed execution framework. To enhance the generalization ability of dealing with various uncertainties, we also propose an improved multi-agent soft actor-critic (MASAC) algorithm, which facilitates energy transactions between multi-agents in MMG, and employs automated machine learning (AutoML) to optimize the MASAC hyperparameters to further improve the generalization of deep reinforcement learning (DRL). The test results demonstrate that the proposed method successfully achieves power complementarity between different entities and reduces the MMG system’s operating cost. Additionally, the proposal significantly outperforms other state-of-the-art reinforcement learning algorithms with better economy and higher calculation efficiency.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16073248"
    },
    {
        "id": 32467,
        "title": "A novel actor–critic–identifier architecture for nonlinear multiagent systems with gradient descent method",
        "authors": "Zhongyang Ming, Huaguang Zhang, Juan Zhang, Xiangpeng Xie",
        "published": "2023-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.automatica.2023.111128"
    },
    {
        "id": 32468,
        "title": "Multi-Timescale Actor-Critic Learning for Computing Resource Management With Semi-Markov Renewal Process Mobility",
        "authors": "Tan Le, Martin Reisslein, Sachin Shetty",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tits.2023.3303953"
    },
    {
        "id": 32469,
        "title": "Hierarchical Sliding-Mode Surface-Based Adaptive Actor–Critic Optimal Control for Switched Nonlinear Systems With Unknown Perturbation",
        "authors": "Haoyan Zhang, Xudong Zhao, Huanqing Wang, Guangdeng Zong, Ning Xu",
        "published": "2024-2",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3183991"
    },
    {
        "id": 32470,
        "title": "Classical Actor-Critic Applied to the Control of a Self – Regulatory Process",
        "authors": "E.H. Bras, T.M. Louw, S.M. Bradshaw",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.598"
    },
    {
        "id": 32471,
        "title": "A novel trajectory tracking algorithm based on soft actor-critic-ResNet model",
        "authors": "Haohua Li, Jie Qi",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3011759"
    },
    {
        "id": 32472,
        "title": "Actor Critic Approach based Anomaly Detection for Edge Computing Environments",
        "authors": "Shruthi N, Siddesh G K",
        "published": "2023-1-30",
        "citations": 0,
        "abstract": "The pivotal role of data security in mobile edge-computing environments forms the foundation for the proposed work. Anomalies and outliers in the sensory data due to network attacks will be a prominent concern in real time. Sensor samples will be considered from a set of sensors at a particular time instant as far as the confidence level on the decision remains on par with the desired value. A “true” on the hypothesis test eventually means that the sensor has shown signs of anomaly or abnormality and samples have to be immediately ceased from being retrieved from the sensor. A deep learning Actor-Criticbased Reinforcement algorithm proposed will be able to detect anomalies in the form of binary indicators and hence decide when to withdraw from receiving further samples from specific sensors. The posterior trust value influences the value of the confidence interval and hence the probability of anomaly detection. The paper exercises a single-tailed normal function to determine the range of the posterior trust metric. The decision taken by the prediction model will be able to detect anomalies with a good percentage of anomaly detection accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijcnc.2023.15104"
    },
    {
        "id": 32473,
        "title": "Soft Actor-Critic Based Power Control Algorithm for Anti-jamming in D2D Communication",
        "authors": "Keyu Jia, Dianjun Chen, Xuebin Sun",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccect57938.2023.10140869"
    },
    {
        "id": 32474,
        "title": "Multi-Agent Attention Actor-Critic Algorithm for Load Balancing in Cellular Networks",
        "authors": "Jikun Kang, Di Wu, Ju Wang, Ekram Hossain, Xue Liu, Gregory Dedek",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279452"
    },
    {
        "id": 32475,
        "title": "Improving Multiagent Actor-Critic Architectures, with Opponent Approximation and Dropout for Control",
        "authors": "Gabor Paczolay, Istvan Harmati",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12700/aph.21.4.2024.4.13"
    },
    {
        "id": 32476,
        "title": "Reinforcement Learning of an Interpretable Fuzzy System through a Neural Fuzzy Actor-Critic Framework for Mobile Robot Control",
        "authors": "Chia-Feng Juang, Zhoa-Boa You",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tfuzz.2024.3380824"
    },
    {
        "id": 32477,
        "title": "Adaptive Control for Virtual Synchronous Generator Parameters Based on Soft Actor Critic",
        "authors": "Chuang Lu, Xiangtao Zhuan",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "This paper introduces a model-free optimization method based on reinforcement learning (RL) aimed at resolving the issues of active power and frequency oscillations present in a traditional virtual synchronous generator (VSG). The RL agent utilizes the active power and frequency response of the VSG as state information inputs and generates actions to adjust the virtual inertia and damping coefficients for an optimal response. Distinctively, this study incorporates a setting-time term into the reward function design, alongside power and frequency deviations, to avoid prolonged system transients due to over-optimization. The soft actor critic (SAC) algorithm is utilized to determine the optimal strategy. SAC, being model-free with fast convergence, avoids policy overestimation bias, thus achieving superior convergence results. Finally, the proposed method is validated through MATLAB/Simulink simulation. Compared to other approaches, this method more effectively suppresses oscillations in active power and frequency and significantly reduces the setting time.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s24072035"
    },
    {
        "id": 32478,
        "title": "A Joint Distribution System State Estimation Framework via Deep Actor-Critic Learning Method",
        "authors": "Yuxuan Yuan, Kaveh Dehghanpour, Zhaoyu Wang, Fankun Bu",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpwrs.2022.3155649"
    },
    {
        "id": 32479,
        "title": "Fusing Vehicle Trajectories and GNSS Measurements to Improve GNSS Positioning Correction Based on Actor-Critic Learning",
        "authors": "Haoli Zhao, Zhenni Li, Ci Chen, Lujia Wang, Kan Xie, Shengli Xie",
        "published": "2023-2-13",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33012/2023.18593"
    },
    {
        "id": 32480,
        "title": "Distributed Actor–Critic Algorithms for Multiagent Reinforcement Learning Over Directed Graphs",
        "authors": "Pengcheng Dai, Wenwu Yu, He Wang, Simone Baldi",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3139138"
    },
    {
        "id": 32481,
        "title": "Adaptive Optimal Tracking Control of an Underactuated Surface Vessel Using Actor–Critic Reinforcement Learning",
        "authors": "Lin Chen, Shi-Lu Dai, Chao Dong",
        "published": "2024",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3214681"
    },
    {
        "id": 32482,
        "title": "ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation",
        "authors": "Xinpeng Wang, Barbara Plank",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.126"
    },
    {
        "id": 32483,
        "title": "Computation Offloading Service in UAV-Assisted Mobile Edge Computing: A Soft Actor-Critic Approach",
        "authors": "You Zhang, Zhengchong Mao",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ucom59132.2023.10257660"
    },
    {
        "id": 32484,
        "title": "Designing a Biped Robot's Gait using Reinforcement Learning's -Actor Critic Method",
        "authors": "J.Dafni Rose, Jenitha Mary.L, Selvakumaran. S, Mohanaprakash T A, M. KrishnaRaj, S. Diviyasri",
        "published": "2023-4-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icict57646.2023.10134079"
    },
    {
        "id": 32485,
        "title": "Belief State Actor-Critic Algorithm from Separation Principle for POMDP",
        "authors": "Yujie Yang, Yuxuan Jiang, Jianyu Chen, Shengbo Eben Li, Ziqing Gu, Yuming Yin, Qian Zhang, Kai Yu",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155792"
    },
    {
        "id": 32486,
        "title": "Resource Allocation for Cognitive Radio Inspired Non-Orthogonal Multiple Access Networks: A Quantum Soft Actor-Critic Method",
        "authors": "Chao Huang, Ying He, Fei Yu, Peigen Zeng",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437345"
    },
    {
        "id": 32487,
        "title": "Control Strategy for DC to DC Converter using MLP embedded Actor-Critic Architecture",
        "authors": "Soubhagya Ranjan Prusty, Sudhakar Sahu, Sarita Nanda, Lipika Nanda, Subrat Behera",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170581"
    },
    {
        "id": 32488,
        "title": "Policy Optimization of the Power Allocation Algorithm Based on the Actor–Critic Framework in Small Cell Networks",
        "authors": "Haibo Chen, Zhongwei Huang, Xiaorong Zhao, Xiao Liu, Youjun Jiang, Pinyong Geng, Guang Yang, Yewen Cao, Deqiang Wang",
        "published": "2023-4-2",
        "citations": 1,
        "abstract": "A practical solution to the power allocation problem in ultra-dense small cell networks can be achieved by using deep reinforcement learning (DRL) methods. Unlike traditional algorithms, DRL methods are capable of achieving low latency and operating without the need for global real-time channel state information (CSI). Based on the actor–critic framework, we propose a policy optimization of the power allocation algorithm (POPA) for small cell networks in this paper. The POPA adopts the proximal policy optimization (PPO) algorithm to update the policy, which has been shown to have stable exploration and convergence effects in our simulations. Thanks to our proposed actor–critic architecture with distributed execution and centralized exploration training, the POPA can meet real-time requirements and has multi-dimensional scalability. Through simulations, we demonstrate that the POPA outperforms existing methods in terms of spectral efficiency. Our findings suggest that the POPA can be of practical value for power allocation in small cell networks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11071702"
    },
    {
        "id": 32489,
        "title": "An Actor-Critic Architecture for Community Detection Ablation Study",
        "authors": "Rafael Henrique Nogalha De Lima, Aurélio Ribeiro Costa, Thiago De Paulo Faleiros, Célia Ghedini Ralha",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371808"
    },
    {
        "id": 32490,
        "title": "Improving real-time energy decision-making model with an actor-critic agent in modern microgrids with energy storage devices",
        "authors": "Karim Bio Gassi, Mustafa Baysal",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2022.126105"
    },
    {
        "id": 32491,
        "title": "Actor-Critic-Based Optimal Tracking Control of Modular Reconfigurable Robot System Using Non-Zero-Sum Game",
        "authors": "Zhendong Ding, Tianjiao An, Bing Ma, Jingkai Liang, Bo Dong",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240992"
    },
    {
        "id": 32492,
        "title": "Soft-Actor-Attention-Critic Based on Unknown Agent Action Prediction for Multi-Agent Collaborative Confrontation",
        "authors": "Ziwei Liu, Changzhen Qiu, Zhiyong Zhang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105797"
    },
    {
        "id": 32493,
        "title": "UAV Assisted Cooperative Caching on Network Edge Using Multi-Agent Actor-Critic Reinforcement Learning",
        "authors": "Sadman Araf, Adittya Soukarjya Saha, Sadia Hamid Kazi, Nguyen H. Tran, Md. Golam Rabiul Alam",
        "published": "2023-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2022.3209079"
    },
    {
        "id": 32494,
        "title": "Receding Horizon Actor–Critic Learning Control for Nonlinear Time-Delay Systems With Unknown Dynamics",
        "authors": "Jiahang Liu, Xinglong Zhang, Xin Xu, Quan Xiong",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2023.3254911"
    },
    {
        "id": 32495,
        "title": "CAC: Enabling Customer-Centered Passenger-Seeking for Self-Driving Ride Service with Conservative Actor-Critic",
        "authors": "Palawat Busaranuvong, Xin Zhang, Yanhua Li, Xun Zhou, Jun Luo",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00011"
    },
    {
        "id": 32496,
        "title": "Manoeuvre decision‐making of unmanned aerial vehicles in air combat based on an expert actor‐based soft actor critic algorithm",
        "authors": "Bo Li, Shuangxia Bai, Shiyang Liang, Rui Ma, Evgeny Neretin, Jingyi Huang",
        "published": "2023-12",
        "citations": 5,
        "abstract": "AbstractThe demand for autonomous motion control of unmanned aerial vehicles in air combat is boosted as taking the initiative in combat appears more and more crucial. Unmanned aerial vehicles inability to manoeuvre autonomously during air combat that features highly dynamic and uncertain manoeuvres of the enemy; however, limits their combat capabilities, which proves to be very challenging. To meet the challenge, this article proposes an autonomous manoeuvre decision model using an expert actor‐based soft actor critic algorithm that reconstructs empirical replay buffer with expert experience. Specifically, the algorithm uses a small amount of expert experience to increase the diversity of the samples, which can largely improve the exploration and utilisation efficiency of deep reinforcement learning. And to simulate the complex battlefield environment, a one‐to‐one air combat model is established and the concept of missile's attack region is introduced. The model enables the one‐to‐one air combat to be simulated under different initial battlefield situations. Simulation results show that the expert actor‐based soft actor critic algorithm can find the most favourable policy for unmanned aerial vehicles to defeat the opponent faster, and converge more quickly, compared with the soft actor critic algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/cit2.12195"
    },
    {
        "id": 32497,
        "title": "A Small Gain Analysis of Single Timescale Actor Critic",
        "authors": "Alex Olshevsky, Bahman Gharesifard",
        "published": "2023-4-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1137/22m1483335"
    },
    {
        "id": 32498,
        "title": "Graph Soft Actor–Critic Reinforcement Learning for Large-Scale Distributed Multirobot Coordination",
        "authors": "Yifan Hu, Junjie Fu, Guanghui Wen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3329530"
    },
    {
        "id": 32499,
        "title": "An improved Soft Actor-Critic strategy for optimal energy management",
        "authors": "Bruno Boato, Carolina Saavedra Sueldo, Luis Avila, Mariano De Paula",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tla.2023.10251801"
    },
    {
        "id": 32500,
        "title": "Approaches That Use Domain-Specific Expertise: Behavioral-Cloning-Based Advantage Actor-Critic in Basketball Games",
        "authors": "Taehyeok Choi, Kyungeun Cho, Yunsick Sung",
        "published": "2023-2-22",
        "citations": 3,
        "abstract": "Research on the application of artificial intelligence (AI) in games has recently gained momentum. Most commercial games still use AI based on a finite state machine (FSM) due to complexity and cost considerations. However, FSM-based AI decreases user satisfaction given that it performs the same patterns of consecutive actions in the same situations. This necessitates a new AI approach that applies domain-specific expertise to existing reinforcement learning algorithms. We propose a behavioral-cloning-based advantage actor-critic (A2C) that improves learning performance by applying a behavioral cloning algorithm to an A2C algorithm in basketball games. The state normalization, reward function, and episode classification approaches are used with the behavioral-cloning-based A2C. The results of the comparative experiments with the traditional A2C algorithms validated the proposed method. Our proposed method using existing approaches solved the difficulty of learning in basketball games.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11051110"
    },
    {
        "id": 32501,
        "title": "Local demand management of charging stations using vehicle-to-vehicle service: A welfare maximization-based soft actor-critic model",
        "authors": "Akhtar Hussain, Van-Hai Bui, Petr Musilek",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.etran.2023.100280"
    },
    {
        "id": 32502,
        "title": "Variational actor-critic algorithms,",
        "authors": "Yuhua Zhu, Lexing Ying",
        "published": "2023",
        "citations": 0,
        "abstract": "We introduce a class of variational actor-critic algorithms based on a variational formulation over both the value function and the policy. The objective function of the variational formulation consists of two parts: one for maximizing the value function and the other for minimizing the Bellman residual. Besides the vanilla gradient descent with both the value function and the policy updates, we propose two variants, the clipping method and the flipping method, in order to speed up the convergence. We also prove that, when the prefactor of the Bellman residual is sufficiently large, the fixed point of the algorithm is close to the optimal policy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/cocv/2023007"
    },
    {
        "id": 32503,
        "title": "Distributed Actor-Critic Approach for Frequency Synchronization of Isolated AC Microgrids",
        "authors": "Shih-Wen Lin, Chien-Feng Tung, Chia-Chi Chu",
        "published": "2023-5-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icps57144.2023.10142098"
    },
    {
        "id": 32504,
        "title": "Intelligent Energy Management for Fuel Cell Bus Based on Enhanced Soft Actor-Critic Algorithm",
        "authors": "Ruchen Huang, Zegong Niu, Qicong Su, Hongwen He, Zheng Zhou, Zhiqiang Zhou",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/vppc60535.2023.10403274"
    }
]