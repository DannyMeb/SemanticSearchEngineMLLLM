[
    {
        "id": 13171,
        "title": "GANs for Image Generation",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8_2"
    },
    {
        "id": 13172,
        "title": "Image generation using generative adversarial networks",
        "authors": "Omkar Metri, H.R Mamatha",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-823519-5.00007-5"
    },
    {
        "id": 13173,
        "title": "Pose image generation for video content creation using controlled human pose image generation GAN",
        "authors": "Lalit Kumar, Dushyant Kumar Singh",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-17856-8"
    },
    {
        "id": 13174,
        "title": "Symmetric Generation for Homomorphic Image",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.52783/jas.v9i1.1443"
    },
    {
        "id": 13175,
        "title": "Reparameterizing and Dynamically Quantizing Image Features for Image Generation",
        "authors": "Mingzhen Sun, Weining Wang, Xinxin Zhu, Jing Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4412855"
    },
    {
        "id": 13176,
        "title": "Multimodal Image-to-Image Translation for Generation of Gastritis Images",
        "authors": "Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2020-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190863"
    },
    {
        "id": 13177,
        "title": "Arabic Text to Image Generation Tasks",
        "authors": "Mourad BAHANI",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCurrent AI systems have shown impressive results in the Automatic synthesis realistic images from texts descriptions tasks. In fact, Generative Adversarial Networks (GANs) are used mostly in this tasks. The Generator generates realistic images given noise and sentence vectors, and the discriminator produce a probability of how the synthetic images are reals. In this paper, in order to generate images from Arabic text, we fuse DF-GAN as a sample and efficient text-to-image generation framework and AraBERT architecture. To achieve this purpose, firstly, we re-create new datasets matching the Arabic text-to-image generation task by applying DeepL-Translator from English to Arabic on texts descriptions of original datasets. Secondly, we leverage the power of AraBERT which is trained on billions of Arabic words to produce a strong sentence embedding, and we reduce that vector's dimension to match with DF-GAN shape. Thirdly, we inject the reduced sentence embedding into the UPBlocks sections of DF-GAN and we train the proposed architecture on two challenging datasets. Following the previous works, we use CUB and Oxford-102 flowers as original datasets. Further, we measure our framework with FID and IS. Our framework is the first that achieve much success in generating high-resolution realistic and text matching images conditioned with Arabic text.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2163664/v1"
    },
    {
        "id": 13178,
        "title": "Semantic Image Synthesis for Realistic Image Generation in Robotic Assisted Partial Nephrectomy",
        "authors": "Stefano Mazzocchetti, Laura Cercenelli, Lorenzo Bianchi, Riccardo Schiavina, Emanuela Marcelli",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012611200003660"
    },
    {
        "id": 13179,
        "title": "Pseudo Computed Tomography Image Generation from Brain Magnetic Resonance Image for Radiation Therapy Treatment Planning Using DCNN-UNET",
        "authors": "Sreeja S., Muhammad Noorul Mubarak D.",
        "published": "2021-10-30",
        "citations": 0,
        "abstract": "Relative to Computed Tomography (CT), the increased soft tissue contrasts of magnetic resonance imaging (MRI) makes it a suitable imaging method to decide radiation therapy (RT). When MRI scans are used for therapy planning, a CT scan is still required for dosage calculation and x-ray-based patient placement. This raises workload, leads to incertitude owing to the requisite of image registration inter-modality and requires needless irradiation. Even though it would be advantageous to only use MR images, a way of estimating a pseudo-CT (pCT) must be used to generate electron density mapping and patient reference imagery. So, this paper brings an effective deep learning model to generate synthesized CT from MRI images using the following steps; a) data acquisition where CT and MRI scan images are collected, b) preprocessing of images to avoid the anomalies and noises using techniques like outlier elimination, data smoothening and data normalizing, c) feature extraction and selection using Principle Component Analysis (PCA) & regression method, d) generating pCT from MRI using Deep Convolutional Neural Network and UNET (DCNN-UNET). Further, we assessed metrics such as DC, SSIM, MAE and MSE for this model. However, our suggested model outperforms with an accuracy of 95%.",
        "link": "http://dx.doi.org/10.14704/web/v18si05/web18256"
    },
    {
        "id": 13180,
        "title": "Unpriortized Autoencoder For Image Generation",
        "authors": "Jaeyoung Yoo, Hojun Lee, Nojun Kwak",
        "published": "2020-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9191173"
    },
    {
        "id": 13181,
        "title": "Research Issues for Next Generation Content-Based Image Retrieval",
        "authors": "Vipin Tyagi",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-6759-4_15"
    },
    {
        "id": 13182,
        "title": "MATLAB code for selectable constant average picture level test image generation",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55410/jshq9099"
    },
    {
        "id": 13183,
        "title": "Panoramic Image Generation",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_300250"
    },
    {
        "id": 13184,
        "title": "Spatiotemporal image generation for embryomics applications",
        "authors": "Dennis Eschweiler, Ina Laube, Johannes Stegmaier",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-824349-7.00030-x"
    },
    {
        "id": 13185,
        "title": "Study and Development of Image Caption Generation using Various Encoders for Different Image Categories",
        "authors": "Anuja Namdev, S. R.N. Reddy",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nImages can act as the source of information or way of communication. Captioning an image by machines in a manner such that it conveys the true meaning is considered the most difficult task. It is indeed a process of deep analysis and most researched area. This paper presents a comparative analysis of different deep learning models such as Inception V3, Resnet50 and VGG16 based on a novel image captioning methodology applied on different picture categories. The hybrid approach is developed to get high BLEU score for each input images. The data set generation, implementation of hybrid approach, and the challenges along with the future work are discussed.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3196450/v1"
    },
    {
        "id": 13186,
        "title": "Single-Image Rain Removal Via Multi-Scale Cascading Image Generation",
        "authors": "Zheng Zhang, Yi Xu, He Wang, Bingbing Ni, Hongteng Xu",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803293"
    },
    {
        "id": 13187,
        "title": "Qualitative failures of image generation models and their application in detecting deepfakes",
        "authors": "Ali Borji",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.imavis.2023.104771"
    },
    {
        "id": 13188,
        "title": "Omni-Directional Image Generation from Single Snapshot Image",
        "authors": "Keisuke Okubo, Takao Yamanaka",
        "published": "2020-10-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc42975.2020.9283047"
    },
    {
        "id": 13189,
        "title": "Text-To-Image Generation Using AI",
        "authors": "Pavithra V, Rosy S, Srinishanthini R B, Prinslin L",
        "published": "2023-4-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55248/gengpi.234.4.38568"
    },
    {
        "id": 13190,
        "title": "Image to Image Content Generation",
        "authors": "Micheal Lanham",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7092-9_5"
    },
    {
        "id": 13191,
        "title": "Image Segmentation: Automatic Cluster Number Generation in K-Means",
        "authors": "Anju Bala, Aman Kumar Sharma",
        "published": "2019-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciip47207.2019.8985826"
    },
    {
        "id": 13192,
        "title": "Image Caption Bot with Keras and Speech Generation  for Visual Aid",
        "authors": "Dhananjay Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>How image classification is done and how it can be helped out for visual aid</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20368674"
    },
    {
        "id": 13193,
        "title": "Generative Adversarial Networks for Image Generation",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8"
    },
    {
        "id": 13194,
        "title": "Superpixel Generation Using SLIC",
        "authors": "Xiuping Jia",
        "published": "2022-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/3.2625662.ch39"
    },
    {
        "id": 13195,
        "title": "Psyncgan for Data Generation Application of Partially Syncronized GAN to Image and Caption Data Generation",
        "authors": "Kovtun Valery, Sajjad Kamali Siahroudi, Wei Gang",
        "published": "2019-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3373419.3373431"
    },
    {
        "id": 13196,
        "title": "FLOW IMAGE GENERATION ALGORITHMS FOR IMPROVING GAN",
        "authors": "Xianhong Zhang, Shusen Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1615/jflowvisimageproc.2020034486"
    },
    {
        "id": 13197,
        "title": "Advanced image generation for cancer using diffusion models",
        "authors": "Benjamin L. Kidder",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTDeep neural networks have significantly advanced medical image analysis, yet their full potential is often limited by the relatively small dataset sizes. Generative modeling has stimulated attention for its potential applications in the synthesis of medical images. Recent advancements in diffusion models have exhibited a remarkable capacity for producing photorealistic images. Despite this promising development, the application of such models in the generation of medical images remains underexplored. In this study, we explored the potential of using diffusion models to generate medical images, with a particular emphasis on producing brain magnetic resonance imaging (MRI) scans, such as those depicting low-grade gliomas. Additionally, we examined the generation of contrast enhanced spectral mammography (CESM) images, as well as chest and lung X-ray images. Utilizing the Dreambooth platform, we trained stable diffusion models based on text prompts, class and instance images, subsequently prompting the trained models to produce medical images. The generation of medical imaging data presents a viable approach for preserving the anonymity of medical images, effectively reducing the likelihood of patient re-identification during the exchange of data for research. The findings of this study reveal that the application of diffusion models in generating images successfully captures attributes specific to oncology within imaging modalities. Consequently, this research establishes a framework that harnesses the power of artificial intelligence for the generation of cancer medical imagery.",
        "link": "http://dx.doi.org/10.1101/2023.08.18.553859"
    },
    {
        "id": 13198,
        "title": "Style encoding for class-specific image generation",
        "authors": "Dana Cohen, Raja Giryes, Hayit Greenspan",
        "published": "2021-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2580878"
    },
    {
        "id": 13199,
        "title": "Biologically Inspired Hexagonal Deep Learning For Hexagonal Image Generation",
        "authors": "Tobias Schlosser, Frederik Beuth, Danny Kowerko",
        "published": "2020-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190995"
    },
    {
        "id": 13200,
        "title": "Image Caption Bot with Keras and Speech Generation  for Visual Aid",
        "authors": "Dhananjay Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>How image classification is done and how it can be helped out for visual aid</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20368674.v1"
    },
    {
        "id": 13201,
        "title": "An Feature Image Generation Based on Adversarial Generation Network",
        "authors": "Mengxing Gong, Yijun Wang",
        "published": "2020-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmtma50254.2020.00109"
    },
    {
        "id": 13202,
        "title": "PREFAB-GEN : AD HOC Image Generation for Pre-Manufacturing of Tires Using Image-To-Image Translation",
        "authors": "Guillaume Déau, Pascal Bourdon, Philippe Carré, Stéphane Mérillou, Alexandre Dervillé, François Mourougaya",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222342"
    },
    {
        "id": 13203,
        "title": "\"Deepfake\" Portrait Image Generation",
        "authors": "Jianfei Cai",
        "published": "2021-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3476099.3480396"
    },
    {
        "id": 13204,
        "title": "Transformer-based image generation from scene graphs",
        "authors": "Renato Sortino, Simone Palazzo, Francesco Rundo, Concetto Spampinato",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103721"
    },
    {
        "id": 13205,
        "title": "High-quality face image generation using particle swarm optimization-based generative adversarial networks",
        "authors": "Long Zhang, Lin Zhao",
        "published": "2021-9",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.future.2021.03.022"
    },
    {
        "id": 13206,
        "title": "A Cross-Layer Based Network for Faster Image Generation",
        "authors": "Zhaoyu Zhang, Yuechuan Sun, Jun Yu",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2018.8451501"
    },
    {
        "id": 13207,
        "title": "Image-to-Image Translation Method for Game-Character Face Generation",
        "authors": "Shinjin Kang, Yoonchan Ok, Hwanhee Kim, Teasung Hahn",
        "published": "2020-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog47356.2020.9231650"
    },
    {
        "id": 13208,
        "title": "An Event-Based, Frame-Based Image Acquisition Mechanism for CMOS Image Sensors",
        "authors": "Arnaud Verdant, Gilles Sicard, Dupoiron camille",
        "published": "2017-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ngcas.2017.32"
    },
    {
        "id": 13209,
        "title": "Enhancing User Profile Authenticity through Automatic Image Caption Generation Using a Bootstrapping Language–Image Pre-Training Model",
        "authors": "Smita Bharne, Pawan Bhaladhare",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023059182"
    },
    {
        "id": 13210,
        "title": "Structure-Aware Generative Adversarial Network for Text-to-Image Generation",
        "authors": "Wenjie Chen, Zhangkai Ni, Hanli Wang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222100"
    },
    {
        "id": 13211,
        "title": "Generation of Image by Sentence Based on Impression Words in Image",
        "authors": "Shohei Namisato",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17706/jcp.14.5.349-358"
    },
    {
        "id": 13212,
        "title": "Image Talk: A Model for Image Caption Generation with Voice",
        "authors": "Akash Bhadange, Rohan Bhole, Vaishali Jabade",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425636"
    },
    {
        "id": 13213,
        "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
        "authors": "Youshan Zhang, Jialu Li",
        "published": "2023-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-980"
    },
    {
        "id": 13214,
        "title": "GENERATION: An Efficient Denoising Autoencoders-Based Approach for Amputated Image Reconstruction",
        "authors": "Leila Ben Othman, Parisa Niloofar, Sadok Ben Yahia",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012460700003636"
    },
    {
        "id": 13215,
        "title": "Gated contextual transformer network for multi-modal retinal image clinical description generation",
        "authors": "Nagur Shareef Shaik, Teja Krishna Cherukuri",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.imavis.2024.104946"
    },
    {
        "id": 13216,
        "title": "CT scan harmonization using Convolutional Neural Networks for synthetic image generation",
        "authors": "Eduardo Ibor Crespo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.630643c2fa816f364423c779"
    },
    {
        "id": 13217,
        "title": "Image generation of train wheel tread damage image based on improved generative adversarial network",
        "authors": "Conghui Zhao",
        "published": "2022-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2636923"
    },
    {
        "id": 13218,
        "title": "All-in-Focus Image Generation Using Improved Blind Image Deconvolution Technique",
        "authors": "Sota Kawakami, Hiroyuki Kudo",
        "published": "2018-11-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3299852.3299859"
    },
    {
        "id": 13219,
        "title": "Modified densely connected convolutional network for content generation in automatic image description generation system",
        "authors": "S.R. Sreela, Sumam Mary Idicula",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tenconspring.2017.8070036"
    },
    {
        "id": 13220,
        "title": "Synthetic Driver Image Generation for Human Pose-Related Tasks",
        "authors": "Romain Guesdon, Carlos Crispim-Junior, Laure Rodet",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011780800003417"
    },
    {
        "id": 13221,
        "title": "Edge-Gan: Edge Conditioned Multi-View Face Image Generation",
        "authors": "Heqing Zou, Kenan E. Ak, Ashraf A. Kassim",
        "published": "2020-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190723"
    },
    {
        "id": 13222,
        "title": "The First Generation of Turkish Male Migrants - A ‘Second Hand Image’ or a ‘First Hand Image’?",
        "authors": "",
        "published": "2017-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315252780-16"
    },
    {
        "id": 13223,
        "title": "Automatic Generation Method of Multimedia Animation Image Based on Surface Mosaic",
        "authors": "Zhang Lan",
        "published": "2021-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipec51340.2021.9421319"
    },
    {
        "id": 13224,
        "title": "Musical Elements Enhancement and Image Content Preservation Network for Image to Music Generation",
        "authors": "Wenzhao Liu, Dechao Meng",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386748"
    },
    {
        "id": 13225,
        "title": "Face Image Inpainting Algorithm Via Progressive Generation Network",
        "authors": "Bojie Ma, Xiaowei An, Nongliang Sun",
        "published": "2020-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsip49896.2020.9339293"
    },
    {
        "id": 13226,
        "title": "Fast Image Segmentation and Animation Generation Algorithm Based on Depth Image Sequence",
        "authors": "Zhigan Liang",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/netcit57419.2022.00150"
    },
    {
        "id": 13227,
        "title": "Inspirational Adversarial Image Generation",
        "authors": "Baptiste Roziere, Morgane Riviere, Olivier Teytaud, Jeremy Rapin, Yann LeCun, Camille Couprie",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tip.2021.3065845"
    },
    {
        "id": 13228,
        "title": "Intraindividual comparison of image artifact of two generation of rotable cochlear implant magnets",
        "authors": "Ingo Todt",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.643fb327ed32600019dcdd0a"
    },
    {
        "id": 13229,
        "title": "Image and video generation",
        "authors": "Vincent Granville",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-44-321857-6.00008-4"
    },
    {
        "id": 13230,
        "title": "A Genetic Algorithm for Automated Test Generation for Satellite On-board Image Processing Applications",
        "authors": "Ulrike Witteck, Denis Grießbach, Paula Herber",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009821101280135"
    },
    {
        "id": 13231,
        "title": "Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes",
        "authors": "Ali Borji",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4402938"
    },
    {
        "id": 13232,
        "title": "Generation of high-resolution and high-precision depth image",
        "authors": "Koji Nishio, Yuta Muraki, Ken-ichi Kobori, Takayuki Kanaya",
        "published": "2018-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwait.2018.8369674"
    },
    {
        "id": 13233,
        "title": "CIGLI: Conditional Image Generation from Language &amp; Image",
        "authors": "Xiaopeng Lu, Lynnette Ng, Jared Fernandez, Hao Zhu",
        "published": "2021-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00349"
    },
    {
        "id": 13234,
        "title": "Component-Based Transformation for Person Image Generation",
        "authors": "Wen-Jiin Tsai, Po-Hsiang Chen",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897683"
    },
    {
        "id": 13235,
        "title": "Low Dynamic Range Image Set Generation from Single Image",
        "authors": "Rappy Saha, Partha Pratim Banik, Ki-Doo Kim",
        "published": "2019-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/elinfocom.2019.8706401"
    },
    {
        "id": 13236,
        "title": "Image Generation from a Hyper Scene Graph with Trinomial Hyperedges",
        "authors": "Ryosuke Miyake, Tetsu Matsukawa, Einoshin Suzuki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011699300003417"
    },
    {
        "id": 13237,
        "title": "Computer Image Generation",
        "authors": "Jon Peddie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-642-35947-7_171-2"
    },
    {
        "id": 13238,
        "title": "Next-generation manufacturing: achieving scalable edge inference for image analytics solutions",
        "authors": "Ravneet Kaur, Joydeep Acharya",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2677241"
    },
    {
        "id": 13239,
        "title": "Customized Image Narrative Generation via Interactive Visual Question Generation and Answering",
        "authors": "Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada",
        "published": "2018-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00930"
    },
    {
        "id": 13240,
        "title": "A Document Image Generation Scheme Based on Face Swapping and Distortion Generation",
        "authors": "Gang Zhu, Yuting Ding, Lin Zhao",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3194117"
    },
    {
        "id": 13241,
        "title": "The 3D modular chaotic map to digital color image encryption",
        "authors": "Ali Broumandnia",
        "published": "2019-10",
        "citations": 54,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.future.2019.04.005"
    },
    {
        "id": 13242,
        "title": "Single image‐based HDR image generation with camera response function estimation",
        "authors": "Yongqing Huo, Xudong Zhang",
        "published": "2017-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/iet-ipr.2016.1075"
    },
    {
        "id": 13243,
        "title": "A 4-channelled hazy image input generation and deep learning-based single image dehazing",
        "authors": "Pavan Kumar Balla, Arvind Kumar, Rajoo Pandey",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2024.104099"
    },
    {
        "id": 13244,
        "title": "Real catcher view image generation method for baseball contents",
        "authors": "Jeyeon Kim, Hongjun Lee, Joong-Sik Kim, Whoi-Yul Kim",
        "published": "2018-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwait.2018.8369793"
    },
    {
        "id": 13245,
        "title": "3d Image Generation from Single 2d Image using Monocular Depth Cues",
        "authors": "",
        "published": "2019-12-30",
        "citations": 0,
        "abstract": "There has been a tremendous increase in the popularity of 3D hardware such as TV's, Smartphone's, gadgets for gaming, medical equipments, 3D printing and many more. 2D to 3D conversion is applied at various levels to get 3D content. In this paper, 3D image is generated from a single 2D image. we try to convert our own Karate and Bharathanatyam (KB) Dataset which contains both indoor and outdoor poses to 3D. Here, Watershed algorithm is employed to segment the image. Depth map is generated by sharpness and contrast as depth cues. The 3D image from single 2D image is created by depth image based rendering method.",
        "link": "http://dx.doi.org/10.35940/ijeat.b3985.129219"
    },
    {
        "id": 13246,
        "title": "Stereo Generation from a Single Image Using Deep Residual Network",
        "authors": "Jun Huang, Tianteng Bi, Yue Liu, Yongtian Wang",
        "published": "2018-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2018.8451311"
    },
    {
        "id": 13247,
        "title": "Controlled moiré effect in multiview three-dimensional displays: image quality and image generation",
        "authors": "Vladimir Saveljev, Sung-Kyu Kim",
        "published": "2018-6-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.oe.57.6.061623"
    },
    {
        "id": 13248,
        "title": "Ssd: Towards Better Text-Image Consistency Metric in Text-to-Image Generation",
        "authors": "Zhaorui Tan, Xi Yang, Zihan Ye, Qiufeng Wang, Yuyao Yan, Anh Nguyen, Kaizhu Huang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4332128"
    },
    {
        "id": 13249,
        "title": "A New Raw Holoscopic Image Simulator and Data Generation",
        "authors": "Bodor Almatrouk, Hongying Meng, Akuha Aondoakaa, Rafiq Swash",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icivc58118.2023.10270247"
    },
    {
        "id": 13250,
        "title": "Improved Framework using Rider Optimization Algorithm for Precise Image Caption Generation",
        "authors": "Chaitrali Prasanna Chaudhari, Satish Devane",
        "published": "2022-4",
        "citations": 1,
        "abstract": " “Image Captioning is the process of generating a textual description of an image”. It deploys both computer vision and natural language processing for caption generation. However, the majority of the image captioning systems offer unclear depictions regarding the objects like “man”, “woman”, “group of people”, “building”, etc. Hence, this paper intends to develop an intelligent-based image captioning model. The adopted model comprises of few steps like word generation, sentence formation, and caption generation. Initially, the input image is subjected to the Deep learning classifier called Convolutional Neural Network (CNN). Since the classifier is already trained in the relevant words that are related to all images, it can easily classify the associated words of the given image. Further, a set of sentences is formed with the generated words using Long-Short Term Memory (LSTM) model. The likelihood of the formed sentences is computed using the Maximum Likelihood (ML) function, and the sentences with higher probability are taken, which is further used for generating the visual representation of the scene in terms of image caption. As a major novelty, this paper aims to enhance the performance of CNN by optimally tuning its weight and activation function. This paper introduces a new enhanced optimization algorithm Rider with Randomized Bypass and Over-taker update (RR-BOU) for this optimal selection. In the proposed RR-BOU is the enhanced version of the Rider Optimization Algorithm (ROA). Finally, the performance of the proposed captioning model is compared over other conventional models with respect to statistical analysis. ",
        "link": "http://dx.doi.org/10.1142/s0219467822500218"
    },
    {
        "id": 13251,
        "title": "Stylized Image Generation based on Music-image Synesthesia Emotional Style Transfer using CNN Network",
        "authors": "",
        "published": "2021-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2021.04.015"
    },
    {
        "id": 13252,
        "title": "Application of an Improved DCGAN for Image GenerationApplication of an Improved DCGAN for Image Generation",
        "authors": "Bingqi Liu, Jiwei Lv, Xinyue Fan, Jie Luo, Tianyi Zou",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nWith the rapid development of deep learning, image generation technology has become one of the current hot research areas. A deep convolutional generative adversarial network (DCGAN) can better adapt to complex image distributions than other methods. In this paper, based on a traditional generative adversarial networks (GANs) image generation model, first, the fully connected layer of the DCGAN is further improved. To solve the problem of gradient disappearance in GANs, the activation functions of all layers of the discriminator are LeakyReLU functions, the output layer of the generator uses the Tanh activation function, and the other layers use ReLU. Second, the improved DCGAN model is verified on the MNIST dataset, and simple initial fraction (ISs) and complex initial fraction (ISc) indexes are established from the two aspects of image quality and image generation diversity, respectively. Finally, through a comparison of the two groups of experiments, it is found that the quality of images generated by the DCGAN model constructed in this paper is 2.02 higher than that of the GANs model, and the diversity of the images generated by the DCGAN is 1.55 higher than that of GANs. The results show that the improved DCGAN model can solve the problem of low-quality images being generated by the GANs and achieve good results.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-266104/v1"
    },
    {
        "id": 13253,
        "title": "Facial Image Generation by Generative Adversarial Networks using Weighted Conditions",
        "authors": "Hiroki Adachi, Hiroshi Fukui, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007377600002108"
    },
    {
        "id": 13254,
        "title": "SynFine: Boosting Image Segmentation Accuracy Through Synthetic Data Generation and Surgical Fine-Tuning",
        "authors": "Mehdi Mounsif, Yassine Motie, Mohamed Benabdelkrim, Florent Brondolo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011848300003411"
    },
    {
        "id": 13255,
        "title": "Facial Image Generation by Generative Adversarial Networks using Weighted Conditions",
        "authors": "Hiroki Adachi, Hiroshi Fukui, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007377601390145"
    },
    {
        "id": 13256,
        "title": "Feature Cycling Block for Improving Gans-Based Image Generation Performance",
        "authors": "Seung Park, Yong-Goo Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4452070"
    },
    {
        "id": 13257,
        "title": "Conclusions",
        "authors": "Xudong Mao, Qing Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6048-8_4"
    },
    {
        "id": 13258,
        "title": "Deep learning, image generation, and the rise of bias automation machines",
        "authors": "Mark Dingemanse",
        "published": "No Date",
        "citations": 0,
        "abstract": "DALL-E, a new image generation system by OpenAI, does impressive visualizations of biased datasets. I like how the first example that OpenAI used to present DALL-E to the world is a meme-like koala dunking a baseball leading into an array of old white men — representing at one blow the past and future of representation and generation.",
        "link": "http://dx.doi.org/10.59350/evr2h-gdr45"
    },
    {
        "id": 13259,
        "title": "Image Caption Generation: A Review",
        "authors": "",
        "published": "2023-8-5",
        "citations": 0,
        "abstract": "In Artificial Intelligence (AI), the contents of an image are generated automatically which involves computer vision and NLP (Natural Language Processing). The neural model which is regenerative, is created. It depends on computer vision and machine translation. This model is used to generate natural sentences which eventually describes the image. This model consists of Convolutional Neural Network (CNN) as well as Recurrent Neural Network (RNN). The CNN is used for feature extraction from image and RNN is used for sentence generation. The model is trained in such a way that if input image is given to model it generates captions which nearly describes the image. The accuracy of model and smoothness or command of language model learns from image descriptions is tested on different datasets. These experiments show that model is frequently giving accurate descriptions for an input image. In this paper investigating various methods used in the existing work for the image caption generation recognition system.",
        "link": "http://dx.doi.org/10.59544/juvz9207/ngcesi23p58"
    },
    {
        "id": 13260,
        "title": "Accelerate Diffusion Based Human Image Generation Via Consistency Models",
        "authors": "Songchuan Zhang, Zhiwei Lin, Yiwei Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4719919"
    },
    {
        "id": 13261,
        "title": "Synthetic Data Generation Through Image Translation for Improving Out-of-domain MRI Lesion Segmentation",
        "authors": "Brandon Mac",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>One of the key limitations in machine learning models is poor performance on data that is out of the domain of the training distribution. This is especially true for image analysis in magnetic resonance (MR) imaging, as variations in hardware and software create non-standard intensities, contrasts, and noise distributions across scanners. Recently, image translation models have been proposed to augment data across domains to create synthetic data points. In this thesis, the objective was to investigate the application an unsupervised image translation model to augment MR images from a source dataset to a target dataset to create synthetic data for training segmentation models. Speciﬁcally, the objective was to observe if training on these synthetic data points can approach the performance of a model trained directly on the target distribution. Three conﬁgurations of augmentation between datasets consisting of translation between images, between scanner vendors, and from labels to images. It was found that the segmentation models trained on synthetic data from labels to images conﬁguration yielded the closest performance to the segmentation model trained directly on the target dataset. The Dice coefﬁcient score per each target vendor (GE, Siemens, Philips) for training on synthetic data was 0.63, 0.64, and 0.58, compared to training directly on target dataset was 0.65, 0.72, and 0.61.</p>",
        "link": "http://dx.doi.org/10.32920/24625203"
    },
    {
        "id": 13262,
        "title": "Artifygan: Animated Face Image Generation Using Gans",
        "authors": "Iqra Bismi, Priya Khandelwal, Saniya Lande, Mohammad Masum",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622124"
    },
    {
        "id": 13263,
        "title": "Synthetic Data Generation Through Image Translation for Improving Out-of-domain MRI Lesion Segmentation",
        "authors": "Brandon Mac",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>One of the key limitations in machine learning models is poor performance on data that is out of the domain of the training distribution. This is especially true for image analysis in magnetic resonance (MR) imaging, as variations in hardware and software create non-standard intensities, contrasts, and noise distributions across scanners. Recently, image translation models have been proposed to augment data across domains to create synthetic data points. In this thesis, the objective was to investigate the application an unsupervised image translation model to augment MR images from a source dataset to a target dataset to create synthetic data for training segmentation models. Speciﬁcally, the objective was to observe if training on these synthetic data points can approach the performance of a model trained directly on the target distribution. Three conﬁgurations of augmentation between datasets consisting of translation between images, between scanner vendors, and from labels to images. It was found that the segmentation models trained on synthetic data from labels to images conﬁguration yielded the closest performance to the segmentation model trained directly on the target dataset. The Dice coefﬁcient score per each target vendor (GE, Siemens, Philips) for training on synthetic data was 0.63, 0.64, and 0.58, compared to training directly on target dataset was 0.65, 0.72, and 0.61.</p>",
        "link": "http://dx.doi.org/10.32920/24625203.v1"
    },
    {
        "id": 13264,
        "title": "Generation of Random Fields for Image Segmentation Techniques: A Review",
        "authors": "Rambabu Pemula, Sagenela Vijaya Kumar, C. Nagaraju",
        "published": "2023-3",
        "citations": 1,
        "abstract": " Generation of random fields (GRF) for image segmentation represents partitioning an image into different regions that are homogeneous or have similar facets of the image. It is one of the most challenging tasks in image processing and a very important pre-processing step in the fields of computer vision, image analysis, medical image processing, pattern recognition, remote sensing, and geographical information system. Many researchers have presented numerous image segmentation approaches, but still, there are challenges like segmentation of low contrast images, removal of shadow in the images, reduction of high dimensional images, and computational complexity of segmentation techniques. In this review paper, the authors address these issues. The experiments are conducted and tested on the Berkely dataset (BSD500), Semantic dataset, and our own dataset, and the results are shown in the form of tables and graphs. ",
        "link": "http://dx.doi.org/10.1142/s0219467823500225"
    },
    {
        "id": 13265,
        "title": "The Task Matters: Comparing Image Captioning and Task-Based Dialogical Image Description",
        "authors": "Nikolai Ilinykh, Sina Zarrieß, David Schlangen",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6547"
    },
    {
        "id": 13266,
        "title": "An image conveys a message: A brief survey on image description generation",
        "authors": "Sidra Shabir, Syed Yasser Arafat",
        "published": "2018-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpesg.2018.8384519"
    },
    {
        "id": 13267,
        "title": "Gamma-ray Image Noise Generation Using Energy-Image Converter Based on Image Histogram",
        "authors": "Ren Komatsu, Hanwool Woo, Yusuke Tamura, Atsushi Yamashita, Hajime Asama",
        "published": "2021-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf49454.2021.9382733"
    },
    {
        "id": 13268,
        "title": "Unsupervised Synthetic Acoustic Image Generation for Audio-Visual Scene Understanding",
        "authors": "Valentina Sanguineti, Pietro Morerio, Alessio Del Bue, Vittorio Murino",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tip.2022.3219228"
    },
    {
        "id": 13269,
        "title": "Intraoral Image Generation by Progressive Growing of Generative Adversarial Network and Evaluation of Generated Image Quality by Dentists",
        "authors": "Kazuma Kokomoto, Rena Okawa, Kazuhiko Nakano, Kazunori Nozaki",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDentists need experience with plenty of clinical cases to practice specialized skills. However, the need to protect patients’ private information limits the ability to utilize lots of intraoral images obtained from clinical cases. In this study, since generating realistic images could making utilizing lots of intraoral images possible, intraoral images are generated by using a progressive growing of generative adversarial network. 35,254 intraoral images were used as training data with resolutions of 128×128, 256×256, 512×512, and 1,024×1,024. The results of training datasets with and without data augmentation were compared. The sliced Wasserstein distance (SWD) was calculated to evaluate the generated images. Next, 50 real images and 50 generated images for each resolution were randomly selected and shuffled. Twelve pediatric dentists were asked to observe these images and assess whether each was real or generated. The accuracy of the assessment of the 1,024×1,024 images was significantly higher than that of the other resolutions. In conclusion, generated intraoral images with resolutions of 512×512 or lower were so realistic that the dentists could not distinguish whether they were real or generated. This implies that generated images can be used for dental education or data augmentation for deep learning free from privacy restrictions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-117942/v1"
    },
    {
        "id": 13270,
        "title": "Pose Guided Person Image Generation With Hidden P-Norm Regression",
        "authors": "Ting-Yao Hu, Alexander G. Hauptmann",
        "published": "2021-9-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506484"
    }
]