[
    {
        "id": 14705,
        "title": "Machine Learning Based Approach in High-Performance of Deep Transfer Learning Model with Batch Normalization Method for Tomato Plant Disease Identification and Categorization",
        "authors": "R Ramya, P. Kumar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4351165"
    },
    {
        "id": 14706,
        "title": "High-performanceÂ deep transfer learning model with batch normalization based on multiscale feature fusion for tomato plant disease identification and categorization",
        "authors": "R Ramya, P Kumar",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nDeep learning and machine learning are cutting-edge methods for analysing images that have considerable potential. Artificial Neural Networks (A-NNs), one of the most well-known methods of computer intelligence, are now used in machine learning (ML) and deep transfer learning (DL) to raise plant production and quality. Identification and primary prevention of plant diseases at the appropriate time are essential for boosting productivity. Due to the phenomenon of minimally intense data in the background and foreground areas of the image, the extensive colour similarity between regions of unhealthy and normal leaves, the presence of noise in the sampling data, and changes in the location, size, and shape of plant leaf, it is difficult to correctly identify and classify plant diseases. In an effort to address these issues, a reliable technique for classifying plant diseases was developed by using a deep AlexNet CNN architecture as the main network with batch normalisation. In the three-step process, the first annotation is made to obtain the RoI (region of interest). The AlexNet CNN is therefore suggested for deep primary feature extraction in a constructed efficient network. The research demonstrates that the existing strategy is superior to more recent ones in terms of accuracy and dependability in recognising diseases in plants. Based on a deep transfer AlexNet CNN model, this research work developed a model for diseases identification and classification in plant leaves. It is trained using additional datasets that include a variety of plant leaf classifications and background images. From Plant Village and Kaggle, we gathered data on healthy and diseased tomato plant leaves. We are obtaining a near-balanced dataset containing ten different leaf disease kinds, such as bacterial, fungal, viral, and nutrient insufficiency. Ten classes have been considered for this research by gathering a dataset with associated images of the typical and abnormal tomato plant leaves. Considered in this work were the various labels for healthy and diseased tomato leaves, such as early blight, Bacterial spot, late bright mold, healthy, etc. Since deep CNN models have shown notable machine vision results, they are used in this case to diagnose and categorise plant illnesses from their leaves. As a result, the proposed CNN models can thus now be evaluated from confusion matrix using data analysis criteria, primarily focusing on metrics for evaluation like training and validation accuracy, loss, Recall, Precision, F1 score, processing speed, and performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/2515-7620/ace594"
    },
    {
        "id": 14707,
        "title": "Batch normalization embeddings for deep domain generalization",
        "authors": "Mattia Segu, Alessio Tonioni, Federico Tombari",
        "published": "2023-3",
        "citations": 43,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2022.109115"
    },
    {
        "id": 14708,
        "title": "Diminishing Batch Normalization",
        "authors": "Yintai Ma, Diego Klabjan",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3210840"
    },
    {
        "id": 14709,
        "title": "FLEC: Federated Learning for Cloud/Edge-Based Smart Industry via Batch Normalization",
        "authors": "Vidushi Agarwal, Sujata Pal",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283541"
    },
    {
        "id": 14710,
        "title": "Spectral Batch Normalization: Normalization in the Frequency Domain",
        "authors": "Rinor Cakaj, Jens Mehnert, Bin Yang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191931"
    },
    {
        "id": 14711,
        "title": "Batch Normalization Preconditioning for Stochastic Gradient Langevin Dynamics",
        "authors": "Susanna Lange, Wei Deng, Qiang Ye, Guang Lin",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4208/jml.220726a"
    },
    {
        "id": 14712,
        "title": "Batch Normalization Damages Federated Learning on NON-IID Data: Analysis and Remedy",
        "authors": "Yanmeng Wang, Qingjiang Shi, Tsung-Hui Chang",
        "published": "2023-6-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095399"
    },
    {
        "id": 14713,
        "title": "Regularized adversarial learning for normalization of multi-batch untargeted metabolomics data",
        "authors": "Andrei Dmitrenko, Michelle Reid, Nicola Zamboni",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "AbstractMotivationUntargeted metabolomics by mass spectrometry is the method of choice for unbiased analysis of molecules in complex samples of biological, clinical or environmental relevance. The exceptional versatility and sensitivity of modern high-resolution instruments allows profiling of thousands of known and unknown molecules in parallel. Inter-batch differences constitute a common and unresolved problem in untargeted metabolomics, and hinder the analysis of multi-batch studies or the intercomparison of experiments.ResultsWe present a new method, Regularized Adversarial Learning Preserving Similarity (RALPS), for the normalization of multi-batch untargeted metabolomics data. RALPS builds on deep adversarial learning with a three-term loss function that mitigates batch effects while preserving biological identity, spectral properties and coefficients of variation. Using two large metabolomics datasets, we showcase the superior performance of RALPS as compared with six state-of-the-art methods for batch correction. Further, we demonstrate that RALPS scales well, is robust, deals with missing values and can handle different experimental designs.Availability and implementationhttps://github.com/zamboni-lab/RALPS.Supplementary informationSupplementary data are available at Bioinformatics online.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btad096"
    },
    {
        "id": 14714,
        "title": "Fast Domain Adaptation in Face Recognition by Decomposed Meta Batch Normalization",
        "authors": "",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/nq.2021.19.6.nq21089"
    },
    {
        "id": 14715,
        "title": "Regularizing deep neural networks for medical image analysis with augmented batch normalization",
        "authors": "Shengqian Zhu, Chengrong Yu, Junjie Hu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2024.111337"
    },
    {
        "id": 14716,
        "title": "A Robust Initialization of Residual Blocks for Effective ResNet Training Without Batch Normalization",
        "authors": "Enrico Civitelli, Alessio Sortino, Matteo Lapucci, Francesco Bagattini, Giulio Galvan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3325541"
    },
    {
        "id": 14717,
        "title": "Optimization of Deep Convolutional Neural Network with the Integrated Batch Normalization and Global pooling",
        "authors": "Priyanga K K, Sabeen S",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "Deep convolutional neural networks (DCNN) have made significant progress in a wide range of applications in recent years, which include image identification, audio recognition, and translation of machine information. These tasks assist machine intelligence in a variety of ways. However, because of the large number of parameters, float manipulations and conversion of machine terminal remains difficult. To handle this issue, optimization of convolution in the DCNN is initiated that adjusts the characteristics of the neural network, and the loss of information is minimized with enriched performance. Minimization of convolution function addresses the optimization issues. Initially, batch normalization is completed, and instead of lowering neighborhood values, a full feature map is minimized to a single value using the global pooling approach. Traditional convolution is split into depth and pointwise to decrease the model size and calculations. The optimized convolution-based DCNN's performance is evaluated with the assistance of accuracy and occurrence of error. The optimized DCNN is compared with the existing state-of-the-art techniques, and the optimized DCNN outperforms the existing technique.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijcnis.v15i1.5617"
    },
    {
        "id": 14718,
        "title": "Research on Improving The Deep Learning Model of Differential Privacy Using Normalization Method",
        "authors": "Boyu Jin",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icceic60201.2023.10426630"
    },
    {
        "id": 14719,
        "title": "Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization",
        "authors": "Yanan Wu, Zhixiang Chi, Yang Wang, Konstantinos N. Plataniotis, Songhe Feng",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Test-time domain adaptation aims to adapt the model trained on source domains to unseen target domains using a few unlabeled images. Emerging research has shown that the label and domain information is separately embedded in the weight matrix and batch normalization (BN) layer. Previous works normally update the whole network naively without explicitly decoupling the knowledge between label and domain. As a result, it leads to knowledge interference and defective distribution adaptation. In this work, we propose to reduce such learning interference and elevate the domain knowledge learning by only manipulating the BN layer. However, the normalization step in BN is intrinsically unstable when the statistics are re-estimated from a few samples. We find that ambiguities can be greatly reduced when only updating the two affine parameters in BN while keeping the source domain statistics. To further enhance the domain knowledge extraction from unlabeled data, we construct an auxiliary branch with label-independent self-supervised learning (SSL) to provide supervision. Moreover, we propose a bi-level optimization based on meta-learning to enforce the alignment of two learning objectives of auxiliary and main branches. The goal is to use the auxiliary branch to adapt the domain and benefit main task for subsequent inference. Our method keeps the same computational cost at inference as the auxiliary branch can be thoroughly discarded after adaptation. Extensive experiments show that our method outperforms the prior works on five WILDS real-world domain shift datasets. Our method can also be integrated with methods with label-dependent optimization to further push the performance boundary. Our code is available at https://github.com/ynanwu/MABN.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i14.29527"
    },
    {
        "id": 14720,
        "title": "Robust RF Data Normalization for Deep Learning",
        "authors": "Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ius51837.2023.10307302"
    },
    {
        "id": 14721,
        "title": "ACP- based Circular target image Rotation normalization system",
        "authors": "Jichang He, Zhijia Zhang, Huaici Zhao, Jiale Yang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166580"
    },
    {
        "id": 14722,
        "title": "An integrated deep learning - short term production scheduling - optimal control framework for batch processes",
        "authors": "Omar Santander, Ioannis Giannikopoulos, Michael Baldea",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155963"
    },
    {
        "id": 14723,
        "title": "Latency-constrained DNN architecture learning for edge systems using zerorized batch normalization",
        "authors": "Shuo Huai, Di Liu, Hao Kong, Weichen Liu, Ravi Subramaniam, Christian Makaya, Qian Lin",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.future.2022.12.021"
    },
    {
        "id": 14724,
        "title": "Batch Mode Deep Active Learning for Regression on Graph Data",
        "authors": "Peter Samoaa, Linus Aronsson, Philipp Leitner, Morteza Haghir Chehreghani",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386685"
    },
    {
        "id": 14725,
        "title": "Safe batch constrained deep reinforcement learning with generative adversarial network",
        "authors": "Wenbo Dong, Shaofan Liu, Shiliang Sun",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.03.108"
    },
    {
        "id": 14726,
        "title": "Batch alignment of single-cell transcriptomics data using deep metric learning",
        "authors": "Xiaokang Yu, Xinyi Xu, Jingxiao Zhang, Xiangjie Li",
        "published": "2023-2-21",
        "citations": 9,
        "abstract": "AbstractscRNA-seq has uncovered previously unappreciated levels of heterogeneity. With the increasing scale of scRNA-seq studies, the major challenge is correcting batch effect and accurately detecting the number of cell types, which is inevitable in human studies. The majority of scRNA-seq algorithms have been specifically designed to remove batch effect firstly and then conduct clustering, which may miss some rare cell types. Here we develop scDML, a deep metric learning model to remove batch effect in scRNA-seq data, guided by the initial clusters and the nearest neighbor information intra and inter batches. Comprehensive evaluations spanning different species and tissues demonstrated that scDML can remove batch effect, improve clustering performance, accurately recover true cell types and consistently outperform popular methods such as Seurat 3, scVI, Scanorama, BBKNN, Harmony et al. Most importantly, scDML preserves subtle cell types in raw data and enables discovery of new cell subtypes that are hard to extract by analyzing each batch individually. We also show that scDML is scalable to large datasets with lower peak memory usage, and we believe that scDML offers a valuable tool to study complex cellular heterogeneity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41467-023-36635-5"
    },
    {
        "id": 14727,
        "title": "Attention adaptive instance normalization style transfer for vascular segmentation using deep learning",
        "authors": "Supriti Mulay, Keerthi Ram, Mohanasankar Sivaprakasam",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-05033-1"
    },
    {
        "id": 14728,
        "title": "Non-Intrusive Load Decomposition Based on Instance-Batch Normalization Networks",
        "authors": "Mao Wang, Dandan Liu, Changzhi Li",
        "published": "2023-3-23",
        "citations": 0,
        "abstract": "At present, the non-intrusive load decomposition method for low-frequency sampling data is as yet insufficient within the context of generalization performance, failing to meet the decomposition accuracy requirements when applied to novel scenarios. To address this issue, a non-intrusive load decomposition method based on instance-batch normalization network is proposed. This method uses an encoder-decoder structure with attention mechanism, in which skip connections are introduced at the corresponding layers of the encoder and decoder. In this way, the decoder can reconstruct a more accurate power sequence of the target. The proposed model was tested on two public datasets, REDD and UKDALE, and the performance was compared with mainstream algorithms. The results show that the F1 score was higher by an average of 18.4 when compared with mainstream algorithms. Additionally, the mean absolute error reduced by an average of 25%, and the root mean square error was reduced by an average of 22%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16072940"
    },
    {
        "id": 14729,
        "title": "Rebalancing Batch Normalization for Exemplar-Based Class-Incremental Learning",
        "authors": "Sungmin Cha, Sungjun Cho, Dasol Hwang, Sunwon Hong, Moontae Lee, Taesup Moon",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01927"
    },
    {
        "id": 14730,
        "title": "A modular approach for multilingual timex detection and normalization using deep learning and grammar-based methods",
        "authors": "Nayla Escribano, German Rigau, Rodrigo Agerri",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110612"
    },
    {
        "id": 14731,
        "title": "Two-dimensional iterative learning control with deep reinforcement learning compensation for the non-repetitive uncertain batch processes",
        "authors": "Jianan Liu, Zike Zhou, Wenjing Hong, Jia Shi",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jprocont.2023.103106"
    },
    {
        "id": 14732,
        "title": "Sentiment Impact of Public Health Agency communication Strategies on TikTok under COVID-19 Normalization: Deep Learning Exploration",
        "authors": "ShaoPeng Che, Jang Hyun Kim",
        "published": "2023-5-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10389-023-01921-5"
    },
    {
        "id": 14733,
        "title": "Dynamic Coupon Targeting Using Batch Deep Reinforcement Learning: An Application to Livestream Shopping",
        "authors": "Xiao Liu",
        "published": "2023-7",
        "citations": 10,
        "abstract": " We present an empirical framework for creating dynamic coupon targeting strategies using deep reinforcement learning. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/mksc.2022.1403"
    },
    {
        "id": 14734,
        "title": "Batch Normalized Siamese Network Deep Learning Based Image Similarity Estimation",
        "authors": "M. Shyamala Devi, J. Arun Pandian, Aparna Joshi, Yeluri Praveen",
        "published": "2023-2-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icecct56650.2023.10179689"
    },
    {
        "id": 14735,
        "title": "Malware Detection Using 1d Convolution with Batch Normalization and L2 Regularization for Android",
        "authors": "Sushil Buriya, Neelam Sharma",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscan58655.2023.10395472"
    },
    {
        "id": 14736,
        "title": "Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks",
        "authors": "Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Sang Woo Kim",
        "published": "2024-6-30",
        "citations": 1,
        "abstract": "\nL\n2\n            regularization for weights in neural networks is widely used as a standard training trick. In addition to weights, the use of batch normalization involves an additional trainable parameter Î³, which acts as a scaling factor. However,\n            L\n2\n            regularization for Î³ remains an undiscussed mystery and is applied in different ways depending on the library and practitioner. In this article, we study whether\n            L\n2\n            regularization for Î³ is valid. To explore this issue, we consider two approaches: (1) variance control to make the residual network behave like an identity mapping and (2) stable optimization through the improvement of effective learning rate. Through two analyses, we specify the desirable and undesirable Î³ to apply\n            L\n2\n            regularization and propose four guidelines for managing them. In several experiments, we observed that applying\n            L\n2\n            regularization to applicable Î³ increased 1% to 4% classification accuracy, whereas applying\n            L\n2\n            regularization to inapplicable Î³ decreased 1% to 3% classification accuracy, which is consistent with our four guidelines. Our proposed guidelines were further validated through various tasks and architectures, including variants of residual networks and transformers.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3643860"
    },
    {
        "id": 14737,
        "title": "Hyperparameter-tuned batch-updated stochastic gradient descent: Plant species identification by using hybrid deep learning",
        "authors": "Deepti Barhate, Sunil Pathak, Ashutosh Kumar Dubey",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ecoinf.2023.102094"
    },
    {
        "id": 14738,
        "title": "Adaptive Batch Normalization for Training Data with Heterogeneous Features",
        "authors": "Wael Alsobhi, Tarik Alafif, Weiwei Zong, Alaa E. Abdel-Hakim",
        "published": "2023-2-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsca57840.2023.10087711"
    },
    {
        "id": 14739,
        "title": "AI-based optimal control of fed-batch biopharmaceutical process leveraging deep reinforcement learning",
        "authors": "Haoran Li, Tong Qiu, Fengqi You",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ces.2024.119990"
    },
    {
        "id": 14740,
        "title": "EUNNet: Efficient UN-Normalized Convolution Layer for Stable Training of Deep Residual Networks Without Batch Normalization Layer",
        "authors": "Khanh-Binh Nguyen, Jaehyuk Choi, Joon-Sung Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3244072"
    },
    {
        "id": 14741,
        "title": "Crowd Counting via Joint SASNet and a Guided Batch Normalization Network",
        "authors": "Cengizhan HaldÄ±z, Sarmad F. Ismael, Hasari Ãelebi, Erchan Aptoula",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223901"
    },
    {
        "id": 14742,
        "title": "Batch Normalization Based Convolutional Neural Network for Segmentation and Classification of Brain Tumor MRI Images",
        "authors": "",
        "published": "2024-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0430.04"
    },
    {
        "id": 14743,
        "title": "Optimizing Hyperspectral Imaging Classification Performance with CNN and Batch Normalization",
        "authors": "Guyang Zhang, Waleed Abdulla",
        "published": "2023-9",
        "citations": 0,
        "abstract": " Background: Hyperspectral imaging systems face numerous challenges in acquiring accurate spatial-spectral hypercubes due to sample surface heterogeneity, environmental instability, and instrumental noise. Preprocessing strategies such as outlier detection, calibration, smoothing, and normalization are typically employed to address these issues, selecting appropriate techniques based on prediction performance evaluation. However, the risk of misusing inappropriate preprocessing methods remains a concern. Methods: In this study, we evaluate the impact of five normalization methods on the classification performance of six different classifiers using honey hyperspectral images. Our results show that different classifiers have varying compatible normalization techniques and that using Batch Normalization with Convolutional Neural Networks (CNN) can significantly improve classification performance and diminish the variations among other normalization techniques. The CNN with Batch Normalization can achieve a macro average F1 score of â¥0.99 with four different normalization methods and â¥0.97 without normalization. Furthermore, we analyze kernel weights' distribution in the CNN models' final convolutional layers using statistical measurements and kernel density estimation (KDE) graphs. Results: We find that the performance improvements resulting from adding BatchNorm layers are associated with kernel weight range, kurtosis, and density around 0. However, the differences among normalization methods do not show a strong correlation with kernel weight distribution. In conclusion, our findings demonstrate that the CNN with Batch Normalization layers can achieve better prediction results and avoid the risk of inappropriate normalization. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/27551857231204622"
    },
    {
        "id": 14744,
        "title": "Heuristic normalization procedure for batch effect correction",
        "authors": "Arthur Yosef, Eli Shnaider, Moti Schneider, Michael Gurevich",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08049-4"
    },
    {
        "id": 14745,
        "title": "Multimodal deep learning: An improvement in prognostication or a reflection of batch effect?",
        "authors": "Frederick M. Howard, Jakob Nikolas Kather, Alexander T. Pearson",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ccell.2022.10.025"
    },
    {
        "id": 14746,
        "title": "On the pitfalls of Batch Normalization for end-to-end video learning: A study on surgical workflow analysis",
        "authors": "Dominik Rivoir, Isabel Funke, Stefanie Speidel",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.media.2024.103126"
    },
    {
        "id": 14747,
        "title": "Learning Generalizable Batch Active Learning Strategies via Deep Q-networks (Student Abstract)",
        "authors": "Yi-Chen Li, Wen-Jie Shen, Boyu Zhang, Feng Mao, Zongzhang Zhang, Yang Yu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "To handle a large amount of unlabeled data, batch active learning (BAL) queries humans for the labels of a batch of the most valuable data points at every round. Most current BAL strategies are based on human-designed heuristics, such as uncertainty sampling or mutual information maximization. However, there exists a disagreement between these heuristics and the ultimate goal of BAL, i.e., optimizing the model's final performance within the query budgets. This disagreement leads to a limited generality of these heuristics. To this end, we formulate BAL as an MDP and propose a data-driven approach based on deep reinforcement learning. Our method learns the BAL strategy by maximizing the model's final performance. Experiments on the UCI benchmark show that our method can achieve competitive performance compared to existing heuristics-based approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26989"
    },
    {
        "id": 14748,
        "title": "Microservice Container Load Trend Prediction based on Informer and Batch Normalization",
        "authors": "Dequan Gao, Meng Yang, Bing Zhang, Bao Feng",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aees59800.2023.10469263"
    },
    {
        "id": 14749,
        "title": "Artificial Neural Network with Dropout and Batch Normalization Applied on Diabetic Patient Data",
        "authors": "Alejandro Malla, Pallav Kumar Bera",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10252319"
    },
    {
        "id": 14750,
        "title": "Impact of H&amp;E Stain Normalization on Deep Learning Models in Cancer Image Classification: Performance, Complexity, and Trade-Offs",
        "authors": "Nuwan Madusanka, Pramudini Jayalath, Dileepa Fernando, Lasith Yasakethu, Byeong-Il Lee",
        "published": "2023-8-17",
        "citations": 1,
        "abstract": "Accurate classification of cancer images plays a crucial role in diagnosis and treatment planning. Deep learning (DL) models have shown promise in achieving high accuracy, but their performance can be influenced by variations in Hematoxylin and Eosin (H&E) staining techniques. In this study, we investigate the impact of H&E stain normalization on the performance of DL models in cancer image classification. We evaluate the performance of VGG19, VGG16, ResNet50, MobileNet, Xception, and InceptionV3 on a dataset of H&E-stained cancer images. Our findings reveal that while VGG16 exhibits strong performance, VGG19 and ResNet50 demonstrate limitations in this context. Notably, stain normalization techniques significantly improve the performance of less complex models such as MobileNet and Xception. These models emerge as competitive alternatives with lower computational complexity and resource requirements and high computational efficiency. The results highlight the importance of optimizing less complex models through stain normalization to achieve accurate and reliable cancer image classification. This research holds tremendous potential for advancing the development of computationally efficient cancer classification systems, ultimately benefiting cancer diagnosis and treatment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cancers15164144"
    },
    {
        "id": 14751,
        "title": "Correction to âStacking and Chaining of Normalization Methods in Deep Learning-Based Classification of Colorectal Cancer Using Gut Microbiome Dataâ",
        "authors": "Mwenge Mulenga, Sameem Abdul Kareem, Aznul Qalid Md Sabri, Manjeevan Seera",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3269309"
    },
    {
        "id": 14752,
        "title": "A fast normalization and de-speckled method for skin optical coherence tomography image via deep learning",
        "authors": "Jarjish Rahaman, Brandon Lukas, Julia May, Carolina Puyana, Maria Tsoukas, Kamran Avanaki",
        "published": "2023-3-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2651211"
    },
    {
        "id": 14753,
        "title": "Enhancing sentence semantic matching with isotropic batch normalization and generalized pooling operator",
        "authors": "Yingjie Shuai",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3026682"
    },
    {
        "id": 14754,
        "title": "Attention Mechanism and Representative Batch Normalization Model for Scene Text Recognition",
        "authors": "Ling Wang, Kexin Luo, Peng Wang, Yane Bai",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiecs59936.2023.10435519"
    },
    {
        "id": 14755,
        "title": "Tooth and Supporting Tissue Anomalies Detection from Panoramic Radiography Using Integrating Convolution Neural Network with Batch Normalization",
        "authors": "",
        "published": "2024-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0430.19"
    },
    {
        "id": 14756,
        "title": "Nuclei Segmentation in Histopathology Images Using Structure-Preserving Color Normalization Based Ensemble Deep Learning Frameworks",
        "authors": "Manas Ranjan Prusty, Rishi Dinesh, Hariket Sukesh Kumar Sheth, Alapati Lakshmi Viswanath, Sandeep Kumar Satapathy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.042718"
    },
    {
        "id": 14757,
        "title": "A New Approach for Fault Diagnosis of Rolling Bearings Based on Adaptive Batch Normalization and Attention Mechanism",
        "authors": "Jingwen Hu, Yashun Wang, Xun Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3850/978-981-18-8071-1_p718-cd"
    },
    {
        "id": 14758,
        "title": "An Effective Recurrent Neural Network For Image Denoising With Long Short-Term Memory Founded Batch Normalization",
        "authors": "Girish Kalele",
        "published": "2023-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ihcsp56702.2023.10127178"
    },
    {
        "id": 14759,
        "title": "BEENE: deep learning-based nonlinear embedding improves batch effect estimation",
        "authors": "Md Ashiqur Rahman, Abdullah Aman Tutul, Mahfuza Sharmin, Md Shamsuzzoha Bayzid",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Abstract\n\nMotivation\nAnalyzing large-scale single-cell transcriptomic datasets generated using different technologies is challenging due to the presence of batch-specific systematic variations known as batch effects. Since biological and technological differences are often interspersed, detecting and accounting for batch effects in RNA-seq datasets are critical for effective data integration and interpretation. Low-dimensional embeddings, such as principal component analysis (PCA) are widely used in visual inspection and estimation of batch effects. Linear dimensionality reduction methods like PCA are effective in assessing the presence of batch effects, especially when batch effects exhibit linear patterns. However, batch effects are inherently complex and existing linear dimensionality reduction methods could be inadequate and imprecise in the presence of sophisticated nonlinear batch effects.\n\n\nResults\nWe present Batch Effect Estimation using Nonlinear Embedding (BEENE), a deep nonlinear auto-encoder network which is specially tailored to generate an alternative lower dimensional embedding suitable for both linear and nonlinear batch effects. BEENE simultaneously learns the batch and biological variables from RNA-seq data, resulting in an embedding that is more robust and sensitive than PCA embedding in terms of detecting and quantifying batch effects. BEENE was assessed on a collection of carefully controlled simulated datasets as well as biological datasets, including two technical replicates of mouse embryogenesis cells, peripheral blood mononuclear cells from three largely different experiments and five studies of pancreatic islet cells.\n\n\nAvailability and implementation\nBEENE is freely available as an open source project at https://github.com/ashiq24/BEENE.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btad479"
    },
    {
        "id": 14760,
        "title": "Mini-Batch Alignment: A Deep-Learning Model for Domain Factor-Independent Feature Extraction for Wi-FiâCSI Data",
        "authors": "Bram van Berlo, Camiel Oerlemans, Francesca Luigia Marogna, Tanir Ozcelebi, Nirvana Meratnia",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "Unobtrusive sensing (device-free sensing) aims to embed sensing into our daily lives. This is achievable by re-purposing communication technologies already used in our environments. Wireless Fidelity (Wi-Fi) sensing, using Channel State Information (CSI) measurement data, seems to be a perfect fit for this purpose since Wi-Fi networks are already omnipresent. However, a big challenge in this regard is CSI data being sensitive to âdomain factorsâ such as the position and orientation of a subject performing an activity or gesture. Due to these factors, CSI signal disturbances vary, causing domain shifts. Shifts lead to the lack of inference generalization, i.e., the model does not always perform well on unseen data during testing. We present a domain factor-independent feature-extraction pipeline called âmini-batch alignmentâ. Mini-batch alignment steers a feature-extraction modelâs training process such that it is unable to separate intermediate feature-probability density functions of input data batches seen previously from the current input data batch. By means of this steering technique, we hypothesize that mini-batch alignment (i) absolves the need for providing a domain label, (ii) reduces pipeline re-building and re-training likelihood when encountering latent domain factors, and (iii) absolves the need for extra model storage and training time. We test this hypothesis via a vast number of performance-evaluation experiments. The experiments involve both one- and two-domain-factor leave-out cross-validation, two open-source gesture-recognition datasets called SignFi and Widar3, two pre-processed input types called Doppler Frequency Spectrum (DFS) and Gramian Angular Difference Field (GADF), and several existing domain-shift mitigation techniques. We show that mini-batch alignment performs on a par with other domain-shift mitigation techniques in both position and orientation one-domain leave-out cross-validation using the Widar3 dataset and DFS as input type. When considering a memory-complexity-reduced version of the GADF as input type, mini-batch alignment shows hints of recuperating performance regarding a standard baseline model to the extent that no additional performance due to weight steering is lost in both one-domain-factor leave-out and two-orientation-domain-factor leave-out cross-validation scenarios. However, this is not enough evidence that the mini-batch alignment hypothesis is valid. We identified pitfalls leading up to the hypothesis invalidation: (i) lack of good-quality benchmark datasets, (ii) invalid probability distribution assumptions, and (iii) non-linear distribution scaling issues.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23239534"
    },
    {
        "id": 14761,
        "title": "Batch-balanced focal loss: a hybrid solution to class imbalance in deep learning",
        "authors": "Jatin Singh, Cameron Beeche, Zhiyi Shi, Oliver Beale, Boris Rosin, Joseph Leader, Jiantao Pu",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/1.jmi.10.5.051809"
    },
    {
        "id": 14762,
        "title": "Why Batch Normalization Damage Federated Learning on Non-IID Data?",
        "authors": "Yanmeng Wang, Qingjiang Shi, Tsung-Hui Chang",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3323302"
    },
    {
        "id": 14763,
        "title": "A Deep Learning Model for the Normalization of Institution Names by Multisource Literature Feature Fusion: Algorithm Development Study",
        "authors": "Yifei Chen, Xiaoying Li, Aihua Li, Yongjie Li, Xuemei Yang, Ziluo Lin, Shirui Yu, Xiaoli Tang",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "\nBackground\nThe normalization of institution names is of great importance for literature retrieval, statistics of academic achievements, and evaluation of the competitiveness of research institutions. Differences in authorsâ writing habits and spelling mistakes lead to various names of institutions, which affects the analysis of publication data. With the development of deep learning models and the increasing maturity of natural language processing methods, training a deep learningâbased institution name normalization model can increase the accuracy of institution name normalization at the semantic level.\n\n\nObjective\nThis study aimed to train a deep learningâbased model for institution name normalization based on the feature fusion of affiliation data from multisource literature, which would realize the normalization of institution name variants with the help of authority files and achieve a high specification accuracy after several rounds of training and optimization.\n\n\nMethods\nIn this study, an institution name normalizationâoriented model was trained based on bidirectional encoder representations from transformers (BERT) and other deep learning models, including the institution classification model, institutional hierarchical relation extraction model, and institution matching and merging model. The model was then trained to automatically learn institutional features by pretraining and fine-tuning, and institution names were extracted from the affiliation data of 3 databases to complete the normalization process: Dimensions, Web of Science, and Scopus.\n\n\nResults\nIt was found that the trained model could achieve at least 3 functions. First, the model could identify the institution name that is consistent with the authority files and associate the name with the files through the unique institution ID. Second, it could identify the nonstandard institution name variants, such as singular forms, plural changes, and abbreviations, and update the authority files. Third, it could identify the unregistered institutions and add them to the authority files, so that when the institution appeared again, the model could identify and regard it as a registered institution. Moreover, the test results showed that the accuracy of the normalization model reached 93.79%, indicating the promising performance of the model for the normalization of institution names.\n\n\nConclusions\nThe deep learningâbased institution name normalization model trained in this study exhibited high accuracy. Therefore, it could be widely applied in the evaluation of the competitiveness of research institutions, analysis of research fields of institutions, and construction of interinstitutional cooperation networks, among others, showing high application value.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/47434"
    },
    {
        "id": 14764,
        "title": "How does Layer Normalization improve Batch Normalization in self-supervised sound source localization?",
        "authors": "Tianyu Liu, Peng Zhang, Wei Huang, Yufei Zha, Tao You, Yanning Zhang",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.127040"
    },
    {
        "id": 14765,
        "title": "Optimization of Fed-Batch Bakerâs Yeast Fermentation Using Deep Reinforcement Learning",
        "authors": "Wan Ying Chai, Min Keng Tan, Kenneth Tze Kin Teo, Heng Jin Tham",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s41660-024-00406-6"
    },
    {
        "id": 14766,
        "title": "Multi-batch single-cell comparative atlas construction by deep learning disentanglement",
        "authors": "Allen W. Lynch, Myles Brown, Clifford A. Meyer",
        "published": "2023-7-12",
        "citations": 1,
        "abstract": "AbstractCell state atlases constructed through single-cell RNA-seq and ATAC-seq analysis are powerful tools for analyzing the effects of genetic and drug treatment-induced perturbations on complex cell systems. Comparative analysis of such atlases can yield new insights into cell state and trajectory alterations. Perturbation experiments often require that single-cell assays be carried out in multiple batches, which can introduce technical distortions that confound the comparison of biological quantities between different batches. Here we propose CODAL, a variational autoencoder-based statistical model which uses a mutual information regularization technique to explicitly disentangle factors related to technical and biological effects. We demonstrate CODALâs capacity for batch-confounded cell type discovery when applied to simulated datasets and embryonic development atlases with gene knockouts. CODAL improves the representation of RNA-seq and ATAC-seq modalities, yields interpretable modules of biological variation, and enables the generalization of other count-based generative models to multi-batched data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41467-023-39494-2"
    },
    {
        "id": 14767,
        "title": "Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization",
        "authors": "Yefei He, Luoming Zhang, Weijia Wu, Hong Zhou",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11338-6"
    },
    {
        "id": 14768,
        "title": "Classification of Alzheimerâs and Parkinsonâs Disease Based on VGG19 Features with Batch Normalization",
        "authors": "Et al. Suganya A,",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "Dementia is a condition when thinking, reasoning and memory skills are lost and patients have emotional instability and personality changes. Researchers are looking into how the underlying disease processes that lead to various kinds of dementia begin and interact. Additionally, they keep researching the various diseases and conditions that cause dementia. Alzheimerâs and Parkinson's disease contribute to dementia development. Recently deep learning-based techniques have surpassed the performance of traditional algorithms in the field of machine vision, image detection, natural language handling, object detection, and medical image analysis. This study proposed a transfer learning-based model for Parkinsonâs and Alzheimerâs disease classification from slices of MRI. Pretrained VGG19 with Batch normalization is used for feature extraction and the final dense (fully connected-FC) layers are fine-tuned to meet our requirements. The performance of the model is analyzed by varying hyperparameters. The proposed model outperformed other pre-trained CNN models by achieving an accuracy of 97.19%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10.8762"
    },
    {
        "id": 14769,
        "title": "Accelerating Direct Normalization Data Acquisition in PET Imaging with Deep Learning",
        "authors": "M. Jafaritadi, A. Groll, M. Chin, G. Chinn, J. Fisher, D. Innes, C. S. Levin",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nssmicrtsd49126.2023.10338673"
    },
    {
        "id": 14770,
        "title": "Location-Aware Adaptive Normalization: A Deep Learning Approach for Wildfire Danger Forecasting",
        "authors": "Mohamad Hakam Shams Eddin, Ribana Roscher, Juergen Gall",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tgrs.2023.3285401"
    },
    {
        "id": 14771,
        "title": "Where Position MattersâDeep-LearningâDriven Normalization and Coregistration of Computed Tomography in the Postoperative Analysis of Deep Brain Stimulation",
        "authors": "Marco Reisert, Bastian E.A. Sajonz, Timo S. Brugger, Peter C. Reinacher, Maximilian F. Russe, Elias Kellner, Henrik Skibbe, Volker A. Coenen",
        "published": "2023-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neurom.2022.10.042"
    },
    {
        "id": 14772,
        "title": "A Migration Learning Method Based on Adaptive Batch Normalization Improved Rotating Machinery Fault Diagnosis",
        "authors": "Xueyi Li, Tianyu Yu, Daiyou Li, Xiangkai Wang, Cheng Shi, Zhijie Xie, Xiangwei Kong",
        "published": "2023-5-15",
        "citations": 3,
        "abstract": "Sustainable development has become increasingly important as one of the key research directions for the future. In the field of rotating machinery, stable operation and sustainable performance are critical, focusing on the fault diagnosis of component bearings. However, traditional normalization methods are ineffective in target domain data due to the difference in data distribution between the source and target domains. To overcome this issue, this paper proposes a bearing fault diagnosis method based on the adaptive batch normalization algorithm, which aims to enhance the generalization ability of the model in different data distributions and environments. The adaptive batch normalization algorithm improves the adaptability and generalization ability to better respond to changes in data distribution and the real-time requirements of practical applications. This algorithm replaces the statistical values in a BN with domain adaptive mean and variance statistics to minimize feature differences between two different domains. Experimental results show that the proposed method outperforms other methods in terms of performance and generalization ability, effectively solving the problems of data distribution changes and real-time requirements in bearing fault diagnosis. The research results indicate that the adaptive batch normalization algorithm is a feasible method to improve the accuracy and reliability of bearing fault diagnosis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su15108034"
    },
    {
        "id": 14773,
        "title": "Batch Normalization based Convolutional Block YOLOv3 Real Time Object Detection of Moving Images with Backdrop Adjustment",
        "authors": "D. Umanandhini, M. Shyamala Devi, N Beulah Jabaseeli, S. Sridevi",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscc59169.2023.10334988"
    },
    {
        "id": 14774,
        "title": "Batch normalization-free weight-binarized SNN based on hardware-saving IF neuron",
        "authors": "G.C. Qiao, N. Ning, Y. Zuo, P.J. Zhou, M.L. Sun, S.G. Hu, Q. Yu, Y. Liu",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126234"
    },
    {
        "id": 14775,
        "title": "Embedding active learning in batch-to-batch optimization using reinforcement learning",
        "authors": "Ha-Eun Byun, Boeun Kim, Jay H. Lee",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.automatica.2023.111260"
    },
    {
        "id": 14776,
        "title": "Unraveling Batch Normalization for Realistic Test-Time Adaptation",
        "authors": "Zixian Su, Jingwei Guo, Kai Yao, Xi Yang, Qiufeng Wang, Kaizhu Huang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "While recent test-time adaptations exhibit efficacy by adjusting batch normalization to narrow domain disparities, their effectiveness diminishes with realistic mini-batches due to inaccurate target estimation. As previous attempts merely introduce source statistics to mitigate this issue, the fundamental problem of inaccurate target estimation still persists, leaving the intrinsic test-time domain shifts unresolved. This paper delves into the problem of mini-batch degradation. By unraveling batch normalization, we discover that the inexact target statistics largely stem from the substantially reduced class diversity in batch. Drawing upon this insight, we introduce a straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge the class diversity gap between training and testing batches. Importantly, our TEMA adaptively extends the scope of typical methods beyond the current batch to incorporate a diverse set of class information, which in turn boosts an accurate target estimation. Built upon this foundation, we further design a novel layer-wise rectification strategy to consistently promote test-time performance. Our proposed method enjoys a unique advantage as it requires neither training nor tuning parameters, offering a truly hassle-free solution. It significantly enhances model robustness against shifted domains and maintains resilience in diverse real-world scenarios with various  batch sizes, achieving state-of-the-art performance on several major benchmarks.  Code is available at https://github.com/kiwi12138/RealisticTTA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i13.29436"
    },
    {
        "id": 14777,
        "title": "CNN with Batch Normalization Adjustment for Offline Hand-written Signature Genuine Verification",
        "authors": "Wifda Muna Fatihia, Arna Fariza, Tita Karlita",
        "published": "2023-2-13",
        "citations": 0,
        "abstract": "Signature genuine verifications of offline hand-written signatures are critical for preventing forgery and fraud. With the growth of protecting personal identity and preventing fraud, the demand for an automatic system for signature verification is high. The signature verification system is then studied by many researchers using various methods, especially deep learning-based methods. Hence, deep learning has a problem. Deep learning requires much training time for the data to obtain the best model accuracy result. Therefore, this paper proposed a CNN Batch Normalization, the CNN architectural adaptation model with a normalization batch number added, to obtain a CNN model optimization with high accuracy and less training time for offline hand-written signature verification. We compare CNN with our proposed model in the experiments. The research method in this study is data collection, pre-processing, and testing using our private signature dataset (collected by capturing signature images using a smartphone), which becomes the difficulties of our study because of the different lighting, media, and pen used to sign. Experiment results show that our model ranks first, with a training accuracy of 88.89%, an accuracy validation of 75.93%, and a testing accuracy of 84.84%Ã¢â¬âalso, the result of 2638.63 s for the training time consumed with CPU usage. The model evaluation results show that our model has a smaller EER value; 2.583, with FAR = 0.333 and FRR = 4.833. Although the results of our proposed model are better than basic CNN, it is still low and overfitted. It has to be enhanced by better pre-processing steps using another augmentation method required to improve dataset quality.ÃÂ ",
        "keywords": "",
        "link": "http://dx.doi.org/10.30630/joiv.7.1.1443"
    },
    {
        "id": 14778,
        "title": "Effect of color-normalization on deep learning segmentation models for tumor-infiltrating lymphocytes scoring using breast cancer histopathology images",
        "authors": "Arian Arab, Victor Garcia, Shuyue Guan, Brandon D. Gallas, Berkman Sahiner, Nicholas Petrick, Weijie Chen",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2653989"
    },
    {
        "id": 14779,
        "title": "Block Attention and Switchable Normalization Based Deep Learning Framework for Segmentation of Retinal Vessels",
        "authors": "Sabri Deari, Ilkay Oksuz, Sezer Ulukaya",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3265729"
    },
    {
        "id": 14780,
        "title": "Profitability related industrial-scale batch processes monitoring via deep learning based soft sensor development",
        "authors": "Cheng Ji, Fangyuan Ma, Jingde Wang, Wei Sun",
        "published": "2023-2",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compchemeng.2022.108125"
    },
    {
        "id": 14781,
        "title": "Normalization of audio signals for the needs of machine learning",
        "authors": "Nebojsa Simic, Ana Gavrovska",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/telfor59449.2023.10372705"
    },
    {
        "id": 14782,
        "title": "Membrane Potential Batch Normalization for Spiking Neural Networks",
        "authors": "Yufei Guo, Yuhan Zhang, Yuanpei Chen, Weihang Peng, Xiaode Liu, Liwen Zhang, Xuhui Huang, Zhe Ma",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01779"
    },
    {
        "id": 14783,
        "title": "Image Intensity Normalization Benefits Deep Learning Brain PET Motion Correction",
        "authors": "E. V. Lieffrig, J. Zhang, T. Zeng, Z. Cai, C. You, Y. Lu, J. A. Onofrey",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nssmicrtsd49126.2023.10338194"
    },
    {
        "id": 14784,
        "title": "Model-level attention and batch-instance style normalization for federated learning on medical image segmentation",
        "authors": "Fubao Zhu, Yanhui Tian, Chuang Han, Yanting Li, Jiaofen Nan, Ni Yao, Weihua Zhou",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2024.102348"
    },
    {
        "id": 14785,
        "title": "Training SNNs Low Latency Utilizing Batch Normalization Through Time and Iterative Initialization Retraining",
        "authors": "Thi Diem Tran, Huu Han Hoang",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067096"
    },
    {
        "id": 14786,
        "title": "Detection of alternative splicing: deep sequencing or deep learning",
        "authors": "Lena Maria Hackl, Jan Baumbach, Olga Tsoy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14293/gof.23.032"
    },
    {
        "id": 14787,
        "title": "Spatial Normalization to Improve Deep Learning-based Head Motion Correction in PET",
        "authors": "J. Zhang, E. V. Lieffrig, T. Zeng, C. You, Z. Cai, T. Toyonaga, Y. Lu, J. A. Onofrey",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nssmicrtsd49126.2023.10338387"
    },
    {
        "id": 14788,
        "title": "Deep Chinese Teaching and Learning Model Based on Deep Learning",
        "authors": "Yipu Wang, Stuart Perrin",
        "published": "2024",
        "citations": 0,
        "abstract": "Deep learning is a more situational and reflective way of learning that integrates complex knowledge and skills into intuitive thinking. As a language that closely combines sound, form and meaning, Chinese teaching and learning from the perspective of deep learning can help break through the limitations of the current teaching model that only focuses on certain language knowledge or cultural behaviors. This paper combines deep learning with international Chinese education, creates deep Chinese teaching and learning model including âfour stages and ten stepsâ, and carries out practical application and teaching effect test. The results show that the deep Chinese teaching and learning model is conducive to improving studentsâ discourse presentation ability and comprehensive skills, cultivating the learnersâ autonomous learning ability and intercultural communication competence, and strengthening the integration of language teaching and cultural teaching. At the same time, this model also has some limitations, need to be further adjusted and optimized.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/ijlll.2024.10.1.479"
    },
    {
        "id": 14789,
        "title": "An improved deep Q-learning algorithm for a trade-off between energy consumption and productivity in batch scheduling",
        "authors": "Xu Zheng, Zhen Chen",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cie.2024.109925"
    },
    {
        "id": 14790,
        "title": "Permutation Invariant Individual Batch Learning",
        "authors": "Yaniv Fogel, Meir Feder",
        "published": "2023-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itw55543.2023.10161673"
    },
    {
        "id": 14791,
        "title": "Towards Defending Multiple $$\\ell _p$$-Norm Bounded Adversarial Perturbations via Gated Batch Normalization",
        "authors": "Aishan Liu, Shiyu Tang, Xinyun Chen, Lei Huang, Haotong Qin, Xianglong Liu, Dacheng Tao",
        "published": "2023-9-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11263-023-01884-w"
    },
    {
        "id": 14792,
        "title": "Human Bone Age Estimation of Carpal Bone X-Ray Using Residual Network with Batch Normalization Classification",
        "authors": "Anisah Nabilah, Riyanto Sigit, Arna Fariza, Madyono Madyono",
        "published": "2023-1-21",
        "citations": 1,
        "abstract": "Bone age is an index used by pediatric radiology and endocrinology departments worldwide to define skeletal maturity for medical and non-medical purposes. In general, the clinical method for bone age assessment (BAA) is based on examining the visual ossification of individual bones in the left hand and then comparing it with a standard radiographic atlas of the hand. However, this method is highly dependent on the experience and conditions of the forensic expert. This paper proposes a new approach to age estimation of human bone based on the carpal bones in the hand and using a residual network architecture. The classification layer was modified with batch normalization to optimize the training process. Before carrying out the training process, we performed an image augmentation technique to make the dataset more varied. The following augmentation techniques were used: resizing; random affine transformation; horizontal flipping; adjusting brightness, contrast, saturation, and hue; and image inversion. The output is the classification of bone age in the range of 1 to 19 years. The results obtained when using a VGG16 model were an MAE value of 5.19 and an R2 value of 0.56 while using the newly developed ResNeXt50(32x4d) model produced an MAE value of 4.75 and an R2 value of 0.63. The research results indicate that the proposed modification of the residual training model improved classification compared to using the VGG16 model, as indicated by an MAE value of 4.75 and an R2 value of 0.63.",
        "keywords": "",
        "link": "http://dx.doi.org/10.30630/joiv.7.1.1024"
    },
    {
        "id": 14793,
        "title": "Batch Normalization Is Blind to the First and Second Derivatives of the Loss",
        "authors": "Zhanpeng Zhou, Wen Shen, Huixin Chen, Ling Tang, Yuefeng Chen, Quanshi Zhang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "We prove that when we do the Taylor series expansion of the loss function, the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. We also find that such a problem is caused by the standardization phase of the BN operation. We believe that proving the blocking of certain loss terms provides an analytic perspective for potential detects of a deep model with BN operations, although the blocking problem is not fully equivalent to significant damages in all tasks on benchmark datasets. Experiments show that the BN operation significantly affects feature representations in specific tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i18.29978"
    },
    {
        "id": 14794,
        "title": "Change detection over the Aral Sea using relative radiometric normalization based on deep learning",
        "authors": "Taeheon Kim, Yerin Yun, Seonyoung Park, Jaehong Oh, Youkyung Han",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/2150704x.2023.2242589"
    },
    {
        "id": 14795,
        "title": "Improving generalization capability of deep learning-based nuclei instance segmentation by non-deterministic train time and deterministic test time stain normalization",
        "authors": "Amirreza Mahbod, Georg Dorffner, Isabella Ellinger, Ramona Woitek, Sepideh Hatamikia",
        "published": "2024-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csbj.2023.12.042"
    },
    {
        "id": 14796,
        "title": "Batch normalization followed by merging is powerful for phenotype prediction integrating multiple heterogeneous studies",
        "authors": "Yilin Gao, Fengzhu Sun",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "Heterogeneity in different genomic studies compromises the performance of machine learning models in cross-study phenotype predictions. Overcoming heterogeneity when incorporating different studies in terms of phenotype prediction is a challenging and critical step for developing machine learning algorithms with reproducible prediction performance on independent datasets. We investigated the best approaches to integrate different studies of the same type of omics data under a variety of different heterogeneities. We developed a comprehensive workflow to simulate a variety of different types of heterogeneity and evaluate the performances of different integration methods together with batch normalization by using ComBat. We also demonstrated the results through realistic applications on six colorectal cancer (CRC) metagenomic studies and six tuberculosis (TB) gene expression studies, respectively. We showed that heterogeneity in different genomic studies can markedly negatively impact the machine learning classifierâs reproducibility. ComBat normalization improved the prediction performance of machine learning classifier when heterogeneous populations are present, and could successfully remove batch effects within the same population. We also showed that the machine learning classifierâs prediction accuracy can be markedly decreased as the underlying disease model became more different in training and test populations. Comparing different merging and integration methods, we found that merging and integration methods can outperform each other in different scenarios. In the realistic applications, we observed that the prediction accuracy improved when applying ComBat normalization with merging or integration methods in both CRC and TB studies. We illustrated that batch normalization is essential for mitigating both population differences of different studies and batch effects. We also showed that both merging strategy and integration methods can achieve good performances when combined with batch normalization. In addition, we explored the potential of boosting phenotype prediction performance by rank aggregation methods and showed that rank aggregation methods had similar performance as other ensemble learning approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1371/journal.pcbi.1010608"
    },
    {
        "id": 14797,
        "title": "On data normalization and batch-effect correction for tumor subtyping with microRNA data",
        "authors": "Yilin Wu, Becky Wing-Yan Yuen, Yingying Wei, Li-Xuan Qin",
        "published": "2023-1-10",
        "citations": 1,
        "abstract": "Abstract\nThe discovery of new tumor subtypes has been aided by transcriptomics profiling. However, some new subtypes can be irreproducible due to data artifacts that arise from disparate experimental handling. To deal with these artifacts, methods for data normalization and batch-effect correction have been utilized before performing sample clustering for disease subtyping, despite that these methods were primarily developed for group comparison. It remains to be elucidated whether they are effective for sample clustering. We examined this issueÂ with a re-sampling-based simulation study that leverages a pair of microRNA microarrayÂ data sets. Our study showed that (i) normalization generally benefited the discovery of sample clusters and quantile normalization tended to be the best performer, (ii) batch-effect correction was harmful when data artifacts confounded with biological signals,Â and (iii) their performance can be influenced by the choice of clustering method with the Prediction Around Medoid method based on Pearson correlation being consistently a best performer. Our study provides important insights on the use of data normalization and batch-effect correction in connection with the design of array-to-sample assignment and the choice of clustering method for facilitating accurate and reproducible discovery of tumor subtypes with microRNAs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/nargab/lqac100"
    },
    {
        "id": 14798,
        "title": "Probabilistic Bayesian Deep Learning Approach for Online Forecasting of Fed-Batch Fermentation",
        "authors": "Tao Wang, Jiebing You, Xiugang Gong, Shanliang Yang, Lei Wang, Zheng Chang",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acsomega.3c02387"
    },
    {
        "id": 14799,
        "title": "Performance Analysis of Extensional Deep Learning And Boosted Intensional Deep Learning Method",
        "authors": "",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/nq.2022.20.3.nq22955"
    },
    {
        "id": 14800,
        "title": "Author Index",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00012"
    },
    {
        "id": 14801,
        "title": "Transfer learning for bearing fault diagnosis: adaptive batch normalization and combined optimization method",
        "authors": "Xueyi Li, Kaiyu Su, Daiyou Li, Qiushi He, Zhijie Xie, Xiangwei Kong",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "Abstract\nBearings are crucial components in rotating machinery equipment. Bearing fault diagnosis plays a significant role in the maintenance of mechanical equipment. This study aims to enhance the practicality of bearing fault diagnosis to meet real-world engineering requirements. In real industrial environments, the continuously changing operating conditions such as equipment speed and load pose challenges in collecting data for bearing fault diagnosis, as it is challenging to gather data for all operational conditions. This paper proposes a transfer learning approach for bearing fault diagnosis based on adaptive batch normalization (AdaBN) and a combined optimization algorithm. Initially, a ResNet neural network is trained using source domain data. Subsequently, the trained model is transferred to the target domain, where AdaBN is applied to mitigate domain shift issues. Furthermore, a combined optimization algorithm is employed during model training to enhance fault diagnosis accuracy. Experimental validation is conducted using bearing data from the Case Western Reserve University dataset and Northeast Forestry University (NEFU) dataset. Comparison shows that AdaBN and the combined optimization algorithm improve bearing fault diagnosis accuracy effectively. On the NEFU dataset, the diagnostic accuracy exceeds 95%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad19c2"
    },
    {
        "id": 14802,
        "title": "Title Page",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00002"
    },
    {
        "id": 14803,
        "title": "Maximum Gaussianality training for deep speaker vector normalization",
        "authors": "Yunqi Cai, Lantian Li, Andrew Abel, Xiaoyan Zhu, Dong Wang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109977"
    },
    {
        "id": 14804,
        "title": "Copyright Page",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00003"
    },
    {
        "id": 14805,
        "title": "Program Committee",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00007"
    },
    {
        "id": 14806,
        "title": "Interval Type-2 Fuzzy Logic System Based on Batch Normalization and Uniform Regularization with Application to Time Series Forecasting",
        "authors": "Hui Zhang, Bo Sun, Lizhi Zhang, Liang Zhang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450808"
    },
    {
        "id": 14807,
        "title": "Classification of Skin Lesions by Incorporating Drop-Block and Batch Normalization Layers in Representative CNN Models",
        "authors": "T. R. Vijaya Lakshmi, Ch. Venkata Krishna Reddy",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13369-023-08131-x"
    },
    {
        "id": 14808,
        "title": "Organizing Committee",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00006"
    },
    {
        "id": 14809,
        "title": "Half Title Page",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00001"
    },
    {
        "id": 14810,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 14811,
        "title": "Table of Contents",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00004"
    },
    {
        "id": 14812,
        "title": "The adsorption and release mechanism of different aged microplastics toward Hg(II) via batch experiment and the deep learning method",
        "authors": "Lianghong Li, Bin Xue, Haiying Lin, Wenlu Lan, Xinyi Wang, Junqi Wei, Mingen Li, Mingzhi Li, Yu Duan, Jiatong Lv, Zixuan Chen",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chemosphere.2023.141067"
    },
    {
        "id": 14813,
        "title": "Effects of Spectral Normalization in Multi-Agent Reinforcement Learning",
        "authors": "Kinal Mehta, Anuj Mahajan, Pawan Kumar",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191226"
    },
    {
        "id": 14814,
        "title": "Adversarial Attacks and Batch Normalization: A Batch Statistics Perspective",
        "authors": "Awais Muhammad, Fahad Shamshad, Sung-Ho Bae",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3250661"
    },
    {
        "id": 14815,
        "title": "Message from the Organizers",
        "authors": "",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00005"
    },
    {
        "id": 14816,
        "title": "Analysis of Differences in Learning Achievement Reviewed From Learning Style (Study on Biology Students Batch 2021)",
        "authors": "Dian Dwi Putri Ulan Sari Patongai",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "Abstract. Tracking learning style profiles is an important thing to do in an educational institution, including universities, to support the success and learning achievements of students. At the university level, learning achievement is indicated by the achievement index (GPA) at the end of each semester. This research aims to determine the profile of learning styles and learning achievements of students in the biology education study program and to see whether there are differences in student learning achievements in terms of their learning styles. This research is descriptive research with a quantitative approach. . The population in this study were students from the 2021 Biology Education Study Program with a total of 108 students. The research sample was 86 students from the Biology Education Study Program Class of 2021 out of 108 students who were determined using the Simple Random Sampling technique. The instrument used in this research is a non-test instrument in the form of a learning style questionnaire. The research results obtained include: the learning style tendencies of biology students in the Class of 2021 are varied, 45% of students tend to have a Kinesthetic learning style, 40% Visual and 15% Auditory. The average learning achievement of the 2021 batch of biology students is in the good category. Hypothesis testing shows that there is no significant difference in learning achievement when viewed from the learning styles of biology students.Key words: Learning Style, Learning achievement, Differences of learning Style, Biology Student",
        "keywords": "",
        "link": "http://dx.doi.org/10.35580/btl.v6i2.54140"
    },
    {
        "id": 14817,
        "title": "Questionable Practices in Methodological Deep Learning Research",
        "authors": "Daniel J. Trosten",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "Evaluation of new methodology in deep learning (DL) research is typically done by reporting point estimates of a few performance metrics, calculated from a single training run. This paper argues that this frequently used evaluation protocol in DL is fundamentally flawed -- presenting 8 questionable practices that are widely adopted in the evaluation of new DL methods. The questionable practices are derived from violations of statistical principles of the scientific method, and from Hansson's definition of pseudoscience. A survey of recent publications from a top-tier DL conference indicates the widespread adoption of these practices in state-of-the-art DL research. Lastly, arguments in favor of the questionable practices, possible reasons for their adoption, and measures that have been taken to remove them, are discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/18.6804"
    },
    {
        "id": 14818,
        "title": "Normalization effects on deep neural networks",
        "authors": "Jiahui Yu,  , Konstantinos Spiliopoulos",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/fods.2023004"
    },
    {
        "id": 14819,
        "title": "Dialect Representation Learning with Neural Dialect-to-Standard Normalization",
        "authors": "Olli Kuparinen, Yves Scherrer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.vardial-1.20"
    },
    {
        "id": 14820,
        "title": "RIDDLE: Rule Induction with Deep Learning",
        "authors": "Cosimo Persia, Ricardo GuimarÃ£es",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "Numerous applications rely on the efficiency of Deep Learning models to address complex classification tasks for critical decisions-making. However, we may not know how each feature in these models contributes towards the prediction. In contrast, Rule Induction algorithms provide an interpretable way to extract patterns from data, but traditional approaches suffer in terms of scalability. In this work, we bridge Deep Learning and Rule Induction and define the RIDDLE (Rule Induction with Deep Learning) architecture. We show that RIDDLE has state-of-the-art performance in Rule Induction via an empirical evaluation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/18.6801"
    },
    {
        "id": 14821,
        "title": "Adaptive Iterative Learning Control for Industry Batch Process with Time-Varying and Unknown Parameters",
        "authors": "Peiyuan Li, Panshuo Li",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166757"
    },
    {
        "id": 14822,
        "title": "Deep Learning Aided SRF Cavity Optimization for Quantum Computing",
        "authors": "Jovan Markovic, Doga Kurkcuoglu",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/1998921"
    },
    {
        "id": 14823,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 14824,
        "title": "Memory-Efficient Batch Normalization By One-Pass Computation for On-Device Training",
        "authors": "He Dai, Hang Wang, Xuchong Zhang, Hongbin Sun",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcsii.2024.3354738"
    },
    {
        "id": 14825,
        "title": "DWA-Watermarking Embedding Mechanism for Deep Learning Models",
        "authors": "Zi-Jie Huang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10165904"
    },
    {
        "id": 14826,
        "title": "Image Style Transfer Based on Deep Feature Rotation and Adaptive Instance Normalization",
        "authors": "Zichuan Yang, Yan Wu, Xuan Cao, Changyi He",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdacai59742.2023.00132"
    },
    {
        "id": 14827,
        "title": "Pedestrian flow monitoring system based on Deep learning pipeline",
        "authors": "Yifan Bu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10167254"
    },
    {
        "id": 14828,
        "title": "Classification of Grapevine Leaf Images with Deep Learning Ensemble Models",
        "authors": "Qingcong Lv",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10165757"
    },
    {
        "id": 14829,
        "title": "Data-Driven Knowledge Transfer in Batch Q* Learning",
        "authors": "Elynn Chen, Xi Chen, Wenbo Jing",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4787816"
    },
    {
        "id": 14830,
        "title": "Batch(Offline) Reinforcement Learning for Recommender System",
        "authors": "Mohammad Amir Rezaei Gazik, Mehdy Roayaei",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334722"
    },
    {
        "id": 14831,
        "title": "Generalizable Embeddings with Cross-Batch Metric Learning",
        "authors": "Yeti Z. GÃ¼rbÃ¼z, A. AydÄ±n Alatan",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222102"
    },
    {
        "id": 14832,
        "title": "Comparison of Image Normalization Methods for Multi-Site Deep Learning",
        "authors": "Steffen Albert, Barbara D. Wichtmann, Wenzhao Zhao, Angelika Maurer, JÃ¼rgen Hesser, Ulrike I. Attenberger, Lothar R. Schad, Frank G. ZÃ¶llner",
        "published": "2023-8-3",
        "citations": 2,
        "abstract": "In this study, we evaluate the influence of normalization on the performance of deep learning networks for tumor segmentation and the prediction of the pathological response of locally advanced rectal cancer to neoadjuvant chemoradiotherapy. The techniques were applied to a multicenter and multimodal magnet resonance imaging data set consisting of 201 patients recorded at six centers. We implemented and investigated six different normalization methods (setting the mean and standard deviation, histogram matching, percentiles, combining percentiles and histogram matching, fixed window and an auto-encoder with adversarial loss using the imaging parameters) and evaluated their impact on four deep learning tasks: tumor segmentation, prediction of treatment outcome, and prediction of sex and age. The latter two tasks were implemented as a reference test. We trained a modified U-Net with different normalization methods in multiple configurations: on all images, images from all centers except one, and images from a single center. Our results show that normalization only plays a minor role in segmentation, with a difference in Dice of less than 0.02 between the best and worst performing networks. For the prediction of sex and treatment outcomes, the percentile method combined with histogram matching works best for all scenarios. The biggest difference in performance, depending on the normalization method, occurs for classification. In conclusion, normalization is especially important for small data sets or for generalizing to different data distributions. The deep learning method was superior to the classical methods only in a minority of cases, probably due to the limited amount of training data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13158923"
    },
    {
        "id": 14833,
        "title": "Machine Learning and Deep Learning Approaches for Cybersecurity: A Review",
        "authors": "Migul Jain",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231023115126"
    },
    {
        "id": 14834,
        "title": "\"Deep Non-Parametic Learning Architecture:  Performance Analysis Of Deep Learning  Architecture\"",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56452/7-4-243"
    },
    {
        "id": 14835,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 14836,
        "title": "Research on Vehicle Object Detection Based on Deep Learning",
        "authors": "Ning Wang, Yinshan Jia",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166204"
    },
    {
        "id": 14837,
        "title": "CLAIRE: contrastive learning-based batch correction framework for better balance between batch mixing and preservation of cellular heterogeneity",
        "authors": "Xuhua Yan, Ruiqing Zheng, Fangxiang Wu, Min Li",
        "published": "2023-3-1",
        "citations": 2,
        "abstract": "AbstractMotivationIntegration of growing single-cell RNA sequencing datasets helps better understand cellular identity and function. The major challenge for integration is removing batch effects while preserving biological heterogeneities. Advances in contrastive learning have inspired several contrastive learning-based batch correction methods. However, existing contrastive-learning-based methods exhibit noticeable ad hoc trade-off between batch mixing and preservation of cellular heterogeneities (mix-heterogeneity trade-off). Therefore, a deliberate mix-heterogeneity trade-off is expected to yield considerable improvements in scRNA-seq dataset integration.ResultsWe develop a novel contrastive learning-based batch correction framework, CIAIRE, which achieves superior mix-heterogeneity trade-off. The key contributions of CLAIRE are proposal of two complementary strategies: construction strategy and refinement strategy, to improve the appropriateness of positive pairs. Construction strategy dynamically generates positive pairs by augmenting inter-batch mutual nearest neighbors (MNN) with intra-batch k-nearest neighbors (KNN), which improves the coverage of positive pairs for the whole distribution of shared cell types between batches. Refinement strategy aims to automatically reduce the potential false positive pairs from the construction strategy, which resorts to the memory effect of deep neural networks. We demonstrate that CLAIRE possesses superior mix-heterogeneity trade-off over existing contrastive learning-based methods. Benchmark results on six real datasets also show that CLAIRE achieves the best integration performance against eight state-of-the-art methods. Finally, comprehensive experiments are conducted to validate the effectiveness of CLAIRE.Availability and implementationThe source code and data used in this study can be found in https://github.com/CSUBioGroup/CLAIRE-release.Supplementary informationSupplementary data are available at Bioinformatics online.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btad099"
    },
    {
        "id": 14838,
        "title": "Domain Specific Batch Normalization Based on Adversarial Domain Adaptation Image Classification",
        "authors": "åæ è",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/airr.2023.122014"
    },
    {
        "id": 14839,
        "title": "A survey of gradient normalization adversarial attack methods",
        "authors": "Jun Chen, Qidong Huang, Yunpeng Zhang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3650215.3650258"
    },
    {
        "id": 14840,
        "title": "Cross-subject EEG linear domain adaption based on batch normalization and depthwise convolutional neural network",
        "authors": "Guofa Li, Delin Ouyang, Liu Yang, Qingkun Li, Kai Tian, Baiheng Wu, Gang Guo",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.111011"
    },
    {
        "id": 14841,
        "title": "Energy-Efficient Train Control Method Based on Batch Constrained Deep Q-Learning",
        "authors": "Ruoqing Li, Shuai Su, Yukun Gu, Lei Zhang, Hao Wang",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422583"
    },
    {
        "id": 14842,
        "title": "Artificial intelligence-driven microalgae autotrophic batch cultivation: A comparative study of machine and deep learning-based image classification models",
        "authors": "Jun Wei Roy Chong, Kuan Shiong Khoo, Kit Wayne Chew, Huong-Yong Ting, Koji Iwamoto, Roger Ruan, Zengling Ma, Pau Loke Show",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.algal.2024.103400"
    },
    {
        "id": 14843,
        "title": "Batch-Less Stochastic Gradient Descent for Compressive Learning of Deep Regularization for Image Denoising",
        "authors": "Hui Shi, Yann Traonmilin, Jean-FranÃ§ois Aujol",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10851-024-01178-x"
    },
    {
        "id": 14844,
        "title": "An Improved Deep Learning Based Test Case Prioritization Using Deep Reinforcement Learning",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.64"
    },
    {
        "id": 14845,
        "title": "Deep Convolutional Tables: Deep Learning Without Convolutions",
        "authors": "Shay Dekel, Yosi Keller, Aharon Bar-Hillel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3270402"
    },
    {
        "id": 14846,
        "title": "Evaluation of learning methods similar to deep learning and device using deep learning for the diagnosis of thyroid nodules",
        "authors": "Kwak Jin Young, Daham Kim, Kim Youngsook Kim, Eunjung Lee",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1530/endoabs.92.op-08-01"
    },
    {
        "id": 14847,
        "title": "Deep learning and deep transfer learning-based OPM for FMF systems",
        "authors": "M.A. Amirabadi, M.H. Kahaei, S.A. Nezamalhosseini",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.phycom.2023.102157"
    },
    {
        "id": 14848,
        "title": "DeepSHAP Summary for Adversarial Example Detection",
        "authors": "Yi-Ching Lin, Fang Yu",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00010"
    },
    {
        "id": 14849,
        "title": "Deep learning-based vehicle tracking and traffic event detection",
        "authors": "Guohao Sun, Kehuan Yan, Chengbin Fan",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166470"
    },
    {
        "id": 14850,
        "title": "Interactive Classification of Maize Seeds with Batch Mode Active Learning",
        "authors": "Ali GÃ¼neÅ, Emrah DÃ¶nmez",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asyu58738.2023.10296572"
    },
    {
        "id": 14851,
        "title": "Deep Transfer Learning in der Arbeitsplanung/Deep transfer learning in process planning â A concept for applying deep transfer learning in process planning using the example of manufacturing operations selection",
        "authors": "Marco Hussong, Moritz Glatt, Jan C. Aurich",
        "published": "2023",
        "citations": 0,
        "abstract": "FÃ¼r die Nutzung von Deep Learning zur UnterstÃ¼tzung der Prozesse innerhalb der Arbeitsplanung wird eine Vielzahl von Daten benÃ¶tigt. In der industriellen Praxis ist die Aufbereitung solcher DatensÃ¤tze sehr komplex und mit hohen Aufwand verbunden. Durch die Nutzung von Deep Transfer Learning kann die benÃ¶tigte Datenmenge reduziert werden. Am Beispiel der Fertigungsvorgangsermittlung wird ein Konzept vorgestellt, das die Anwendung von Deep Transfer Learning innerhalb der Arbeitsplanung ermÃ¶glicht. \nÂ \nA large amount of data is required for the use of deep learning to support process planning. In industrial practice, the preparation of such data sets is very complex and requires a lot of manual effort. By using deep transfer learning, the required amount of data can be reduced. Therefore, using the example of manufacturing operation selection, a concept is introduced that enables the application of deep transfer learning within process planning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37544/1436-4980-2023-06-16"
    },
    {
        "id": 14852,
        "title": "Deep Learning in Image Classification: An Overview",
        "authors": "Kejian Xu, Jinlong Chen, Yi Ning, Wei Tang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10167276"
    },
    {
        "id": 14853,
        "title": "EAN: An Efficient Attention Module Guided by Normalization for Deep Neural Networks",
        "authors": "Jiafeng Li, Zelin Li, Ying Wen",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in various fields, and two powerful techniques, feature normalization and attention mechanisms, have been widely used to enhance model performance. However, they are usually considered as two separate approaches or combined in a simplistic manner.\nIn this paper, we investigate the intrinsic relationship between feature normalization and attention mechanisms and propose an Efficient Attention module guided by Normalization, dubbed EAN. Instead of using costly fully-connected layers for attention learning, EAN leverages the strengths of feature normalization and incorporates an Attention Generation (AG) unit to re-calibrate features. The proposed AG unit exploits the normalization component as a measure of the importance of distinct features and generates an attention mask using GroupNorm, L2 Norm, and Adaptation operations. By employing a grouping, AG unit and aggregation strategy, EAN is established, offering a unified module that harnesses the advantages of both normalization and attention, while maintaining minimal computational overhead. Furthermore, EAN serves as a plug-and-play module that can be seamlessly integrated with classic backbone architectures. Extensive quantitative evaluations on various visual tasks demonstrate that EAN achieves highly competitive performance compared to the current state-of-the-art attention methods while sustaining lower model complexity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i4.28093"
    },
    {
        "id": 14854,
        "title": "DeepPatch: A Patching-Based Method for Repairing Deep Neural Networks",
        "authors": "Hao Bu, Meng Sun",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00011"
    },
    {
        "id": 14855,
        "title": "Truncated Normal Mixture Prior Based Deep Latent Model for Color Normalization of Histology Images",
        "authors": "Suman Mahapatra, Pradipta Maji",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmi.2023.3238425"
    },
    {
        "id": 14856,
        "title": "Lightweight real-time lane detection algorithm based on ghost convolution and self batch normalization",
        "authors": "Xuecun Yang, Wei Ji, Shanghui Zhang, Yijing Song, Lintao He, Hang Xue",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11554-023-01323-6"
    },
    {
        "id": 14857,
        "title": "Adaptive differential private in deep learning",
        "authors": "Hao Fei, Gehao Lu, Yaling Luo",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679158"
    },
    {
        "id": 14858,
        "title": "Deep Learning zur  Kariesdiagnostik",
        "authors": "Norbert KrÃ¤mer, Roland Frankenberger",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s44190-023-0647-4"
    },
    {
        "id": 14859,
        "title": "Automatic Detection for Road Voids from GPR Images using Deep Learning Method",
        "authors": "Qian Liu, Chuanhui Zhu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10167036"
    },
    {
        "id": 14860,
        "title": "Real time waste classification using deep learning and AV: Deep learning and implementation in the frontend",
        "authors": "Akshat Shrivastava",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "To combat climate change, accurate waste disposal is essential at the point of disposal. Strong greenhouse gases like methane are released into the atmosphere when items that might be recycled or composted are instead dumped in landfills. Current efforts to lessen the disposal of incorrect garbage are often costly, incorrect, and lengthy. In this project, we offer NoWa, an intuitive smartphone app that instantly categorises waste into recyclable or compost for consumers. NoWa uses highly efficient deep learning algorithms, using modern deep learning techniques and models. We have tested several convolution neural network topologies for garbage detection and classification. On the test set, our best model, a multi-layer deep learning residual convoluted neural network, has an accuracy of more than 95%- a number higher than what has ever been achieved in such models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47611/jsrhs.v12i2.4208"
    },
    {
        "id": 14861,
        "title": "Heterogeneous deep graph convolutional network with iterative deep graph learning for Covid-19 inline recommendation",
        "authors": "Zhefei Meng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424807"
    },
    {
        "id": 14862,
        "title": "Deep learning based single image deraining: datasets, metrics and methods",
        "authors": "Xinyi Liu",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679282"
    },
    {
        "id": 14863,
        "title": "Improvement of Prostate Cancer Aggressiveness Prediction Based on the Deep Learning Model Using Size Normalization and Multiple Loss Functions on Multi-parametric MR Images",
        "authors": "Yoon-Jo Kim, Julip Jung, Sung-Il Hwang, Helen Hong",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5626/jok.2023.50.10.866"
    },
    {
        "id": 14864,
        "title": "Towards Scalable Within-Season Crop Mapping With Phenology Normalization and Deep Learning",
        "authors": "Zijun Yang, Chunyuan Diao, Feng Gao",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jstars.2023.3237500"
    },
    {
        "id": 14865,
        "title": "Dynamics in Deep Classifiers Trained with the Square Loss: Normalization, Low Rank, Neural Collapse, and Generalization Bounds",
        "authors": "Mengjia Xu, Akshay Rangamani, Qianli Liao, Tomer Galanti, Tomaso Poggio",
        "published": "2023-1",
        "citations": 3,
        "abstract": "We overview several propertiesâold and newâof training overparameterized deep networks under the square loss. We first consider a model of the dynamics of gradient flow under the square loss in deep homogeneous rectified linear unit networks. We study the convergence to a solution with the absolute minimumÏ, which is the product of the Frobenius norms of each layer weight matrix, when normalization by Lagrange multipliers is used together with weight decay under different forms of gradient descent. A main property of the minimizers that bound their expected error for a specific network architecture isÏ. In particular, we derive novel norm-based bounds for convolutional layers that are orders of magnitude better than classical bounds for dense networks. Next, we prove that quasi-interpolating solutions obtained by stochastic gradient descent in the presence of weight decay have a bias toward low-rank weight matrices, which should improve generalization. The same analysis predicts the existence of an inherent stochastic gradient descent noise for deep networks. In both cases, we verify our predictions experimentally. We then predict neural collapse and its properties without any specific assumptionâunlike other published proofs. Our analysis supports the idea that the advantage of deep networks relative to other classifiers is greater for problems that are appropriate for sparse deep architectures such as convolutional neural networks. The reason is that compositionally sparse target functions can be approximated well by âsparseâ deep networks without incurring in the curse of dimensionality.",
        "keywords": "",
        "link": "http://dx.doi.org/10.34133/research.0024"
    },
    {
        "id": 14866,
        "title": "Comparative Analysis of Deep Learning Techniques",
        "authors": "Rutika Pawar",
        "published": "2024-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr24127165416"
    },
    {
        "id": 14867,
        "title": "Considering pickersâ learning effects in selecting between batch picking and batch-synchronized zone picking for online-to-offline groceries",
        "authors": "Jun Zhang, Xueyan Zhang, Ning Zhang",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apm.2022.09.009"
    },
    {
        "id": 14868,
        "title": "Achieving small-batch accuracy with large-batch scalability via Hessian-aware learning rate adjustment",
        "authors": "Sunwoo Lee, Chaoyang He, Salman Avestimehr",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.007"
    },
    {
        "id": 14869,
        "title": "Learning the Deep and the Shallow: Deep-Learning-Based Depth Phase Picking and Earthquake Depth Estimation",
        "authors": "Jannes MÃ¼nchmeyer, Joachim Saul, Frederik Tilmann",
        "published": "2023-10-17",
        "citations": 2,
        "abstract": "Abstract\nAutomated teleseismic earthquake monitoring is an essential part of global seismicity analysis. Although constraining epicenters in an automated fashion is an established technique, constraining event depths is substantially more difficult. One solution to this challenge is teleseismic depth phases, but these can currently not be identified precisely by automatic detection methods. Here, we propose two deep-learning models, DepthPhaseTEAM and DepthPhaseNet, to detect and pick depth phases. For training the models, we create a dataset based on the ISC-EHB bulletinâa high-quality catalog with detailed phase annotations. We show how backprojecting the predicted phase arrival probability curves onto the depth axis yields accurate estimates of earthquake depth. Furthermore, we show how a multistation model, DepthPhaseTEAM, leads to better and more consistent predictions than the single-station model, DepthPhaseNet. To allow direct application of our models, we integrate them within the SeisBench library.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1785/0220230187"
    },
    {
        "id": 14870,
        "title": "Two-Dimensional Model Predictive Iterative Learning Control based on Just-in-Time Learning Method for Batch Processes",
        "authors": "Chuangkai Zheng, Liuming Zhou, Feng Li",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166437"
    },
    {
        "id": 14871,
        "title": "Superâresolution using deep residual network with spectral normalization",
        "authors": "Yogendra Rao Musunuri, OhâSeol Kwon",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ell2.12734"
    },
    {
        "id": 14872,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 14873,
        "title": "Vulnerability Detection Using Two-Stage Deep Learning Models",
        "authors": "",
        "published": "2023-5-3",
        "citations": 0,
        "abstract": "Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary classification model) and the second stage is CNN-LTSM that classifies this vulnerability into a class of 50 different types of vulnerabilities (multiclass classification model). Experiments were done on SySeVR dataset. Results show an accuracy of 99% for the first and 98% for the second stage.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jctcsr.02.02.04"
    },
    {
        "id": 14874,
        "title": "Integrated deep learning and ensemble learning model for deep feature-based wheat disease detection",
        "authors": "Hatice Catal Reis, Veysel Turk",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.microc.2023.109790"
    },
    {
        "id": 14875,
        "title": "Leveraging Normalization Layer in Adapters with Progressive Learning and Adaptive Distillation for Cross-Domain Few-Shot Learning",
        "authors": "YongJin Yang, Taehyeon Kim, Se-Young Yun",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Cross-domain few-shot learning presents a formidable challenge, as models must be trained on base classes and then tested on novel classes from various domains with only a few samples at hand. While prior approaches have primarily focused on parameter-efficient methods of using adapters, they often overlook two critical issues: shifts in batch statistics and noisy sample statistics arising from domain discrepancy variations. In this paper, we introduce Leveraging Normalization Layer in Adapters with Progressive Learning and Adaptive Distillation (ProLAD), marking two principal contributions. First, our methodology utilizes two separate adapters: one devoid of a normalization layer, which is more effective for similar domains, and another embedded with a normalization layer, designed to leverage the batch statistics of the target domain, thus proving effective for dissimilar domains. Second, to address the pitfalls of noisy statistics, we deploy two strategies: a progressive training of the two adapters and an adaptive distillation technique derived from features determined by the model solely with the adapter devoid of a normalization layer. Through this adaptive distillation, our approach functions as a modulator, controlling the primary adapter for adaptation, based on each domain. Evaluations on standard cross-domain few-shot learning benchmarks confirm that our technique outperforms existing state-of-the-art methodologies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29573"
    },
    {
        "id": 14876,
        "title": "From adaptive score normalization to adaptive data normalization for speaker verification systems",
        "authors": "Sandro Cumani, Salvatore Sarni",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-266"
    },
    {
        "id": 14877,
        "title": "Supervised Feature Learning for Offline Writer Identification Using VLAD and Double Power Normalization",
        "authors": "Dawei Liang, Meng Wu, Yan Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.035279"
    },
    {
        "id": 14878,
        "title": "Deep LearningâDriven Transformation: A Novel Approach for Mitigating Batch Effects in Diffusion MRI Beyond Traditional Harmonization",
        "authors": "Akihiko Wada, Toshiaki Akashi, Akifumi Hagiwara, Mitsuo Nishizawa, Keigo Shimoji, Junko Kikuta, Tomoko Maekawa, Katsuhiro Sano, Koji Kamagata, Atsushi Nakanishi, Shigeki Aoki",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "BackgroundâBatch effectâ in MR images, due to vendorâspecific features, MR machine generations, and imaging parameters, challenges image quality and hinders deep learning (DL) model generalizability.PurposeWe aim to develop a DL model using contrast adjustment and superâresolution to reduce diffusionâweighted images (DWIs) diversity across magnetic field strengths and imaging parameters.Study TypeRetrospective.SubjectsThe DL model was built using an open dataset from one individual. The MR machine identification model was trained and validated on a dataset of 1134 adults (54% females, 46% males), with 1050 subjects showing no DWI abnormalities and 84 with conditions like stroke and tumors. The 21,000 images were divided into 80% for training, 20% for validation, and 3500 for testing.Field Strength/SequenceSeven MR scanners from four manufacturers with 1.5âT and 3âT magnetic field strengths. DWIs were acquired using spinâecho sequences and highâresolution T2WIs using the T2âSPACE sequence.AssessmentAn experienced, boardâcertified radiologist evaluated the effectiveness of restoring highâresolution T2WI and harmonizing diverse DWI with metrics such as PSNR and SSIM, and the texture and frequency attributes were further analyzed using grayâlevel coâoccurrence matrix and 1âdimensional power spectral density. The model's impact on machineâspecific characteristics was gauged through the performance metrics of a ResNetâ50 model. Comprehensive statistical tests were employed for statistical robustness, including McNemar's test and the Dice index.ResultsOur DL protocol reduced DWI contrast and resolution variation. ResNetâ50 model's accuracy decreased from 0.9443 to 0.5786, precision from 0.9442 to 0.6494, recall from 0.9443 to 0.5786, and F1 score from 0.9438 to 0.5587. The tâSNE visualization indicated more consistent image features across multiple MR devices. Autoencoder halved learning iterations; Dice coefficient >0.74 confirmed signal reproducibility in 84 lesions.ConclusionThis study presents a DL strategy to mitigate batch effects in diffusion MR images, improving their quality and generalizability.Evidence Level3Technical EfficacyStage 1",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/jmri.29088"
    },
    {
        "id": 14879,
        "title": "Automatic Outdoor Fire detection using Deep learning",
        "authors": "Aesr Saad Abdalsattar, Naji M. Sahib",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "Abstract\r\nOne of the unlucky phenomena that contribute to environmental disasters is fire, which also poses a serious threat to human safety and life, particularly when it is not recognized by sensor-based fire detection systems. Therefore, putting inexpensive and efficient sensors in certain locations will greatly speed up the detection of fires. Vision-based fire detection systems take advantage of the three fundamental features of fire: color, movement, and shape (fire shape). Work has been done to build smoke and fire detection systems based on images that use security cameras. In this study, fire photos are used to extract features before inputs are made. To one of the classification techniques that applies SVM-based machine learning. and CNN-based deep learning technique. We use the linear regression model so that the bounding box of the classified object determines the correct coordinates.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56990/bajest/2023.020104"
    },
    {
        "id": 14880,
        "title": "Data-Driven Soft Sensing for Batch Processes Using Neural Network-Based Deep Quality-Relevant Representation Learning",
        "authors": "Qingchao Jiang, Ziwen Wang, Shifu Yan, Zhixing Cao",
        "published": "2023-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3145758"
    },
    {
        "id": 14881,
        "title": "Fault diagnosis with high accuracy and timeliness for semiâbatch crystallization process based on deep learning with multiple pattern representation",
        "authors": "Silin Rao, Ziteng Wang, Jingtao Wang",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "AbstractThe research on chemical process fault diagnosis has made significant progress, but there is still a big gap in its application to complex practical industrial processes. As for the fault diagnosis of batch crystallization processes, the recentlyâproposed dynamic time warpingâconvolutional neural network (DTWâCNN) model has achieved a great improvement in the fault diagnosis. However, its fault diagnosis rate (FDR) and timeliness of fault diagnosis are still low, and thus, it needs to improve further before being applied to the practical application. In this paper, a multiple pattern representationâconvolutional neural network (MPRâCNN) model is proposed and applied for the fault diagnosis of a semiâbatch crystallization process. The MPRâCNN model enables the manual extraction of features with four pattern representation algorithms in the data preâprocessing stage, and generates a threeâdimensional matrix which is used as the training sample and input to the CNN for the formal feature extraction and weight learning. An excellent classification performance, with an average FDR of 97.5%, is achieved. This model is also applied for the fault diagnosis of process data within a shorter period of time after the occurrence of faults. The results indicate that the model could make timely fault diagnosis with a highly stable and accurate performance after the occurrence of a fault.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cjce.25247"
    },
    {
        "id": 14882,
        "title": "Deep Learning for Active Robotic Perception",
        "authors": "Nikolaos Passalis, Pavlos Tosidis, Theodoros Manousis, Anastasios Tefas",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012295800003595"
    },
    {
        "id": 14883,
        "title": "Real-time generation of realistic defective wafer maps via deep learning network of CycleGAN",
        "authors": "Lamia Alam, Nasser Kehtarnavaz",
        "published": "2023-6-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663364"
    },
    {
        "id": 14884,
        "title": "Covid-19 Detection using Deep Learning",
        "authors": "",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55248/gengpi.4.523.41359"
    },
    {
        "id": 14885,
        "title": "Deep Learning in Digital Breast Pathology",
        "authors": "Madison Rose, Joseph Geradts, Nic Herndon",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012576100003657"
    },
    {
        "id": 14886,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 14887,
        "title": "Deep Clustering Based on Contractive Autoencoder and Self-paced Learning",
        "authors": "HaoRan Bu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166071"
    },
    {
        "id": 14888,
        "title": "Hybrid deep modeling of a CHO-K1 fed-batch process: combining first-principles with deep neural networks",
        "authors": "JosÃ© Pinto, JoÃ£o R. C. Ramos, Rafael S. Costa, Sergio Rossell, Patrick Dumas, Rui Oliveira",
        "published": "2023-9-8",
        "citations": 4,
        "abstract": "Introduction: Hybrid modeling combining First-Principles with machine learning is becoming a pivotal methodology for Biopharma 4.0 enactment. Chinese Hamster Ovary (CHO) cells, being the workhorse for industrial glycoproteins production, have been the object of several hybrid modeling studies. Most previous studies pursued a shallow hybrid modeling approach based on three-layered Feedforward Neural Networks (FFNNs) combined with macroscopic material balance equations. Only recently, the hybrid modeling field is incorporating deep learning into its framework with significant gains in descriptive and predictive power.Methods: This study compares, for the first time, deep and shallow hybrid modeling in a CHO process development context. Data of 24 fed-batch cultivations of a CHO-K1 cell line expressing a target glycoprotein, comprising 30 measured state variables over time, were used to compare both methodologies. Hybrid models with varying FFNN depths (3-5 layers) were systematically compared using two training methodologies. The classical training is based on the Levenberg-Marquardt algorithm, indirect sensitivity equations and cross-validation. The deep learning is based on the Adaptive Moment Estimation Method (ADAM), stochastic regularization and semidirect sensitivity equations.Results and conclusion: The results point to a systematic generalization improvement of deep hybrid models over shallow hybrid models. Overall, the training and testing errors decreased by 14.0% and 23.6% respectively when applying the deep methodology. The Central Processing Unit (CPU) time for training the deep hybrid model increased by 31.6% mainly due to the higher FFNN complexity. The final deep hybrid model is shown to predict the dynamics of the 30 state variables within the error bounds in every test experiment. Notably, the deep hybrid model could predict the metabolic shifts in key metabolites (e.g., lactate, ammonium, glutamine and glutamate) in the test experiments. We expect deep hybrid modeling to accelerate the deployment of high-fidelity digital twins in the biopharma sector in the near future.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fbioe.2023.1237963"
    },
    {
        "id": 14889,
        "title": "An improved class incremental learning method based on multi-level distillation and continual normalization",
        "authors": "Qingbo Ji, Qiang Zhang, Qingfeng Ma",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2678920"
    },
    {
        "id": 14890,
        "title": "Generalized pancreatic cancer diagnosis via multiple instance learning and anatomically-guided shape normalization",
        "authors": "Jiaqi Qu, Xunbin Wei, Xiaohua Qian",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.media.2023.102774"
    },
    {
        "id": 14891,
        "title": "Deep Learning for Plant Disease Detection",
        "authors": "Munaf Mudheher Khalid, Oguz Karan",
        "published": "2023-11-18",
        "citations": 4,
        "abstract": "Agriculture, an essential bedrock of human survival, continually grapples with the menace of plant diseases, culminating in substantial yield reductions. While conventional detection techniques remain widespread, they often entail laborious efforts and are susceptible to inaccuracies, underscoring the pressing need for more efficient, scalable, and immediate solutions. Our research explores the transformative capabilities of Deep Learning (DL) models, primarily focusing on Convolutional Neural Networks (CNNs) and MobileNet architectures in the early and precise identification of plant ailments. We augmented our exploration by incorporating eXplainable Artificial Intelligence (XAI) through GradCAM, which elucidated the decision-making process of these models, providing a visual interpretation of disease indicators in plant images. Through rigorous testing, our CNN model yielded an accuracy of 89%, a precision and recall of 96%, and an F1-score of 96%. Conversely, the MobileNet design showcased an accuracy of 96% but recorded slightly lesser precision, recall, and F1-scores of 90%, 89%, and 89%, respectively. Such results amplify the transformative role of DL in redefining plant disease detection methodologies, presenting a formidable counterpart to conventional techniques and ushering in an era of heightened agricultural security.",
        "keywords": "",
        "link": "http://dx.doi.org/10.59543/ijmscs.v2i.8343"
    },
    {
        "id": 14892,
        "title": "Watch Your English Language: Text Normalization from Written Expressions to Spoken Forms",
        "authors": "Yufeng Jiang",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3639479.3639516"
    },
    {
        "id": 14893,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 14894,
        "title": "Classification Of Cloud Platform Attacks Using Machine Learning And Deep Learning Approaches",
        "authors": "",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/nq.2022.20.2.nq22344"
    },
    {
        "id": 14895,
        "title": "Gradient Clipping in Deep Learning: A Dynamical Systems Perspective",
        "authors": "Arunselvan Ramaswamy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678000003411"
    },
    {
        "id": 14896,
        "title": "A Voice-Activated, Deep-Learning-Based Emergency Alarm for Arabic Dialects",
        "authors": "",
        "published": "2023-5-18",
        "citations": 0,
        "abstract": "In this report, we will talk in almost every detail about our project, in order to cover most aspects of the project. We tried as much as possible to make the report suitable for two segments, the specialists and the public so that the reader can make the most of the report. We will talk about the hardware and software used in the project and how they work, in order to explain to the ordinary reader how some technical matters are done and to explain to the specialist some things that he or she might consider while developing. In our project, samples are the cornerstone of the project, and for this, we have focused on them in the report, from the methods of identifying samples to access to taking samples and analyzing them project to other dimensions. When we wrote the project, we took into account that development on the project is an important part of this academic process, so we paved the way for those who want to develop by collecting our own database in Arabic for scientific purposes so that development on it becomes easier and more productive.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.02.10"
    },
    {
        "id": 14897,
        "title": "Applications of Deep Learning and Deep Reinforcement Learning in 6G Networks",
        "authors": "Tri-Hai Nguyen, Heejae Park, Kihyun Seol, Seonghyeon So, Laihyuk Park",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icufn57995.2023.10200390"
    },
    {
        "id": 14898,
        "title": "Metamorphic Testing of Machine Translation Models using Back Translation",
        "authors": "Wentao Gao, Jiayuan He, Van-Thuan Pham",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/deeptest59248.2023.00008"
    },
    {
        "id": 14899,
        "title": "An energy-efficient voice activity detector using reconfigurable Gaussian base normalization deep neural network",
        "authors": "Anu Samanta, Indranil Hatai, Ashis Kumar Mal",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-14699-1"
    },
    {
        "id": 14900,
        "title": "Learning Activities that Influence Deep Active Learning in Reading Circles Learning",
        "authors": "Qihui Hu",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaie56796.2023.00023"
    },
    {
        "id": 14901,
        "title": "Hybrid Deep Learning Implementation for Crop Yield Prediction",
        "authors": "Halit ÃETÄ°NER",
        "published": "2023-6-22",
        "citations": 0,
        "abstract": "Agriculture producers should be supported technologically in order to continue production in a way that meets the worldwide food supply and demand. Automatic realization of crop yield estimation calculation is a desired need of farmers. Automatic yield estimation also facilitates the work of agricultural producers with different goals such as imports and exports. To achieve the stated objectives, deep learning models have been developed that estimated yield using parameters such as the amount of water per hectare, the average amount of sunlight received by the hectare, the amount of fertilization per hectare, the number of pesticides used per hectare, and the area of cultivation. With the hybrid model created by combining the strengths of the LSTM and CNN models developed within the scope of this article, the success rate of data prediction has increased with fine adjustments. Success rates of 89.71 R2, 0.0035 MSE, 0.0248 RMSE, 0.0461 MAE, and 10.10 MAPE have been achieved with the Proposed hybrid model. This model is competitive with similar studies with the stated values.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35414/akufemubid.1116187"
    },
    {
        "id": 14902,
        "title": "Classroom behavior recognition and teaching quality evaluation system based on deep learning",
        "authors": "Likai Su, Weifa Zheng",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679251"
    },
    {
        "id": 14903,
        "title": "Towards Fraudulent URL Classification with Large Language Model based on Deep Learning",
        "authors": "Fei Tang, Boyang Yu, Shixiang Zhao, MeiMei Xu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166535"
    },
    {
        "id": 14904,
        "title": "Machine Learning-Based Batch Processing for Calibration of Model and Noise Parameters",
        "authors": "Kyuman Lee",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasc58513.2023.10311101"
    }
]