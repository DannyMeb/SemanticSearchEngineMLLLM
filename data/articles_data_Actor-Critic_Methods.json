[
    {
        "id": 18771,
        "title": "Review for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "",
        "published": "2022-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v1/review2"
    },
    {
        "id": 18772,
        "title": "Variance Reduction in Actor Critic Methods (ACM)",
        "authors": "Eric Benhamou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3424668"
    },
    {
        "id": 18773,
        "title": "Review for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "Faisal Khan",
        "published": "2022-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v1/review1"
    },
    {
        "id": 18774,
        "title": "Collaborative Learning of Human and Computer: Supervised Actor-Critic based Collaboration Scheme",
        "authors": "Ashwin Devanga, Koichiro Yamauchi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007568407940801"
    },
    {
        "id": 18775,
        "title": "Decision letter for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "",
        "published": "2022-3-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v1/decision1"
    },
    {
        "id": 18776,
        "title": "Decision letter for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "",
        "published": "2022-4-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v2/decision1"
    },
    {
        "id": 18777,
        "title": "Generalized Critic Policy Optimization: A Model For Combining Advantage Estimates In Actor Critic Methods",
        "authors": "Roumeissa Kitouni, Abderrahim Kitouni, Feng Jiang",
        "published": "2020-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190994"
    },
    {
        "id": 18778,
        "title": "Master-Slave Policy Collaboration for Actor-Critic Methods",
        "authors": "Xiaomu Li, Quan Liu",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892603"
    },
    {
        "id": 18779,
        "title": "Author response for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": " Dutta, Debaprasad,  Upreti, Simant R.",
        "published": "2022-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v2/response1"
    },
    {
        "id": 18780,
        "title": "Noisy Importance Sampling Actor-Critic: An Off-Policy Actor-Critic With Experience Replay",
        "authors": "Norman Tasfi, Miriam Capretz",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207681"
    },
    {
        "id": 18781,
        "title": "Actor-Critic Models and the A3C",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_11"
    },
    {
        "id": 18782,
        "title": "Decentralized Multi-Agent Advantage Actor-Critic",
        "authors": "Scott Barnes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>We present a decentralized advantage actor-critic algorithm that utilizes learning agents in parallel environments with synchronous gradient descent. This approach decorrelates agents’ experiences, stabilizing observations and eliminating the need for a replay buffer, requires no knowledge of the other agents’ internal state during training or execution, and runs on a single multi-core CPU.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19166384.v1"
    },
    {
        "id": 18783,
        "title": "Selector-Actor-Critic and Tuner-Actor-Critic Algorithms for Reinforcement Learning",
        "authors": "Ala'eddin Masadeh, Zhengdao Wang, Ahmed E. Kamal",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcsp.2019.8928124"
    },
    {
        "id": 18784,
        "title": "Decentralized Multi-Agent Advantage Actor-Critic",
        "authors": "Scott Barnes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>We present a decentralized advantage actor-critic algorithm that utilizes learning agents in parallel environments with synchronous gradient descent. This approach decorrelates agents’ experiences, stabilizing observations and eliminating the need for a replay buffer, requires no knowledge of the other agents’ internal state during training or execution, and runs on a single multi-core CPU.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19166384"
    },
    {
        "id": 18785,
        "title": "Learning to Trade with Deep Actor Critic Methods",
        "authors": "Jinke Li, Ruonan Rao, Jun Shi",
        "published": "2018-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscid.2018.10116"
    },
    {
        "id": 18786,
        "title": "Visualizing the Loss Landscape of Actor Critic Methods with Applications in Inventory Optimization",
        "authors": "Recep Yusuf Bekci, Mehmet Gumus",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4059578"
    },
    {
        "id": 18787,
        "title": "Actor-Critic Reinforcement Learning with Neural Networks in Continuous Games",
        "authors": "Gabriel Leuenberger, Marco A. Wiering",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006556500530060"
    },
    {
        "id": 18788,
        "title": "TD-regularized actor-critic methods",
        "authors": "Simone Parisi, Voot Tangkaratt, Jan Peters, Mohammad Emtiyaz Khan",
        "published": "2019-9",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-019-05788-0"
    },
    {
        "id": 18789,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 18790,
        "title": "Twin Delayed Stochastic Actor-Critic",
        "authors": "Mohammad Asadolahi, Arash Sharifi, Touraj Banirostam",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn both discrete and continuous domains, model-free reinforcement learning algorithms have been successfully applied to the vast majority of reinforcement learning problems and are the main solution to real world problems. In reinforcement learning problems with continuous action space, state-of-the-art algorithms are extremely sample-inefficient and need lots of training interactions to become proficient, which could be catastrophically expensive and infeasible in real-world problems. So far, frontier algorithms have not used well-defined methods to explore the decision space. Exploring new behaviors is a prerequisite to look for optimal policies. All of the leading algorithms in the field leverage a blind form of exploration added to agent decisions to search for better policies. Such solutions fail to mindfully explore the environment, disrupting the learning process. This makes these algorithms very prone to failing in specific domains. In this research, a novel stochastic Off-Policy Actor-Critic algorithm, TDS for short, is presented. Combining the policy gradient theorem with the deterministic policy gradient, the TDS algorithm can learn how to mindfully explore the environment. The proposed update method enables TDS to learn how to modify the decision stochasticity bonds for each state and action. This is done according to gradients information derived from learning feedbacks. Evaluations in MuJoCo and Box2D tasks show faster convergence or outperform the state-of-the-art algorithms including TD3, SAC, and DDPG in every environment tested.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3041837/v1"
    },
    {
        "id": 18791,
        "title": "Data augmented Approach to Optimizing Asynchronous Actor-Critic Methods",
        "authors": "Sandeep Varma N, Pradyumna Rahul K, Vaishnavi Sinha",
        "published": "2022-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcece53908.2022.9792764"
    },
    {
        "id": 18792,
        "title": "Bi-level Multi-Agent Actor-Critic Methods with ransformers",
        "authors": "Tianjiao Wan, Haibo Mi, Zijian Gao, Yuanzhao Zhai, Bo Ding, Dawei Feng",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jcc59055.2023.00007"
    },
    {
        "id": 18793,
        "title": "Neural Architecture Search with Synchronous Advantage Actor-Critic Methods and Partial Training",
        "authors": "George Kyriakides, Konstantinos G. Margaritis",
        "published": "2018-7-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3200947.3208068"
    },
    {
        "id": 18794,
        "title": "Distributional Safety Critic for Stochastic Latent Actor-Critic",
        "authors": "Thiago S. Miranda, Heder S. Bernardino",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "When employing reinforcement learning techniques in real-world applications, one may desire to constrain the agent by limiting actions that lead to potential damage, harm, or unwanted scenarios. Particularly, recent approaches focus on developing safe behavior under partial observability conditions. In this vein, we develop a method that combines distributional reinforcement learning techniques with methods used to facilitate learning in partially observable environments, called distributional safe stochastic latent actor-critic (DS-SLAC). We evaluate the DS-SLAC performance on four Safety-Gym tasks and DS-SLAC obtained results better than those reached by state-of-the-art algorithms in two of the evaluated environments while being able to develop a safe policy in three of them. Lastly, we also identify the main challenges of performing distributional reinforcement learning in the safety-constrained partially observable setting.",
        "link": "http://dx.doi.org/10.5753/eniac.2023.234620"
    },
    {
        "id": 18795,
        "title": "Multiple Agents Dispatch via Batch Synchronous Actor Critic in Autonomous Mobility on Demand Systems",
        "authors": "Jiyao Li, Vicki Allan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012351700003636"
    },
    {
        "id": 18796,
        "title": "Group Random Access Control Scheme Based on Asynchronous Advantage Actor Critic",
        "authors": "Su Kim, Han-Seung Jang",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.2.258"
    },
    {
        "id": 18797,
        "title": "Addressing Different Goal Selection Strategies In Hindsight Experience Replay With Actor-Critic Methods For Robotic Hand Manipulation",
        "authors": "Ayman Shams, Thomas Fevens",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/raai56146.2022.10092979"
    },
    {
        "id": 18798,
        "title": "The Effect of Discounting Actor-loss in Actor-Critic Algorithm",
        "authors": "Jordi Yaputra, Suyanto Suyanto",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isriti54043.2021.9702883"
    },
    {
        "id": 18799,
        "title": "The Proposal of Double Agent Architecture using Actor-critic Algorithm for Penetration Testing",
        "authors": "Hoang Nguyen, Songpon Teerakanok, Atsuo Inomata, Tetsutaro Uehara",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010232504400449"
    },
    {
        "id": 18800,
        "title": "Soft Actor-Critic With Integer Actions",
        "authors": "Ting-Han Fan, Yubo Wang",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867395"
    },
    {
        "id": 18801,
        "title": "Multi-actor mechanism for actor-critic reinforcement learning",
        "authors": "Lin Li, Yuze Li, Wei Wei, Yujia Zhang, Jiye Liang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119494"
    },
    {
        "id": 18802,
        "title": "Statistical arbitrage trading across electricity markets using advantage actor–critic methods",
        "authors": "Sumeyra Demir, Koen Kok, Nikolaos G. Paterakis",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101023"
    },
    {
        "id": 18803,
        "title": "The Actor-Critic Algorithm for Infinite Horizon Discounted Cost Revisited",
        "authors": "Abhijit Gosavi",
        "published": "2020-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc48552.2020.9384016"
    },
    {
        "id": 18804,
        "title": "Actor-Double-Critic: Incorporating Model-Based Critic for Task-Oriented Dialogue Systems",
        "authors": "Yen-chen Wu, Bo-Hsiang Tseng, Milica Gasic",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.75"
    },
    {
        "id": 18805,
        "title": "Actor–Critic Learning Based Pid Control for Robotic Manipulators",
        "authors": "Hamed Rahimi Nohooji, Abolfazl Zaraki, Holger Voos",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4409551"
    },
    {
        "id": 18806,
        "title": "Improving sample efficiency in Multi-Agent Actor-Critic methods",
        "authors": "Zhenhui Ye, Yining Chen, Xiaohong Jiang, Guanghua Song, Bowei Yang, Sheng Fan",
        "published": "2022-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-021-02554-5"
    },
    {
        "id": 18807,
        "title": "Actor-Critic Methods in Stock Trading : A Comparative Study",
        "authors": "Firdaous Khemlichi, Houda Elyousfi Elfilali, Hiba Chougrad, Safae Elhaj Ben Ali, Youness Idrissi Khamlichi",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10253277"
    },
    {
        "id": 18808,
        "title": "A Path Planning for Unmanned Aerial Vehicles Using SAC (Soft Actor Critic) Algorithm",
        "authors": "Soo-Jong Hyeon, Tae-Young Kang, Chang-Kyung Ryoo",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5302/j.icros.2022.21.0220"
    },
    {
        "id": 18809,
        "title": "Soft Actor-Criticの改良による出力抑制と頑健化",
        "authors": "Taisuke KOBAYASHI",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2023.2a2-e14"
    },
    {
        "id": 18810,
        "title": "Enhancing Exploration in Actor-Critic Algorithms:An Approach to Incentivize Plausible Novel States",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Actor-critic (AC) algorithms are model-free deep reinforcement learning techniques that have consistently demon- strated their effectiveness across various domains, especially in addressing continuous control challenges. Enhancing exploration (action entropy) and exploitation (expected return) through more efficient sample utilization is pivotal in AC algorithms. The fundamental strategy of a learning algorithm is to intelligently navigate the environment’s state space, prioritizing the explo- ration of rarely visited states over frequently encountered ones. In alignment with this strategy, we propose an innovative approach to bolster exploration by employing an intrinsic reward based on a state’s novelty and the potential benefits of exploring that state, which we term plausible novelty. Our approach is designed for seamless integration into off-policy AC algorithms. Through incentivized exploration of plausibly novel states, an AC algo- rithm can substantially enhance its sample efficiency and overall training performance. The new approach is verified through extensive simulations across various continuous control tasks within MuJoCo environments, utilizing a range of prominent off-policy AC algorithmsIncentivizing Plausible Novel States: An Exploration Boosting Approach for Actor Critic Algorithms</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24631455"
    },
    {
        "id": 18811,
        "title": "Enhancing Exploration in Actor-Critic Algorithms:An Approach to Incentivize Plausible Novel States",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Actor-critic (AC) algorithms are model-free deep reinforcement learning techniques that have consistently demon- strated their effectiveness across various domains, especially in addressing continuous control challenges. Enhancing exploration (action entropy) and exploitation (expected return) through more efficient sample utilization is pivotal in AC algorithms. The fundamental strategy of a learning algorithm is to intelligently navigate the environment’s state space, prioritizing the explo- ration of rarely visited states over frequently encountered ones. In alignment with this strategy, we propose an innovative approach to bolster exploration by employing an intrinsic reward based on a state’s novelty and the potential benefits of exploring that state, which we term plausible novelty. Our approach is designed for seamless integration into off-policy AC algorithms. Through incentivized exploration of plausibly novel states, an AC algo- rithm can substantially enhance its sample efficiency and overall training performance. The new approach is verified through extensive simulations across various continuous control tasks within MuJoCo environments, utilizing a range of prominent off-policy AC algorithmsIncentivizing Plausible Novel States: An Exploration Boosting Approach for Actor Critic Algorithms</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24631455.v1"
    },
    {
        "id": 18812,
        "title": "Critic-over-Actor-Critic Modeling: Finding Optimal Strategy in ICU Environments",
        "authors": "Riazat Ryan, Ming Shao",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10021125"
    },
    {
        "id": 18813,
        "title": "Estimation Error Correction in Deep Reinforcement Learning for Deterministic Actor-Critic Methods",
        "authors": "Baturay Saglam, Enes Duran, Dogan C. Cicek, Furkan B. Mutlu, Suleyman S. Kozat",
        "published": "2021-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai52525.2021.00027"
    },
    {
        "id": 18814,
        "title": "3. The Humorist as Critic, Writer and Actor",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1355/9789814762434-007"
    },
    {
        "id": 18815,
        "title": "Combining Backpropagation with Equilibrium Propagation to improve an Actor-Critic RL framework",
        "authors": "Yoshimasa Kubo, Eric Chalmers, Artur Luczak",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackpropagation has been used to train neural networks for many years, allowing them to solve a wide variety of tasks like image classification, speech recognition, and reinforcement learning tasks. But the biological plausibility of backpropagation as a mechanism of neural learning has been questioned. Equilibrium Propagation (EP) has been proposed as a more biologically plausible alternative and achieves comparable accuracy on the CIFAR-10 image classification task. This study proposes the first EP-based reinforcement learning architecture: an actor-critic architecture with the actor network trained by EP. We show that this model can solve the basic control tasks often used as benchmarks for BP-based models. Interestingly, our trained model demonstrates more consistent high-reward behavior than a comparable model trained exclusively by backpropagation.",
        "link": "http://dx.doi.org/10.1101/2022.06.21.496871"
    },
    {
        "id": 18816,
        "title": "Twin Delayed Hierarchical Actor-Critic",
        "authors": "Mihai Anca, Matthew Studley",
        "published": "2021-2-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icara51699.2021.9376459"
    },
    {
        "id": 18817,
        "title": "Online control basis selection by a regularized actor critic algorithm",
        "authors": " Jianjun Yuan, Andrew Lamperski",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963640"
    },
    {
        "id": 18818,
        "title": "Robust Soft Actor Critic Tracking Network",
        "authors": "Kexin Chen, Baojie Fan, Yang Ding, Zhiquan Wang",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055621"
    },
    {
        "id": 18819,
        "title": "Novel Methods Inspired by Reinforcement Learning Actor-Critic Mechanism for Eye-in-Hand Calibration in Robotics",
        "authors": "Chenxing Li, Yinlong Liu, Yingbai Hu, Fabian Schreier, Jan Seyler, Shahram Eivazi",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364560"
    },
    {
        "id": 18820,
        "title": "Variance-Reduced Deep Actor-Critic with an Optimally Sub-Sampled Actor Recursion",
        "authors": "Lakshmi Mandal, Raghuram Bharadwaj Diddigi, Shalabh Bhatnagar",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2024.3379109"
    },
    {
        "id": 18821,
        "title": "Actor-Critic Methods for IRS Design in Correlated Channel Environments: A Closer Look Into the Neural Tangent Kernel of the Critic",
        "authors": "Spilios Evmorfos, Athina P. Petropulu, H. Vincent Poor",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsp.2023.3322830"
    },
    {
        "id": 18822,
        "title": "The application of actor-critic reinforcement learning for fab dispatching scheduling",
        "authors": "Namyong Kim, Hayong Shin",
        "published": "2017-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc.2017.8248209"
    },
    {
        "id": 18823,
        "title": "Transformer Model Based Soft Actor-Critic Learning for HEMS",
        "authors": "Ulrich Ludolfinger, Vedran S. Peric, Thomas Hamacher, Sascha Hauke, Maren Martens",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The transition to weather dependent renewable energy generators requires the electric loads to be adjusted to generation. This is made possible by demand response programs and home energy management systems. However, practically easy to use rule-based control systems often miss many optimization potentials. Self-learning alternatives employing reinforcement learning often ignore the partial observability of the building control problem and consequently neglect the importance of the observation history. Adaptive control systems that do consider that history often rely on policies that suffer from catastrophic forgetting, which makes them unable to fully grasp long histories.</p>\n<p>As an alternative, we present a new reinforcement learning method for autonomous building energy management control based on the soft actor-critic method and the transformer deep neural network architecture. For the control of a heat pump and an the inlet port of a thermal storage, under consideration of photovoltaic generations and dynamic electricity prices, we formulate the problem as partially observable and use the history of observations to determine the control signals. We show, based on a validated building simulation, that our method outperforms rule-based as well as reinforcement learning methods that use multi layer perceptrons or recurrent neural networks as policy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23264429.v1"
    },
    {
        "id": 18824,
        "title": "Soft Actor Critic Swing Up of a Real Inverted Pendulum on a Cart",
        "authors": "Raniero Humberto Calderon",
        "published": "2023",
        "citations": 0,
        "abstract": "The inverted pendulum, is a classical experiment widely used as a benchmark for research in control systems, due to its challenging dynamics. In this paper, Deep Reinforcement Learning is used to control a real inverted pendulum on a cart. The Soft Actor Critic algorithm with automatic entropy tuning is used to train an agent capable of acting as a controller. The agent is trained on real data collected on an episodic basis and learns to carry out the swing up control task successfully.",
        "link": "http://dx.doi.org/10.53375/icmame.2023.403"
    },
    {
        "id": 18825,
        "title": "An Actor-Critic Algorithm With Second-Order Actor and Critic",
        "authors": "Jing Wang, Ioannis Ch. Paschalidis",
        "published": "2017-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tac.2016.2616384"
    },
    {
        "id": 18826,
        "title": "Actor-Critic Method for Solving High Dimensional Hamilton-Jacobi-Bellman type PDEs",
        "authors": "Jianfeng Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this talk, we will discuss numerical approach to solve high dimensional Hamilton-Jacobi-Bellman (HJB) type elliptic partial differential equations (PDEs). The HJB PDEs, reformulated as optimal control problems, are tackled by the actor-critic framework inspired by reinforcement learning, based on neural network parametrization of the value and control functions. Within the actor-critic framework, we employ a policy gradient approach to improve the control, while for the value function, we derive a variance reduced least-squares temporal difference method using stochastic calculus. We will also discuss convergence analysis for the actor-critic method, in particular the policy gradient method for solving stochastic optimal control. Joint work with Jiequn Han (Flatiron Institute) and Mo Zhou (Duke University).",
        "link": "http://dx.doi.org/10.52843/cassyni.dtfn52"
    },
    {
        "id": 18827,
        "title": "Asymmetric Actor-Critic with Approximate Information State",
        "authors": "Amit Sinha, Aditya Mahajan",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383636"
    },
    {
        "id": 18828,
        "title": "Actor-Critic Learning-Based Yarn Processing Quality Control Method for Spinning Cyber Physical Production Systems",
        "authors": "Shiyong Yin, Shixi Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442542"
    },
    {
        "id": 18829,
        "title": "Transformer Model Based Soft Actor-Critic Learning for HEMS",
        "authors": "Ulrich Ludolfinger, Vedran S. Peric, Thomas Hamacher, Sascha Hauke, Maren Martens",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The transition to weather dependent renewable energy generators requires the electric loads to be adjusted to generation. This is made possible by demand response programs and home energy management systems. However, practically easy to use rule-based control systems often miss many optimization potentials. Self-learning alternatives employing reinforcement learning often ignore the partial observability of the building control problem and consequently neglect the importance of the observation history. Adaptive control systems that do consider that history often rely on policies that suffer from catastrophic forgetting, which makes them unable to fully grasp long histories.</p>\n<p>As an alternative, we present a new reinforcement learning method for autonomous building energy management control based on the soft actor-critic method and the transformer deep neural network architecture. For the control of a heat pump and an the inlet port of a thermal storage, under consideration of photovoltaic generations and dynamic electricity prices, we formulate the problem as partially observable and use the history of observations to determine the control signals. We show, based on a validated building simulation, that our method outperforms rule-based as well as reinforcement learning methods that use multi layer perceptrons or recurrent neural networks as policy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23264429"
    },
    {
        "id": 18830,
        "title": "Regularized Soft Actor-Critic for Behavior Transfer Learning",
        "authors": "Mingxi Tan, Andong Tian, Ludovic Denoyer",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893655"
    },
    {
        "id": 18831,
        "title": "Development and Validation of Active Roll Control based on Actor-critic Neural Network Reinforcement Learning",
        "authors": "Matthias Bahr, Sebastian Reicherts, Philipp Sieberg, Luca Morss, Dieter Schramm",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007787400360046"
    },
    {
        "id": 18832,
        "title": "Efficient Multi-Agent Exploration with Mutual-Guided Actor-Critic",
        "authors": "Renlong Chen, Ying Tan",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254169"
    },
    {
        "id": 18833,
        "title": "High Performance Visual Tracking With Siamese Actor-Critic Network",
        "authors": "Dawei Zhang, Zhonglong Zheng",
        "published": "2020-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9191326"
    },
    {
        "id": 18834,
        "title": "Deep Actor-Critic Reinforcement Learning for Anomaly Detection",
        "authors": "Chen Zhong, M. Cenk Gursoy, Senem Velipasalar",
        "published": "2019-12",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013223"
    },
    {
        "id": 18835,
        "title": "Experimental Validation of an Actor-Critic Model Predictive Force Controller for Robot-Environment Interaction Tasks",
        "authors": "Alessandro Pozzi, Luca Puricelli, Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio, Francesco Braghin, Loris Roveda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012160700003543"
    },
    {
        "id": 18836,
        "title": "When Visible Light Communication Meets RIS: A Soft Actor-Critic Approach",
        "authors": "Long Zhang, Xingliang Jia, Choong Seon Hong, Zhu Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170326666.65516785/v1"
    },
    {
        "id": 18837,
        "title": "Actor Critic Agents for Wind Farm Control",
        "authors": "Claire Bizon Monroc, Ana Bušić, Donatien Dubuc, Jiamin Zhu",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156453"
    },
    {
        "id": 18838,
        "title": "Optimization of High-Speed Train Operation Control Based on Soft Actor-Critic Deep Reinforcement Learning Algorithm",
        "authors": "Huiqin Pei, Zhuyuan Lan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573607"
    },
    {
        "id": 18839,
        "title": "Follow then Forage Exploration: Improving Asynchronous Advantage Actor Critic",
        "authors": "James B. Holliday, T.H. Ngan Le",
        "published": "2020-7-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/csit.2020.100909"
    },
    {
        "id": 18840,
        "title": "Honey. I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL",
        "authors": "Siddharth Mysore, Bassel El Mabsout, Renato Mancuso, Kate Saenko",
        "published": "2021-8-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619008"
    },
    {
        "id": 18841,
        "title": "Fuzzy actor-critic learning automaton algorithm for the pursuit-evasion differential game",
        "authors": "Ahmad A. Al-Talabi",
        "published": "2017-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacs.2017.8284278"
    },
    {
        "id": 18842,
        "title": "Latent Context Based Soft Actor-Critic",
        "authors": "Yuan Pu, Shaochen Wang, Xin Yao, Bin Li",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207008"
    },
    {
        "id": 18843,
        "title": "Autonomous Decision-Making Generation of UAV based on Soft Actor-Critic Algorithm",
        "authors": "Yan Cheng, Yong Song",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188886"
    },
    {
        "id": 18844,
        "title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "link": "http://dx.doi.org/10.18421/tem111-27"
    },
    {
        "id": 18845,
        "title": "An Actor-Critic Hierarchical Reinforcement Learning Model for Course Recommendation",
        "authors": "Kun Liang, Guoqiang Zhang, Jinhui Guo, Wentao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract—Online learning platforms provide diverse course resources, but this often result in the issue of information overload. Learners always want to learn courses that are appropriate for their knowledge level and preferences quickly and accurately. Effective course recommendation plays a key role in helping learners select appropriate courses and improving the efficiency of online learning. However, when a user is enrolled in multiple courses, Existing course recommendation methods face the challenge in accurately recommending the target course that is most relevant to the user, because of the noise courses. In this paper, we propose a novel reinforcement learning model named Actor-Critic Hierarchical Reinforcement Learning (ACHRL). The model incorporates the Actor-Critic method to construct the profile reviser. This can remove noise courses and make personalized course recommendation effectively. Furthermore, we propose a policy gradient based on temporal difference error to reduce the variance in the training process, to speed up the convergence of the model, and improves the accuracy of the recommendation. We evaluate the proposed model on two real datasets, and the experimental results show that the proposed model is significantly outperforms the existing recommendation models (improving 3.77% to 13.66% in terms of HR@5).",
        "link": "http://dx.doi.org/10.20944/preprints202310.1367.v1"
    },
    {
        "id": 18846,
        "title": "Actor-Critic Instance Segmentation",
        "authors": "Nikita Araslanov, Constantin A. Rothkopf, Stefan Roth",
        "published": "2019-6",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00843"
    },
    {
        "id": 18847,
        "title": "Diffusion welding furnace temperature controller based on Actor-Critic",
        "authors": "Qiang Li, Gang Li, Xing Wang, Min Wei",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866554"
    },
    {
        "id": 18848,
        "title": "Online Virtual Training in Soft Actor-Critic for Autonomous Driving",
        "authors": "Maryam Savari, Yoonsuck Choe",
        "published": "2021-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533791"
    },
    {
        "id": 18849,
        "title": "Optimal Tracking Control for Robotic Manipulator using Actor-Critic Network",
        "authors": "Yong Hu, Lingguo Cui, Senchun Chai",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549419"
    },
    {
        "id": 18850,
        "title": "Crawling the Deep Web Using Asynchronous Advantage Actor Critic Technique",
        "authors": "Kapil Madan, Rajesh Bhatia",
        "published": "2021-6-10",
        "citations": 0,
        "abstract": "In the digital world, World Wide Web magnitude is expanding very promptly. Now a day, a rising number of data-centric websites require a mechanism to crawl the information. The information accessible through hyperlinks can easily be retrieved with general-purpose search engines. A massive chunk of the structured information is invisible behind the search forms. Such immense information is recognized as the deep web and has structured information as compared to the surface web. Crawling the content of deep web is very challenging and requires filling the search forms with suitable queries. This paper proposes an innovative technique using an Asynchronous Advantage Actor-Critic (A3C) to explore the unidentified deep web pages. It is based on the policy gradient deep reinforcement learning technique that parameterizes the policy and value function based on the reward system. A3C has one coordinator and various agents. These agents learn from different environments, update the local gradients to a coordinator, and produce a more stable system. The proposed technique has been validated with Open Directory Project (ODP). The experimental outcome shows that the proposed technique outperforms most of the prevailing techniques based on various metrics such as average precision-recall, average harvest rate, and coverage ratio.",
        "link": "http://dx.doi.org/10.13052/jwe1540-9589.20314"
    },
    {
        "id": 18851,
        "title": "Implementation on benchmark of SC2LE environment with advantage actor – critic method",
        "authors": "Huan Hu, Qingling Wang",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas48674.2020.9214032"
    },
    {
        "id": 18852,
        "title": "An accelerated asynchronous advantage actor-critic algorithm applied in papermaking",
        "authors": "Xuechun Wang, Zhiwei Zhuang, Luobao Zou, Weidong Zhang",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866243"
    },
    {
        "id": 18853,
        "title": "A deep actor critic reinforcement learning framework for learning to rank",
        "authors": "Vaibhav Padhye, Kailasam Lakshmanan",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126314"
    },
    {
        "id": 18854,
        "title": "Distributional Actor-Critic Ensemble for Uncertainty-Aware Continuous Control",
        "authors": "Takuya Kanazawa, Haiyan Wang, Chetan Gupta",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892771"
    },
    {
        "id": 18855,
        "title": "When Visible Light Communication Meets RIS: A Soft Actor-Critic Approach",
        "authors": "Long Zhang, Xingliang Jia, Ni Tian, Choong Seon Hong, Zhu Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "This letter considers a reconfigurable intelligent surface (RIS)-aided\nindoor visible light communication system, where a mirror array-based\nRIS is deployed to assist the communication from a light-emitting diode\n(LED) to multiple user terminals (UTs). We aim to maximize the sum-rate\nin an entire serving period by jointly optimizing the orientation of the\nRIS reflecting unit, the time fraction for the UT, and the transmit\npower at the LED, subject to the communication and illumination\nintensity requirements. To solve this high-dimensional non-convex\nproblem, we transform it as a constrained Markov decision process. Then,\na soft actor-critic (SAC)-based deep reinforcement learning algorithm is\nproposed with the goal of maximizing both the average reward and the\nexpected policy entropy. Simulation results prove the effectiveness of\nthe proposed SAC-based joint optimization design in improving the\nsum-rate and long-term average reward.",
        "link": "http://dx.doi.org/10.36227/techrxiv.170326666.65516785/v2"
    },
    {
        "id": 18856,
        "title": "Asymmetric Actor Critic for Image-Based Robot Learning",
        "authors": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, Pieter Abbeel",
        "published": "2018-6-26",
        "citations": 84,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2018.xiv.008"
    },
    {
        "id": 18857,
        "title": "Actor-Critic Network for O-RAN Resource Allocation: xApp Design, Deployment, and Analysis",
        "authors": "Mohammadreza Kouchaki, Vuk Marojevic",
        "published": "2022-12-4",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps56602.2022.10008713"
    },
    {
        "id": 18858,
        "title": "Implicit incremental natural actor critic algorithm",
        "authors": "Ryo Iwaki, Minoru Asada",
        "published": "2019-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2018.10.007"
    },
    {
        "id": 18859,
        "title": "A3C in Code",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_12"
    },
    {
        "id": 18860,
        "title": "Learning-based Power Delay Profile Estimation for 5G NR via Advantage Actor-Critic (A2C)",
        "authors": "Hyukjoon Kwon",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-spring54318.2022.9860924"
    },
    {
        "id": 18861,
        "title": "Multi-Agent Natural Actor-Critic Reinforcement Learning Algorithms",
        "authors": "Prashant Trivedi, Nandyala Hemachandra",
        "published": "2022-6-16",
        "citations": 1,
        "abstract": "AbstractMulti-agent actor-critic algorithms are an important part of the Reinforcement Learning (RL) paradigm. We propose three fully decentralized multi-agent natural actor-critic (MAN) algorithms in this work. The objective is to collectively find a joint policy that maximizes the average long-term return of these agents. In the absence of a central controller and to preserve privacy, agents communicate some information to their neighbors via a time-varying communication network. We prove convergence of all the three MAN algorithms to a globally asymptotically stable set of the ODE corresponding to actor update; these use linear function approximations. We show that the Kullback–Leibler divergence between policies of successive iterates is proportional to the objective function’s gradient. We observe that the minimum singular value of the Fisher information matrix is well within the reciprocal of the policy parameter dimension. Using this, we theoretically show that the optimal value of the deterministic variant of the MAN algorithm at each iterate dominates that of the standard gradient-based multi-agent actor-critic (MAAC) algorithm. To our knowledge, it is the first such result in multi-agent reinforcement learning (MARL). To illustrate the usefulness of our proposed algorithms, we implement them on a bi-lane traffic network to reduce the average network congestion. We observe an almost 25% reduction in the average congestion in 2 MAN algorithms; the average congestion in another MAN algorithm is on par with the MAAC algorithm. We also consider a generic 15 agent MARL; the performance of the MAN algorithms is again as good as the MAAC algorithm.",
        "link": "http://dx.doi.org/10.1007/s13235-022-00449-9"
    },
    {
        "id": 18862,
        "title": "An Actor-Critic Reinforcement Learning for Device-to-Device Communication Underlaying Cellular Network",
        "authors": "Pratap Khuntia, Ranjay Hazra",
        "published": "2018-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2018.8650160"
    },
    {
        "id": 18863,
        "title": "Distributed off-Policy Actor-Critic Reinforcement Learning with Policy Consensus",
        "authors": "Yan Zhang, Michael M. Zavlanos",
        "published": "2019-12",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029969"
    },
    {
        "id": 18864,
        "title": "A Novel Model-Free Actor-Critic Reinforcement Learning Approach for Dynamic Target Tracking",
        "authors": "Amr Elhussein, Md Suruz Miah",
        "published": "2020-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mic50194.2020.9209618"
    },
    {
        "id": 18865,
        "title": "Toward Resilient Multi-Agent Actor-Critic Algorithms for Distributed Reinforcement Learning",
        "authors": "Yixuan Lin, Shripad Gade, Romeil Sandhu, Ji Liu",
        "published": "2020-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147381"
    },
    {
        "id": 18866,
        "title": "Actor-Critic Learning Hierarchical Sliding Mode Control for a Class of Underactuated Systems",
        "authors": "Wei Liu, Siyi Chen, Huixian Huang",
        "published": "2019-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8997174"
    },
    {
        "id": 18867,
        "title": "Temporal Detection of Anomalies via Actor-Critic Based Controlled Sensing",
        "authors": "Geethu Joseph, M. Cenk Gursoy, Pramod K. Varshney",
        "published": "2021-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685919"
    },
    {
        "id": 18868,
        "title": "Comparing Actor-Critic Deep Reinforcement Learning Controllers for Enhanced Performance on a Ball-and-Plate System",
        "authors": "Daniel Udekwe, Ore-Ofe Ajayi, Osichinaka Ubadike",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4601865"
    },
    {
        "id": 18869,
        "title": "Visual Navigation with Actor-Critic Deep Reinforcement Learning",
        "authors": "Kun Shao, Dongbin Zhao, Yuanheng Zhu, Qichao Zhang",
        "published": "2018-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489185"
    },
    {
        "id": 18870,
        "title": "Actor-Critic Sequence Generation for Relative Difference Captioning",
        "authors": "Zhengcong Fei",
        "published": "2020-6-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3372278.3390679"
    }
]