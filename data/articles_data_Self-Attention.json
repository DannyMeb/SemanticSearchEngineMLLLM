[
    {
        "id": 10360,
        "title": "Attention, Not Self",
        "authors": "Jonardon Ganeri",
        "published": "2018-1-18",
        "citations": 9,
        "abstract": "Attention is of fundamental importance in the philosophy of mind, in epistemology, in action theory, and in ethics. This book presents an account in which attention, not self, explains the experiential and normative situatedness of human beings in the world. Attention consists in an organization of awareness and action at the centre of which there is neither a practical will nor a phenomenological witness. Attention performs two roles in experience, a selective role of placing and a focal role of access. Attention improves our epistemic standing, because it is in the nature of attention to settle on what is real and to shun what is not real. When attention is informed by expertise, it is sufficient for knowledge. That gives attention a reach beyond the perceptual: for attention is a determinable whose determinates include the episodic memory from which our narrative identities are made, the empathy for others that situates us in a social world, and the introspection that makes us self-aware. Empathy is other-directed attention, placed on you and focused on your states of mind; it is akin to listening. Empathetic attention is central to a range of experiences that constitutively require a contrast between oneself and others, all of which involve an awareness of oneself as the object of another’s attention. An analysis of attention as mental action gainsays authorial conceptions of self, because it is the nature of intending itself, effortful attention in action, to settle on what to do and to shun what not to do.",
        "link": "http://dx.doi.org/10.1093/oso/9780198757405.001.0001"
    },
    {
        "id": 10361,
        "title": "Peer Review #1 of \"Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard (v0.1)\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.15348v0.1/reviews/1"
    },
    {
        "id": 10362,
        "title": "Peer Review #3 of \"Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard (v0.1)\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.15348v0.1/reviews/3"
    },
    {
        "id": 10363,
        "title": "Peer Review #1 of \"Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard (v0.2)\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.15348v0.2/reviews/1"
    },
    {
        "id": 10364,
        "title": "Peer Review #2 of \"Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard (v0.1)\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.15348v0.1/reviews/2"
    },
    {
        "id": 10365,
        "title": "Peer Review #3 of \"Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard (v0.2)\"",
        "authors": "",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.15348v0.2/reviews/3"
    },
    {
        "id": 10366,
        "title": "Self-reported interoceptive accuracy and interoceptive attention differentially correspond to measures of visual attention and self-regard",
        "authors": "Erik M. Benau",
        "published": "2023-5-9",
        "citations": 2,
        "abstract": "\nBackground\nInteroception, the perception of bodily functions and sensations, is a crucial contributor to cognition, emotion, and well-being. However, the relationship between these three processes is not well understood. Further, it is increasingly clear that dimensions of interoception differentially corresponds to these processes, yet this is only recently being explored. The present study addresses two important questions: Are subjective interoceptive accuracy and interoceptive attention related to self-regard and well-being? And are they related to exteroceptive (visual) attention?\n\n\nMethods\nParticipants (N = 98; 29% women; aged 23–64 years) completed: a battery of questionnaires to assess subjective accuracy (how well one predicts bodily sensations), interoceptive attention (a tendency to notice bodily signals), self-regard (self-esteem, self-image, life satisfaction), state negative affect (depression, anxiety, and stress), a self-esteem Implicit Association Task (a measure of implicit self-esteem), and a flanker task to assess visual selective attention. Subjective interoceptive accuracy and attention served as dependent variables. Correlations and principal component analysis was used to establish correlations among variables and determine how, or whether, these measures are associated with subjective interoceptive accuracy or attention.\n\n\nResults\nGreater scores on measures of self-regard, implicit self-esteem, cognition and lower negative affect were broadly associated with greater subjective interoceptive accuracy. Conversely, only explicit self-esteem, satisfaction with life, and self-image corresponded to subjective interoceptive attention. An exploratory analysis with a more inclusive scale of interoceptive attention was conducted. Results of this exploratory analysis showed that the broader measure was a stronger correlate to self-regard than subjective interoceptive accuracy, though it, too, did not correlate with visual attention. In short, both subjective interoceptive accuracy and attention corresponded to well-being and mental health, but only accuracy was associated with exteroceptive attention.\n\n\nConclusion\nThese results add to a growing literature suggesting different dimensions of (subjective) interoception differentially correspond to indices of well-being. The links between exteroceptive and interoceptive attention, and their association with merit further study.\n",
        "link": "http://dx.doi.org/10.7717/peerj.15348"
    },
    {
        "id": 10367,
        "title": "Self-Attention in Evagrius Ponticus: A Specific Modality of Self-Regulation of Attention as a Way of Self-Knowledge",
        "authors": "María Teresa Gargiulo, Santiago Hernán Vázquez",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11089-022-01035-x"
    },
    {
        "id": 10368,
        "title": "Hybrid self-attention NEAT: a novel evolutionary self-attention approach to improve the NEAT algorithm in high dimensional inputs",
        "authors": "Saman Khamesian, Hamed Malek",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12530-023-09510-3"
    },
    {
        "id": 10369,
        "title": "Attention without self",
        "authors": "Silvia Caprioglio Panizza",
        "published": "2022-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003164852-4"
    },
    {
        "id": 10370,
        "title": "Homogeneous Learning: Self-Attention Decentralized Deep Learning",
        "authors": "Yuwei Sun, Hideya Ochiai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Federated learning (FL) has been facilitating privacy-preserving deep learning in many walks of life such as medical image classification, network intrusion detection, and so forth. Whereas it necessitates a central parameter server for model aggregation, which brings about delayed model communication and vulnerability to adversarial attacks. A fully decentralized architecture like Swarm Learning allows peer-to-peer communication among distributed nodes, without the central server. One of the most challenging issues in decentralized deep learning is that data owned by each node are usually non-independent and identically distributed (non-IID), causing time-consuming convergence of model training. To this end, we propose a decentralized learning model called Homogeneous Learning (HL) for tackling non-IID data with a self-attention mechanism. In HL, training performs on each round’s selected node, and the trained model of a node is sent to the next selected node at the end of each round. Notably, for the selection, the self-attention mechanism leverages reinforcement learning to observe a node’s inner state and its surrounding environment’s state, and find out which node should be selected to optimize the training. We evaluate our method with various scenarios for two different image classification tasks. The result suggests that HL can achieve a better performance compared with standalone learning and greatly reduce both the total training rounds by 50.8% and the communication cost by 74.6% for decentralized learning with non-IID data.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17456786"
    },
    {
        "id": 10371,
        "title": "Homogeneous Learning: Self-Attention Decentralized Deep Learning",
        "authors": "Yuwei Sun, Hideya Ochiai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Federated learning (FL) has been facilitating privacy-preserving deep learning in many walks of life such as medical image classification, network intrusion detection, and so forth. Whereas it necessitates a central parameter server for model aggregation, which brings about delayed model communication and vulnerability to adversarial attacks. A fully decentralized architecture like Swarm Learning allows peer-to-peer communication among distributed nodes, without the central server. One of the most challenging issues in decentralized deep learning is that data owned by each node are usually non-independent and identically distributed (non-IID), causing time-consuming convergence of model training. To this end, we propose a decentralized learning model called Homogeneous Learning (HL) for tackling non-IID data with a self-attention mechanism. In HL, training performs on each round’s selected node, and the trained model of a node is sent to the next selected node at the end of each round. Notably, for the selection, the self-attention mechanism leverages reinforcement learning to observe a node’s inner state and its surrounding environment’s state, and find out which node should be selected to optimize the training. We evaluate our method with various scenarios for two different image classification tasks. The result suggests that HL can achieve a better performance compared with standalone learning and greatly reduce both the total training rounds by 50.8% and the communication cost by 74.6% for decentralized learning with non-IID data.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17456786.v1"
    },
    {
        "id": 10372,
        "title": "Attention, Mental Causation, and the Self",
        "authors": "",
        "published": "2020-3-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108164238.004"
    },
    {
        "id": 10373,
        "title": "Should we pay more attention to self-esteem in young people?",
        "authors": "",
        "published": "2020-8-18",
        "citations": 0,
        "abstract": "Researchers in Bordeaux, France have investigated the association between self-esteem (assessed in adolescence or adulthood), with adult academic and psychosocial outcomes.",
        "link": "http://dx.doi.org/10.13056/acamh.12690"
    },
    {
        "id": 10374,
        "title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
        "authors": "Shu-wen Yang, Andy T. Liu, Hung-yi Lee",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2231"
    },
    {
        "id": 10375,
        "title": "Attention without self-concern",
        "authors": "Silvia Caprioglio Panizza",
        "published": "2022-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003164852-3"
    },
    {
        "id": 10376,
        "title": "Self-Focused Attention",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-24612-3_302394"
    },
    {
        "id": 10377,
        "title": "Self-Attention Autoencoder for Anomaly Segmentation",
        "authors": "Yang Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Anomaly detection and segmentation aim at distinguishing abnormal images from normal images and further localizing the anomalous regions. Feature reconstruction based method has become one of the mainstream methods for this task. This kind of method has two assumptions: (1) The features extracted by neural network is a good representation of the image. (2) The autoencoder solely trained on the features of normal images cannot reconstruct the features of anomalous regions well. But these two assumptions are hard to meet. In this paper, we propose a new anomaly segmentation method based on feature reconstruction. Our approach mainly consists of two parts: (1) We use a pretrained vision transformer (ViT) to extract the features of the input image. (2) We design a self-attention autoencoder to reconstruct the features. We regard that the self-attention operation which has a global receptive field is beneficial to the methods based on feature reconstruction both in feature extraction and reconstruction. The experiments show that our method outperforms the state-of-the-art approaches for anomaly segmentation on the MVTec dataset. It is both effective and time-efficient.",
        "link": "http://dx.doi.org/10.20944/preprints202108.0570.v1"
    },
    {
        "id": 10378,
        "title": "Self-Attention Based Factor Models",
        "authors": "Xiang Xiao, Xia Hua, Cynthia Qin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4634686"
    },
    {
        "id": 10379,
        "title": "Shuffle Mixing: An Efficient Alternative to Self Attention",
        "authors": "Ryouichi Furukawa, Kazuhiro Hotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011720200003417"
    },
    {
        "id": 10380,
        "title": "Self-attention Aligner: A Latency-control End-to-end Model for ASR Using Self-attention Network and Chunk-hopping",
        "authors": "Linhao Dong, Feng Wang, Bo Xu",
        "published": "2019-5",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2019.8682954"
    },
    {
        "id": 10381,
        "title": "Self-Attention Factor-Tuning for Parameter Efficient Fine-Tuning",
        "authors": "Jason Abohwo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformers have revolutionized the fields of Natural Language Processing and Computer Vision - a result of their ability to capture long-range dependencies with their key innovation: the attention mechanism. Despite the success of these models, their growing complexity has led to an ever-increasing need for processing power, making their practical applications less feasible. In recent years, tensor decomposition-based parameter-efficient fine-tuning techniques have emerged as a promising solution to the computational bottleneck. In this research, we investigate the use of a modified version of Factor Tuning that lessens inter-layer associations that the original Factor Tuning creates and focuses exclusively on attention mechanisms. We refer to this method as Self-Attention Factor-Tuning. To evaluate the effectiveness of our approach, we conduct experiments with Vision Transformers using all 19 datasets from the VTAB-1k benchmark for image classification. The results demonstrate that the proposed framework effectively reduces the number of parameters required to fine-tune a transformer, achieving new state-of-the-art performance on three of the 19 datasets in the benchmark and outperforming the original Factor-Tuning paradigm as well as various other competitive techniques, whilst using significantly fewer parameters.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3487308/v2"
    },
    {
        "id": 10382,
        "title": "Peer Review #2 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.3)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.3/reviews/2"
    },
    {
        "id": 10383,
        "title": "Korean Dependency Parsing using the Self-Attention Head Recognition Model",
        "authors": "Joon-Ho Lim, Hyun-ki Kim",
        "published": "2019-1-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/jok.2019.46.1.22"
    },
    {
        "id": 10384,
        "title": "Self-Focused Attention",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781483365817.n1212"
    },
    {
        "id": 10385,
        "title": "Self-Attention Factor-Tuning for Parameter Efficient Fine-Tuning",
        "authors": "Jason Abohwo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformers have revolutionized the fields of natural language processing and computer vision - a result of their ability to capture long-range dependencies with their key innovation: the attention mechanism. Despite the success of these models, their growing complexity has led to an ever-increasing need for processing power, making their practical applications less feasible. In recent years, tensor decomposition based parameter-efficient fine-tuning techniques have emerged as a promising solution to the computational bottleneck. In this research, we investigate the use of a specialized version of Factor Tuning that focuses exclusively on tensor decomposition for attention layers, which we refer to as Self-Attention Factor-Tuning. This selective decomposition not only allows for more inter-layer weight redundancy reduction, but also improves the model's efficiency and scalability. To evaluate the effectiveness of our approach, we conduct experiments with Vision Transformers using several benchmark datasets for image classification. The results demonstrate that the proposed framework effectively reduces the number of parameters required to train a transformer, outperforming the original Factor-Tuning paradigm as well as various other state-of-the-art techniques such as Low-Rank Adaptation whilst using significantly less parameters.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3487308/v1"
    },
    {
        "id": 10386,
        "title": "Peer Review #1 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.2)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.2/reviews/1"
    },
    {
        "id": 10387,
        "title": "Peer Review #2 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.1)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.1/reviews/2"
    },
    {
        "id": 10388,
        "title": "Peer Review #2 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.2)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.2/reviews/2"
    },
    {
        "id": 10389,
        "title": "Peer Review #1 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.1)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.1/reviews/1"
    },
    {
        "id": 10390,
        "title": "Peer Review #1 of \"Self-supervised recurrent depth estimation with attention mechanisms (v0.3)\"",
        "authors": "",
        "published": "2022-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.865v0.3/reviews/1"
    },
    {
        "id": 10391,
        "title": "Self-Attention Linguistic-Acoustic Decoder",
        "authors": "Santiago Pascual, Antonio Bonafonte, Joan Serrà",
        "published": "2018-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/iberspeech.2018-32"
    },
    {
        "id": 10392,
        "title": "An Improved Siamese Tracking Network Based On Self-Attention And Cross-Attention",
        "authors": "Lai Yijun, Song Jianmei, She Haoping",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326870"
    },
    {
        "id": 10393,
        "title": "Underwater image imbalance attenuation compensation based on attention and self-attention mechanism",
        "authors": "Danxu Wang, Yanhui Wei, Junnan Liu, Wenjia Ouyang, Xilin Zhou",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9977186"
    },
    {
        "id": 10394,
        "title": "ESRNN: Effective Residual Self-Attention Recurrent Neural Network for Sound Event Location",
        "authors": "Bin Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Sound event location is a momentous subtask of two-dimensional direction-of-arrival (2D-DOA) estimation to forecast the azimuth and elevation angle from each active sound event class of an audio fragment using multi-label regression in the 3D Cartesian coordinates. The main problem with the traditional multi-signal classification (MUSIC) algorithm and the existing baseline convolution recurrent neural network (BCRNN) are lower precision and huge computation in weaker signal-noise-ratio (SNR) environment. In particular, MUSIC algorithm will cause complete distortion when the SNR is lower than -5 dB. We thus design effective residual self-attention recurrent neural network (ESRNN) not only to overcome the distortion of traditional MUSIC algorithm under lower SNR but also further reduce predicted 2D-DOA error in different SNR reverberation environments. Two different filter structures, ESRNN-L and ESRNN-G, are designed to improve the SNR of our model when the SNR is above 0 dB or below -5 dB in a targeted manner respectively. To verify the efficiency and robustness, we train and test our model using TAU Spatial Sound Events 2019 datasets extracted the phase and magnitude spectrogram with different synthetic SNRs ranging from -10 dB to 30 dB. The experimental results show that the optimal 2D-DOA error of our ESRNN-L is 21% lower than BCRNN when the SNR is less than -5 dB, and ESRNN-G is 15% error lower with 10% parameters reduction when the SNR is higher than 0 dB.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22320505"
    },
    {
        "id": 10395,
        "title": "A Person Re-Identification Network Based upon Channel Attention and Self-Attention",
        "authors": "Mengzhe Sun, Zhaohui Wang",
        "published": "2021-10-22",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsip52628.2021.9688968"
    },
    {
        "id": 10396,
        "title": "Using Keypoint Matching and Interactive Self Attention Network to Verify Retail POSMs",
        "authors": "Harshita Seth, Sonaal Kant, Muktabh Srivastava",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011087800003209"
    },
    {
        "id": 10397,
        "title": "Self-Attention Based Optimal Factorization Machine for Ctr Prediction",
        "authors": "Zhenguo Gao, Shuang Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442847"
    },
    {
        "id": 10398,
        "title": "ESRNN: Effective Residual Self-Attention Recurrent Neural Network for Sound Event Location",
        "authors": "Bin Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Sound event location is a momentous subtask of two-dimensional direction-of-arrival (2D-DOA) estimation to forecast the azimuth and elevation angle from each active sound event class of an audio fragment using multi-label regression in the 3D Cartesian coordinates. The main problem with the traditional multi-signal classification (MUSIC) algorithm and the existing baseline convolution recurrent neural network (BCRNN) are lower precision and huge computation in weaker signal-noise-ratio (SNR) environment. In particular, MUSIC algorithm will cause complete distortion when the SNR is lower than -5 dB. We thus design effective residual self-attention recurrent neural network (ESRNN) not only to overcome the distortion of traditional MUSIC algorithm under lower SNR but also further reduce predicted 2D-DOA error in different SNR reverberation environments. Two different filter structures, ESRNN-L and ESRNN-G, are designed to improve the SNR of our model when the SNR is above 0 dB or below -5 dB in a targeted manner respectively. To verify the efficiency and robustness, we train and test our model using TAU Spatial Sound Events 2019 datasets extracted the phase and magnitude spectrogram with different synthetic SNRs ranging from -10 dB to 30 dB. The experimental results show that the optimal 2D-DOA error of our ESRNN-L is 21% lower than BCRNN when the SNR is less than -5 dB, and ESRNN-G is 15% error lower with 10% parameters reduction when the SNR is higher than 0 dB.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22320505.v1"
    },
    {
        "id": 10399,
        "title": "Easy attention: A simple self-attention mechanism for transformer-based time-series reconstruction and prediction",
        "authors": "Ricardo Vinuesa, Marcial Sanchis-Agudo, Yuning Wang, Luca Guastoni, Karthik Duraisamy",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention which we\ndemonstrate in time-series reconstruction and prediction. As a consequence of the fact that self attention only makes use of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture\nlong-term dependencies in temporal sequences. Through the singular-value decomposition (SVD)\non the softmax attention score, we further observe that self attention compresses the contributions\nfrom both queries and keys in the spanned space of the attention score. Therefore, our proposed\neasy-attention method directly treats the attention scores as learnable parameters. This approach\nproduces excellent results when reconstructing and predicting the temporal dynamics of chaotic\nsystems exhibiting more robustness and less complexity than self attention or the widely-used long\nshort-term memory (LSTM) network. Our results show great potential for applications in more\ncomplex high-dimensional dynamical systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3545247/v1"
    },
    {
        "id": 10400,
        "title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
        "authors": "Rafael Pedro, Arlindo L. Oliveira",
        "published": "2022-7-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892274"
    },
    {
        "id": 10401,
        "title": "Self Multi-Head Attention for Speaker Recognition",
        "authors": "Miquel India, Pooyan Safari, Javier Hernando",
        "published": "2019-9-15",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2616"
    },
    {
        "id": 10402,
        "title": "Short Term Power Load Forecasting Based on VMD Self Attention-LSTM",
        "authors": "俞霖 朵",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/aam.2023.123121"
    },
    {
        "id": 10403,
        "title": "Attention as a Self-Regulative System",
        "authors": "",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b18012-27"
    },
    {
        "id": 10404,
        "title": "Self-Observation: Recording the Focus of Attention*",
        "authors": "",
        "published": "2017-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780203787380-14"
    },
    {
        "id": 10405,
        "title": "S2AC: Self-Supervised Attention CORAL",
        "authors": "Zhi-Yong Wang, Dae-Ki Kang, Cui-Ping Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4445230"
    },
    {
        "id": 10406,
        "title": "Effectiveness of attention-shaping training in reinforcing attention and academic development and self-efficacy for primary school children with attention deficit hyperactive disorder",
        "authors": "M. Nazer",
        "published": "2017-4",
        "citations": 1,
        "abstract": "IntroductionThe present research have done with concentration on examining the impact of selected and divided attention-shaping training in reinforcing attention and the academic improvement and self-efficacy of primary school children with attention deficit-hyperactivity disorder.MethodThe research method was a quasi-experimental design with per- and post-test and follow-up with control group. Statistical society contained all of the primary school children with attention deficit-hyperactivity disorder in Rafsanjan City. Forty children with attention deficit-hyperactivity disorder were chosen by the method of random sampling and randomly assigned in control and experimental groups (20 children in experimental group and 20 children in control group). At first in pretest stage used academic self-efficacy questionnaire and the software of selected and divided attention. For measurements of academic improvement were used the scores of a teacher build test mathematic and spelling. Then, experimental group take 8 sessions education of selected and divided attention-shaping Training. The data were analyzed by analysis of variance with repeated measurement test.ResultsResults of this research show that selected and divided attention training improved the amount of divided and selected attention and academical improvement of children < but it's not effective significantly on reaction time and academic self-efficacy of children with attention deficit disorder-hyperactivity.ConclusionAttention training can be effective to increase attention and academical achievement in children with ADHD.Disclosure of interestThe authors have not supplied their declaration of competing interest.",
        "link": "http://dx.doi.org/10.1016/j.eurpsy.2017.01.470"
    },
    {
        "id": 10407,
        "title": "Self-Reported Attention Control Skills Moderate the Effect of Self-Focused Attention on Depression",
        "authors": "Haruyuki Ishikawa, Fusako Koshikawa",
        "published": "2021-4",
        "citations": 2,
        "abstract": " This study aimed to examine whether attention control skills (selective, switching, and divided attention) moderate the influence of self-preoccupation (the tendency to maintain self-focused attention) on depression. We conducted a cross-sectional survey at a Japanese university. A total of 283 undergraduate and graduate students answered Preoccupation Scale (measuring self-preoccupation), Voluntary Attention Control Scale (measuring self-reported attention control skills), and the Center for Epidemiologic Studies Depression Scale (the standardized measurement of depression), and we analyzed 267 questionnaires (101 men and 166 women). No cut-off points were set for screening individuals depression score. The results of the hierarchical multiple regression analysis were as follows: Higher skills of switching attention were associated with higher depression scores when combined with greater self-preoccupation tendencies. In contrast, higher levels of divided attention skill were associated with lower depression levels when combined with greater self-preoccupation. This study is the first to provide an overview of the protective role of divided attention skill against depression among individuals with high self-preoccupation. We conclude this article by showing that the interventions aiming to increase the divided attention skill rather than switching skill are expected to be effective in decreasing depressive symptoms and discussing the study’s limitations. ",
        "link": "http://dx.doi.org/10.1177/21582440211027965"
    },
    {
        "id": 10408,
        "title": "Fluctuations of attention during self-paced naturalistic goal-directed behavior in ADHD",
        "authors": "Juha Salmitaival",
        "published": "No Date",
        "citations": 0,
        "abstract": "Objective: Temporal fluctuations of attention detected in externally paced neuropsychological tests is a potential behavioral marker for attention-deficit/hyperactivity disorder (ADHD). Here we examined whether intra-individual variability (IIV) in response latencies (RLs) can be detected also in open-ended virtual real-world situations where the participants perform goal-directed behaviors at their own pace.Methods: We derived three ex-Gaussian parameters, mu, sigma, and tau, from RLs in two tasks obtained from two datasets comprising 9–13-year-old children (72 with ADHD and 71 typically developing, TD). In the naturalistic EPELI task, participants performed instructed sets of everyday household chores in a virtual apartment. The Continuous Performance Test (CPT) served as a comparison task. Results: Children with ADHD had shorter RLs than TD children in EPELI, while in the CPT their RLs were longer. Tau reflecting occasional sluggish responses was smaller in EPELI and larger in the CPT in children with ADHD compared with TD children. Within EPELI, however, the group differences in tau were dependent on whether the trials were task-relevant (smaller tau in ADHD children) or task-irrelevant (larger tau in ADHD children). Importantly, IIV in the naturalistic EPELI task explained more of the symptom variability than the CPT and correlations between the two tasks were negligible.Conclusion: We demonstrate that the task context and stimulus relevance considerably influence how the IIV in attention is manifested in ADHD. Virtual reality tasks provide a promising avenue for ecologically relevant quantification of this common cognitive deficit in neuropsychiatric disorders.",
        "link": "http://dx.doi.org/10.31234/osf.io/xtwy2"
    },
    {
        "id": 10409,
        "title": "Self-Attention for Speech Emotion Recognition",
        "authors": "Lorenzo Tarantino, Philip N. Garner, Alexandros Lazaridis",
        "published": "2019-9-15",
        "citations": 62,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2822"
    },
    {
        "id": 10410,
        "title": "Your “Attention” Deserves Attention: A Self-Diversified Multi-Channel Attention for Facial Action Analysis",
        "authors": "Xiaotian Li, Zhihua Li, Huiyuan Yang, Geran Zhao, Lijun Yin",
        "published": "2021-12-15",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fg52635.2021.9666970"
    },
    {
        "id": 10411,
        "title": "Attention Visualization of Gated Convolutional Neural Networks with Self Attention in Sentiment Analysis",
        "authors": "Hidekazu Yanagimto, Kiyota Hashimoto, Makoto Okada",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlde.2018.00024"
    },
    {
        "id": 10412,
        "title": "Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing",
        "authors": "Soohyeong Kim, Whanhee Cho, Minji Kim, Yong Choi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.25"
    },
    {
        "id": 10413,
        "title": "CHAPTER ONE The Ascetic Self",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781771103947-004"
    },
    {
        "id": 10414,
        "title": "Relationship Between Self-Control and Continuous Partial Attention: Case of Prospective Teachers",
        "authors": "Mehmet Fırat,  , Ulaş İlic,  ",
        "published": "2020-9-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17220/ijpes.2020.03.004"
    },
    {
        "id": 10415,
        "title": "Self Attention in Variational Sequential Learning for Summarization",
        "authors": "Jen-Tzung Chien, Chun-Wei Wang",
        "published": "2019-9-15",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1548"
    },
    {
        "id": 10416,
        "title": "Temporal Self Attention-Based Residual Network for Environmental Sound Classification",
        "authors": "Achyut Tripathi, Konark Paul",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-488"
    },
    {
        "id": 10417,
        "title": "Similarity and Content-based Phonetic Self Attention for Speech Recognition",
        "authors": "Kyuhong Shim, Wonyong Sung",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-422"
    },
    {
        "id": 10418,
        "title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
        "authors": "Dipesh Gyawali, Jian Zhang, Bijaya Karki",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012424500003660"
    },
    {
        "id": 10419,
        "title": "Image Super Resolution Using Multi-Layer Efficient Lightweight Self-Attention Network",
        "authors": "Qingting Tang, Liejun Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4669848"
    },
    {
        "id": 10420,
        "title": "Self-Attention Enhanced Arcface Towards Improved Deep Face Recognitions",
        "authors": "Yang Xin, Yu Zhou, Jianmin Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4637491"
    },
    {
        "id": 10421,
        "title": "Local Context-aware Self-attention for Continuous Sign Language Recognition",
        "authors": "Ronglai Zuo, Brian Mak",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-164"
    },
    {
        "id": 10422,
        "title": "Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation",
        "authors": "Vinay Kothapally, John H.L. Hansen",
        "published": "2022-9-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11277"
    },
    {
        "id": 10423,
        "title": "Self-Attention Encoding and Pooling for Speaker Recognition",
        "authors": "Pooyan Safari, Miquel India, Javier Hernando",
        "published": "2020-10-25",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1446"
    },
    {
        "id": 10424,
        "title": "Love and Attention",
        "authors": "Rob Stone",
        "published": "2022-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003240907-4"
    },
    {
        "id": 10425,
        "title": "Enformer: Encoder-based sparse periodic self-attention time-series forecasting",
        "authors": "Na Wang, Xianglian Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTime-series forecasting makes reasonable prediction and planning on stock forecast, traffic flow, power consumption and extreme weather warning through the correlation characteristics of its own multi-dimensional data and time. The proposed Transformer structure in natural language processing is proved to be beneficial for establishing the global dependence between vectors, which is undoubtedly advantageous for the time prediction problem. However, the unchanged introduction of Transformer into the time-series forecasting will also bring corresponding problems, including high computing complexity, vector information redundancy. And bound by the unique processing methods of natural language processing problems, it ignores the common processing methods of time-series forecasting problems. To this end, we design a lightweight Transformer structure that breaks the cascade structure of traditional Transformer encoders-decoders, only using encoder to build the network. At the same time, the CMM (Coarse Matching Module) is proposed to construct the multi-scale output of the input sequence, so that the subsequent networks can capture the dependencies with different time granularity. The self-attention head is also redesigned, called sparse periodic attention, which pays more attention to the periodic dependence of sequence context while reducing the computational complexity. The overall change gives full play to the advantages of Transformer, making it more suitable for time series prediction. Thanks to the synergy of the above innovative work, the accuracy of the algorithm proposed in this paper is improved by 7.1% on average in the testing experiments of multiple public datasets, and the prediction time efficiency is also improved by 44.1%. It is proved that the reasonable improvement of transformer structure in time-series prediction can reduce the amount of calculation and give consideration to the accuracy at the same time.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1786364/v1"
    },
    {
        "id": 10426,
        "title": "Condensed Convolution Neural Network by Attention over Self-attention for Stance Detection in Twitter",
        "authors": "Shengping Zhou, Junjie Lin, Lianzhi Tan, Xin Liu",
        "published": "2019-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8851965"
    },
    {
        "id": 10427,
        "title": "Application of self-attention in offline handwritten Chinese character recognition",
        "authors": "Zhanghui Chen, Guochun Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo improve the accuracy of offline handwritten Chinese character recognition (offline HCCR), combined with self-attention, this paper proposes a collaborative multi-model approach for offline HCCR. Most existing offline HCCR models use different network structures to obtain different features, which may lead to different results in terms of accuracy, and the results that errors occurred may also be different. Utilizing this feature, combined with self-attention, we propose our method hoping to improve their accuracy. In this paper, five models, including AlexNet, VGG16, GoogLeNet, ResNet34 and ResNet50, are selected and modified as base models for offline HCCR, and the outputs of the adopted 2, 3, 4, and 5 models are corrected by the self-attention fusion module. Our methods are tested on the evaluation dataset of the ICDAR 2013 Chinese Handwriting Recognition Contest. Our HCCR results obtained are at least 0.485, 0.786, 0.981 and 1.065 percentage points higher than the highest accuracy of all 2, 3, 4 and 5 base models, respectively. The experiments show that our method is effective in improving the accuracy of offline HCCR.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3293248/v1"
    },
    {
        "id": 10428,
        "title": "A Self-Attention Model for Inferring Cooperativity between Regulatory Features",
        "authors": "Fahad Ullah, Asa Ben-Hur",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTDeep learning has demonstrated its predictive power in modeling complex biological phenomena such as gene expression. The value of these models hinges not only on their accuracy, but also on the ability to extract biologically relevant information from the trained models. While there has been much recent work on developing feature attribution methods that discover the most important features for a given sequence, inferring cooperativity between regulatory elements, which is the hallmark of phenomena such as gene expression, remains an open problem. We present SATORI, a Self-ATtentiOn based model to detect Regulatory element Interactions. Our approach combines convolutional layers with a self-attention mechanism that helps us capture a global view of the landscape of interactions between regulatory elements in a sequence. A comprehensive evaluation demonstrates the ability of SATORI to identify numerous statistically significant TF-TF interactions, many of which have been previously reported. Our method is able to detect higher numbers of experimentally verified TF-TF interactions than existing methods, and has the advantage of not requiring a computationally expensive post-processing step. Finally, SATORI can be used for detection of any type of feature interaction in models that use a similar attention mechanism, and is not limited to the detection of TF-TF interactions.",
        "link": "http://dx.doi.org/10.1101/2020.01.31.927996"
    },
    {
        "id": 10429,
        "title": "CHAPTER TWO Control of the Self",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781771103947-005"
    },
    {
        "id": 10430,
        "title": "A sentence sentiment classification method based on Self-supervised and Self-attention",
        "authors": "Jianqiong Xiao, Zhiyong Zhou",
        "published": "2020-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec49072.2020.9141650"
    },
    {
        "id": 10431,
        "title": "Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention",
        "authors": "Philipp Dufter, Martin Schmitt, Hinrich Schütze",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.324"
    },
    {
        "id": 10432,
        "title": "The Effects of Self-Focused Attention on Mood: Differences of Self-Aspect Valence while Self-Focusing",
        "authors": "Rie Tabuchi, Asami Yamazaki, Megumi Oikawa",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4236/psych.2017.89093"
    },
    {
        "id": 10433,
        "title": "Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?",
        "authors": "Vandana Rajan, Alessio Brutti, Andrea Cavallaro",
        "published": "2022-5-23",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp43922.2022.9746924"
    },
    {
        "id": 10434,
        "title": "1D-SalsaSAN: Semantic Segmentation of LiDAR Point Cloud with Self-Attention",
        "authors": "Takahiro Suzuki, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011624100003417"
    },
    {
        "id": 10435,
        "title": "Attention and Self-Control in Education",
        "authors": "Yi-Yuan Tang",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-810508-5.00003-1"
    },
    {
        "id": 10436,
        "title": "Rice yield prediction and self-attention visualization using Video Vision Transformer",
        "authors": "Dahyun Kim, Myung Hwan Na,  ",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "The government and farmers' organizations are paying much attention to the problem of predicting how much rice can be produced each year. However, it is difficult to accurately predict the yield of rice due to variable factors such as extreme climate change and various pests and diseases that change every year. In this study, images were collected several times during the growing season of rice through a multi-spectral sensor mounted on an unmanned aerial vehicle, and rice yield was predicted using a deep learning algorithm. Multispectral images can be viewed as a kind of image data taken several times at regular intervals, and rice yield was predicted using the Video Vision Transformer (ViViT) model, which applies the Transformer structure to image computer vision among deep learning algorithms. The ViViT model generates patches by dividing the input image into a certain size, and as a result of learning the model by setting the size of these patches differently, it was found that the smaller the patch size, the better the predictive power. In addition, as a result of comparing prediction performance with a 3D CNN model that receives an image as an input in a CNN (Convolutional Neural Network) structure used in the image processing field, it was found that the ViViT model using a small patch size performed better. As a result of visualizing the weight matrix of the ViViT model as a heat map, images taken in mid- to late August appear to be important in yield prediction, making it possible to predict yield about two months before rice harvest.",
        "link": "http://dx.doi.org/10.37727/jkdas.2023.25.4.1249"
    },
    {
        "id": 10437,
        "title": "Interpretable Multi-Head Self-Attention Architecture for Sarcasm Detection in Social Media",
        "authors": "Ramya Akula, Ivan Garibay",
        "published": "No Date",
        "citations": 5,
        "abstract": "Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions, make sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. Multi-head self-attention module aids in identifying crucial sarcastic cue-words from the input, and the recurrent units learn long-range dependencies between these cue-words to better classify the input text. We show the effectiveness of our approach by achieving state-of-the-art results on multiple datasets from social networking platforms and online media. Models trained using our proposed approach are easily interpretable and enable identifying sarcastic cues in the input text which contribute to the final classification score. We visualize the learned attention weights on few sample input texts to showcase the effectiveness and interpretability of our model.",
        "link": "http://dx.doi.org/10.20944/preprints202101.0302.v1"
    },
    {
        "id": 10438,
        "title": "Attention, Not Self",
        "authors": "Monima Chadha",
        "published": "2019-7-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1215/00318108-7537335"
    },
    {
        "id": 10439,
        "title": "A dual attention mechanism network with self-attention and frequency channel attention for intelligent diagnosis of multiple rolling bearing fault types",
        "authors": "Wenxing Zhang, Jianhong Yang, Xinyu Bo, Zhenkai Yang",
        "published": "2024-3-1",
        "citations": 1,
        "abstract": "Abstract\nDifferent fault types of rolling bearings correspond to different features, and classical deep learning models using a single attention mechanism (AM) have limitations in capturing feature diversity. Therefore, a novel dual attention mechanism network (DAMN) with self-attention (SA) and frequency channel attention (FCA) is proposed for rolling bearing fault diagnosis. The SA mechanism is used to capture global relationships between the input features and fault types, and the FCA mechanism applies multi-spectral attention to learn the local useful information among different input channels. The results of the ablation study on the effects of FCA blocks showed that including a proper combination of multiple frequency components is helpful in achieving higher accuracy. Experiments were conducted to diagnose rolling bearings with multiple types of faults. The results show that, compared with current fault diagnosis models, the proposed DAMN has better comprehensive performance in terms of diagnosis accuracy and model convergence speed. It was also demonstrated that the backbone of DAMN based on a dual AM could achieve better performance than the backbone based on a single AM.",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad1811"
    },
    {
        "id": 10440,
        "title": "Self-Attention and Forgetting Fusion Knowledge Tracking Algorithm",
        "authors": "Song Jianfeng, Wang Yukai, Zhang Chu, Xie Kun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4520741"
    },
    {
        "id": 10441,
        "title": "Testing the Feasibility of a Media Multitasking Self-regulation Intervention for Students: Behaviour Change, Attention, and Self-perception",
        "authors": "Douglas Parry, Daniel B. le Roux, Jason Bantjes",
        "published": "No Date",
        "citations": 0,
        "abstract": "Media multitasking has been associated with a number of adverse cognitive, psychosocial, and functional outcomes. In particular, associations between media multitasking and the executive or cognitive control processes theorised to underlie the execution of goal-directed behaviour have been shown. In response to calls for investigations considering the remedial efficacy of interventions targeting media multitasking and related cognitive effects, the present study investigates the feasibility of a self-regulation based media multitasking intervention for a student population. Through a mixed-methods study involving a between-subjects, pre/post experimental design, usage tracking, and follow-up interviews, four feasibility dimensions were investigated: demand, implementation, acceptability, and efficacy. The findings indicate, firstly, that a greater cognisance of media behaviour is key to behaviour change and goal-alignment, secondly, that such behavioural changes were perceived to enable more instances of single-tasking, goal-oriented task-execution and, as a result, engender state-level changes in attentional strategies and, thirdly, that short-term behavioural changes do not necessarily imply trait-level changes in cognitive functioning. Key implications for media-effects research in general and, more specifically, for research concerning media-related interference are discussed.",
        "link": "http://dx.doi.org/10.31234/osf.io/r8kdp"
    },
    {
        "id": 10442,
        "title": "SABRE: Self-Attention Based model for predicting T-cell Receptor Epitope Specificity",
        "authors": "Zicheng Wang, Yufeng Shen",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractT cell receptors (TCR) recognize antigens on the surface of T cells, which is the critical event in the adaptive immune response to infection and vaccination. The ability to determine TCR-antigen recognition would benefit research in basic immunology and therapeutics. High-throughput experimental approaches for determining TCR-antigen specificity have produced valuable data, but the TCR-antigen pairing space is astronomically more significant than what can reached by experiments. Here, we describe a computational method for predicting TCR-antigen recognition, SABRE (Self-Attention-based Transformer Model for predicting T-cell Receptor-Epitope specificity). SABRE captures sequence properties of matching TCR and antigen pairs by selfsupervised pre-training using known pairs from curated databases and large-scale experiments. It then fine-tunes by supervised learning to predict TCRs that can recognize each antigen. We showed that SABRE’s AUROC reaches 0.726 ± 0.008 for predicting TCR-epitope recognition. We meticulously designed a training and testing scheme to evaluate the model’s performance on unseen TCR species: 60% of the data was allocated for training, 20% for validation, and the remaining 20% exclusively for testing. Notably, this testing set comprised entirely of TCRs not present in the training phase, ensuring a genuine assessment of the model’s ability to generalize to novel data.",
        "link": "http://dx.doi.org/10.1101/2023.10.02.560555"
    },
    {
        "id": 10443,
        "title": "Attention meets Geometry: Geometry Guided Spatial-Temporal Attention for Consistent Self-Supervised Monocular Depth Estimation",
        "authors": "Patrick Ruhkamp, Daoyi Gao, Hanzhi Chen, Nassir Navab, Beniamin Busam",
        "published": "2021-12",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/3dv53792.2021.00092"
    },
    {
        "id": 10444,
        "title": "Self-care is worthy of our attention",
        "authors": "Narelle Lemon",
        "published": "2021-10-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003144397-1"
    },
    {
        "id": 10445,
        "title": "Multi-label text classification based on graph attention network and self-attention mechanism",
        "authors": "Can Lin, Cui Zhu, Wenjun Zhu",
        "published": "2022-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2653459"
    },
    {
        "id": 10446,
        "title": "Interpreting Decision of Self-Attention Network’s in the Context of Image Recognition by Efficiently Utilizing Attention Scores",
        "authors": "S.M. Haider Ali Shuvo",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eict61409.2023.10427964"
    },
    {
        "id": 10447,
        "title": "Self-Supervised Monocular Trained Depth Estimation Using Self-Attention and Discrete Disparity Volume",
        "authors": "Adrian Johnston, Gustavo Carneiro",
        "published": "2020-6",
        "citations": 140,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr42600.2020.00481"
    },
    {
        "id": 10448,
        "title": "Self-Supervised Temporal Graph Learning based on Multi-Head Self-Attention Weighted Neighborhood Sequences",
        "authors": "Yulong Cao",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdai59165.2023.10256426"
    },
    {
        "id": 10449,
        "title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers",
        "authors": "Juntae Kim, Jeehye Lee",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-581"
    },
    {
        "id": 10450,
        "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
        "authors": "Axel Berg, Mark O’Connor, Miguel Tairum Cruz",
        "published": "2021-8-30",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1286"
    },
    {
        "id": 10451,
        "title": "PhyloTransformer: A Self-supervised Discriminative Model for Mutation Prediction Based on a Multi-head Self-attention Mechanism",
        "authors": "Yingying Wu, Shusheng Xu, Shing-Tung Yau, Yi Wu",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nAlthough coronaviruses have RNA proofreading functions, a large number of variants still exist as quasispecies. Identified coronaviruses might just be the tip of the iceberg, and potentially more fatal variants of concern (VOCs) may emerge over time. These VOCs may exhibit increased pathogenicity, infectivity, transmissibility, angiotensin-converting enzyme 2 (ACE2) binding affinity, and antigenicity, causing an increased threat to public health. In this article, we developed PhyloTransformer, a Transformer-based self-supervised discriminative model, which can model genetic mutations that may lead to viral reproductive advantage. We trained PhyloTransformer on 1,765,297 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) sequences to infer fitness advantages, by directly modeling the amino acid sequence mutations. PhyloTransformer utilizes advanced techniques from natural language processing, including the Fast Attention Via positive Orthogonal Random features approach (FAVOR+) and the Masked Language Model (MLM), which enable efficient and accurate intra-sequence dependency modeling over the entire RNA sequence. We measured the prediction accuracy of novel mutations and novel combinations using our method and baseline models that only take local segments as input. We found that PhyloTransformer outperformed every baseline method with statistical significance. In order to identify mutations associated with altered glycosylation that might be favored during viral evolution, we predicted the occurrence of mutations in each nucleotide of the receptor binding motif (RBM) and predicted modifications of N-glycosylation sites. We anticipate that the viral mutations predicted by PhyloTransformer may identify potential mutations of threat to guide therapeutics and vaccine design for effective targeting of future SARS-CoV-2 variants.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1578020/v1"
    },
    {
        "id": 10452,
        "title": "I'm so touched! Self-touch increases attitude extremity via self-focused attention",
        "authors": "Ann Kronrod, Joshua M. Ackerman",
        "published": "2019-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.actpsy.2019.02.005"
    },
    {
        "id": 10453,
        "title": "Attention Not Self",
        "authors": "Jane Suilin Lavelle",
        "published": "2020-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/pq/pqaa028"
    },
    {
        "id": 10454,
        "title": "An Anomaly Prediction of Spark Log Based on Self-Attention GRU Network",
        "authors": "Yanyu Gong, Xinjiang Chen, Xiaoli Zhang, Haotian Xu, Xue Zhang, Haifeng Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012285100003807"
    },
    {
        "id": 10455,
        "title": "The ratio of voluntary attention in the performance of tasks and self-control of professional activity",
        "authors": "Natalya Mihaylova",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5cf632c4af72dec2b0554be2"
    },
    {
        "id": 10456,
        "title": "Simone Weil, Performance, and Attention",
        "authors": "Giuliano Campo",
        "published": "2022-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003173076-10"
    },
    {
        "id": 10457,
        "title": "Low-Resolution Image Classification Using Knowledge Distillation From High-Resolution Image Via Self-Attention Map",
        "authors": "Sungho Shin, Joosoon Lee, Junseok Lee, Seungjun Choi, Kyoobin Lee",
        "published": "2020-11-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/jok.2020.47.11.1027"
    },
    {
        "id": 10458,
        "title": "Chapter 11 How Can Attention Seeking Be Good? From Strategic Ignorance to Self-Experiments",
        "authors": "Cor van der Weele",
        "published": "2022-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783110647242-012"
    },
    {
        "id": 10459,
        "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
        "authors": "Saebom Leem, Hyunseok Seo",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
        "link": "http://dx.doi.org/10.1609/aaai.v38i4.28077"
    }
]