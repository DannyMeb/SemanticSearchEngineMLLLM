[
    {
        "id": 23705,
        "title": "Exploring the Transfer Learning: A Comparative Study of Pre-trained Models and Fine-tuning Techniques on image dataset",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56452/6-3-664"
    },
    {
        "id": 23706,
        "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
        "authors": "Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.336"
    },
    {
        "id": 23707,
        "title": "Math Function Recognition with Fine-Tuning Pre-Trained Models",
        "authors": "Fatimah Alshamari, Abdou Youssef",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "A Mathematical Function Recognition (MFR) is an important research direction for efficient downstream math tasks such as information retrieval, knowledge extraction, and question answering. The aim of this task is to identify and classify mathematical function into a predefined set of function. However, the lack of annotated data is the bottleneck in the development of an MFR automated model. We begin this paper by describing our approach to creating a labelled dataset for MFR. Then, to identify five categories of mathematical functions, we fine-tuned a set of common pre-trained models: BERT base-cased, BERT baseuncased, DistilBERT-cased, and DistilBERT-uncased. As a result, our contributions in this paper include: (1) an annotated MFR dataset that future researchers can use; and (2) SOTA results obtained by finetuning pre-trained models for the MFR task. Our experiments demonstrate that the proposed approach achieved a high-quality recognition, with an F1 score of 96.80% on a held-out test set provided by DistilBERT-cased model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijci.2023.120204"
    },
    {
        "id": 23708,
        "title": "Prompting and Fine-tuning Pre-trained Generative Language Models",
        "authors": "Johny Moreira, Altigran da Silva, Luciano Barbosa",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "There has been an explosion of available pre-trained and fine-tuned Generative Language Models (LM). They vary in the number of parameters, architecture, training strategy, and training set size. Aligned with it, alternative strategies exist to exploit these models, such as Fine-tuning and Prompt Engineering. However, many questions may arise throughout this process: Which model to apply for a given task? Which strategies to use? Will Prompt Engineering solve all tasks? What are the computational and financial costs involved? This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5753/sbbd_estendido.2023.25636"
    },
    {
        "id": 23709,
        "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
        "authors": "Abir Betka, Zeyd Ferhat, Riyadh Barka, Selma Boutiba, Zineddine Kahhoul, Tiar Lakhdar, Ahmed Abdelali, Habiba Dahmani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.33"
    },
    {
        "id": 23710,
        "title": "Comparing The Fine-Tuning and Performance of Whisper Pre-Trained Models for Turkish Speech Recognition Task",
        "authors": "Saadin Oyucu",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304891"
    },
    {
        "id": 23711,
        "title": "PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent",
        "authors": "Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Johnson, Rongrong Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.748"
    },
    {
        "id": 23712,
        "title": "An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models",
        "authors": "Jiaxing Liu, Chaofeng Sha, Xin Peng",
        "published": "2023-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ase56229.2023.00125"
    },
    {
        "id": 23713,
        "title": "On Fine-Tuning Pre-Trained Speech Models With EMA-Target Self-Supervised Loss",
        "authors": "Hejung Yang, Hong-Goo Kang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446468"
    },
    {
        "id": 23714,
        "title": "Improving Fine-tuning Pre-trained Models on Small Source Code Datasets via Variational Information Bottleneck",
        "authors": "Jiaxing Liu, Chaofeng Sha, Xin Peng",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/saner56733.2023.00039"
    },
    {
        "id": 23715,
        "title": "Fine-tuning Pre-trained Language Models to Detect In-game Trash Talks",
        "authors": "Daniel Fesalbon -, Arvin De La Cruz -, Marvin Mallari -, Nelson Rodelas -",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages. The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models’ state-of-the-art performance, this study concludes pre-trained language models’ promising potential for addressing online hate speech and in-game insulting trash talk.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2024.v06i02.14927"
    },
    {
        "id": 23716,
        "title": "Reducing Bias in Pre-Trained Models by Tuning While Penalizing Change",
        "authors": "Niklas Penzel, Gideon Stein, Joachim Denzler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012345800003660"
    },
    {
        "id": 23717,
        "title": "Comparative Analysis of Fine-tuning Multiple Pre- Trained Convolutional Neural Network (CNN) Models for Oryza Sativa Disease Detection",
        "authors": "Roky Das, Iqbal Ahmed",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5120/ijca2023923117"
    },
    {
        "id": 23718,
        "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
        "authors": "Ting Jiang, Deqing Wang, Fuzhen Zhuang, Ruobing Xie, Feng Xia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.35"
    },
    {
        "id": 23719,
        "title": "Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-Trained Language Models",
        "authors": "Youness Moukafih, Mounir Ghogho, Kamel Smaili",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095108"
    },
    {
        "id": 23720,
        "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
        "authors": "Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, Greg Ver Steeg",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.539"
    },
    {
        "id": 23721,
        "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
        "authors": "Jungwoo Heo, Chan-yeong Lim, Ju-ho Kim, Hyun-seo Shin, Ha-Jin Yu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-605"
    },
    {
        "id": 23722,
        "title": "GamMa: Efficient Fine-Tuning of Pre-Trained Language Models Using Gradient Activation Mapping Masking",
        "authors": "Anchun Gui, Jinqiang Ye, Han Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191351"
    },
    {
        "id": 23723,
        "title": "Confounder balancing in adversarial domain adaptation for pre-trained large models fine-tuning",
        "authors": "Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan, Xiangping Wu, Yukang Lin",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106173"
    },
    {
        "id": 23724,
        "title": "Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts",
        "authors": "Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Zhang, Jun Zhou, Defu Lian, Ying Wei",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.808"
    },
    {
        "id": 23725,
        "title": "Analyzing Pre-trained and Fine-tuned Language Models",
        "authors": "Marius Mosbach",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bigpicture-1.10"
    },
    {
        "id": 23726,
        "title": "Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation",
        "authors": "Mohammadreza Tayaranian Hosseini, Alireza Ghaffari, Marzieh S. Tahaei, Mehdi Rezagholizadeh, Masoud Asgharian, Vahid Partovi Nia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.143"
    },
    {
        "id": 23727,
        "title": "Rhythmic lyrics translation: Customizing a pre-trained language model using stacked fine-tuning",
        "authors": "Jiwon Chong, Jaehyok Chong",
        "published": "2023",
        "citations": 0,
        "abstract": "Neural machine translation (NMT) is a software that uses neural network techniques to translate text from one language to another. As NMT models are on the rise, the focus is on translating everyday mundane sentences. However, it is also necessary to start paying attention to the translation of domain-specific text, such as lyrics or poetry. For example, even one of the most famous NMT models—Google Translate—failed to give an accurate English translation of a famous Korean nursery rhyme, \"Airplane\" (비행기). To teach the model to retain specific information other than semantics, we need specific data which contains the exact information that we are attempting to teach. In the case of rhythmically accurate lyrics translation—translated lyrics that can be used to sing along to the original melody—we need corresponding data, containing lyrical and rhythmical properties, all the while being semantically accurate. However, as there is not enough data that fits our criteria, we propose a novel method we call 'stacked fine-tuning'. We fine-tuned a pre-trained model first with a dataset from the lyrics domain, and then with a smaller dataset containing the rhythmical properties, to teach the model to translate rhythmically accurate lyrics. To evaluate the effectiveness of our approach, we translated two famous Korean nursery rhymes to English and matched them to the original melody. Our stacked fine-tuning method resulted in an NMT model that could maintain the rhythmical characteristics of lyrics during translation while single fine-tuned models failed to do so.",
        "keywords": "",
        "link": "http://dx.doi.org/10.59720/22-177"
    },
    {
        "id": 23728,
        "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
        "authors": "Zhong Zhang, Bang Liu, Junming Shao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.95"
    },
    {
        "id": 23729,
        "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
        "authors": "Rheeya Uppaal, Junjie Hu, Yixuan Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.717"
    },
    {
        "id": 23730,
        "title": "KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models for Clinical Trial Entailment Identification",
        "authors": "Weiqi Wang, Baixuan Xu, Tianqing Fang, Lirong Zhang, Yangqiu Song",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.1"
    },
    {
        "id": 23731,
        "title": "Fine-Tuning Pre-Trained Models for Automated Analysis of Ophthalmic Imaging in Diagnosing Eye Diseases",
        "authors": "Rabia Emhamed Al Mamlook, Ahmad Nasayreh, Hasan Gharaibeh, Mohammad Aljaidi, Qais Al-Na'amneh, Dalia Alzu'bi, Hanin Bzizi, Abdelrahman Abdeltawab",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acit58888.2023.10453701"
    },
    {
        "id": 23732,
        "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
        "authors": "Yiwen Tang, Ray Zhang, Zoey Guo, Xianzheng Ma, Bin Zhao, Zhigang Wang, Dong Wang, Xuelong Li",
        "published": "2024-3-24",
        "citations": 1,
        "abstract": "The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggregate point cloud features within spatial neighborhoods to capture fine-grained geometric information through local interactions. Extensive experiments indicate that our Point-PEFT can achieve better performance than the full fine-tuning on various downstream tasks, while using only 5% of the trainable parameters, demonstrating the efficiency and effectiveness of our approach. Code is released at https://github.com/Ivan-Tang-3D/Point-PEFT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i6.28323"
    },
    {
        "id": 23733,
        "title": "Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond",
        "authors": "Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, Hongbin Sun",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597926.3598036"
    },
    {
        "id": 23734,
        "title": "Fine-Tuning BERT-Based Pre-Trained Models for Arabic Dependency Parsing",
        "authors": "Sharefah Al-Ghamdi, Hend Al-Khalifa, Abdulmalik Al-Salman",
        "published": "2023-3-27",
        "citations": 5,
        "abstract": "With the advent of pre-trained language models, many natural language processing tasks in various languages have achieved great success. Although some research has been conducted on fine-tuning BERT-based models for syntactic parsing, and several Arabic pre-trained models have been developed, no attention has been paid to Arabic dependency parsing. In this study, we attempt to fill this gap and compare nine Arabic models, fine-tuning strategies, and encoding methods for dependency parsing. We evaluated three treebanks to highlight the best options and methods for fine-tuning Arabic BERT-based models to capture syntactic dependencies in the data. Our exploratory results show that the AraBERTv2 model provides the best scores for all treebanks and confirm that fine-tuning to the higher layers of pre-trained models is required. However, adding additional neural network layers to those models drops the accuracy. Additionally, we found that the treebanks have differences in the encoding techniques that give the highest scores. The analysis of the errors obtained by the test examples highlights four issues that have an important effect on the results: parse tree post-processing, contextualized embeddings, erroneous tokenization, and erroneous annotation. This study reveals a direction for future research to achieve enhanced Arabic BERT-based syntactic parsing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13074225"
    },
    {
        "id": 23735,
        "title": "Layer-wise Fine-tuning Based Pre-trained Language Model Syntactic Knowledge Injection Methodology",
        "authors": "Hye-Lynn Kim, Sanghyun Cho, Hyuk-Chul Kwon",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.6109/jkiice.2023.27.12.1473"
    },
    {
        "id": 23736,
        "title": "Fine-Tuning of Pre-Trained Deep Face Sketch Models Using Smart Switching Slime Mold Algorithm",
        "authors": "Khaled Mohammad Alhashash, Hussein Samma, Shahrel Azmin Suandi",
        "published": "2023-4-19",
        "citations": 0,
        "abstract": "There are many pre-trained deep learning-based face recognition models developed in the literature, such as FaceNet, ArcFace, VGG-Face, and DeepFace. However, performing transfer learning of these models for handling face sketch recognition is not applicable due to the challenge of limited sketch datasets (single sketch per subject). One promising solution to mitigate this issue is by using optimization algorithms, which will perform a fine-tuning and fitting of these models for the face sketch problem. Specifically, this research introduces an enhanced optimizer that will evolve these models by performing automatic weightage/fine-tuning of the generated feature vector guided by the recognition accuracy of the training data. The following are the key contributions to this work: (i) this paper introduces a novel Smart Switching Slime Mold Algorithm (S2SMA), which has been improved by embedding several search operations and control rules; (ii) the proposed S2SMA aims to fine-tune the pre-trained deep learning models in order to improve the accuracy of the face sketch recognition problem; and (iii) the proposed S2SMA makes simultaneous fine-tuning of multiple pre-trained deep learning models toward further improving the recognition accuracy of the face sketch problem. The performance of the S2SMA has been evaluated on two face sketch databases, which are XM2VTS and CUFSF, and on CEC’s 2010 large-scale benchmark. In addition, the outcomes were compared to several variations of the SMA and related optimization techniques. The numerical results demonstrated that the improved optimizer obtained a higher level of fitness value as well as better face sketch recognition accuracy. The statistical data demonstrate that S2SMA significantly outperforms other optimization techniques with a rapid convergence curve.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13085102"
    },
    {
        "id": 23737,
        "title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network",
        "authors": "Rongxiang Weng, Wen Sen Cheng, Min Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.291"
    },
    {
        "id": 23738,
        "title": "BERT4ST:: Fine-tuning pre-trained large language model for wind power forecasting",
        "authors": "Zefeng Lai, Tangjie Wu, Xihong Fei, Qiang Ling",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.enconman.2024.118331"
    },
    {
        "id": 23739,
        "title": "Two-Stage Fine-Tuning For Low-resource Englishbased Creole with Pre-Trained LLMs",
        "authors": "Shu Hui Amanda Tan, Eint Sandi Aung, Hayato YAMANA",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csde59766.2023.10487143"
    },
    {
        "id": 23740,
        "title": "$$\\cal{Y}$$-Tuning: an efficient tuning paradigm for large-scale pre-trained models via label representation learning",
        "authors": "Yitao Liu, Chenxin An, Xipeng Qiu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-3131-8"
    },
    {
        "id": 23741,
        "title": "Efficient Adapter Tuning of Pre-Trained Speech Models for Automatic Speaker Verification",
        "authors": "Mufan Sang, John H.L. Hansen",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446686"
    },
    {
        "id": 23742,
        "title": "Fine-tuning a pre-trained ResNet50 model to detect distributed  denial of service attack",
        "authors": "Ahmad Sanmorino, Hendra Di Kesuma",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "Distributed denial-of-service (DDoS) attacks pose a significant risk to the dependability and consistency of network services. The utilization of deep learning (DL) models has displayed encouraging outcomes in the identification of DDoS attacks. Nevertheless, crafting a precise DL model necessitates an extensive volume of labeled data and substantial computational capabilities. Within this piece, we introduce a technique to enhance a pre-trained DL model for the identification of DDoS attacks. Our strategy’s efficacy is showcased on an openly accessible dataset, revealing that the fine-tuned model we propose surpasses both the initial pre-trained model and other cutting-edge approaches in performance. The suggested fine-tuned model attained 95.1% accuracy, surpassing the initial pre-trained model as well as other leading-edge techniques. Please note that the specific evaluation metrics and their values may vary depending on the implementation, hyperparameter settings, number of datasets, or dataset characteristics. The proposed approach has several advantages, including reducing the amount of labeled data required and accelerating the training process. Initiating with a pre-existing ResNet50 model can also enhance the eventual model’s accuracy, given that the pre-trained model has already acquired the ability to extract significant features from unprocessed data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/eei.v13i2.7014"
    },
    {
        "id": 23743,
        "title": "TRUST-SER: On The Trustworthiness Of Fine-Tuning Pre-Trained Speech Embeddings For Speech Emotion Recognition",
        "authors": "Tiantian Feng, Rajat Hebbar, Shrikanth Narayanan",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446616"
    },
    {
        "id": 23744,
        "title": "Co2PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
        "authors": "Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.390"
    },
    {
        "id": 23745,
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": "Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.182"
    },
    {
        "id": 23746,
        "title": "MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained Vision-Language Models",
        "authors": "Yongzhu Miao, Shasha Li, Jintao Tang, Ting Wang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00013"
    },
    {
        "id": 23747,
        "title": "WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive Learning",
        "authors": "Ghadeer Mobasher, Wolfgang Müller, Olga Krebs, Michael Gertz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.40"
    },
    {
        "id": 23748,
        "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
        "authors": "Jun Bai, Xiaofeng Zhang, Chen Li, Hanhua Hong, Xi Xu, Chenghua Lin, Wenge Rong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.357"
    },
    {
        "id": 23749,
        "title": "Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues",
        "authors": "Chuyuan Li, Patrick Huber, Wen Xiao, Maxime Amblard, Chloe Braud, Giuseppe Carenini",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.194"
    },
    {
        "id": 23750,
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "authors": "Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.410"
    },
    {
        "id": 23751,
        "title": "Zero-shot language extension for dialogue state tracking via pre-trained models and multi-auxiliary-tasks fine-tuning",
        "authors": "Lu Xiang, Yang Zhao, Junnan Zhu, Yu Zhou, Chengqing Zong",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.110015"
    },
    {
        "id": 23752,
        "title": "Unveiling the Power of Pre - Trained Language Models in NLP Applications",
        "authors": "Shrinath Pai",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115202502"
    },
    {
        "id": 23753,
        "title": "Enhancing Pre-Trained ASR System Fine-Tuning for Dysarthric Speech Recognition Using Adversarial Data Augmentation",
        "authors": "Huimeng Wang, Zengrui Jin, Mengzhe Geng, Shujie Hu, Guinan Li, Tianzi Wang, Haoning Xu, Xunying Liu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447702"
    },
    {
        "id": 23754,
        "title": "Fine-Tuning Pre-Trained Language Model for Urgency Classification on Food Safety Feedback",
        "authors": "Umamaheswari Vasanthakumar, Jia Rui Bryna Goh, Siu Cheung Hui, Kwok Yan Lam, Benjamin Er, Muhd Tarmidzi Fua'di, Kyaw Thu Aung",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciss59129.2023.10291215"
    },
    {
        "id": 23755,
        "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation",
        "authors": "Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, Ali Ghodsi",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.239"
    },
    {
        "id": 23756,
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models",
        "authors": "Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, Zenglin Xu",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.632"
    },
    {
        "id": 23757,
        "title": "Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization",
        "authors": "Ze-Feng Gao, Kun Zhou, Peiyu Liu, Wayne Xin Zhao, Ji-Rong Wen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.212"
    },
    {
        "id": 23758,
        "title": "Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models",
        "authors": "Lei Wang, Jiabang He, Xing Xu, Ning Liu, Hui Liu",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Alignment between image and text has shown promising improvements on patch-level pre-trained document image models. However, investigating more effective or finer-grained alignment techniques during pre-training requires a large amount of computation cost and time. Thus, a question naturally arises: Could we fine-tune the pre-trained models adaptive to downstream tasks with alignment objectives and achieve comparable or better performance? In this paper, we propose a new model architecture with alignment-enriched tuning (dubbed AETNet) upon pre-trained document image models, to adapt downstream tasks with the joint task-specific supervised and alignment-aware contrastive objective. Specifically, we introduce an extra visual transformer as the alignment-ware image encoder and an extra text transformer as the alignment-ware text encoder before multimodal fusion. We consider alignment in the following three aspects: 1) document-level alignment by leveraging the cross-modal and intra-modal contrastive loss; 2) global-local alignment for modeling localized and structural information in document images; and 3) local-level alignment for more accurate patch-level information. Experiments on various downstream tasks show that AETNet can achieve state-of-the-art performance on various downstream tasks. Notably, AETNet consistently outperforms state-of-the-art pre-trained models, such as LayoutLMv3 with fine-tuning techniques, on three different downstream tasks. Code is available at https://github.com/MAEHCM/AET.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i2.25357"
    },
    {
        "id": 23759,
        "title": "nanoT5: Fast &amp; Simple Pre-training and Fine-tuning of T5 Models with Limited Resources",
        "authors": "Piotr Nawrot",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.11"
    },
    {
        "id": 23760,
        "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models",
        "authors": "Kecheng Zheng, Wei Wu, Ruili Feng, Kai Zhu, Jiawei Liu, Deli Zhao, Zheng-Jun Zha, Wei Chen, Yujun Shen",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01071"
    },
    {
        "id": 23761,
        "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models",
        "authors": "Xuxi Chen, Tianlong Chen, Weizhu Chen, Ahmed Hassan Awadallah, Zhangyang Wang, Yu Cheng",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.456"
    },
    {
        "id": 23762,
        "title": "Fine-Tuning Air Pollution Models",
        "authors": "Sarah Derouin",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "InMAP estimates air pollution within cities, but its predictions are flawed for specific chemicals. Now, scientists are addressing that shortcoming.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1029/2023eo230181"
    },
    {
        "id": 23763,
        "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
        "authors": "Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, Maosong Sun",
        "published": "2023-3-2",
        "citations": 53,
        "abstract": "AbstractWith the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s42256-023-00626-4"
    },
    {
        "id": 23764,
        "title": "Fine-Tuning Pre-Trained CodeBERT for Code Search in Smart Contract",
        "authors": "Huan JIN, Qinying LI",
        "published": "2023-6",
        "citations": 0,
        "abstract": "Smart contracts, which automatically execute on decentralized platforms like Ethereum, require high security and low gas consumption. As a result, developers have a strong demand for semantic code search tools that utilize natural language queries to efficiently search for existing code snippets. However, existing code search models face a semantic gap between code and queries, which requires a large amount of training data. In this paper, we propose a fine-tuning approach to bridge the semantic gap in code search and improve the search accuracy. We collect 80 723 different pairs of <comment, code snippet> from Etherscan.io and use these pairs to fine-tune, validate, and test the pre-trained CodeBERT model. Using the fine-tuned model, we develop a code search engine specifically for smart contracts. We evaluate the Recall@k and Mean Reciprocal Rank (MRR) of the fine-tuned CodeBERT model using different proportions of the fine-tuned data. It is encouraging that even a small amount of fine-tuned data can produce satisfactory results. In addition, we perform a comparative analysis between the fine-tuned CodeBERT model and the two state-of-the-art models. The experimental results show that the fine-tuned CodeBERT model has superior performance in terms of Recall@k and MRR. These findings highlight the effectiveness of our fine-tuning approach and its potential to significantly improve the code search accuracy. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/wujns/2023283237"
    },
    {
        "id": 23765,
        "title": "CPT: Colorful Prompt Tuning for pre-trained vision-language models",
        "authors": "Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun",
        "published": "2024",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aiopen.2024.01.004"
    },
    {
        "id": 23766,
        "title": "OPTIMIZING ULTRASOUND IMAGE CLASSIFICATION THROUGH TRANSFER LEARNING: FINE-TUNING STRATEGIES AND CLASSIFIER IMPACT ON PRE-TRAINED INNER-LAYERS",
        "authors": "Mohamed Bal-Ghaoui, My Hachem El Yousfi Alaoui, Abdelilah Jilbab, Abdennaser Bourouhou",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "Transfer Learning (TL) is a popular deep learning technique used in medical image analysis, especially when data is limited. It leverages pre-trained knowledge from State-Of-The-Art (SOTA) models and applies it to specific applications through Fine-Tuning (FT). However, fine-tuning large models can be time-consuming, and determining which layers to use can be challenging. This study explores different fine-tuning strategies for five SOTA models (VGG16, VGG19, ResNet50, ResNet101, and InceptionV3) pre-trained on ImageNet. It also investigates the impact of the classifier by using a linear SVM for classification. The experiments are performed on four open-access ultrasound datasets related to breast cancer, thyroid nodules cancer, and salivary glands cancer. Results are evaluated using a five-fold stratified cross-validation technique, and metrics like accuracy, precision, and recall are computed. The findings show that fine-tuning 15% of the last layers in ResNet50 and InceptionV3 achieves good results. Using SVM for classification further improves overall performance by 6% for the two best-performing models. This research provides insights into fine-tuning strategies and the importance of the classifier in transfer learning for ultrasound image classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35784/iapgos.4464"
    },
    {
        "id": 23767,
        "title": "Sentiment analysis of COP9-related tweets: a comparative study of pre-trained models and traditional techniques",
        "authors": "Sherif Elmitwalli, John Mehegan",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "IntroductionSentiment analysis has become a crucial area of research in natural language processing in recent years. The study aims to compare the performance of various sentiment analysis techniques, including lexicon-based, machine learning, Bi-LSTM, BERT, and GPT-3 approaches, using two commonly used datasets, IMDB reviews and Sentiment140. The objective is to identify the best-performing technique for an exemplar dataset, tweets associated with the WHO Framework Convention on Tobacco Control Ninth Conference of the Parties in 2021 (COP9).MethodsA two-stage evaluation was conducted. In the first stage, various techniques were compared on standard sentiment analysis datasets using standard evaluation metrics such as accuracy, F1-score, and precision. In the second stage, the best-performing techniques from the first stage were applied to partially annotated COP9 conference-related tweets.ResultsIn the first stage, BERT achieved the highest F1-scores (0.9380 for IMDB and 0.8114 for Sentiment 140), followed by GPT-3 (0.9119 and 0.7913) and Bi-LSTM (0.8971 and 0.7778). In the second stage, GPT-3 performed the best for sentiment analysis on partially annotated COP9 conference-related tweets, with an F1-score of 0.8812.DiscussionThe study demonstrates the effectiveness of pre-trained models like BERT and GPT-3 for sentiment analysis tasks, outperforming traditional techniques on standard datasets. Moreover, the better performance of GPT-3 on the partially annotated COP9 tweets highlights its ability to generalize well to domain-specific data with limited annotations. This provides researchers and practitioners with a viable option of using pre-trained models for sentiment analysis in scenarios with limited or no annotated data across different domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fdata.2024.1357926"
    },
    {
        "id": 23768,
        "title": "Arthur Caplan at SemEval-2023 Task 4: Enhancing Human Value Detection through Fine-tuned Pre-trained Models",
        "authors": "Xianxian Song, Jinhui Zhao, Ruiqi Cao, Linchi Sui, Binyang Li, Tingyue Guan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.268"
    },
    {
        "id": 23769,
        "title": "APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models",
        "authors": "Quanjun Zhang, Chunrong Fang, Weisong Sun, Yan Liu, Tieke He, Xiaodong Hao, Zhenyu Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tse.2024.3354969"
    },
    {
        "id": 23770,
        "title": "Exploring Parameter-Efficient Fine-Tuning of a Large-Scale Pre-Trained Model for scRNA-seq Cell Type Annotation",
        "authors": "Yuhang Liu, Tianhao Li, Zixuan Wang, Guiquan Zhu, Yongqing Zhang, Quan Zou",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385599"
    },
    {
        "id": 23771,
        "title": "Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning",
        "authors": "Ji-Rong Wen, Zi Huang, Hanwang Zhang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11633-023-1431-y"
    },
    {
        "id": 23772,
        "title": "Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning",
        "authors": "Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, Songfang Huang, Jun Huang",
        "published": "2023-2-27",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3539597.3570398"
    },
    {
        "id": 23773,
        "title": "APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models",
        "authors": "Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang, Xiaojun Quan, Zenglin Xu, Dongfang Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.567"
    },
    {
        "id": 23774,
        "title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models",
        "authors": "Alireza Ghaffari, Justin Yu, Mahsa Nejad, Masoud Asgharian, Boxing Chen, Vahid Nia",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012567700003654"
    },
    {
        "id": 23775,
        "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?",
        "authors": "Ivan Kobyzev, Aref Jafari, Mehdi Rezagholizadeh, Tianda Li, Alan Do-Omri, Peng Lu, Pascal Poupart, Ali Ghodsi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.13"
    },
    {
        "id": 23776,
        "title": "Sartipi-Sedighin at SemEval-2023 Task 2: Fine-grained Named Entity Recognition with Pre-trained Contextual Language Models and Data Augmentation from Wikipedia",
        "authors": "Amir Sartipi, Amirreza Sedighin, Afsaneh Fatemi, Hamidreza Baradaran Kashani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.78"
    },
    {
        "id": 23777,
        "title": "Multilingual Fairness Analysis and Research based on Pre-Trained Models",
        "authors": "",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.53469/jissr.2023.10(12).07"
    },
    {
        "id": 23778,
        "title": "A Natural Language Processing Model Based on Pre-trained Deep Learning Models",
        "authors": "Yitong Niu",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.57237/j.cst.2022.01.007"
    },
    {
        "id": 23779,
        "title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained",
        "authors": "Hongliang Dai, Ziqian Zeng",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.126"
    },
    {
        "id": 23780,
        "title": "On the N-gram Approximation of Pre-trained Language Models",
        "authors": "Aravind Krishnan, Jesujoba O. Alabi, Dietrich Klakow",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2182"
    },
    {
        "id": 23781,
        "title": "Author Correction: Prediction of oxygen requirement in patients with COVID-19 using a pre-trained chest radiograph xAI model: efficient development of auditable risk prediction models via a fine-tuning approach",
        "authors": "Joowon Chung, Doyun Kim, Jongmun Choi, Sehyo Yune, Kyoung Doo Song, Seonkyoung Kim, Michelle Chua, Marc D. Succi, John Conklin, Maria G. Figueiro Longo, Jeanne B. Ackman, Milena Petranovic, Michael H. Lev, Synho Do",
        "published": "2023-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-31333-0"
    },
    {
        "id": 23782,
        "title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": "Philippe J. Giabbanelli",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408017"
    },
    {
        "id": 23783,
        "title": "Pre-Training and Fine-Tuning Attention Based Encoder Decoder Improves Sea Surface Height Multi-Variate Inpainting",
        "authors": "Théo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Béréziat",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357400003660"
    },
    {
        "id": 23784,
        "title": "Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models",
        "authors": "Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, Shu-Tao Xia",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01302"
    },
    {
        "id": 23785,
        "title": "Bidirectional English-Marathi Translation using Pretrained Models: A Comparative Study of Different Pre-Trained Models",
        "authors": "Lekhraj Saini, Deepti Vidhyarthi",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425770"
    },
    {
        "id": 23786,
        "title": "Fine-Tuning and Aligning Question Answering Models for Complex Information Extraction Tasks",
        "authors": "Matthias Engelbach, Dennis Klau, Felix Scheerer, Jens Drawehn, Maximilien Kintz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012159000003598"
    },
    {
        "id": 23787,
        "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
        "authors": "Richard Rose, Oscar Chang, Olivier Siohan",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1952"
    },
    {
        "id": 23788,
        "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
        "authors": "Hejung Yang, Hong-Goo Kang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-623"
    },
    {
        "id": 23789,
        "title": "Interpreting Art by Leveraging Pre-Trained Models",
        "authors": "Niklas Penzel, Joachim Denzler",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10216010"
    },
    {
        "id": 23790,
        "title": "Fine-tuning of pre-processing filters enables scalp-EEG based training of subcutaneous EEG models",
        "authors": "Lukas Lechner, Asbjoern Wulff Helge, Esben Ahrens, Martin Bachler, Bernhard Hametner, Gerhard Gritsch, Tilmann Kluge, Manfred Hartmann",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bsn58485.2023.10331106"
    },
    {
        "id": 23791,
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": "Liwen Hou, David Smith",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.230"
    },
    {
        "id": 23792,
        "title": "PFC-UNIT: Unsupervised Image-to-Image Translation with Pre-Trained Fine-Grained Classification",
        "authors": "Yu-Ying Liang, Yuan-Gen Wang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222166"
    },
    {
        "id": 23793,
        "title": "Palmprint Recognition Using Pre-Trained Convolutional Neural Networks",
        "authors": "Nader Ebrahimpour, Faruk Baturalp Günay",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.36287/setsci.6.1.018"
    },
    {
        "id": 23794,
        "title": "Exploring Robust Overfitting for Pre-trained Language Models",
        "authors": "Bin Zhu, Yanghui Rao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.340"
    },
    {
        "id": 23795,
        "title": "Modularized Zero-shot VQA with Pre-trained Models",
        "authors": "Rui Cao, Jing Jiang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.5"
    },
    {
        "id": 23796,
        "title": "From RNN techniques to pre-trained models: emphasis on the use in Arabic machine translation",
        "authors": "Nouhaila Bensalah, Habib Ayad, Abdellah Adib, Abdelhamid Ibn El Farouk",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "<p>Neural Machine Translation (NMT) has gained increasing attention in recent years due to its promising performance over conventional approaches such as Statistical Machine Translation. Nevertheless, when applied to languages with different structures, such as the pair (English, Arabic) that interests us in this work, its efficiency is degraded.<br />Alternatively, the use of unsupervised pre-training of large neural models has recently made a significant leap forward in the field of Natural Language Processing (NLP). By warm starting from published checkpoints, NLP practitioners have been able to push the state-of-the-art boundaries on multiple benchmarks while simultaneously saving considerable computational time. The emphasis so far has been mainly on natural language understanding problems. In this paper, we show the efficacy of pre-trained checkpoints for Arabic Machine Translation. We have evolved a transformer-based sequence-to-sequence model which is compatible with pre-trained checkpoints publicly available for Arabic Bidirectional Encoder Representations from Transformers (AraBERT) and Arabic Generative Pre-trained Transformer (AraGPT) and conducted a thorough empirical study on the usefulness of initializing our Arabic MT model, both encoder and decoder, with these checkpoints. Our models yield new state-of-the-art results in Arabic MT.</p>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp2403-2412"
    },
    {
        "id": 23797,
        "title": "Enhancing pre-trained contextual embeddings with triplet loss as an effective fine-tuning method for extracting clinical features from electronic health record derived mental health clinical notes",
        "authors": "Deepali Kulkarni, Abhijit Ghosh, Amey Girdhari, Shaomin Liu, L. Alexander Vance, Melissa Unruh, Joydeep Sarkar",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100045"
    },
    {
        "id": 23798,
        "title": "Fine-tuning a pre-trained Transformers-based model for gene name entity recognition in biomedical text using a customized dataset: case of Desulfovibrio vulgaris Hildenborough",
        "authors": "Alain Bertrand Bomgni, Dialo Abdala, Bichar Dip Shrestha Gurung, Marcellin Julius Nkenlifack, Venkataramana Gadhamshetty, Z. Etienne Gnimpieba",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385403"
    },
    {
        "id": 23799,
        "title": "Transfer Learning and Tuning of Deep Pre-trained Architecture for Face Recognition",
        "authors": " Shem L. Gonzales",
        "published": "2023-7-30",
        "citations": 0,
        "abstract": "Automatic Image Identification is one of the interests of software developers with the application of machine and deep learning methods. With the incorporation of Transfer Learning and Tuning in pre-trained architecture, a substantial increase in the model’s performance is evident. This paper performs face recognition using an image identification and recognition approach. Feature extraction was performed using ResNet50 pre-trained architecture with Support Vector Machine as a classifier. Initial evaluation was made to generate a precision of 62.50%, recall of 65.55%, and f1-score of 63.99%. With this poor performance of ResNet50, the hyperparameters were tuned using transfer learning and tuning. After several times of manual experiments, a significant increase in precision is 93.75%, recall is 94.36%, and f1-score is 94.05%. Based on the remarkable yield of 35.25% for accuracy, 38.79% for recall, and an f1-score of 30.06%, it is advisable to apply the model for image identification and recognition",
        "keywords": "",
        "link": "http://dx.doi.org/10.48175/ijarsct-12196"
    },
    {
        "id": 23800,
        "title": "Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method for Pre-trained Language Models",
        "authors": "Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang Lou, Shi Han, Dongmei Zhang, Zhixu Li, Yanghua Xiao",
        "published": "2023-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583780.3614904"
    },
    {
        "id": 23801,
        "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
        "authors": "Joshua Jansen van Vüren, Thomas Niesler",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1841"
    },
    {
        "id": 23802,
        "title": "An Analysis of Pre-trained Models Versus Custom Deep Learning Models for Forest Fire Detection",
        "authors": "Shouthiri Partheepan, Farzad Sanati, Jahan Hassan",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itnac59571.2023.10368557"
    },
    {
        "id": 23803,
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": "Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-910"
    },
    {
        "id": 23804,
        "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
        "authors": "S. B. Atallah, N. R. Banda, A. Banda, N. A. Roeck",
        "published": "2023-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10151-023-02837-8"
    },
    {
        "id": 23805,
        "title": "CrudeBERT: Applying Economic Theory Towards Fine-Tuning Transformer-Based Sentiment Analysis Models to the Crude Oil Market",
        "authors": "Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011749600003467"
    },
    {
        "id": 23806,
        "title": "Efficiently Robustify Pre-Trained Models",
        "authors": "Nishant Jain, Harkirat Behl, Yogesh Singh Rawat, Vibhav Vineet",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00507"
    },
    {
        "id": 23807,
        "title": "Malware image classification: comparative analysis of a fine-tuned CNN and pre-trained models",
        "authors": "Santosh Kumar Majhi, Abhipsa Panda, Suresh Kumar Srichandan, Usha Desai, Biswaranjan Acharya",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/1206212x.2023.2270804"
    },
    {
        "id": 23808,
        "title": "Comparison of Pre-trained vs Custom-trained Word Embedding Models for Word Sense Disambiguation",
        "authors": "Muhammad Farhat Ullah, Ali Saeed, Naveed Hussain",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "\n\nThe prime objective of word sense disambiguation (WSD) is to develop such machines that can automatically recognize the actual meaning (sense) of ambiguous words in a sentence. WSD can improve various NLP and HCI challenges. Researchers explored a wide variety of methods to resolve this issue of sense ambiguity. However, majorly, their focus was on English and some other well-reputed languages. Urdu with more than 300 million users and a large amount of electronic text available on the web is still unexplored. In recent years, for a variety of Natural Language Processing tasks, word embedding methods have proven extremely successful. This study evaluates, compares, and applies a variety of word embedding approaches to Urdu Word embedding (both Lexical Sample and All-Words), including pre-trained (Word2Vec, Glove, and FastText) as well as custom-trained (Word2Vec, Glove, and FastText trained on the Ur-Mono corpus). Two benchmark corpora are used for the evaluation in this study: (1) the UAW-WSD-18 corpus and (2) the ULS-WSD-18 corpus. For Urdu All-Words WSD tasks, top results have been achieved (Accuracy=60.07 and F1=0.45) using pre-trained FastText. For the Lexical Sample, WSD has been achieved (Accuracy=70.93 and F1=0.60) using custom-trained GloVe word embedding method.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.14201/adcaij.31084"
    },
    {
        "id": 23809,
        "title": "Fine Tuning Named Entity Extraction Models for the Fantasy Domain",
        "authors": "Aravinth Sivaganeshan, Nisansa De Silva",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mercon60487.2023.10355501"
    },
    {
        "id": 23810,
        "title": "Classification of Hazelnut Species with Pre-Trained Deep Learning Models",
        "authors": "Selçuk Harmancı, Yavuz Ünal, Barış Ateş",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "A form of shelled nut in the Betulaceae family is the hazelnut. The majority of it is grown in Türkiye internationally. It grows in the provinces of Türkiye's Black Sea region, which is a significant global production hub. Hazelnuts can be eaten in a variety of ways and are a great source of protein, fat, fiber, vitamins, and minerals. There are numerous applications for hazelnuts in the food business. This study uses pre-trained networks to categorize eight of the most popular hazelnut kinds farmed in Türkiye. In this study, locally named hazelnut varieties grown in Türkiye were examined. An automated computer vision system was used to capture the images of the different hazelnut kinds. Our dataset includes a total of 2722 images, consisting of 155 palaz, 340 yagli, 399 deve disi, 236 tombul, 399 damat, 354 cakildak, 437 kara findik, and 402 sivri hazelnuts. Using transfer learning, the DenseNet121 and InceptionV3 models of convolutional neural networks were employed to categorize these images. The dataset was split into training and testing portions, respectively. With InceptionV3 and DenseNet121, respectively, the research revealed classification accuracy of 96.99% and 96.18%.\n\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.58190/imiens.2023.14"
    },
    {
        "id": 23811,
        "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
        "authors": "Shotaro Ishihara",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.23"
    },
    {
        "id": 23812,
        "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
        "authors": "Siyu Ren, Kenny Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.573"
    },
    {
        "id": 23813,
        "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
        "authors": "Wangchunshu Zhou, Ronan Le Bras, Yejin Choi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.368"
    },
    {
        "id": 23814,
        "title": "Improving Braille–Chinese translation with jointly trained and pre-trained language models",
        "authors": "Tianyuan Huang, Wei Su, Lei Liu, Chuan Cai, Hailong Yu, Yongna Yuan",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.displa.2024.102660"
    },
    {
        "id": 23815,
        "title": "Implementation of Soft Computing Techniques in Fine-Tuning of TRM-Concrete Bond Strength Models",
        "authors": "D Prasannan, R S Thenmozhi, Makhmudjon Mamadjanovich Ergashev, Awari Mahesh Babu, Oluwadare Joshua Oyebode, Ruchira Rawat",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icacite57410.2023.10182838"
    },
    {
        "id": 23816,
        "title": "Tuning Pre-trained Model via Moment Probing",
        "authors": "Mingze Gao, Qilong Wang, Zhenyi Lin, Pengfei Zhu, Qinghua Hu, Jingbo Zhou",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01084"
    },
    {
        "id": 23817,
        "title": "Efficient Data Learning for Open Information Extraction with Pre-trained Language Models",
        "authors": "Zhiyuan Fan, Shizhu He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.869"
    },
    {
        "id": 23818,
        "title": "Leaf Image Classification Based on Pre-trained Convolutional Neural Network Models",
        "authors": "Yunus CAMGÖZLÜ, Yakup KUTLU",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "It is important to identify a high-performance model that can classify all leaves and even differentiate according to regional variations of the same leaf type. In this study, a leaf classification model was created using 5 different datasets with different number of images and compared with models. For this purpose, 4 different pre-trained models called VGG16, InceptionV3, MobileNet and DenseNet are used. In addition, a new model was proposed and model training was carried out using these datasets . Using the all models, inputs are transformed into feature vectors by parameter transfer method and used for classification with the nearest neighbor algorithm and support vector machine. The performance of the classifications were compared with similar studies in the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28978/nesciences.1405175"
    },
    {
        "id": 23819,
        "title": "Performance Evaluation of Pre-Trained Deep Learning Models for Bird Species Identification",
        "authors": "Kerolos Hany, Youssef Ayman, Ayman Atia",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10255070"
    },
    {
        "id": 23820,
        "title": "Pre-trained Deep Learning Models for UAV-based Weed Recognition",
        "authors": "Faiza Mekhalfa, Fouad Yacef, Mahmoud Belhocine",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/spa59660.2023.10274449"
    },
    {
        "id": 23821,
        "title": "Revolutionizing Translation with AI: Unravelling Neural Machine Translation and Generative Pre-Trained Large Language Models",
        "authors": "Sai Cheong Siu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499768"
    },
    {
        "id": 23822,
        "title": "Ensemble fusion model for improved lung abnormality classification: Leveraging pre-trained models",
        "authors": "Suresh Kumar Samarla, Maragathavalli P",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mex.2024.102640"
    },
    {
        "id": 23823,
        "title": "Parameter-efficient Tuning of a Pre-trained Model via Prompt Learning in Cross-modal Retrieval",
        "authors": "Huaying Zhang, Rintaro Yanagi, Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce-taiwan58799.2023.10226785"
    },
    {
        "id": 23824,
        "title": "Analyzing Out-of-Domain Generalization Performance of Pre-Trained Segmentation Models",
        "authors": "Johnson Zhong",
        "published": "2023-2-16",
        "citations": 0,
        "abstract": "Artists illustrate objects to various degrees of complexity. As the amount of detail or the similarity to reality of a depiction decreases, the object tends to be reduced to its simplest, most relevant higher-level features (Harrison, 1981). One of the reasons Deep Neural Networks (DNN) may fail to identify objects in an image is that models are unable to recognize the order of importance of features such as shape, depth, or color within an image, which means even the most minute distortions of pixels within an image that would be imperceptible to humans would greatly impact the performance of the object detection models (Eykholt et al., 2018). However, by training DNN on artworks where the most prominent features defining specific objects are emphasized, perhaps a model can be made to be more resilient against small-scale changes in an image. In this paper, the correlation between the level of similarity to reality of images and artworks of an object and the accuracy of object detection models is investigated to test the ability of object detection models in identifying the most salient features of a particular object. The results of this report can help outline the efficacy of models only trained on real images in identifying increasingly abstract artworks that have simplified an object to its most prominent features. The experiment shows that the accuracies of models decrease as the images or illustrations provided become more abstract or simplified, which suggests the higher level features that identify a particular object are different in object detection models and humans. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.5539/nct.v8n1p1"
    },
    {
        "id": 23825,
        "title": "Classification of Conversational Sentences Using an Ensemble Pre-Trained Language Model with the Fine-Tuned Parameter",
        "authors": "R. Sujatha, K. Nimala",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.046963"
    },
    {
        "id": 23826,
        "title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
        "authors": "Nikitha Karkera, Sathwik Acharya, Sucheendra K. Palaniappan",
        "published": "2023-7-19",
        "citations": 3,
        "abstract": "Abstract\nBackground\nThe growing recognition of the microbiome’s impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature.\n\nResults\nIn this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over $$>0.8$$\n\n>\n0.8\n\n compared to the previous best of  0.74.\n\nConclusion\nOverall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12859-023-05411-z"
    },
    {
        "id": 23827,
        "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
        "authors": "Sweta Agrawal, Marine Carpuat",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.790"
    },
    {
        "id": 23828,
        "title": "Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection",
        "authors": "Yunze Xiao, Firoj Alam",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.58"
    },
    {
        "id": 23829,
        "title": "Generative Pre-trained Transformer (GPT) Models for Irony Detection and Classification",
        "authors": "Mustafa Ulvi Aytekin, O. Ayhan Erdem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391005"
    },
    {
        "id": 23830,
        "title": "Gender Bias in Pre-Trained Deep Learning Models",
        "authors": "Ahsan Ul Islam, ABM Rezbaul Islam",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csde59766.2023.10487724"
    },
    {
        "id": 23831,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 23832,
        "title": "Pre-Trained Language Models and Their Applications",
        "authors": "Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, Yu Sun",
        "published": "2023-6",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eng.2022.04.024"
    },
    {
        "id": 23833,
        "title": "Token-level Identification of Multiword Expressions using Pre-trained Multilingual Language Models",
        "authors": "Raghuraman Swaminathan, Paul Cook",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.mwe-1.1"
    },
    {
        "id": 23834,
        "title": "Representation Transfer Learning via Multiple Pre-trained models for Linear Regression",
        "authors": "Navjot Singh, Suhas Diggavi",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isit54713.2023.10207013"
    },
    {
        "id": 23835,
        "title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
        "authors": "Aleksa Cvetanović, Predrag Tadić",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/telfor59449.2023.10372792"
    },
    {
        "id": 23836,
        "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
        "authors": "Yangyi Chen, Xingyao Wang, Heng Ji",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.624"
    },
    {
        "id": 23837,
        "title": "Predicting Defective and Good Tyre Quality Status with Pre-Trained CNN Models",
        "authors": "Yavuz Selim Taspinar",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mosicom59118.2023.10458796"
    },
    {
        "id": 23838,
        "title": "Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization",
        "authors": "Bashar Al-Rfooh, Gheith Abandah, Rami Al-Rfou",
        "published": "2023-5-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jeeit58638.2023.10185725"
    },
    {
        "id": 23839,
        "title": "DrugFinder: Druggable Protein Identification Model Based on Pre-Trained Models and Evolutionary Information",
        "authors": "Mu Zhang, Fengqiang Wan, Taigang Liu",
        "published": "2023-5-25",
        "citations": 1,
        "abstract": "The identification of druggable proteins has always been the core of drug development. Traditional structure-based identification methods are time-consuming and costly. As a result, more and more researchers have shifted their attention to sequence-based methods for identifying druggable proteins. We propose a sequence-based druggable protein identification model called DrugFinder. The model extracts the features from the embedding output of the pre-trained protein model Prot_T5_Xl_Uniref50 (T5) and the evolutionary information of the position-specific scoring matrix (PSSM). Afterwards, to remove redundant features and improve model performance, we used the random forest (RF) method to select features, and the selected features were trained and tested on multiple different machine learning classifiers, including support vector machines (SVM), RF, naive Bayes (NB), extreme gradient boosting (XGB), and k-nearest neighbors (KNN). Among these classifiers, the XGB model achieved the best results. DrugFinder reached an accuracy of 94.98%, sensitivity of 96.33% and specificity of 96.83% on the independent test set, which is much better than the results from existing identification methods. Our model also performed well on another additional test set related to tumors, achieving an accuracy of 88.71% and precision of 93.72%. This further demonstrates the strong generalization capability of the model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16060263"
    },
    {
        "id": 23840,
        "title": "Study for Performance of Un-Pretrained and Pre-trained Models based on CNN",
        "authors": "Bingsen Wang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "In recent years, as the accuracy of deep learning algorithms in image classification tasks exceeds that of the human brain, Artificial Intelligence (AI) auxiliary diagnosis systems have attracted more and more attention. In this paper, some commonly used Convolutional Neural Network (CNN) models e.g. MobileNet, VGG and ResNet are trained and compared on the cancer detection dataset, and it is found that the pre-trained models based on the idea of the transfer learning perform better than the newly trained models in terms of training speed and model performance. Thus, it can be seen that the transfer learning method has great potential in the field of cancer diagnosis. This study provides some experimental support and suggestions on how to further improve the property of the transfer learning method in the field of cancer diagnosis. Meantime, the performance of VGG19 can be proved to be better compared to other models (i.e., MobileNet and ResNet).",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6486"
    },
    {
        "id": 23841,
        "title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation",
        "authors": "Chunliu Wang, Huiyuan Lai, Malvina Nissim, Johan Bos",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.345"
    },
    {
        "id": 23842,
        "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
        "authors": "Santosh Kesiraju, Marek Sarvaš, Tomáš Pavlíček, Cécile Macaire, Alejandro Ciuba",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2506"
    },
    {
        "id": 23843,
        "title": "Automatic Text Summarization Based on Pre-trained Models",
        "authors": "Alaa Ahmed Al-Banna, Abeer K. Al-Mashhadany",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiccit57614.2023.10218006"
    },
    {
        "id": 23844,
        "title": "On Calibration of Pre-trained Code Models",
        "authors": "Zhenhao Zhou, Chaofeng Sha, Xin Peng",
        "published": "2024-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597503.3639126"
    },
    {
        "id": 23845,
        "title": "Pre-Trained Lightweight Deep Learning Models for Surgical Instrument Detection: Performance Evaluation for Edge Inference",
        "authors": "Md Sabbir Ahmed, Stefano Giordano",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437676"
    },
    {
        "id": 23846,
        "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
        "authors": "Basel Mousi, Nadir Durrani, Fahim Dalvi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.196"
    },
    {
        "id": 23847,
        "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
        "authors": "Minglun Han, Feilong Chen, Jing Shi, Shuang Xu, Bo Xu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-423"
    },
    {
        "id": 23848,
        "title": "Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models",
        "authors": "Byounggyu Lew, Donghyun Son, Buru Chang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00478"
    },
    {
        "id": 23849,
        "title": "TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models",
        "authors": "Amrit Nagarajan, Anand Raghunathan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.782"
    },
    {
        "id": 23850,
        "title": "Pre-trained language models in Spanish for health insurance coverage",
        "authors": "Claudio Aracena, Nicolás Rodríguez, Victor Rocco, Jocelyn Dunstan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.46"
    },
    {
        "id": 23851,
        "title": "Pre-trained deep learning models for brain MRI image classification",
        "authors": "Srigiri Krishnapriya, Yepuganti Karuna",
        "published": "2023-4-20",
        "citations": 17,
        "abstract": "Brain tumors are serious conditions caused by uncontrolled and abnormal cell division. Tumors can have devastating implications if not accurately and promptly detected. Magnetic resonance imaging (MRI) is one of the methods frequently used to detect brain tumors owing to its excellent resolution. In the past few decades, substantial research has been conducted in the field of classifying brain images, ranging from traditional methods to deep-learning techniques such as convolutional neural networks (CNN). To accomplish classification, machine-learning methods require manually created features. In contrast, CNN achieves classification by extracting visual features from unprocessed images. The size of the training dataset had a significant impact on the features that CNN extracts. The CNN tends to overfit when its size is small. Deep CNNs (DCNN) with transfer learning have therefore been developed. The aim of this work was to investigate the brain MR image categorization potential of pre-trained DCNN VGG-19, VGG-16, ResNet50, and Inception V3 models using data augmentation and transfer learning techniques. Validation of the test set utilizing accuracy, recall, Precision, and F1 score showed that the pre-trained VGG-19 model with transfer learning exhibited the best performance. In addition, these methods offer an end-to-end classification of raw images without the need for manual attribute extraction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnhum.2023.1150120"
    },
    {
        "id": 23852,
        "title": "Heterogeneous data-based information retrieval using a fine-tuned pre-trained BERT language model",
        "authors": "Amjan Shaik, Surabhi Saxena, Manisha Gupta, Nikhat Parveen",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17868-4"
    },
    {
        "id": 23853,
        "title": "A Novel Virtual Cosmetics Recommender System Based On Pre-Trained Computer Vision Models",
        "authors": "Samia A. Abu-Shanab, Shadi AlZu'bi, Amjad Zraiqat",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icit58056.2023.10225835"
    },
    {
        "id": 23854,
        "title": "Arabic Extractive Summarization Using Pre-Trained Models",
        "authors": "Yasmin Einieh, Amal AlMansour, Amani Jamal",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "Automatic Text Summarization (ATS) is a crucial area of study in Natural Language Processing (NLP) due to the vast amount of online information available. Extractive summarization, which involves selecting important sentences from the original document without altering their wording, is one approach to generating summaries. While many methods for Arabic text summarization exist, deep learning applications are still in their early stages, and there is a shortage of available datasets. Unlike English, there have been fewer experiments conducted on Arabic language summarization due to its unique characteristics. This study aims to fill this gap by experimenting with several models for summarizing Arabic text, including QARiB, AraELECTRA, and AraBERT-base models, all trained using the KALIMA dataset. The AraBERT model performed exceptionally well, achieving high scores of 0.44, 0.26, and 0.44 on the ROUGE-1, ROUGE-2, and ROUGE-L measures, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4197/comp.12-1.6"
    },
    {
        "id": 23855,
        "title": "Do Pre-trained Models Benefit Equally in Continual Learning?",
        "authors": "Kuan-Ying Lee, Yuanyi Zhong, Yu-Xiong Wang",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00642"
    },
    {
        "id": 23856,
        "title": "Leveraging pre-trained language models for code generation",
        "authors": "Ahmed Soliman, Samir Shaheen, Mayada Hadhoud",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "AbstractCode assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software development. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer productivity, reducing errors, and facilitating a more efficient coding workflow. This assistance can manifest in various forms, including code autocompletion, error detection and correction, code generation, documentation support, and context-aware suggestions. Language models have emerged as integral components of code assistance, offering developers the capability to receive intelligent suggestions, generate code snippets, and enhance overall coding proficiency. In this paper, we propose new hybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with the Marian Causal Language Model. Selecting these models based on their strong performance in various natural language processing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to existing state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize code generation, offering improved precision and efficiency in navigating complex coding scenarios. Additionally, conducting error analysis and refining the generated code. Our results show that these models, when combined with the Marian Decoder, significantly improve code generation accuracy and efficiency. Notably, the RoBERTaMarian model achieved a maximum BLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of 89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/AhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40747-024-01373-8"
    },
    {
        "id": 23857,
        "title": "Comparison of large-scale pre-trained models based ViT, swin transformer and ConvNeXt",
        "authors": "Jiapeng Yu",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671201"
    },
    {
        "id": 23858,
        "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models",
        "authors": "Mingyang Song, Yi Feng, Liping Jing",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.161"
    },
    {
        "id": 23859,
        "title": "Text clustering based on pre-trained models and autoencoders",
        "authors": "Qiang Xu, Hao Gu, ShengWei Ji",
        "published": "2024-1-5",
        "citations": 1,
        "abstract": "Text clustering is the task of grouping text data based on similarity, and it holds particular importance in the medical field. sIn healthcare, medical data clustering is a highly active and effective research area. It not only provides strong support for making correct medical decisions from medical datasets but also aids in patient record management and medical information retrieval. With the development of the healthcare industry, a large amount of medical data is being generated, and traditional medical data clustering faces significant challenges. Many existing text clustering algorithms are primarily based on the bag-of-words model, which has issues such as high dimensionality, sparsity, and the neglect of word positions and context. Pre-trained models are a deep learning-based approach that treats text as a sequence to accurately capture word positions and context information. Moreover, compared to traditional K-means and fuzzy C-means clustering models, deep learning-based clustering algorithms are better at handling high-dimensional, complex, and nonlinear data. In particular, clustering algorithms based on autoencoders can learn data representations and clustering information, effectively reducing noise interference and errors during the clustering process. This paper combines pre-trained language models with deep embedding clustering models. Experimental results demonstrate that our model performs exceptionally well on four public datasets, outperforming most existing text clustering algorithms, and can be applied to medical data clustering.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fncom.2023.1334436"
    },
    {
        "id": 23860,
        "title": "Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging",
        "authors": "Rachel M. Harrison, Anton Dereventsov, Anton Bibin",
        "published": "2023-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdmw60847.2023.00195"
    },
    {
        "id": 23861,
        "title": "Self-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models",
        "authors": "Christopher Simic, Tobias Bocklet",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448047"
    },
    {
        "id": 23862,
        "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models",
        "authors": "Raymond Li, Gabriel Murray, Giuseppe Carenini",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.634"
    },
    {
        "id": 23863,
        "title": "Enhancement of Pre-Trained Deep Learning Models to Improve Brain Tumor",
        "authors": "Zaka Ullah, Ayman Odeh, Ihtisham Khattak, Muath Al Hasan",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v47i6.4645"
    },
    {
        "id": 23864,
        "title": "Prediction of Diseases in Potato Plant using Pre-trained and Traditional Machine Learning Models",
        "authors": "Swati Laxmi Sahu, Renta Chintala Bhargavi",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170149"
    },
    {
        "id": 23865,
        "title": "A Systematic Survey of Chemical Pre-trained Models",
        "authors": "Jun Xia, Yanqiao Zhu, Yuanqi Du, Stan Z. Li",
        "published": "2023-8",
        "citations": 6,
        "abstract": "Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Chemical Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder architectures, pre-training strategies, and applications. We also highlight the challenges and promising avenues for future research, providing a useful resource for both machine learning and scientific communities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/760"
    },
    {
        "id": 23866,
        "title": "Domain-specific language models pre-trained on construction management systems corpora",
        "authors": "Yunshun Zhong, Sebastian D. Goodfellow",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.autcon.2024.105316"
    },
    {
        "id": 23867,
        "title": "Evaluation of Pre-Trained CNN Models for Cardiovascular Disease Classification: A Benchmark Study",
        "authors": "",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18576/isl/120755"
    },
    {
        "id": 23868,
        "title": "Transfer Learning using Very Deep Pre-Trained Models for Food Image Classification",
        "authors": "Pranjal Kumar Singh, Seba Susan",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307479"
    },
    {
        "id": 23869,
        "title": "Exploring Arabic Pre-Trained Language Models for Arabic Abstractive Text Summarization",
        "authors": "Dhuha Alqahtani, Maha Al-Yahya",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/snams60348.2023.10375464"
    },
    {
        "id": 23870,
        "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation",
        "authors": "Tianyu Yang, Thy Tran, Iryna Gurevych",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.313"
    },
    {
        "id": 23871,
        "title": "Avoiding parameter fine-tuning in mass varying neutrino models of DE?",
        "authors": "Michael Maziashvili, Vakhtang Tsintsabadze",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.astropartphys.2023.102901"
    },
    {
        "id": 23872,
        "title": "PLM-AS: Pre-trained Language Models Augmented with Scanpaths for Sentiment Classification",
        "authors": "Duo Yang, Nora Hollenstein",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "Recent research demonstrated that deep neural networks could generate meaningful feature representations from both eye-tracking data and sentences without designing handcrafted features, which achieved competitive performance across cognitive NLP tasks, such as sentiment classification over gaze datasets, but the previous works mainly encode the text and gaze data separately without considering the interaction between these two modalities or applying large-scaled pre-trained models. To address these challenges, we introduce PLM-AS, a novel framework to take full advantage of textual and eye-tracking features by sequence modeling in a highly interactive way for multimodal fusion. It is also the first attempt to combine large-scaled pre-trained models with eye-tracking features in the cognitive reading task. We show that PLM-AS captures cognitive signals from eye-tracking data and shows improved performance on sentiment classification within and across three datasets of different domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/18.6797"
    },
    {
        "id": 23873,
        "title": "Deep Learning based Bone Fracture Prediction using Convolutional Neural Networks: A Comparative Study of Transfer Learning and Fine-tuning Techniques",
        "authors": "Sriram R, O R Aruna",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icotl59758.2023.10435047"
    },
    {
        "id": 23874,
        "title": "‘Pre-training+Fine-tuning’ Model-based 1/2 Folded Image Restoration",
        "authors": "Huijia Song, Jiacheng Wei, Xiaozhu Lin, Huainian Zhang",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isceic59030.2023.10271228"
    },
    {
        "id": 23875,
        "title": "Enhancing smart contract security: Leveraging pre‐trained language models for advanced vulnerability detection",
        "authors": "Fei He, Fei Li, Peili Liang",
        "published": "2024-3-29",
        "citations": 0,
        "abstract": "AbstractThe burgeoning interest in decentralized applications (Dapps), spurred by advancements in blockchain technology, underscores the critical role of smart contracts. However, many Dapp users, often without deep knowledge of smart contracts, face financial risks due to hidden vulnerabilities. Traditional methods for detecting these vulnerabilities, including manual inspections and automated static analysis, are plagued by issues such as high rates of false positives and overlooked security flaws. To combat this, the article introduces an innovative approach using the bidirectional encoder representations from transformers (BERT)‐ATT‐BiLSTM model for identifying potential weaknesses in smart contracts. This method leverages the BERT pre‐trained model to discern semantic features from contract opcodes, which are then refined using a Bidirectional Long Short‐Term Memory Network (BiLSTM) and augmented by an attention mechanism that prioritizes critical features. The goal is to improve the model's generalization ability and enhance detection accuracy. Experiments on various publicly available smart contract datasets confirm the model's superior performance, outperforming previous methods in key metrics like accuracy, F1‐score, and recall. This research not only offers a powerful tool to bolster smart contract security, mitigating financial risks for average users, but also serves as a valuable reference for advancements in natural language processing and deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/blc2.12072"
    },
    {
        "id": 23876,
        "title": "TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models",
        "authors": "Sucheng Ren, Fangyun Wei, Zheng Zhang, Han Hu",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00359"
    },
    {
        "id": 23877,
        "title": "Pre-Trained Deep Learning Models for Detecting Strikeouts in Kannada Handwritten Documents",
        "authors": " Bhargav, Shivayogi B. Ullagaddi",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "In the digital document analysis, handwritten character recognition is a challenging area.  Various methods are proposed in the literature to identify the strike-outs in various languages. Kannada handwritten classification is a crucial machine vision problem due to its various practical applications in the development of recognition systems. Identifying of strikeout string on the given Kannada statement is an important challenge to develop robust recognition system. In the present study, pre-trained models such as DenseNet121, EficientNetB0, MobileNet, InceptionResNetV2 are used to recognize strikeout Kannada words.  Experimental analysis indicates that EficientNetB0 performed better for strikeout Kannada words and InceptionResNetV2 results in higher performance for non-strikeout Kannada words.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/tjjpt.v44.i3.556"
    },
    {
        "id": 23878,
        "title": "Attenuate Class Imbalance Problem for Pneumonia Diagnosis Using Ensemble Parallel Stacked Pre-Trained Models",
        "authors": "Aswathy Ravikumar, Harini Sriraman",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.035848"
    },
    {
        "id": 23879,
        "title": "Two-stage Thai Misspelling Correction based on Pre-trained Language Models",
        "authors": "Idhibhat Pankam, Peerat Limkonchotiwat, Ekapol Chuangsuwanich",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jcsse58229.2023.10202006"
    },
    {
        "id": 23880,
        "title": "Photo-based Carbohydrates Counting using Pre-trained Transformer Models",
        "authors": "Ivan Contreras, Marti Guso, Aleix Beneyto, Josep Vehi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.445"
    },
    {
        "id": 23881,
        "title": "ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning",
        "authors": "Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, Yang Liu",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icse48619.2023.00207"
    },
    {
        "id": 23882,
        "title": "The Presence of Bias Based on Pre-trained Language Models",
        "authors": "Rui Liu, Yiqi Wu, Yufei Xing",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "One of the most active fields of machine learning research is natural language processing (NLP). Although existing linguistic machine learning models excel numerically on a variety of linguistic comprehension tasks, they frequently lack implicit bias reduction optimization. To ensure that deep learning models can avoid the traps of implicit bias and that robots can make fair judgments, bias in NLP must be addressed adequately. The ramifications of permitting biased models to reach the actual world are serious. Thus must address this issue as quickly as feasible. This paper conducted data validation-bias experiments on several real datasets to verify the presence of gender bias in the pre-trained models, then proposed a word vector balancing algorithm to modify the actual vector representation of words by biasing the models and verifying the effectiveness of the debiasing method through debiasing experiments to mitigate the gender bias of the models while ensuring data accuracy, thereby improving the fairness of the classification results. Furthermore, this paper provides more accurate information for the future development and use of deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/8/20230250"
    },
    {
        "id": 23883,
        "title": "On Checking Robustness on Named Entity Recognition with Pre-trained Transformers Models",
        "authors": "Aitor García-Pablos, Justina Mandravickaitė, Egidija Veršinskienė",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22364/bjmc.2023.11.4.05"
    },
    {
        "id": 23884,
        "title": "Aspects of creating a corporate question-and-answer system using generative pre-trained language models",
        "authors": "Aleksei Golikov, Dmitrii Akimov, Maksim Romanovskii, Sergei Trashchenkov",
        "published": "2023-12",
        "citations": 0,
        "abstract": "\n The article describes various ways to use generative pre-trained language models to build a corporate question-and-answer system. A significant limitation of the current generative pre-trained language models is the limit on the number of input tokens, which does not allow them to work \"out of the box\" with a large number of documents or with a large document. To overcome this limitation, the paper considers the indexing of documents with subsequent search query and response generation based on two of the most popular open source solutions at the moment – the Haystack and LlamaIndex frameworks. It has been shown that using the open source Haystack framework with the best settings allows you to get more accurate answers when building a corporate question-and-answer system compared to the open source LlamaIndex framework, however, requires the use of an average of several more tokens.    The article used a comparative analysis to evaluate the effectiveness of using generative pre-trained language models in corporate question-and-answer systems using the Haystack and Llamaindex frameworks. The evaluation of the obtained results was carried out using the EM (exact match) metric. The main conclusions of the conducted research on the creation of question-answer systems using generative pre-trained language models are: 1. Using hierarchical indexing is currently extremely expensive in terms of the number of tokens used (about 160,000 tokens for hierarchical indexing versus 30,000 tokens on average for sequential indexing), since the response is generated by sequentially processing parent and child nodes. 2. Processing information using the Haystack framework with the best settings allows you to get somewhat more accurate answers than using the LlamaIndex framework (0.7 vs. 0.67 with the best settings). 3. Using the Haystack framework is more invariant with respect to the accuracy of responses in terms of the number of tokens in the chunk. 4. On average, using the Haystack framework is more expensive in terms of the number of tokens (about 4 times) than the LlamaIndex framework. 5. The \"create and refine\" and \"tree summarize\" response generation modes for the LlamaIndex framework are approximately the same in terms of the accuracy of the responses received, however, more tokens are required for the \"tree summarize\" mode.\n\t",
        "keywords": "",
        "link": "http://dx.doi.org/10.25136/2409-8698.2023.12.69353"
    },
    {
        "id": 23885,
        "title": "Afrikaans Literary Genre Recognition using Embeddings and Pre-Trained Multilingual Language Models",
        "authors": "Eduan Kotzé, Burgert A. Senekal",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467838"
    },
    {
        "id": 23886,
        "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
        "authors": "Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Tomohiro Tanaka, Takatomo Kano, Atsunori Ogawa, Marc Delcroix",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1307"
    },
    {
        "id": 23887,
        "title": "Adaptive Textual Label Noise Learning based on Pre-trained Models",
        "authors": "Shaohuan Cheng, Wenyu Chen, Fu Mingsheng, Xuanting Xie, Hong Qu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.209"
    },
    {
        "id": 23888,
        "title": "Classifying Dog Emotions Using Transfer Learning and Pre-trained Models",
        "authors": "Rishabh Chauhan, Harsh Manchanda, Neha Tyagi",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/confluence60223.2024.10463254"
    },
    {
        "id": 23889,
        "title": "Collaborative Learning across Heterogeneous Systems with Pre-Trained Models",
        "authors": "Trong Nghia Hoang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The increasingly decentralized and private nature of data in our digital society has  motivated the development of personalized, collaborative intelligent systems that enable knowledge aggregation across multiple data owners while accommodating for their data privacy and system constraints. However, collaborative learning has only been investigated in simple and limited settings: isolated task scenarios where learning begins from scratch and does not build on prior expertise; learned model is represented in task-specific forms which are not generalizable to unseen, emerging scenarios; and more often, a universal model representation is assumed across collaborators, ignoring their local compute constraints or input representations. This restricts its practicality in continual learning scenarios with limited task data, which demand continuous adaptation and knowledge transfer across different information silos, tasks, and learning models, as well as the utilization of prior solution expertises. To overcome these limitations, my research has been focused on developing effective and scalable resource-aware collaborative learning frameworks across heterogeneous systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30284"
    },
    {
        "id": 23890,
        "title": "A Weighted Ensemble Approach with Multiple Pre-trained Deep Learning Models for Classification of Stroke",
        "authors": "Rusul Ali Jabbar Alhatemi, Serkan Savaş",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "Stroke ranks as one of the deadliest diseases globally, emphasizing the crucial need for early diagnosis. This study aims to create a two-stage classification system for stroke and non-stroke images to support early clinical detection. Deep learning, a cornerstone of diagnosis, detection, and prompt treatment, is the primary methodology. Transfer learning adapts successful deep learning architectures for diverse problems, and ensemble learning combines multiple classifiers for enhanced results. These two techniques are applied to classify stroke using a dataset of stroke and normal images. In the initial stage, six pre-trained models are fine-tuned, with DenseNet, Xception, and EfficientNetB2 emerging as the top performers, achieving validation accuracies of 98.4%, 98.4%, and 98%, respectively. These models serve as base learners within an ensemble framework. A weighted average ensemble method combines them, resulting in a remarkable 99.84% accuracy on a reserved test dataset. This approach exhibits promise for stroke detection, a life-threatening condition, while also demonstrating the effectiveness of ensemble techniques in enhancing model performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47852/bonviewmedin32021963"
    },
    {
        "id": 23891,
        "title": "The Impact of Training Methods on the Development of Pre-trained Language Models",
        "authors": "Diego Uribe, Enrique Cuan, Elisa Urquizo",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.13053/cys-28-1-4718"
    },
    {
        "id": 23892,
        "title": "Fine-tuning Text Classification Models for Named Entity Oriented Sentiment Analysis of Russian Texts",
        "authors": "Anna Glazkova,  ",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "The paper presents an approach to named entity oriented sentiment analysis of Russian news texts proposed during the RuSentNE evaluation. The approach is based on RuRoBERTa-large, a pre-trained RoBERTa model for Russian. We compared several types of entity representation in the input text, and evaluated strategies for handling class imbalance and resampling entity tags in the training set. We demonstrated that some strategies improve the results of pre-trained models obtained on the dataset presented by the organizers of the evaluation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-104-116"
    },
    {
        "id": 23893,
        "title": "Personalized Aging-in-Place Support Through Fine-Tuning of Generative AI Models",
        "authors": "Henry Griffith, Heena Rathore",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mobisecserv58080.2023.10329224"
    },
    {
        "id": 23894,
        "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
        "authors": "Xuanjie Fang, Sijie Cheng, Yang Liu, Wei Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.461"
    },
    {
        "id": 23895,
        "title": "Dataset Construction and Opinion Holder Detection Using Pre-trained Models",
        "authors": "Al- Mahmud, Kazutaka Shimada",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52731/ijskm.v7.i2.779"
    },
    {
        "id": 23896,
        "title": "Classification of Regional Food Using Pre-Trained Transfer Learning Models",
        "authors": "Jeet Gadhiya, Anjali Khatik, Shruti Kodinariya, Dipak Ramoliya",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceca58529.2023.10395249"
    },
    {
        "id": 23897,
        "title": "An efficient ptychography reconstruction strategy through fine-tuning of large pre-trained deep learning model",
        "authors": "Xinyu Pan, Shuo Wang, Zhongzheng Zhou, Liang Zhou, Peng Liu, Chun Li, Wenhui Wang, Chenglong Zhang, Yuhui Dong, Yi Zhang",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.isci.2023.108420"
    },
    {
        "id": 23898,
        "title": "Fine-tuning and multilingual pre-training for abstractive summarization task for the Arabic language",
        "authors": "Mram Kahla, Attila Novák, Zijian Győző Yang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33039/ami.2022.11.002"
    },
    {
        "id": 23899,
        "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
        "authors": "Derek Chen, Kun Qian, Zhou Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.115"
    },
    {
        "id": 23900,
        "title": "Towards Inadequately Pre-trained Models in Transfer Learning",
        "authors": "Andong Deng, Xingjian Li, Di Hu, Tianyang Wang, Haoyi Xiong, Cheng-Zhong Xu",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01777"
    },
    {
        "id": 23901,
        "title": "A Named Entity Recognition Method Based on Pre trained Models MBERT and BiLSTM",
        "authors": "Tao Jia",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3625156.3625161"
    },
    {
        "id": 23902,
        "title": "Sound Event Classification Based on Fusion of Multiple Pre-trained Models",
        "authors": "Xichang Cai, Juan Wu, Menglong Wu, Yanggang Gan",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icceic60201.2023.10426682"
    },
    {
        "id": 23903,
        "title": "Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion",
        "authors": "Ryo Sekizawa, Hitomi Yanaka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.starsem-1.5"
    },
    {
        "id": 23904,
        "title": "Class-Incremental Learning Based on Big Dataset Pre-Trained Models",
        "authors": "Bin Wen, Qiuyu Zhu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3287771"
    }
]