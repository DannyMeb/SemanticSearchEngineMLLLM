[
    {
        "id": 20001,
        "title": "The Triple Attention Transformer: Advancing Contextual Coherence in Transformer Models",
        "authors": "Shadi Ghaith",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper introduces the Triple Attention Transformer (TAT), a transformative approach in transformer models, tailored for enhancing long-term contextual coherence in dialogue systems. TAT innovates by representing dialogues as chunks of sequences, coupled with a triple attention mechanism. This novel architecture enables TAT to effectively manage extended sequences, addressing the coherence challenges inherent in traditional transformer models. Empirical evaluations using the Schema-Guided Dialogue Dataset from DSTC8 demonstrate TAT's enhanced performance, with significant improvements in Character Error Rate, Word Error Rate, and BLEU score. Importantly, TAT excels in generating coherent, extended dialogues, showcasing its advanced contextual comprehension. The integration of Conv1D networks, dual-level positional encoding, and decayed attention weighting are pivotal to TAT's robust context management. The paper also highlights the BERT variant of TAT, which leverages pre-trained language models to further enrich dialogue understanding and generation capabilities. Future developments include refining attention mechanisms, improving role distinction, and architectural optimizations. TAT's applicability extends to various complex NLP tasks, affirming its potential as a pioneering advancement in natural language processing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3916608/v1"
    },
    {
        "id": 20002,
        "title": "Contextual Emotion Recognition Using Transformer-Based Models",
        "authors": "Aayush Devgan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In order to increase the precision of emotion identification in text, this research suggests a context-aware emotion recognition system employing transformer models, especially BERT. The model is able to comprehend complex emotions and context-dependent expressions since it was trained on a broad, emotion-labeled dataset. On a benchmark dataset, its efficacy is assessed in comparison to conventional techniques and standard transformer models. The system is proficient at gathering contextual information, and the findings demonstrate a considerable improvement in emotion recognition accuracy. This study improves textual emotion identification, opening the door to applications like chatbots that can recognize emotions and systems for tracking mental health. It also identifies potential areas for further study in developing transformer models for context-sensitive NLP applications.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23804025"
    },
    {
        "id": 20003,
        "title": "Contextual Emotion Recognition Using Transformer-Based Models",
        "authors": "Aayush Devgan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In order to increase the precision of emotion identification in text, this research suggests a context-aware emotion recognition system employing transformer models, especially BERT. The model is able to comprehend complex emotions and context-dependent expressions since it was trained on a broad, emotion-labeled dataset. On a benchmark dataset, its efficacy is assessed in comparison to conventional techniques and standard transformer models. The system is proficient at gathering contextual information, and the findings demonstrate a considerable improvement in emotion recognition accuracy. This study improves textual emotion identification, opening the door to applications like chatbots that can recognize emotions and systems for tracking mental health. It also identifies potential areas for further study in developing transformer models for context-sensitive NLP applications.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23804025.v2"
    },
    {
        "id": 20004,
        "title": "Contextual Emotion Recognition Using Transformer-Based Models",
        "authors": "Aayush Devgan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In order to increase the precision of emotion identification in text, this research suggests a context-aware emotion recognition system employing transformer models, especially BERT. The model is able to comprehend complex emotions and context-dependent expressions since it was trained on a broad, emotion-labeled dataset. On a benchmark dataset, its efficacy is assessed in comparison to conventional techniques and standard transformer models. The system is proficient at gathering contextual information, and the findings demonstrate a considerable improvement in emotion recognition accuracy. This study improves textual emotion identification, opening the door to applications like chatbots that can recognize emotions and systems for tracking mental health. It also identifies potential areas for further study in developing transformer models for context-sensitive NLP applications.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23804025.v1"
    },
    {
        "id": 20005,
        "title": "Episode 23: NLP/Transformer Models for Radiology",
        "authors": "",
        "published": "2022-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1148/ryai.10042022.podcast"
    },
    {
        "id": 20006,
        "title": "Implementation of Traditional Vs. Transformer Machine\nLearning Models",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29121/web/v18i4/144"
    },
    {
        "id": 20007,
        "title": "LVCSR with Transformer Language Models",
        "authors": "Eugen Beck, Ralf Schlüter, Hermann Ney",
        "published": "2020-10-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1164"
    },
    {
        "id": 20008,
        "title": "Transformer-Based Language Models as Psycholinguistic Subjects: Focusing on Understanding Metaphor",
        "authors": "Wonil Chung,  ",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "Metaphor is a fundamental aspect of human language and cognition, playing a crucial role in communication, comprehension, and creative expression. In light of the recent advancements demonstrated by prominent language models, a pivotal question arises: Can these expansive language models effectively discern metaphorical knowledge? The primary objective involves comparing the surprisal values estimated from neural network language models like autoregressive and bidirectional language models to the reaction times of human when exposed to both metaphorical and literal sentences. Our secondary objective involves assessing the AI's comprehension of metaphors by utilizing the sensicality ratings generated by sophisticated ChatGPT. To achieve this, we used psycholinguistic methods, and adopted the experimental materials from Lai, Currana, and Menna (2009). We found the surprisal values estimated from the autoregressive language model demonstrate metaphor processing that closely resembles that of native speakers. Furthermore, ChatGPT's processing of conventional metaphorical sentences closely resembles its approach to literal sentences, mirroring the convergence observed in native speakers' ERP response to conventional metaphorical sentences and their alignment with that of literal sentences.",
        "link": "http://dx.doi.org/10.14342/smog.2023.119.87"
    },
    {
        "id": 20009,
        "title": "Learning Embeddings from Free-text Triage Notes using Pretrained Transformer Models",
        "authors": "Émilien Arnaud, Mahmoud Elbattah, Maxime Gignon, Gilles Dequen",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011012800003123"
    },
    {
        "id": 20010,
        "title": "Explainable Transformer Models for Functional Genomics in Prokaryotes",
        "authors": "Jim Clauwaert, Gerben Menschaert, Willem Waegeman",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe effectiveness of deep learning methods can be largely attributed to the automated extraction of relevant features from raw data. In the field of functional genomics, this generally comprises the automatic selection of relevant nucleotide motifs from DNA sequences. To benefit from automated learning methods, new strategies are required that unveil the decision-making process of trained models. In this paper, we present several methods that can be used to gather insights on biological processes that drive any genome annotation task. This work builds upon a transformer-based neural network framework designed for prokaryotic genome annotation purposes. We find that the majority of sub-units (attention heads) of the model are specialized towards identifying DNA binding sites. Working with a neural network trained to detect transcription start sites in E. coli, we successfully characterize both locations and consensus sequences of transcription factor binding sites, including both well-known and potentially novel elements involved in the initiation of the transcription process.",
        "link": "http://dx.doi.org/10.1101/2020.03.16.993501"
    },
    {
        "id": 20011,
        "title": "Comparative Evaluation of Transformer-Based Nepali Language Models",
        "authors": "Suyogya Ratna Tamrakar, Chaklam Silpasuwanchai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLarge pre-trained transformer models using self-supervised learning have achieved state-of-the-art performances in various NLP tasks. However, for low-resource language like Nepali, pre-training of monolingual models remains a problem due to lack of training data and well-designed and balanced benchmark datasets. Furthermore, several multilingual pre-trained models such as mBERT and XLM-RoBERTa have been released, but their performance remains unknown for Nepali language. We compared Nepali monolingual pre-trained transformer models with multilingual models to determine their performance using a Nepali text classification dataset as a downstream task based on different number of classes and data sizes, taking machine learning (ML) and deep learning (DL) algorithms as baselines. Under-representation of Nepali language in mBERT resulted in overall poor performance, but, XLM-RoBERTa, which has a larger vocabulary size, produced state-of-the-art performance which is relatively similar to that of Nepali DistilBERT and DeBERTa, which outperformed all of the baseline algorithms. Bi-LSTM and SVM from the baselines also performed very well in variety of settings. Moreover, to assess the cross-language knowledge transfer for the cases when mono-lingual models are not available, we also evaluated HindiRoBERTa, a monolingual Indian language model on Nepali text dataset. This research mainly contributes to the Nepali NLP community by creation of news classification dataset with 20 classes, with over 200,000 articles and performance evaluation of various pre-trained monolingual Nepali transformers with multilingual transformers, DL and ML algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2289743/v1"
    },
    {
        "id": 20012,
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": "Jia Lim, Hady Lauw",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.534"
    },
    {
        "id": 20013,
        "title": "Pre-Trained Transformer-Based Language Models for Sundanese",
        "authors": "Wilson Wongso, Henry Lucky, Derwin Suhartono",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThe Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-907893/v1"
    },
    {
        "id": 20014,
        "title": "Classifying Drug Ratings Using User Reviews with Transformer-Based Language Models",
        "authors": "Akhil Shiju, Zhe He",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractDrugs.com provides users’ textual reviews and numeric ratings of drugs. However, text reviews may not always be consistent with the numeric ratings. Overly positive or negative rating may be misleading. In this project, to classify user ratings of drugs with their textual reviews, we built classification models using traditional machine learning and deep learning approaches. Machine learning models including Random Forest and Naive Bayesian classifiers were built using TF-IDF features as input. Also, transformer-based neural network models including BERT, BioBERT, RoBERTa, XLNet, ELECTRA, and ALBERT were built using the raw text as input. Overall, BioBERT model outperformed the other models with an overall accuracy of 87%. We further identified UMLS concepts from the postings and analyzed their semantic types in the postings stratified by the classification result. This research demonstrated that transformer-based models can be used to classify drug reviews and identify reviews that are inconsistent with the ratings.",
        "link": "http://dx.doi.org/10.1101/2021.04.15.21255573"
    },
    {
        "id": 20015,
        "title": "Predicting Rapid Impact Compaction Outcomes with Transformer-Based Deep Learning Models",
        "authors": "Sompote Youwai, Sirasak Detcheewa",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper introduces a novel generative deep learning approach to predict the engineering properties of the ground improved by Rapid Impact Compaction (RIC), which is a ground improvement technique that uses a drop hammer to compact the soil and fill layers. The proposed approach uses transformer-based neural networks to capture the complex nonlinear relationships between the input features, such as the hammer energy, drop height, and number of blows, and the output variables, such as the cone resistance. The approach is applied to a real-world dataset from a trial test section for the new apron construction of the Utapao International Airport in Thailand. The results show that the proposed approach outperforms the existing methods in terms of prediction accuracy and efficiency and provides interpretable attention maps that reveal the importance of different features for RIC prediction. The paper also discusses the limitations and future directions of applying deep learning methods to RIC prediction.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3349505/v1"
    },
    {
        "id": 20016,
        "title": "Finnish ASR with Deep Transformer Models",
        "authors": "Abhilash Jain, Aku Rouhe, Stig-Arne Grönroos, Mikko Kurimo",
        "published": "2020-10-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1784"
    },
    {
        "id": 20017,
        "title": "Multi-label emotion classification of Tweets with transformer models",
        "authors": "T. L. Hettikankanamge, A. J. Pinidiyaarachchi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAnalysis and classification of emotions expressed in social media content such as tweets have been useful for numerous commercial and social purposes for tasks like hate speech detection. Emotion classification of social media content has been performed using traditional techniques such as Recurrent Neural Networks (RNN) and Multivariate Long Short Term Memory (LSTM) in the past, which can be outperformed by the new transformer models. The ‘SemEval-2018 Task 1: Affect in Tweets’ (Mohommad et al. 2018) presents a challenge on multi-label classification of emotions expressed in tweets into 11 sentiment classes. The datasets given for this challenge are used in this work to explore the accuracy of the transformer models against other techniques used by the competitors of the particular challenge. Additionally the transformer models (BERT, RoBERTa and XLM RoBERTa) were compared with each other on their performance based on accuracy and speed. The best performing BERT-large model which is trained using bert-large-uncased tokenizer has shown a multi-label accuracy (Jaccard Index) which is higher than the sixth recorded score in the SemEval-2018 Task 1 competition, F1-micro which is higher than the best F1 score recorded.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2583392/v1"
    },
    {
        "id": 20018,
        "title": "20.5 C-Transformer: A 2.6-18.1μJ/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models",
        "authors": "Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, Hoi-Jun Yoo",
        "published": "2024-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isscc49657.2024.10454330"
    },
    {
        "id": 20019,
        "title": "When Transformer models are more compositional than humans: The case of the depth charge illusion",
        "authors": "Dario Paape",
        "published": "No Date",
        "citations": 0,
        "abstract": "State-of-the-art Transformer-based language models like GPT-3 are very good at generating syntactically well-formed and semantically plausible text. However, it is unclear to what extent these models encode the compositional rules of human language and to what extent their impressive performance is due to the use of relatively shallow heuristics, which have also been argued to be a factor in human language processing. One example is the so-called depth charge illusion, which occurs when a semantically complex, incongruous sentence like \"No head injury is too trivial to be ignored\" is assigned a plausible but not compositionally licensed meaning (\"Don't ignore head injuries, even if they appear to be trivial\"). I present an experiment that investigated how depth charge sentences are processed by Transformer models, which are free of many human performance bottlenecks. The results are mixed: Transformers do show evidence of non-compositionality in depth charge contexts, but also appear to be more compositional than humans in some respects.",
        "link": "http://dx.doi.org/10.31234/osf.io/gx4q5"
    },
    {
        "id": 20020,
        "title": "Comparing Two Topology Transformer Hysteresis Models with Power Transformer Measurements",
        "authors": "D. Albert, L. Domenig, D. Maletic, A. Reinbacher-Kostinger, K. Roppert, H. Renner",
        "published": "2022-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compumag55718.2022.9827499"
    },
    {
        "id": 20021,
        "title": "Comparative Analysis of Transformer based Language Models",
        "authors": "Aman Pathak",
        "published": "2021-1-23",
        "citations": 3,
        "abstract": "Natural language processing (NLP) has witnessed many substantial advancements in the past three years. With the introduction of the Transformer and self-attention mechanism, language models are now able to learn better representations of the natural language. These attentionbased models have achieved exceptional state-of-the-art results on various NLP benchmarks. One of the contributing factors is the growing use of transfer learning. Models are pre-trained on unsupervised objectives using rich datasets that develop fundamental natural language abilities that are fine-tuned further on supervised data for downstream tasks. Surprisingly, current researches have led to a novel era of powerful models that no longer require finetuning. The objective of this paper is to present a comparative analysis of some of the most influential language models. The benchmarks of the study are problem-solving methodologies, model architecture, compute power, standard NLP benchmark accuracies and shortcomings.",
        "link": "http://dx.doi.org/10.5121/csit.2021.110111"
    },
    {
        "id": 20022,
        "title": "Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models?",
        "authors": "Emma Tysinger, Brajesh Rai, Anton Sinitskiy",
        "published": "No Date",
        "citations": 0,
        "abstract": "Meaningful exploration of the chemical space of druglike molecules in drug design is a highly challenging task due to a combinatorial explosion of possible modifications of molecules. In this work, we address this problem with transformer models, a type of machine learning (ML) model, with recent demonstrated success in applications to machine translation and other tasks. By training transformer models on pairs of similar bioactive molecules from the public ChEMBL dataset, we enable them to learn medicinal-chemistry-meaningful, context-dependent transformations of molecules, including those absent from the training set. Most generated molecules are highly plausible and follow similar distributions of simple properties (molecular weight, polarity, hydrogen bond donor and acceptor numbers) as the training dataset. By retrospective analysis of the performance of transformer models on ChEMBL subsets of ligands binding to COX2, DRD2, or HERG protein targets, we demonstrate that the models can generate structures identical or highly similar to highly active ligands, despite the models having not seen any ligands active against the corresponding protein target during training. Thus, our work demonstrates that transformer models, originally developed to translate texts from one natural language to another, can be easily and quickly extended to “translations” from known molecules active against a given protein target to novel molecules active against the same target, and thereby contribute to hit expansion in drug design.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-gln27"
    },
    {
        "id": 20023,
        "title": "DIDACTIC MATHEMATICAL DERIVATION OF THE REAL AND IDEAL TRANSFORMER MODELS",
        "authors": "Julio Gonzalez",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/iceri.2018.0431"
    },
    {
        "id": 20024,
        "title": "Single-sequence protein structure prediction using supervised transformer protein language models",
        "authors": "Wenkai Wang, Zhenling Peng, Jianyi Yang",
        "published": "No Date",
        "citations": 8,
        "abstract": "AbstractIt remains challenging for single-sequence protein structure prediction with AlphaFold2 and other deep learning methods. In this work, we introduce trRosettaX-Single, a novel algorithm for singlesequence protein structure prediction. It is built on sequence embedding from s-ESM-1b, a supervised transformer protein language model optimized from the pre-trained model ESM-1b. The sequence embedding is fed into a multi-scale network with knowledge distillation to predict inter-residue 2D geometry, including distance and orientations. The predicted 2D geometry is then used to reconstruct 3D structure models based on energy minimization. Benchmark tests show that trRosettaX-Single outperforms AlphaFold2 and RoseTTAFold on natural proteins. For instance, with single-sequence input, trRosettaX-Single generates structure models with an average TM-score ~0.5 on 77 CASP14 domains, significantly higher than AlphaFold2 (0.35) and RoseTTAFold (0.34). Further test on 101 human-designed proteins indicates that trRosettaX-Single works very well, with accuracy (average TM-score 0.77) approaching AlphaFold2 and higher than RoseTTAFold, but using much less computing resource. On 2000 designed proteins from network hallucination, trRosettaX-Single generates structure models highly consistent to the hallucinated ones. These data suggest that trRosettaX-Single may find immediate applications in de novo protein design and related studies. trRosettaX-Single is available through the trRosetta server at: http://yanglab.nankai.edu.cn/trRosetta/.",
        "link": "http://dx.doi.org/10.1101/2022.01.15.476476"
    },
    {
        "id": 20025,
        "title": "Effectiveness of Data Augmentation and Ensembling Using Transformer-Based Models for Sentiment Analysis: Software Engineering Perspective",
        "authors": "Zubair Tusar, Sadat Sharfuddin, Muhtasim Abid, Md. Haque, Md. Mostafa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012092500003538"
    },
    {
        "id": 20026,
        "title": "DGA domain detection using pretrained character based transformer models",
        "authors": "Bronjon Gogoi, Tasiruddin Ahmed",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183602"
    },
    {
        "id": 20027,
        "title": "Improving protein secondary structure prediction by deep language models and transformer networks",
        "authors": "Tianqi Wu, Weihang Cheng, Jianlin Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProtein secondary structure prediction is useful for many applications. It can be considered a language translation problem, i.e., translating a sequence of 20 different amino acids into a sequence of secondary structure symbols (e.g., alpha helix, beta strand, and coil). Here, we develop a novel protein secondary structure predictor called TransPross based on the transformer network and attention mechanism widely used in natural language processing to directly extract the evolutionary information from the protein language (i.e., raw multiple sequence alignment (MSA) of a protein) to predict the secondary structure. The method is different from traditional methods that first generate a MSA and then calculate expert-curated statistical profiles from the MSA as input. The attention mechnism used by TransPross can effectively capture long-range residue-residue interactions in protein sequences to predict secondary structures. Benchmarked on several datasets, TransPross outperforms the state-of-art methods. Moreover, our experiment shows that the prediction accuracy of TransPross positively correlates with the depth of MSAs and it is able to achieve the average prediction accuracy (i.e., Q3 score) above 80% for hard targets with few homologous sequences in their MSAs. TransPross is freely available athttps://github.com/BioinfoMachineLearning/TransPro",
        "link": "http://dx.doi.org/10.1101/2022.11.21.517442"
    },
    {
        "id": 20028,
        "title": "Flood Prediction for Tropical Climates Using Lstm and Transformer Machine Learning Models",
        "authors": "Tharindu Madhushanka, Thishan Jayasinghe, Ruwan Rajapakse",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4736261"
    },
    {
        "id": 20029,
        "title": "Transformer-based Language Models for Semantic Search and Mobile Applications Retrieval",
        "authors": "João Coelho, António Neto, Miguel Tavares, Carlos Coutinho, João Oliveira, Ricardo Ribeiro, Fernando Batista",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010657300003064"
    },
    {
        "id": 20030,
        "title": "Efficient Transformer based Sentiment Classification Models",
        "authors": "Leeja Mathew, Bindu V R",
        "published": "2022-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31449/inf.v46i8.4332"
    },
    {
        "id": 20031,
        "title": "Improving Stability of Transformer-based Named Entity Recognition Models with Combined Data Representation",
        "authors": "Michał Marcińczuk",
        "published": "No Date",
        "citations": 0,
        "abstract": "This study leverages transformer-based models and focuses on data representation strategies in the named entity recognition task, including &quot;single&quot; (one sentence per vector), &quot;merged&quot; (multiple sentences per vector), and &quot;context&quot; (sentences joined with attention to context). Performance analysis reveals that models trained with a single strategy may not perform well on different data representations. A combined training procedure is proposed to address this limitation, using all three strategies to enhance the stability and adaptability of the model. The results of this approach are presented and discussed for various datasets for four languages (English, Polish, Czech, and German), demonstrating the effectiveness of the combined strategy.",
        "link": "http://dx.doi.org/10.20944/preprints202309.1859.v1"
    },
    {
        "id": 20032,
        "title": "A literature review on multimodal deep learning models for detecting mental disorders in conversational data: Pre-transformer and transformer-based approaches",
        "authors": "Zilei Shao",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "This paper provides a comprehensive review of multimodal deep learning models that utilize conversational data to detect mental health disorders. In addition to discussing models based on the Transformer, such as BERT (Bidirectional Encoder Representations from Transformers), this paper addresses models that existed prior to the Transformer, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The paper covers the application of these models in the construction of multimodal deep learning systems to detect mental disorders. In addition, the difficulties encountered by multimodal deep learning systems are brought up. Furthermore, the paper proposes research directions for enhancing the performance and robustness of these models in mental health applications. By shedding light on the potential of multimodal deep learning in mental health care, this paper aims to foster further research and development in this critical domain.",
        "link": "http://dx.doi.org/10.54254/2755-2721/18/20230993"
    },
    {
        "id": 20033,
        "title": "Enhancing Text Summarization: Evaluating Transformer-Based Models and the Role of Large Language Models like ChatGPT",
        "authors": "Pınar Savcı, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391040"
    },
    {
        "id": 20034,
        "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
        "authors": "Laurent Sartran, Samuel Barrett, Adhiguna Kuncoro, Miloš Stanojević, Phil Blunsom, Chris Dyer",
        "published": "2022-12-22",
        "citations": 1,
        "abstract": "Abstract\nWe introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.",
        "link": "http://dx.doi.org/10.1162/tacl_a_00526"
    },
    {
        "id": 20035,
        "title": "Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks for Document Layout Analysis",
        "authors": "",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfer have been developed, it remains to be investigated to what degree they can effectively transfer knowledge. In this study, we aim to fill these gaps by conducting a comparative evaluation of state-of-the-art models in document layout analysis and investigating the potential of cross-lingual layout analysis by utilizing machine translation techniques.",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.17"
    },
    {
        "id": 20036,
        "title": "CrudeBERT: Applying Economic Theory Towards Fine-Tuning Transformer-Based Sentiment Analysis Models to the Crude Oil Market",
        "authors": "Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011749600003467"
    },
    {
        "id": 20037,
        "title": "Fast-Fnet: Accelerating Transformer Encoder Models Via Efficient Fourier Layers",
        "authors": "Nurullah Sevim, Ege  Ozan Ozyedek, Furkan Sahinuc, Aykut Koç",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566618"
    },
    {
        "id": 20038,
        "title": "Quantifying Interpretation Reproducibility in Vision Transformer Models with TAVAC",
        "authors": "Yue Zhao, Dylan Agyemang, Yang Liu, Matt Mahoney, Sheng Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe use of deep learning algorithms to extract meaningful diagnostic features from biomedical images holds the promise to improve patient care given the expansion of digital pathology. Among these deep learning models, Vision Transformer (ViT) models have been demonstrated to capture long-range spatial relationships with more robust prediction power for image classification tasks than regular convolutional neural network (CNN) models, and also better model interpretability. Model interpretation is important for understanding and elucidating how a deep learning model makes predictions, especially for developing transparent models for digital pathology. However, like other deep learning algorithms, with limited annotated biomedical imaging datasets, ViT models are prone to poor performance due to overfitting, which can lead to false predictions due to random noise. Overfitting affects model interpretation when predictions are made out of random noise. To address this issue, we introduce a novel metric – Training Attention and Validation Attention Consistency (TAVAC) – for evaluating ViT model degree of overfitting on imaging datasets and quantifying the reproducibility of interpretation. Specifically, the model interpretation is performed by comparing the high-attention regions in the image between training and testing. We test the method on four publicly available image classification datasets and two independent breast cancer histological image datasets. All overfitted models exhibited significantly lower TAVAC scores than the good-fit models. The TAVAC score quantitatively measures the level of generalization of model interpretation on a fine-grained level for small groups of cells in each H&E image, which cannot be provided by traditional performance evaluation metrics like prediction accuracy. Furthermore, the application of TAVAC extends beyond medical diagnostic AI models; it enhances the monitoring of model interpretative reproducibility at pixel-resolution in basic research, to reveal critical spatial patterns and cellular structures essential to understanding biological processes and disease mechanisms. TAVAC sets a new standard for evaluating the performance of deep learning model interpretation and provides a method for determining the significance of high-attention regions detected from the attention map of the biomedical images.",
        "link": "http://dx.doi.org/10.1101/2024.01.18.576252"
    },
    {
        "id": 20039,
        "title": "Development of series resonant converter models with output transformer",
        "authors": "Nikolay Hinov, Tsveti Hranov",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/et.2017.8124348"
    },
    {
        "id": 20040,
        "title": "Evaluation of Transformer-Based Neural Language Models for Writing Feedback and Automated Essay Scoring",
        "authors": "Temesgen Abraha, Amril Nazir",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWriting remains a challenging skill for many students due to inadequate feedback tools. There is a need to develop more effective tools for supporting students’ writing skill development. This study aims to evaluate the effectiveness of transformer-based neural language models for assessing and automatically scoring argumentative essays written by 8th-12th grade English Language Learners (ELLs). The students’ English essays were assessed and scored based on six criteria, including cohesion, syntax, vocabulary, phraseology, grammar, and conventions. The models were trained on real teacher feedback from 2700 scored essays. We also compared various transformer-based neural language models to find the most effective model. Several metrics were used for evaluation, with the root mean square error (RMSE) as the primary measure. The results show that a specific model, DeBERTa-v3-large, outperforms others in most categories. In conclusion, this study suggests that transformer-based neural language models, especially when using the DeBERTa-v3-large model, hold significant promise in improving automated essay scoring and feedback, potentially leading to enhanced writing skills among English language learners.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3979085/v1"
    },
    {
        "id": 20041,
        "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
        "authors": "Michael Hanna, Yonatan Belinkov, Sandro Pezzelle",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.744"
    },
    {
        "id": 20042,
        "title": "Impact of Transformer-Based Models and User Clustering in Early Fake News Detection in Social Media",
        "authors": "Sakshi Kalra, Yashvardhan Sharma, Mehul Agrawal, Sai Mantri, Gajendra Chauhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011684000003411"
    },
    {
        "id": 20043,
        "title": "Transformer protein language models are unsupervised structure learners",
        "authors": "Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, Alexander Rives",
        "published": "No Date",
        "citations": 99,
        "abstract": "AbstractUnsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1",
        "link": "http://dx.doi.org/10.1101/2020.12.15.422761"
    },
    {
        "id": 20044,
        "title": "Nowcasting with Transformer-based Models using Multi-Source Data&amp;#160;",
        "authors": "Çağlar Küçük, Apostolos Giannakos, Stefan Schneider, Alexander Jann",
        "published": "No Date",
        "citations": 0,
        "abstract": "Rapid advancements in data-driven weather prediction have shown notable success, particularly in nowcasting, where forecast lead times span just a few hours. Transformer-based models, in particular, have proven effective in learning spatiotemporal connections of varying scales by leveraging the attention mechanism with efficient space-time patching of data. This offers potential improvements over traditional nowcasting techniques, enabling early detection of convective activity and reducing computational costs.&#160;\nIn this presentation, we demonstrate the effectiveness of a modified Earthformer model, a space-time Transformer framework, in addressing two specific nowcasting challenges. First, we introduce a nowcasting model that predicts ground-based 2D radar mosaics up to 2-hour lead time with 5-minute temporal resolution, using geostationary satellite data from the preceding two hours. Trained on a benchmark dataset sampled across the United States, our model exhibits robust performance against various impactful weather events with distinctive features. Through permutation tests, we interpret the model to understand the effects of input channels and input data length. We found that the infrared channel centered at 10.3 &#181;m contains skillful information for all weather conditions, while, interestingly, satellite-based lightning data is the most skilled at predicting severe weather events in short lead times. Both findings align with existing literature, enhancing confidence in our model and guiding better usage of satellite data for nowcasting. Moreover, we found the model is sensitive to input data length in predicting severe weather events, suggesting early detection of convective activity by the model in rapidly growing fields.&#160;\nSecond, we present the initial attempts to develop a multi-source precipitation nowcasting model for Austria, tailored to predict impactful events with convective activities. This model integrates satellite- and ground-based observations with analysis and numerical weather prediction data to predict precipitation up to 2-hour lead time with 5-minute temporal resolution. &#160;\nWe conclude by discussing the broad spectrum of applications for such models, ranging from enhancing operational nowcasting systems to providing synthetic data to data-scarce regions, and the challenges therein.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-7536"
    },
    {
        "id": 20045,
        "title": "MuLan-Methyl - Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction",
        "authors": "Wenhuan Zeng, Anupam Gautam, Daniel H. Huson",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractTransformer-based language models are successfully used to address massive text-related tasks. DNA methylation is an important epigenetic mechanism and its analysis provides valuable insights into gene regulation and biomarker identification. Several deep learning-based methods have been proposed to identify DNA methylation and each seeks to strike a balance between computational effort and accuracy. Here, we introduce MuLan-Methyl, a deep-learning framework for predicting DNA methylation sites, which is based on five popular transformer-based language models. The framework identifies methylation sites for three different types of DNA methylation, namely N6-adenine, N4-cytosine, and 5-hydroxymethylcytosine. Each of the employed language models is adapted to the task using the “pre-train and fine-tune” paradigm. Pre-training is performed on a custom corpus of DNA fragments and taxonomy lineages using self-supervised learning. Fine-tuning aims at predicting the DNA-methylation status of each type. The five models are used to collectively predict the DNA methylation status. We report excellent performance of MuLan-Methyl on a benchmark dataset. Moreover, we argue that the model captures characteristic differences between different species that are relevant for methylation. This work demonstrates that language models can be successfully adapted to applications in biological sequence analysis and that joint utilization of different language models improves model performance. Mulan-Methyl is open source and we provide a web server that implements the approach.Key pointsMuLan-Methyl aims at identifying three types of DNA-methylation sites.It uses an ensemble of five transformer-based language models, which were pre-trained and fine-tuned on a custom corpus.The self-attention mechanism of transformers give rise to importance scores, which can be used to extract motifs.The method performs favorably in comparison to existing methods.The implementation can be applied to chromosomal sequences to predict methylation sites.",
        "link": "http://dx.doi.org/10.1101/2023.01.04.522704"
    },
    {
        "id": 20046,
        "title": "Transformer-Based Multilingual Language Models in Cross-Lingual Plagiarism Detection",
        "authors": "Tatevik Ter-Hovhannisyan, Karen Avetisyan",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivmem57067.2022.9983968"
    },
    {
        "id": 20047,
        "title": "Comparing Transformer and RNN Models in BCIs for Handwritten Text Decoding via Neural Signals",
        "authors": "Aashna Hari",
        "published": "No Date",
        "citations": 0,
        "abstract": "The purpose of this study is to ​​explore the use of a custom Transformer model in brain-computer interfaces (BCIs) that translate the neural activity present when an individual with limited verbal and fine-motor skills attempts to handwrite. As found in previous studies, Transformers have performed better than recurrent neural networks (RNNs) in translation tasks which are similar to its usage here in decoding neural signals into intended handwritten text. Due to the known benefits of a Transformer, the hypothesis was that the Transformer would show promise in the context of a BCI through the recorded metrics. The neural signals of a tetraplegic individual, when they attempted to handwrite, were provided by existing research. Four trials were conducted using data with or without augmentation which the model used when training to separately minimize training loss and validation loss. When comparing the results of a BCI with the implementation of a Transformer model with the original RNN BCI (the original data source), the Transformer model performed less favorably across all four trials. Although the results do not indicate that the Transformer model currently outperforms an RNN BCI, it is important to note that further testing of the model's capabilities (such as training it with a larger and more preferable dataset and/or for longer, comparing training times between the RNN and Transformer, and/or seeing how the Transformer is improved with an offline autocorrect feature) is necessary before determining whether Transformers can enhance communication in this manner.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0674.v1"
    },
    {
        "id": 20048,
        "title": "Learning the Language of NMR: Structure Elucidation from NMR spectra using Transformer Models",
        "authors": "Marvin Alberts, Federico Zipoli, Alain C. Vaucher",
        "published": "No Date",
        "citations": 1,
        "abstract": "The application of machine learning models in chemistry has made remarkable strides in recent years. Even though there is considerable interest in automating common proce- dure in analytical chemistry using machine learning, very few models have been adopted into everyday use. Among the analytical instruments available to chemists, Nuclear Mag- netic Resonance (NMR) spectroscopy is one of the most important, offering insights into molecular structure unobtainable with other methods. However, most processing and analysis of NMR spectra is still performed manually, making the task tedious and time consuming especially for larger quantities of spectra. We present a transformer-based machine learning model capable of predicting the molecular structure directly from the NMR spectrum. Our model is pretrained on synthetic NMR spectra, achieving a top–1 accuracy of 67.0% when predicting the structure from both the 1H and 13C spectrum. Additionally, we train a model which, given a spectrum and a set of likely compounds, selects the one corresponding to the spectrum. This model achieves a top–1 accuracy of 96.0% when trained on 1H spectra.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-8wxcz"
    },
    {
        "id": 20049,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Priyanshu Priya",
        "published": "2023-12-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ejpn8g"
    },
    {
        "id": 20050,
        "title": "Relaxed Attention for Transformer Models",
        "authors": "Timo Lohrenz, Björn Möller, Zhengyang Li, Tim Fingscheidt",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191643"
    },
    {
        "id": 20051,
        "title": "Toward Capturing Effective Interactions of Transformer based Models for Click-Through Rate Prediction",
        "authors": "Soyoung Kim, Dongjun Lee, Jaekwang Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPredicting click-through rates involves estimating how likely it is that users will click on a website’s recommendations, such as advertisements or product listings. With the growth in data complexity and volume, it has become economically vital for online platforms to recommend appropriate items to users. Yet, this task remains difficult due to the sparse and high-dimensional categorical nature of the data inputs used in prediction models. Thanks to the superior performance of deep learning models compared to traditional methods, various deep learning strategies have been introduced to understand both simple and complex interactions from the mentioned data inputs. Nevertheless, feed-forward networks struggle to effectively capture common feature interactions and are limited in their capacity to efficiently model functions involving complex, high-order interactions. Moreover, as the complexity of the model layers increases, it becomes more challenging to reflect simple, low-order interactions in the output. In this paper, we introduce Low-Order Cross Attention Networks (LOCAN) designed to explicitly capture high-order interactions while also reflecting low-order interactions in the final prediction. We incorporate low-order interactions, identified through Factorization Machines, into the attention layer in a continuous manner, ensuring these interactions are carried through to the final prediction. Our experiments on three real-world datasets show that our model surpasses current leading methods, confirming the effectiveness of the proposed model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3987103/v1"
    },
    {
        "id": 20052,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Hao Yu",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/16y6ju"
    },
    {
        "id": 20053,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Sunitha Sabbu",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/z8vi7n"
    },
    {
        "id": 20054,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Utkarsh Sharma",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/c87pxf"
    },
    {
        "id": 20055,
        "title": "Simultaneous paraphrasing and translation by fine-tuning Transformer models",
        "authors": "Rakesh Chada",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.ngt-1.23"
    },
    {
        "id": 20056,
        "title": "Do transformer models do phonology like a linguist?",
        "authors": "Saliha Muradoglu, Mans Hulden",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.541"
    },
    {
        "id": 20057,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Yousef Sharrab",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/dznrwf"
    },
    {
        "id": 20058,
        "title": "Evaluation of Hottest-Spot Temperature Models using Field Measured Transformer Data",
        "authors": "Oluwaseun Amoda, Daniel J. Tylavsky, Gary Mcculla, Wesley Knuth",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00103"
    },
    {
        "id": 20059,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Gilad Shamir",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/687rre"
    },
    {
        "id": 20060,
        "title": "The Differential Expressions for Transformer Tap and Shunt Capacitor Unit",
        "authors": "",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-813231-9.09990-8"
    },
    {
        "id": 20061,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Abdul Jaleel",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/s142n1"
    },
    {
        "id": 20062,
        "title": "Creation &amp; Validation of Transformer Residual Life Models",
        "authors": "Caleb Walker, David Walker",
        "published": "2022-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl49583.2022.9830925"
    },
    {
        "id": 20063,
        "title": "Three-Phase Transformer Models",
        "authors": "William H. Kersting, Robert J. Kerestes",
        "published": "2022-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003261094-8"
    },
    {
        "id": 20064,
        "title": "Sentiment Analysis on Code-Mixed Tamil-English Corpus: A Comprehensive Study of Transformer-Based Models",
        "authors": "M. Sangeetha, K. Nimala",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNLP, or natural language processing, is a subfield of AI that aims to equip computers with the ability to understand and analyze human language. Sentiment analysis is a widely used application of NLP, particularly for examining attitudes expressed in online conversations. Nevertheless, many social media comments are written in languages that are not native to the authors, making sentiment analysis more difficult, especially for languages with limited resources, such as Tamil. To tackle this issue, a code-mixed and sentiment-annotated corpus in Tamil and English was created. This article will explain how the corpus was established, including the process of data collection and the assignment of polarities. The article will also explore the agreement between annotators and the results of sentiment analysis performed on the corpus. This work signifies various performance metrics such as precision, recall, support, and F1-score for the transformer-based model such as BERT, RoBerta, and XLM-RoBerta. Among the various models, XLM-Robert shows slightly significant positive results on the code-mixed corpus when compared to the state of art models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3418283/v1"
    },
    {
        "id": 20065,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Enrico Ferrari",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/03oa0c"
    },
    {
        "id": 20066,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Ismail Dergaa",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hnu7u9"
    },
    {
        "id": 20067,
        "title": "Groundwater Prediction in the Thames Basin, London, Using Temporal Fusion Transformer Models&amp;#160;",
        "authors": "Ali Ali, Ashraf Ahmed, Maysam Abbod",
        "published": "No Date",
        "citations": 0,
        "abstract": "Addressing Thames Basin aquifer complex dynamics in England, this study uses a Temporal Fusion Transformer (TFT) for groundwater level prediction. Our research combines extensive hydrological data with advanced machine learning suited to Thames Basin, where a complex network of rivers and streams substantially affects groundwater dynamics. Unlike previous studies, this research focuses on long-term forecasting with deep learning, offering a long prediction horizon. To rigorously examine the model performance and robustness on new, unseen data, we applied the walk-forward validation method and other matrices such as RMSE and R2 coupled with the Holdout technique. Our approach contrasts traditional Long-Short Term Memory (LSTM), Attention-based LSTM, and TFT, focusing on the basin&#8217;s aquifers, Chalk, Oolitic Limestone, and Lower Greensand. Whilst both LSTM models were optimised using the Bayesian technique, TFT was applied for its inherent capability in complex time series. Our methodology processed historical groundwater and rainfall data from 2001-2023, accounting for the potential lag in aquifer response to the proximity of the river system. The dataset served as training, validation, and holdout for each model, focusing on capturing the dynamic temporal fluctuation. The results clearly showed the superiority of the TFT model in all aquifer types compared to other models across all horizons 7, 30, and 60 days. In the 60 days, the best results were observed in the Chalk aquifer with RMSE of 0.04 and R2 of 0.97 in holdout validation. However, in Limestone and Lower greensand aquifers, the TFT showed RMSEs of 0.12 and 0.016 and R2s of 0.65 and 0.32, respectively. Traditional LSTM models demonstrated limited predictive power, with negative values in all aquifers, while Attention-based LSTM slightly improved the efficacy. This study highlights the potential of sophisticated machine learning in managing complex aquifers and predicting water tables.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-10089"
    },
    {
        "id": 20068,
        "title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models",
        "authors": "Sebastian Koch, Stefan Pasch",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386765"
    },
    {
        "id": 20069,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Rashmita Khilar",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/4dbqec"
    },
    {
        "id": 20070,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Vedanarayanan.v Venugopal",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/9ws8op"
    },
    {
        "id": 20071,
        "title": "Transformer &amp; Lstm Based Models for Multi Day Ahead Flood Prediction in Tropical Climates",
        "authors": "Tharindu Madhushanka, Thishan Jayasinghe, Ruwan Rajapakse",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4746297"
    },
    {
        "id": 20072,
        "title": "Data Augmentation for Automated Essay Scoring using Transformer Models",
        "authors": "Kshitij Gupta",
        "published": "2023-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aisc56616.2023.10085523"
    },
    {
        "id": 20073,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Deepali Bajaj",
        "published": "2023-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jsq5fe"
    },
    {
        "id": 20074,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Everton Gomede",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/d2sppz"
    },
    {
        "id": 20075,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Otilia Manta",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/yvujxa"
    },
    {
        "id": 20076,
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": "Liwen Hou, David Smith",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.230"
    },
    {
        "id": 20077,
        "title": "Question Formulation and Transformer Model Resilience: Effects of Question-Wording Variants on the Robustness of Transformer-Based Question-Answer Models",
        "authors": "Ashraf Elnashar, William Schreiber, Jules White, Douglas C. Schmidt",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci58124.2022.00319"
    },
    {
        "id": 20078,
        "title": "Detecting Hate Speech Utilizing Deep Convolutional Network and Transformer Models",
        "authors": "Utkarsh Mittal",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370502"
    },
    {
        "id": 20079,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Anatoliy V. Zavdoveev",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/kqjoq5"
    },
    {
        "id": 20080,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Wazib Ansar",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/t3w9gi"
    },
    {
        "id": 20081,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Elakkiya R",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/42pqdl"
    },
    {
        "id": 20082,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Anna Bryniarska",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/05mbhg"
    },
    {
        "id": 20083,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Hassina Aliane",
        "published": "2023-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/18e0og"
    },
    {
        "id": 20084,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "G. N. Vivekananda",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/4ehkp5"
    },
    {
        "id": 20085,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Dr.M.Nafees Muneera",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/qszltq"
    },
    {
        "id": 20086,
        "title": "Detection and Classification of ChatGPT Generated Contents Using Deep Transformer Models",
        "authors": "Mahdi Maktab Dar Oghaz, Kshipra Dhame, Gayathri Singaram, Lakshmi Babu Saheer",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> The scope of this manuscript covers the use of machine learning and deep learning models to detect and classify AI-generated text, with a particular focus on maintaining academic integrity in computer science. It also includes the creation and public release of a dataset comprising human and AI-generated content. The work also compares these models with existing solutions like Turnitin's AI plagiarism detector, contributing to a robust baseline for identifying AI-generated content in academia. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23895951"
    },
    {
        "id": 20087,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Mostafa Aref",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/pnq9or"
    },
    {
        "id": 20088,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Claudio Ferretti",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/nzfyda"
    },
    {
        "id": 20089,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "豐緒 王",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/bidlef"
    },
    {
        "id": 20090,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Wen Juan",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fhbomb"
    },
    {
        "id": 20091,
        "title": "Investigating Lightweight Transformer Models for Defect Detection",
        "authors": "Hanyun Wang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "In industrial production, product defect detection is vital for quality control. Traditional manual inspection is inefficient and error-prone. Deep learning, particularly in image processing, has enabled computer-based automated defect detection. This paper proposes a Visual Transformer-based model to overcome limitations in industrial anomaly detection. Leveraging pretrained Vision Transformer and Point Transformer models, it extracts features from RGB images and point cloud data. Multimodal feature fusion enhances anomaly perception, with residual connections mitigating feature loss. On the MVTec AD dataset, it achieves 96.3% AU PRO for anomaly detection and 99.3% Pixel ROCAUC for anomaly segmentation. To enable deployment on devices like Raspberry Pi, the paper introduces a lightweight model via post-training quantization and pruning. This results in a 28.52% inference speedup with only a 1.08% average detection accuracy drop, facilitating practical industrial applications on compact devices.",
        "link": "http://dx.doi.org/10.54097/ajst.v7i3.12694"
    },
    {
        "id": 20092,
        "title": "Time Series Forecasting with Transformer Models and Application to Asset Management",
        "authors": "Edmond Lezmi, Jiali Xu",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4375798"
    },
    {
        "id": 20093,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Joaquín Gayoso-Cabada",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hlv4qx"
    },
    {
        "id": 20094,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Rodrigo Pérez Fernández",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/pr6zic"
    },
    {
        "id": 20095,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Hao Yu",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/nq5g3u"
    },
    {
        "id": 20096,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Ashfia Jannat Keya",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fpmtsi"
    },
    {
        "id": 20097,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Roheen Qamar",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ntsmus"
    },
    {
        "id": 20098,
        "title": "Use of a Three-Winding Step-Up Transformer",
        "authors": "Robert E. Henry",
        "published": "2017-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b22411-6"
    },
    {
        "id": 20099,
        "title": "Online Textual Hate Content Recognition Using Fine-Tuned Transformer Models",
        "authors": "Sneha Chinivar, Roopa M S, Arunalatha J S, Venugopal K R",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4463161"
    },
    {
        "id": 20100,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Jaganathan Dhanalakshmi",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/j8d6yn"
    },
    {
        "id": 20101,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Hao Yu",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/nq5g3u"
    },
    {
        "id": 20102,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Joaquín Gayoso-Cabada",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hlv4qx"
    },
    {
        "id": 20103,
        "title": "A Survey of Transformer-Based Natural Language Processing Models",
        "authors": "鸣姝 赖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/airr.2023.123025"
    },
    {
        "id": 20104,
        "title": "Plant Identification Using Convolution Neural Network and Vision Transformer-Based Models",
        "authors": "Virender Singh, Mathew Rees, Simon Hampton, Sivaram Annadurai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Identification of plants is a challenging task which aims to identify the family, genus, and species level according to morphological features. Automated deep learning-based computer vision algorithms are widely used for identifying plants and can help users to narrow down the possibilities. However, numerous morphological similarities between and within species make the classification difficult. In this paper, we tested a custom convolution neural network (CNN) and vision transformer (ViT) based models using the PyTorch framework to classify plants. We used a large dataset of 88K and 16K images for classifying plants at genus and species levels respectively. Our results show that for classifying plants at the genus level, ViT models perform better compared to CNN-based models ResNet50 and ResNet-RS-420, and other state-of-the-art CNN-based models suggested in previous studies on a similar dataset. The ViT model achieved top accuracy of 83.3% for classifying plants at the genus level. ViT models also perform better for classifying plants at the species level compared to CNN-based models ResNet50 and ResNet-RS-420, with a top accuracy of 92.5%. We show that the correct set of augmentation techniques plays an important role in classification success.",
        "link": "http://dx.doi.org/10.20944/preprints202308.1330.v1"
    },
    {
        "id": 20105,
        "title": "CQA Transformer Models in the Home Improvement Domain",
        "authors": "Macedo Maia, Markus Endres",
        "published": "2022-2",
        "citations": 0,
        "abstract": "To find answers for subjective questions about many topics through Q\\&A forums, questioners and answerers can cooperatively help themselves by sharing their doubts or answers based on their background and life experiences. These experiences can help machines redirect questioners to find better answers based on community question-answering models. This work proposes a comparative analysis of the pairwise community answer retrieval models in the home improvement domain considering different kinds of user question context information. Community Question-Answering (CQA) models must rank candidate answers in decreasing order of relevance for a user question.  Our contribution consists of transformer-based language models using different kinds of user information to accurate the model generalisation. To train our model, we propose a proper CQA dataset in the home improvement domain that consists of information extracted from community forums, including question context information. We evaluate our approach by comparing the performance of each baseline model based on rank-aware evaluation measures.",
        "link": "http://dx.doi.org/10.26421/jdi3.1-3"
    },
    {
        "id": 20106,
        "title": "Blockwise compression of transformer-based models without retraining",
        "authors": "Gaochen Dong, W. Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.001"
    },
    {
        "id": 20107,
        "title": "Accurate, interpretable predictions of materials properties within transformer language models",
        "authors": "Vadim Korolev, Pavel Protsenko",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patter.2023.100803"
    },
    {
        "id": 20108,
        "title": "Transformer-based Extraction of Deep Image Models",
        "authors": "Verena Battis, Alexander Penner",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eurosp53844.2022.00028"
    },
    {
        "id": 20109,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Sainik Kumar Mahata",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/s2s9tb"
    },
    {
        "id": 20110,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Juan Carlos Rincon Acuña",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/es9m9a"
    },
    {
        "id": 20111,
        "title": "Transformer-Based Deep Neural Network Language Models for Alzheimer's Disease Detection from Targeted Speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early diagnosis of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classiﬁcation tasks with small training sets. The overall classiﬁcation model is a simple classiﬁer on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classiﬁer achieves classiﬁcation accuracy of 88.08%, which improves the state-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of suﬃciently large datasets, but also reduces the need for expert-deﬁned features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v1"
    },
    {
        "id": 20112,
        "title": "Transformer-based deep neural network language models for Alzheimer’s disease detection from targeted speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early diagnosis of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classification tasks with small training sets. The overall classification model is a simple classifier on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classifier achieves classification accuracy of 88.08%, which improves thestate-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of sufficiently large datasets, but also reduces the need for expert-defined features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v2"
    },
    {
        "id": 20113,
        "title": "Exploring the Power of Transformer Models in Hospitality Domain",
        "authors": "Jyoti Parsola",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "\r\n\r\n\r\n\r\nABSTRACT\r\nDespite decades of medical advancements and a rising interest in precision healthcare, the great majority of diagnoses are made after patients start to exhibit observable symptoms of sickness. However, early disease indication and detection can give patients and caregivers the opportunity for early intervention, better disease management, and effective use of healthcare resources. Deep learning and other recent advancements in machine learning provide a fantastic chance to fill this unmet demand. Transformer designs are very expressive because they encode long-range relationships in the input sequences via self-attention methods. The models we offer in this work are Transformer-based (TB), and we provide a thorough description of each one in contrast to the Transformer's typical design. This study focuses on text-based task (TB) models used in Natural Language Processing (NLP). An examination of the key ideas at the core of the effectiveness of these models comes first.  NLP's flexible architecture allows it to incorporate various heterogeneous concepts (such as diagnoses, treatments, measurements, and more) to further improve the accuracy of its predictions. Its (pre-)training results in disease and patient representations can also be helpful for future studies (i.e., transfer learning).\r\n\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.17762/msea.v70i1.2314"
    },
    {
        "id": 20114,
        "title": "Transformer-based Learning Models of Dynamical Systems for Robotic State Prediction",
        "authors": "Alec Reed, Doncey Albin, Anuh Pasricha, Alessandro Roncone, Christoffer Heckman",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, we propose a novel approach to data-driven dynamical forecasting using transformerbased learning methods. We explore and evaluate the effectiveness of this approach by developing new benchmarks for generalizable, data-driven dynamical forecasting on a robotic arm and a 4-wheeled ground vehicle. The transformer-based approach utilizes a spatio-temporal forecasting model called the Spacetimeformer (STF), which was originally designed for long-horizon fields such as weather and economics. We demonstrate that with key innovations, the STF model can also be a powerful tool for short-range dynamical forecasting. The results show that our approach can even outperform popular analytical dynamics models, such as the bicycle model for four-wheeled vehicles and the robot dynamics model for serial manipulator arms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3919154/v1"
    },
    {
        "id": 20115,
        "title": "Insights into the inner workings of transformer models for protein function prediction",
        "authors": "Markus Wenzel, Erik Grüner, Nils Strodthoff",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "Abstract\n\nMotivation\nWe explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too.\n\n\nResults\nThe approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins.\n\n\nAvailability and Implementation\nSource code can be accessed at https://github.com/markuswenzel/xai-proteins.\n",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btae031"
    },
    {
        "id": 20116,
        "title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
        "authors": "Aleksa Cvetanović, Predrag Tadić",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/telfor59449.2023.10372792"
    },
    {
        "id": 20117,
        "title": "Learning Accurate Integer Transformer Machine-Translation Models",
        "authors": "Ephrem Wu",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-021-00688-4"
    },
    {
        "id": 20118,
        "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
        "authors": "Junyi Peng, Oldřich Plchot, Themos Stafylakis, Ladislav Mosner, Lukáš Burget, Jan \"Honza\" Černocký",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-453"
    },
    {
        "id": 20119,
        "title": "EEG Classification with Transformer-Based Models",
        "authors": "Jiayao Sun, Jin Xie, Huihui Zhou",
        "published": "2021-3-9",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lifetech52111.2021.9391844"
    },
    {
        "id": 20120,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Juan Carlos Rincon Acuña",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jumydu"
    },
    {
        "id": 20121,
        "title": "Review of: \"The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models\"",
        "authors": "Jac Ka Lok Leung",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/656efp"
    },
    {
        "id": 20122,
        "title": "A Study of Levenshtein Transformer and Editor Transformer Models for Under-Resourced Languages",
        "authors": "Mya Ei San, Ye Kyaw Thu, Zar Zar Hlaing, Hlaing Myat Nwe, Thepchai Supnithi, Sasiporn Usanavasin",
        "published": "2021-12-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isai-nlp54397.2021.9678159"
    },
    {
        "id": 20123,
        "title": "Life assessment of transformer using thermal models",
        "authors": "Shraddha Acharya, Pawan C. Tapre",
        "published": "2017-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecds.2017.8390114"
    },
    {
        "id": 20124,
        "title": "Biomechanics Multiactivity Transformer (BioMAT) Dataset",
        "authors": "Mohsen Sharifi Renani",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56902/cob.bmat.2023.1"
    },
    {
        "id": 20125,
        "title": "Decoding protein binding landscape on circular RNAs with base-resolution Transformer models",
        "authors": "Hehe Wu, Yi Fang, Yang Yang, Xiaoyong Pan, Hong-Bin Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractCircular RNAs (circRNAs) interact with RNA-binding proteins (RBPs) to modulate gene expression. To date, most computational methods for predicting RBP binding sites on circRNAs focus on circRNA fragments instead of circRNAs. These methods detect whether a circRNA fragment contains binding sites, but cannot determine where are the binding sites and how many binding sites are on the circRNA transcript. We report a hybrid deep learning-based tool, CircSite, to predict RBP binding sites at single-nucleotide resolution and detect key contributed nucleotides on circRNA transcripts. CircSite takes advantage of convolutional neural networks (CNNs) and Transformer for learning local and global representations of circRNAs binding to RBPs, respectively. We construct 37 datasets of RBP-binding circRNAs for benchmarking and the experimental results show that CircSite offers accurate predictions of RBP binding nucleotides and detects key subsequences aligning well with known binding motifs.",
        "link": "http://dx.doi.org/10.1101/2022.11.20.517239"
    },
    {
        "id": 20126,
        "title": "Persian Ezafeh Recognition using Transformer-Based Models",
        "authors": "Ali Ansari, Zahra Ebrahimian, Ramin Toosi, Mohammad Ali Akhaee",
        "published": "2023-5-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwr57742.2023.10139204"
    },
    {
        "id": 20127,
        "title": "Comparative Analysis of Transformer based Models for Question Answering",
        "authors": "Anchal Rawat, Surender Singh Samant",
        "published": "2022-12-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisct55310.2022.10046525"
    },
    {
        "id": 20128,
        "title": "Genealogical Relationship Extraction from Unstructured Text Using Fine-Tuned Transformer Models",
        "authors": "Carloangello Parrolivelli, Lubomir Stanchev",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsc56153.2023.00035"
    },
    {
        "id": 20129,
        "title": "Transformer-based deep neural network language models for Alzheimer's disease risk assessment from targeted speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nBackground: We developed transformer-based deep learning models based on natural language processing for early risk assessment of Alzheimer’s disease from the picture description test.Methods: The lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classiﬁcation tasks with small training sets. The overall classiﬁcation model is a simple classiﬁer on top of the pre-trained deep language model.Results: The models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classiﬁer achieves classiﬁcation accuracy of 88.08%, which improves the state-of-the-art by 2.48%.Conclusions: Using pre-trained language models can improve AD prediction. This not only solves the problem of lack of suﬃciently large datasets, but also reduces the need for expert-deﬁned features.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-49267/v3"
    },
    {
        "id": 20130,
        "title": "Symbolic Semantic Memory in Transformer Language Models",
        "authors": "Robert Morain, Kenneth Vargas, Dan Ventura",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00166"
    },
    {
        "id": 20131,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Quoc-Dai Luong Tran",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/wcflc3"
    },
    {
        "id": 20132,
        "title": "Detecting Sarcasm in Conversation Context Using Transformer-Based Models",
        "authors": "Adithya Avvaru, Sanath Vobilisetty, Radhika Mamidi",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.figlang-1.15"
    },
    {
        "id": 20133,
        "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models",
        "authors": "Adrian David Cheok, Emma Yann Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Generative transformers have revolutionized the realm of artificial intelligence, particularly in the domain of natural language processing. This paper embarks on a historical journey, tracing the roots of computational theory with Alan Turing and culminating in the sophisticated generative transformer architectures of today. Through a blend of review, history, and tutorial, we aim to provide a holistic understanding of these models, emphasizing their significance, underlying mechanisms, and vast applications. The tutorial segment offers a hands-on approach, guiding readers through the intricacies of building a basic generative transformer model. As we navigate this transformative landscape, we also shed light on challenges, ethical considerations, and future prospects in the world of generative models.\n",
        "link": "http://dx.doi.org/10.32388/3ntolq.2"
    },
    {
        "id": 20134,
        "title": "The Future of Education and Human Development in The Era of Generative Pre-Trained Transformer (GPT) Models",
        "authors": "Deepu Kurian, Amin Alizadeh, Courtney M Peebles",
        "published": "No Date",
        "citations": 0,
        "abstract": "The impact of generative artificial intelligence (AI) on education and human development is currently unknown and may have substantial ethical implications in these fields. In this commentary, we discuss the nature of Generative Pre-trained Transformer (GPT) models that use deep learning to produce human-like text. This is an effort to understand how GPT models can support students, educators, and human development professionals to enhance learning while assisting in developing their professions. We conclude by outlining the need for policy decisions and ethical considerations on incorporating these technologies into educational and human development settings.\n",
        "link": "http://dx.doi.org/10.32388/mzqcd3"
    },
    {
        "id": 20135,
        "title": "Phishing and Fraudulent Email Detection through Transfer Learning using pretrained transformer models",
        "authors": "Bronjon Gogoi, Tasiruddin Ahmed",
        "published": "2022-11-24",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon56171.2022.10040097"
    },
    {
        "id": 20136,
        "title": "Behavior of LSTM and Transformer Deep Learning Models in Flood Simulation Considering South Asian Tropical Climate",
        "authors": "G.W.T.I. Madhushanka, M.T.R. Jayasinghe, R.A. Rajapakse",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe imperative for a reliable and accurate flood forecasting procedure stem from the hazardous nature of the disaster. In response, researchers are increasingly turning to innovative approaches, particularly machine learning models, which offer enhanced accuracy compared to traditional methods. However, a notable gap exists in the literature concerning studies focused on the South Asian tropical region, which possesses distinct climate characteristics. This study investigates the applicability and behavior of Long Short-Term Memory (LSTM) and Transformer models in flood simulation with one day lead time, at the lower reach of Mahaweli catchment in Sri Lanka, which is mostly affected by the Northeast Monsoon. The importance of different input variables in the prediction was also a key focus of this study. Input features for the models included observed rainfall data collected from three nearby rain gauges, as well as historical discharge data from the target river gauge. Results showed that use of past water level data denotes a higher impact on the output compared to the other input features such as rainfall, for both architectures. All models denoted satisfactory performances in simulating daily water levels, especially low stream flows, with Nash Sutcliffe Efficiency (NSE) values greater than 0.77 while Transformer Encoder model showed a superior performance compared to Encoder Decoder models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4115691/v1"
    },
    {
        "id": 20137,
        "title": "NLP for Responsible Finance: Fine-Tuning Transformer-Based Models for ESG",
        "authors": "Stefan Pasch, Daniel Ehnes",
        "published": "2022-12-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020755"
    },
    {
        "id": 20138,
        "title": "Streaming Transformer-Based Acoustic Models Using Self-Attention with Augmented Memory",
        "authors": "Chunyang Wu, Yongqiang Wang, Yangyang Shi, Ching-Feng Yeh, Frank Zhang",
        "published": "2020-10-25",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2079"
    },
    {
        "id": 20139,
        "title": "Biomechanics Multiactivity Transformer (BioMAT) Model",
        "authors": "Mohsen Sharifi Renani",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56902/cob.bmat.2023.2"
    },
    {
        "id": 20140,
        "title": "Sentiment Analysis of Thai Stock Reviews Using Transformer Models",
        "authors": "Pongsatorn Harnmetta, Taweesak Samanchuen",
        "published": "2022-6-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jcsse54890.2022.9836278"
    },
    {
        "id": 20141,
        "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
        "authors": "Eric Sun, Jinyu Li, Zhong Meng, Yu Wu, Jian Xue, Shujie Liu, Yifan Gong",
        "published": "2021-8-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1949"
    },
    {
        "id": 20142,
        "title": "Depressive Post Classification using Transformer Models",
        "authors": "Iftehaz Newaz, Adib Wahid Quader, Muhammed J. A. Patwary",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441302"
    },
    {
        "id": 20143,
        "title": "Greedy Layer Pruning: Decreasing Inference Time of Transformer Models",
        "authors": "David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodrı́guez-Sánchez",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3985429"
    },
    {
        "id": 20144,
        "title": "Deep Transfer Learning &amp;amp;Amp; Beyond: Transformer Language Models in Information Systems Research",
        "authors": "Ross Gruetzemacher, David Paradice",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3944942"
    },
    {
        "id": 20145,
        "title": "The Equivalence Conditions of 2D and 3D Models of Phase Shifting Transformer",
        "authors": "M. Ciesielski, P. Witczak",
        "published": "2022-10-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/arwtr54586.2022.9959940"
    },
    {
        "id": 20146,
        "title": "Knowledge Transfer Between Tasks and Languages in the Multi-task Encoder-agnostic Transformer-based Models",
        "authors": "Dmitry Karpov,  , Vasily Konovalov",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "We explore the knowledge transfer in the simple multi-task encoder-agnostic transformer-based models on five dialog tasks: emotion classification, sentiment classification, toxicity classification, intent classification, and topic classification. We show that these mo dels’ accuracy differs from the analogous single-task models by ∼0.9%. These results hold for the multiple transformer backbones. At the same time, these models have the same backbone for all tasks, which allows them to have about 0.1% more parameters than any analogous single-task model and to support multiple tasks simultaneously. We also found that if we decrease the dataset size to a certain extent, multi-task models outperform singletask ones, especially on the smallest datasets. We also show that while training multilingual models on the Russian data, adding the English data from the same task to the training sample can improve model performance for the multi-task and single-task settings. The improvement can reach 4-5% if the Russian data are scarce enough. We have integrated these models to the DeepPavlov library and to the DREAM dialogue platform.",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-200-214"
    },
    {
        "id": 20147,
        "title": "Revisiting Transformer-based Models for Long Document Classification",
        "authors": "Xiang Dai, Ilias Chalkidis, Sune Darkner, Desmond Elliott",
        "published": "2022",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.534"
    },
    {
        "id": 20148,
        "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
        "authors": "Jan Lehečka, Jan Švec, Josef V. Psutka, Pavel Ircing",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-872"
    },
    {
        "id": 20149,
        "title": "Arabic Dialect Identification and Sentiment Classification using Transformer-based Models",
        "authors": "Joseph Attieh, Fadi Hassan",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.54"
    },
    {
        "id": 20150,
        "title": "Identifying and Classifying Fake COVID-19 Tweets using Transformer Models",
        "authors": "Yasmine Eid Mahmoud, Farid Ali Mousa, Ayat Mahmoud",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217786"
    },
    {
        "id": 20151,
        "title": "Improve Transformer Models with Better Relative Position Embeddings",
        "authors": "Zhiheng Huang, Davis Liang, Peng Xu, Bing Xiang",
        "published": "2020",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.298"
    },
    {
        "id": 20152,
        "title": "Fake News Detection Using Transformer and Ensemble Learning Models",
        "authors": "Raquiba Sultana, Tetsuro Nishino",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiai-aai-winter58034.2022.00044"
    },
    {
        "id": 20153,
        "title": "Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models",
        "authors": "Shucong Zhang, Erfan Loweimi, Peter Bell, Steve Renals",
        "published": "2021-8-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-280"
    },
    {
        "id": 20154,
        "title": "Applying Transformer Models for Disease Named Entity Recognition",
        "authors": "S.T. Jarashanth, R.D. Nawarathna",
        "published": "2022-2-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarc54489.2022.9754023"
    },
    {
        "id": 20155,
        "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding",
        "authors": "Young Jin Kim, Hany Hassan",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.sustainlp-1.20"
    },
    {
        "id": 20156,
        "title": "Transformer in Action: A Comparative Study of Transformer-Based Acoustic Models for Large Scale Speech Recognition Applications",
        "authors": "Yongqiang Wang, Yangyang Shi, Frank Zhang, Chunyang Wu, Julian Chan, Ching-Feng Yeh, Alex Xiao",
        "published": "2021-6-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9414087"
    },
    {
        "id": 20157,
        "title": "Fine-tuning strategies for classifying community-engaged research studies using transformer-based models: Classification System and Improvement Study (Preprint)",
        "authors": "Brian J. Ferrell",
        "published": "No Date",
        "citations": 1,
        "abstract": "\nBACKGROUND\nCommunity-Engaged Research (CEnR) is where institutions of higher education collaborate with organizations in their communities to exchange resources and knowledge that enhance its wellbeing. Community engagement is an integral part of a university's mission; however, there are unique challenges with reporting CEnR metrics. These challenges often restrict external relations within communities as well as federally funded research programs. Capturing studies where communities are \"highly engaged\" allows institutions to be more informed about the prevalence of CEnR.\n\n\nOBJECTIVE\nWe propose an updated approach done on our own previous experiments to classify Community-Engaged Research, capturing distinct levels of involvement a community partner has in the overall direction of a research study.\n\n\nMETHODS\nThis paper describes the use of fine-tuning methods such as discriminative learning rates and freezing layers across three transformer-based models (BERT, Bio+ClinicalBERT, and XLM-RoBERTa), adding to the empirical evidence that the utilization of fine-tuning strategies significantly improves transformer-based models. Using deep learning to classify hand labeled CEnR studies to aid the tracking of these studies to our knowledge has not been done before.\n\n\nRESULTS\nBio+ClinicalBERT appears to be superior, achieving a 73.08% accuracy and 62.94% F1 score on the hold out set. All the models trained in these experiments outperformed our previous ones by 10-23% in F1 score and accuracy.\n\n\nCONCLUSIONS\nTransfer learning is a viable method for tracking these studies, and we were able to provide evidence that the utilization of fine-tuning strategies significantly improves transformer-based models, as well as a tool for categorizing the type and volume of engagement.\n",
        "link": "http://dx.doi.org/10.2196/preprints.41137"
    },
    {
        "id": 20158,
        "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
        "authors": "Chong Yu, Tao Chen, Zhongxue Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.15"
    },
    {
        "id": 20159,
        "title": "Medical text prediction and suggestion using generative pre-trained transformer models with dental medical notes",
        "authors": "Joseph Sirriani, Emre Sezgin, Daniel Claman, Simon L Linwood",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractBackgroundGenerative pre-trained transformer (GPT) models are one of the latest large pre-trained natural language processing (NLP) models, which enables model training with limited datasets, and reduces dependency on large datasets which are scarce and costly to establish and maintain. There is a rising interest to explore the use of GPT models in healthcare.ObjectiveWe investigate the performance of GPT-2 and GPT-Neo models for medical text prediction using 374,787 free-text dental notes.MethodsWe fine-tune pre-trained GPT-2 and GPT-Neo models for next word prediction on a dataset of over 374,000 manually written sections of dental clinical notes. Each model was trained on 80% of the dataset, validated on 10%, and tested on the remaining 10%. We report model performance in terms of next word prediction accuracy and loss. Additionally, we analyze the performance of the models on different types of prediction tokens for categories. We annotate each token in 100 randomly sampled notes by category (e.g. Names, Abbreviations, Clinical Terms, Punctuation, etc.) and compare the performance of each model by token category.ResultsModels present acceptable accuracy scores (GPT-2: 76%, GPT-Neo: 53%), and the GPT-2 model also performs better in manual evaluations, especially for names, abbreviations, and punctuation. The results suggest that pre-trained models have the potential to assist medical charting in the future. We share the lessons learned, insights, and suggestions for future implementations.ConclusionThe results suggest that pre-trained models have the potential to assist medical charting in the future. Our study presented one of the first implementations of the GPT model used with medical notes.",
        "link": "http://dx.doi.org/10.1101/2022.04.29.22274513"
    },
    {
        "id": 20160,
        "title": "OVERVIEW OF TRANSFORMER-BASED MODELS FOR MEDICAL IMAGE SEGMENTATION",
        "authors": "Diana Nam, Alexandr Pak",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Premedical diagnostics is the process of examining survey results. Correct premedical diagnostics can improve the process of patient management and reduce the burden on the medical sector. Diagnostics of medical images such as computed tomography and X-ray are obligatory steps for further treatment. However, the shortage of clinicians causes delays in this step. We observed two state-of-the-art algorithms proposed for medical image segmentation: TransUnet and Swin-Unet. We conducted a theoretical comparison of algorithms in terms of the applicability of pre-hospital diagnostics according to quality and speed of training. The comparison is based on the original source of code provided by the authors of the original articles. We chose these two algorithms because they have similar U-form architecture, a high level of citation, and show competitive DICE scores on pictures of various human organs. Some architectural features were also important. Both models inherit key elements of U-net. TransUnet is a hybrid Transformer and CNN model. It consists of Transformer encoder and a convolutional decoder. Some additional computations are required in the bottleneck. Swin-Unet is a fully Transformer-based model. These architectural differences give rise to a difference in the number of trainable parameters. Generally, deeper architectures with a bigger number of parameters usually show better performance, however, according to our review, Swin-Unet has a smaller number of parameters and shows better DICE and Hausdorff Distance. It should be noted that the distribution between false positive and false negative predictions is important in medical image processing. It is crucial to avoid overloading the medical sector while also not missing any sick patients. Precision and recall can be used to evaluate the ratio of incorrect predictions. Therefore, we also observed the results of caries segmentation where precision and DICE were provided. In this specific case, TransUnet shows better DICE and recall values but worse precision.",
        "link": "http://dx.doi.org/10.37943/13bkbf2003"
    },
    {
        "id": 20161,
        "title": "Ensemble precipitation nowcasting by combination of generative and transformer deep learning models",
        "authors": "Gabriele Franch, Elena Tomasi, Virginia Poli, Chiara Cardinali, Marco Cristoforetti, Pier Paolo Alberoni",
        "published": "No Date",
        "citations": 2,
        "abstract": "This work introduces a novel deep-learning method for generating realistic ensembles nowcast of radar-based precipitation at a five-minute time resolution for the next 60 minutes and longer.The proposed method is composed of a combination of two models: the first model is trained to compress and decompress the spatial domain into and from a discrete representation (tokens), while the second model evolves the compressed representation over time. Specifically, the compression and decompression model is based on a combination of a Quantized Variational Autoencoder with a Generative Adversarial Network, while the prediction over time leverages a Generative Pretrained Transformer (GPT) architecture.This separation of concerns (discretized spatial compression/decompression and temporal extrapolation) adds several desirable features not present in more commonly used deep learning methods based on recurrent/convolutional deep learning architectures:&#160;transformer output probabilities can be leveraged to generate ensemble/probabilistic forecasts (without the need of injecting noise)\nthe discretized spatial representation can be used to characterize each token, adding interpretability and explainability to the model\nthe combination of transformer probabilities and token characterization can be used at inference time for forecasts conditioning based on external factors (e.g. NWP forecast output)\nThe presented architecture is trained and tested on a 7-year radar dataset of reflectivity composites of the Emilia-Romagna Region, Italy. The method is then applied at two different scales: regional, over Emilia-Romagna, and national, on the entire Italian domain, showing the adaptability of the approach to multiple spatial domains. We will present the performance of this model for both deterministic and ensemble settings by comparing it with respect to other commonly used extrapolation and deep learning methods.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-15153"
    },
    {
        "id": 20162,
        "title": "Forecasting Bitcoin Volatility Spikes from Whale Transactions and Cryptoquant Data Using Synthesizer Transformer Models",
        "authors": "Dorien Herremans, Kah Wee Low",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247685"
    },
    {
        "id": 20163,
        "title": "Load flow analysis using transformer models in alternative simulation tools",
        "authors": "Vladislav Sitar, Jan Veleba, Karel Nohac",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2017.7967336"
    },
    {
        "id": 20164,
        "title": "Are transformer-based models more robust than CNN-based models?",
        "authors": "Zhendong Liu, Shuwei Qian, Changhong Xia, Chongjun Wang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.045"
    },
    {
        "id": 20165,
        "title": "Forecasting Bitcoin Volatility Spikes from Whale Transactions and Cryptoquant Data Using Synthesizer Transformer Models",
        "authors": "Dorien Herremans, Kah Wee Low",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247684"
    },
    {
        "id": 20166,
        "title": "Context-Driven Device-Edge Collaborative Vision Transformer Models for Edge Ai",
        "authors": "Sumaiya Tabassum Nimi, Md Adnan Arefeen, Md Yusuf Sarwar Uddin, Biplob Debnath, Srimat Chakradhar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4591026"
    },
    {
        "id": 20167,
        "title": "Predictive Modelling of Optical Beam  from Mixed Pitch Grating using Simplified Transformer Models",
        "authors": "Yu Dian Lim, Peng Zhao, Luca Guidoni, Jean-Pierre Likforman, Chuan Seng Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this study, a simplified transformer model is used to predict the beam waist of 1,092 nm light coupled out from SiN-based mixed pitch gratings at various heights. Two regions are isolated as the prediction target. With the grating located at z = 10 µm height, Region B ranges between 10 to 30 µm; while Region A ranges between 25 to 45 µm. Between the predicted and actual beam waists, average percentage error (APE) of 41.5% is obtained for Region B, which indicates a prediction accuracy of 58.5%. Meanwhile, for Region A, APE value of 15.2% is obtained, which indicates a prediction accuracy of 84.8%. This study provides a pioneering approach to using natural language processing model and finance-inspired indicators to perform predictive modelling on photonics data.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24225319.v1"
    },
    {
        "id": 20168,
        "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
        "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.276"
    },
    {
        "id": 20169,
        "title": "Comparative Analysis of Sentiment in Original and Summarized Tweets: Leveraging Transformer Models for Enhanced NLP Insights",
        "authors": "Kun Bu, Kandethody Ramachandran",
        "published": "2024-2-24",
        "citations": 0,
        "abstract": "This paper investigates the sentiments of Twitter users towards the emergent topic of ChatGPT, leveraging advanced techniques in natural language processing (NLP) and sentiment analysis (SA). Our approach uniquely incorporates a dual setting for sentiment analysis: one analyzes the sentiments of original, full-length tweets, while the other first condenses these tweets into succinct summaries before performing sentiment analysis. By employing this dual approach, we are able to offer a comparative analysis of sentiment assessment pre- and post-text summarization, exploring the accuracy and reliability of the summarized sentiments. Central to our methodology is the application of Transformer models, specifically ProphetNet, which facilitates a deeper and more nuanced understanding of the original text. Unlike traditional methods that rely on keyword extraction and aggregation, our approach generates coherent and contextually rich summaries, providing a novel lens for sentiment analysis. This research contributes to the field by presenting a comprehensive study comparing sentiment analysis outcomes between original texts and their summarized counterparts, and examining the effectiveness of different NLP techniques, namely NLTK and the Transformer-based ProphetNet model. The findings offer valuable insights into the dynamics of sentiment analysis in the context of social media and the efficacy of state-of-the-art NLP technologies in processing complex, real-world data.",
        "link": "http://dx.doi.org/10.5121/csit.2024.140404"
    },
    {
        "id": 20170,
        "title": "Node Context Selection in Transformer-Based Graph Representation Learning Models",
        "authors": "Tianze Wang, Amir H. Payberah, Vladimir Vlassov",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020988"
    },
    {
        "id": 20171,
        "title": "Transformer-Based Models for pHLA Binding Affinity Prediction",
        "authors": "Donghong Yang, Xin Peng, Shenglan Peng",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsess58500.2023.10293021"
    },
    {
        "id": 20172,
        "title": "Improving Transformer Models by Reordering their Sublayers",
        "authors": "Ofir Press, Noah A. Smith, Omer Levy",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.270"
    },
    {
        "id": 20173,
        "title": "When Transformer models are more compositional than humans: The case of the depth charge illusion",
        "authors": "Dario Paape",
        "published": "2023-1-27",
        "citations": 0,
        "abstract": "State-of-the-art Transformer-based language models like GPT-3 are very good at generating syntactically well-formed and semantically plausible text. However, it is unclear to what extent these models encode the compositional rules of human language and to what extent their impressive performance is due to the use of relatively shallow heuristics, which have also been argued to be a factor in human language processing. One example is the so-called depth charge illusion, which occurs when a semantically complex, incongruous sentence like No head injury is too trivial to be ignored is assigned a plausible but not compositionally licensed meaning (Don't ignore head injuries, even if they appear to be trivial). I present an experiment that investigated how depth charge sentences are processed by Transformer models, which are free of many human performance bottlenecks. The results are mixed: Transformers do show evidence of non-compositionality in depth charge contexts, but also appear to be more compositional than humans in some respects.",
        "link": "http://dx.doi.org/10.3765/elm.2.5370"
    },
    {
        "id": 20174,
        "title": "Token Level Evaluation and Feature Enhancer for Transformer-based Models",
        "authors": "Khanddorj Mendbayar, Masaki Aono",
        "published": "2021-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta53211.2021.9640269"
    },
    {
        "id": 20175,
        "title": "Predicting Real-time Scientific Experiments Using Transformer models and Reinforcement Learning",
        "authors": "Juan Manuel Parrilla-Gutierrez",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00084"
    },
    {
        "id": 20176,
        "title": "Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis (Preprint)",
        "authors": "Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nSemantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations.\n\n\nOBJECTIVE\nWe created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models.\n\n\nMETHODS\nGiven a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model.\n\n\nRESULTS\nOur model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set.\n\n\nCONCLUSIONS\nWe designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP.\n",
        "link": "http://dx.doi.org/10.2196/preprints.23099"
    },
    {
        "id": 20177,
        "title": "Automated Seizure Detection using Transformer Models on Multi-Channel EEGs",
        "authors": "Yuanda Zhu, May D. Wang",
        "published": "2023-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bhi58575.2023.10313440"
    },
    {
        "id": 20178,
        "title": "Sentence Bottleneck Autoencoders from Transformer Language Models",
        "authors": "Ivan Montero, Nikolaos Pappas, Noah A. Smith",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.137"
    },
    {
        "id": 20179,
        "title": "Generalizable Solar Irradiation Prediction using Large Transformer Models with Sky Imagery",
        "authors": "Kuber Reddy Gorantla, Aditi Roy",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10216081"
    },
    {
        "id": 20180,
        "title": "Sentiment Analysis of StockTwits Using Transformer Models",
        "authors": "Aysun Bozanta, Sabrina Angco, Mucahit Cevik, Ayse Basar",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00204"
    },
    {
        "id": 20181,
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": "Soo Ryu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.27"
    },
    {
        "id": 20182,
        "title": "Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights",
        "authors": "Munhak Lee, Joon-Hyuk Chang, Sang-Eon Lee, Ju-Seok Seong, Chanhee Park, Haeyoung Kwon",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-362"
    },
    {
        "id": 20183,
        "title": "Work-in-Progress: Computing Sentence Similarity for Short Texts using Transformer models",
        "authors": "Vidasha Ramnarain-Seetohul, Vandana Bassoo, Yasmine Rosunally",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/educon52537.2022.9766649"
    },
    {
        "id": 20184,
        "title": "Explaining transformer-based image captioning models: An empirical analysis",
        "authors": "Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",
        "published": "2022-7-18",
        "citations": 11,
        "abstract": "Image Captioning is the task of translating an input image into a textual description. As such, it connects Vision and Language in a generative fashion, with applications that range from multi-modal search engines to help visually impaired people. Although recent years have witnessed an increase in accuracy in such models, this has also brought increasing complexity and challenges in interpretability and visualization. In this work, we focus on Transformer-based image captioning models and provide qualitative and quantitative tools to increase interpretability and assess the grounding and temporal alignment capabilities of such models. Firstly, we employ attribution methods to visualize what the model concentrates on in the input image, at each step of the generation. Further, we propose metrics to evaluate the temporal alignment between model predictions and attribution scores, which allows measuring the grounding capabilities of the model and spot hallucination flaws. Experiments are conducted on three different Transformer-based architectures, employing both traditional and Vision Transformer-based visual features.",
        "link": "http://dx.doi.org/10.3233/aic-210172"
    },
    {
        "id": 20185,
        "title": "Optimized Recurrent and Transformer-Based Models for Multi-Step Forecasting of Agricultural Commodity Prices",
        "authors": "Lucas  Grogenski Meloca, Rodrigo  Clemente Thom de Souza, Ademir  Aparecido Constantino",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4525428"
    },
    {
        "id": 20186,
        "title": "Review of: \"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models\"",
        "authors": "Roheen Qamar",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.32388/6rmwu1"
    },
    {
        "id": 20187,
        "title": "Predictive Modelling of Optical Beam  from Mixed Pitch Grating using Simplified Transformer Models",
        "authors": "Yu Dian Lim, Peng Zhao, Luca Guidoni, Jean-Pierre Likforman, Chuan Seng Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this study, a simplified transformer model is used to predict the beam waist of 1,092 nm light coupled out from SiN-based mixed pitch gratings at various heights. Two regions are isolated as the prediction target. With the grating located at z = 10 µm height, Region B ranges between 10 to 30 µm; while Region A ranges between 25 to 45 µm. Between the predicted and actual beam waists, average percentage error (APE) of 41.5% is obtained for Region B, which indicates a prediction accuracy of 58.5%. Meanwhile, for Region A, APE value of 15.2% is obtained, which indicates a prediction accuracy of 84.8%. This study provides a pioneering approach to using natural language processing model and finance-inspired indicators to perform predictive modelling on photonics data.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24225319"
    },
    {
        "id": 20188,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 8,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 20189,
        "title": "Generative Pre-trained Transformer (GPT) Models for Irony Detection and Classification",
        "authors": "Mustafa Ulvi Aytekin, O. Ayhan Erdem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391005"
    },
    {
        "id": 20190,
        "title": "Multiclass malaria parasite recognition based on transformer models and a generative adversarial network",
        "authors": "Dianhuan Tan, Xianghui Liang",
        "published": "2023-10-10",
        "citations": 1,
        "abstract": "AbstractMalaria is an extremely infectious disease and a main cause of death worldwide. Microscopic examination of thin slide serves as a common method for the diagnosis of malaria. Meanwhile, the transformer models have gained increasing popularity in many regions, such as computer vision and natural language processing. Transformers also offer lots of advantages in classification task, such as Fine-grained Feature Extraction, Attention Mechanism etc. In this article, we propose to assist the medical professionals by developing an effective framework based on transformer models and a generative adversarial network for multi-class plasmodium classification and malaria diagnosis. The Generative Adversarial Network is employed to generate extended training samples from multiclass cell images, with the aim of enhancing the robustness of the resulting model. We aim to optimize plasmodium classification to achieve an exact balance of high accuracy and low resource consumption. A comprehensive comparison of the transformer models to the state-of-the-art methods proves their efficiency in the classification of malaria parasite through thin blood smear microscopic images. Based on our findings, the Swin Transformer model and MobileVit outperform the baseline architectures in terms of precision, recall, F1-score, specificity, and FPR on test set (the data was divided into train: validation: test splits). It is evident that the Swin Transformer achieves superior detection performance (up to 99.8% accuracy), while MobileViT demonstrates lower memory usage and shorter inference times. High accuracy empowers healthcare professionals to conduct precise diagnoses, while low memory usage and short inference times enable the deployment of predictive models on edge devices with limited computational and memory resources.",
        "link": "http://dx.doi.org/10.1038/s41598-023-44297-y"
    },
    {
        "id": 20191,
        "title": "Transformer-Based Models Aid Prediction of Transient Production of Oil Wells",
        "authors": "Chris Carpenter",
        "published": "2022-10-1",
        "citations": 1,
        "abstract": "\n_\nThis article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 206537, “Development of Deep Transformer-Based Models for Long-Term Prediction of Transient Production of Oil Wells,” by Ildar R. Abdrakhmanov, Evgenii A. Kanin, SPE, and Sergei A. Boronin, Skolkovo Institute of Science and Technology, et al. The paper has not been peer reviewed.\n\n\n_\nThe authors of the complete paper propose a novel approach to data-driven modeling of transient production of oil wells, applying transformer-based neural networks trained on multivariate time series composed of various parameters of oil wells measured during exploitation. By tuning machine-learning models for a single well (ignoring the effect of neighboring wells) on open-source field data sets, the authors of the paper demonstrate that the transformer-based method outperforms recurrent neural networks (RNNs) with long- and short-term memory (LSTM) and gated recurrent-unit cells in the forecasting of bottomhole pressure dynamics.\n\n\nIntroduction\nThe authors apply a novel deep-learning algorithm called a transformer to build surrogate models for simulations of well performance. Transformer architecture initially was developed for natural-language processing problems. However, in recent years, researchers have adapted transformers for time-series forecasting. In contrast with RNNs, the transformer does not process data sequentially. Instead, it handles the entire sequence by a multihead self-attention mechanism. The transformer is much more computationally efficient than an RNN because its training can be performed using graphics processing units.\nTransformer networks allow for application of the transfer-learning technique. First, the model is trained to predict the target parameter (e.g., bottomhole pressure) using the production data from a certain well. Next, the tuned weights are used to initialize the training procedure on the data from a target well. In other words, the fine-tuning is performed based on the pretrained model. The described technique allows the model to transfer knowledge from one time-series forecasting problem to another, leading to acceleration of the training process and improvement of model-prediction capability. The recurrent networks are not well-suited for transferring learning because the hidden state of recurrent cells is not transferred commonly.\n\n\nProblem Formulation\nThe authors consider a transient production of an oil well with arbitrary completion. In most cases, various parameters of the well are measured during its exploitation, including bottomhole and wellhead pressure and temperature, flow rates for each phase, choke size, and parameters of electric centrifugal pumps (if available). In this case, the authors propose an alternative approach to find the dependence between bottomhole pressure and flow rate that is based on deep-learning algorithms trained on available field data and that allows for obtaining quick predictions of bottomhole pressure or flow rates. The data-driven coupled model of a well and reservoir can be used to plan and optimize the production process. Also, the developed model can be used to estimate well and formation properties using a constant flow-rate response of a well.\n",
        "link": "http://dx.doi.org/10.2118/1022-0091-jpt"
    },
    {
        "id": 20192,
        "title": "IMPLEMENTATION OF INFORMATION MODELS OF DIAGNOSING AND PREDICTION OF CURRENT TRANSFORMER  INSULATION STATE",
        "authors": "L. S. Skrupskaya",
        "published": "2017-1-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15588/1607-3274-2016-4-5"
    },
    {
        "id": 20193,
        "title": "Evaluating Transformer Models and Human Behaviors on Chinese Character Naming",
        "authors": "Xiaomeng Ma, Lingyu Gao",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Abstract\nNeural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we first collect human speakers’ answers on unknown Character naming tasks and then evaluate a set of transformer models by comparing their performance with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models’ answers are highly correlated with humans’ answers. These results suggested that the transformer models can capture humans’ character naming behavior well.1",
        "link": "http://dx.doi.org/10.1162/tacl_a_00573"
    },
    {
        "id": 20194,
        "title": "Multi-task Active Learning for Pre-trained Transformer-based Models",
        "authors": "Guy Rotman, Roi Reichart",
        "published": "2022-11-7",
        "citations": 1,
        "abstract": "Abstract\nMulti-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1",
        "link": "http://dx.doi.org/10.1162/tacl_a_00515"
    },
    {
        "id": 20195,
        "title": "Achievement Expedients of Fuzzy Queuing Models with an Unreliable Electrical Transformer",
        "authors": "R. Ramesh, M. Seenivasan",
        "published": "2022-2-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeict53079.2022.9768437"
    },
    {
        "id": 20196,
        "title": "How Relevant Are Selectional Preferences for Transformer-based Language Models?",
        "authors": "Eleni Metheniti, Tim Van de Cruys, Nabil Hathout",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.109"
    },
    {
        "id": 20197,
        "title": "Transformer models used for text-based question answering systems",
        "authors": "Khalid Nassiri, Moulay Akhloufi",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-04052-8"
    },
    {
        "id": 20198,
        "title": "The Identification of Red-Meat Types using The Fine-Tuned Vision Transformer and MobileNet Models",
        "authors": "Nagham ALHAWAS, Zekeriya TÜFEKCİ",
        "published": "2022-5-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31590/ejosat.1112892"
    },
    {
        "id": 20199,
        "title": "Explainability in transformer models for functional genomics",
        "authors": "Jim Clauwaert, Gerben Menschaert, Willem Waegeman",
        "published": "2021-9-2",
        "citations": 18,
        "abstract": "Abstract\nThe effectiveness of deep learning methods can be largely attributed to the automated extraction of relevant features from raw data. In the field of functional genomics, this generally concerns the automatic selection of relevant nucleotide motifs from DNA sequences. To benefit from automated learning methods, new strategies are required that unveil the decision-making process of trained models. In this paper, we present a new approach that has been successful in gathering insights on the transcription process in Escherichia coli. This work builds upon a transformer-based neural network framework designed for prokaryotic genome annotation purposes. We find that the majority of subunits (attention heads) of the model are specialized towards identifying transcription factors and are able to successfully characterize both their binding sites and consensus sequences, uncovering both well-known and potentially novel elements involved in the initiation of the transcription process. With the specialization of the attention heads occurring automatically, we believe transformer models to be of high interest towards the creation of explainable neural networks in this field.",
        "link": "http://dx.doi.org/10.1093/bib/bbab060"
    },
    {
        "id": 20200,
        "title": "QUESTION ANSWERING SYSTEM FOR HOSPITALITY DOMAIN USING TRANSFORMER-BASED LANGUAGE MODELS",
        "authors": "Sathish Sathish Dhanasegar",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "Recent research demonstrates significant success on a wide range of Natural Language Processing (NLP) tasks by utilizing Transformer architectures. Question answering (QA) is an important aspect of the NLP task. The systems enable users to ask a question in natural language and receive an answer accordingly. Most questions in the hospitality industry are content-based, with the expected response being accurate data rather than”yes” or ”no.” Therefore, it requires the system to understand the semantics of the questions and return relevant answers. Despite several advancements in transformer-based models for QA, we are interested in evaluating how it performs with unlabeled data using a pre-trained model, which could also define-tune. This project aims to develop a Question-Answering system for the hospitality domain, in which text will have hospitality content, and the user will be able to ask a question about them. We use an Attention mechanism to train a span-based model that predicts the position of the start and end tokens in a paragraph. By using the model, the users can directly type in their questions in the interactive user interface and receive the response. The data set for this study is created using response templates from the existing dialogue system. We use the Stanford Question and Answer (SQuAD 2.0) data structure to form the dataset, which is mostly used for QA models. During phase1, we evaluate the pre-trained QA models BERT, ROBERTa, and DistilBERT to predict answers and measure the results using Exact Match(EM) and ROUGE-LF1-Score. In Phase 2 of the project, we fine-tune the QA models and their hyper-parameters by training the model with hospitality data sets, and the results are compared. The fine-tuned ROBERTa models achieved the maximum of ROUGE-L F1-Score and EM of 71.39 and 52.17, respectively, which is a relatively 4% increase in F1-Score and 8.7% increase in EM score compared to the pre-trained model. The results of this project will be used to improve the efficiency of the dialogue system in the hospitality industry.",
        "link": "http://dx.doi.org/10.26562/irjcs.2022.v0905.003"
    },
    {
        "id": 20201,
        "title": "Performance Optimization for Transformer Models on Text Classification Tasks",
        "authors": "Kshitij Malvankar, Enda Fallon, Paul Connolly, Kieran Flanagan",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icetci58599.2023.10330958"
    },
    {
        "id": 20202,
        "title": "Itri Amigos at ArAIEval Shared Task: Transformer vs. Compression-Based Models for Persuasion Techniques and Disinformation Detection",
        "authors": "Jehad Oumer, Nouman Ahmed, Natalia Manrique",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.53"
    },
    {
        "id": 20203,
        "title": "Deep Transformer Language Models for Arabic Text Summarization: A Comparison Study",
        "authors": "Hasna Chouikhi, Mohammed Alsuhaibani",
        "published": "2022-11-23",
        "citations": 6,
        "abstract": "Large text documents are sometimes challenging to understand and time-consuming to extract vital information from. These issues are addressed by automatic text summarizing techniques, which condense lengthy texts while preserving their key information. Thus, the development of automatic summarization systems capable of fulfilling the ever-increasing demands of textual data becomes of utmost importance. It is even more vital with complex natural languages. This study explores five State-Of-The-Art (SOTA) Arabic deep Transformer-based Language Models (TLMs) in the task of text summarization by adapting various text summarization datasets dedicated to Arabic. A comparison against deep learning and machine learning-based baseline models has also been conducted. Experimental results reveal the superiority of TLMs, specifically the PEAGASUS family, against the baseline approaches, with an average F1-score of 90% on several benchmark datasets.",
        "link": "http://dx.doi.org/10.3390/app122311944"
    },
    {
        "id": 20204,
        "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models",
        "authors": "Dusan Varis, Ondřej Bojar",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.650"
    },
    {
        "id": 20205,
        "title": "IBM Stock Forecast Using LSTM, GRU, Attention and Transformer Models",
        "authors": "Sihan Fu, Zining Tang, Jialin Li",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccect57938.2023.10140896"
    },
    {
        "id": 20206,
        "title": "Leveraging Readability and Sentiment in Spam Review Filtering Using Transformer Models",
        "authors": "Sujithra Kanmani, Surendiran Balasubramanian",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/csse.2023.029953"
    },
    {
        "id": 20207,
        "title": "Vision Transformer Based Models for Plant Disease Detection and Diagnosis",
        "authors": "Rayene Amina Boukabouya, Abdelouahab Moussaoui, Mohamed Berrimi",
        "published": "2022-11-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isia55826.2022.9993508"
    },
    {
        "id": 20208,
        "title": "Parameterisation methods for piezoelecric transformer equivalent circuit models",
        "authors": "H. O'Keeffe, M. P. Foster, J. N. Davidson",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.1152"
    },
    {
        "id": 20209,
        "title": "Multilingual Text Summarization for German Texts Using Transformer Models",
        "authors": "Tomas Humberto Montiel Alcantara, David Krütli, Revathi Ravada, Thomas Hanne",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "The tremendous increase in documents available on the Web has turned finding the relevant pieces of information into a challenging, tedious, and time-consuming activity. Text summarization is an important natural language processing (NLP) task used to reduce the reading requirements of text. Automatic text summarization is an NLP task that consists of creating a shorter version of a text document which is coherent and maintains the most relevant information of the original text. In recent years, automatic text summarization has received significant attention, as it can be applied to a wide range of applications such as the extraction of highlights from scientific papers or the generation of summaries of news articles. In this research project, we are focused mainly on abstractive text summarization that extracts the most important contents from a text in a rephrased form. The main purpose of this project is to summarize texts in German. Unfortunately, most pretrained models are only available for English. We therefore focused on the German BERT multilingual model and the BART monolingual model for English, with a consideration of translation possibilities. As the source of the experiment setup, took the German Wikipedia article dataset and compared how well the multilingual model performed for German text summarization when compared to using machine-translated text summaries from monolingual English language models. We used the ROUGE-1 metric to analyze the quality of the text summarization.",
        "link": "http://dx.doi.org/10.3390/info14060303"
    },
    {
        "id": 20210,
        "title": "Exploiting Ensemble of Transformer Models for Detecting Informative Tweets",
        "authors": "Abu Nowhash Chowdhury, Shawon Guha, Nurul Amin, Shahidul Islam Khan",
        "published": "2021-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon52576.2021.9691734"
    },
    {
        "id": 20211,
        "title": "Pre-trained transformer-based language models for Sundanese",
        "authors": "Wilson Wongso, Henry Lucky, Derwin Suhartono",
        "published": "2022-12",
        "citations": 4,
        "abstract": "AbstractThe Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",
        "link": "http://dx.doi.org/10.1186/s40537-022-00590-7"
    },
    {
        "id": 20212,
        "title": "Analyzing Redundancy in Pretrained Transformer Models",
        "authors": "Fahim Dalvi, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.398"
    },
    {
        "id": 20213,
        "title": "IMPLEMENTATION OF INFORMATION MODELS OF DIAGNOSING AND PREDICTION OF CURRENT TRANSFORMER  INSULATION STATE",
        "authors": "L. S. Skrupskaya",
        "published": "2017-1-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15588/1607-3274-2016-4-5"
    },
    {
        "id": 20214,
        "title": "Evaluating Transformer Models and Human Behaviors on Chinese Character Naming",
        "authors": "Xiaomeng Ma, Lingyu Gao",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Abstract\nNeural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we first collect human speakers’ answers on unknown Character naming tasks and then evaluate a set of transformer models by comparing their performance with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models’ answers are highly correlated with humans’ answers. These results suggested that the transformer models can capture humans’ character naming behavior well.1",
        "link": "http://dx.doi.org/10.1162/tacl_a_00573"
    },
    {
        "id": 20215,
        "title": "Multi-task Active Learning for Pre-trained Transformer-based Models",
        "authors": "Guy Rotman, Roi Reichart",
        "published": "2022-11-7",
        "citations": 1,
        "abstract": "Abstract\nMulti-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1",
        "link": "http://dx.doi.org/10.1162/tacl_a_00515"
    },
    {
        "id": 20216,
        "title": "Achievement Expedients of Fuzzy Queuing Models with an Unreliable Electrical Transformer",
        "authors": "R. Ramesh, M. Seenivasan",
        "published": "2022-2-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeict53079.2022.9768437"
    },
    {
        "id": 20217,
        "title": "Transformer models used for text-based question answering systems",
        "authors": "Khalid Nassiri, Moulay Akhloufi",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-04052-8"
    },
    {
        "id": 20218,
        "title": "How Relevant Are Selectional Preferences for Transformer-based Language Models?",
        "authors": "Eleni Metheniti, Tim Van de Cruys, Nabil Hathout",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.109"
    },
    {
        "id": 20219,
        "title": "Mr-Fosdick at SemEval-2023 Task 5: Comparing Dataset Expansion Techniques for Non-Transformer and Transformer Models: Improving Model Performance through Data Augmentation",
        "authors": "Christian Falkenberg, Erik Schönwälder, Tom Rietzke, Chris-Andris Görner, Robert Walther, Julius Gonsior, Anja Reusch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.11"
    },
    {
        "id": 20220,
        "title": "The Risk of Thermal Damage to the HTS Transformer”s Coils During the Inrush Current",
        "authors": "Grzegorz Komarzyniec",
        "published": "2018-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epmccs.2018.8596610"
    },
    {
        "id": 20221,
        "title": "HealthMavericks@MEDIQA-Chat 2023: Benchmarking different Transformer based models for Clinical Dialogue Summarization",
        "authors": "Kunal Suri, Saumajit Saha, Atul Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.50"
    },
    {
        "id": 20222,
        "title": "Zelda Rose: a tool for hassle-free training of transformer models",
        "authors": "Loïc Grobol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.6"
    },
    {
        "id": 20223,
        "title": "Piccolo: Exposing Complex Backdoors in NLP Transformer Models",
        "authors": "Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, Xiangyu Zhang",
        "published": "2022-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sp46214.2022.9833579"
    },
    {
        "id": 20224,
        "title": "Analyzing Human and ChatGPT Responses: A Comparative Study of Transformer Models in Natural Language Processing",
        "authors": "Tunahan Gökçimen, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391042"
    },
    {
        "id": 20225,
        "title": "Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition",
        "authors": "Hamid Ahmadabadi, Omid Nejati Manzari, Ahmad Ayatollahi",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke60553.2023.10326272"
    },
    {
        "id": 20226,
        "title": "Comparative Study of Bert Models and Roberta in Transformer based Question Answering",
        "authors": "N Akhila, Sanjanasri J. P, Soman K. P",
        "published": "2023-6-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205622"
    },
    {
        "id": 20227,
        "title": "Ensemble Learning based on CNN and Transformer Models for Leaf Diseases Classification",
        "authors": "Li-Hua Li, Radius Tanone",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imcom60618.2024.10418393"
    },
    {
        "id": 20228,
        "title": "On Isotropy Calibration of Transformer Models",
        "authors": "Yue Ding, Karolis Martinkus, Damian Pascual, Simon Clematide, Roger Wattenhofer",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.insights-1.1"
    },
    {
        "id": 20229,
        "title": "Author Correction: Multiclass malaria parasite recognition based on transformer models and a generative adversarial network",
        "authors": "Dianhuan Tan, Xianghui Liang",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41598-023-45535-z"
    },
    {
        "id": 20230,
        "title": "Software requirement specific entity extraction using transformer models",
        "authors": "Garima Malik, Mucahit Cevik, Swayami Bera, Savas Yildirim, Devang Parikh, Ayse Basar",
        "published": "2022-5-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.9e433d7c"
    },
    {
        "id": 20231,
        "title": "Comparison of Analytical Transformer Leakage Inductance Models: Accuracy vs. Computational Effort",
        "authors": "Richard Schlesinger, Jurgen Biela",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/epe.2019.8915455"
    },
    {
        "id": 20232,
        "title": "INFLUENCE OF LEARNING RATE PARAMETER IN DIFFERENT TRANSFORMER MODELS IN TEXT CLASSIFICATION TASK",
        "authors": "B. Pavlyshenko, M. Stasiuk",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.30970/eli.21.4"
    },
    {
        "id": 20233,
        "title": "Leveraging Transformer-based Language Models for Enhanced Service Insight in Tourism",
        "authors": "Aleyna Er, Şuayb Talha Özçelik, Meltem Turhan Yöndem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391041"
    },
    {
        "id": 20234,
        "title": "Comparative Analysis of Transformer Models on WikiHow Dataset",
        "authors": "Dev Jadeja, Avinash Khetri, Anubhav Mittal, Dinesh Kumar Vishwakarma",
        "published": "2022-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscds53736.2022.9761043"
    },
    {
        "id": 20235,
        "title": "3D Morphable Models as Spatial Transformer Networks",
        "authors": "Anil Bas, Patrik Huber, William A. P. Smith, Muhammad Awais, Josef Kittler",
        "published": "2017-10",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw.2017.110"
    },
    {
        "id": 20236,
        "title": "Efficient Image Captioning Based on Vision Transformer Models",
        "authors": "Samar Elbedwehy, T. Medhat, Taher Hamza, Mohammed F. Alrahmawy",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2022.029313"
    },
    {
        "id": 20237,
        "title": "Towards COVID-19 fake news detection using transformer-based models",
        "authors": "Jawaher Alghamdi, Yuqing Lin, Suhuai Luo",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110642"
    },
    {
        "id": 20238,
        "title": "Winding of Distribution Transformer in Substations Models",
        "authors": "Eyenubo O.J, Ebisne E.E, Okieke U.J",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14445/23497157/ijres-v10i6p101"
    },
    {
        "id": 20239,
        "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
        "authors": "Luke Gessler, Nathan Schneider",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.17"
    },
    {
        "id": 20240,
        "title": "The evolution of transformer models from unidirectional to  bidirectional in Natural Language Processing",
        "authors": "Yihang Sun",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Transformer models have revolutionized Natural Language Processing (NLP), transitioning from traditional sequential models to innovative architectures based on attention mechanisms. The shift from unidirectional to bidirectional models has been a remarkable development in NLP. This paper mainly focuses on the evolution of NLP caused by Transformer models, with the transition from unidirectional to bidirectional modeling. This paper explores how the transformer model has revolutionized NLP, and the evolution from traditional sequential models to innovative attention-driven architectures. In this paper, it mainly discusses the limitations of traditional NLP models like RNNs, LSTMs and CNN when handling lengthy text sequences and complex dependencies, highlighting how transformer models, employing self-attention mechanisms and bidirectional modeling (e.g., BERT and GPT), have significantly improved NLP tasks. It provides a thorough review of the shift from unidirectional to bidirectional transformer models, offering insights into their utilization and development. Finally, this paper concludes with a summary and outlook for the entire study.",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230794"
    },
    {
        "id": 20241,
        "title": "Parameter-Efficient Tuning of Transformer Models for Anglicism Detection and Substitution in Russian",
        "authors": "Daniil Lukichev, Darya Kryanina, Anastasia Bystrova, Alena Fenogenova, Maria Tikhonova",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "This article is devoted to the problem of Anglicisms in texts in Russian: the tasks of detection and automatic rewriting of the text with the substitution of Anglicisms by their Russian-language equivalents. Within the framework of the study, we present a parallel corpus of Anglicisms and models that identify Anglicisms in the text and replace them with the Russian equivalent, preserving the stylistics of the original text.",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-295-306"
    },
    {
        "id": 20242,
        "title": "How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?",
        "authors": "Laura Zanella, Yannick Toussaint",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.12"
    },
    {
        "id": 20243,
        "title": "Failure Analysis Using Different Transformer Models in EMTP-ATP",
        "authors": "Dusan Medved, Martin Kanalik",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cando-epe47959.2019.9110993"
    },
    {
        "id": 20244,
        "title": "Log Anomaly Detection Method based on Hybrid Transformer-BiLSTM Models",
        "authors": "Xuedong Ou, Jing Liu",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qrs-c57518.2022.00123"
    },
    {
        "id": 20245,
        "title": "Developmental Negation Processing in Transformer Language Models",
        "authors": "Antonio Laverghetta Jr., John Licato",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-short.60"
    },
    {
        "id": 20246,
        "title": "Measurements for validation of manufacturer’s white-box transformer models",
        "authors": "Bjørn Gustavsen, Alvaro Portillo, Rodrigo Ronchi, Asgeir Mjelve",
        "published": "2017",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.711"
    },
    {
        "id": 20247,
        "title": "Validation of Gaussian mixture LV load models based on MV/LV transformer stations measurements",
        "authors": "A. Ishchenko, S.S. Nibhanupudi, C. Willemsen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.1084"
    },
    {
        "id": 20248,
        "title": "Comparative Analysis of Pretrained Encoder-Decoder Transformer Models for Extreme Text Summarization",
        "authors": "Tamma RajyaLakshmi, K.S. Kuppusamy",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacic59454.2023.10435363"
    },
    {
        "id": 20249,
        "title": "CALCULATION OF CFD-THERMAL MODELS OF OIL-COOLED TRANSFORMER EQUIPMENT",
        "authors": "V. F. Ivankov, A. V. Basova",
        "published": "2017-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15588/1607-6761-2016-2-3"
    },
    {
        "id": 20250,
        "title": "Comparison of large-scale pre-trained models based ViT, swin transformer and ConvNeXt",
        "authors": "Jiapeng Yu",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2671201"
    },
    {
        "id": 20251,
        "title": "Application and Analysis of Convolutional Neural Networks and Vision Transformer Models in Fruit Recognition",
        "authors": "Shouzhe Liu",
        "published": "2023-1-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpeca56706.2023.10076218"
    },
    {
        "id": 20252,
        "title": "Transformer-Based Conditional Language Models to Generate Filipino News Articles",
        "authors": "Kenrick Lance T. Buñag, Rosanna A. Esquivel",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46254/an13.20230595"
    },
    {
        "id": 20253,
        "title": "KS@LTH at SemEval-2020 Task 12: Fine-tuning Multi- and Monolingual Transformer Models for Offensive Language Detection",
        "authors": "Kasper Socha",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.semeval-1.270"
    },
    {
        "id": 20254,
        "title": "Prediction of SARS-CoV-2 spike protein mutations using Sequence-to-Sequence and Transformer models",
        "authors": "Hamed Ahmadi, Vahid Nikoofard, Hossein Nikoofard, Rouhollah Abdolvahab, Narges Nikoofard, Mahdi Esmaeilzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn the study of viral epidemics, having information about the structural evolution of the virus can be very helpful in controlling the disease and making vaccines. Various deep learning and natural language processing techniques (NLP) can be used to analyze genetic structure of viruses, namely to predict their mutations. In this paper, by using Sequence-to-Sequence (Seq2Seq) model with Long Short-Term Memory (LSTM) cell and Transformer model with the attention mechanism, we investigate the spike protein mutations of SARS-CoV-2 virus. We make time-series datasets of the spike protein sequences of this virus and generate upcoming spike protein sequences. We also determine the mutations of the generated spike protein sequences, by comparing these sequences with the Wuhan spike protein sequence. We train the models to make predictions in December 2021, February 2022, and October 2022. Furthermore, we find that some of our generated spike protein sequences have been reported in December 2021 and February 2022, which belong to Delta and Omicron variants. The results obtained in the present study could be useful for prediction of future mutations of SARS-CoV-2 and other viruses.",
        "link": "http://dx.doi.org/10.1101/2023.01.23.525130"
    },
    {
        "id": 20255,
        "title": "Transformer-based deep learning models for the sentiment analysis of social media data",
        "authors": "Sayyida Tabinda Kokab, Sohail Asghar, Shehneela Naz",
        "published": "2022-7",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.array.2022.100157"
    },
    {
        "id": 20256,
        "title": "Detecting Tweets Containing Cannabidiol-Related COVID-19 Misinformation Using Transformer Language Models and FDA Warning Letters (Preprint)",
        "authors": "Jason Turner, Mehmed Kantardzic, Rachel Vickers-Smith, Andrew Brown",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nThe COVID-19 pandemic introduced yet another medical condition for online sellers of loosely regulated substances such as cannabidiol (CBD) to falsely promote sales. As a result, it has become necessary to innovate ways to identify such instances of misinformation.\n\n\nOBJECTIVE\nWe used transformer-based language models to identify COVID-19 misinformation as it relates to the sales and/or promotion of CBD, by finding tweets that are semantically similar to quotes taken from known instances of misinformation, specifically the publicly available FDA warning letters.\n\n\nMETHODS\nWe collected tweets using CBD and COVID-19 related terms. Using a previously trained model, we extracted the tweets indicating commercialization/sales of CBD, and annotated those containing COVID-19 misinformation, according to the FDA’s definitions. We encoded the collection of tweets and misinformation quotes into sentence vectors, and then calculated the cosine similarity between each quote and each tweet, so that a threshold could be established to identify tweets that are making false claims regarding CBD and COVID-19, while minimizing the instance of false-positives.\n\n\nRESULTS\nWe demonstrated that by using quotes taken from FDA warning letters of known offenses we can identify semantically similar tweets that also contain similar misinformation. By identifying a cosine distance threshold between the sentence vector of the warning letters and the sentence vector of the tweets, we can identify tweets that contain similar forms of misinformation.\n\n\nCONCLUSIONS\nOur framework shows that commercial CBD/COVID-19 misinformation can potentially be identified and consequently curbed by using transformer-based language models and known prior instances of misinformation. Our approach functions without need for labeled data, potentially reducing the time in which misinformation could be identified. Our proposed framework shows promise in being easily adapted to identify other forms of misinformation related to loosely regulated substances, such as that related to autism, dementia, and Alzheimer’s disease.\n",
        "link": "http://dx.doi.org/10.2196/preprints.38390"
    },
    {
        "id": 20257,
        "title": "ASRtrans at SemEval-2022 Task 5: Transformer-based Models for Meme Classification",
        "authors": "Ailneni Rakshitha Rao, Arjun Rao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.82"
    },
    {
        "id": 20258,
        "title": "ANLP-RG at NADI 2023 shared task: Machine Translation of Arabic Dialects: A Comparative Study of Transformer Models",
        "authors": "Wiem Derouich, Sameh Kchaou, Rahma Boujelbane",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.75"
    },
    {
        "id": 20259,
        "title": "ASRtrans at SemEval-2022 Task 4: Ensemble of Tuned Transformer-based Models for PCL Detection",
        "authors": "Ailneni Rakshitha Rao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.44"
    },
    {
        "id": 20260,
        "title": "FlowFormers: Transformer-based Models for Real-time Network Flow Classification",
        "authors": "Rushi Babaria, Sharat Chandra Madanapalli, Himal Kumar, Vijay Sivaraman",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/msn53354.2021.00046"
    },
    {
        "id": 20261,
        "title": "ANALYSIS OF DIFFERENT MODELS OF MOA SURGE ARRESTER FOR THE TRANSFORMER PROTECTION",
        "authors": "Ahmed Abugalia, Mohamed Shaglouf",
        "published": "2018-1-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26480/amm.02.2018.19.21"
    },
    {
        "id": 20262,
        "title": "Explainability to Business: Demystify Transformer Models with Attention-based Explanations",
        "authors": "Rajasekhar Thiruthuvaraj, Ashly Ann Jo, Ebin Deni Raj",
        "published": "2023-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaaic56838.2023.10141005"
    },
    {
        "id": 20263,
        "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks",
        "authors": "Sunit Bhattacharya, Ondřej Bojar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.9"
    },
    {
        "id": 20264,
        "title": "Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models",
        "authors": "Alejo Lopez-Avila, Víctor Suárez-Paniagua",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.124"
    },
    {
        "id": 20265,
        "title": "Assessing Audio-Based Transformer Models for Speech Emotion Recognition",
        "authors": "Ulku Bayraktar, Hasan Kilimci, H.Hakan Kilinc, Zeynep Hilal Kilimci",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isas60782.2023.10391313"
    },
    {
        "id": 20266,
        "title": "Post-hoc analysis of Arabic transformer models",
        "authors": "Ahmed Abdelali, Nadir Durrani, Fahim Dalvi, Hassan Sajjad",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.blackboxnlp-1.8"
    },
    {
        "id": 20267,
        "title": "Transformer and Knowledge Based Siamese Models for Medical Document Retrieval",
        "authors": "Aprameya Dash, Alimurtaza Mustafa Merchant, Suyash Chintawar, Sowmya Kamath S.",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globconet56651.2023.10150081"
    },
    {
        "id": 20268,
        "title": "Transformer-Based Deep Learning Models for Well Log Processing and Quality Control by Modelling Global Dependence of the Complex Sequences",
        "authors": "Ashutosh Kumar",
        "published": "2021-12-9",
        "citations": 1,
        "abstract": "AbstractA single well from any mature field produces approximately 1.7 million Measurement While Drilling (MWD) data points. We either use cross-correlation and covariance measurement, or Long Short-Term Memory (LSTM) based Deep Learning algorithms to diagnose long sequences of extremely noisy data. LSTM's context size of 200 tokens barely accounts for the entire depth. Proposed work develops application of Transformer-based Deep Learning algorithm to diagnose and predict events in complex sequences of well-log data.Sequential models learn geological patterns and petrophysical trends to detect events across depths of well-log data. However, vanishing gradients, exploding gradients and the limits of convolutional filters, limit the diagnosis of ultra-deep wells in complex subsurface information. Vast number of operations required to detect events between two subsurface points at large separation limits them. Transformers-based Models (TbMs) rely on non-sequential modelling that uses self-attention to relate information from different positions in the sequence of well-log, allowing to create an end-to-end, non-sequential, parallel memory network. We use approximately 21 million data points from 21 wells of Volve for the experiment.LSTMs, in addition to auto-regression (AR), autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) conventionally models the events in the time-series well-logs. However, complex global dependencies to detect events in heterogeneous subsurface are challenging for these sequence models. In the presented work we begin with one meter depth of data from Volve, an oil-field in the North Sea, and then proceed up to 1000 meters. Initially LSTMs and ARIMA models were acceptable, as depth increased beyond a few 100 meters their diagnosis started underperforming and a new methodology was required. TbMs have already outperformed several models in large sequences modelling for natural language processing tasks, thus they are very promising to model well-log data with very large depth separation. We scale features and labels according to the maximum and minimum value present in the training dataset and then use the sliding window to get training and evaluation data pairs from well-logs. Additional subsurface features were able to encode some information in the conventional sequential models, but the result did not compare significantly with the TbMs. TbMs achieved Root Mean Square Error of 0.27 on scale of (0-1) while diagnosing the depth up to 5000 meters.This is the first paper to show successful application of Transformer-based deep learning models for well-log diagnosis. Presented model uses a self-attention mechanism to learn complex dependencies and non-linear events from the well-log data. Moreover, the experimental setting discussed in the paper will act as a generalized framework for data from ultra-deep wells and their extremely heterogeneous subsurface environment.",
        "link": "http://dx.doi.org/10.2118/208109-ms"
    },
    {
        "id": 20269,
        "title": "Probabilistic Well Production Forecasting in Volve Field Using Temporal Fusion Transformer Deep Learning Models",
        "authors": "Zainab Al-Ali Hussain Al-Ali, Roland Horne",
        "published": "2023-3-13",
        "citations": 3,
        "abstract": "AbstractAccurate well rate forecasting is critical for field development. In the oil industry, recurrent-based deep learning models have been used for production forecasting. Modern research is shifting towards using the novel transformer architecture in natural language and time series applications to better handle long-term temporal dependencies using attention layers. In this paper, we present a novel approach of applying a transformer-based deep learning model for probabilistic production forecasting.The Temporal Fusion Transformer (TFT) model, a modern transformer-based model for time series forecasting, was used to provide a better oil rate prediction in the Norwegian Volve field. The historical bottomhole pressure, wellhead pressure, wellhead temperature and choke size opening real time series data were used as past input features in the TFT model to forecast ahead the oil rate of two wells. The model was trained with a quantile loss function to produce a probabilistic prediction with both upper and lower bounds of uncertainties. After optimization the TFT model was blindly tested on the last 20% of the data to evaluate its prediction performance compared Block Recurrent Neural Network architectures (BlockRNN).The real-time production data used in this multivariate forecasting problem was found to be complex with no clear trend or seasonality. The BlockRNN model failed to produce a good prediction of oil rate compared to the TFT model. The TFT model was better at encoding the input features including, the choke size, wellhead pressure, and temperature, and understanding their long-range dependencies with the oil rate.The model was able to minimize the testing Mean Squared Error (MSE) for the two wells F-11H and F-12H reaching values of 0.08 and 0.03, respectively. In addition, the model was able to forecast a prediction bandwidth in between the 10th and 90th quantiles to account for uncertainty ranges which laid in-between the blind test intervals. Overall, the TFT model was proven successful in accurately forecasting complex trends of oil rates overcoming the limitations of conventional, BlockRNN uses a memory to understand and make predictions about new data models of information loss over long-term multivariate time series prediction.Our work presents a novel approach of using the TFT probabilistic deep learning model for multivariate oil rate forecasting in the oil and gas industry. The model showed very promising results outperforming conventional BlockRNN-based models in addition to providing a range of the forecasting uncertainty using quantile regression. Knowing the uncertainty range helps in making critical decision particularly in the well intervention and field development.",
        "link": "http://dx.doi.org/10.2118/214133-ms"
    },
    {
        "id": 20270,
        "title": "Attacking a Transformer-Based Models for Arabic Language as Low Resources Language (LRL) Using Word-Substitution Methods",
        "authors": "Hanin Alshalan, Banafsheh Rekabdar",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00025"
    },
    {
        "id": 20271,
        "title": "Photo-based Carbohydrates Counting using Pre-trained Transformer Models",
        "authors": "Ivan Contreras, Marti Guso, Aleix Beneyto, Josep Vehi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.445"
    },
    {
        "id": 20272,
        "title": "Survey on Automatic Text Summarization and Transformer Models Applicability",
        "authors": "Wang Guan, Ivan Smetannikov, Man Tianxing",
        "published": "2020-10-27",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3437802.3437832"
    },
    {
        "id": 20273,
        "title": "Domain-specific transformer models for query translation",
        "authors": "Mandar Kulkarni, Nikesh Garera, Anusua Trivedi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-industry.10"
    },
    {
        "id": 20274,
        "title": "Performance Comparison of Vision Transformer-Based Models in Medical Image Classification",
        "authors": "Elif Kanca, Selen Ayas, Elif Baykal Kablan, Murat Ekinci",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223892"
    },
    {
        "id": 20275,
        "title": "Power Transformer Oil-paper Insulation Breakdown Tests in Full Scale Models",
        "authors": "Ying Li, Guijuan Wang, Wei Hu, Xian Yang, Dan Zhou",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec48194.2019.8974926"
    },
    {
        "id": 20276,
        "title": "Creating and Comparing Dictionary, Word Embedding, and Transformer-based Models to Measure Discrete Emotions in German Political Text",
        "authors": "Tobias Widmann, Maximilian Wich",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4127133"
    },
    {
        "id": 20277,
        "title": "Comparative Analysis of ESG Indicators in Enhancing Debt Default Prediction with Transformer-derived Attention Models",
        "authors": "Zilin Zheng",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "This study explores the application of Transformer-derived algorithms, namely TabNet and Tab Transformer, in predicting corporate debt defaults and compares their performance with classical machine learning models. Concurrently, it assesses the efficacy of incorporating ESG indicators into the corporate debt default risk assessment framework. The research focuses on Chinese A-share listed companies from January 2018 to June 2023, comprising 23 companies with observed defaults and a control group of 846 companies with regular debt maturities. The findings indicate that although attention-based models like TabNet and Tab Transformer provide enhanced interpretability, their performance does not significantly surpass ensemble algorithms such as XGBoost. Attention-based models emphasize the importance of merging the advantages of deep learning with the interpretability of traditional algorithms, especially when dealing with vast, high-dimensional datasets. Additionally, the incorporation of ESG data did not yield a significant improvement in prediction outcomes. Potential reasons for the limited impact of ESG indicators on predictions, including data quality and the comprehensiveness of existing financial disclosures, are discussed. Given the limited sample size and constraints related to test data, future research directions suggest expanding the dataset and diversifying ESG data sources.",
        "link": "http://dx.doi.org/10.54097/bd49q651"
    },
    {
        "id": 20278,
        "title": "On Robustness of Finetuned Transformer-based NLP Models",
        "authors": "Pavan Kalyan Reddy Neerudu, Subba Oota, Mounika Marreddy, Venkateswara Kagita, Manish Gupta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.477"
    },
    {
        "id": 20279,
        "title": "JUSTers at SemEval-2020 Task 4: Evaluating Transformer Models against Commonsense Validation and Explanation",
        "authors": "Ali Fadel, Mahmoud Al-Ayyoub, Erik Cambria",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.semeval-1.66"
    },
    {
        "id": 20280,
        "title": "Sparsifying Transformer Models with Trainable Representation Pooling",
        "authors": "Michał Pietruszka, Łukasz Borchmann, Łukasz Garncarek",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-long.590"
    },
    {
        "id": 20281,
        "title": "What All Do Audio Transformer Models Hear? Probing Acoustic Representations for Language Delivery and Its Structure",
        "authors": "Yaman Kumar, Jui Shah, Rajiv Ratn Shah, Changyou Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
        "link": "http://dx.doi.org/10.20944/preprints202101.0081.v1"
    },
    {
        "id": 20282,
        "title": "Poirot at CMCL 2022 Shared Task: Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models",
        "authors": "Harshvardhan Srivastava",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.cmcl-1.11"
    },
    {
        "id": 20283,
        "title": "USING AN ENSEMBLE OF TRANSFORMER-BASED MODELS FOR AUTOMATED WRITING EVALUATION OF ESSAYS",
        "authors": "Alexandru Stanciu, Irina Cristescu, Ella Magdalena Ciuperca, Carmen Elena Cîrnu",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/edulearn.2022.1247"
    },
    {
        "id": 20284,
        "title": "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models",
        "authors": "Anlin Qu, Jianwei Niu, Shasha Mo",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.237"
    },
    {
        "id": 20285,
        "title": "Strong Lens Detection 2.0: Machine Learning and Transformer Models",
        "authors": "Hareesh Thuruthipilly",
        "published": "2022-12",
        "citations": 0,
        "abstract": "AbstractUpcoming large-scale surveys like LSST are expected to uncover approximately 105 strong gravitational lenses within massive datasets. Traditional manual techniques are too time-consuming and impractical for such volumes of data. Consequently, machine learning methods have emerged as an alternative. In our prior work (Thuruthipilly et al. 2022), we introduced a self-attention-based machine learning model (transformers) for detecting strong gravitational lenses in simulated data from the Bologna Lens Challenge. These models offer advantages over simpler convolutional neural networks (CNNs) and competitive performance compared to state-of-the-art CNN models. We applied this model to the datasets from Bologna Lens Challenge 1 and 2 and simulated data on Euclid.",
        "link": "http://dx.doi.org/10.1017/s1743921323003897"
    },
    {
        "id": 20286,
        "title": "Evaluation of Computational Models for Electromagnetic Force Calculation in Transformer Windings",
        "authors": "Arthur Francisco Andrade, Edson  Guedes Costa, João Pedro  Costa Souza, Filipe  Lucena Medeiros Andrade, Jalberth  Fernandes Araujo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4446775"
    },
    {
        "id": 20287,
        "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition",
        "authors": "Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko Takashima, Takafumi Moriya, Takanori Ashihara, Shota Orihashi, Naoki Makishima",
        "published": "2021-8-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1992"
    },
    {
        "id": 20288,
        "title": "Can Transformer Language Models Predict Psychometric Properties?",
        "authors": "Antonio Laverghetta Jr., Animesh Nighojkar, Jamshidbek Mirzakhalov, John Licato",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.starsem-1.2"
    },
    {
        "id": 20289,
        "title": "Compact Transformer-based Language Models for the Moroccan Darija",
        "authors": "Mohamed Aghzal, Mohamed Amine El Bouni, Saad Driouech, Asmaa Mourhir",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cist56084.2023.10409912"
    },
    {
        "id": 20290,
        "title": "Challenges of Self-Supervised Learning for Unified, Multi-Modal, Multi-Task Transformer Models",
        "authors": "Graham Annett, Tim Andersen, Robert Annett",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci58124.2022.00056"
    },
    {
        "id": 20291,
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
        "authors": "Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.99"
    },
    {
        "id": 20292,
        "title": "Optimizing Election Result Prediction Through Fine-Tuned Transformer Models",
        "authors": "Sajal Singhal, Gautam Pruthi, Ayush Kumar, Lakshay Kapoor, Vandana Bhatia",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nmitcon58196.2023.10276268"
    },
    {
        "id": 20293,
        "title": "Automatic Question Generation Monolingual Multilingual pre-trained Models using RNN and Transformer in Low Resource Indonesian Language",
        "authors": "Karissa Vincentio, Derwin Suhartono",
        "published": "2022-11-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31449/inf.v46i7.4236"
    },
    {
        "id": 20294,
        "title": "Improved Emotion Detection Framework for Arabic Text using Transformer Models *",
        "authors": "Hatem Noaman",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21608/aeta.2023.299595"
    },
    {
        "id": 20295,
        "title": "Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models (Preprint)",
        "authors": "Xi Yang, Xing He, Hansi Zhang, Yinghan Ma, Jiang Bian, Yonghui Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nSemantic textual similarity (STS) is one of the fundamental tasks in natural language processing (NLP). Many shared tasks and corpora for STS have been organized and curated in the general English domain; however, such resources are limited in the biomedical domain. In 2019, the National NLP Clinical Challenges (n2c2) challenge developed a comprehensive clinical STS dataset and organized a community effort to solicit state-of-the-art solutions for clinical STS.\n\n\nOBJECTIVE\nThis study presents our transformer-based clinical STS models developed during this challenge as well as new models we explored after the challenge. This project is part of the 2019 n2c2/Open Health NLP shared task on clinical STS.\n\n\nMETHODS\nIn this study, we explored 3 transformer-based models for clinical STS: Bidirectional Encoder Representations from Transformers (BERT), XLNet, and Robustly optimized BERT approach (RoBERTa). We examined transformer models pretrained using both general English text and clinical text. We also explored using a general English STS dataset as a supplementary corpus in addition to the clinical training set developed in this challenge. Furthermore, we investigated various ensemble methods to combine different transformer models.\n\n\nRESULTS\nOur best submission based on the XLNet model achieved the third-best performance (Pearson correlation of 0.8864) in this challenge. After the challenge, we further explored other transformer models and improved the performance to 0.9065 using a RoBERTa model, which outperformed the best-performing system developed in this challenge (Pearson correlation of 0.9010).\n\n\nCONCLUSIONS\nThis study demonstrated the efficiency of utilizing transformer-based models to measure semantic similarity for clinical text. Our models can be applied to clinical applications such as clinical text deduplication and summarization.\n",
        "link": "http://dx.doi.org/10.2196/preprints.19735"
    },
    {
        "id": 20296,
        "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
        "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.195"
    },
    {
        "id": 20297,
        "title": "IMG2InChI: Extracting Molecular Big Data from Chemical Images Using Transformer Models",
        "authors": "Zhenyu Wu, Zhenyuan Zhang, Zhiyang Ding, Joel J. P. C. Rodrigues",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436782"
    },
    {
        "id": 20298,
        "title": "Assessment of Hydraulic Network Models in Predicting Reverse Flows in OD Cooled Disc Type Transformer Windings",
        "authors": "Xiang Zhang, Zhongdong Wang",
        "published": "2019",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2943566"
    },
    {
        "id": 20299,
        "title": "The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction",
        "authors": "Dimitris Alikaniotis, Vipul Raheja",
        "published": "2019",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-4412"
    },
    {
        "id": 20300,
        "title": "SVIT: Hybrid Vision Transformer Models with Scattering Transform",
        "authors": "Tianming Qiu, Ming Gui, Cheng Yan, Ziqing Zhao, Hao Shen",
        "published": "2022-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp55214.2022.9943334"
    },
    {
        "id": 20301,
        "title": "Abusive Content Detection in Arabic Tweets Using Multi-Task Learning and Transformer-Based Models",
        "authors": "Bedour Alrashidi, Amani Jamal, Ali Alkhathlan",
        "published": "2023-5-9",
        "citations": 1,
        "abstract": "Different social media platforms have become increasingly popular in the Arab world in recent years. The increasing use of social media, however, has also led to the emergence of a new challenge in the form of abusive content, including hate speech, offensive language, and abusive language. Existing research work focuses on automatic abusive content detection as a binary classification problem. In addition, the existing research work on the automatic detection task surrounding abusive Arabic content fails to tackle the dialect-specific phenomenon. Consequently, this has led to two important issues in the automatic abusive Arabic content detection task. In this study, we used a multi-aspect annotation schema to tackle the automatic abusive content detection problem in Arabic countries, based on the multi-class classification task and the dialectal Arabic (DA)-specific phenomenon. More precisely, the multi-aspect annotation schema includes five attributes: directness, hostility, target, group, and annotator. We specifically developed a framework to automatically detecting abusive content on Twitter using natural language processing (NLP) techniques. The developed framework used different models of machine learning (ML), deep learning (DL), and pretrained Arabic language models (LMs) using the multi-aspect annotation dataset. In addition, to investigate the impact of the other approaches, such as multi-task learning (MTL), we developed four MTL models built on top of a pretrained DA language model (called MARBERT) and trained on the multi-aspect annotation dataset. Our MTL models and pretrained Arabic LMs enhanced the performance compared to the existing DL model mentioned in the literature.",
        "link": "http://dx.doi.org/10.3390/app13105825"
    },
    {
        "id": 20302,
        "title": "A Comparison of Transformer and LSTM Encoder Decoder Models for ASR",
        "authors": "Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schluter, Hermann Ney",
        "published": "2019-12",
        "citations": 75,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9004025"
    },
    {
        "id": 20303,
        "title": "SVIT: Hybrid Vision Transformer Models with Scattering Transform",
        "authors": "Tianming Qiu, Ming Gui, Cheng Yan, Ziqing Zhao, Hao Shen",
        "published": "2022-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp55214.2022.9943334"
    },
    {
        "id": 20304,
        "title": "Constraint-Based Neural Question Generation Using Sequence-to-Sequence and Transformer Models for Privacy Policy Documents",
        "authors": "Deepti Lamba, William H. Hsu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijke.2021.7.2.135"
    },
    {
        "id": 20305,
        "title": "PUM at SemEval-2020 Task 12: Aggregation of Transformer-based Models’ Features for Offensive Language Recognition",
        "authors": "Piotr Janiszewski, Mateusz Skiba, Urszula Walińska",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.semeval-1.210"
    },
    {
        "id": 20306,
        "title": "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU",
        "authors": "Patrick Kahardipraja, Brielen Madureira, David Schlangen",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.90"
    },
    {
        "id": 20307,
        "title": "Roles and Utilization of Attention Heads in Transformer-based Neural Language Models",
        "authors": "Jae-young Jo, Sung-Hyon Myaeng",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.311"
    },
    {
        "id": 20308,
        "title": "Cryptocurrency Price Prediction with LSTM and Transformer Models Leveraging Momentum and Volatility Technical Indicators",
        "authors": "Siddharth Penmetsa, Maruthi Vemula",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsca59871.2023.10393319"
    },
    {
        "id": 20309,
        "title": "Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction",
        "authors": "Christoph Alt, Marc Hübner, Leonhard Hennig",
        "published": "2019",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1134"
    },
    {
        "id": 20310,
        "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior",
        "authors": "Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, Dan Roth",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.64"
    },
    {
        "id": 20311,
        "title": "Punctuation Restoration using Transformer Models for High-and Low-Resource Languages",
        "authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
        "published": "2020",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.wnut-1.18"
    },
    {
        "id": 20312,
        "title": "Multi-parametric sensitivity analysis of improved transformer thermal models considering nonlinear effect of oil time constant",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.02430"
    },
    {
        "id": 20313,
        "title": "Multiple Linear Regression Models for Partial Discharge Location Along Transformer Windings",
        "authors": "Arismar Morais Gonçalves Júnior, Hélder de Paula, Wallace do Couto Boaventura",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17648/sbai-2019-111184"
    },
    {
        "id": 20314,
        "title": "Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation",
        "authors": "Kangwook Jang, Sungnyun Kim, Se-Young Yun, Hoirin Kim",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1329"
    },
    {
        "id": 20315,
        "title": "Effectiveness of masked autoencoder in vision transformer models for image classification",
        "authors": "Zhicheng Li, Yingze Liu, Xinran Wang",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "This paper uses the literature reading method to systematically sort out and introduce the basic principles of the three algorithms of the transformer model and their application in the field of image classification, which has high theoretical value and social value, and has strong reference for the development of the transformer model in the future. ViT is simply an innovation in computer vision based on the transformer model. It first separates an image into several local patches (16x16), and then maps each one to a feature vector. These vectors will be delivered to an encoder for polishing. Finally, a special token is appended to these vectors for integrating location information. The final prediction is based on these tokesn. Swin-T is a new Transformer architecture, which is proposed by Microsoft Research to improve the performance of computer vision tasks. It adopts a new windowed feature extraction strategy, which can maintain high accuracy while significantly reducing the amount of computation and memory consumption. It has achieved leading performance in multiple computer vision tasks, becoming one of the most advanced visual Transformer models. In computer vision image classification, the information is highly redundant, the lack of an image piece, may not make the model produce much confusion, the model can be inferred from the surrounding pixel information, masked autoencoder (MAE) is to mask a high proportion of image pieces, create a difficult learning task, the method is simple but extremely effective.",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231077"
    },
    {
        "id": 20316,
        "title": "Performance Evaluation of Transformer-based NLP Models on Fake News Detection Datasets",
        "authors": "Raveen Narendra Babu, Chung-Horng Lung, Marzia Zaman",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00049"
    },
    {
        "id": 20317,
        "title": "Classical Machine Learning and Transformer Models for Offensive and Abusive Language Classification on Dziri Language",
        "authors": "Mohammed Mehdi Bouchene, Kheireddine Abainia",
        "published": "2023-9-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasa59624.2023.10286654"
    },
    {
        "id": 20318,
        "title": "Transformer-based Models for Enhanced Amur Tiger Re-Identification",
        "authors": "Xufeng Bai, Tasmina Islam, M A Hannan Bin Azhar",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sami60510.2024.10432893"
    },
    {
        "id": 20319,
        "title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models",
        "authors": "Laura Pérez-Mayos, Alba Táboas García, Simon Mille, Leo Wanner",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.333"
    },
    {
        "id": 20320,
        "title": "Using of Transformer-Based Language Models to Separate Traffic Packets of Different Protocols",
        "authors": "Zalina Rusinova, Yury Chernyshov",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/redundancy59964.2023.10330184"
    },
    {
        "id": 20321,
        "title": "Improved Hybrid Streaming ASR with Transformer Language Models",
        "authors": "Pau Baquero-Arnal, Javier Jorge, Adrià Giménez, Joan Albert Silvestre-Cerdà, Javier Iranzo-Sánchez, Albert Sanchis, Jorge Civera, Alfons Juan",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2770"
    },
    {
        "id": 20322,
        "title": "Transformer Language Models with LSTM-Based Cross-Utterance Information Representation",
        "authors": "G. Sun, C. Zhang, P. C. Woodland",
        "published": "2021-6-6",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9414477"
    },
    {
        "id": 20323,
        "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models",
        "authors": "Rakesh Chada, Pradeep Natarajan, Darshan Fofadiya, Prathap Ramachandra",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.44"
    },
    {
        "id": 20324,
        "title": "Comparison of ResNet-50 and vision transformer models for trash classification",
        "authors": "Jiongli Liu, Jiayou Sun, Xuan Zhou",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2671208"
    },
    {
        "id": 20325,
        "title": "Impacts of Transformer-Based Language Models and Imbalanced Data for Hate Speech Detection on Vietnamese Social Media Texts",
        "authors": "Son T. Luu, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nHate speech detection on social media networks is the classification task that automatically detects harmful comments from users and prevents the appearance of those toxic comments on social sites. The profit of the hate speech detection task is preventing harassment and toxicity content on the social networks site to protect the users that join the social media networks. However, hate speech detection is challenging, especially for low-resource languages like Vietnamese. In this paper, we investigate the effect of the transformer-based language model and data augmentation techniques for hate speech detection on Vietnamese texts. Then, we proposed our ensemble model to boost the accuracy of hate speech detection in Vietnamese texts.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2242843/v1"
    },
    {
        "id": 20326,
        "title": "Real-Time Social Media Analytics with Deep Transformer Language Models: A Big Data Approach",
        "authors": "Ahmed Ahmet, Tariq Abdullah",
        "published": "2020-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdatase50710.2020.00014"
    },
    {
        "id": 20327,
        "title": "On finetuning Adapter-based Transformer models for classifying Abusive Social Media Tamil Comments",
        "authors": "Malliga Subramanian, Kogilavani Shanmugavadivel, Nandhini Subbarayan, Adhithiya Ganesan, Deepti Ravi, Vasanth Palanikumar, Bharathi Raja Chakravarthi",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nSpeaking or expressing oneself in an abusive manner is a form of verbal abuse that targets individuals or groups on the basis of their membership in a particular social group, which is differentiated by traits such as culture, gender, sexual orientation, religious affiliation etc. In today's world, the dissemination of evil and depraved content on social media has increased exponentially. Abusive language on the internet has been linked to an increase in violence against minorities around the world, including mass shootings, murders, and ethnic cleansing. People who use social media in places where English is not the main language often use a code-mixed form of text. This makes it harder to find abusive texts, and when combined with the fact that there aren't many resources for languages like Tamil, the task becomes significantly challenging. This work makes use of abusive Tamil language comments released by the workshop “Tamil DravidianLangTech@ACL 2022” and develops adapter-based multilingual transformer models namely Muril, XLMRoBERTa and mBERT to classify the abusive comments. These transformers have been utilized as fine-tuners and adapters. This study shows that in low-resource languages like Tamil, adapter-based strategies work better than fine-tuned models. In addition, we use Optuna, a hyperparameter optimization framework to find the ideal values of the hyper-parameters that lead to better classification. Of all the proposed models, MuRIL (Large) gives 74.7%, which is comparatively better than other models proposed for the same dataset.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2601766/v1"
    },
    {
        "id": 20328,
        "title": "Power Transformer FRA Studies Using Ladder, Multi-Conductor Transmission Lines (MTL), Circular MTL, and Hybrid Winding Models",
        "authors": "Manoj Samal, Mithun Mondal",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126461"
    },
    {
        "id": 20329,
        "title": "Answer-Agnostic Question Generation in Privacy Policy Domain using Sequence-to-Sequence and Transformer Models",
        "authors": "Deepti Lamba, William H. Hsu",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cecit53797.2021.00052"
    },
    {
        "id": 20330,
        "title": "Disaster Image Classification Using Pre-trained Transformer and Contrastive Learning Models",
        "authors": "Soudabeh Taghian Dinani, Doina Caragea",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302517"
    },
    {
        "id": 20331,
        "title": "Transformer-based models for predictive simulations of vortex-induced vibrations",
        "authors": "Jacques Honigbaum, Fernando Rochinha, Rodolfo Freitas, Souleymane Zio, Gabriel Mario Guerra Bernadá",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26678/abcm.cobem2023.cob2023-1881"
    },
    {
        "id": 20332,
        "title": "Vision transformer models to measure solar irradiance using sky images in temperate climates",
        "authors": "Thomas M. Mercier, Amin Sabet, Tasmiat Rahman",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2024.122967"
    },
    {
        "id": 20333,
        "title": "Probing for Bridging Inference in Transformer Language Models",
        "authors": "Onkar Pandit, Yufang Hou",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.327"
    },
    {
        "id": 20334,
        "title": "Scope and Challenges in Conversational AI using Transformer Models",
        "authors": " Arighna Chakraborty,  Asoke Nath",
        "published": "2021-12-26",
        "citations": 0,
        "abstract": "Conversational AI is an interesting problem in the field of Natural Language Processing and combines natural language processing with machine learning. There has been quite a lot of advancements in this field with each new model architecture capable of processing more data, better optimisation and execution, handling more parameters and having higher accuracy and efficiency. This paper discusses various trends and advancements in the field of natural language processing and conversational AI like RNNs and RNN based architectures such as LSTMs, Sequence to Sequence models, and finally, the Transformer networks, the latest in NLP and conversational AI. The authors have given a comparison between the various models discussed in terms of efficiency/accuracy and also discussed the scope and challenges in Transformer models.",
        "link": "http://dx.doi.org/10.32628/cseit217696"
    },
    {
        "id": 20335,
        "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
        "authors": "Lina Conti, Guillaume Wisniewski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.641"
    },
    {
        "id": 20336,
        "title": "Dementia Detection using Transformer-Based Deep Learning and Natural Language Processing Models",
        "authors": "Ploypaphat Saltz, Shih Yin Lin, Sunny Chieh Cheng, Dong Si",
        "published": "2021-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi52183.2021.00094"
    },
    {
        "id": 20337,
        "title": "Comparison of Transformer Based and Traditional Models on Sentiment Analysis on Social Media Datasets",
        "authors": "Arif Ridho Lubis, Yulia Fatmi, Deden Witarsyah",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2ie60547.2023.10331232"
    },
    {
        "id": 20338,
        "title": "Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models",
        "authors": "Ryota Iijima, Miki Tanaka, Sayaka Shiota, Hitoshi Kiya",
        "published": "2023-10-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315690"
    },
    {
        "id": 20339,
        "title": "Leveraging Transformer Models in the Cyberbullying Text Classification System for the Low-resource Bengali Language",
        "authors": "Md. Nesarul Hoque, Md. Hanif Seddiqui",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441412"
    },
    {
        "id": 20340,
        "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
        "authors": "William Timkey, Marten van Schijndel",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.372"
    },
    {
        "id": 20341,
        "title": "On Exploring Attention-based Explanation for Transformer Models in Text Classification",
        "authors": "Shengzhong Liu, Franck Le, Supriyo Chakraborty, Tarek Abdelzaher",
        "published": "2021-12-15",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata52589.2021.9671639"
    },
    {
        "id": 20342,
        "title": "Transformer Load Forecasting Method Based on SARIMA, FBProphet and LSTM Models",
        "authors": "Dilanka P Wijesena, Kapila Bandara, Priyashantha Tennakoon, Bhagya Wickramasinghe",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciis58898.2023.10253519"
    },
    {
        "id": 20343,
        "title": "One Wug, Two Wug+s Transformer Inflection Models Hallucinate Affixes",
        "authors": "Farhan Samir, Miikka Silfverberg",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.computel-1.5"
    },
    {
        "id": 20344,
        "title": "Towards Intrusive Non-Destructive Online Ageing Detection of Transformer Oil Leveraging Bootsrapped Machine Learning Models",
        "authors": "Ugochukwu Elele, Issouf Fofana, Azam Nekahi, Kate McAulay, Arshad Arshad",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177331"
    },
    {
        "id": 20345,
        "title": "Transfer Learning Approach to Multilabel Biomedical Literature Classification using Transformer Models",
        "authors": "Pahalage Dona Thushari, Sakina Niazi, Shweta Meena",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126262"
    },
    {
        "id": 20346,
        "title": "Fine-tuning Strategies for Classifying Community-Engaged Research Studies Using Transformer-Based Models: Algorithm Development and Improvement Study",
        "authors": "Brian J Ferrell",
        "published": "2023-2-7",
        "citations": 2,
        "abstract": "\nBackground\nCommunity-engaged research (CEnR) involves institutions of higher education collaborating with organizations in their communities to exchange resources and knowledge to benefit a community’s well-being. While community engagement is a critical aspect of a university's mission, tracking and reporting CEnR metrics can be challenging, particularly in terms of external community relations and federally funded research programs. In this study, we aimed to develop a method for classifying CEnR studies that have been submitted to our university's institutional review board (IRB) to capture the level of community involvement in research studies. Tracking studies in which communities are “highly engaged” enables institutions to obtain a more comprehensive understanding of the prevalence of CEnR.\n\n\nObjective\nWe aimed to develop an updated experiment to classify CEnR and capture the distinct levels of involvement that a community partner has in the direction of a research study. To achieve this goal, we used a deep learning–based approach and evaluated the effectiveness of fine-tuning strategies on transformer-based models.\n\n\nMethods\nIn this study, we used fine-tuning techniques such as discriminative learning rates and freezing layers to train and test 135 slightly modified classification models based on 3 transformer-based architectures: BERT (Bidirectional Encoder Representations from Transformers), Bio+ClinicalBERT, and XLM-RoBERTa. For the discriminative learning rate technique, we applied different learning rates to different layers of the model, with the aim of providing higher learning rates to layers that are more specialized to the task at hand. For the freezing layers technique, we compared models with different levels of layer freezing, starting with all layers frozen and gradually unfreezing different layer groups. We evaluated the performance of the trained models using a holdout data set to assess their generalizability.\n\n\nResults\nOf the models evaluated, Bio+ClinicalBERT performed particularly well, achieving an accuracy of 73.08% and an F1-score of 62.94% on the holdout data set. All the models trained in this study outperformed our previous models by 10%-23% in terms of both F1-score and accuracy.\n\n\nConclusions\nOur findings suggest that transfer learning is a viable method for tracking CEnR studies and provide evidence that the use of fine-tuning strategies significantly improves transformer-based models. Our study also presents a tool for categorizing the type and volume of community engagement in research, which may be useful in addressing the challenges associated with reporting CEnR metrics.\n",
        "link": "http://dx.doi.org/10.2196/41137"
    },
    {
        "id": 20347,
        "title": "Comparison of Transformer-Based Models Trained in Turkish and Different Languages on Turkish Natural Language Processing Problems",
        "authors": "Burak Aytan, C. Okan Sakar",
        "published": "2022-5-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu55565.2022.9864818"
    },
    {
        "id": 20348,
        "title": "Hybrid Convolution-Transformer models for breast cancer classification using histopathological images",
        "authors": "Sif Eddine Boudjellal, Abdelwahhab Boudjelal, Naceur-Eddine Boukezzoula",
        "published": "2022-12-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ntic55069.2022.10100518"
    },
    {
        "id": 20349,
        "title": "Transformer-based Models for Supervised Monocular Depth Estimation",
        "authors": "Arijit Gupta, A. Amalin Prince, A. R. Jac Fredo, Femi Robert",
        "published": "2022-7-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciccsp53532.2022.9862348"
    },
    {
        "id": 20350,
        "title": "DLRG@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil using Multilingual Transformer Models",
        "authors": "Ratnavel Rajalakshmi, Ankita Duraphe, Antonette Shibani",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.dravidianlangtech-1.32"
    },
    {
        "id": 20351,
        "title": "Transformer-Based Models for Named Entity Recognition: A Comparative Study",
        "authors": "Prasanna Kumar R, Bharathi Mohan G, Parthasarathy Srinivasan, Venkatakrishnan R",
        "published": "2023-7-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10308039"
    },
    {
        "id": 20352,
        "title": "Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation",
        "authors": "Premanand Ghadekar, Neel Malwatkar, Nikhil Sontakke, Nirvisha Soni",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asiancon58793.2023.10270018"
    },
    {
        "id": 20353,
        "title": "Understanding the Attention Mechanism in Neural Network Transformer Models in Image Restoration Tasks",
        "authors": "Nikita Berezhnov, Alexander Sirota",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349626"
    },
    {
        "id": 20354,
        "title": "COVID-19 Semantic Search Engine Using Sentence-Transformer Models",
        "authors": "Anagha Jose, Sandhya Harikumar",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-15556-7_14"
    },
    {
        "id": 20355,
        "title": "Dynamic Low-rank Estimation for Transformer-based Language Models",
        "authors": "Ting Hua, Xiao Li, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, Hongxia Jin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.621"
    },
    {
        "id": 20356,
        "title": "AMMU: A survey of transformer-based biomedical pretrained language models",
        "authors": "Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, Sivanesan Sangeetha",
        "published": "2022-2",
        "citations": 82,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2021.103982"
    },
    {
        "id": 20357,
        "title": "Reward modeling for mitigating toxicity in transformer-based language models",
        "authors": "Farshid Faal, Ketra Schmitt, Jia Yuan Yu",
        "published": "2023-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-03944-z"
    },
    {
        "id": 20358,
        "title": "Generating Fake Cyber Threat Intelligence Using Transformer-Based Models",
        "authors": "Priyanka Ranade, Aritran Piplai, Sudip Mittal, Anupam Joshi, Tim Finin",
        "published": "2021-7-18",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534192"
    },
    {
        "id": 20359,
        "title": "Noise and performance analysis on fundus images with CNN and transformer models",
        "authors": "Niranjana Vannadil, Priyanka Kokil",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cict59886.2023.10455148"
    },
    {
        "id": 20360,
        "title": "On the Consistency of Tap-Changing Transformer Models in Power System Studies",
        "authors": "Jose M. Cano, Md Rejwanur R. Mojumdar, Gonzalo A. Orcajo",
        "published": "2020-8-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281405"
    },
    {
        "id": 20361,
        "title": "Online Aggression Identification Using Ensembled Transformer-based Language Models",
        "authors": "Sneha Chinivar, Roopa M S, Arunalatha J S, Venugopal K R",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307921"
    },
    {
        "id": 20362,
        "title": "Using Transformer Models and Textual Analysis for Log Parsing",
        "authors": "Vithor Bertalan, Daniel Aloise",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/issre59848.2023.00037"
    },
    {
        "id": 20363,
        "title": "Modeling of the Power Losses and the Efficiency of a 21 MVA Superconducting Transformer",
        "authors": "Pawel Surdacki, Leszek Jaroszynski, Lukasz Wozniak",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epmccs.2018.8596405"
    },
    {
        "id": 20364,
        "title": "Probing the representations of named entities in Transformer-based Language Models",
        "authors": "Stefan Schouten, Peter Bloem, Piek Vossen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.blackboxnlp-1.32"
    },
    {
        "id": 20365,
        "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
        "authors": "Akshita Jha, Adithya Samavedhi, Vineeth Rakesh, Jaideep Chandrashekar, Chandan Reddy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.178"
    },
    {
        "id": 20366,
        "title": "Automatic Recognition of Emotions in Speech With Large Self-Supervised Learning Transformer Models",
        "authors": "Mrunal Prakash Gavali, Abhishek Verma",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aibthings58340.2023.10292462"
    },
    {
        "id": 20367,
        "title": "Transformer models and meters in MATLAB and PSCAD for GIC and leakage dc studies",
        "authors": "Pitambar Jankee, Hilary Chisepo, Victor Adebayo, David Oyedokun, Charles Trevor Gaunt",
        "published": "2020-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec/robmech/prasa48453.2020.9041060"
    },
    {
        "id": 20368,
        "title": "Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space - Transformer Ensemble Models Tackling Deception and Persuasion",
        "authors": "Sudeep Mangalvedhekar, Kshitij Deshpande, Yash Patwardhan, Vedant Deshpande, Ravindra Murumkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.48"
    },
    {
        "id": 20369,
        "title": "Solar Irradiance Prediction Using Transformer-based Machine Learning Models",
        "authors": "Ayda Demir, Luis Felipe Gutierrez, Akbar Siami Namin, Stephen Bayne",
        "published": "2022-12-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020615"
    },
    {
        "id": 20370,
        "title": "Fabricated Hadith Detection: A Novel Matn-Based Approach With Transformer Language Models",
        "authors": "Kamel Gaanoun, Mohammed Alsuhaibani",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3217457"
    },
    {
        "id": 20371,
        "title": "What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models",
        "authors": "Mor Geva, Uri Katz, Aviv Ben-Arie, Jonathan Berant",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.646"
    },
    {
        "id": 20372,
        "title": "Hierarchical Transformer Models for Age Macular Degeneration OCT Images Classification",
        "authors": "Antonio Zarauz-Moreno, Andrei Martinez-Finkelshtein, Noelia Sanchez-Linan, Alberto.J Benaya-Alamos, Gracia Castro-Luna",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4220677"
    },
    {
        "id": 20373,
        "title": "An ensemble of pre-trained transformer models for imbalanced multiclass malware classification",
        "authors": "Ferhat Demirkıran, Aykut Çayır, Uğur Ünal, Hasan Dağ",
        "published": "2022-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cose.2022.102846"
    },
    {
        "id": 20374,
        "title": "Pretrained Transformer Language Models Versus Pretrained Word Embeddings for the Detection of Accurate Health Information on Arabic Social Media: Comparative Study (Preprint)",
        "authors": "Yahya Albalawi, Nikola S Nikolov, Jim Buckley",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nIn recent years, social media has become a major channel for health-related information in Saudi Arabia. Prior health informatics studies have suggested that a large proportion of health-related posts on social media are inaccurate. Given the subject matter and the scale of dissemination of such information, it is important to be able to automatically discriminate between accurate and inaccurate health-related posts in Arabic.\n\n\nOBJECTIVE\nThe first aim of this study is to generate a data set of generic health-related tweets in Arabic, labeled as either accurate or inaccurate health information. The second aim is to leverage this data set to train a state-of-the-art deep learning model for detecting the accuracy of health-related tweets in Arabic. In particular, this study aims to train and compare the performance of multiple deep learning models that use pretrained word embeddings and transformer language models.\n\n\nMETHODS\nWe used 900 health-related tweets from a previously published data set extracted between July 15, 2019, and August 31, 2019. Furthermore, we applied a pretrained model to extract an additional 900 health-related tweets from a second data set collected specifically for this study between March 1, 2019, and April 15, 2019. The 1800 tweets were labeled by 2 physicians as <i>accurate</i>, <i>inaccurate</i>, or <i>unsure</i>. The physicians agreed on 43.3% (779/1800) of tweets, which were thus labeled as <i>accurate</i> or <i>inaccurate</i>. A total of 9 variations of the pretrained transformer language models were then trained and validated on 79.9% (623/779 tweets) of the data set and tested on 20% (156/779 tweets) of the data set. For comparison, we also trained a bidirectional long short-term memory model with 7 different pretrained word embeddings as the input layer on the same data set. The models were compared in terms of their accuracy, precision, recall, F<sub>1</sub> score, and macroaverage of the F<sub>1</sub> score.\n\n\nRESULTS\nWe constructed a data set of labeled tweets, 38% (296/779) of which were labeled as inaccurate health information, and 62% (483/779) of which were labeled as accurate health information. We suggest that this was highly efficacious as we did not include any tweets in which the physician annotators were unsure or in disagreement. Among the investigated deep learning models, the Transformer-based Model for Arabic Language Understanding version 0.2 (AraBERTv0.2)-large model was the most accurate, with an F<sub>1</sub> score of 87%, followed by AraBERT version 2–large and AraBERTv0.2-base.\n\n\nCONCLUSIONS\nOur results indicate that the pretrained language model AraBERTv0.2 is the best model for classifying tweets as carrying either inaccurate or accurate health information. Future studies should consider applying ensemble learning to combine the best models as it may produce better results.\n",
        "link": "http://dx.doi.org/10.2196/preprints.34834"
    },
    {
        "id": 20375,
        "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models",
        "authors": "Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann",
        "published": "2020",
        "citations": 48,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-demos.22"
    },
    {
        "id": 20376,
        "title": "Integrating prior knowledge to build transformer models",
        "authors": "Pei Jiang, Takashi Obi, Yoshikazu Nakajima",
        "published": "2024-3",
        "citations": 2,
        "abstract": "AbstractThe big Artificial General Intelligence models inspire hot topics currently. The black box problems of Artificial Intelligence (AI) models still exist and need to be solved urgently, especially in the medical area. Therefore, transparent and reliable AI models with small data are also urgently necessary. To build a trustable AI model with small data, we proposed a prior knowledge-integrated transformer model. We first acquired prior knowledge using Shapley Additive exPlanations from various pre-trained machine learning models. Then, we used the prior knowledge to construct the transformer models and compared our proposed models with the Feature Tokenization Transformer model and other classification models. We tested our proposed model on three open datasets and one non-open public dataset in Japan to confirm the feasibility of our proposed methodology. Our results certified that knowledge-integrated transformer models perform better (1%) than general transformer models. Meanwhile, our proposed methodology identified that the self-attention of factors in our proposed transformer models is nearly the same, which needs to be explored in future work. Moreover, our research inspires future endeavors in exploring transparent small AI models.",
        "link": "http://dx.doi.org/10.1007/s41870-023-01635-7"
    },
    {
        "id": 20377,
        "title": "Arabic dialect identification using machine learning and transformer-based models: Submission to the NADI 2022 Shared Task",
        "authors": "Nouf AlShenaifi, Aqil Azmi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.50"
    },
    {
        "id": 20378,
        "title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models",
        "authors": "James O’Neill, Sourav Dutta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.114"
    },
    {
        "id": 20379,
        "title": "A novel framework for mispronunciation detection of Arabic phonemes using audio-oriented transformer models",
        "authors": "Şükrü Selim Çalık, Ayhan Küçükmanisa, Zeynep Hilal Kilimci",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.apacoust.2023.109711"
    },
    {
        "id": 20380,
        "title": "Breast cancer diagnosis through knowledge distillation of Swin transformer-based teacher–student models",
        "authors": "Bhavannarayanna Kolla, Venugopal P",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nBreast cancer is a significant global health concern, emphasizing the crucial need for a timely and accurate diagnosis to enhance survival rates. Traditional diagnostic methods rely on pathologists analyzing whole-slide images (WSIs) to identify and diagnose malignancies. However, this task is complex, demanding specialized expertise and imposing a substantial workload on pathologists. Additionally, existing deep learning models, commonly employed for classifying histopathology images, often need enhancements to ensure their suitability for real-time deployment on WSI, especially when trained for small regions of interest (ROIs). This article introduces two Swin transformer-based architectures: the teacher model, characterized by its moderate size, and the lightweight student model. Both models are trained using a publicly available dataset of breast cancer histopathology images, focusing on ROIs with varying magnification factors. Transfer learning is applied to train the teacher model, and knowledge distillation (KD) transfers its capabilities to the student model. To enhance validation accuracy and minimize the total loss in KD, we employ the state–action–reward–state–action (SARSA) reinforcement learning algorithm. The algorithm dynamically computes temperature and a weighting factor throughout the KD process to achieve high accuracy within a considerably shorter training timeframe. Additionally, the student model is deployed to analyze malignancies in WSI. Despite the student model being only one-third the size and flops of the teacher model, it achieves an impressive accuracy of 98.71%, slightly below the teacher’s accuracy of 98.91%. Experimental results demonstrate that the student model can process WSIs at a throughput of 1.67 samples s−1 with an accuracy of 82%. The proposed student model, trained using KD and the SARSA algorithm, exhibits promising breast cancer classification and WSI analysis performance. These findings indicate its potential for assisting pathologists in diagnosing breast cancer accurately and effectively.",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad10cc"
    },
    {
        "id": 20381,
        "title": "Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient?",
        "authors": "Quentin Bouniot, Angelique Loesch, Amaury Habrard, Romaric Audigier",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00016"
    },
    {
        "id": 20382,
        "title": "Revolutionizing COVID-19 Diagnosis with Swin Transformer: A Comparative Study on CT Image Attention Analysisand CNN Models performance",
        "authors": "Jianbo Yang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10167142"
    },
    {
        "id": 20383,
        "title": "Exploiting deep transformer models in textual review based recommender systems",
        "authors": "Shivangi Gheewala, Shuxiang Xu, Soonja Yeom, Sumbal Maqsood",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121120"
    },
    {
        "id": 20384,
        "title": "A Novel Approach for Infant Cry Classification Using Transformer Models",
        "authors": "Suraj Kumar Singh, Bismanpal Singh Anand, Aadika Bhatia, Gurmeet Singh",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icidea59866.2023.10295053"
    },
    {
        "id": 20385,
        "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
        "authors": "Zehan Li, Haoran Miao, Keqi Deng, Gaofeng Cheng, Sanli Tian, Ta Li, Yonghong Yan",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-707"
    },
    {
        "id": 20386,
        "title": "Transformer Models for Recommending Related Questions in Web Search",
        "authors": "Rajarshee Mitra, Manish Gupta, Sandipan Dandapat",
        "published": "2020-10-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3340531.3412067"
    },
    {
        "id": 20387,
        "title": "Bangla Documents Classification using Transformer Based Deep Learning Models",
        "authors": "Md Mahbubur Rahman, Md. Aktaruzzaman Pramanik, Rifat Sadik, Monikrishna Roy, Partha Chakraborty",
        "published": "2020-12-19",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sti50764.2020.9350394"
    },
    {
        "id": 20388,
        "title": "UoR at SemEval-2020 Task 4: Pre-trained Sentence Transformer Models for Commonsense Validation and Explanation",
        "authors": "Thanet Markchom, Bhuvana Dhruva, Chandresh Pravin, Huizhi Liang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.semeval-1.52"
    },
    {
        "id": 20389,
        "title": "Influence of Language Proficiency on the Readability of Review Text and Transformer-based Models for Determining Language Proficiency",
        "authors": "Salim Sazzed",
        "published": "2022-4-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3487553.3524666"
    },
    {
        "id": 20390,
        "title": "Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings",
        "authors": "Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong Zhao, Subendhu Rongali, Haidar Khan, Michael Kayser",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.423"
    },
    {
        "id": 20391,
        "title": "Transformer Models for Recognizing Abusive Language An investigation and review on Tweeteval and SOLID dataset",
        "authors": "Fabeela Ali Rawther, Geevarghese Titus",
        "published": "2023-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeict56924.2023.10157848"
    },
    {
        "id": 20392,
        "title": "Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation",
        "authors": "Usman Naseem, Ajay Bandi, Shaina Raza, Junaid Rashid, Bharathi Raja Chakravarthi",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.bionlp-1.10"
    },
    {
        "id": 20393,
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": "Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.459"
    },
    {
        "id": 20394,
        "title": "Transformer-based language models for mental health issues: A survey",
        "authors": "Candida M. Greco, Andrea Simeri, Andrea Tagarelli, Ester Zumpano",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.02.016"
    },
    {
        "id": 20395,
        "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
        "authors": "Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal",
        "published": "2020",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.748"
    },
    {
        "id": 20396,
        "title": "A comparison of different wideband models for a single-phase distribution transformer",
        "authors": "Phelippe R. Rodrigues, Alberto De Conti, Vinicius C. Oliveira, Jose L. Silvino",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sipda.2017.8116903"
    },
    {
        "id": 20397,
        "title": "Suicide Tendency Prediction from Psychiatric Notes Using Transformer Models",
        "authors": "Zehan Li, Iqra Ameer, Yan Hu, Ahmed Abdelhameed, Cui Tao, Salih Selek, Hua Xu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi57859.2023.00074"
    },
    {
        "id": 20398,
        "title": "BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bangla",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.17"
    },
    {
        "id": 20399,
        "title": "Efficient method for transformer models implementation in distribution load flow matrix",
        "authors": "M. Kadri, A. Hamouda, S. Sayah",
        "published": "2023-4-23",
        "citations": 0,
        "abstract": "Introduction. Most distribution networks are unbalanced and therefore require a specific solution for load flow. There are many works on the subject in the literature, but they mainly focus on simple network configurations. Among the methods dedicated to this problem, one can refer to the load flow method based on the bus injection to branch current and branch current to bus voltage matrices. Problem. Although this method is regarded as simple and complete, its drawback is the difficulty in supporting the transformer model as well as its winding connection types. Nevertheless, the method requires the system per unit to derive the load flow solution. Goal. In the present paper, our concern is the implementation of distribution transformers in the modeling and calculation of load flow in unbalanced networks. Methodology. Unlike previous method, distribution transformer model is introduced in the topology matrices without simplifying assumptions. Particularly, topology matrices were modified to take into account all winding types of both primary and secondary sides of transformer that conserve the equivalent scheme of an ideal transformer in series with an impedance. In addition, the adopted transformer models overcome the singularity problem that can be encountered when switching from the primary to the secondary side of transformer and inversely. Practical value. The proposed approach was applied to various distribution networks such as IEEE 4-nodes, IEEE 13-nodes and IEEE 37-nodes. The obtained results validate the method and show its effectiveness. ",
        "link": "http://dx.doi.org/10.20998/2074-272x.2023.3.11"
    },
    {
        "id": 20400,
        "title": "Convolutional Neural Network or Vision Transformer? Benchmarking Various Machine Learning Models for Distracted Driver Detection",
        "authors": "Hong Vin Koay, Joon Huang Chuah, Chee-Onn Chow",
        "published": "2021-12-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon54134.2021.9707341"
    },
    {
        "id": 20401,
        "title": "Evaluating Pretrained Transformer-based Models for COVID-19 Fake News Detection",
        "authors": "Adeep Hande, Karthik Puranik, Ruba Priyadharshini, Sajeetha Thavareesan, Bharathi Raja Chakravarthi",
        "published": "2021-4-8",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccmc51019.2021.9418446"
    },
    {
        "id": 20402,
        "title": "silpa_nlp at SemEval-2022 Tasks 11: Transformer based NER models for Hindi and Bangla languages",
        "authors": "Sumit Singh, Pawankumar Jawale, Uma Tiwary",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.211"
    },
    {
        "id": 20403,
        "title": "Transformer Entity Automatic Extraction Models in Multi-layer Soft Location Matching Format",
        "authors": "Shuli Guo, Lina Han, Wentao Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-2665-7_4"
    },
    {
        "id": 20404,
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": "Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.931"
    },
    {
        "id": 20405,
        "title": "FactionFormer: Context-Driven Collaborative Vision Transformer Models for Edge Intelligence",
        "authors": "Sumaiya Tabassum Nimi, Md Adnan Arefeen, Md Yusuf Sarwar Uddin, Biplob Debnath, Srimat Chakradhar",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcomp58114.2023.00084"
    },
    {
        "id": 20406,
        "title": "Classification of Abnormal Cardiac Rhythm from Brief Single-Lead ECG Recordings using Embeddings from Transformer Encoder Models",
        "authors": "B.S. Utkars Jain, Prahlad G Menon",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385454"
    },
    {
        "id": 20407,
        "title": "Transformer-based Models for Arabic Online Handwriting Recognition",
        "authors": "Fakhraddin Alwajih, Eman Badr, Sherif Abdou",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2022.01305102"
    },
    {
        "id": 20408,
        "title": "On the development of power transformer failure models: An Australian case study",
        "authors": "D. Martin, J. Marks, T. Saha, O. Krause, G. Russell, A. Alibegovic-Memisevic",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274571"
    },
    {
        "id": 20409,
        "title": "INCORPORATING ATTENTION SCORE TO IMPROVE FORESIGHT PRUNING ON TRANSFORMER MODELS",
        "authors": "А. В. Мельниченко, К. А. Здор",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "With rapid development of technologies and growing number of application of neural networks, the problem of optimization arises. Among other methods to optimize training and inference time, neural network pruning has attracted attention in recent years. The main goal of pruning is to reduce the computational complexity of neural network models while retaining performance metrics on desired level. Among the various approaches to pruning, Single-shot Network Pruning (SNIP) methods was designed as a straightforward and effective approach to optimize number of parameters before training. However, as neural network architectures have evolved, particularly with the growing popularity of transformers, a need to reevaluate traditional pruning methods arises. This paper aims to revisit SNIP pruning method, evaluate its performance on transformer model, and introduce an enhanced version of SNIP, specifically designed for transformer architectures. The paper outlines the mathematical framework of SNIP algorithm, and proposes a modification, based on specifics of transformers models. Transformer models achieved impressive results because of their attention mechanisms for a multitude of tasks such as language modeling, translation, computer vision tasks and many others. The proposed modification takes into account this unique feature and combines this information with traditional loss gradients. Traditional method calculates importance score for weights of the network using only gradients from loss function, in the case of enhanced algorithm. In the enhanced version, the importance score is a composite metric that incorporates not only the gradient from the loss function but also from the attention activations. To evaluate the efficiency of proposed modifications, a series of experiments were conducted on image classification task, using Linformer variation of transformer architectures. The results of experiments demonstrate the efficiency of incorporating attention scores in pruning. Conducted experiments show that model pruned by modified algorithm outperforms model pruned by original SNIP by 34% in validation accuracy, confirming the validity of the improvements introduced.",
        "link": "http://dx.doi.org/10.26661/2786-6254-2023-2-03"
    },
    {
        "id": 20410,
        "title": "From Context to Code: Rational De Novo DNA Design and Predicting Cross-Species DNA Functionality Using Deep Learning Transformer Models",
        "authors": "Gurvinder Singh Dahiya, Thea Isabel Bakken, Maxime Fages-Lartaud, Rahmi Lale",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTSynthetic biology currently operates under a framework dominated by trial-and-error approaches, which hinders the effective engineering of organisms and the expansion of large-scale biomanufacturing. Motivated by the success of computational designs in areas like architecture and aeronautics, we aspire to transition to a more efficient and predictive methodology in synthetic biology. In this study, we report a DNA Design Platform that relies on the predictive power of Transformer-based deep learning architectures. The platform transforms the conventional paradigms in synthetic biology by enabling the context-sensitive and host-specific engineering of 5′ regulatory elements—promoters and 5′ untranslated regions (UTRs) along with an array of codon-optimised coding sequence (CDS) variants. This allows us to generate context-sensitive 5′ regulatory sequences and CDSs, achieving an unparalleled level of specificity and adaptability in different target hosts. With context-aware design, we significantly broaden the range of possible gene expression profiles and phenotypic outcomes, substantially reducing the need for laborious high-throughput screening efforts. Our context-aware, AI-driven design strategy marks a significant advancement in synthetic biology, offering a scalable and refined approach for gene expression optimisation across a diverse range of expression hosts. In summary, this study represents a substantial leap forward in the field, utilising deep learning models to transform the conventional design, build, test, learn-cycle into a more efficient and predictive framework.",
        "link": "http://dx.doi.org/10.1101/2023.10.15.562386"
    },
    {
        "id": 20411,
        "title": "An Efficient Approach With Application of Linear and Nonlinear Models for Evaluation of Power Transformer Health Index",
        "authors": "Hamed Zeinoddini-Meymand, Salah Kamel, Baseem Khan",
        "published": "2021",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3124845"
    },
    {
        "id": 20412,
        "title": "Probing for Multilingual Numerical Understanding in Transformer-Based Language Models",
        "authors": "Devin Johnson, Denise Mak, Andrew Barker, Lexi Loessberg-Zahl",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.blackboxnlp-1.18"
    },
    {
        "id": 20413,
        "title": "Comparison of Different Multi-winding Transformer Models in Multi-port AC-coupled Converter Application",
        "authors": "Haojun Qin, Huan Zhang, Ming Liu, Chengbin Ma",
        "published": "2021-10-13",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon48115.2021.9589283"
    },
    {
        "id": 20414,
        "title": "State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis",
        "authors": "Igor V. Tetko, Pavel Karpov, Ruud Van Deursen, Guillaume Godin",
        "published": "2020-11-4",
        "citations": 168,
        "abstract": "AbstractWe investigated the effect of different training scenarios on predicting the (retro)synthesis of chemical compounds using text-like representation of chemical reactions (SMILES) and Natural Language Processing (NLP) neural network Transformer architecture. We showed that data augmentation, which is a powerful method used in image processing, eliminated the effect of data memorization by neural networks and improved their performance for prediction of new sequences. This effect was observed when augmentation was used simultaneously for input and the target data simultaneously. The top-5 accuracy was 84.8% for the prediction of the largest fragment (thus identifying principal transformation for classical retro-synthesis) for the USPTO-50k test dataset, and was achieved by a combination of SMILES augmentation and a beam search algorithm. The same approach provided significantly better results for the prediction of direct reactions from the single-step USPTO-MIT test set. Our model achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed set and 97% top-5 accuracy for the USPTO-MIT separated set. It also significantly improved results for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. The appearance frequency of the most abundantly generated SMILES was well correlated with the prediction outcome and can be used as a measure of the quality of reaction prediction.",
        "link": "http://dx.doi.org/10.1038/s41467-020-19266-y"
    },
    {
        "id": 20415,
        "title": "DS4DH at SemEval-2022 Task 11: Multilingual Named Entity Recognition Using an Ensemble of Transformer-based Language Models",
        "authors": "Hossein Rouhizadeh, Douglas Teodoro",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.212"
    },
    {
        "id": 20416,
        "title": "Enhancing Biomedical Relation Extraction with Transformer Models using Shortest Dependency Path Features and Triplet Information",
        "authors": "Vani Kanjirangat, Fabio Rinaldi",
        "published": "2021-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2021.103893"
    },
    {
        "id": 20417,
        "title": "Transformer-Based Models for Hyperspectral Point Clouds Segmentation",
        "authors": "Aldino Rizaldy, Ahmed J. Afifi, Pedram Ghamisi, Richard Gloaguen",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/whispers61460.2023.10431346"
    },
    {
        "id": 20418,
        "title": "Bangla Fake News Detection using Machine Learning, Deep Learning and Transformer Models",
        "authors": "Risul Islam Rasel, Anower Hossen Zihad, Nasrin Sultana, Mohammed Moshiul Hoque",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit57492.2022.10055592"
    },
    {
        "id": 20419,
        "title": "Automatic text summarization using transformer-based language models",
        "authors": "Ritika Rao, Sourabh Sharma, Nitin Malik",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13198-024-02280-4"
    },
    {
        "id": 20420,
        "title": "Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models",
        "authors": "Batsergelen Myagmar, Jie Li, Shigetomo Kimura",
        "published": "2019",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2952360"
    },
    {
        "id": 20421,
        "title": "Mixed Precision Quantization of Transformer Language Models for Speech Recognition",
        "authors": "Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",
        "published": "2021-6-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9414076"
    },
    {
        "id": 20422,
        "title": "How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?",
        "authors": "Simeng Sun, Brian Dillon, Mohit Iyyer",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.insights-1.6"
    },
    {
        "id": 20423,
        "title": "Greedy-layer pruning: Speeding up transformer models for natural language processing",
        "authors": "David Peer, Sebastian Stabinger, Stefan Engl, Antonio Rodríguez-Sánchez",
        "published": "2022-5",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2022.03.023"
    },
    {
        "id": 20424,
        "title": "Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection",
        "authors": "Luca Di Liello, Siddhant Garg, Luca Soldaini, Alessandro Moschitti",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.810"
    },
    {
        "id": 20425,
        "title": "Combining Variational Autoencoders and Transformer Language Models for Improved Password Generation",
        "authors": "David Biesner, Kostadin Cvejoski, Rafet Sifa",
        "published": "2022-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3538969.3539000"
    },
    {
        "id": 20426,
        "title": "LKAU23 at Qur’an QA 2023: Using Transformer Models for Retrieving Passages and Finding Answers to Questions from the Qur’an",
        "authors": "Sarah Alnefaie, Abdullah Alsaleh, Eric Atwell, Mohammad Alsalka, Abdulrahman Altahhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.80"
    },
    {
        "id": 20427,
        "title": "AIMH at SemEval-2021 Task 6: Multimodal Classification Using an Ensemble of Transformer Models",
        "authors": "Nicola Messina, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.semeval-1.140"
    },
    {
        "id": 20428,
        "title": "An Evaluation of Transformer-Based Models in Personal Health Mention Detection",
        "authors": "Alvi Aveen Khan, Fida Kamal, Nuzhat Nower, Tasnim Ahmed, Tareque Mohmud Chowdhury",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit57492.2022.10054937"
    },
    {
        "id": 20429,
        "title": "Comparison of Analytical Models of Transformer Leakage Inductance: Accuracy Versus Computational Effort",
        "authors": "Richard Schlesinger, Jurgen Biela",
        "published": "2021-1",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2020.3001056"
    },
    {
        "id": 20430,
        "title": "Custom Coarse Grained Named Entity Recognition for Filipino Storytelling Data Using Uncased Transformer Models",
        "authors": "Sherwyne Costiniano, Rose Ann Mae Santos, Julius Simon Mendoza, Allen Jay Gale",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4310555"
    },
    {
        "id": 20431,
        "title": "Deep Transfer Learning &amp; Beyond: Transformer Language Models in Information Systems Research",
        "authors": "Ross Gruetzemacher, David Paradice",
        "published": "2022-1-31",
        "citations": 16,
        "abstract": "\n            AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving\n            transformer language models (TLMs)\n            offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.\n          ",
        "link": "http://dx.doi.org/10.1145/3505245"
    },
    {
        "id": 20432,
        "title": "Improving machine translation and post-editing for Chinese tourism texts using transformer-based models",
        "authors": "Yuan Feng, Qiwei Hu, Siran Yang",
        "published": "2024-2-4",
        "citations": 0,
        "abstract": "As the digital age and globalization continue to evolve, the demand for accurate machine translation of tourism texts has increased substantially. This paper investigates how to improve the quality of machine translation (MT) and machine translation post-editing (MTPE) of Chinese tourism texts for non-native speakers. A review of the machine translation literature reveals a significant progression in translation methods from rule-based to corpus-based, statistical, and finally to the current neural machine translation (NMT) models. Despite its advanced capabilities, NMT requires large amounts of parallel data for training, which often presents challenges. This study proposes the use of Transformer-based models for MT and MTPE to improve translation quality. A dataset was curated from online sources, mainly Chinese tourism websites. The methodology involved pre-processing the data, performing machine translation using the Transformer model, and post-editing the results. The experiment demonstrated an increase in the BLEU score, suggesting an improvement in translation quality. However, challenges such as the handling of synonyms and geographical nouns were encountered, indicating the need for further research and model optimization.",
        "link": "http://dx.doi.org/10.54254/2755-2721/34/20230286"
    },
    {
        "id": 20433,
        "title": "Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models",
        "authors": "Mohammadreza Banaei, Klaudia Bałazy, Artur Kasymov, Rémi Lebret, Jacek Tabor, Karl Aberer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.133"
    },
    {
        "id": 20434,
        "title": "Application of Transformer Models to Landslide Susceptibility Mapping",
        "authors": "Shuai Bao, Jiping Liu, Liang Wang, Xizhi Zhao",
        "published": "2022-11-23",
        "citations": 3,
        "abstract": "Landslide susceptibility mapping (LSM) is of great significance for the identification and prevention of geological hazards. LSM is based on convolutional neural networks (CNNs); CNNs use fixed convolutional kernels, focus more on local information and do not retain spatial information. This is a property of the CNN itself, resulting in low accuracy of LSM. Based on the above problems, we use Vision Transformer (ViT) and its derivative model Swin Transformer (Swin) to conduct LSM for the selected study area. Machine learning and a CNN model are used for comparison. Fourier transform amplitude, feature similarity and other indicators were used to compare and analyze the difference in the results. The results show that the Swin model has the best accuracy, F1-score and AUC. The results of LSM are combined with landslide points, faults and other data analysis; the ViT model results are the most consistent with the actual situation, showing the strongest generalization ability. In this paper, we believe that the advantages of ViT and its derived models in global feature extraction ensure that ViT is more accurate than CNN and machine learning in predicting landslide probability in the study area.",
        "link": "http://dx.doi.org/10.3390/s22239104"
    },
    {
        "id": 20435,
        "title": "(Psycho-)Linguistic Features Meet Transformer Models for Improved Explainable and Controllable Text Simplification",
        "authors": "Yu Qiao, Xiaofei Li, Daniel Wiechmann, Elma Kerz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.tsar-1.12"
    },
    {
        "id": 20436,
        "title": "On the effect of dropping layers of pre-trained transformer models",
        "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov",
        "published": "2023-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.csl.2022.101429"
    },
    {
        "id": 20437,
        "title": "Harnessing artificial intelligence for ophthalmic disease diagnosis: a comparative study of CNNs and Swin transformer models",
        "authors": "LIUYI ZHANG",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3021475"
    },
    {
        "id": 20438,
        "title": "A Review for Pre-Trained Transformer-Based Time Series Forecasting Models",
        "authors": "Yunus Emre Midilli, Sergei Parshutin",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itms59786.2023.10317721"
    },
    {
        "id": 20439,
        "title": "Development of Symbolic Signal Processing and Transformer Models for Predicting Respiratory System Mechanics in Mechanical Ventilation",
        "authors": "Yang Junwei, Pawin Numthavaj, Anuchate Pattanateepapon, Chanon Puttanawarut, Detajin Junhasavasdikul",
        "published": "2023-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bmeicon60347.2023.10322095"
    },
    {
        "id": 20440,
        "title": "Tesla at SemEval-2022 Task 4: Patronizing and Condescending Language Detection using Transformer-based Models with Data Augmentation",
        "authors": "Sahil Bhatt, Manish Shrivastava",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.52"
    },
    {
        "id": 20441,
        "title": "Optimizing the transformer model for speech recognition",
        "authors": "В.Я. Чучупал",
        "published": "2021-12-28",
        "citations": 0,
        "abstract": "Применение нейросетевых моделей «кодер – декодер с вниманием» в системах распознавания речи обеспечивает высокие показатели точности. К существенным ограничениям этих моделей относятся: отсутствие режима реального времени и большая вычислительная сложность (количество операций и объем требуемой памяти). Формально сложность алгоритма вычисления функции самовнимания растет пропорционально ( ) 2 O n , где n – количество элементов слоя сети. Использование физических свойств речевого сигнала позволяет построить такой алгоритм вычисления функции самовнимания, который имеет оценку сложности O n( ) и при этом рассчитан на потоковую обработку сигнала, в том числе работу кодера трансформера в реальном масштабе времени.\nThe use of neural models “encoder – decoder with attention” in speech recognition applications provides high recognition accuracy rates. The significant limitations of these models include the lack of streaming mode and high computational complexity (in terms of the number of operations and the amount of memory). Formally, the complexity of the self-attention algorithm grows as 2 O n( ), where  n  is the number of elements in the attention layer. The use of the physical properties of a speech signal makes it possible to construct such an algorithm for calculating the self-attention of Transformer’s encoder, which has the complexity as low as O n( ) and at the same time is suitable for recognition in streaming and realtime operation modes.",
        "link": "http://dx.doi.org/10.18137/rnu.v9187.21.04/1.p.058"
    },
    {
        "id": 20442,
        "title": "An Attention-Based Backend Allowing Efficient Fine-Tuning of Transformer Models for Speaker Verification",
        "authors": "Junyi Peng, Oldrich Plchot, Themos Stafylakis, Ladislav Mosner, Lukas Burget, Jan Cernocky",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022775"
    },
    {
        "id": 20443,
        "title": "Comparison of different Transformer models for Meeting Recapitulation Tool",
        "authors": "Neha Hegde, Shreya Sreedhar, Harsha S,  Mashooda, Vasudeva Rao P V",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/discover58830.2023.10316690"
    },
    {
        "id": 20444,
        "title": "Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis",
        "authors": "Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux",
        "published": "2021-5-26",
        "citations": 13,
        "abstract": "\nBackground\nSemantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations.\n\n\nObjective\nWe created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models.\n\n\nMethods\nGiven a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model’s loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model.\n\n\nResults\nOur model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model’s loss, we identified the system’s failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)–style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set.\n\n\nConclusions\nWe designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model’s outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these “black box” models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP.\n",
        "link": "http://dx.doi.org/10.2196/23099"
    },
    {
        "id": 20445,
        "title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.34"
    },
    {
        "id": 20446,
        "title": "Insar Time-Series Deformation Forecasting Surrounding Salt Lake Using Deep Transformer Models",
        "authors": "jing wang, Chao Li, Lu Li, Zhihua Huang, Chao Wang, Hong Zhang, Zhengjia Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4197041"
    },
    {
        "id": 20447,
        "title": "Detecting Offensive Tamil Texts Using Machine Learning And Multilingual Transformer Models",
        "authors": "Malliga Subramanian, G J Adhithiya, S Gowthamkrishnan, R Deepti",
        "published": "2022-3-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstsn53084.2022.9761335"
    },
    {
        "id": 20448,
        "title": "ConvFormer: parameter reduction in transformer models for 3D human pose estimation by leveraging dynamic multi-headed convolutional attention",
        "authors": "Alec Diaz-Arias, Dmitriy Shin",
        "published": "2023-7-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00371-023-02936-5"
    },
    {
        "id": 20449,
        "title": "Comparative Analysis of Vision Transformer Models for Facial Emotion Recognition Using Augmented Balanced Datasets",
        "authors": "Sukhrob Bobojanov, Byeong Man Kim, Mukhriddin Arabboev, Shohruh Begmatov",
        "published": "2023-11-13",
        "citations": 3,
        "abstract": "Facial emotion recognition (FER) has a huge importance in the field of human–machine interface. Given the intricacies of human facial expressions and the inherent variations in images, which are characterized by diverse facial poses and lighting conditions, the task of FER remains a challenging endeavour for computer-based models. Recent advancements have seen vision transformer (ViT) models attain state-of-the-art results across various computer vision tasks, encompassing image classification, object detection, and segmentation. Moreover, one of the most important aspects of creating strong machine learning models is correcting data imbalances. To avoid biased predictions and guarantee reliable findings, it is essential to maintain the distribution equilibrium of the training dataset. In this work, we have chosen two widely used open-source datasets, RAF-DB and FER2013. As well as resolving the imbalance problem, we present a new, balanced dataset, applying data augmentation techniques and cleaning poor-quality images from the FER2013 dataset. We then conduct a comprehensive evaluation of thirteen different ViT models with these three datasets. Our investigation concludes that ViT models present a promising approach for FER tasks. Among these ViT models, Mobile ViT and Tokens-to-Token ViT models appear to be the most effective, followed by PiT and Cross Former models.",
        "link": "http://dx.doi.org/10.3390/app132212271"
    },
    {
        "id": 20450,
        "title": "TAG: Gradient Attack on Transformer-based Language Models",
        "authors": "Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, Caiwen Ding",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.305"
    },
    {
        "id": 20451,
        "title": "Rotary Transformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-19"
    },
    {
        "id": 20452,
        "title": "RESEARCH ON THE SPECIFIC FEATURES OF DETERMINING  THE SEMANTIC SIMILARITY OF ARBITRARY-LENGTH TEXT CONTENT  USING MULTILINGUAL TRANSFORMER-BASED MODELS",
        "authors": "Serhii Olizarenko, Vladimir Argunov",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20998/2522-9052.2020.3.13"
    },
    {
        "id": 20453,
        "title": "Reinforcement learning using fully connected, attention, and transformer models in knapsack problem solving",
        "authors": "Beytullah Yildiz",
        "published": "2022-4-25",
        "citations": 10,
        "abstract": "SummaryKnapsack is a combinatorial optimization problem that involves a variety of resource allocation challenges. It is defined as non‐deterministic polynomial time (NP) hard and has a wide range of applications. Knapsack problem (KP) has been studied in applied mathematics and computer science for decades. Many algorithms that can be classified as exact or approximate solutions have been proposed. Under the category of exact solutions, algorithms such as branch‐and‐bound and dynamic programming and the approaches obtained by combining these algorithms can be classified. Due to the fact that exact solutions require a long processing time, many approximate methods have been introduced for knapsack solution. In this research, deep Q‐learning using models containing fully connected layers, attention, and transformer as function estimators were used to provide the solution for KP. We observed that deep Q‐networks, which continued their training by observing the reward signals provided by the knapsack environment we developed, optimized the total reward gained over time. The results showed that our approaches give near‐optimum solutions and work about 40 times faster than an exact algorithm using dynamic programming.",
        "link": "http://dx.doi.org/10.1002/cpe.6509"
    },
    {
        "id": 20454,
        "title": "Novelty fused image and text models based on deep neural network and transformer for multimodal sentiment analysis",
        "authors": "Bui Thanh Hung, Nguyen Hoang Minh Thu",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-18105-8"
    },
    {
        "id": 20455,
        "title": "<scp>AI</scp> unveiled personalities: Profiling optimistic and pessimistic attitudes in <scp>Hindi</scp> dataset using transformer‐based models",
        "authors": "Dipika Jain, Akshi Kumar",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "AbstractBoth optimism and pessimism are intricately intertwined with an individual's inherent personality traits and people of all personality types can exhibit a wide range of attitudes and behaviours, including levels of optimism and pessimism. This paper undertakes a comprehensive analysis of optimistic and pessimistic tendencies present within Hindi textual data, employing transformer‐based models. The research represents a pioneering effort to define and establish an interaction between the personality and attitude chakras within the realm of human psychology. Introducing an innovative “Chakra” system to illustrate complex interrelationships within human psychology, this work aligns the Myers‐Briggs Type Indicator (MBTI) personality traits with optimistic and pessimistic attitudes, enriching our understanding of emotional projection in text. The study employs meticulously fine‐tuned transformer models—specifically mBERT, XLM‐RoBERTa, IndicBERT, mDeBERTa and a novel stacked mDeBERTa—trained on the novel Hindi dataset ‘मनोभाव’ (pronounced as Manobhav). Remarkably, the proposed Stacked mDeBERTa model outperforms others, recording an accuracy of 0.7785 along with elevated precision, recall, and F1 score values. Notably, its ROC AUC score of 0.7226 underlines its robustness in distinguishing between positive and negative emotional attitudes. The comparative analysis highlights the superiority of the Stacked mDeBERTa model in effectively capturing emotional attitudes in Hindi text.",
        "link": "http://dx.doi.org/10.1111/exsy.13572"
    },
    {
        "id": 20456,
        "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
        "authors": "Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14722/ndss.2024.24115"
    },
    {
        "id": 20457,
        "title": "Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition",
        "authors": "Cedric Lothritz, Kevin Allix, Lisa Veiber, Tegawendé F. Bissyandé, Jacques Klein",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.334"
    },
    {
        "id": 20458,
        "title": "Using Transformer models for gender attribution in Polish",
        "authors": "Karol Kaczmarek, Jakub Pokrywka, Filip Graliński",
        "published": "2022-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15439/2022f197"
    },
    {
        "id": 20459,
        "title": "LastResort at SemEval-2022 Task 4: Towards Patronizing and Condescending Language Detection using Pre-trained Transformer Based Models Ensembles",
        "authors": "Samyak Agrawal, Radhika Mamidi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.45"
    },
    {
        "id": 20460,
        "title": "Towards Multimodal Spatio-Temporal Transformer-based Models for Traffic Congestion Prediction",
        "authors": "Huy Quang Ung, Yutaro Mishima, Hao Niu, Shinya Wada",
        "published": "2023-6-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3592571.3592969"
    },
    {
        "id": 20461,
        "title": "Benchmarking Transformer-Based Models for Identifying Social Determinants of Health in Clinical Notes",
        "authors": "Xiaoyu Wang, Dipankar Gupta, Michael Killian, Zhe He",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi57859.2023.00102"
    },
    {
        "id": 20462,
        "title": "ProtTrans-Glutar: Incorporating Features From Pre-trained Transformer-Based Models for Predicting Glutarylation Sites",
        "authors": "Fatma Indriani, Kunti Robiatul Mahmudah, Bedy Purnama, Kenji Satou",
        "published": "2022-5-31",
        "citations": 5,
        "abstract": "Lysine glutarylation is a post-translational modification (PTM) that plays a regulatory role in various physiological and biological processes. Identifying glutarylated peptides using proteomic techniques is expensive and time-consuming. Therefore, developing computational models and predictors can prove useful for rapid identification of glutarylation. In this study, we propose a model called ProtTrans-Glutar to classify a protein sequence into positive or negative glutarylation site by combining traditional sequence-based features with features derived from a pre-trained transformer-based protein model. The features of the model were constructed by combining several feature sets, namely the distribution feature (from composition/transition/distribution encoding), enhanced amino acid composition (EAAC), and features derived from the ProtT5-XL-UniRef50 model. Combined with random under-sampling and XGBoost classification method, our model obtained recall, specificity, and AUC scores of 0.7864, 0.6286, and 0.7075 respectively on an independent test set. The recall and AUC scores were notably higher than those of the previous glutarylation prediction models using the same dataset. This high recall score suggests that our method has the potential to identify new glutarylation sites and facilitate further research on the glutarylation process.",
        "link": "http://dx.doi.org/10.3389/fgene.2022.885929"
    },
    {
        "id": 20463,
        "title": "Transformer-based deep learning models for predicting permeability of porous media",
        "authors": "Yinquan Meng, Jianguo Jiang, Jichun Wu, Dong Wang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10512732.1"
    },
    {
        "id": 20464,
        "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
        "authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov",
        "published": "2019",
        "citations": 1094,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1285"
    },
    {
        "id": 20465,
        "title": "Classification of EEG signals using Transformer based deep learning and ensemble models",
        "authors": "Mahsa Zeynali, Hadi Seyedarabi, Reza Afrouzian",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.105130"
    },
    {
        "id": 20466,
        "title": "Extracting User Behavior at Electric Vehicle Charging Stations with Transformer Deep Learning Models",
        "authors": "Omar Isaac Asensio, Daniel J Marchetto, Sooji Ha, Sameer Dharur",
        "published": "2020-7-8",
        "citations": 1,
        "abstract": "Mobile applications have become widely popular for their ability to access real-time information. In electric vehicle (EV) mobility, these applications are used by drivers to locate charging stations in public spaces, pay for charging transactions, and engage with other users. This activity generates a rich source of data about charging infrastructure and behavior. However, an increasing share of this data is stored as unstructured text—inhibiting our ability to interpret behavior in real-time. In this article, we implement recent transformer-based deep learning algorithms, BERT and XLnet, that have been tailored to automatically classify short user reviews about EV charging experiences. We achieve classification results with a mean accuracy of over 91% and a mean F1 score of over 0.81 allowing for more precise detection of topic categories, even in the presence of highly imbalanced data. Using these classification algorithms as a pre-processing step, we analyze a U.S. national dataset with econometric methods to discover the dominant topics of discourse in charging infrastructure. After adjusting for station characteristics and other factors, we find that the functionality of a charging station is the dominant topic among EV drivers and is more likely to be discussed at points-of-interest with negative user experiences.",
        "link": "http://dx.doi.org/10.4995/carma2020.2020.11613"
    },
    {
        "id": 20467,
        "title": "Adding Linguistic Information to Transformer Models Improves Biomedical Event Detection?",
        "authors": "Laura Zanella, Yannick Toussaint",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15439/2023f2076"
    },
    {
        "id": 20468,
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models",
        "authors": "Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.765"
    },
    {
        "id": 20469,
        "title": "Enhancing Fine-Grained 3D Object Recognition Using Hybrid Multi-Modal Vision Transformer-CNN Models",
        "authors": "Songsong Xiong, Georgios Tziafas, Hamidreza Kasaei",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342235"
    },
    {
        "id": 20470,
        "title": "A Robust Approach to Fine-tune Pre-trained Transformer-based models for Text Summarization through Latent Space Compression",
        "authors": "Ala Alam Falaki, Robin Gras",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00030"
    },
    {
        "id": 20471,
        "title": "Evaluation of transformer models for financial targeted sentiment analysis in Spanish",
        "authors": "Ronghao Pan, José Antonio García-Díaz, Francisco Garcia-Sanchez, Rafael Valencia-García",
        "published": "2023-5-9",
        "citations": 2,
        "abstract": "Nowadays, financial data from social media plays an important role to predict the stock market. However, the exponential growth of financial information and the different polarities of sentiment that other sectors or stakeholders may have on the same information has led to the need for new technologies that automatically collect and classify large volumes of information quickly and easily for each stakeholder. In this scenario, we conduct a targeted sentiment analysis that can automatically extract the main economic target from financial texts and obtain the polarity of a text towards such main economic target, other companies and society in general. To this end, we have compiled a novel corpus of financial tweets and news headlines in Spanish, constituting a valuable resource for the Spanish-focused research community. In addition, we have carried out a performance comparison of different Spanish-specific large language models, with MarIA and BETO achieving the best results. Our best result has an overall performance of 76.04%, 74.16%, and 68.07% in macro F1-score for the sentiment classification towards the main economic target, society, and other companies, respectively, and an accuracy of 69.74% for target detection. We have also evaluated the performance of multi-label classification models in this context and obtained a performance of 71.13%.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1377"
    },
    {
        "id": 20472,
        "title": "Full/Regular Research Paper submission to (CSCI-RTCW): Multi Class Classification of Online Radicalization Using Transformer Models",
        "authors": "Chesta Sofat, Shabeg Singh Gill, Divya Bansal",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci58124.2022.00183"
    },
    {
        "id": 20473,
        "title": "An Empirical Analysis of the Long Short Term Memory and Temporal Fusion Transformer Models on Regional Air Quality Forecast",
        "authors": "Chengzhang Zhu, Ying Tang",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsi58851.2023.10303941"
    },
    {
        "id": 20474,
        "title": "Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models",
        "authors": "David Rozado, Ruth Hughes, Jamin Halberstadt",
        "published": "2022-10-18",
        "citations": 13,
        "abstract": "This work describes a chronological (2000–2019) analysis of sentiment and emotion in 23 million headlines from 47 news media outlets popular in the United States. We use Transformer language models fine-tuned for detection of sentiment (positive, negative) and Ekman’s six basic emotions (anger, disgust, fear, joy, sadness, surprise) plus neutral to automatically label the headlines. Results show an increase of sentiment negativity in headlines across written news media since the year 2000. Headlines from right-leaning news media have been, on average, consistently more negative than headlines from left-leaning outlets over the entire studied time period. The chronological analysis of headlines emotionality shows a growing proportion of headlines denoting anger, fear, disgust and sadness and a decrease in the prevalence of emotionally neutral headlines across the studied outlets over the 2000–2019 interval. The prevalence of headlines denoting anger appears to be higher, on average, in right-leaning news outlets than in left-leaning news media.",
        "link": "http://dx.doi.org/10.1371/journal.pone.0276367"
    },
    {
        "id": 20475,
        "title": "Recognizing Emotions from Texts Using an Ensemble of Transformer-Based Language Models",
        "authors": "F. A. Acheampong, H. Nunoo-Mensah, Wenyu Chen",
        "published": "2021-12-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip53232.2021.9674102"
    },
    {
        "id": 20476,
        "title": "COVID-19 Fake News Detection With Pre-trained Transformer Models",
        "authors": "Bakti Amirul Jabar, Seline Seline, Bintang Bintang, Cameron Jane Victoria, Rio Nur Arifin",
        "published": "2022-12-30",
        "citations": 0,
        "abstract": "COVID-19 is a new virus that first appeared in the year 2020 and is still currently plaguing our world. With the emergence of this virus, much information, both fake and real, has circulated in the internet. Fake information can lead to misleading information and cause a riot in society. In this paper, we aim to build a hoax detection system using the pre-trained transformer models BERT, RoBERTa, DeBERTa and Electra. From these four models, we will find which model gives the most accurate results. BERT gives a validation accuracy of 97.15% and test accuracy of 97.01%. RoBERTa gives a validation accuracy of 97.34% and test accuracy of 97.15%. DeBERTa gives a test accuracy of 97.48% and a test accuracy of 97.25%. Lastly, Electra gives a validation accuracy of 97.95% and a test accuracy of 97.76%. Electra is one of the newer models and is proven to be the most accurate model in our experiment and the one we will choose to implement fake news detection.",
        "link": "http://dx.doi.org/10.31937/ti.v14i2.2776"
    },
    {
        "id": 20477,
        "title": "SSN_MLRG1@DravidianLangTech-ACL2022: Troll Meme Classification in Tamil using Transformer Models",
        "authors": "Shruthi Hariprasad, Sarika Esackimuthu, Saritha Madhavan, Rajalakshmi Sivanaiah, Angel S",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.dravidianlangtech-1.21"
    },
    {
        "id": 20478,
        "title": "Text classification is keyphrase explainable! Exploring local interpretability of transformer models with keyphrase extraction",
        "authors": "Dimitrios Akrivousis, Nikolaos Mylonas, Ioannis Mollas, Grigorios Tsoumakas",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302566"
    },
    {
        "id": 20479,
        "title": "Explaining Transformer-based Code Models: What Do They Learn? When They Do Not Work?",
        "authors": "Ahmad Haji Mohammadkhani, Chakkrit Tantithamthavorn, Hadi Hemmatif",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/scam59687.2023.00020"
    },
    {
        "id": 20480,
        "title": "CSECU-DSG at SemEval-2021 Task 1: Fusion of Transformer Models for Lexical Complexity Prediction",
        "authors": "Abdul Aziz, MD. Akram Hossain, Abu Nowshed Chy",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.semeval-1.80"
    },
    {
        "id": 20481,
        "title": "Automated Essay Scoring Using Transformer Models",
        "authors": "Sabrina Ludwig, Christian Mayer, Christopher Hansen, Kerstin Eilers, Steffen Brandt",
        "published": "2021-12-14",
        "citations": 14,
        "abstract": "Automated essay scoring (AES) is gaining increasing attention in the education sector as it significantly reduces the burden of manual scoring and allows ad hoc feedback for learners. Natural language processing based on machine learning has been shown to be particularly suitable for text classification and AES. While many machine-learning approaches for AES still rely on a bag of words (BOW) approach, we consider a transformer-based approach in this paper, compare its performance to a logistic regression model based on the BOW approach, and discuss their differences. The analysis is based on 2088 email responses to a problem-solving task that were manually labeled in terms of politeness. Both transformer models considered in the analysis outperformed without any hyperparameter tuning of the regression-based model. We argue that, for AES tasks such as politeness classification, the transformer-based approach has significant advantages, while a BOW approach suffers from not taking word order into account and reducing the words to their stem. Further, we show how such models can help increase the accuracy of human raters, and we provide a detailed instruction on how to implement transformer-based models for one’s own purposes.",
        "link": "http://dx.doi.org/10.3390/psych3040056"
    },
    {
        "id": 20482,
        "title": "TCS_WITM_2021 @FinSim-2: Transformer based Models for Automatic Classification of Financial Terms",
        "authors": "Tushar Goel, Vipul Chauhan, Ishan Verma, Tirthankar Dasgupta, Lipika Dey",
        "published": "2021-4-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3442442.3451386"
    },
    {
        "id": 20483,
        "title": "Case study on transformer models for calculation of high frequency transmitted overvoltages",
        "authors": "Bruno Jurišić, Ivo Uglešić, Alain Xemard, Françoise Paladian",
        "published": "2022-7-4",
        "citations": 0,
        "abstract": "Events such as lightning, switching of vacuum circuit breaker or switching operations in gas insulated substation (GIS) generate high frequency overvoltages. An equipment in a transmission or a distribution system has to be protected against such phenomena. Unfortunately, the traditional transformer models available in Electromagnetic transient simulations program (EMTP-like) software packages are not capable of representing transformer behavior in a transient state, which includes high frequencies. Moreover, high frequency transformer models are often too complex or require confidential information on transformer geometry. However, in the design stage of insulation coordination it is particularly important to accurately calculate transmitted overvoltages through transformers. In the scope of this paper two different transformer models for high frequency, are developed in an EMTP-type software program. The first model named “Black box” derives solely from the values measured on the transformer terminals and does not require any knowledge of the transformer inner geometry. The second model named “Grey box”, is based on a lumped RLC parameters network, whose values are derived from the simple geometry of the transformer window and from the nameplate data. Furthermore, the models’ capabilities to characterize a transformer at high frequencies are analyzed. The case study is done on a distribution transformer which is to be located inside a power plant. The transmitted overvoltages calculated with the models in the EMTP-type software program are compared with measurements.",
        "link": "http://dx.doi.org/10.37798/2014631-4186"
    },
    {
        "id": 20484,
        "title": "Efficient Open Domain Question Answering With Delayed Attention in Transformer-Based Models",
        "authors": "Wissam Siblini, Mohamed Challal, Charlotte Pasqual",
        "published": "2022-4-15",
        "citations": 0,
        "abstract": "Open Domain Question Answering (ODQA) on a large-scale corpus of documents (e.g. Wikipedia) is a key challenge in computer science. Although Transformer-based language models such as Bert have shown an ability to outperform humans to extract answers from small pre-selected passages of text, they suffer from their high complexity if the search space is much larger. The most common way to deal with this problem is to add a preliminary information retrieval step to strongly filter the corpus and keep only the relevant passages. In this article, the authors consider a more direct and complementary solution which consists in restricting the attention mechanism in Transformer-based models to allow a more efficient management of computations. The resulting variants are competitive with the original models on the extractive task and allow, in the ODQA setting, a significant acceleration of predictions and sometimes even an improvement in the quality of response.",
        "link": "http://dx.doi.org/10.4018/ijdwm.298005"
    },
    {
        "id": 20485,
        "title": "Performance of 4 Pre-Trained Sentence Transformer Models in the Semantic Query of a Systematic Review Dataset on Peri-Implantitis",
        "authors": "Carlo Galli, Nikolaos Donos, Elena Calciolari",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "Systematic reviews are cumbersome yet essential to the epistemic process of medical science. Finding significant reports, however, is a daunting task because the sheer volume of published literature makes the manual screening of databases time-consuming. The use of Artificial Intelligence could make literature processing faster and more efficient. Sentence transformers are groundbreaking algorithms that can generate rich semantic representations of text documents and allow for semantic queries. In the present report, we compared four freely available sentence transformer pre-trained models (all-MiniLM-L6-v2, all-MiniLM-L12-v2, all-mpnet-base-v2, and All-distilroberta-v1) on a convenience sample of 6110 articles from a published systematic review. The authors of this review manually screened the dataset and identified 24 target articles that addressed the Focused Questions (FQ) of the review. We applied the four sentence transformers to the dataset and, using the FQ as a query, performed a semantic similarity search on the dataset. The models identified similarities between the FQ and the target articles to a varying degree, and, sorting the dataset by semantic similarities using the best-performing model (all-mpnet-base-v2), the target articles could be found in the top 700 papers out of the 6110 dataset. Our data indicate that the choice of an appropriate pre-trained model could remarkably reduce the number of articles to screen and the time to completion for systematic reviews.",
        "link": "http://dx.doi.org/10.3390/info15020068"
    },
    {
        "id": 20486,
        "title": "Investigation on MLP, CNNs and Vision Transformer models performance for Extracting a Human Emotions via Facial Expressions",
        "authors": "Aomsup Panlima, Kanjanapan Sukvichai",
        "published": "2023-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ica-symp56348.2023.10044742"
    },
    {
        "id": 20487,
        "title": "scubeMSEC@LT-EDI-ACL2022: Detection of Depression using Transformer Models",
        "authors": "Sivamanikandan S, Santhosh V, Sanjaykumar N, Jerin Mahibha C, Thenmozhi Durairaj",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.ltedi-1.29"
    },
    {
        "id": 20488,
        "title": "Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models",
        "authors": "Christian W. F. Mayer, Sabrina Ludwig, Steffen Brandt",
        "published": "2023-1-3",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/15391523.2022.2142872"
    },
    {
        "id": 20489,
        "title": "Current Transformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-16"
    },
    {
        "id": 20490,
        "title": "Power Transformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-7"
    },
    {
        "id": 20491,
        "title": "Modified Transformer Architecture to Explain Black Box Models in Narrative Form",
        "authors": "Diksha Malhotra, Poonam Saini, Awadhesh Kumar Singh",
        "published": "2022-7-21",
        "citations": 0,
        "abstract": "The current XAI techniques present explanations mainly as visuals and structured data. However, these explanations are difficult to interpret for a non-expert user. Here, the use of natural language generation (NLG)-based techniques can help to represent explanations in a human-understandable format. The paper addresses the issue of automatic generation of narratives using a modified transformer approach. Further, due to the unavailability of a relevant annotated dataset for development and testing, the authors also propose a verbalization template approach to generate the same. The input of the transformer is linearized to convert the data-to-text task into text-to-text task. The proposed work is evaluated on a verbalized explained PIMA Indians diabetes dataset and exhibits significant improvement as compared to existing baselines for both manual and automatic evaluation. Also, the narratives provide better comprehensibility to be trusted by human evaluators than the non-NLG counterparts. Lastly, an ablation study is performed in order to understand the contribution of each component.",
        "link": "http://dx.doi.org/10.4018/ijswis.297040"
    },
    {
        "id": 20492,
        "title": "A Comparative Study of Transformer Based Pretrained AI Models for Content Summarization",
        "authors": "Ashika Sameem Abdul Rasheed, Mohammad Mehedy Masud, Mohammed Abduljabbar",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iit59782.2023.10366411"
    },
    {
        "id": 20493,
        "title": "An Ensemble of Arabic Transformer-based Models for Arabic Sentiment Analysis",
        "authors": "Ikram El Karfi, Sanaa El Fkihi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2022.0130865"
    },
    {
        "id": 20494,
        "title": "Comparison of Transformer Models for Information Extraction from Court Room Records in Pakistan",
        "authors": "Nida Ahmed, Seemab Latif, Rabia Irfan, Adnan Ul-Hasan, Faisal Shafait",
        "published": "2022-11-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceccme55909.2022.9988642"
    },
    {
        "id": 20495,
        "title": "Can Lexicon-Based Sentiment Analysis Boost Performances of Transformer-Based Models?",
        "authors": "Lindung Parningotan Manik, Harry Susianto, Arawinda Dinakaramani, Niken Pramanik, Totok Suhardijanto",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conmedia60526.2023.10428401"
    },
    {
        "id": 20496,
        "title": "Team LRL_NC at SemEval-2022 Task 4: Binary and Multi-label Classification of PCL using Fine-tuned Transformer-based Models",
        "authors": "Kushagri Tandon, Niladri Chatterjee",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.57"
    },
    {
        "id": 20497,
        "title": "IrEne-viz: Visualizing Energy Consumption of Transformer Models",
        "authors": "Yash Kumar Lal, Reetu Singh, Harsh Trivedi, Qingqing Cao, Aruna Balasubramanian, Niranjan Balasubramanian",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-demo.29"
    },
    {
        "id": 20498,
        "title": "Learning Deep Transformer Models for Machine Translation",
        "authors": "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao",
        "published": "2019",
        "citations": 221,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1176"
    },
    {
        "id": 20499,
        "title": "Behavioral Analysis of Transformer Models on Complex Grammatical Structures",
        "authors": "Kanyanut Kriengket, Kanchana Saengthongpattana, Peerachet Porkaew, Vorapon Luantangsrisuk, Prachya Boonkwan, Thepchai Supnithi",
        "published": "2020-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isai-nlp51646.2020.9376782"
    },
    {
        "id": 20500,
        "title": "Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention",
        "authors": "Soo Hyun Ryu, Richard Lewis",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.cmcl-1.6"
    },
    {
        "id": 20501,
        "title": "Using Transformer-based Models for Taxonomy Enrichment and Sentence Classification",
        "authors": "Parag Pravin Dakle, Shrikumar Patil, Sai Krishna Rallabandi, Chaitra Hegde, Preethi Raghavan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.finnlp-1.34"
    },
    {
        "id": 20502,
        "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
        "authors": "Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.3"
    },
    {
        "id": 20503,
        "title": "End-to-End Transformer-Based Models in Textual-Based NLP",
        "authors": "Abir Rahali, Moulay A. Akhloufi",
        "published": "2023-1-5",
        "citations": 15,
        "abstract": "Transformer architectures are highly expressive because they use self-attention mechanisms to encode long-range dependencies in the input sequences. In this paper, we present a literature review on Transformer-based (TB) models, providing a detailed overview of each model in comparison to the Transformer’s standard architecture. This survey focuses on TB models used in the field of Natural Language Processing (NLP) for textual-based tasks. We begin with an overview of the fundamental concepts at the heart of the success of these models. Then, we classify them based on their architecture and training mode. We compare the advantages and disadvantages of popular techniques in terms of architectural design and experimental value. Finally, we discuss open research, directions, and potential future work to help solve current TB application challenges in NLP.",
        "link": "http://dx.doi.org/10.3390/ai4010004"
    },
    {
        "id": 20504,
        "title": "Identification and Visualization of Key Topics in Scientific Publications with Transformer-Based Language Models and Document Clustering Methods",
        "authors": "Min-Hsien Weng, Shaoqun Wu, Mark Dyer",
        "published": "2022-11-5",
        "citations": 3,
        "abstract": "With the rapidly growing number of scientific publications, researchers face an increasing challenge of discovering the current research topics and methodologies in a scientific domain. This paper describes an unsupervised topic detection approach that utilizes the new development of transformer-based GPT-3 (Generative Pretrained Transformer 3) similarity embedding models and modern document clustering techniques. In total, 593 publication abstracts across urban study and machine learning domains were used as a case study to demonstrate the three phases of our approach. The iterative clustering phase uses the GPT-3 embeddings to represent the semantic meaning of abstracts and deploys the HDBSCAN (Hierarchical Density-based Spatial Clustering of Applications with Noise) clustering algorithm along with silhouette scores to group similar abstracts. The keyword extraction phase identifies candidate words from each abstract and selects keywords using the Maximal Marginal Relevance ranking algorithm. The keyword grouping phase produces the keyword groups to represent topics in each abstract cluster, again using GPT-3 embeddings, the HDBSCAN algorithm, and silhouette scores. The results are visualized in a web-based interactive tool that allows users to explore abstract clusters and examine the topics in each cluster through keyword grouping. Our unsupervised topic detection approach does not require labeled datasets for training and has the potential to be used in bibliometric analysis in a large collection of publications.",
        "link": "http://dx.doi.org/10.3390/app122111220"
    },
    {
        "id": 20505,
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "authors": "Elad Ben Zaken, Yoav Goldberg, Shauli Ravfogel",
        "published": "2022",
        "citations": 56,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-short.1"
    },
    {
        "id": 20506,
        "title": "Identifying missing relationships of CAPEC attack patterns by transformer models and graph structure",
        "authors": "Rikuho Miyata, Hironori Washizaki, Kensuke Sumoto, Nobukazu Yoshioka, Yoshiaki Fukazawa, Takao Okubo",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/svm59160.2023.00008"
    },
    {
        "id": 20507,
        "title": "NLP-LISAC at SemEval-2023 Task 12: Sentiment Analysis for Tweets expressed in African languages via Transformer-based Models",
        "authors": "Abdessamad Benlahbib, Achraf Boumhidi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.28"
    },
    {
        "id": 20508,
        "title": "Transformer-Based Punctuation Restoration Models for Indonesian with English Codeswitching Speech Transcripts",
        "authors": "Liu Changsong, Ho Thi Nga, Yip Jia Qi, Chng Eng Siong",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390242"
    },
    {
        "id": 20509,
        "title": "MuLan-Methyl—multiple transformer-based language models for accurate DNA methylation prediction",
        "authors": "Wenhuan Zeng, Anupam Gautam, Daniel H Huson",
        "published": "2022-12-28",
        "citations": 2,
        "abstract": "Abstract\nTransformer-based language models are successfully used to address massive text-related tasks. DNA methylation is an important epigenetic mechanism, and its analysis provides valuable insights into gene regulation and biomarker identification. Several deep learning–based methods have been proposed to identify DNA methylation, and each seeks to strike a balance between computational effort and accuracy. Here, we introduce MuLan-Methyl, a deep learning framework for predicting DNA methylation sites, which is based on 5 popular transformer-based language models. The framework identifies methylation sites for 3 different types of DNA methylation: N6-adenine, N4-cytosine, and 5-hydroxymethylcytosine. Each of the employed language models is adapted to the task using the “pretrain and fine-tune” paradigm. Pretraining is performed on a custom corpus of DNA fragments and taxonomy lineages using self-supervised learning. Fine-tuning aims at predicting the DNA methylation status of each type. The 5 models are used to collectively predict the DNA methylation status. We report excellent performance of MuLan-Methyl on a benchmark dataset. Moreover, we argue that the model captures characteristic differences between different species that are relevant for methylation. This work demonstrates that language models can be successfully adapted to applications in biological sequence analysis and that joint utilization of different language models improves model performance. Mulan-Methyl is open source, and we provide a web server that implements the approach.",
        "link": "http://dx.doi.org/10.1093/gigascience/giad054"
    },
    {
        "id": 20510,
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": "Byung-Doh Oh, William Schuler",
        "published": "2023-3-27",
        "citations": 12,
        "abstract": "AbstractThis work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
        "link": "http://dx.doi.org/10.1162/tacl_a_00548"
    },
    {
        "id": 20511,
        "title": "Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)",
        "authors": "Wesley Morris, Scott Crossley, Langdon Holmes, Anne Trumbore",
        "published": "2023-3-13",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3576050.3576098"
    },
    {
        "id": 20512,
        "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
        "authors": "Candida M. Greco, Andrea Tagarelli",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "AbstractTransformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
        "link": "http://dx.doi.org/10.1007/s10506-023-09374-7"
    },
    {
        "id": 20513,
        "title": "Building an Ensemble of Transformer Models for Arabic Dialect Classification and Sentiment Analysis",
        "authors": "Abdullah Salem Khered, Ingy Yasser Hassan Abdou Abdelhalim, Riza Batista-Navarro",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.53"
    },
    {
        "id": 20514,
        "title": "Transformer-Based Composite Language Models for Text Evaluation and Classification",
        "authors": "Mihailo Škorić, Miloš Utvić, Ranka Stanković",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "Parallel natural language processing systems were previously successfully tested on the tasks of part-of-speech tagging and authorship attribution through mini-language modeling, for which they achieved significantly better results than independent methods in the cases of seven European languages. The aim of this paper is to present the advantages of using composite language models in the processing and evaluation of texts written in arbitrary highly inflective and morphology-rich natural language, particularly Serbian. A perplexity-based dataset, the main asset for the methodology assessment, was created using a series of generative pre-trained transformers trained on different representations of the Serbian language corpus and a set of sentences classified into three groups (expert translations, corrupted translations, and machine translations). The paper describes a comparative analysis of calculated perplexities in order to measure the classification capability of different models on two binary classification tasks. In the course of the experiment, we tested three standalone language models (baseline) and two composite language models (which are based on perplexities outputted by all three standalone models). The presented results single out a complex stacked classifier using a multitude of features extracted from perplexity vectors as the optimal architecture of composite language models for both tasks.",
        "link": "http://dx.doi.org/10.3390/math11224660"
    },
    {
        "id": 20515,
        "title": "Recognition and Prediction of Surgical Gestures and Trajectories Using Transformer Models in Robot-Assisted Surgery",
        "authors": "Chang Shi, Yi Zheng, Ann Majewicz Fey",
        "published": "2022-10-23",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981611"
    },
    {
        "id": 20516,
        "title": "Constant Voltage Transformer (CVT)",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-11"
    },
    {
        "id": 20517,
        "title": "Transformer Fundamentals",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-1"
    },
    {
        "id": 20518,
        "title": "Comparison of Transformer Models for Performance on Domain Specific Texts: A Systematic Evaluation of Intrinsic Model Performance",
        "authors": "Giavid Valiyev, Philip Eles, Arvid Kok, Riccardo D'Ercole",
        "published": "2023-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmcis59922.2023.10253491"
    },
    {
        "id": 20519,
        "title": "Object detection using convolutional neural networks and transformer-based models: a review",
        "authors": "Shrishti Shah, Jitendra Tembhurne",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "AbstractTransformer models are evolving rapidly in standard natural language processing tasks; however, their application is drastically proliferating in computer vision (CV) as well. Transformers are either replacing convolution networks or being used in conjunction with them. This paper aims to differentiate the design of convolutional neural networks (CNNs) built models and models based on transformer, particularly in the domain of object detection. CNNs are designed to capture local spatial patterns through convolutional layers, which is well suited for tasks that involve understanding visual hierarchies and features. However, transformers bring a new paradigm to CV by leveraging self-attention mechanisms, which allows to capture both local and global context in images. Here, we target the various aspects such as basic level of understanding, comparative study, application of attention model, and highlighting tremendous growth along with delivering efficiency are presented effectively for object detection task. The main emphasis of this work is to offer basic understanding of architectures for object detection task and motivates to adopt the same in computer vision tasks. In addition, this paper highlights the evolution of transformer-based models in object detection and their growing importance in the field of computer vision, we also identified the open research direction in the same field.",
        "link": "http://dx.doi.org/10.1186/s43067-023-00123-z"
    },
    {
        "id": 20520,
        "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
        "authors": "Ling Liu, Mans Hulden",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-short.84"
    },
    {
        "id": 20521,
        "title": "Research on Pricing and Replenishment Strategies of Supermarkets Based on PSO-LSTM- Transformer Models",
        "authors": "Jiacheng Song, Jing Huang, Yukai Gao, Wenxun Jiang",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "The supermarket's role in ensuring market supply and enhancing livelihoods prompted the implementation of a new PSO-LSTM-Transformer hybrid model in this study. This amalgamation combines Particle Swarm Optimization (PSO), Long Short-Term Memory (LSTM), and Transformer to forecast commodity prices and inventory needs. By combining these methods, the research aimed to satisfy market requirements while maximizing grocery store profits and improving market operations sustainability.Particle Swarm Optimization (PSO) enhances prediction accuracy by effectively exploring the vast solution space for optimal solutions. Long Short-Term Memory (LSTM), which is known for capturing long-term data dependencies, enhances the comprehension and forecasting of market trends. Furthermore, the Transformer model enhances the forecasting process by capturing intricate patterns and relationships in market data through its attention mechanism.The research concludes that the PSO-LSTM-Transformer model effectively predicts commodity pricing and replenishment needs, thereby aiding supermarkets in making informed decisions to balance market demands and optimize revenue. The findings of the study contribute to the promotion of forecasting methods within the supermarket sector, facilitating efficient management of the market and sustainable economic growth.",
        "link": "http://dx.doi.org/10.54097/qy5emy53"
    },
    {
        "id": 20522,
        "title": "TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection",
        "authors": "Siddhant Garg, Thuy Vu, Alessandro Moschitti",
        "published": "2020-4-3",
        "citations": 64,
        "abstract": "We propose TandA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the state of the art on two well-known benchmarks, WikiQA and TREC-QA, achieving the impressive MAP scores of 92% and 94.3%, respectively, which largely outperform the the highest scores of 83.4% and 87.5% of previous work. We empirically show that TandA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TandA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TandA in an industrial setting, using domain specific datasets subject to different types of noise.",
        "link": "http://dx.doi.org/10.1609/aaai.v34i05.6282"
    },
    {
        "id": 20523,
        "title": "Modified Transformer Architecture to Explain Black Box Models in Narrative Form",
        "authors": "Diksha Malhotra, Poonam Saini, Awadhesh Kumar Singh",
        "published": "2022-7-21",
        "citations": 0,
        "abstract": "The current XAI techniques present explanations mainly as visuals and structured data. However, these explanations are difficult to interpret for a non-expert user. Here, the use of natural language generation (NLG)-based techniques can help to represent explanations in a human-understandable format. The paper addresses the issue of automatic generation of narratives using a modified transformer approach. Further, due to the unavailability of a relevant annotated dataset for development and testing, the authors also propose a verbalization template approach to generate the same. The input of the transformer is linearized to convert the data-to-text task into text-to-text task. The proposed work is evaluated on a verbalized explained PIMA Indians diabetes dataset and exhibits significant improvement as compared to existing baselines for both manual and automatic evaluation. Also, the narratives provide better comprehensibility to be trusted by human evaluators than the non-NLG counterparts. Lastly, an ablation study is performed in order to understand the contribution of each component.",
        "link": "http://dx.doi.org/10.4018/ijswis.297040"
    },
    {
        "id": 20524,
        "title": "An Ensemble of Arabic Transformer-based Models for Arabic Sentiment Analysis",
        "authors": "Ikram El Karfi, Sanaa El Fkihi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2022.0130865"
    },
    {
        "id": 20525,
        "title": "A Comparative Study of Transformer Based Pretrained AI Models for Content Summarization",
        "authors": "Ashika Sameem Abdul Rasheed, Mohammad Mehedy Masud, Mohammed Abduljabbar",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iit59782.2023.10366411"
    },
    {
        "id": 20526,
        "title": "Comparison of Transformer Models for Information Extraction from Court Room Records in Pakistan",
        "authors": "Nida Ahmed, Seemab Latif, Rabia Irfan, Adnan Ul-Hasan, Faisal Shafait",
        "published": "2022-11-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceccme55909.2022.9988642"
    },
    {
        "id": 20527,
        "title": "Can Lexicon-Based Sentiment Analysis Boost Performances of Transformer-Based Models?",
        "authors": "Lindung Parningotan Manik, Harry Susianto, Arawinda Dinakaramani, Niken Pramanik, Totok Suhardijanto",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conmedia60526.2023.10428401"
    },
    {
        "id": 20528,
        "title": "Flyback Converters, Transformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-13"
    },
    {
        "id": 20529,
        "title": "Three-Phase Transformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-12"
    },
    {
        "id": 20530,
        "title": "Transformer Design Trade-Offs",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-5"
    },
    {
        "id": 20531,
        "title": "TOPOLOGICAL TRANSIENT MODELS OF THREE-PHASE FIVE-LIMB TRANSFORMER",
        "authors": "S. E. Zirka, Y. I. Moroz, C. M. Arturi, D. Bonnman",
        "published": "2018-2-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15588/1607-6761-2017-2-2"
    },
    {
        "id": 20532,
        "title": "Investigating transformer-based models for automated e-governance in Indian Railway using Twitter",
        "authors": "Swati Agarwal, Ashrut Kumar, Rijul Ganguly",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-15331-y"
    },
    {
        "id": 20533,
        "title": "Predicting Future Laboratory Fault Friction Through Deep Learning Transformer Models",
        "authors": "Kun Wang, Christopher W. Johnson, Kane C. Bennett, Paul A. Johnson",
        "published": "2022-10-16",
        "citations": 8,
        "abstract": "AbstractMachine learning models using seismic emissions as input can predict instantaneous fault characteristics such as displacement and friction in laboratory experiments, and slow slip in Earth. Here, we address whether the seismic/acoustic emission (AE) from laboratory experiments contains information about future frictional behavior. The approach uses a convolutional encoder‐decoder containing a transformer model in the latent space, similar to models used for natural language processing. We test the model limits using progressively larger AE input time windows and progressively larger output friction time windows. The results demonstrate that very near‐term friction predictions are indeed contained in the AE signal, and predictions are progressively worse farther into the future. The future predictions by the model of impending failure in the near‐term are remarkably robust. This first effort predicting future fault frictional behavior with machine learning will aid in guiding efforts for applications in Earth.",
        "link": "http://dx.doi.org/10.1029/2022gl098233"
    },
    {
        "id": 20534,
        "title": "Mathematical models of the new remote transformer current transducers",
        "authors": "Sultan Faizullaevich Amirov, Nargisa Kamilovna Babanazarova",
        "published": "2021-7-1",
        "citations": 1,
        "abstract": "Abstract\nNew designs of remote transformer current converters (RTCC) have been developed in this article. It is shown that an increase in sensitivity is achieved due to the implementation of the magnetic circuit in the form of a multiturn core with a common measuring winding, and the use of three identical RTCC installed to measure currents of all three phases with connected measuring windings according to open triangle circuit expands the functionality of device due to the simultaneous measurement of currents in three phases and unbalanced currents in a three-phase circuit. Generalized mathematical models of new RCTT have been developed taking into account the influence of magnetic fields of currents of neighboring buses for the most general case of their location in space in relation to the buses of a three-phase line. It is shown that for RTCC type TVM-P, in which the parallel rods are made in the form of a multi-turn core with a corresponding measuring winding, in order to reduce the effect on the readings of the current transducer installed to measure the current in phases A, the number of turns of the measuring winding located at a distance from of buses B and C, the multiturn core of the U-shaped magnetic circuit should be selected large in relation to the measuring winding placed on the multiturn core closest to the buses B and C.",
        "link": "http://dx.doi.org/10.1088/1755-1315/808/1/012001"
    },
    {
        "id": 20535,
        "title": "Ensemble and Personalized Transformer Models for Subject Identification and Relapse Detection in E-Prevention Challenge",
        "authors": "Salvatore Calcagno, Raffaele Mineo, Daniela Giordano, Concetto Spampinato",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095438"
    },
    {
        "id": 20536,
        "title": "TurboTransformers",
        "authors": "Jiarui Fang, Yang Yu, Chengduo Zhao, Jie Zhou",
        "published": "2021-2-17",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3437801.3441578"
    },
    {
        "id": 20537,
        "title": "LightSeq2: Accelerated Training for Transformer-Based Models on GPUs",
        "authors": "Xiaohui Wang, Yang Wei, Ying Xiong, Guyue Huang, Xian Qian, Yufei Ding, Mingxuan Wang, Lei Li",
        "published": "2022-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sc41404.2022.00043"
    },
    {
        "id": 20538,
        "title": "MODELS OF INFLUENCE OF TECHNOLOGICAL AND CONTROL OPERATIONS OF PRODUCTION ON REAL RESOURCE OF POWER TRANSFORMER",
        "authors": "Vladimir Vasilevskij,  ",
        "published": "2017-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15276/eltecs.25.101.2017.59"
    },
    {
        "id": 20539,
        "title": "Multi-Document Summarization Made Easy: An Abstractive Query-Focused System Using Web Scraping and Transformer Models",
        "authors": "P Isaac Ritharson, D. Sujitha Juliet, J. Anitha, S. Immanuel Alex Pandian",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205946"
    },
    {
        "id": 20540,
        "title": "Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models",
        "authors": "James Fiacco, David Adamson, Carolyn Ros",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.20"
    },
    {
        "id": 20541,
        "title": "Efficient Finite Element Models for Calculation of the No-load losses of the Transformer",
        "authors": "Kamran Dawood, Mehmet Aytac Cınar, Bora Alboyacı, Olus Sonmez",
        "published": "2017-8-23",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24107/ijeas.309933"
    },
    {
        "id": 20542,
        "title": "Deep Text Retrieval Models based on DNN, CNN, RNN and Transformer: A review",
        "authors": "Jianping Liu, Xintao Chu, Yingfei Wang, Meng Wang",
        "published": "2022-11-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccis57298.2022.10016379"
    },
    {
        "id": 20543,
        "title": "Prompt-Based Tuning of Transformer Models for Multi-Center Medical Image Segmentation of Head and Neck Cancer",
        "authors": "Numan Saeed, Muhammad Ridzuan, Roba Al Majzoub, Mohammad Yaqub",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "Medical image segmentation is a vital healthcare endeavor requiring precise and efficient models for appropriate diagnosis and treatment. Vision transformer (ViT)-based segmentation models have shown great performance in accomplishing this task. However, to build a powerful backbone, the self-attention block of ViT requires large-scale pre-training data. The present method of modifying pre-trained models entails updating all or some of the backbone parameters. This paper proposes a novel fine-tuning strategy for adapting a pretrained transformer-based segmentation model on data from a new medical center. This method introduces a small number of learnable parameters, termed prompts, into the input space (less than 1% of model parameters) while keeping the rest of the model parameters frozen. Extensive studies employing data from new unseen medical centers show that the prompt-based fine-tuning of medical segmentation models provides excellent performance regarding the new-center data with a negligible drop regarding the old centers. Additionally, our strategy delivers great accuracy with minimum re-training on new-center data, significantly decreasing the computational and time costs of fine-tuning pre-trained models. Our source code will be made publicly available.",
        "link": "http://dx.doi.org/10.3390/bioengineering10070879"
    },
    {
        "id": 20544,
        "title": "A Transformer Based Approach To Detect Suicidal Ideation Using Pre-Trained Language Models",
        "authors": "Farsheed Haque, Ragib Un Nur, Shaeekh Al Jahan, Zarar Mahmud, Faisal Muhammad Shah",
        "published": "2020-12-19",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit51783.2020.9392692"
    },
    {
        "id": 20545,
        "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
        "authors": "Oliver Eberle, Stephanie Brandl, Jonas Pilot, Anders Søgaard",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-long.296"
    },
    {
        "id": 20546,
        "title": "Arabic Narrative Question Answering (QA) Using Transformer Models",
        "authors": "Mohammad A. Ateeq, Sabrina Tiun, Hamed Abdelhaq, Nawras Rahhal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3348410"
    },
    {
        "id": 20547,
        "title": "Evaluation of Neural Network Transformer Models for Named-Entity Recognition on Low-Resourced Languages",
        "authors": "Ridewaan Hanslo",
        "published": "2021-9-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15439/2021f7"
    },
    {
        "id": 20548,
        "title": "Advancing AI-Generated Image Detection: Enhanced Accuracy through CNN and Vision Transformer Models with Explainable AI Insights",
        "authors": "Md. Zahid Hossain, Farhad Uz Zaman, Md. Rakibul Islam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10440990"
    },
    {
        "id": 20549,
        "title": "Study on Heat Transfer Agent Models of Transmission Line and Transformer",
        "authors": "B Wang, P P Zhang",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1755-1315/133/1/012007"
    },
    {
        "id": 20550,
        "title": "TransDTI: Transformer-Based Language Models for Estimating DTIs and Building a Drug Recommendation Workflow",
        "authors": "Yogesh Kalakoti, Shashank Yadav, Durai Sundar",
        "published": "2022-1-25",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/acsomega.1c05203"
    },
    {
        "id": 20551,
        "title": "Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?",
        "authors": "Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, Lingxiao Jiang",
        "published": "2020-9",
        "citations": 54,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsme46990.2020.00017"
    },
    {
        "id": 20552,
        "title": "Sign Language-to-Text Dictionary with Lightweight Transformer Models",
        "authors": "Jérôme Fink, Pierre Poitier, Maxime André, Loup Meurice, Benoît Frénay, Anthony Cleve, Bruno Dumas, Laurence Meurant",
        "published": "2023-8",
        "citations": 0,
        "abstract": "The recent advances in deep learning have been beneficial to automatic sign language recognition (SLR). However, free-to-access, usable, and accessible tools are still not widely available to the deaf community. The need for a sign language-to-text dictionary was raised by a bilingual deaf school in Belgium and linguist experts in sign languages (SL) in order to improve the autonomy of students. To meet that need, an efficient SLR system was built based on a specific transformer model. The proposed system is able to recognize 700 different signs, with a top-10 accuracy of 83%. Those results are competitive with other systems in the literature while using 10 times less parameters than existing solutions. The integration of this model into a usable and accessible web application for the dictionary is also introduced. A user-centered human-computer interaction (HCI) methodology was followed to design and implement the user interface. To the best of our knowledge, this is the first publicly released sign language-to-text dictionary using video captured by a standard camera.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/662"
    },
    {
        "id": 20553,
        "title": "Shared functional specialization in transformer-based language models and the human brain",
        "authors": "Sreejan Kumar, Theodore R. Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A. Norman, Thomas L. Griffiths, Robert D. Hawkins, Samuel A. Nastase",
        "published": "No Date",
        "citations": 12,
        "abstract": "AbstractHumans use complex linguistic structures to transmit ideas to one another. The brain is thought to deploy specialized computations to process these structures. Recently, a new class of artificial neural networks based on the Transformer architecture has revolutionized the field of language modeling, attracting attention from neuroscientists seeking to understand the neurobiology of languagein silico. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. Prior work has focused on the internal representations (the “embeddings”) generated by these circuits. In this paper, we instead analyze the circuit computations directly: we deconstruct these computations into functionally-specialized “transformations” to provide a complementary window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we first verify that the transformations account for considerable variance in brain activity across the cortical language network. We then demonstrate that the emergent syntactic computations performed by individual, functionally-specialized “attention heads” differentially predict brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings indicate that large language models and the cortical language network may converge on similar trends of functional specialization for processing natural language.",
        "link": "http://dx.doi.org/10.1101/2022.06.08.495348"
    },
    {
        "id": 20554,
        "title": "Unexpectedness as a Measure of Semantic Learning When Training Transformer Models",
        "authors": "Ricardo A. Calix, Leili Javadpour",
        "published": "2023-2",
        "citations": 0,
        "abstract": " Many problems in NLP such as language translation and sentiment analysis have shown a lot of improvement in recent years. As simpler language problems are solved or better understood, the focus shifts to more complex problems such as semantic analysis and understanding. Unfortunately, a lot of studies in the literature suffer from a too much specificity problem. The algorithms and datasets are too domain specific. In this study, we analyze and elaborate on this notion of generality. Instead of selecting a highly specialized data set for semantic analysis, we take a generic and possibly dry data set, and we study how a plain vanilla Transformer performs in learning higher level semantic patterns beyond what was obvious or expected. We tune our Transformer model on a classic language task to ensure correct performance. Once tuned, the goal is to select sentences with specific key words and study whether higher level semantic patterns may have been learned by our model. We believe that we obtained promising results. The average BLEU score for sentences less than 25 words is equal to 39.79. Our initial qualitative analysis of possible semantic content of interest shows a 17 percent rate in finding interesting semantic patterns. We provide discussion of data driven results of unexpectedness as a measure of semantic learning. ",
        "link": "http://dx.doi.org/10.1142/s0218213023500070"
    },
    {
        "id": 20555,
        "title": "IIITN NLP at SMM4H 2021 Tasks: Transformer Models for Classification on Health-Related Imbalanced Twitter Datasets",
        "authors": "Varad Pimpalkhute, Prajwal Nakhate, Tausif Diwan",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.smm4h-1.24"
    },
    {
        "id": 20556,
        "title": "Analysis of transformer-based models for time series data, natural language processing, and computer vision",
        "authors": "Huanzhang Chen, Yunfan Hou, Tianhao Miao, Jiayu Xue",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The birth of the Transformer revolutionarily signalled the start of a new epic chapter in the deep learning era. Through an encoder-decoder architecture, including residual connection, multi-head self-attention, etc., it completely reformed the deep models and unified the models used in traditional computer vision (CV) and natural language processing (NLP) problems. In recent years, many papers published have adapted the original Transformer model to better complete tasks in time series analysis, CV, and NLP. In the area of natural language processing, Bidirectional Encoder Representations from Transformers (BERT) employs a two-way transformer structure to learn context-based language representation, whereas Generative Pre-trained Transformer (GPT) employs a one-way transformer but enhances corpus training to enhance the model effect. The Vision Transformer model is the cornerstone of computer vision. It separates the input image into various patches, projects each patch into vectorized features, and then passes the them to Transformer. Based on the idea of the Vision Transformer, Swin Transformer and Biformer further optimized the Transformer and achieved better results. Time series combines the ideas embodied in CV and NLP, and in doing so, improves the specificity and various difficulties of time series problems to lower algorithm complexity and increase prediction accuracy. This article summarizes the uses and improvements of the Transformer in NLP, CV and time series, explores the development history and ideas on algorithm optimization, and predicts the potential developments of Transformer in these three fields.",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231084"
    },
    {
        "id": 20557,
        "title": "On the use of transformer-based detection models for accurate sleep event annotation and analysis",
        "authors": "A. Neergaard Zahid, M. Jonika, P.F. Hulgaard, M.Y. Chen, M. Mørup",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sleep.2023.11.1106"
    },
    {
        "id": 20558,
        "title": "Sentiment Analysis using Transformer Based Pre-Trained Models for the Hindi Language",
        "authors": "Akshat Verma, Shivam Walbe, Ishwar Wani, Ritesh Wankhede, Radha Thakare, Sanika Patankar",
        "published": "2022-2-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sceecs54111.2022.9741028"
    },
    {
        "id": 20559,
        "title": "Energy Saving Based on Transformer Models with LeakyReLU Activation Function",
        "authors": "Jiasen Wang, Xinqi Li, Jun Wang",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icist59754.2023.10367091"
    },
    {
        "id": 20560,
        "title": "Adam-Smith at SemEval-2023 Task 4: Discovering Human Values in Arguments with Ensembles of Transformer-based Models",
        "authors": "Daniel Schroter, Daryna Dementieva, Georg Groh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.74"
    },
    {
        "id": 20561,
        "title": "The relationship between stockholder sentiment lag and stock price prediction accuracy: an empirical analysis based on LSTM and Transformer models",
        "authors": "Haoqian Guo, Yuxin Xu",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "This comprehensive paper investigates the nuanced relationship between retail investor sentiment and stock prices in the Chinese stock market, with a special focus on the role of sentiment time lags. Using advanced time-series models, specifically Long Short-Term Memory (LSTM) and Transformer models, the study takes a detailed look at the stock price of Oriental Finance (Ticker: 300059A). The research employs varying time lags of stockholder sentiment (ranging from 0 to 4 days) as well as technical indicators to predict stock prices. Our experimental design involves comparative analysis under these two models to isolate the impact of sentiment time lags on prediction accuracy. The results reveal that the LSTM model consistently outperforms the Transformer model, particularly when a 4-day lag in stockholder sentiment is considered. Interestingly, the prediction accuracy did not uniformly improve with increased sentiment lags, suggesting a complex relationship between investor sentiment and stock prices.",
        "link": "http://dx.doi.org/10.54097/hset.v70i.13888"
    },
    {
        "id": 20562,
        "title": "Enhancing Cancer Gene Prediction through Aligned Fusion of Multiple PPI Networks Using Graph Transformer Models",
        "authors": "Zebei Han, Gufeng Yu, Yang Yang",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385593"
    },
    {
        "id": 20563,
        "title": "Entity-aware answer sentence selection for question answering with transformer-based language models",
        "authors": "Zahra Abbasiantaeb, Saeedeh Momtazi",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10844-022-00724-6"
    },
    {
        "id": 20564,
        "title": "Calibration of Transformer-Based Models for Identifying Stress and Depression in Social Media",
        "authors": "Loukas Ilias, Spiros Mouzakitis, Dimitris Askounis",
        "published": "2024",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tcss.2023.3283009"
    },
    {
        "id": 20565,
        "title": "Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models",
        "authors": "Xi Yang, Xing He, Hansi Zhang, Yinghan Ma, Jiang Bian, Yonghui Wu",
        "published": "2020-11-23",
        "citations": 19,
        "abstract": "\nBackground\nSemantic textual similarity (STS) is one of the fundamental tasks in natural language processing (NLP). Many shared tasks and corpora for STS have been organized and curated in the general English domain; however, such resources are limited in the biomedical domain. In 2019, the National NLP Clinical Challenges (n2c2) challenge developed a comprehensive clinical STS dataset and organized a community effort to solicit state-of-the-art solutions for clinical STS.\n\n\nObjective\nThis study presents our transformer-based clinical STS models developed during this challenge as well as new models we explored after the challenge. This project is part of the 2019 n2c2/Open Health NLP shared task on clinical STS.\n\n\nMethods\nIn this study, we explored 3 transformer-based models for clinical STS: Bidirectional Encoder Representations from Transformers (BERT), XLNet, and Robustly optimized BERT approach (RoBERTa). We examined transformer models pretrained using both general English text and clinical text. We also explored using a general English STS dataset as a supplementary corpus in addition to the clinical training set developed in this challenge. Furthermore, we investigated various ensemble methods to combine different transformer models.\n\n\nResults\nOur best submission based on the XLNet model achieved the third-best performance (Pearson correlation of 0.8864) in this challenge. After the challenge, we further explored other transformer models and improved the performance to 0.9065 using a RoBERTa model, which outperformed the best-performing system developed in this challenge (Pearson correlation of 0.9010).\n\n\nConclusions\nThis study demonstrated the efficiency of utilizing transformer-based models to measure semantic similarity for clinical text. Our models can be applied to clinical applications such as clinical text deduplication and summarization.\n",
        "link": "http://dx.doi.org/10.2196/19735"
    },
    {
        "id": 20566,
        "title": "Multiterminal 3-Phase Transformer Model*",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-9"
    },
    {
        "id": 20567,
        "title": "Transformer Installation and Maintenance",
        "authors": "Troy D. Kabrich",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-22"
    },
    {
        "id": 20568,
        "title": "Toward Foundational Deep Learning Models for Medical Imaging in the New Era of Transformer Networks",
        "authors": "Martin J. Willemink, Holger R. Roth, Veit Sandfort",
        "published": "2022-11-1",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1148/ryai.210284"
    },
    {
        "id": 20569,
        "title": "Transformer-based Hebrew NLP models for Short Answer Scoring in Biology",
        "authors": "Abigail Gurin Schleifer, Beata Beigman Klebanov, Moriah Ariely, Giora Alexandron",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.46"
    },
    {
        "id": 20570,
        "title": "Towards Robust Diagnosis of Alzheimer's Disease Using Ensemble Framework of Convolutional Neural Network and Vision Transformer",
        "authors": "Poonguzhali Elangovan, Malaya Kumar Nath",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003363361-9"
    },
    {
        "id": 20571,
        "title": "Review of Vision Transformer Models for Remote Sensing Image Scene Classification",
        "authors": "Pengyuan Lv, Wenjun Wu, Yanfei Zhong, Liangpei Zhang",
        "published": "2022-7-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883054"
    },
    {
        "id": 20572,
        "title": "Strawberry disease identification with vision transformer-based models",
        "authors": "Hai Thanh Nguyen, Tri Dac Tran, Thanh Tuong Nguyen, Nhi Minh Pham, Phuc Hoang Nguyen Ly, Huong Hoang Luong",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18266-0"
    },
    {
        "id": 20573,
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "authors": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, Sumit Sanghai",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.298"
    },
    {
        "id": 20574,
        "title": "Parameter Estimation of Three-Phase Transformer Models for Low-Frequency Transient Studies From Terminal Measurements",
        "authors": "Qiong Wu, Saeed Jazebi, Francisco de Leon",
        "published": "2017-7",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tmag.2016.2563389"
    },
    {
        "id": 20575,
        "title": "Transformer models for enhancing AttnGAN based text to image generation",
        "authors": "S. Naveen, M. S. S Ram Kiran, M. Indupriya, T.V. Manikanta, P.V. Sudeep",
        "published": "2021-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.imavis.2021.104284"
    },
    {
        "id": 20576,
        "title": "Interpreting Potts and Transformer Protein Models Through the Lens of Simplified Attention",
        "authors": "Nicholas Bhattacharya, Neil Thomas, Roshan Rao, Justas Dauparas, Peter K. Koo, David Baker, Yun S. Song, Sergey Ovchinnikov",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811250477_0004"
    },
    {
        "id": 20577,
        "title": "Pre-trained Deep Learning Models for COVID19 Classification: CNNs vs. Vision Transformer",
        "authors": "Maisarah Mohd Sufian, Ervin Gubin Moung, Jamal Ahmad Dargham, Farashazillah Yahya, Sigeru Omatu",
        "published": "2022-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iicaiet55139.2022.9936852"
    },
    {
        "id": 20578,
        "title": "Bridging the Gap: A Fusion of CNN and Transformer Models for Real-Time Object Detection",
        "authors": "Yuanke Pan, Chengmin Zhou, Liyilei Su, Haseeb Hassan, Bingding Huang",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic58329.2023.10408921"
    },
    {
        "id": 20579,
        "title": "ADAPTATION OF DOMAIN-SPECIFIC TRANSFORMER MODELS WITH TEXT OVERSAMPLING FOR SENTIMENT ANALYSIS OF SOCIAL MEDIA POSTS ON COVID-19 VACCINE",
        "authors": "Anmol Bansal, Arjun Choudhry, Anubhav Sharma, Seba Susan",
        "published": "2023-3-10",
        "citations": 3,
        "abstract": "Covid-19 has spread across the world and many different vaccines have been developed to counter its surge. To identify the correct sentiments associated with the vaccines from social media posts, this paper aims to fine-tune pre-trained transformer models on tweets associated with different Covid vaccines, specifically RoBERTa, XLNet and BERT which are recently introduced state-of-the-art bi-directional transformer models, and domain-specific transformer models BERTweet and CT-BERT that are pre-trained on Covid-19 tweets. We further explore the option of data augmentation by text oversampling using LMOTE to improve the accuracies of these models, specifically, for small sample datasets where there is an imbalanced class distribution among the positive, negative and neutral sentiment classes. Our results summarize our findings on the suitability of text oversampling for imbalanced, small sample datasets that are used to fine-tune state-of-the-art pre-trained transformer models, and the utility of having domain-specific transformer models for the classification task.",
        "link": "http://dx.doi.org/10.7494/csci.2023.24.2.4761"
    },
    {
        "id": 20580,
        "title": "Transformer Connections",
        "authors": "Stephen Shull, Dan D. Perco",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-16"
    },
    {
        "id": 20581,
        "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches",
        "authors": "Francisca Adoma Acheampong, Henry Nunoo-Mensah, Wenyu Chen",
        "published": "2021-12",
        "citations": 166,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10462-021-09958-2"
    },
    {
        "id": 20582,
        "title": "CardiffNLP-Metaphor at SemEval-2022 Task 2: Targeted Fine-tuning of Transformer-based Language Models for Idiomaticity Detection",
        "authors": "Joanne Boisson, Jose Camacho-Collados, Luis Espinosa-Anke",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.20"
    },
    {
        "id": 20583,
        "title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting",
        "authors": "Beltrán Labrador, Guanlong Zhao, Ignacio López Moreno, Angelo Scorza Scarpati, Liam Fowl, Quan Wang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095444"
    },
    {
        "id": 20584,
        "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
        "authors": "Przemyslaw Joniak, Akiko Aizawa",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gebnlp-1.6"
    },
    {
        "id": 20585,
        "title": "Ablations over transformer models for biomedical relationship extraction",
        "authors": "Richard G Jackson, Erik Jansson, Aron Lagerberg, Elliot Ford, Vladimir Poroshin, Timothy Scrivener, Mats Axelsson, Martin Johansson, Lesly Arun Franco, Eliseo Papa",
        "published": "2020-7-16",
        "citations": 3,
        "abstract": "Background: Masked language modelling approaches have enjoyed success in improving benchmark performance across many general and biomedical domain natural language processing tasks, including biomedical relationship extraction (RE). However, the recent surge in both the number of novel architectures and the volume of training data they utilise may lead us to question whether domain specific pretrained models are necessary. Additionally, recent work has proposed novel classification heads for RE tasks, further improving performance. Here, we perform ablations over several pretrained models and classification heads to try to untangle the perceived benefits of each. Methods: We use a range of string preprocessing strategies, combined with Bidirectional Encoder Representations from Transformers (BERT), BioBERT and RoBERTa architectures to perform ablations over three RE datasets pertaining to drug-drug and chemical protein interactions, and general domain relationship extraction. We explore the use of the RBERT classification head, compared to a simple linear classification layer across all architectures and datasets. Results: We observe a moderate performance benefit in using the BioBERT pretrained model over the BERT base cased model, although there appears to be little difference when comparing BioBERT to RoBERTa large. In addition, we observe a substantial benefit of using the RBERT head on the general domain RE dataset, but this is not consistently reflected in the biomedical RE datasets. Finally, we discover that randomising the token order of training data does not result in catastrophic performance degradation in our selected tasks. Conclusions: We find a recent general domain pretrained model performs approximately the same as a biomedical specific one, suggesting that domain specific models may be of limited use given the tendency of recent model pretraining regimes to incorporate ever broader sets of data. In addition, we suggest that care must be taken in RE model training, to prevent fitting to non-syntactic features of datasets.",
        "link": "http://dx.doi.org/10.12688/f1000research.24552.1"
    },
    {
        "id": 20586,
        "title": "Bangla Emergency Post Classification on Social Media using Transformer Based BERT Models",
        "authors": "Alvi Ahmmed Nabil, Dola Das, Md. Shahidul Salim, Shamsul Arifeen, H. M. Abdul Fattah",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eict61409.2023.10427900"
    },
    {
        "id": 20587,
        "title": "Sam Miller at SemEval-2023 Task 5: Classification and Type-specific Spoiler Extraction Using XLNET and Other Transformer Models",
        "authors": "Pia Störmer, Tobias Esser, Patrick Thomasius",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.169"
    },
    {
        "id": 20588,
        "title": "Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension",
        "authors": "Ronghan Li, Zejun Jiang, Lifang Wang, Xinyu Lu, Meng Zhao, Daqing Chen",
        "published": "2021-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2021.106936"
    },
    {
        "id": 20589,
        "title": "Integrated Training for Sequence-to-Sequence Models Using Non-Autoregressive Transformer",
        "authors": "Evgeniia Tokarchuk, Jan Rosendahl, Weiyue Wang, Pavel Petrushkov, Tomer Lancewicki, Shahram Khadivi, Hermann Ney",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.iwslt-1.32"
    },
    {
        "id": 20590,
        "title": "A Comprehensive Analysis of Transformer-Deep Neural Network Models in Twitter Disaster Detection",
        "authors": "Vimala Balakrishnan, Zhongliang Shi, Chuan Liang Law, Regine Lim, Lee Leng Teh, Yue Fan, Jeyarani Periasamy",
        "published": "2022-12-9",
        "citations": 1,
        "abstract": "Social media platforms such as Twitter are a vital source of information during major events, such as natural disasters. Studies attempting to automatically detect textual communications have mostly focused on machine learning and deep learning algorithms. Recent evidence shows improvement in disaster detection models with the use of contextual word embedding techniques (i.e., transformers) that take the context of a word into consideration, unlike the traditional context-free techniques; however, studies regarding this model are scant. To this end, this paper investigates a selection of ensemble learning models by merging transformers with deep neural network algorithms to assess their performance in detecting informative and non-informative disaster-related Twitter communications. A total of 7613 tweets were used to train and test the models. Results indicate that the ensemble models consistently yield good performance results, with F-score values ranging between 76% and 80%. Simpler transformer variants, such as ELECTRA and Talking-Heads Attention, yielded comparable and superior results compared to the computationally expensive BERT, with F-scores ranging from 80% to 84%, especially when merged with Bi-LSTM. Our findings show that the newer and simpler transformers can be used effectively, with less computational costs, in detecting disaster-related Twitter communications.",
        "link": "http://dx.doi.org/10.3390/math10244664"
    },
    {
        "id": 20591,
        "title": "Simulation and Analysis of Multilevel DC Transformer Using Different Dual-Active-Bridge DC- DC Converter Models",
        "authors": "Yanhui Qin, Kaike Wang, Zhendong Tan, Yue Xia, Ying Chen, Yankan Song",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt-asia.2019.8881310"
    },
    {
        "id": 20592,
        "title": "Omuz İmplantı Sınıflandırmasında Görü Dönüştürücü Tabanlı Modellerin Performans Karşılaştırması",
        "authors": "Elif BAYKAL KABLAN, Yavuz KABLAN",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Total Shoulder Arthroplasty (TSA) is a surgical procedure addressing severe pain and restricted shoulder joint movement. During TSA surgery, X-ray images guide the selection of the prosthetic implant suitable for the patient from a variety of models produced by different manufacturers. However, prostheses may wear or loosen over time, thus requiring periodic evaluation and replacement. Currently, the process involves taking new X-ray images from patients, resulting in variability in expert opinions on implant types. Automated systems can provide objective assessments, saving time and effort. In this study, we present a performance comparison of Vision Transformer (ViT) based models for automatic shoulder implant classification from X-ray images. Fine-tuning pre-trained ViT models on a shared dataset showed high success in accuracy, precision, sensitivity, and F-measure metrics. This approach can provide reliable identification of shoulder implant manufacturers and model information and time efficiency, especially for specialists, contributing to improving treatment planning.",
        "link": "http://dx.doi.org/10.28948/ngumuh.1400666"
    },
    {
        "id": 20593,
        "title": "Side Break",
        "authors": "",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0013"
    },
    {
        "id": 20594,
        "title": "Development of a Weather Prediction Device Using Transformer Models and IoT Techniques",
        "authors": "Iyapo Kamoru Olarewaju, Kyung Ki Kim",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46670/jsst.2023.32.3.164"
    },
    {
        "id": 20595,
        "title": "Scaling Out Transformer Models for Retrosynthesis on Supercomputers",
        "authors": "Joris Mollinga, Valeriu Codreanu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80119-9_4"
    },
    {
        "id": 20596,
        "title": "On Finetuning Adapter-Based Transformer Models for Classifying Abusive Social Media Tamil Comments",
        "authors": "Malliga S, Kogilavani Shanmugavadivel, Ramya Chinnasamy, Nandhini Subbarayan, Adhithiya Ganesan, Deepti Ravi, Vasanth Palanikumar, Bharathi Raja Chakravarthi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247185"
    },
    {
        "id": 20597,
        "title": "Nonlinear Autoregressive Neural Network Models for Prediction of Transformer Oil-Dissolved Gas Concentrations",
        "authors": "Fabio Pereira, Francisco Bezerra, Shigueru Junior, Josemir Santos, Ivan Chabu, Gilberto Souza, Fábio Micerino, Silvio Nabeta",
        "published": "2018-6-28",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/en11071691"
    },
    {
        "id": 20598,
        "title": "Active learning for transformer models in direction query tagging",
        "authors": "Jasper Huang, Chiqun Zhang, Dragomir Yankov, Maryam Mousaarab Najafabadi, Tsheko Mutungu",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3557915.3561006"
    },
    {
        "id": 20599,
        "title": "Single-board device individual authentication based on hardware performance and autoencoder transformer models",
        "authors": "Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Gérôme Bovet, Gregorio Martínez Pérez",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cose.2023.103596"
    },
    {
        "id": 20600,
        "title": "Abusive Bangla comments detection on Facebook using transformer-based deep learning models",
        "authors": "Tanjim Taharat Aurpa, Rifat Sadik, Md Shoaib Ahmed",
        "published": "2022-12",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13278-021-00852-x"
    },
    {
        "id": 20601,
        "title": "Ablations over transformer models for biomedical relationship extraction",
        "authors": "Richard G Jackson, Erik Jansson, Aron Lagerberg, Elliot Ford, Vladimir Poroshin, Timothy Scrivener, Mats Axelsson, Martin Johansson, Lesly Arun Franco, Eliseo Papa",
        "published": "2020-7-16",
        "citations": 3,
        "abstract": "Background: Masked language modelling approaches have enjoyed success in improving benchmark performance across many general and biomedical domain natural language processing tasks, including biomedical relationship extraction (RE). However, the recent surge in both the number of novel architectures and the volume of training data they utilise may lead us to question whether domain specific pretrained models are necessary. Additionally, recent work has proposed novel classification heads for RE tasks, further improving performance. Here, we perform ablations over several pretrained models and classification heads to try to untangle the perceived benefits of each. Methods: We use a range of string preprocessing strategies, combined with Bidirectional Encoder Representations from Transformers (BERT), BioBERT and RoBERTa architectures to perform ablations over three RE datasets pertaining to drug-drug and chemical protein interactions, and general domain relationship extraction. We explore the use of the RBERT classification head, compared to a simple linear classification layer across all architectures and datasets. Results: We observe a moderate performance benefit in using the BioBERT pretrained model over the BERT base cased model, although there appears to be little difference when comparing BioBERT to RoBERTa large. In addition, we observe a substantial benefit of using the RBERT head on the general domain RE dataset, but this is not consistently reflected in the biomedical RE datasets. Finally, we discover that randomising the token order of training data does not result in catastrophic performance degradation in our selected tasks. Conclusions: We find a recent general domain pretrained model performs approximately the same as a biomedical specific one, suggesting that domain specific models may be of limited use given the tendency of recent model pretraining regimes to incorporate ever broader sets of data. In addition, we suggest that care must be taken in RE model training, to prevent fitting to non-syntactic features of datasets.",
        "link": "http://dx.doi.org/10.12688/f1000research.24552.1"
    },
    {
        "id": 20602,
        "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches",
        "authors": "Francisca Adoma Acheampong, Henry Nunoo-Mensah, Wenyu Chen",
        "published": "2021-12",
        "citations": 166,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10462-021-09958-2"
    },
    {
        "id": 20603,
        "title": "Transformer-Inductor Efficiency, Regulation, and Temperature Rise",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-6"
    },
    {
        "id": 20604,
        "title": "Transformer–System Interactions and Modeling",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-13"
    },
    {
        "id": 20605,
        "title": "Computer Vision-Based Monitoring of Construction Site Housekeeping: An Evaluation of CNN and Transformer-Based Models",
        "authors": "Zherui Shao, Yang Miang Goh, Jing Tian, Yu Guang Lim, Vincent Jie Long Gan",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784485248.061"
    },
    {
        "id": 20606,
        "title": "COMPARISON OF BERT TRANSFORMER MODELS IN IDENTIFYING DESTRUCTIVE CONTENT IN SOCIAL MEDIA",
        "authors": "Минаев, Владимир Александрович, Симонов, Александр Валерьевич",
        "published": "2022-10-24",
        "citations": 0,
        "abstract": "Цель статьи состоит в определении наиболее эффективной модели из семейства BERT по выявлению деструктивного контента в социальных медиа. Произведено сравнение пяти наиболее известных моделей BERT по выявлению деструктивного контента. Для этого осуществлено создание текстового корпуса из материалов социальных медиа (СМ), дополненного запрещённым к распространению в Российской Федерации контентом нацистского характера из Федерального списка экстремистских материалов. Представлена структура классификатора текстовых данных, основанного на глубокой искусственной нейронной сети BERT, и описана его работа на каждом этапе. Проведен поиск наиболее эффективного метода предварительной обработки текстов. Оценена эффективность работы различных голов классификаторов, основанных на трансформере BERT. Оценено влияние дообучения модели BERT и доказана эффективность его применения с расчетом перплексии. Представлены сравнительные таблицы работы классификаторов на каждом этапе исследования. Найдена наиболее эффективная архитектура классификатора на основе трансформера BERT, выполняющего задачу выявления деструктивного контента с точностью 96,99%.\nThe purpose of the article is to determine the most effective model from the BERT family for identifying destructive content in social media. A comparison of the five most well-known BERT models for identifying destructive content was made. For this purpose, a text corpus was created from social media materials (CM), supplemented with Nazi content prohibited for distribution in the Russian Federation from the Federal List of Extremist Materials. The structure of the text data classifier based on the deep artificial neural network BERT is presented and its operation at each stage is described. The search for the most effective method of preprocessing texts was carried out. The efficiency of various heads of classifiers based on the BERT transformer is evaluated. The influence of BERT model retraining is estimated and the effectiveness of its application with the calculation of perplexy is proved. Comparative tables of classifiers' work at each stage of the study are presented. The most effective architecture of the classifier based on the BERT transformer has been found, which performs the task of identifying destructive content with an accuracy of 96.99%.",
        "link": "http://dx.doi.org/10.36622/vstu.2022.25.3.003"
    },
    {
        "id": 20607,
        "title": "How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models",
        "authors": "Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "Abstract\nDespite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., kitchen chair: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.",
        "link": "http://dx.doi.org/10.1162/coli_a_00495"
    },
    {
        "id": 20608,
        "title": "Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks",
        "authors": "Kerstin Denecke, Richard May, Octavio Rivera-Romero",
        "published": "2024-2-17",
        "citations": 0,
        "abstract": "AbstractLarge Language Models (LLMs) such as General Pretrained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT), which use transformer model architectures, have significantly advanced artificial intelligence and natural language processing. Recognized for their ability to capture associative relationships between words based on shared context, these models are poised to transform healthcare by improving diagnostic accuracy, tailoring treatment plans, and predicting patient outcomes. However, there are multiple risks and potentially unintended consequences associated with their use in healthcare applications. This study, conducted with 28 participants using a qualitative approach, explores the benefits, shortcomings, and risks of using transformer models in healthcare. It analyses responses to seven open-ended questions using a simplified thematic analysis. Our research reveals seven benefits, including improved operational efficiency, optimized processes and refined clinical documentation. Despite these benefits, there are significant concerns about the introduction of bias, auditability issues and privacy risks. Challenges include the need for specialized expertise, the emergence of ethical dilemmas and the potential reduction in the human element of patient care. For the medical profession, risks include the impact on employment, changes in the patient-doctor dynamic, and the need for extensive training in both system operation and data interpretation.",
        "link": "http://dx.doi.org/10.1007/s10916-024-02043-5"
    },
    {
        "id": 20609,
        "title": "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
        "authors": "Dohwan Ko, Joonmyung Choi, Hyeong Kyu Choi, Kyoung-Woon On, Byungseok Roh, Hyunwoo J. Kim",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01925"
    },
    {
        "id": 20610,
        "title": "Ensemble and Transformer Models for Infectious Disease Prediction",
        "authors": "Blessing Isoyiza Adeika, Joseph Aina, Temileye Ibirinde, Tijesunimi Adeyemi, Md Mahmudur Rahman, Saroj Pramanik",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibe60311.2023.00068"
    },
    {
        "id": 20611,
        "title": "What Context Features Can Transformer Language Models Use?",
        "authors": "Joe O’Connor, Jacob Andreas",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-long.70"
    },
    {
        "id": 20612,
        "title": "CASE 2021 Task 2: Zero-Shot Classification of Fine-Grained Sociopolitical Events with Transformer Models",
        "authors": "Benjamin J. Radford",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.case-1.25"
    },
    {
        "id": 20613,
        "title": "A Comparative Study on COVID-19 Fake News Detection Using Different Transformer Based Models",
        "authors": "Sajib Kumar Saha Joy, Dibyo Fabian Dofadar, Riyo Hayat Khan, Md. Sabbir Ahmed, Rafeed Rahman",
        "published": "2022-7-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isiea54517.2022.9873797"
    },
    {
        "id": 20614,
        "title": "Analyzing Encoded Concepts in Transformer Language Models",
        "authors": "Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Khan, Jia Xu",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.naacl-main.225"
    },
    {
        "id": 20615,
        "title": "Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications",
        "authors": "Yuting Guo, Yao Ge, Mohammed Ali Al-Garadi, Abeed Sarker",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.smm4h-1.8"
    },
    {
        "id": 20616,
        "title": "Idea Generation using Transformer Decoder Models",
        "authors": "Musammet Rafia Karim, Siam Shibly Antar, Mohammad Ashrafuzzaman Khan",
        "published": "2022-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3579654.3579706"
    },
    {
        "id": 20617,
        "title": "LMCodec: A Low Bitrate Speech Codec with Causal Transformer Models",
        "authors": "Teerapat Jenrungrot, Michael Chinen, W. Bastiaan Kleijn, Jan Skoglund, Zalán Borsos, Neil Zeghidour, Marco Tagliasacchi",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095442"
    },
    {
        "id": 20618,
        "title": "Social Computational Design Method for Generating Product Shapes with GAN and Transformer Models",
        "authors": "Maolin Yang, Tianshuo Zang, Pingyu Jiang",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2022.09.401"
    },
    {
        "id": 20619,
        "title": "Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models",
        "authors": "Muhammad Asad Arshed, Shahzad Mumtaz, Muhammad Ibrahim, Saeed Ahmed, Muhammad Tahir, Muhammad Shafi",
        "published": "2023-7-18",
        "citations": 6,
        "abstract": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
        "link": "http://dx.doi.org/10.3390/info14070415"
    },
    {
        "id": 20620,
        "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
        "authors": "Tharindu Madusanka, Iqra Zahid, Hao Li, Ian Pratt-Hartmann, Riza Batista-Navarro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.536"
    },
    {
        "id": 20621,
        "title": "Advances In Planar Transformer Circuit Models: Stray Capacitance Equivalent Circuit for Two and Four Winding Transformers With H-Parameter Network Model",
        "authors": "Haitham M. Kanakri, Euzeli Cipriano Dos Santos, Maher Rizkalla",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naecon58068.2023.10366008"
    },
    {
        "id": 20622,
        "title": "Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning",
        "authors": "Hongwu Peng, Shaoyi Huang, Tong Geng, Ang Li, Weiwen Jiang, Hang Liu, Shusen Wang, Caiwen Ding",
        "published": "2021-4-7",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isqed51717.2021.9424344"
    },
    {
        "id": 20623,
        "title": "End-to-End generation of Multiple-Choice questions using Text-to-Text transfer Transformer models",
        "authors": "Ricardo Rodriguez-Torrealba, Eva Garcia-Lopez, Antonio Garcia-Cabot",
        "published": "2022-12",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.118258"
    },
    {
        "id": 20624,
        "title": "Transformer Testing",
        "authors": "Shirish P. Mehta, William R. Henning",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-17"
    },
    {
        "id": 20625,
        "title": "Recent Trends in Transformer Technology",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-15"
    },
    {
        "id": 20626,
        "title": "Zero-Shot Ranking Socio-Political Texts with Transformer Language Models to Reduce Close Reading Time",
        "authors": "Kiymet Akdemir, Ali Hürriyetoğlu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.case-1.17"
    },
    {
        "id": 20627,
        "title": "Using Generative Pretrained Transformer-3 Models for Russian News Clustering and Title Generation tasks",
        "authors": "Maria Tikhonova,  , Tatiana Shavrina, Dina Pisarevskaya, Oleh Shliazhko,  ",
        "published": "2021-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.28995/2075-7182-2021-20-1214-1223"
    },
    {
        "id": 20628,
        "title": "Tackling Italian University Assessment Tests with Transformer-Based Language Models",
        "authors": "Daniele Puccinelli, Silvia Demartini, Pier Luigi Ferrari",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.aaccademia.11009"
    },
    {
        "id": 20629,
        "title": "A Comparative Analysis of Transformer-based Protein Language Models for Remote Homology Prediction",
        "authors": "Anowarul Kabir, Asher Moldwin, Amarda Shehu",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3584371.3612942"
    },
    {
        "id": 20630,
        "title": "Semantics Squad at BLP-2023 Task 2: Sentiment Analysis of Bangla Text with Fine Tuned Transformer Based Models",
        "authors": "Krishno Dey, Md. Arid Hasan, Prerona Tarannum, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.41"
    },
    {
        "id": 20631,
        "title": "Development of Deep Transformer-Based Models for Long-Term Prediction of Transient Production of Oil Wells",
        "authors": "Ildar Radikovich Abdrakhmanov, Evgenii Alekseevich Kanin, Sergei Andreevich Boronin, Evgeny Vladimirovich Burnaev, Andrei Aleksandrovich Osiptsov",
        "published": "2021-10-12",
        "citations": 6,
        "abstract": "Abstract\nWe propose a novel approach to data-driven modeling of a transient production of oil wells. We apply the transformer-based neural networks trained on the multivariate time series composed of various parameters of oil wells measured during their exploitation. By tuning the machine learning models for a single well (ignoring the effect of neighboring wells) on the open-source field datasets, we demonstrate that transformer outperforms recurrent neural networks with LSTM/GRU cells in the forecasting of the bottomhole pressure dynamics. We apply the transfer learning procedure to the transformer-based surrogate model, which includes the initial training on the dataset from a certain well and additional tuning of the model's weights on the dataset from a target well. Transfer learning approach helps to improve the prediction capability of the model. Next, we generalize the single-well model based on the transformer architecture for multiple wells to simulate complex transient oilfield-level patterns. In other words, we create the global model which deals with the dataset, comprised of the production history from multiple wells, and allows for capturing the well interference resulting in more accurate prediction of the bottomhole pressure or flow rate evolutions for each well under consideration. The developed instruments for a single-well and oilfield-scale modelling can be used to optimize the production process by selecting the operating regime and submersible equipment to increase the hydrocarbon recovery. In addition, the models can be helpful to perform well-testing avoiding costly shut-in operations.",
        "link": "http://dx.doi.org/10.2118/206537-ms"
    },
    {
        "id": 20632,
        "title": "The great Transformer: Examining the role of large language models in the political economy of AI",
        "authors": "Dieuwertje Luitse, Wiebke Denkena",
        "published": "2021-7",
        "citations": 32,
        "abstract": " In recent years, AI research has become more and more computationally demanding. In natural language processing (NLP), this tendency is reflected in the emergence of large language models (LLMs) like GPT-3. These powerful neural network-based models can be used for a range of NLP tasks and their language generation capacities have become so sophisticated that it can be very difficult to distinguish their outputs from human language. LLMs have raised concerns over their demonstrable biases, heavy environmental footprints, and future social ramifications. In December 2020, critical research on LLMs led Google to fire Timnit Gebru, co-lead of the company’s AI Ethics team, which sparked a major public controversy around LLMs and the growing corporate influence over AI research. This article explores the role LLMs play in the political economy of AI as infrastructural components for AI research and development. Retracing the technical developments that have led to the emergence of LLMs, we point out how they are intertwined with the business model of big tech companies and further shift power relations in their favour. This becomes visible through the Transformer, which is the underlying architecture of most LLMs today and started the race for ever bigger models when it was introduced by Google in 2017. Using the example of GPT-3, we shed light on recent corporate efforts to commodify LLMs through paid API access and exclusive licensing, raising questions around monopolization and dependency in a field that is increasingly divided by access to large-scale computing power. ",
        "link": "http://dx.doi.org/10.1177/20539517211047734"
    },
    {
        "id": 20633,
        "title": "A Residual-Inception U-Net (RIU-Net) Approach and Comparisons with U-Shaped CNN and Transformer Models for Building Segmentation from High-Resolution Satellite Images",
        "authors": "Batuhan Sariturk, Dursun Zafer Seker",
        "published": "2022-10-8",
        "citations": 12,
        "abstract": "Building segmentation is crucial for applications extending from map production to urban planning. Nowadays, it is still a challenge due to CNNs’ inability to model global context and Transformers’ high memory need. In this study, 10 CNN and Transformer models were generated, and comparisons were realized. Alongside our proposed Residual-Inception U-Net (RIU-Net), U-Net, Residual U-Net, and Attention Residual U-Net, four CNN architectures (Inception, Inception-ResNet, Xception, and MobileNet) were implemented as encoders to U-Net-based models. Lastly, two Transformer-based approaches (Trans U-Net and Swin U-Net) were also used. Massachusetts Buildings Dataset and Inria Aerial Image Labeling Dataset were used for training and evaluation. On Inria dataset, RIU-Net achieved the highest IoU score, F1 score, and test accuracy, with 0.6736, 0.7868, and 92.23%, respectively. On Massachusetts Small dataset, Attention Residual U-Net achieved the highest IoU and F1 scores, with 0.6218 and 0.7606, and Trans U-Net reached the highest test accuracy, with 94.26%. On Massachusetts Large dataset, Residual U-Net accomplished the highest IoU and F1 scores, with 0.6165 and 0.7565, and Attention Residual U-Net attained the highest test accuracy, with 93.81%. The results showed that RIU-Net was significantly successful on Inria dataset. On Massachusetts datasets, Residual U-Net, Attention Residual U-Net, and Trans U-Net provided successful results.",
        "link": "http://dx.doi.org/10.3390/s22197624"
    },
    {
        "id": 20634,
        "title": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
        "authors": "J Alammar",
        "published": "2021",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-demo.30"
    },
    {
        "id": 20635,
        "title": "BpHigh at WASSA 2023: Using Contrastive Learning to build Sentence Transformer models for Multi-Class Emotion Classification in Code-mixed Urdu",
        "authors": "Bhavish Pahwa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.59"
    },
    {
        "id": 20636,
        "title": "Semantics Squad at BLP-2023 Task 1: Violence Inciting Bangla Text Detection with Fine-Tuned Transformer-Based Models",
        "authors": "Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.28"
    },
    {
        "id": 20637,
        "title": "Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model (Preprint)",
        "authors": "Emre Sezgin, Joseph Sirrianni, Simon L Linwood",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nUNSTRUCTURED\nGenerative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In contrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited training data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like logical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching pronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in health care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we briefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical practice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure, (2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational factors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability Act compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint can inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful artificial intelligence tools integrated into hospital systems and health care.\n",
        "link": "http://dx.doi.org/10.2196/preprints.32875"
    },
    {
        "id": 20638,
        "title": "Exploring the Influence of Focal Loss on Transformer Models for Imbalanced Maintenance Data in Industry 4.0",
        "authors": "Juan Pablo Usuga-Cadavid, Bernard Grabot, Samir Lamouri, Arnaud Fortin",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.08.121"
    },
    {
        "id": 20639,
        "title": "A Novel Approach for Sentiment Analysis on social media using BERT &amp; ROBERTA Transformer-Based Models",
        "authors": "Kundeti Naga Prasanthi, Rallabandi Eswari Madhavi, Degala Naga Sai Sabarinadh, Battula Sravani",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126206"
    },
    {
        "id": 20640,
        "title": "Fine-tuning Arabic Pre-Trained Transformer Models for Egyptian-Arabic Dialect Offensive Language and Hate Speech Detection and Classification",
        "authors": "Ibrahim Ahmed, Mostafa Abbas, Rany Hatem, Andrew Ihab, Mohamed Waleed Fahkr",
        "published": "2022-10-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/esolec54569.2022.10009167"
    },
    {
        "id": 20641,
        "title": "Peer Review #1 of \"S-Swin Transformer: simplified Swin Transformer model for offline handwritten Chinese character recognition (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1093v0.1/reviews/1"
    },
    {
        "id": 20642,
        "title": "Forward Converter, Transformer Design, and Output Inductor Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-14"
    },
    {
        "id": 20643,
        "title": "Peer Review #2 of \"S-Swin Transformer: simplified Swin Transformer model for offline handwritten Chinese character recognition (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1093v0.1/reviews/2"
    },
    {
        "id": 20644,
        "title": "Detecting Derogatory Comments on Women using Transformer-Based Models",
        "authors": "Sara Jerin Prithila, Fariha Hasan Tonima, Tahsina Tajrim Oishi, Md. Nazrul Islam, Ehsanur Rahman Rhythm, Adib Muhammad Amit, Annajiat Alim Rasel",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comnetsat59769.2023.10420692"
    },
    {
        "id": 20645,
        "title": "CSECU-DSG at SemEval-2022 Task 3: Investigating the Taxonomic Relationship Between Two Arguments using Fusion of Multilingual Transformer Models",
        "authors": "Abdul Aziz, Md. Akram Hossain, Abu Nowshed Chy",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.32"
    },
    {
        "id": 20646,
        "title": "Analysis of Asset Management Models for a Transformer Fleet in the National Laboratory of Smart Grids (LAB+i)",
        "authors": "Kevin Steven Morgado Gómez, Javier Rosero García",
        "published": "2022-3-23",
        "citations": 0,
        "abstract": "The study of the degradation of power transformers in the electrical network has become a subject of relevant analysis by network operators and companies, associated with the probability of failures and operation quality. For this reason this paper firstly presents the classification of a set of 12 Asset Management models related to power transformers monitoring and, then, the application of three of them in three substations of the National Laboratory of Smart Grids (LAB+i) located at Bogot´a Campus of the Universidad Nacional de Colombia. As a result, the main challenges were identified concerning the Asset Management application in transformer fleets related to data availability and precision. Finally, it was identified that the development of an Asset Management model that uses non-invasive real-time measurements is needed for continuous monitoring of power systems and diagnosis.",
        "link": "http://dx.doi.org/10.32397/tesea.vol3.n1.2"
    },
    {
        "id": 20647,
        "title": "cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora",
        "authors": "Abhilash Nandy, Sayantan Adak, Tanurima Halder, Sai Mahesh Pokala",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.semeval-1.87"
    },
    {
        "id": 20648,
        "title": "Transforming Text Generation in NLP: Deep Learning with GPT Models and 2023 Twitter Corpus Using Transformer Architecture",
        "authors": "Et al. Ghaith Alomari",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "This paper presents the design, implementation, and evaluation of a Transformer-based Generative Pre-trained Transformer (GPT) model tailored for character-level text generation. Leveraging the robust architecture of the Transformer, the model has been trained on a corpus sourced from social media text data, with the aim of exploring the intricacies of language patterns within a condensed and informal text setting. Key aspects of the model include a multi-head self-attention mechanism with a custom head configuration, positional embeddings, and layer normalization to promote stability in learning. It operates with a defined set of hyperparameters: a batch size of 32, a block size of 128, 200 iterations, a learning rate of 3e-4, and employs 4 attention heads across 4 layers with an embedding dimension of 384. The model has been optimized using the AdamW optimizer and includes regularization through dropout to prevent overfitting.Through a series of training iterations, the model demonstrates a converging behavior in loss metrics, indicating effective learning, and showcases the capacity to generate coherent text sequences post-training. Training and validation losses have been reported, revealing the nuances in model performance and generalization capabilities. The generated text samples postulate the model's potential in capturing the contextual flow of the dataset. This study further plots the loss curves, visually representing the training dynamics and convergence patterns. The final model, encapsulated within a PyTorch framework, presents a step forward in the realm of neural text generation, contributing to the ongoing advancements in language modeling and its applications in understanding and generating human-like text.",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i9.9463"
    },
    {
        "id": 20649,
        "title": "Assessment of bidirectional transformer encoder model and attention based bidirectional LSTM language models for fake news detection",
        "authors": "Anshika Choudhary, Anuja Arora",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jretconser.2023.103545"
    },
    {
        "id": 20650,
        "title": "Competence-Level Prediction and Resume &amp; Job Description Matching Using Context-Aware Transformer Models",
        "authors": "Changmao Li, Elaine Fisher, Rebecca Thomas, Steve Pittard, Vicki Hertzberg, Jinho D. Choi",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.679"
    },
    {
        "id": 20651,
        "title": "Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications",
        "authors": "Daniel Biś, Maksim Podkorytov, Xiuwen Liu",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.403"
    },
    {
        "id": 20652,
        "title": "Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages",
        "authors": "Michael A. Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia Markus, Dietrich Klakow",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.204"
    },
    {
        "id": 20653,
        "title": "Detecting Kids Cyberbullying Using Transfer Learning Approach: Transformer Fine-Tuning Models",
        "authors": "Wael M. S. Yafooz, Arafat Al-Dhaqm, Abdullah Alsaeedi",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-21199-7_18"
    },
    {
        "id": 20654,
        "title": "Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models",
        "authors": "Behrad TaghiBeyglou, Frank Rudzicz",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100046"
    },
    {
        "id": 20655,
        "title": "A Task-Oriented Dialogue Architecture via Transformer Neural Language Models and Symbolic Injection",
        "authors": "Oscar J. Romero, Antian Wang, John Zimmerman, Aaron Steinfeld, Anthony Tomasic",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.sigdial-1.46"
    },
    {
        "id": 20656,
        "title": "Automatic BI-RADS Classification of Breast Magnetic Resonance Medical Records Using Transformer-Based Models for Brazilian Portuguese",
        "authors": "Ricardo de Oliveira, Bruno Menezes, Júnia Ortiz, Erick Nascimento",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "This chapter aims to present a classification model for categorizing textual clinical records of breast magnetic resonance imaging, based on lexical, syntactic and semantic analysis of clinical reports according to the Breast Imaging-Reporting and Data System (BI-RADS) classification, using Deep Learning and Natural Language Processing (NLP). The model was developed from transfer learning based on the pre-trained BERTimbau model, BERT model (Bidirectional Encoder Representations from Transformers) trained in Brazilian Portuguese. The dataset is composed of medical reports in Brazilian Portuguese classified into six categories: Inconclusive; Normal or Negative; Certainly Benign Findings; Probably Benign Findings; Suspicious Findings; High Risk of Cancer; Previously Known Malignant Injury. The following models were implemented and compared: Random Forest, SVM, Naïve Bayes, BERTimbau with and without finetuning. The BERTimbau model presented better results, with better performance after finetuning.",
        "link": "http://dx.doi.org/10.5772/intechopen.113886"
    },
    {
        "id": 20657,
        "title": "Analysis of the evolution of advanced transformer-based language models: experiments on opinion mining",
        "authors": "Nour Eddine Zekaouiu, Siham Yousfi, Maryem Rhanoui, Mounia Mikram",
        "published": "2023-12-1",
        "citations": 1,
        "abstract": "<p>Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.</p>",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i4.pp1995-2010"
    },
    {
        "id": 20658,
        "title": "Pretrained Transformer Language Models Versus Pretrained Word Embeddings for the Detection of Accurate Health Information on Arabic Social Media: Comparative Study",
        "authors": "Yahya Albalawi, Nikola S Nikolov, Jim Buckley",
        "published": "2022-6-29",
        "citations": 3,
        "abstract": "\nBackground\nIn recent years, social media has become a major channel for health-related information in Saudi Arabia. Prior health informatics studies have suggested that a large proportion of health-related posts on social media are inaccurate. Given the subject matter and the scale of dissemination of such information, it is important to be able to automatically discriminate between accurate and inaccurate health-related posts in Arabic.\n\n\nObjective\nThe first aim of this study is to generate a data set of generic health-related tweets in Arabic, labeled as either accurate or inaccurate health information. The second aim is to leverage this data set to train a state-of-the-art deep learning model for detecting the accuracy of health-related tweets in Arabic. In particular, this study aims to train and compare the performance of multiple deep learning models that use pretrained word embeddings and transformer language models.\n\n\nMethods\nWe used 900 health-related tweets from a previously published data set extracted between July 15, 2019, and August 31, 2019. Furthermore, we applied a pretrained model to extract an additional 900 health-related tweets from a second data set collected specifically for this study between March 1, 2019, and April 15, 2019. The 1800 tweets were labeled by 2 physicians as accurate, inaccurate, or unsure. The physicians agreed on 43.3% (779/1800) of tweets, which were thus labeled as accurate or inaccurate. A total of 9 variations of the pretrained transformer language models were then trained and validated on 79.9% (623/779 tweets) of the data set and tested on 20% (156/779 tweets) of the data set. For comparison, we also trained a bidirectional long short-term memory model with 7 different pretrained word embeddings as the input layer on the same data set. The models were compared in terms of their accuracy, precision, recall, F1 score, and macroaverage of the F1 score.\n\n\nResults\nWe constructed a data set of labeled tweets, 38% (296/779) of which were labeled as inaccurate health information, and 62% (483/779) of which were labeled as accurate health information. We suggest that this was highly efficacious as we did not include any tweets in which the physician annotators were unsure or in disagreement. Among the investigated deep learning models, the Transformer-based Model for Arabic Language Understanding version 0.2 (AraBERTv0.2)-large model was the most accurate, with an F1 score of 87%, followed by AraBERT version 2–large and AraBERTv0.2-base.\n\n\nConclusions\nOur results indicate that the pretrained language model AraBERTv0.2 is the best model for classifying tweets as carrying either inaccurate or accurate health information. Future studies should consider applying ensemble learning to combine the best models as it may produce better results.\n",
        "link": "http://dx.doi.org/10.2196/34834"
    },
    {
        "id": 20659,
        "title": "Designing universal causal deep learning models: The geometric (Hyper)transformer",
        "authors": "Beatrice Acciaio, Anastasis Kratsios, Gudmund Pammer",
        "published": "2024-4",
        "citations": 2,
        "abstract": "AbstractSeveral problems in stochastic analysis are defined through their geometry, and preserving that geometric structure is essential to generating meaningful predictions. Nevertheless, how to design principled deep learning (DL) models capable of encoding these geometric structures remains largely unknown. We address this open problem by introducing a universal causal geometric DL framework in which the user specifies a suitable pair of metric spaces  and  and our framework returns a DL model capable of causally approximating any “regular” map sending time series in  to time series in  while respecting their forward flow of information throughout time. Suitable geometries on  include various (adapted) Wasserstein spaces arising in optimal stopping problems, a variety of statistical manifolds describing the conditional distribution of continuous‐time finite state Markov chains, and all Fréchet spaces admitting a Schauder basis, for example, as in classical finance. Suitable spaces  are compact subsets of any Euclidean space. Our results all quantitatively express the number of parameters needed for our DL model to achieve a given approximation error as a function of the target map's regularity and the geometric structure both of  and of . Even when omitting any temporal structure, our universal approximation theorems are the first guarantees that Hölder functions, defined between such  and  can be approximated by DL models.",
        "link": "http://dx.doi.org/10.1111/mafi.12389"
    },
    {
        "id": 20660,
        "title": "Performance Analysis of Transformer Based Models (BERT, ALBERT, and RoBERTa) in Fake News Detection",
        "authors": "Shafna Fitria Nur Azizah, Hasan Dwi Cahyono, Sari Widya Sihwi, Wisnu Widiarto",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoiact59844.2023.10455849"
    },
    {
        "id": 20661,
        "title": "Examining the Generalizability of Pretrained De-identification Transformer Models on Narrative Nursing Notes",
        "authors": "Fangyi Chen, Syed Mohtashim Abbas Bokhari, Kenrick Cato, Gamze Gürsoy, Sarah Collins Rossetti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Narrative nursing notes are a valuable resource in informatics research with unique predictive signals about patient care. The open sharing of these data, however, is appropriately constrained by rigorous regulations set by the Health Insurance Portability and Accountability Act (HIPAA) for the protection of privacy. Several models have been developed and evaluated on the open-source i2b2 dataset. A focus on the generalizability of these models with respect to nursing notes remains understudied. The study aims to understand the generalizability of pre-trained transformer models and investigate the variability of personal protected health information (PHI) distribution patterns between discharge summaries and nursing notes with a goal to inform the future design for model evaluation schema. Two pre-trained transformer models (RoBERTa, ClinicalBERT) fine-tuned on i2b2 2014 discharge summaries were evaluated on our data inpatient nursing notes and compared with the baseline performance. Statistical testing was deployed to assess differences in PHI distribution across discharge summaries and nursing notes. RoBERTa achieved the optimal performance when tested on an external source of data, with a F1 score of 0.887 across PHI categories and 0.932 in the PHI binary task. Overall, discharge summaries contained a higher number of PHI instances and categories of PHI compared to inpatient nursing notes. The study investigated the applicability of two pre-trained transformers on inpatient nursing notes and examined the distinctions between nursing notes and discharge summaries concerning the utilization of personal protected health information. Discharge summaries presented a greater quantity of PHI instances and types when compared to narrative nursing notes, but narrative nursing notes exhibited more diversity in the types of PHI present, with some pertaining to patient’s personal life. The insights obtained from the research help improve the design and selection of algorithms, as well as contribute to the development of suitable performance thresholds for PHI.",
        "link": "http://dx.doi.org/10.1055/a-2282-4340"
    },
    {
        "id": 20662,
        "title": "Exploring Multi-Task Multi-Lingual Learning of Transformer Models for Hate Speech and Offensive Speech Identification in Social Media",
        "authors": "Sudhanshu Mishra, Shivangi Prasad, Shubhanshu Mishra",
        "published": "2021-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-021-00455-5"
    },
    {
        "id": 20663,
        "title": "SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification",
        "authors": "S.M. Nabil Ashraf, Md. Adyelullahil Mamun, Hasnat Md. Abdullah, Md. Golam Rabiul Alam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441433"
    },
    {
        "id": 20664,
        "title": "Performance Comparison of Transformer-Based Models on Twitter Health Mention Classification",
        "authors": "Pervaiz Iqbal Khan, Imran Razzak, Andreas Dengel, Sheraz Ahmed",
        "published": "2023-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tcss.2022.3143768"
    },
    {
        "id": 20665,
        "title": "The Study of Two Models for Power Transformer Winding Deformation",
        "authors": "Danyang Zheng, Yangchun Cheng, Jiangang Bi, Wenzhi Chang, Guocheng Ding, Feifei Sun",
        "published": "2020-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichve49031.2020.9279863"
    },
    {
        "id": 20666,
        "title": "Mokey",
        "authors": "Ali Hadi Zadeh, Mostafa Mahmoud, Ameer Abdelhadi, Andreas Moshovos",
        "published": "2022-6-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3470496.3527438"
    },
    {
        "id": 20667,
        "title": "Transformer-based deep neural network language models for Alzheimer’s disease risk assessment from targeted speech",
        "authors": "Alireza Roshanzamir, Hamid Aghajan, Mahdieh Soleymani Baghshah",
        "published": "2021-12",
        "citations": 42,
        "abstract": "AbstractBackgroundWe developed transformer-based deep learning models based on natural language processing for early risk assessment of Alzheimer’s disease from the picture description test.MethodsThe lack of large datasets poses the most important limitation for using complex models that do not require feature engineering. Transformer-based pre-trained deep language models have recently made a large leap in NLP research and application. These models are pre-trained on available large datasets to understand natural language texts appropriately, and are shown to subsequently perform well on classification tasks with small training sets. The overall classification model is a simple classifier on top of the pre-trained deep language model.ResultsThe models are evaluated on picture description test transcripts of the Pitt corpus, which contains data of 170 AD patients with 257 interviews and 99 healthy controls with 243 interviews. The large bidirectional encoder representations from transformers (BERTLarge) embedding with logistic regression classifier achieves classification accuracy of 88.08%, which improves the state-of-the-art by 2.48%.ConclusionsUsing pre-trained language models can improve AD prediction. This not only solves the problem of lack of sufficiently large datasets, but also reduces the need for expert-defined features.",
        "link": "http://dx.doi.org/10.1186/s12911-021-01456-3"
    },
    {
        "id": 20668,
        "title": "Dodrio: Exploring Transformer Models with Interactive Visualization",
        "authors": "Zijie J. Wang, Robert Turko, Duen Horng Chau",
        "published": "2021",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-demo.16"
    },
    {
        "id": 20669,
        "title": "Semi-Supervised Anomaly Detection Based on Deep Generative Models with Transformer",
        "authors": "Weimin Shangguan, Wentao Fan, Ziyi Chen",
        "published": "2022-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529466.3529470"
    },
    {
        "id": 20670,
        "title": "A novel hybrid face mask detection approach using Transformer and convolutional neural network models",
        "authors": "Haifa M. Al-Sarrar, Heyam H. Al-Baity",
        "published": "2023-3-27",
        "citations": 0,
        "abstract": "Face and face mask detection are one of the most popular topics in computer vision literature. Face mask detection refers to the detection of people’s faces in digital images and determining whether they are wearing a face mask. It can be of great benefit in different domains by ensuring public safety through the monitoring of face masks. Current research details a range of proposed face mask detection models, but most of them are mainly based on convolutional neural network models. These models have some drawbacks, such as their not being robust enough for low quality images and their being unable to capture long-range dependencies. These shortcomings can be overcome using transformer neural networks. Transformer is a type of deep learning that is based on the self-attention mechanism, and its strong capabilities have attracted the attention of computer vision researchers who apply this advanced neural network architecture to visual data as it can handle long-range dependencies between input sequence elements. In this study, we developed an automatic hybrid face mask detection model that is a combination of a transformer neural network and a convolutional neural network models which can be used to detect and determine whether people are wearing face masks. The proposed hybrid model’s performance was evaluated and compared to other state-of-the-art face mask detection models, and the experimental results proved the proposed model’s ability to achieve a highest average precision of 89.4% with an execution time of 2.8 s. Thus, the proposed hybrid model is fit for a practical, real-time trial and can contribute towards public healthcare in terms of infectious disease control.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1265"
    },
    {
        "id": 20671,
        "title": "A Fine Line Between Irony and Sincerity: Identifying Bias in Transformer Models for Irony Detection",
        "authors": "Aaron Maladry, Els Lefever, Cynthia Van Hee, Veronique Hoste",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.28"
    },
    {
        "id": 20672,
        "title": "Where do Clinical Language Models Break Down? A Critical Behavioural Exploration of the ClinicalBERT Deep Transformer Model",
        "authors": "Alexander MacLean, Alexander Wong",
        "published": "2021-1-15",
        "citations": 0,
        "abstract": "\r\nThe introduction of Bidirectional Encoder Representations from Transformers (BERT) was a major breakthrough for transfer learning in natural language processing, enabling state-of-the-art performance across a large variety of complex language understanding tasks. In the realm of clinical language modeling, the advent of BERT led to the creation of ClinicalBERT, a state-of-the-art deep transformer model pretrained on a wealth of patient clinical notes to facilitate for downstream predictive tasks in the clinical domain. While ClinicalBERT has been widely leveraged by the research community as the foundation for building clinical domain-specific predictive models given its overall improved performance in the Medical Natural Language inference (MedNLI) challenge compared to the seminal BERT model, the fine-grained behaviour and intricacies of this popular clinical language model has not been well-studied. Without this deeper understanding, it is very challenging to understand where ClinicalBERT does well given its additional exposure to clinical knowledge, where it doesn't, and where it can be improved in a meaningful manner. Motivated to garner a deeper understanding, this study presents a critical behaviour exploration of the ClinicalBERT deep transformer model using MedNLI challenge dataset to better understanding the following intricacies: 1) decision-making similarities between ClinicalBERT and BERT (leverage a new metric we introduce called Model Alignment), 2) where ClinicalBERT holds advantages over BERT given its clinical knowledge exposure, and 3) where ClinicalBERT struggles when compared to BERT. The insights gained about the behaviour of ClinicalBERT will help guide towards new directions for designing and training clinical language models in a way that not only addresses the remaining gaps and facilitates for further improvements in clinical language understanding performance, but also highlights the limitation and boundaries of use for such models.\r\n",
        "link": "http://dx.doi.org/10.15353/jcvis.v6i1.3548"
    },
    {
        "id": 20673,
        "title": "CogNLP-Sheffield at CMCL 2021 Shared Task: Blending Cognitively Inspired Features with Transformer-based Language Models for Predicting Eye Tracking Patterns",
        "authors": "Peter Vickers, Rosa Wainwright, Harish Tayyar Madabushi, Aline Villavicencio",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.cmcl-1.16"
    },
    {
        "id": 20674,
        "title": "Accurate linearized models for Saturated Transformer of LCC-HVDC under Two Types of DC-Biased Currents",
        "authors": "X. Chen, T. Liu, S. Wang, Q. Xin, X. Zhao, Q. Liu, J. Chu, Y. Wu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.2217"
    },
    {
        "id": 20675,
        "title": "Classification of Highly Divergent Viruses from DNA/RNA Sequence Using Transformer-Based Models",
        "authors": "Tariq Sadad, Raja Atif Aurangzeb, Mejdl Safran,  Imran, Sultan Alfarhood, Jungsuk Kim",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "Viruses infect millions of people worldwide each year, and some can lead to cancer or increase the risk of cancer. As viruses have highly mutable genomes, new viruses may emerge in the future, such as COVID-19 and influenza. Traditional virology relies on predefined rules to identify viruses, but new viruses may be completely or partially divergent from the reference genome, rendering statistical methods and similarity calculations insufficient for all genome sequences. Identifying DNA/RNA-based viral sequences is a crucial step in differentiating different types of lethal pathogens, including their variants and strains. While various tools in bioinformatics can align them, expert biologists are required to interpret the results. Computational virology is a scientific field that studies viruses, their origins, and drug discovery, where machine learning plays a crucial role in extracting domain- and task-specific features to tackle this challenge. This paper proposes a genome analysis system that uses advanced deep learning to identify dozens of viruses. The system uses nucleotide sequences from the NCBI GenBank database and a BERT tokenizer to extract features from the sequences by breaking them down into tokens. We also generated synthetic data for viruses with small sample sizes. The proposed system has two components: a scratch BERT architecture specifically designed for DNA analysis, which is used to learn the next codons unsupervised, and a classifier that identifies important features and understands the relationship between genotype and phenotype. Our system achieved an accuracy of 97.69% in identifying viral sequences.",
        "link": "http://dx.doi.org/10.3390/biomedicines11051323"
    },
    {
        "id": 20676,
        "title": "Health-Related Content in Transformer-Based Deep Neural Network Language Models: Exploring Cross-Linguistic Syntactic Bias",
        "authors": "Giuseppe Samo, Caterina Bonan, Fuzhen Si",
        "published": "2022-6-29",
        "citations": 1,
        "abstract": "This paper explores a methodology for bias quantification in transformer-based deep neural network language models for Chinese, English, and French. When queried with health-related mythbusters on COVID-19, we observe a bias that is not of a semantic/encyclopaedical knowledge nature, but rather a syntactic one, as predicted by theoretical insights of structural complexity. Our results highlight the need for the creation of health-communication corpora as training sets for deep learning.",
        "link": "http://dx.doi.org/10.3233/shti220702"
    },
    {
        "id": 20677,
        "title": "Combatting online harassment by using transformer language models for the detection of emotions, hate speech and offensive language on social media",
        "authors": "Doorgesh Sookarah, Loovesh S. Ramwodin",
        "published": "2022-11-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elecom54934.2022.9965237"
    },
    {
        "id": 20678,
        "title": "Learning Light-Weight Translation Models from Deep Transformer",
        "authors": "Bei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, Jingbo Zhu",
        "published": "2021-5-18",
        "citations": 9,
        "abstract": "Recently, deep models have shown tremendous improvements in neural machine translation (NMT). However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is 8 times shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i15.17561"
    },
    {
        "id": 20679,
        "title": "Peer Review #1 of \"S-Swin Transformer: simplified Swin Transformer model for offline handwritten Chinese character recognition (v0.2)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1093v0.2/reviews/1"
    },
    {
        "id": 20680,
        "title": "Peer Review #2 of \"S-Swin Transformer: simplified Swin Transformer model for offline handwritten Chinese character recognition (v0.2)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1093v0.2/reviews/2"
    },
    {
        "id": 20681,
        "title": "Load-Tap-Change Control and Transformer Paralleling",
        "authors": "James H. Harlow",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-18"
    },
    {
        "id": 20682,
        "title": "NSUT-NLP at CASE 2022 Task 1: Multilingual Protest Event Detection using Transformer-based Models",
        "authors": "Manan Suri, Krish Chopra, Adwita Arora",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.case-1.23"
    },
    {
        "id": 20683,
        "title": "Exploring the Performance and Efficiency of Transformer Models for NLP on Mobile Devices",
        "authors": "Ioannis Panopoulos, Sokratis Nikolaidis, Stylianos I. Venieris, Iakovos S. Venieris",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscc58397.2023.10217850"
    },
    {
        "id": 20684,
        "title": "Interpretable Movie Review Analysis Using Machine Learning and Transformer Models Leveraging XAI",
        "authors": "Farzad Ahmed, Samiha Sultana, Md Tanzim Reza, Sajib Kumar Saha Joy, Md. Golam Rabiul Alam",
        "published": "2022-12-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csde56538.2022.10089294"
    },
    {
        "id": 20685,
        "title": "Enhancing Named Entity Recognition for Holocaust Testimonies through Pseudo Labelling and Transformer-based Models",
        "authors": "Isuri Anuradha Nanomi Arachchige, Le Ha, Ruslan Mitkov, Johannes-Dieter Steinert",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604951.3605514"
    },
    {
        "id": 20686,
        "title": "Power Transformer Protection",
        "authors": "Armando Guzmán, Hector J. Altuve, Gabriel Benmouyal",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-19"
    },
    {
        "id": 20687,
        "title": "Characterization of MPC-based Private Inference for Transformer-based Models",
        "authors": "Yongqin Wang, G. Edward Suh, Wenjie Xiong, Benjamin Lefaudeux, Brian Knott, Murali Annavaram, Hsien-Hsin S. Lee",
        "published": "2022-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispass55109.2022.00025"
    },
    {
        "id": 20688,
        "title": "When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",
        "authors": "Sebastian Schuster, Tal Linzen",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.naacl-main.71"
    },
    {
        "id": 20689,
        "title": "Comparisons of transformer top oil temperature calculation models using support vector regression optimised by genetic algorithm",
        "authors": "Tong Qian, Wenhu Tang, Wenjuan Jin, Lin Gan, Yuqing Liu, Guojun Lu",
        "published": "2017-10-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/oap-cired.2017.0539"
    },
    {
        "id": 20690,
        "title": "Interactively Providing Explanations for Transformer Language Models",
        "authors": "Felix Friedrich, Patrick Schramowski, Christopher Tauchmann, Kristian Kersting",
        "published": "2022-9-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/faia220218"
    },
    {
        "id": 20691,
        "title": "UPennHLP at WNUT-2020 Task 2 : Transformer models for classification of COVID19 posts on Twitter",
        "authors": "Arjun Magge, Varad Pimpalkhute, Divya Rallapalli, David Siguenza, Graciela Gonzalez-Hernandez",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.wnut-1.52"
    },
    {
        "id": 20692,
        "title": "Investigating the Effect of Discourse Connectives on Transformer Surprisal: Language Models Understand Connectives, Even So They Are Surprised",
        "authors": "Yan Cong, Emmanuele Chersoni, Yu-Yin Hsu, Philippe Blache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.17"
    },
    {
        "id": 20693,
        "title": "Single-sequence protein structure prediction using supervised transformer protein language models",
        "authors": "Wenkai Wang, Zhenling Peng, Jianyi Yang",
        "published": "2022-12-19",
        "citations": 43,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s43588-022-00373-3"
    },
    {
        "id": 20694,
        "title": "Emotion Classification in German Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language",
        "authors": "Thomas Schmidt, Katrin Dennerlein, Christian Wolff",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.latechclfl-1.8"
    },
    {
        "id": 20695,
        "title": "Tweets Topic Classification and Sentiment Analysis Based on Transformer-Based Language Models",
        "authors": "Ranju Mandal, Jinyan Chen, Susanne Becken, Bela Stantic",
        "published": "2023-5",
        "citations": 3,
        "abstract": "People provide information on their thoughts, perceptions, and activities through a wide range of channels, including social media. The wide acceptance of social media results in vast volume of valuable data, in variety of format as well as veracity. Analysis of such ‘big data’ allows organizations and analysts to make better and faster decisions. However, this data had to be quantified and information has to be extracted, which can be very challenging because of possible data ambiguity and complexity. To address information extraction, many analytic techniques, such as text mining, machine learning, predictive analytics, and diverse natural language processing, have been proposed in the literature. Recent advances in Natural Language Understanding-based techniques more specifically transformer-based architectures can solve sequence-to-sequence modeling tasks while handling long-range dependencies efficiently. In this work, we applied transformer-based sequence modeling on short texts’ topic classification and sentiment analysis from user-posted tweets. Applicability of models is investigated on posts from the Great Barrier Reef tweet dataset and obtained findings are encouraging providing insight that can be valuable for researchers working on classification of large datasets as well as large number of target classes.",
        "link": "http://dx.doi.org/10.1142/s2196888822500269"
    },
    {
        "id": 20696,
        "title": "Transformer-based Models for Language Identification: A Comparative Study",
        "authors": "Bharathi Mohan G, R Prasanna Kumar, Elakkiya R, Venkatakrishnan R, Harrieni Shankar, Y Sree Harshitha, Harini K, M Nikhil Reddy",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscan58655.2023.10394757"
    },
    {
        "id": 20697,
        "title": "Overview of the Transformer-based Models for NLP Tasks",
        "authors": "Anthony Gillioz, Jacky Casas, Elena Mugellini, Omar Abou Khaled",
        "published": "2020-9-26",
        "citations": 81,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15439/2020f20"
    },
    {
        "id": 20698,
        "title": "Identification of Dietary Supplement Use from Electronic Health Records Using Transformer-based Language Models",
        "authors": "Sicheng Zhou, Dalton Schutte, Aiwen Xing, Jiyang Chen, Julian Wolfson, Zhe He, Fang Yu, Rui Zhang",
        "published": "2021-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi52183.2021.00096"
    },
    {
        "id": 20699,
        "title": "Causes and Effects of Transformer Sound Levels",
        "authors": "Jeewan L. Puri",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-20"
    },
    {
        "id": 20700,
        "title": "Constrained Nonlinear Optimization with Application to Transformer Design",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-20"
    },
    {
        "id": 20701,
        "title": "U.S. Power Transformer Equipment Standards and Processes",
        "authors": "Philip J. Hopkinson",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-25"
    },
    {
        "id": 20702,
        "title": "Power transformer asset management and remnant life",
        "authors": "Norazhar Abu Bakar",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch8"
    },
    {
        "id": 20703,
        "title": "Middle Frequency Transformer Investigation for Solid-State Transformer",
        "authors": "Noriyuki Kimura, Toshimitsu Morizane",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgwcp.2018.8634561"
    },
    {
        "id": 20704,
        "title": "Circuit Model of a 2-Winding Transformer with Core",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-4"
    },
    {
        "id": 20705,
        "title": "Pyramid Swin Transformer: Different-Size Windows Swin Transformer for Image Classification and Object Detection",
        "authors": "Chenyu Wang, Toshio Endo, Takahiro Hirofuchi, Tsutomu Ikegami",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011675800003417"
    },
    {
        "id": 20706,
        "title": "Transformer Losses and Efficiency",
        "authors": "Anthony Walsh",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_7"
    },
    {
        "id": 20707,
        "title": "Transformer and Reactor Manufacturing",
        "authors": "Mike Lamb",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_11"
    },
    {
        "id": 20708,
        "title": "Smart Transformer as a Variable Frequency Transformer",
        "authors": "Dwijasish Das, Chandan Kumar",
        "published": "2021-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid52793.2021.9662138"
    },
    {
        "id": 20709,
        "title": "Chapter 3: Transformer Architecture Introduction",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-004"
    },
    {
        "id": 20710,
        "title": "Transformer and Reactor Project Management",
        "authors": "Craig Swinderman",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_9"
    },
    {
        "id": 20711,
        "title": "Extraction of Transformer Parameters from FEMM Simulations and Automated Creation of a Transformer SPICE Model Using a Scripting Language",
        "authors": "DENYS ZAIKIN",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p><em>This study presents a method for extracting transformer parameters using simulations in the Finite Element Method Magnetics (FEMM) electromagnetic solver. The extracted parameters represent a full model of a linear transformer and can be used in Simulation Program with Integrated Circuit Emphasis (SPICE) simulations. A model of the two-winding transformer is presented in three variants, for which different approaches were used in the transformer simulation in the SPICE program, all yielding the same simulation results. A three-winding transformer T-model extraction is also developed in the study. A method for extracting transformer parameters from FEMM is proposed, along with an automated tool based on a scripting language built into the FEMM software.</em></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22263358.v3"
    },
    {
        "id": 20712,
        "title": "Transformer and Reactor Transport",
        "authors": "Asgeir Mjelve",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_15"
    },
    {
        "id": 20713,
        "title": "An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models",
        "authors": "Amir Ganiev, Colton Chapin, Anderson De Andrade, Chen Liu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-industry.21"
    },
    {
        "id": 20714,
        "title": "Investigating the Representation of Open Domain Dialogue Context for Transformer Models",
        "authors": "Vishakh Padmakumar, Behnam Hedayatnia, Di Jin, Patrick Lange, Seokhwan Kim, Nanyun Peng, Yang Liu, Dilek Hakkani-Tur",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.sigdial-1.50"
    },
    {
        "id": 20715,
        "title": "Experimental Study of Language Models of \"Transformer\" in the Problem of Finding the Answer to a Question in a Russian-Language Text",
        "authors": "Denis Galeev, Vladimir Panishchev",
        "published": "2022-5-6",
        "citations": 0,
        "abstract": "The aim of the study is to obtain a more lightweight language model that is comparable in terms of EM and F1 with the best modern language models in the task of finding the answer to a question in a text in Russian. The results of the work can be used in various question-and-answer systems for which response time is important. Since the lighter model has fewer parameters than the original one, it can be used on less powerful computing devices, including mobile devices. In this paper, methods of natural language processing, machine learning, and the theory of artificial neural networks are used. The neural network is configured and trained using the Torch and Hugging face machine learning libraries. In the work, the DistilBERT model was trained on the SberQUAD dataset with and without distillation. The work of the received models is compared. The distilled DistilBERT model (EM 58,57 and F1 78,42) was able to outperform the results of the larger ruGPT-3-medium generative network (EM 57,60 and F1 77,73), despite the fact that ruGPT-3-medium had 6,5 times more parameters. The model also showed better EM and F1 metrics than the same model, but to which only conventional training without distillation was applied (EM 55,65, F1 76,51). Unfortunately, the resulting model lags further behind the larger robert discriminative model (EM 66,83, F1 84,95), which has 3,2 times more parameters. The application of the DistilBERT model in question-and-answer systems in Russian is substantiated. Directions for further research are proposed.",
        "link": "http://dx.doi.org/10.15622/ia.21.3.3"
    },
    {
        "id": 20716,
        "title": "Classifying Drug Ratings Using User Reviews with Transformer-Based Language Models",
        "authors": "Akhil Shiju, Zhe He",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi54592.2022.00035"
    },
    {
        "id": 20717,
        "title": "Detecting Tweets Containing Cannabidiol-Related COVID-19 Misinformation Using Transformer Language Models and Warning Letters From Food and Drug Administration: Content Analysis and Identification",
        "authors": "Jason Turner, Mehmed Kantardzic, Rachel Vickers-Smith, Andrew G Brown",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "\nBackground\nThe COVID-19 has introduced yet another opportunity to web-based sellers of loosely regulated substances, such as cannabidiol (CBD), to promote sales under false pretenses of curing the disease. Therefore, it has become necessary to innovate ways to identify such instances of misinformation.\n\n\nObjective\nWe sought to identify COVID-19 misinformation as it relates to the sales or promotion of CBD and used transformer-based language models to identify tweets semantically similar to quotes taken from known instances of misinformation. In this case, the known misinformation was the publicly available Warning Letters from Food and Drug Administration (FDA).\n\n\nMethods\nWe collected tweets using CBD- and COVID-19–related terms. Using a previously trained model, we extracted the tweets indicating commercialization and sales of CBD and annotated those containing COVID-19 misinformation according to the FDA definitions. We encoded the collection of tweets and misinformation quotes into sentence vectors and then calculated the cosine similarity between each quote and each tweet. This allowed us to establish a threshold to identify tweets that were making false claims regarding CBD and COVID-19 while minimizing the instances of false positives.\n\n\nResults\nWe demonstrated that by using quotes taken from Warning Letters issued by FDA to perpetrators of similar misinformation, we can identify semantically similar tweets that also contain misinformation. This was accomplished by identifying a cosine distance threshold between the sentence vectors of the Warning Letters and tweets.\n\n\nConclusions\nThis research shows that commercial CBD or COVID-19 misinformation can potentially be identified and curbed using transformer-based language models and known prior instances of misinformation. Our approach functions without the need for labeled data, potentially reducing the time at which misinformation can be identified. Our approach shows promise in that it is easily adapted to identify other forms of misinformation related to loosely regulated substances.\n",
        "link": "http://dx.doi.org/10.2196/38390"
    },
    {
        "id": 20718,
        "title": "Fake News Detection in Dravidian Languages Using Transformer Models",
        "authors": "Eduri Raja, Badal Soni, Samir Kumar Borgohain",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-6690-5_39"
    },
    {
        "id": 20719,
        "title": "Efficient Latent Space Compression for Lightning-Fast Fine-Tuning and Inference of Transformer-Based Models",
        "authors": "Ala Alam Falaki, Robin Gras",
        "published": "2023-7-30",
        "citations": 0,
        "abstract": "This paper presents a technique to reduce the number of parameters in a transformer-based encoder–decoder architecture by incorporating autoencoders. To discover the optimal compression, we trained different autoencoders on the embedding space (encoder’s output) of several pre-trained models. The experiments reveal that reducing the embedding size has the potential to dramatically decrease the GPU memory usage while speeding up the inference process. The proposed architecture was included in the BART model and tested for summarization, translation, and classification tasks. The summarization results show that a 60% decoder size reduction (from 96 M to 40 M parameters) will make the inference twice as fast and use less than half of GPU memory during fine-tuning process with only a 4.5% drop in R-1 score. The same trend is visible for translation and partially for classification tasks. Our approach reduces the GPU memory usage and processing time of large-scale sequence-to-sequence models for fine-tuning and inference. The implementation and checkpoints are available on GitHub.",
        "link": "http://dx.doi.org/10.3390/make5030045"
    },
    {
        "id": 20720,
        "title": "Anomaly-Based Intrusion Detection in IIoT Networks Using Transformer Models",
        "authors": "Jorge Casajús-Setién, Concha Bielza, Pedro Larrañaga",
        "published": "2023-7-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csr57506.2023.10224965"
    },
    {
        "id": 20721,
        "title": "Structural Guidance for Transformer Language Models",
        "authors": "Peng Qian, Tahira Naseem, Roger Levy, Ramón Fernandez Astudillo",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-long.289"
    },
    {
        "id": 20722,
        "title": "BERT Learns From Electroencephalograms About Parkinson’s Disease: Transformer-Based Models for Aid Diagnosis",
        "authors": "Alberto Nogales, Alvaro J. Garcia-Tejedor, Ana M. Maitin, Antonio Perez-Morales, Maria Dolores Del Castillo, Juan Pablo Romero",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3201843"
    },
    {
        "id": 20723,
        "title": "Swinv2-Imagen: hierarchical vision transformer diffusion models for text-to-image generation",
        "authors": "Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai",
        "published": "2023-10-6",
        "citations": 1,
        "abstract": "AbstractRecently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google’s Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
        "link": "http://dx.doi.org/10.1007/s00521-023-09021-x"
    },
    {
        "id": 20724,
        "title": "PICT-CLRL at WASSA 2023 Empathy, Emotion and Personality Shared Task: Empathy and Distress Detection using Ensembles of Transformer Models",
        "authors": "Tanmay Chavan, Kshitij Deshpande, Sheetal Sonawane",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.52"
    },
    {
        "id": 20725,
        "title": "Utilization of Transformer-Based Language Models in Understanding Citizens’ Interests, Sentiments and Emotions Towards Public Services Digitalization",
        "authors": "Gloria Hristova, Nikolay Netov",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "We live in an era of digital revolution not only in the industry, but also in the public sector. User opinion is key in e-services development. Currently the most established approaches for analyzing citizens’ opinions are surveys and personal interviews. However, governments should focus not only on developing public e-services but also on implementing modern solutions for data analysis based on machine learning and artificial intelligence. The main aim of the current study is to engage state-of-the-art natural language processing technologies to develop an analytical approach for public opinion analysis. We utilize transformer-based language models to derive valuable insights into citizens’ interests and expressed sentiments and emotions towards digitalization of educational, administrative and health public services. Our research brings empirical evidence on the practical usefulness of such methods in the government domain.",
        "link": "http://dx.doi.org/10.3233/faia230711"
    },
    {
        "id": 20726,
        "title": "Comparative Study of Content-Based Phishing Email Detection Using Global Vector (GloVe) and Bidirectional Encoder Representation from Transformer (BERT) Word Embedding Models",
        "authors": "Surajit Giri, Siddhartha Banerjee, Kunal Bag, Dipanjan Maiti",
        "published": "2022-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceeict53079.2022.9768612"
    },
    {
        "id": 20727,
        "title": "Towards Accurate Knowledge Transfer between Transformer-based Models for Code Summarization",
        "authors": "Chaochen Shi, Yong Xiang, Jiangshan Yu, Longxiang Gao",
        "published": "2022-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18293/seke2022-111"
    },
    {
        "id": 20728,
        "title": "A Comparative Study on the Effectiveness of three Generative Pre-trained Transformer Models in Assessing Spinal Metastasis from both Physician and Patient Perspectives (Preprint)",
        "authors": "Wei Xu, Jianru Xiao, Xiang Wang, Bo Li, Guanyu Fang, Zhaoyu Chen, Jiefu Fan",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nWith technological advancements, large language models (LLMs) like ChatGPT, NewBing, and Google Bard are emerging in medicine.\n\n\nOBJECTIVE\nThis study examines their application in orthopedics, particularly spinal metastases diagnosis and treatment, assessing their efficacy in aiding surgeons and informing patients.\n\n\nMETHODS\nThe study utilized questions from both doctor and patient viewpoints. We derived 15 questions on spinal metastases surgical treatment from the doctor's perspective, encompassing preoperative diagnosis to postoperative care. Additionally, 30 patient concerns about spinal metastases were integrated, representing outpatient and inpatient uncertainties. These questions were posed to ChatGPT, NewBing, and Google Bard. Two orthopedic surgeons assessed each response, categorizing them using a five-point Likert scale.\n\n\nRESULTS\nChatGPT addressed all 39 questions with 58.97% in Strong Agreement, 30.77% Agreement, and lesser percentages in other categories. NewBing's results were 25.64% Strong Agreement, 48.72% Agreement, and varied for others. Google Bard had 17.95% Strong Agreement and 53.85% Agreement. For doctor-centric questions, ChatGPT led in Strong Agreement at 58.9%, trailed by NewBing (25.6%) and Google Bard (17.9%). ChatGPT notably surpassed NewBing and Google Bard in comprehensive doctor responses (OR = 6.000, P = .006). For patient queries, the three LLMs showed comparable performance in Strong Agreement and Agreement.\n\n\nCONCLUSIONS\nThe research underscores ChatGPT's proficiency in supporting orthopedic decisions on spinal metastasis. All models exhibited parallel performance for patient queries. While LLMs provide essential perspectives, they should complement, not replace, medical expertise.\n",
        "link": "http://dx.doi.org/10.2196/preprints.52409"
    },
    {
        "id": 20729,
        "title": "Adult Content Detection on Indonesian Tweets by Fine-tuning Transformer-based Models",
        "authors": "Ahmad Fathan Hidayatullah, Rosyzie Anna Apong, Daphne Teck Ching Lai, Atika Qazi",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aciis59385.2023.10367283"
    },
    {
        "id": 20730,
        "title": "Restoring Punctuation and Capitalization Using Transformer Models",
        "authors": "Andris Vāravs, Askars Salimbajevs",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-00810-9_9"
    },
    {
        "id": 20731,
        "title": "Reconstructing the cascade of language processing in the brain using the internal computations of transformer language models",
        "authors": "Sreejan Kumar, Theodore Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth Norman, Thomas Griffiths, Robert Hawkins, Samuel Nastase",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1255-0"
    },
    {
        "id": 20732,
        "title": "Comparison of BLSTM-Attention and BLSTM-Transformer Models for Wind Speed Prediction",
        "authors": "Liu Zhifeng, Ding Feng, Lu Jianyong, Zhou Yue, Chu Hetao",
        "published": "2022-2-2",
        "citations": 1,
        "abstract": "\r\n\r\n\r\nAccurate estimation of wind speed is essential for many meteorological applications. A novel short-term wind speed prediction method of Bi-directional LSTM and Transformer Network (BLSTM-TRA) model is proposed by combining the Transformer model and LSTM model, and a hybrid model of Bi-directional Long Short-term Memory and Attention Network (BLSTM-ATT) is proposed based on Attention mechanism and LSTM model. The proposed BLSTM-ATT and BLSTM-TRA model are used for predicting the wind speed of seven meteorological stations in Qingdao. In combination with historical ob- servation data, the proposed models outperform the Numerical Weather Prediction (NWP) system of European Centre for Medium-Range Weather Forecasts (ECMWF). By comparing the results of BLSTM-ATT, BLSTM-TRA and ECMWF forecast model, RMSE and MAE of BLSTM-ATT are reduced by 44.7% and 50.3% on average, respectively, as well as an average decrease of 43.0% in the RMSE, an average decrease of 47.4% in the MAE of the BLSTM-TRA model. This demonstrates that the BLSTM-ATT model and the BLSTM-TRA model are more accurate than the ECMWF model in wind speed prediction.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.7546/crabs.2022.01.10"
    },
    {
        "id": 20733,
        "title": "Automatic Title Generation for Learning Resources and Pathways with Pre-trained Transformer Models",
        "authors": "Prakhar Mishra, Chaitali Diwan, Srinath Srinivasa, G. Srinivasaraghavan",
        "published": "2021-12",
        "citations": 3,
        "abstract": " To create curiosity and interest for a topic in online learning is a challenging task. A good preview that outlines the contents of a learning pathway could help learners know the topic and get interested in it. Towards this end, we propose a hierarchical title generation approach to generate semantically relevant titles for the learning resources in a learning pathway and a title for the pathway itself. Our approach to Automatic Title Generation for a given text is based on pre-trained Transformer Language Model GPT-2. A pool of candidate titles are generated and an appropriate title is selected among them which is then refined or de-noised to get the final title. The model is trained on research paper abstracts from arXiv and evaluated on three different test sets. We show that it generates semantically and syntactically relevant titles as reflected in ROUGE, BLEU scores and human evaluations. We propose an optional abstractive Summarizer module based on pre-trained Transformer model T5 to shorten medium length documents. This module is also trained and evaluated on research papers from arXiv dataset. Finally, we show that the proposed model of hierarchical title generation for learning pathways has promising results. ",
        "link": "http://dx.doi.org/10.1142/s1793351x21400134"
    },
    {
        "id": 20734,
        "title": "CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended Sarcasm Detection in English and Arabic",
        "authors": "Abdelkader El Mahdaouy, Abdellah El Mekki, Kabil Essefar, Abderrahman Skiredj, Ismail Berrada",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.117"
    },
    {
        "id": 20735,
        "title": "Can We Quickly Learn to “Translate” Bioactive Molecules with Transformer Models?",
        "authors": "Emma P. Tysinger, Brajesh K. Rai, Anton V. Sinitskiy",
        "published": "2023-3-27",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/acs.jcim.2c01618"
    },
    {
        "id": 20736,
        "title": "Text Classification of News Using Transformer-based Models for Portuguese",
        "authors": "Isabel N. Santana, Raphael S. Oliveira, Erick G. S. Nascimento",
        "published": "2022-10",
        "citations": 0,
        "abstract": "This work proposes the use of a fine-tuned Transformers-based Natural Language Processing (NLP) model called BERTimbau to generate the word embeddings from texts published in a Brazilian newspaper, to create a robust NLP model to classify news in Portuguese, a task that is costly for humans to perform for big amounts of data. To assess this approach, besides the generation of embeddings by the fine-tuned BERTimbau, a comparative analysis was conducted using the Word2Vec technique. The first step of the work was to rearrange news from nineteen to ten categories to reduce the existence of class imbalance in the corpus, using the K-means and TF-IDF techniques. In the Word2Vec step, the CBOW and Skip-gram architectures were applied. In BERTimbau and Word2Vec steps, the Doc2Vec method was used to represent each news as a unique embedding, generating a document embedding for each news. Metrics accuracy, weighted accuracy, precision, recall, F1-Score, AUC ROC and AUC PRC were applied to evaluate the results. It was noticed that the fine-tuned BERTimbau captured distinctions in the texts of the different categories, showing that the classification model based on this model has a superior performance than the other explored techniques.",
        "link": "http://dx.doi.org/10.54808/jsci.20.05.33"
    },
    {
        "id": 20737,
        "title": "Comparison of Different Machine Learning Models for Short-Term Load Forecasting at Transformer Level with High Amounts of Photovoltaic Generation",
        "authors": "Timon Jungh, Bastian Steinhagen, Marc Hesse, Katrin Schulte",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgteurope56780.2023.10407216"
    },
    {
        "id": 20738,
        "title": "Comparative study of DC/DC electric vehicle charging system with conventional transformer and planar transformer",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2023.145412"
    },
    {
        "id": 20739,
        "title": "A Full Transformer Video Captioning Model via Vision Transformer",
        "authors": "Heeju Im, Yong Suk Choi",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2023.29.8.378"
    },
    {
        "id": 20740,
        "title": "Transformer and Reactor Supplier Selection",
        "authors": "Khayakazi Dioka",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_8"
    },
    {
        "id": 20741,
        "title": "SSN_MLRG3 @LT-EDI-ACL2022-Depression Detection System from Social Media Text using Transformer Models",
        "authors": "Sarika Esackimuthu, Shruthi Hariprasad, Rajalakshmi Sivanaiah, Angel S, Sakaya Milton Rajendram, Mirnalinee T T",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.ltedi-1.26"
    },
    {
        "id": 20742,
        "title": "Exploring Transformer Model in Longitudinal Pharmacokinetic/Pharmacodynamic Analyses and Comparing with Alternative Natural Language Processing Models",
        "authors": "Yiming Cheng, Hongxiang Hu, Xin Dong, Xiaoran Hao, Yan Li",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.xphs.2024.02.008"
    },
    {
        "id": 20743,
        "title": "Comparative Analysis of Six Short-Term Load Forecasting Models for a Distribution Transformer",
        "authors": "Mukesh Kumar, Praveer Kumar Jha, J Crawford Alasdair, Shalini Dandriyal, Rahul Maurya",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/redec58286.2023.10208192"
    },
    {
        "id": 20744,
        "title": "Masked Token Similarity Transfer for Compressing Transformer-Based ASR Models",
        "authors": "Euntae Choi, Youshin Lim, Byeong-Yeol Kim, Hyung Yong Kim, Hanbin Lee, Yunkyu Lim, Seung Woo Yu, Sungjoo Yoo",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096531"
    },
    {
        "id": 20745,
        "title": "Cyberbullying Text Identification based on Deep Learning and Transformer-based Language Models",
        "authors": "Khalid Saifullah, Muhammad Ibrahim Khan, Suhaima Jamal, Iqbal H. Sarker",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "In the contemporary digital age, social media platforms like Facebook, Twitter, and YouTube serve as vital channels for individuals to express ideas and connect with others. Despite fostering increased connectivity, these platforms have inadvertently given rise to negative behaviors, particularly cyberbullying. While extensive research has been conducted on high-resource languages such as English, there is a notable scarcity of resources for low-resource languages like Bengali, Arabic, Tamil, etc., particularly in terms of language modeling. This study addresses this gap by developing a cyberbullying text identification system called BullyFilterNeT tailored for social media texts, considering Bengali as a test case. The intelligent BullyFilterNeT system devised overcomes Out-of-Vocabulary (OOV) challenges associated with non-contextual embeddings and addresses the limitations of context-aware feature representations. To facilitate a comprehensive understanding, three non-contextual embedding models GloVe, FastText, and Word2Vec are developed for feature extraction in Bengali. These embedding models are utilized in the classification models, employing three statistical models (SVM, SGD, Libsvm), and four deep learning models (CNN, VDCNN, LSTM, GRU). Additionally, the study employs six transformer-based language models: mBERT, bELECTRA, IndicBERT, XML-RoBERTa, DistilBERT, and BanglaBERT, respectively to overcome the limitations of earlier models. Remarkably, BanglaBERT-based BullyFilterNeT achieves the highest accuracy of 88.04% in our test set, underscoring its effectiveness in cyberbullying text identification in the Bengali language.",
        "link": "http://dx.doi.org/10.4108/eetinis.v11i1.4703"
    },
    {
        "id": 20746,
        "title": "Evaluating Transformer-based Models in the Information Extraction of Fiscal Documents",
        "authors": "João Macedo, Byron Bezerra, Estanislau Lima, Alysson Soares, Celso Lopes, Cleber Zanchettin",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/la-cci58595.2023.10409483"
    },
    {
        "id": 20747,
        "title": "Transformer-Based Language Models for Software Vulnerability Detection",
        "authors": "Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, Surya Nepal",
        "published": "2022-12-5",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3564625.3567985"
    },
    {
        "id": 20748,
        "title": "Reconciling Tap-Changing Transformer Models",
        "authors": "Jose M. Cano, Md Rejwanur Rashid Mojumdar, Gonzalo Alonso Orcajo",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrd.2019.2940422"
    },
    {
        "id": 20749,
        "title": "Jointly Learning to Align and Translate with Transformer Models",
        "authors": "Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik",
        "published": "2019",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-1453"
    },
    {
        "id": 20750,
        "title": "FEM-Based Power Transformer Model for Superconducting and Conventional Power Transformer Optimization",
        "authors": "Tamás Orosz",
        "published": "2022-8-25",
        "citations": 8,
        "abstract": "There were many promising superconducting materials discovered in the last decades that can significantly increase the efficiency of large power transformers. However, these large machines are generally custom-made and tailored to the given application. During the design process the most economical design should be selected from thousands of applicable solutions in a short design period. Due to the nonlinearity of the task, the cost-optimal transformer design, which has the smallest costs during the transformers’ planned lifetime, is usually not the design with the highest efficiency. Due to the topic’s importance, many simplified transformer models were published in the literature to resolve this problem. However, only a few papers considered this preliminary design optimization problem in the case of superconducting transformers and none of them made a comparison with a validated conventional transformer optimization model. This paper proposes a novel FEM-based two-winding transformer model, which can be used to calculate the main dimension of conventional and superconducting transformer designs. The models are stored in a unified JSON-file format, which can be easily integrated into an evolutionary or genetic algorithm-based optimization. The paper shows the used methods and their accuracy on conventional 10 MVA and superconducting 1.2 MVA transformer designs. Moreover, a simple cost optimization with the 10 MVA transformer was performed for two realistic economic scenarios. The results show that in some cases the cheaper, but less efficient, transformer can be the more economic.",
        "link": "http://dx.doi.org/10.3390/en15176177"
    },
    {
        "id": 20751,
        "title": "Application of ACF-wavelet feature extraction for classification of some artificial PD models of power transformer",
        "authors": "Vahid PARVIN DARABAD",
        "published": "2018-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3906/elk-1708-17"
    },
    {
        "id": 20752,
        "title": "A comparative analysis of transformer based models for figurative language classification",
        "authors": "Taha Junaid, D. Sumathi, A.N. Sasikumar, S. Suthir, J. Manikandan, Rashmita Khilar, P.G. Kuppusamy, M. Janardhana Raju",
        "published": "2022-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2022.108051"
    },
    {
        "id": 20753,
        "title": "Comparative Study of Transformer Models",
        "authors": "Ashwin Sankar, R. Dhanalakshmi",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-15512-3_17"
    },
    {
        "id": 20754,
        "title": "Unmasking Deception: A Comparative Study of Tree-Based and Transformer-Based Models for Fake Review Detection on Yelp",
        "authors": "Pengqi WANG, Yue LIN, Junyi CHAI",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394573"
    },
    {
        "id": 20755,
        "title": "Astroconformer: The prospects of analysing stellar light curves with transformer-based deep learning models",
        "authors": "Jia-Shu Pan, Yuan-Sen Ting, Jie Yu",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "ABSTRACT\nStellar light curves contain valuable information about oscillations and granulation, offering insights into stars’ internal structures and evolutionary states. Traditional asteroseismic techniques, primarily focused on power spectral analysis, often overlook the crucial phase information in these light curves. Addressing this gap, recent machine learning applications, particularly those using Convolutional Neural Networks (CNNs), have made strides in inferring stellar properties from light curves. However, CNNs are limited by their localized feature extraction capabilities. In response, we introduce Astroconformer, a Transformer-based deep learning framework, specifically designed to capture long-range dependencies in stellar light curves. Our empirical analysis centres on estimating surface gravity (log g), using a data set derived from single-quarter Kepler light curves with log g values ranging from 0.2 to 4.4. Astroconformer demonstrates superior performance, achieving a root-mean-square-error (RMSE) of 0.017 dex at log g ≈ 3 in data-rich regimes and up to 0.1 dex in sparser areas. This performance surpasses both K-nearest neighbour models and advanced CNNs. Ablation studies highlight the influence of receptive field size on model effectiveness, with larger fields correlating to improved results. Astroconformer also excels in extracting νmax with high precision. It achieves less than 2 per cent relative median absolute error for 90-d red giant light curves. Notably, the error remains under 3 per cent for 30-d light curves, whose oscillations are undetectable by a conventional pipeline in 30 per cent cases. Furthermore, the attention mechanisms in Astroconformer align closely with the characteristics of stellar oscillations and granulation observed in light curves.",
        "link": "http://dx.doi.org/10.1093/mnras/stae068"
    },
    {
        "id": 20756,
        "title": "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications",
        "authors": "Berdakh Abibullaev, Aigerim Keutayeva, Amin Zollanvari",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3329678"
    },
    {
        "id": 20757,
        "title": "Transformer Design Using Advanced Methods",
        "authors": "Alvaro Portillo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_10"
    },
    {
        "id": 20758,
        "title": "Transformer and Reactor Pre-Commissioning",
        "authors": "John Lapworth",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_18"
    },
    {
        "id": 20759,
        "title": "Extraction of Transformer Parameters from FEMM Simulations and Automated Creation of a Transformer SPICE Model Using a Scripting Language",
        "authors": "DENYS ZAIKIN",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study presents a method for extracting transformer parameters using simulations in the Finite Element Method Magnetics (FEMM) electromagnetic solver. The extracted parameters represent a full model of a linear transformer and can be used in Simulation Program with Integrated Circuit Emphasis (SPICE) simulations. A model of the two-winding transformer is presented in three variants, for which different approaches were used in the transformer simulation in the SPICE program, all yielding the same simulation results. A three-winding transformer T-model extraction is also developed in the study. A method for extracting transformer parameters from FEMM is proposed, along with an automated tool based on a scripting language built into the FEMM software.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22263358.v4"
    },
    {
        "id": 20760,
        "title": "Extraction of Transformer Parameters from FEMM Simulations and Automated Creation of a Transformer SPICE Model Using a Scripting Language",
        "authors": "DENYS ZAIKIN",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study presents a method for extracting transformer parameters using simulations in the Finite Element Method Magnetics (FEMM) electromagnetic solver. The extracted parameters represent a full model of a linear transformer and can be used in Simulation Program with Integrated Circuit Emphasis (SPICE) simulations. A model of the transformer is presented in three variants, for which different approaches were used in the transformer simulation in the SPICE program, all yielding the same simulation results. A method for extracting transformer parameters from FEMM is proposed, along with an automated tool based on a scripting language built into the FEMM software.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22263358.v2"
    },
    {
        "id": 20761,
        "title": "UC3M-PUCPR at SemEval-2022 Task 11: An Ensemble Method of Transformer-based Models for Complex Named Entity Recognition",
        "authors": "Elisa Schneider, Renzo M. Rivera-Zavala, Paloma Martinez, Claudia Moro, Emerson Paraiso",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.199"
    },
    {
        "id": 20762,
        "title": "A Comparative Study of Traditional and Transformer-based Deep Learning Models for Multi-Class Eye Movement Recognition Using Collected Dataset",
        "authors": "Ali A. Masaoodi, Hawraa Hassan Abbas, Haider Ismael Shahadi",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icamimia60881.2023.10427563"
    },
    {
        "id": 20763,
        "title": "How Can Transformer Models Shape Future Healthcare: A Qualitative Study",
        "authors": "Kerstin Denecke, Richard May, Octavio Rivera Romero",
        "published": "2023-10-20",
        "citations": 1,
        "abstract": "Transformer models have been successfully applied to various natural language processing and machine translation tasks in recent years, e.g. automatic language understanding. With the advent of more efficient and reliable models (e.g. GPT-3), there is a growing potential for automating time-consuming tasks that could be of particular benefit in healthcare to improve clinical outcomes. This paper aims at summarizing potential use cases of transformer models for future healthcare applications. Precisely, we conducted a survey asking experts on their ideas and reflections for future use cases. We received 28 responses, analyzed using an adapted thematic analysis. Overall, 8 use case categories were identified including documentation and clinical coding, workflow and healthcare services, decision support, knowledge management, interaction support, patient education, health management, and public health monitoring. Future research should consider developing and testing the application of transformer models for such use cases.",
        "link": "http://dx.doi.org/10.3233/shti230736"
    },
    {
        "id": 20764,
        "title": "Extraction of Transformer Parameters from FEMM Simulations and Automated Creation of a Transformer SPICE Model Using a Scripting Language",
        "authors": "DENYS ZAIKIN",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study presents a method for extracting transformer parameters using simulations in the Finite Element Method Magnetics (FEMM) electromagnetic solver. The extracted parameters represent a full model of a linear transformer and can be used in Simulation Program with Integrated Circuit Emphasis (SPICE) simulations. A model of the transformer is presented in three variants, for which different approaches were used in the transformer simulation in the SPICE program, all yielding the same simulation results. A method for extracting transformer parameters from FEMM is proposed, along with an automated tool based on a scripting language built into the FEMM software.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22263358.v1"
    },
    {
        "id": 20765,
        "title": "Extraction of Transformer Parameters from FEMM Simulations and Automated Creation of a Transformer SPICE Model Using a Scripting Language",
        "authors": "DENYS ZAIKIN",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study presents a method for extracting transformer parameters using simulations in the Finite Element Method Magnetics (FEMM) electromagnetic solver. The extracted parameters represent a full model of a linear transformer and can be used in Simulation Program with Integrated Circuit Emphasis (SPICE) simulations. A model of the two-winding transformer is presented in three variants, for which different approaches were used in the transformer simulation in the SPICE program, all yielding the same simulation results. A three-winding transformer T-model extraction is also developed in the study. A method for extracting transformer parameters from FEMM is proposed, along with an automated tool based on a scripting language built into the FEMM software.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22263358"
    },
    {
        "id": 20766,
        "title": "Transformer Insulation Materials and Ageing",
        "authors": "",
        "published": "2017-8-7",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch1"
    },
    {
        "id": 20767,
        "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
        "authors": "Yidi Jiang, Bidisha Sharma, Maulik Madhavi, Haizhou Li",
        "published": "2021-8-30",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-402"
    },
    {
        "id": 20768,
        "title": "Transformer-Based Language Models for Bulgarian",
        "authors": "Iva Marinova,  , Kiril Simov, Petya Osenova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_077"
    },
    {
        "id": 20769,
        "title": "Dielectric Response Analysis of Mineral Oil Immersed Transformer, Natural Ester(FR3) Immersed Transformer, and Palm Oil Immersed Transformer",
        "authors": "Sakda Maneerot, Phethai Nimsanong, Jompatara Siriworachanyadee, Monthon Leelajindakrairerk, Kittipod Jariyanurat, Norasage Pattanadech",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2019.8796580"
    },
    {
        "id": 20770,
        "title": "Smart Transformer Condition Monitoring and Diagnosis",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch9"
    },
    {
        "id": 20771,
        "title": "A data efficient transformer based on Swin Transformer",
        "authors": "Dazhi Yao, Yunxue Shao",
        "published": "2023-7-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00371-023-02939-2"
    },
    {
        "id": 20772,
        "title": "Transformer and Reactor Installation",
        "authors": "Mike Lamb, Ross Willoughby",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_17"
    },
    {
        "id": 20773,
        "title": "Transformer and Reactor Testing – Dielectric Tests",
        "authors": "Tom Breckenridge",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_14"
    },
    {
        "id": 20774,
        "title": "transformer, n.",
        "authors": "",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/oed/5999611419"
    },
    {
        "id": 20775,
        "title": "Chapter 4: Transformer Architecture in Greater Depth",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-005"
    },
    {
        "id": 20776,
        "title": "Transformer and Reactor Storage",
        "authors": "Khayakazi Dioka, Tara-Lee MacArthur",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_16"
    },
    {
        "id": 20777,
        "title": "Guanella Transformer/Balun Implemented with Planar Transformer Technology",
        "authors": "James S. McLean",
        "published": "2022-4-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wmcs55582.2022.9866125"
    },
    {
        "id": 20778,
        "title": "Sparseswin: Swin Transformer with Sparse Transformer Block",
        "authors": "Krisna Pinasthika, Blessius  Sheldo Putra Laksono, Riyandi  Banovbi Putera Irsal, Syifa’  Hukma Shabiyya, Novanto Yudistira",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4588785"
    },
    {
        "id": 20779,
        "title": "The Grounded Wye-Delta Transformer with a Zig-Zag Transformer",
        "authors": "William Kersting, Wayne Carr, Robert Kerestes",
        "published": "2022-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/repec55671.2022.00021"
    },
    {
        "id": 20780,
        "title": "STS-Transformer:&amp;nbsp;A Spatiotemporal Symmetrical Transformer Structure for EEG Emotion Recognition",
        "authors": "bo pan, wei zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4142410"
    },
    {
        "id": 20781,
        "title": "Overview of Transformer and Reactor Procurement",
        "authors": "Tom Breckenridge, Simon Ryder",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_1"
    },
    {
        "id": 20782,
        "title": "Creating and Comparing Dictionary, Word Embedding, and Transformer-Based Models to Measure Discrete Emotions in German Political Text",
        "authors": "Tobias Widmann, Maximilian Wich",
        "published": "2023-10",
        "citations": 6,
        "abstract": "AbstractPrevious research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowd-coded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.",
        "link": "http://dx.doi.org/10.1017/pan.2022.15"
    },
    {
        "id": 20783,
        "title": "Serial KinderMiner (SKiM) Discovers and Annotates Biomedical Knowledge Using Co-Occurrence and Transformer Models",
        "authors": "Robert J. Millikin, Kalpana Raja, John Steill, Cannon Lock, Xuancheng Tu, Ian Ross, Lam C Tsoi, Finn Kuusisto, Zijian Ni, Miron Livny, Brian Bockelman, James Thomson, Ron Stewart",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackgroundThe PubMed database contains more than 34 million articles; consequently, it is becoming increasingly difficult for a biomedical researcher to keep up-to-date with different knowledge domains. Computationally efficient and interpretable tools are needed to help researchers find and understand associations between biomedical concepts. The goal of literature-based discovery (LBD) is to connect concepts in isolated literature domains that would normally go undiscovered. This usually takes the form of an A-B-C relationship, where A and C terms are linked through a B term intermediate. Here we describe Serial KinderMiner (SKiM), an LBD algorithm for finding statistically significant links between an A term and one or more C terms through some B term intermediate(s). The development of SKiM is motivated by the the observation that there are only a few LBD tools that provide a functional web interface, and that the available tools are limited in one or more of the following ways: 1) they identify a relationship but not the type of relationship, 2) they do not allow the user to provide their own lists of B or C terms, hindering flexibility, 3) they do not allow for querying thousands of C terms (which is crucial if, for instance, the user wants to query connections between a disease and the thousands of available drugs), or 4) they are specific for a particular biomedical domain (such as cancer). We provide an open-source tool and web interface that improves on all of these issues.ResultsWe demonstrate SKiM’s ability to discover useful A-B-C linkages in three control experiments: classic LBD discoveries, drug repurposing, and finding associations related to cancer. Furthermore, we supplement SKiM with a knowledge graph built with transformer machine-learning models to aid in interpreting the relationships between terms found by SKiM. Finally, we provide a simple and intuitive open-source web interface (https://skim.morgridge.org) with comprehensive lists of drugs, diseases, phenotypes, and symptoms so that anyone can easily perform SKiM searches.ConclusionsSKiM is a simple algorithm that can perform LBD searches to discover relationships between arbitrary user-defined concepts. SKiM is generalized for any domain, can perform searches with many thousands of C term concepts, and moves beyond the simple identification of an existence of a relationship; many relationships are given relationship type labels from our knowledge graph.",
        "link": "http://dx.doi.org/10.1101/2023.05.30.542911"
    },
    {
        "id": 20784,
        "title": "SeqL at SemEval-2022 Task 11: An Ensemble of Transformer Based Models for Complex Named Entity Recognition Task",
        "authors": "Fadi Hassan, Wondimagegnhue Tufa, Guillem Collell, Piek Vossen, Lisa Beinborn, Adrian Flanagan, Kuan Eeik Tan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.218"
    },
    {
        "id": 20785,
        "title": "Comparing the New Improved <i>RLC</i> and CMTL Models for Measuring Partial Discharge in Transformer Winding",
        "authors": "Peyman Rezaei Baravati, Seyed Mohammad Hassan Hosseini, Majid Moazzami",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tim.2021.3088428"
    },
    {
        "id": 20786,
        "title": "Sentiment Analysis in Turkish Using Transformer-Based Deep Learning Models",
        "authors": "Oktay Ozturk, Alper Ozcan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-31956-3_1"
    },
    {
        "id": 20787,
        "title": "Transformer models for mining intents and predicting activities from emails in knowledge-intensive processes",
        "authors": "Faria Khandaker, Arik Senderovich, Junda Zhao, Eldan Cohen, Eric Yu, Sebastian Carbajales, Allen Chan",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107450"
    },
    {
        "id": 20788,
        "title": "Improving Natural Language Inference in Arabic Using Transformer Models and Linguistically Informed Pre-Training",
        "authors": "Mohammad Majd Saad Al Deen, Maren Pielka, Jörn Hees, Bouthaina Soulef Abdou, Rafet Sifa",
        "published": "2023-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371891"
    },
    {
        "id": 20789,
        "title": "Using Transformer Models for Knowledge Graph Construction in Computer Science Education",
        "authors": "Alexander Katyshev, Anton Anikin, Oleg Sychev",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3545947.3576365"
    },
    {
        "id": 20790,
        "title": "Identification of parameters of power transformer models using artificial intelligence methods",
        "authors": "A N Alyunov, O S Vyatkina",
        "published": "2023",
        "citations": 0,
        "abstract": "A large number of tasks for analyzing the state of power transformers are solved on the basis of mathematical models, the validity of which is undeniable. The disadvantage of standard methods for diagnosing current-carrying parts of transformers is the requirement to remove voltage. The applied diagnostic methods without stress relief require improvement in terms of increasing accuracy, speed and ensuring predictive response. The paper presents methods for identifying the parameters of mathematical models of power transformers using artificial neural networks.",
        "link": "http://dx.doi.org/10.1051/e3sconf/202341101001"
    },
    {
        "id": 20791,
        "title": "Calculation of Circuit Parameters of High Frequency Models for Power Transformers Using FEM",
        "authors": "Álvaro Portillo, Luiz Fernando de Oliveira, Federico Portillo",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_14"
    },
    {
        "id": 20792,
        "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models",
        "authors": "Ulme Wennberg, Gustav Eje Henter",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-short.18"
    },
    {
        "id": 20793,
        "title": "Discharge summary hospital course summarisation of in patient Electronic Health Record text with clinical concept guided deep pre-trained Transformer models",
        "authors": "Thomas Searle, Zina Ibrahim, James Teo, Richard J.B. Dobson",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2023.104358"
    },
    {
        "id": 20794,
        "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
        "authors": "S. B. Atallah, N. R. Banda, A. Banda, N. A. Roeck",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10151-023-02837-8"
    },
    {
        "id": 20795,
        "title": "Narrating Models of Development—China and the EU Between “Strategic Modernizer” and “Rules-Based Transformer”",
        "authors": "Constantin Holzer, Matthias Hackler",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-86122-3_2"
    },
    {
        "id": 20796,
        "title": "Emotion Recognition from Videos Using Transformer Models",
        "authors": "Prabhitha Nagarajan, Gem Rose Kuriakose, Arpana Dipak Mahajan, Selvakuberan Karuppasamy, Subhashini Lakshminarayanan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9819-5_4"
    },
    {
        "id": 20797,
        "title": "NCUEE-NLP at SemEval-2023 Task 8: Identifying Medical Causal Claims and Extracting PIO Frames Using the Transformer Models",
        "authors": "Lung-Hao Lee, Yuan-Hao Cheng, Jen-Hao Yang, Kao-Yuan Tien",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.42"
    },
    {
        "id": 20798,
        "title": "Ballistic fragmentation confinement of coated brittle transformer bushing models",
        "authors": "Christine N. Henderson, Charles S. DeFrance, Paul Predecki, Timothy V. McCloskey, Ellen Truitt, Joseph Hoffman, Maciej Kumosa",
        "published": "2018-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijimpeng.2018.08.017"
    },
    {
        "id": 20799,
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
        "authors": "Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.26"
    },
    {
        "id": 20800,
        "title": "Development and Validation of Deep Learning Transformer Models for Building a Comprehensive and Real-time Trauma Monitoring (Preprint)",
        "authors": "Gabrielle Chenais, Cédric Gil-Jardiné, Hélène Touchais, Marta Avalos Fernandez, Benjamin Contrand, Eric Tellier, Xavier Combes, Loick Bourdois, Philippe Revel, Emmanuel Lagarde",
        "published": "No Date",
        "citations": 1,
        "abstract": "\nBACKGROUND\nIn order to study the feasibility of setting up a national trauma observatory in France,\n\n\nOBJECTIVE\nwe compared the performance of several automatic language processing methods on a multi-class classification task of unstructured clinical notes.\n\n\nMETHODS\nA total of 69,110 free-text clinical notes related to visits to the emergency departments of the University Hospital of Bordeaux, France, between 2012 and 2019 were manually annotated. Among those clinical notes 22,481 were traumas. We trained 4 transformer models (deep learning models that encompass attention mechanism) and compared them with the TF-IDF (Term- Frequency - Inverse Document Frequency) associated with SVM (Support Vector Machine) method.\n\n\nRESULTS\nThe transformer models consistently performed better than TF-IDF/SVM. Among the transformers, the GPTanam model pre-trained with a French corpus with an additional auto-supervised learning step on 306,368 unlabeled clinical notes showed the best performance with a micro F1-score of 0.969.\n\n\nCONCLUSIONS\nThe transformers proved efficient multi-class classification task on narrative and medical data. Further steps for improvement should focus on abbreviations expansion and multiple outputs multi-class classification.\n",
        "link": "http://dx.doi.org/10.2196/preprints.40843"
    },
    {
        "id": 20801,
        "title": "Advances of Transformer-Based Models for News Headline Generation",
        "authors": "Alexey Bukhtiyarov, Ilya Gusev",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59082-6_4"
    },
    {
        "id": 20802,
        "title": "On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs",
        "authors": "Jarrod Ragsdale, Rajendra V. Boppana",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3326104"
    },
    {
        "id": 20803,
        "title": "Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues",
        "authors": "Avisha Das, Salih Selek, Alia R. Warner, Xu Zuo, Yan Hu, Vipina Kuttichi Keloth, Jianfu Li, W. Jim Zheng, Hua Xu",
        "published": "2022",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.bionlp-1.27"
    },
    {
        "id": 20804,
        "title": "A bio-inspired positional embedding network for transformer-based models",
        "authors": "Xue-song Tang, Kuangrong Hao, Hui Wei",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.015"
    },
    {
        "id": 20805,
        "title": "Transformer-based anti-noise models for CRISPR-Cas9 off-target activities prediction",
        "authors": "Zengrui Guan, Zhenran Jiang",
        "published": "2023-5-19",
        "citations": 2,
        "abstract": "Abstract\nThe off-target effect occurring in the CRISPR-Cas9 system has been a challenging problem for the practical application of this gene editing technology. In recent years, various prediction models have been proposed to predict potential off-target activities. However, most of the existing prediction methods do not fully exploit guide RNA (gRNA) and DNA sequence pair information effectively. In addition, available prediction methods usually ignore the noise effect in original off-target datasets. To address these issues, we design a novel coding scheme, which considers the key features of mismatch type, mismatch location and the gRNA-DNA sequence pair information. Furthermore, a transformer-based anti-noise model called CrisprDNT is developed to solve the noise problem that exists in the off-target data. Experimental results of eight existing datasets demonstrate that the method with the inclusion of the anti-noise loss functions is superior to available state-of-the-art prediction methods. CrisprDNT is available at https://github.com/gzrgzx/CrisprDNT.",
        "link": "http://dx.doi.org/10.1093/bib/bbad127"
    },
    {
        "id": 20806,
        "title": "Forecasting the S&amp;P 500 Index Using Mathematical-Based Sentiment Analysis and Deep Learning Models: A FinBERT Transformer Model and LSTM",
        "authors": "Jihwan Kim, Hui-Sang Kim, Sun-Yong Choi",
        "published": "2023-8-29",
        "citations": 2,
        "abstract": "Stock price prediction has been a subject of significant interest in the financial mathematics field. Recently, interest in natural language processing models has increased, and among them, transformer models, such as BERT and FinBERT, are attracting attention. This study uses a mathematical framework to investigate the effects of human sentiment on stock movements, especially in text data. In particular, FinBERT, a domain-specific language model based on BERT tailored for financial language, was employed for the sentiment analysis on the financial texts to extract sentiment information. In this study, we use “summary” text data extracted from The New York Times, representing concise summaries of news articles. Accordingly, we apply FinBERT to the summary text data to calculate sentiment scores. In addition, we employ the LSTM (Long short-term memory) methodology, one of the machine learning models, for stock price prediction using sentiment scores. Furthermore, the LSTM model was trained by stock price data and the estimated sentiment scores. We compared the predictive power of LSTM models with and without sentiment analysis based on error measures such as MSE, RMSE, and MAE. The empirical results demonstrated that including sentiment scores through the LSTM model led to improved prediction accuracy for all three measures. These findings indicate the significance of incorporating news sentiment into stock price predictions, shedding light on the potential impact of psychological factors on financial markets. By using the FinBERT transformer model, this study aimed to investigate the interplay between sentiment and stock price predictions, contributing to a deeper understanding of mathematical-based sentiment analysis in finance and its role in enhancing forecasting in financial mathematics. Furthermore, we show that using summary data instead of entire news articles is a useful strategy for mathematical-based sentiment analysis.",
        "link": "http://dx.doi.org/10.3390/axioms12090835"
    },
    {
        "id": 20807,
        "title": "Investigations on Magnetization Characteristics of Transformer Models with Single-Phase Four-limb Core Composed of Different Grades of Steel Sheets",
        "authors": "Li Gang, Wang Ke, Zhang Shuqi, Liu Xueli, Li Jinzhong, Tang Hao, Yu Xinru, Wu Xingwang",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec48194.2019.8975014"
    },
    {
        "id": 20808,
        "title": "Bayesian Transformer Language Models for Speech Recognition",
        "authors": "Boyang Xue, Jianwei Yu, Junhao Xu, Shansong Liu, Shoukang Hu, Zi Ye, Mengzhe Geng, Xunying Liu, Helen Meng",
        "published": "2021-6-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp39728.2021.9414046"
    },
    {
        "id": 20809,
        "title": "Dual mode information fusion with pre-trained CNN models and transformer for video-based non-invasive anaemia detection",
        "authors": "Abhishek Kesarwani, Sunanda Das, Dakshina Ranjan Kisku, Mamata Dalui",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.105592"
    },
    {
        "id": 20810,
        "title": "Early Depression Detection with Transformer Models: Analyzing the Relationship between Linguistic and Psychology-Based Features",
        "authors": "Haya Halimeh, Matthew Caron, Oliver Müller",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24251/hicss.2023.415"
    },
    {
        "id": 20811,
        "title": "Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models",
        "authors": "Akhilesh Deepak Gotmare, Junnan Li, Shafiq Joty, Steven C.H. Hoi",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3611643.3616369"
    },
    {
        "id": 20812,
        "title": "DeepTweet: Leveraging Transformer-based Models for Enhanced Fake News Detection in Twitter Sentiment Analysis",
        "authors": "N. Vallileka, P. Sundaravadivel, U. Karthikeyan, R. Santhana Krishnan, K. Lakshmi Narayanan, S. Sundararajan",
        "published": "2023-10-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i-smac58438.2023.10290217"
    },
    {
        "id": 20813,
        "title": "Impact of Electromagnetic and Optical CTs on Transformer Differential Protection during Transformer Re-energization",
        "authors": "Nikolay Ivanov, Ruslan Kanafeev, Vladimir Terzija",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202781"
    },
    {
        "id": 20814,
        "title": "LF-Transformer: Latent Factorizer Transformer for Tabular Learning",
        "authors": "Kwangtek Na, Ju-Hong Lee, Eunchan Kim",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3354972"
    },
    {
        "id": 20815,
        "title": "Serial KinderMiner (SKiM) Discovers and Annotates Biomedical Knowledge Using Co-Occurrence and Transformer Models",
        "authors": "Robert J. Millikin, Kalpana Raja, John Steill, Cannon Lock, Xuancheng Tu, Ian Ross, Lam C Tsoi, Finn Kuusisto, Zijian Ni, Miron Livny, Brian Bockelman, James Thomson, Ron Stewart",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackgroundThe PubMed database contains more than 34 million articles; consequently, it is becoming increasingly difficult for a biomedical researcher to keep up-to-date with different knowledge domains. Computationally efficient and interpretable tools are needed to help researchers find and understand associations between biomedical concepts. The goal of literature-based discovery (LBD) is to connect concepts in isolated literature domains that would normally go undiscovered. This usually takes the form of an A-B-C relationship, where A and C terms are linked through a B term intermediate. Here we describe Serial KinderMiner (SKiM), an LBD algorithm for finding statistically significant links between an A term and one or more C terms through some B term intermediate(s). The development of SKiM is motivated by the the observation that there are only a few LBD tools that provide a functional web interface, and that the available tools are limited in one or more of the following ways: 1) they identify a relationship but not the type of relationship, 2) they do not allow the user to provide their own lists of B or C terms, hindering flexibility, 3) they do not allow for querying thousands of C terms (which is crucial if, for instance, the user wants to query connections between a disease and the thousands of available drugs), or 4) they are specific for a particular biomedical domain (such as cancer). We provide an open-source tool and web interface that improves on all of these issues.ResultsWe demonstrate SKiM’s ability to discover useful A-B-C linkages in three control experiments: classic LBD discoveries, drug repurposing, and finding associations related to cancer. Furthermore, we supplement SKiM with a knowledge graph built with transformer machine-learning models to aid in interpreting the relationships between terms found by SKiM. Finally, we provide a simple and intuitive open-source web interface (https://skim.morgridge.org) with comprehensive lists of drugs, diseases, phenotypes, and symptoms so that anyone can easily perform SKiM searches.ConclusionsSKiM is a simple algorithm that can perform LBD searches to discover relationships between arbitrary user-defined concepts. SKiM is generalized for any domain, can perform searches with many thousands of C term concepts, and moves beyond the simple identification of an existence of a relationship; many relationships are given relationship type labels from our knowledge graph.",
        "link": "http://dx.doi.org/10.1101/2023.05.30.542911"
    },
    {
        "id": 20816,
        "title": "SeqL at SemEval-2022 Task 11: An Ensemble of Transformer Based Models for Complex Named Entity Recognition Task",
        "authors": "Fadi Hassan, Wondimagegnhue Tufa, Guillem Collell, Piek Vossen, Lisa Beinborn, Adrian Flanagan, Kuan Eeik Tan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.218"
    },
    {
        "id": 20817,
        "title": "Comparing the New Improved <i>RLC</i> and CMTL Models for Measuring Partial Discharge in Transformer Winding",
        "authors": "Peyman Rezaei Baravati, Seyed Mohammad Hassan Hosseini, Majid Moazzami",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tim.2021.3088428"
    },
    {
        "id": 20818,
        "title": "Sentiment Analysis in Turkish Using Transformer-Based Deep Learning Models",
        "authors": "Oktay Ozturk, Alper Ozcan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-31956-3_1"
    },
    {
        "id": 20819,
        "title": "Creating and Comparing Dictionary, Word Embedding, and Transformer-Based Models to Measure Discrete Emotions in German Political Text",
        "authors": "Tobias Widmann, Maximilian Wich",
        "published": "2023-10",
        "citations": 6,
        "abstract": "AbstractPrevious research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowd-coded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.",
        "link": "http://dx.doi.org/10.1017/pan.2022.15"
    },
    {
        "id": 20820,
        "title": "Transformer models for mining intents and predicting activities from emails in knowledge-intensive processes",
        "authors": "Faria Khandaker, Arik Senderovich, Junda Zhao, Eldan Cohen, Eric Yu, Sebastian Carbajales, Allen Chan",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107450"
    },
    {
        "id": 20821,
        "title": "Transformer and Reactor Testing: Temperature Rise",
        "authors": "Tom Breckenridge, Santhiago Montenegro",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_13"
    },
    {
        "id": 20822,
        "title": "Transformer 4.0 - A Smart Transformer for a Smarter Living",
        "authors": "Sanjay S Tippannavar, Yashwanth D",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisct57197.2023.10351417"
    },
    {
        "id": 20823,
        "title": "STS-Transformer:&amp;nbsp;A Spatiotemporal Symmetrical Transformer Structure for EEG Emotion Recognition",
        "authors": "bo pan, wei zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4142410"
    },
    {
        "id": 20824,
        "title": "Overview of Transformer and Reactor Procurement",
        "authors": "Tom Breckenridge, Simon Ryder",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_1"
    },
    {
        "id": 20825,
        "title": "The Development and Evaluation of a High-Frequency Toroidal Transformer for Solid-State Transformer Applications",
        "authors": "Abdul Shakoor, Azhar Ul Haq, Taosif Iqbal",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023045011"
    },
    {
        "id": 20826,
        "title": "High-frequency transformer design for the soft-switching solid state transformer (S4T)",
        "authors": "Hao Chen, Deepak Divan",
        "published": "2017-3",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec.2017.7931054"
    },
    {
        "id": 20827,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v1/review2"
    },
    {
        "id": 20828,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v1/review1"
    },
    {
        "id": 20829,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v3/review2"
    },
    {
        "id": 20830,
        "title": "Transformer Device",
        "authors": " ",
        "published": "2020-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/tp2m0a"
    },
    {
        "id": 20831,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v1/review3"
    },
    {
        "id": 20832,
        "title": "An introduction to power transformer monitoring",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00001-x"
    },
    {
        "id": 20833,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v3/review1"
    },
    {
        "id": 20834,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v2/review2"
    },
    {
        "id": 20835,
        "title": "Transformer",
        "authors": "Andrew Murphy, Zemar Vajuhudeen",
        "published": "2020-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-79689"
    },
    {
        "id": 20836,
        "title": "Intelligent Transformer (I-Transformer)",
        "authors": "Kiran Sara Thomas, Melbin Benny, Roy J Mathew, Sebin Jose",
        "published": "2021-4-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31871/ijntr.7.4.18"
    },
    {
        "id": 20837,
        "title": "Comparative Analysis of Vibration Signal of Converter Transformer and AC Power Transformer",
        "authors": "Zhicheng Pan, Jun Deng",
        "published": "2022-5-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cieec54735.2022.9846199"
    },
    {
        "id": 20838,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v1/review4"
    },
    {
        "id": 20839,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v2/review1"
    },
    {
        "id": 20840,
        "title": "Review for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v2/review3"
    },
    {
        "id": 20841,
        "title": "Context-Based Narrative Generation Transformer (NGen-Transformer)",
        "authors": "Abraar Raza Samar, Bostan Khan, Adeel Mumtaz",
        "published": "2022-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ibcast54850.2022.9990496"
    },
    {
        "id": 20842,
        "title": "Transformer Based Models in Fake News Detection",
        "authors": "Sebastian Kula, Rafał Kozik, Michał Choraś, Michał Woźniak",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77970-2_3"
    },
    {
        "id": 20843,
        "title": "Comparative Analysis of Phasor-Phasor and Detailed-Phasor Models of Regulating Transformer",
        "authors": "Nitin Karnwal, Mukul Singh, Nidhi Singh, M. A. Ansari",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-8721-1_4"
    },
    {
        "id": 20844,
        "title": "Molecular language models: RNNs or transformer?",
        "authors": "Yangyang Chen, Zixu Wang, Xiangxiang Zeng, Yayang Li, Pengyong Li, Xiucai Ye, Tetsuya Sakurai",
        "published": "2023-7-17",
        "citations": 1,
        "abstract": "Abstract\nLanguage models have shown the capacity to learn complex molecular distributions. In the field of molecular generation, they are designed to explore the distribution of molecules, and previous studies have demonstrated their ability to learn molecule sequences. In the early times, recurrent neural networks (RNNs) were widely used for feature extraction from sequence data and have been used for various molecule generation tasks. In recent years, the attention mechanism for sequence data has become popular. It captures the underlying relationships between words and is widely applied to language models. The Transformer-Layer, a model based on a self-attentive mechanism, also shines the same as the RNN-based model. In this research, we investigated the difference between RNNs and the Transformer-Layer to learn a more complex distribution of molecules. For this purpose, we experimented with three different generative tasks: the distributions of molecules with elevated scores of penalized LogP, multimodal distributions of molecules and the largest molecules in PubChem. We evaluated the models on molecular properties, basic metrics, Tanimoto similarity, etc. In addition, we applied two different representations of the molecule, SMILES and SELFIES. The results show that the two language models can learn complex molecular distributions and SMILES-based representation has better performance than SELFIES. The choice between RNNs and the Transformer-Layer needs to be based on the characteristics of dataset. RNNs work better on data focus on local features and decreases with multidistribution data, while the Transformer-Layer is more suitable when meeting molecular with larger weights and focusing on global features.",
        "link": "http://dx.doi.org/10.1093/bfgp/elad012"
    },
    {
        "id": 20845,
        "title": "Am I hurt?: Evaluating Psychological Pain Detection in Hindi Text using Transformer-based Models",
        "authors": "Ravleen Kaur, M. P. S. Bhatia, Akshi Kumar",
        "published": "2024-3-5",
        "citations": 0,
        "abstract": "\n            The automated evaluation of pain is critical for developing effective pain management approaches that seek to alleviate while preserving patients’ functioning. Transformer-based models can aid in detecting pain from Hindi text data gathered from social media by leveraging their ability to capture complex language patterns and contextual information. By understanding the nuances and context of Hindi text, transformer models can effectively identify linguistic cues, sentiment and expressions associated with pain enabling the detection and analysis of pain-related content present in social media posts. The purpose of this research is to analyse the feasibility of utilizing NLP techniques to automatically identify pain within Hindi textual data, providing a valuable tool for pain assessment in Hindi-speaking populations. The research showcases the HindiPainNet model, a deep neural network that employs the IndicBERT model, classifying the dataset into two class labels {pain, no_pain} for detecting pain in Hindi textual data. The model is trained and tested using a novel dataset, दर्द-ए-शायरी (pronounced as\n            Dard-e-Shayari\n            ) curated using posts from social media platforms. The results demonstrate the model's effectiveness, achieving an accuracy of 70.5%. This pioneer research highlights the potential of utilizing textual data from diverse sources to identify and understand pain experiences based on psychosocial factors. This research could pave the path for the development of automated pain assessment tools that help medical professionals comprehend and treat pain in Hindi speaking populations. Additionally, it opens avenues to conduct further NLP-based multilingual pain detection research, addressing the needs of diverse language communities.\n          ",
        "link": "http://dx.doi.org/10.1145/3650206"
    },
    {
        "id": 20846,
        "title": "ViTRMSE: a three-dimensional RMSE scoring method for protein-ligand docking models based on Vision Transformer",
        "authors": "Linyuan Guo, Jianxin Wang",
        "published": "2022-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995694"
    },
    {
        "id": 20847,
        "title": "Practical partial discharge pulse generation and location within transformer windings using regression models adjusted with simulated signals",
        "authors": "Arismar Morais Gonçalves Júnior, Hélder de Paula, Wallace do Couto Boaventura",
        "published": "2018-4",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2017.12.013"
    },
    {
        "id": 20848,
        "title": "Parallelizable Loop Detection using Pre-trained Transformer Models for Code Understanding",
        "authors": "Soratouch Pornmaneerattanatri, Keichi Takahashi, Yutaro Kashiwa, Kohei Ichikawa, Hajimu Iida",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-8211-0_4"
    },
    {
        "id": 20849,
        "title": "Identification of asymptomatic COVID-19 patients on chest CT images using Transformer-or CNN-based deep learning models",
        "authors": "Minyue Yin, Xiaolong Liang, Zilan Wang, Yijia Zhou, Yu He, Yuhan Xue, Jingwen Gao, Jiaxi Lin, Chenyan Yu, Lu Liu, Xiaolin Liu, Chao Xu, Jinzhou Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackgroud\n To evaluate the feasibility of deep learning (DL) models in identifying asymptomatic COVID-19 patients, based on chest CT images.\nMethods\n In this retrospective study, six DL models (Xception, NASNet, ResNet, EfficientNet, ViT, and Swin), based on convolutional neural networks (CNNs)-or Transformer-architectures, were trained to identify asymptomatic patients with COVID-19 on chest CT images. Data from Yangzhou was randomly split into the training set (n = 2,140) and the internal-validation set (n = 360). Data from Suzhou was the external-test set (n = 200). Models’ performance was assessed by accuracy, recall and specificity and was compared with that of two radiologists.\nResults\n A total of 2,700 chest CT images were collected in this study. In the validation dataset, the Swin model achieved the highest accuracy of 0.994, followed by EfficientNet model (0.954). The recall and precision of the Swin model were 0.989 and 1.000. In the test dataset, the Swin model still was the best that achieved the highest accuracy (0.980). All the DL models performed remarkable than two experts. Lastly, the time on the test set diagnosis spent by two experts 42min17s (Junior) and 29min43s (Senior), was significantly higher than that of those DL models (all below 2min).\nConclusions\n This study evaluated the feasibility of multiple DL models in distinguishing asymptomatic patients with COVID-19 from healthy subjects on chest CT images. It found a Transformer model, the Swin model, performed best.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1590051/v1"
    },
    {
        "id": 20850,
        "title": "InSAR time-series deformation forecasting surrounding Salt Lake using deep transformer models",
        "authors": "Jing Wang, Chao Li, Lu Li, Zhihua Huang, Chao Wang, Hong Zhang, Zhengjia Zhang",
        "published": "2023-2",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.scitotenv.2022.159744"
    },
    {
        "id": 20851,
        "title": "Question-aware transformer models for consumer health question summarization",
        "authors": "Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina Demner-Fushman",
        "published": "2022-4",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2022.104040"
    },
    {
        "id": 20852,
        "title": "Analysis of the Impact of Power Transformer Loading on the Transformer Oil Aging Intensity",
        "authors": "Oleg Shutenko, Serhii Ponomarenko",
        "published": "2020-10-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/khpiweek51551.2020.9250159"
    },
    {
        "id": 20853,
        "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
        "authors": "Marie Lopez, Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Thomas Pierrot",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nClosing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the sequence representations alone match or outperform specialized methods on 12 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2644486/v1"
    },
    {
        "id": 20854,
        "title": "MSR92 Can Artificial Intelligence (AI) Large Language Models (LLMS) Such as Generative Pre-Trained Transformer (GPT) Be Used to Automate Literature Reviews?",
        "authors": "I. Guerra, J. Gallinaro, K. Rtveladze, A. Lambova, E. Asenova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jval.2023.09.2151"
    },
    {
        "id": 20855,
        "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale",
        "authors": "Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, Yuxiong He",
        "published": "2022-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sc41404.2022.00051"
    },
    {
        "id": 20856,
        "title": "Improving Zero-Shot Cross-Lingual Hate Speech Detection with Pseudo-Label Fine-Tuning of Transformer Language Models",
        "authors": "Haris Bin Zia, Ignacio Castro, Arkaitz Zubiaga, Gareth Tyson",
        "published": "2022-5-31",
        "citations": 3,
        "abstract": "Hate speech has proliferated on social media platforms in recent years. While this has been the focus of many studies, most works have exclusively focused on a single language, generally English. Low-resourced languages have been neglected due to the dearth of labeled resources. These languages, however, represent an important portion of the data due to the multilingual nature of social media. This work presents a novel zero-shot, cross-lingual transfer learning pipeline based on pseudo-label fine-tuning of Transformer Language Models for automatic hate speech detection. We employ our pipeline on benchmark datasets covering English (source) and 6 different non-English (target) languages written in 3 different scripts. Our pipeline achieves an average improvement of 7.6% (in terms of macro-F1) over previous zero-shot, cross-lingual models. This demonstrates the feasibility of high accuracy automatic hate speech detection for low-resource languages. We release our code and models at https://github.com/harisbinzia/ZeroshotCrosslingualHateSpeech.",
        "link": "http://dx.doi.org/10.1609/icwsm.v16i1.19402"
    },
    {
        "id": 20857,
        "title": "Visual Transformer-Based Models: A Survey",
        "authors": "Xiaonan Huang, Ning Bi, Jun Tan",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09282-4_25"
    },
    {
        "id": 20858,
        "title": "Phase-Shifting Control Strategy for MMC-Based DC Transformer Using DAB Virtual Models",
        "authors": "YeQiang Zhang, ChuYang Wang, Li Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-97-0869-7_69"
    },
    {
        "id": 20859,
        "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
        "authors": "Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song",
        "published": "2024-3-31",
        "citations": 21,
        "abstract": "Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
        "link": "http://dx.doi.org/10.1145/3617680"
    },
    {
        "id": 20860,
        "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
        "authors": "Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, Peter Ramadge",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.102"
    },
    {
        "id": 20861,
        "title": "Integration of an improved transformer with physical models for the spatiotemporal simulation of urban flooding depths",
        "authors": "Hengxu Jin, Haipeng Lu, Yu Zhao, Zhizhou Zhu, Wujie Yan, Qiqi Yang, Shuliang Zhang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ejrh.2023.101627"
    },
    {
        "id": 20862,
        "title": "Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)–Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study (Preprint)",
        "authors": "José Alberto Benítez-Andrades, José-Manuel Alija-Pérez, Maria-Esther Vidal, Rafael Pastor-Vargas, María Teresa García-Ordás",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nEating disorders affect an increasing number of people. Social networks provide information that can help.\n\n\nOBJECTIVE\nWe aimed to find machine learning models capable of efficiently categorizing tweets about eating disorders domain.\n\n\nMETHODS\nWe collected tweets related to eating disorders, for 3 consecutive months. After preprocessing, a subset of 2000 tweets was labeled: (1) messages written by people suffering from eating disorders or not, (2) messages promoting suffering from eating disorders or not, (3) informative messages or not, and (4) scientific or nonscientific messages. Traditional machine learning and deep learning models were used to classify tweets. We evaluated accuracy, F1 score, and computational time for each model.\n\n\nRESULTS\nA total of 1,058,957 tweets related to eating disorders were collected. were obtained in the 4 categorizations, with The bidirectional encoder representations from transformer–based models had the best score among the machine learning and deep learning techniques applied to the 4 categorization tasks (F1 scores 71.1%-86.4%).\n\n\nCONCLUSIONS\nBidirectional encoder representations from transformer–based models have better performance, although their computational cost is significantly higher than those of traditional techniques, in classifying eating disorder–related tweets.\n",
        "link": "http://dx.doi.org/10.2196/preprints.34492"
    },
    {
        "id": 20863,
        "title": "Fake News Detection: GA-Transformer And IG-Transformer Based Approach",
        "authors": "Vivek Ranjan, Pragati Agrawal",
        "published": "2022-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/confluence52989.2022.9734180"
    },
    {
        "id": 20864,
        "title": "Generating Competitive Priority Strategy in Transformer Industry",
        "authors": "Rita Ambarwati",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007414600280035"
    },
    {
        "id": 20865,
        "title": "Comparison of pretrained transformer-based models for influenza and COVID-19 detection using social media text data in Saskatchewan, Canada",
        "authors": "Yuan Tian, Wenjing Zhang, Lujie Duan, Wade McDonald, Nathaniel Osgood",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "BackgroundThe use of social media data provides an opportunity to complement traditional influenza and COVID-19 surveillance methods for the detection and control of outbreaks and informing public health interventions.ObjectiveThe first aim of this study is to investigate the degree to which Twitter users disclose health experiences related to influenza and COVID-19 that could be indicative of recent plausible influenza cases or symptomatic COVID-19 infections. Second, we seek to use the Twitter datasets to train and evaluate the classification performance of Bidirectional Encoder Representations from Transformers (BERT) and variant language models in the context of influenza and COVID-19 infection detection.MethodsWe constructed two Twitter datasets using a keyword-based filtering approach on English-language tweets collected from December 2016 to December 2022 in Saskatchewan, Canada. The influenza-related dataset comprised tweets filtered with influenza-related keywords from December 13, 2016, to March 17, 2018, while the COVID-19 dataset comprised tweets filtered with COVID-19 symptom-related keywords from January 1, 2020, to June 22, 2021. The Twitter datasets were cleaned, and each tweet was annotated by at least two annotators as to whether it suggested recent plausible influenza cases or symptomatic COVID-19 cases. We then assessed the classification performance of pre-trained transformer-based language models, including BERT-base, BERT-large, RoBERTa-base, RoBERT-large, BERTweet-base, BERTweet-covid-base, BERTweet-large, and COVID-Twitter-BERT (CT-BERT) models, on each dataset. To address the notable class imbalance, we experimented with both oversampling and undersampling methods.ResultsThe influenza dataset had 1129 out of 6444 (17.5%) tweets annotated as suggesting recent plausible influenza cases. The COVID-19 dataset had 924 out of 11939 (7.7%) tweets annotated as inferring recent plausible COVID-19 cases. When compared against other language models on the COVID-19 dataset, CT-BERT performed the best, supporting the highest scores for recall (94.8%), F1(94.4%), and accuracy (94.6%). For the influenza dataset, BERTweet models exhibited better performance. Our results also showed that applying data balancing techniques such as oversampling or undersampling method did not lead to improved model performance.ConclusionsUtilizing domain-specific language models for monitoring users’ health experiences related to influenza and COVID-19 on social media shows improved classification performance and has the potential to supplement real-time disease surveillance.",
        "link": "http://dx.doi.org/10.3389/fdgth.2023.1203874"
    },
    {
        "id": 20866,
        "title": "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy",
        "authors": "Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H. Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, Teodoro Laino",
        "published": "2020",
        "citations": 169,
        "abstract": "We present an extension of our Molecular Transformer model combined with a hyper-graph exploration strategy for automatic retrosynthesis route planning without human intervention.",
        "link": "http://dx.doi.org/10.1039/c9sc05704h"
    },
    {
        "id": 20867,
        "title": "Simple and Effective Multimodal Learning Based on Pre-Trained Transformer Models",
        "authors": "Kazuki Miyazawa, Yuta Kyuragi, Takayuki Nagai",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3159346"
    },
    {
        "id": 20868,
        "title": "A Fault Diagnosis Strategy for Analog Circuits with Limited Samples Based on the Combination of the Transformer and Generative Models",
        "authors": "Zhen Jia, Qiqi Yang, Yang Li, Siyu Wang, Peng Xu, Zhenbao Liu",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "As a pivotal integral component within electronic systems, analog circuits are of paramount importance for the timely detection and precise diagnosis of their faults. However, the objective reality of limited fault samples in operational devices with analog circuitry poses challenges to the direct applicability of existing diagnostic methods. This study proposes an innovative approach for fault diagnosis in analog circuits by integrating deep convolutional generative adversarial networks (DCGANs) with the Transformer architecture, addressing the problem of insufficient fault samples affecting diagnostic performance. Firstly, the employment of the continuous wavelet transform in combination with Morlet wavelet basis functions serves as a means to derive time–frequency images, enhancing fault feature recognition while converting time-domain signals into time–frequency representations. Furthermore, the augmentation of datasets utilizing deep convolutional GANs is employed to generate synthetic time–frequency signals from existing fault data. The Transformer-based fault diagnosis model was trained using a mixture of original signals and generated signals, and the model was subsequently tested. Through experiments involving single and multiple fault scenarios in three simulated circuits, a comparative analysis of the proposed approach was conducted with a number of established benchmark methods, and its effectiveness in various scenarios was evaluated. In addition, the ability of the proposed fault diagnosis technique was investigated in the presence of limited fault data samples. The outcome reveals that the proposed diagnostic method exhibits a consistently high overall accuracy of over 96% in diverse test scenarios. Moreover, it delivers satisfactory performance even when real sample sizes are as small as 150 instances in various fault categories.",
        "link": "http://dx.doi.org/10.3390/s23229125"
    },
    {
        "id": 20869,
        "title": "Investigation of Partial Discharge Due to Copper Spherical Particle in Power Transformer Under Various Oil Flow Models Using CFD",
        "authors": "N. Vasantha Gowri",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-1616-0_38"
    },
    {
        "id": 20870,
        "title": "Comparative assessment of generative models for transformer- and consumer-level load profiles generation",
        "authors": "Weijie Xia, Hanyue Huang, Edgar Mauricio Salazar Duque, Shengren Hou, Peter Palensky, Pedro P. Vergara",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.segan.2024.101338"
    },
    {
        "id": 20871,
        "title": "TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models",
        "authors": "Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei",
        "published": "2023-6-26",
        "citations": 29,
        "abstract": "Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26538"
    },
    {
        "id": 20872,
        "title": "Augmenting human innovation teams with artificial intelligence: Exploring transformer‐based language models",
        "authors": "Sebastian G. Bouschery, Vera Blazevic, Frank T. Piller",
        "published": "2023-3",
        "citations": 44,
        "abstract": "AbstractThe use of transformer‐based language models in artificial intelligence (AI) has increased adoption in various industries and led to significant productivity advancements in business operations. This article explores how these models can be used to augment human innovation teams in the new product development process, allowing for larger problem and solution spaces to be explored and ultimately leading to higher innovation performance. The article proposes the use of the AI‐augmented double diamond framework to structure the exploration of how these models can assist in new product development (NPD) tasks, such as text summarization, sentiment analysis, and idea generation. It also discusses the limitations of the technology and the potential impact of AI on established practices in NPD. The article establishes a research agenda for exploring the use of language models in this area and the role of humans in hybrid innovation teams. (Note: Following the idea of this article, GPT‐3 alone generated this abstract. Only minor formatting edits were performed by humans.)",
        "link": "http://dx.doi.org/10.1111/jpim.12656"
    },
    {
        "id": 20873,
        "title": "Transformer and Reactor Testing – Introduction and Performance Tests",
        "authors": "Tom Breckenridge, Santhiago Montenegro",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_12"
    },
    {
        "id": 20874,
        "title": "Introduction to Power Transformer Protection",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_1"
    },
    {
        "id": 20875,
        "title": "Comparisons of the AC and DC Magnetization Characteristics of Transformer Models with Single-Phase Four-Limb Core Composed of Different Grades of Steel Sheets",
        "authors": "Li Gang, Wang Ke, Zhang Shuqi, Liu Xueli, Li Jinzhong, Yu Xinru, Ding Guocheng",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec48194.2019.8975237"
    },
    {
        "id": 20876,
        "title": "GazPNE2: A General Place Name Extractor for Microblogs Fusing Gazetteers and Pretrained Transformer Models",
        "authors": "Xuke Hu, Zhiyong Zhou, Yeran Sun, Jens Kersten, Friederike Klan, Hongchao Fan, Matti Wiegmann",
        "published": "2022-9-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jiot.2022.3150967"
    },
    {
        "id": 20877,
        "title": "Total electricity consumption forecasting based on Transformer time series models",
        "authors": "Xuerong Li, Yiqiang Zhong, Wei Shang, Xun Zhang, Baoguo Shan, Xiang Wang",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.11.180"
    },
    {
        "id": 20878,
        "title": "A performance analysis of transformer-based deep learning models for Arabic image captioning",
        "authors": "Ashwaq Alsayed, Thamir M. Qadah, Muhammad Arif",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jksuci.2023.101750"
    },
    {
        "id": 20879,
        "title": "The Effectiveness of Transformer-Based Models for BEC Attack Detection",
        "authors": "Amirah Almutairi, BooJoong Kang, Nawfal Fadhel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-39828-5_5"
    },
    {
        "id": 20880,
        "title": "Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model",
        "authors": "Emre Sezgin, Joseph Sirrianni, Simon L Linwood",
        "published": "2022-2-10",
        "citations": 52,
        "abstract": "Generative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In contrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited training data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like logical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching pronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in health care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we briefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical practice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure, (2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational factors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability Act compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint can inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful artificial intelligence tools integrated into hospital systems and health care.",
        "link": "http://dx.doi.org/10.2196/32875"
    },
    {
        "id": 20881,
        "title": "Optimizing Chromosome Analysis through VGT-MS: Leveraging Visual Geometric Transformer, VGG-16, and Vision Transformer for Enhanced Abnormality Identification",
        "authors": "Nelliyadan Nimitha, Periyathambi Ezhumalai, Arun Chokkalingam",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nChromosomes possess genetic information about the human body and their structures look like threads in the cell nucleus. Their analysis is crucial work and it is referred to as karyotyping, in which the identifications of the abnormalities from the chromosome are carried out. An efficient approach is needed to identify the abnormalities in the chromosome, even though several methods are developed their performance for detection is not efficient and encounters more problems like being time-consuming, feature extraction is not effective, etc. Therefore an effective model named Visual Geometric Transformer-based Mantis search (VGT-MS) algorithm is proposed to perform abnormality detection by overcoming the above mentioned issues. In the beginning, two datasets are exploited in the work, which comprise more chromosome images. The collection of images is carried out utilizing those two datasets such as the Chromosome karyotype Images dataset and CRCN-NE Chromosomes DataSet. But, these images contain various unnecessary things that should be eliminated. In this context, four pre-processing steps are included for the pre-processing such as Noise elimination, Contrast enhancement, Image resizing, and Normalization. After this, the VGG-16 model is deployed for making the successful feature extraction, and the Vision Transformer for the identification of the Chromosome abnormality is utilized. An initial strategy-based Mantis Search Algorithm is adopted for the hyperparameter tuning in this work. The evaluation of the developed model’s effectiveness is accomplished through F1-score, accuracy, recall, ROC, and precision where its performance is the finest performance among existing methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3872462/v1"
    },
    {
        "id": 20882,
        "title": "Detection Transformer: Ultrasonic Echo Signal Inclusion Detection with Transformer",
        "authors": "Xiaoxin Fang, Xueqi Yang, Fei Wang, Xiong Chen",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc55942.2022.10004075"
    },
    {
        "id": 20883,
        "title": "Solid state transformer parallel operation with a tap changing line frequency transformer",
        "authors": "Nuwantha Fernando, Lasantha Meegahapola, Chathura Thilakarathne",
        "published": "2017-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt-asia.2017.8378448"
    },
    {
        "id": 20884,
        "title": "Comparison between transformer, informer, autoformer and non-stationary transformer in financial market",
        "authors": "Yumin Wu",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "This paper delves into the significance of predicting stock prices and carries out comparative experiments using a variety of models, including Support Vector Regression, Long Short-Term Memory model, Transformer, Informer, Autoformer, and Non-Stationary Transformer. These models are used to train and forecast the China Securities Index, Hang Seng Index, and S&P 500 Index. The results of the experiments are measured using indicators such as Mean Absolute Error and Root Mean Square Error. The findings show that the Non-Stationary Transformer model has the highest prediction accuracy. Additionally, a simple trading strategy is designed for each model and their Sharpe and Calmar ratios are compared. Since Autoformer has the highest Sharpe and Calmar, it can be concluded that Autoformer is the most practical in financial market among the four models. This research contributes to the field of stock price prediction by providing an empirical study on the application of Transformer and its derivative models which have been less explored in this domain. In conclusion, this paper offers valuable insights and recommendations for data scientists and financial engineer and introduces new methods for predicting stock prices.",
        "link": "http://dx.doi.org/10.54254/2755-2721/29/20230874"
    },
    {
        "id": 20885,
        "title": "Relevance Vector Machine Based Transformer Protection",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_5"
    },
    {
        "id": 20886,
        "title": "HE-ELM Technique Based Transformer Protection",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_6"
    },
    {
        "id": 20887,
        "title": "High-Frequency Transformer Review and Design for Low-Power Solid-State Transformer Topology",
        "authors": "Akrem Mohamed Elrajoubi, Simon S. Ang",
        "published": "2019-2",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpec.2019.8662131"
    },
    {
        "id": 20888,
        "title": "iCNN-Transformer: An improved CNN-Transformer with Channel-spatial Attention and Keyword Prediction for Automated Audio Captioning",
        "authors": "Kun Chen, Jun Wang, Feng Deng, Xiaorui Wang",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10073"
    },
    {
        "id": 20889,
        "title": "Transformer (illustration)",
        "authors": "Arlene Campos, Lachlan McKay",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-184605"
    },
    {
        "id": 20890,
        "title": "Traffic State Estimation with A multi-head Attention-based Transformer by Spatio-Temporal Autoencoding Transformer (STAT Model)",
        "authors": "Ali Reza Sattarzadeh, Pubudu N. Pathiran, Van. Thanh Huynh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTraffic state estimation is an essential component of Intelligent Transportation System (ITS) designed for alleviating traffic congestions. As traffic data is composed of intricate information which can also be impacted by various factors, scholars have been attempting to utilize state-of-the-art deep learning forecasting models in recent years. However, a more complex and robust model is required to extract long-range correlations with large-scale traffic data sequences. In order to overcome the weaknesses of deep learning models, the superior performance of transformers is expected to address this effectively in time-series forecasting with transport data. Employing the capabilities of transformers in extracting long-term trends and dynamic dependencies, proposed model improves the deep learning prediction performance for real datasets. The findings indicate that the transformer-based model exhibited promising performance in forecasting long-term traffic patterns and characteristics with a large quantity of data. In this paper, a comparison across conventional hybrid deep learning models with the Spatio-Temporal Autoencoder Transformer (STAT) model was conducted using real-world datasets. The multi-head attention-based transformer model outperformed all other comparative approaches for large-scale data demonstrating its importance in measuring the error criteria.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3989727/v1"
    },
    {
        "id": 20891,
        "title": "Efficiency Comparison of Solid-State Transformer and Low-Frequency Power Transformer",
        "authors": "Archit Joshi, Shabari Nath",
        "published": "2021-3-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icepe50861.2021.9404376"
    },
    {
        "id": 20892,
        "title": "Movie Caption Generation with Vision Transformer and Transformer-based Language Model",
        "authors": "Sorato Nakamura, Hidekazu Yanagimoto, Kiyota Hashimoto",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiai-aai59060.2023.00027"
    },
    {
        "id": 20893,
        "title": "Adaptive Digital Differential Protection of Power Transformer",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_4"
    },
    {
        "id": 20894,
        "title": "Decision letter for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v3/decision1"
    },
    {
        "id": 20895,
        "title": "Development of models for calculating parameters of converter transformers with foil windings taking into account current displacement",
        "authors": "A.V. Stulov,  , A.I. Tikhonov, I.A. Trofimovich, K.V. Semenova,  ,  ,  ",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17588/2072-2672.2017.6.025-032"
    },
    {
        "id": 20896,
        "title": "Decision letter for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v2/decision1"
    },
    {
        "id": 20897,
        "title": "Social Transformer: A Pedestrian Trajectory Prediction Method based on Social Feature Processing Using Transformer",
        "authors": "Fan Wen, Ming Li, Ruiyang Wang",
        "published": "2022-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9891949"
    },
    {
        "id": 20898,
        "title": "Split-Transformer Impute (STI): Genotype Imputation Using a Transformer-Based Model",
        "authors": "Mohammad Erfan Mowlaei, Chong Li, Junjie Chen, Benyamin Jamialahmadi, Sudhir Kumar, Timothy Richard Rebbeck, Xinghua Shi",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractWith recent advances in DNA sequencing technologies, researchers are able to acquire increasingly larger volumes of genomic datasets, enabling the training of powerful models for downstream genomic tasks. However, genome scale dataset often contain many missing values, decreasing the accuracy and power in drawing robust conclusions drawn in genomic analysis. Consequently, imputation of missing information by statistical and machine learning methods has become important. We show that the current state-of-the-art can be advanced significantly by applying a novel variation of the Transformer architecture, called Split-Transformer Impute (STI), coupled with improved preprocessing of data input into deep learning models. We performed extensive experiments to benchmark STI against existing methods using resequencing datasets from human 1000 Genomes Project and yeast genomes. Results establish superior performance of our new methods compared to competing genotype imputation methods in terms of accuracy and imputation quality score in the benchmark datasets.",
        "link": "http://dx.doi.org/10.1101/2023.03.05.531190"
    },
    {
        "id": 20899,
        "title": "BiG-Transformer: Integrating Hierarchical Features for Transformer via Bipartite Graph",
        "authors": "Xiaobo Shu, Mengge Xue, Yanzeng Li, Zhenyu Zhang, Tingwen Liu",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207132"
    },
    {
        "id": 20900,
        "title": "Smart Transformer and Low Frequency Transformer Comparison on Power Delivery Characteristics in the Power System",
        "authors": "Junru Chen, Rongwu Zhu, Terence OlDonnell, Marco Liserre",
        "published": "2018-10",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/aeit.2018.8577375"
    },
    {
        "id": 20901,
        "title": "Decision letter for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": "",
        "published": "2020-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v1/decision1"
    },
    {
        "id": 20902,
        "title": "BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks",
        "authors": "Jong-Hoon Oh, Ryu Iida, Julien Kloetzer, Kentaro Torisawa",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-long.164"
    },
    {
        "id": 20903,
        "title": "Answering Fill-in-the-Blank Questions in Portuguese with Transformer Language Models",
        "authors": "Hugo Gonçalo Oliveira",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-86230-5_58"
    },
    {
        "id": 20904,
        "title": "Assessment of Effect of Load Models on Loss-of-Life Calculation of a Transformer Using a Point Estimation Method",
        "authors": "Kanhaiya Kumar, Balu Ganesh Kumbhar, Saran Satsangi",
        "published": "2018-10-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/15325008.2018.1511874"
    },
    {
        "id": 20905,
        "title": "Task-Specific Transformer-Based Language Models in Medicine: A Survey (Preprint)",
        "authors": "Ha Na Cho, Tae Joon Jun, Young-Hak Kim, Hee Jun Kang, Imjin Ahn, Hansle Gwon, Yunha Kim, Hyeram Seo, Heejung Choi, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Soyoung Ko",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nBACKGROUND\nIn the field of artificial intelligence, language models, which are used to convey knowledge in the medical domain, have rapidly increased in number. However, no comprehensive review is available to guide researchers in constructing and applying language models for medical applications.\n\n\nOBJECTIVE\nWe aim to leverage the power of these language models to improve healthcare by addressing the challenges in the six tasks we reviewed.\n\n\nMETHODS\nWe present potential solutions to the identified limitations to provide useful insights for future research in natural language processing and the development of language models for medical applications.\n\n\nRESULTS\nWe surveyed studies on medical transformer-based language models, categorizing them into six tasks: dialogue generation, question-answering, summarization, text classification, sentiment analysis, and named entity recognition.\n\n\nCONCLUSIONS\nBy proposing potential solutions, we hope to facilitate the creation of more effective and accurate language models that can be utilized to enhance healthcare delivery and improve patient outcomes.\n",
        "link": "http://dx.doi.org/10.2196/preprints.49724"
    },
    {
        "id": 20906,
        "title": "Generating 3D CAD Models using a Transformer Autoencoders and a Diffusion Model",
        "authors": "Minseop Jung, Minseong Kim, Jibum Kim",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieie.2023.60.12.25"
    },
    {
        "id": 20907,
        "title": "Hey Article, What Are You About? Question Answering for Information Systems Articles  through Transformer Models for Long Sequences",
        "authors": "Louisa Ebert, Sebastian Huettemann, Roland M. Mueller",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24251/hicss.2023.076"
    },
    {
        "id": 20908,
        "title": "Deep Learning Transformer Models for Building a Comprehensive and Real-time Trauma Observatory: Development and Validation Study",
        "authors": "Gabrielle Chenais, Cédric Gil-Jardiné, Hélène Touchais, Marta Avalos Fernandez, Benjamin Contrand, Eric Tellier, Xavier Combes, Loick Bourdois, Philippe Revel, Emmanuel Lagarde",
        "published": "2023-1-12",
        "citations": 1,
        "abstract": "\nBackground\nPublic health surveillance relies on the collection of data, often in near-real time. Recent advances in natural language processing make it possible to envisage an automated system for extracting information from electronic health records.\n\n\nObjective\nTo study the feasibility of setting up a national trauma observatory in France, we compared the performance of several automatic language processing methods in a multiclass classification task of unstructured clinical notes.\n\n\nMethods\nA total of 69,110 free-text clinical notes related to visits to the emergency departments of the University Hospital of Bordeaux, France, between 2012 and 2019 were manually annotated. Among these clinical notes, 32.5% (22,481/69,110) were traumas. We trained 4 transformer models (deep learning models that encompass attention mechanism) and compared them with the term frequency–inverse document frequency associated with the support vector machine method.\n\n\nResults\nThe transformer models consistently performed better than the term frequency–inverse document frequency and a support vector machine. Among the transformers, the GPTanam model pretrained with a French corpus with an additional autosupervised learning step on 306,368 unlabeled clinical notes showed the best performance with a micro F1-score of 0.969.\n\n\nConclusions\nThe transformers proved efficient at the multiclass classification of narrative and medical data. Further steps for improvement should focus on the expansion of abbreviations and multioutput multiclass classification.\n",
        "link": "http://dx.doi.org/10.2196/40843"
    },
    {
        "id": 20909,
        "title": "Probabilistic generative transformer language models for generative design of molecules",
        "authors": "Lai Wei, Nihang Fu, Yuqi Song, Qian Wang, Jianjun Hu",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "AbstractSelf-supervised neural language models have recently found wide applications in the generative design of organic molecules and protein sequences as well as representation learning for downstream structure classification and functional prediction. However, most of the existing deep learning models for molecule design usually require a big dataset and have a black-box architecture, which makes it difficult to interpret their design logic. Here we propose the Generative Molecular Transformer (GMTransformer), a probabilistic neural network model for generative design of molecules. Our model is built on the blank filling language model originally developed for text processing, which has demonstrated unique advantages in learning the “molecules grammars” with high-quality generation, interpretability, and data efficiency. Benchmarked on the MOSES datasets, our models achieve high novelty and Scaf compared to other baselines. The probabilistic generation steps have the potential in tinkering with molecule design due to their capability of recommending how to modify existing molecules with explanation, guided by the learned implicit molecule chemistry. The source code and datasets can be accessed freely at https://github.com/usccolumbia/GMTransformer",
        "link": "http://dx.doi.org/10.1186/s13321-023-00759-z"
    },
    {
        "id": 20910,
        "title": "TRANSFORMER MODELS FOR MULTI-TEMPORAL LAND COVER CLASSIFICATION USING REMOTE SENSING IMAGES",
        "authors": "M. Voelsen, S. Lauble, F. Rottensteiner, C. Heipke",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "Abstract. The pixel-wise classification of land cover, i.e. the task of identifying the physical material of the Earth’s surface in an image, is one of the basic applications of satellite image time series (SITS) processing. With the availability of large amounts of SITS it is possible to use supervised deep learning techniques such as Transformer models to analyse the Earth’s surface at global scale and with high spatial and temporal resolution. While most approaches for land cover classification focus on the generation of a mono-temporal output map, we extend established deep learning models to multi-temporal input and output: using images acquired at different epochs we generate one output map for each input timestep. This has the advantage that the temporal change of land cover can be monitored. In addition, features conflicting over time are not averaged. We extend the Swin Transformer for SITS and introduce a new spatio-temporal transformer block (ST-TB) that extracts spatial and temporal features. We combine the ST-TB with the swin transformer block (STB) that is used in parallel for the individual input timesteps to extract spatial features. Furthermore, we investigate the usage of a temporal position encoding and different patch sizes. The latter is used to merge neighbouring pixels in the input embedding. Using SITS from Sentinel-2, the classification of land cover is improved by +1.8% in the mean F1-Score when using the ST-TB in the first stage of the Swin Transformer compared to a Swin Transformer without the ST-TB layer and by +1,6% compared to fully convolutional approaches. This demonstrates the advantage of the introduced ST-TB layer for the classification of SITS.\n                    ",
        "link": "http://dx.doi.org/10.5194/isprs-annals-x-1-w1-2023-981-2023"
    },
    {
        "id": 20911,
        "title": "Two-Capacitor Transformer Winding Capacitance Models for Common-Mode EMI Noise Analysis in Isolated DC–DC Converters",
        "authors": "Huan Zhang, Shuo Wang, Yiming Li, Qinghai Wang, Dianbo Fu",
        "published": "2017-11",
        "citations": 86,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2017.2650952"
    },
    {
        "id": 20912,
        "title": "Transformer-based models for ICD-10 coding of death certificates with Portuguese text",
        "authors": "Isabel Coutinho, Bruno Martins",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2022.104232"
    },
    {
        "id": 20913,
        "title": "Improving the Execution Speed of Transformer-based Object Tracking Models through Multi-head Attention Parallelization",
        "authors": "Inmo Kim, Myungsun Kim",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieie.2023.60.4.39"
    },
    {
        "id": 20914,
        "title": "Value-Wise ConvNet for Transformer Models: An Infinite Time-Aware Recommender System",
        "authors": "Mohsen Saaki, Saeid Hosseini, Sana Rahmani, Mohammad Reza Kangavari, Wen Hua, Xiaofang Zhou",
        "published": "2023-10-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tkde.2022.3219231"
    },
    {
        "id": 20915,
        "title": "Quantifying Cognitive Load from Voice using Transformer-Based Models and a Cross-Dataset Evaluation",
        "authors": "Pascal Hecker, Arpita M. Kappattanavar, Maximilian Schmitt, Sidratul Moontaha, Johannes Wagner, Florian Eyben, Björn W. Schuller, Bert Arnrich",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00055"
    },
    {
        "id": 20916,
        "title": "Classifying multi-level product categories using dynamic masking and transformer models",
        "authors": "Ozan Ozyegen, Hadi Jahanshahi, Mucahit Cevik, Beste Bulut, Deniz Yigit, Fahrettin F. Gonen, Ayşe Başar",
        "published": "2022-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42488-022-00066-6"
    },
    {
        "id": 20917,
        "title": "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test",
        "authors": "Philippe Laban, Luke Dai, Lucas Bandarkar, Marti A. Hearst",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-short.134"
    },
    {
        "id": 20918,
        "title": "Transformer with a Hybrid Magnetic Core for High-Efficiency Aircraft Transformer-Rectifier Unit",
        "authors": "Denis Gusakov, Denis Masalimov, Viktoriia Vavilova",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoecs46375.2019.8949989"
    },
    {
        "id": 20919,
        "title": "Intelligent Transformer Unit Topology Using Additional Small Power Converter Based on Conventional Distribution Transformer",
        "authors": "Hyun-Jun Lee, Young-Doo Yoon",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8912672"
    },
    {
        "id": 20920,
        "title": "Chapitre 14 - Le manager-designer ou comment le manager peut-il se transformer pour transformer son entreprise, grâce au design thinking ?",
        "authors": "Cécile Dejoux",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.frimo.2017.01.0179"
    },
    {
        "id": 20921,
        "title": "CartoonDiff: Training-free Cartoon Image Generation with Diffusion Transformer Models",
        "authors": "Feihong He, Gang Li, Lingyu Si, Leilei Yan, Shimeng Hou, Hongwei Dong, Fanzhang Li",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447821"
    },
    {
        "id": 20922,
        "title": "EmptyMind at BLP-2023 Task 2: Sentiment Analysis of Bangla Social Media Posts using Transformer-Based Models",
        "authors": "Karnis Fatema, Udoy Das, Md Ayon Mia, Md Sajidul Mowla, Mahshar Yahan, Md Fayez Ullah, Arpita Sarker, Hasan Murad",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.39"
    },
    {
        "id": 20923,
        "title": "Pre-training Polish Transformer-Based Language Models at Scale",
        "authors": "Sławomir Dadas, Michał Perełkiewicz, Rafał Poświata",
        "published": "2020",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-61534-5_27"
    },
    {
        "id": 20924,
        "title": "Transformer-based Language models for the Identification of Idiomatic Expressions",
        "authors": "Isuri Anuradha Nanomi Arachchige,  , Sachith Suraweera, Dulip Herath,  ,  ",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-080-9_015"
    },
    {
        "id": 20925,
        "title": "Automatic Fake News Detection with Pre-trained Transformer Models",
        "authors": "Mina Schütz, Alexander Schindler, Melanie Siegel, Kawa Nazemi",
        "published": "2021",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68787-8_45"
    },
    {
        "id": 20926,
        "title": "Development of models for calculating parameters of converter transformers with foil windings taking into account current displacement",
        "authors": "A.V. Stulov,  , A.I. Tikhonov, I.A. Trofimovich, K.V. Semenova,  ,  ,  ",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17588/2072-2672.2017.6.025-032"
    },
    {
        "id": 20927,
        "title": "TranSFormer: Slow-Fast Transformer for Machine Translation",
        "authors": "Bei Li, Yi Jing, Xu Tan, Zhen Xing, Tong Xiao, Jingbo Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.430"
    },
    {
        "id": 20928,
        "title": "Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective",
        "authors": "ChatGPT Generative Pre-trained Transformer, Alex Zhavoronkov",
        "published": "2022-12-21",
        "citations": 100,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18632/oncoscience.571"
    },
    {
        "id": 20929,
        "title": "Induced Local Attention for Transformer Models in Speech Recognition",
        "authors": "Tobias Watzel, Ludwig Kürzinger, Lujun Li, Gerhard Rigoll",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-87802-3_71"
    },
    {
        "id": 20930,
        "title": "Medical Text Prediction and Suggestion Using Generative Pretrained Transformer Models with Dental Medical Notes",
        "authors": "Joseph Sirrianni, Emre Sezgin, Daniel Claman, Simon L. Linwood",
        "published": "2022-12",
        "citations": 4,
        "abstract": "Abstract\n          Background Generative pretrained transformer (GPT) models are one of the latest large pretrained natural language processing models that enables model training with limited datasets and reduces dependency on large datasets, which are scarce and costly to establish and maintain. There is a rising interest to explore the use of GPT models in health care.\n          Objective We investigate the performance of GPT-2 and GPT-Neo models for medical text prediction using 374,787 free-text dental notes.\n          Methods We fine-tune pretrained GPT-2 and GPT-Neo models for next word prediction on a dataset of over 374,000 manually written sections of dental clinical notes. Each model was trained on 80% of the dataset, validated on 10%, and tested on the remaining 10%. We report model performance in terms of next word prediction accuracy and loss. Additionally, we analyze the performance of the models on different types of prediction tokens for categories. For comparison, we also fine-tuned a non-GPT pretrained neural network model, XLNet (large), for next word prediction. We annotate each token in 100 randomly sampled notes by category (e.g., names, abbreviations, clinical terms, punctuation, etc.) and compare the performance of each model by token category.\n          Results Models present acceptable accuracy scores (GPT-2: 76%; GPT-Neo: 53%), and the GPT-2 model also performs better in manual evaluations, especially for names, abbreviations, and punctuation. Both GPT models outperformed XLNet in terms of accuracy. The results suggest that pretrained models have the potential to assist medical charting in the future. We share the lessons learned, insights, and suggestions for future implementations.\n          Conclusion The results suggest that pretrained models have the potential to assist medical charting in the future. Our study presented one of the first implementations of the GPT model used with medical notes.",
        "link": "http://dx.doi.org/10.1055/a-1900-7351"
    },
    {
        "id": 20931,
        "title": "A Wavelet-Based Transformer Differential Protection With Differential Current Transformer Saturation and Cross-Country Fault Detection",
        "authors": "Flavio Costa, Rodrigo Prado",
        "published": "2018-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2018.8586618"
    },
    {
        "id": 20932,
        "title": "Transformer Protection",
        "authors": "",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781420014174-19"
    },
    {
        "id": 20933,
        "title": "Lou Reed’s Transformer",
        "authors": "Ezra Furman",
        "published": "2018",
        "citations": 1,
        "abstract": "Transformer, Lou Reed’s most enduringly popular album, is described with varying labels: it’s often called a glam rock album, a proto-punk album, a commercial breakthrough for Lou Reed, and an album about being gay. And yet, it doesn’t neatly fit into any of these descriptors. Buried underneath the radio-friendly exterior lie coded confessions of the subversive, wounded intelligence that gives this album its staying power as a work of art. Here Lou Reed managed to make a fun, accessible rock‘n’roll record that is also a troubled meditation on the ambiguities—sexual, musical and otherwise—that defined his public persona and helped make him one of the most fascinating and influential figures in rock history. Through close listening and personal reflections, songwriter Ezra Furman explores Reed’s and Transformer’s unstable identities, and the secrets the songs challenge us to uncover.",
        "link": "http://dx.doi.org/10.5040/9781501323089"
    },
    {
        "id": 20934,
        "title": "A Comparative Analysis of Transformer-Based Models for Document Visual Question Answering",
        "authors": "Vijay Kumari, Yashvardhan Sharma, Lavika Goel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-0609-3_16"
    },
    {
        "id": 20935,
        "title": "Transformer Models for Question Answering on Autism Spectrum Disorder QA Dataset",
        "authors": "Victoria Firsanova",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-93715-7_9"
    },
    {
        "id": 20936,
        "title": "Crosslingual Content Scoring in Five Languages Using Machine-Translation and Multilingual Transformer Models",
        "authors": "Andrea Horbach, Joey Pehlke, Ronja Laarmann-Quante, Yuning Ding",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "AbstractThis paper investigates crosslingual content scoring, a scenario where scoring models trained on learner data in one language are applied to data in a different language. We analyze data in five different languages (Chinese, English, French, German and Spanish) collected for three prompts of the established English ASAP content scoring dataset. We cross the language barrier by means of both shallow and deep learning crosslingual classification models using both machine translation and multilingual transformer models. We find that a combination of machine translation and multilingual models outperforms each method individually - our best results are reached when combining the available data in different languages, i.e. first training a model on the large English ASAP dataset before fine-tuning on smaller amounts of training data in the target language.",
        "link": "http://dx.doi.org/10.1007/s40593-023-00370-1"
    },
    {
        "id": 20937,
        "title": "Applications of transformer-based language models in bioinformatics: a survey",
        "authors": "Shuang Zhang, Rui Fan, Yuti Liu, Shuang Chen, Qiao Liu, Wanwen Zeng",
        "published": "2023-1-5",
        "citations": 24,
        "abstract": "AbstractSummaryThe transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods.Supplementary informationSupplementary data are available at Bioinformatics Advances online.",
        "link": "http://dx.doi.org/10.1093/bioadv/vbad001"
    },
    {
        "id": 20938,
        "title": "Estimation of no-Load Losses in Distribution Transformer Design Finite Element Analysis Techniques in Transformer Design",
        "authors": "G.U. Nnachi, A.O. Akumu, C.G. Richards, D.V. Nicolae",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica.2018.8521142"
    },
    {
        "id": 20939,
        "title": "Novel Convolutional Transformer for Autonomous Driving",
        "authors": "John Feng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/3329"
    },
    {
        "id": 20940,
        "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
        "authors": "Wen-Chin Huang, Tomoki Hayashi, Yi-Chiao Wu, Hirokazu Kameoka, Tomoki Toda",
        "published": "2020-10-25",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1066"
    },
    {
        "id": 20941,
        "title": "Eight Twilight Years of an Old World Transformer, 1930–1958",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-011"
    },
    {
        "id": 20942,
        "title": "Phasor Angle Based Differential Protection of Power Transformer",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_3"
    },
    {
        "id": 20943,
        "title": "Informer, an Information Organization Transformer Architecture",
        "authors": "Cristian Ojeda, Cayetano Artal, Francisco Tejera",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010372703810389"
    },
    {
        "id": 20944,
        "title": "Defect transformer: An efficient hybrid transformer architecture for surface defect detection",
        "authors": "Junpu Wang, Guili Xu, Fuju Yan, Jinjin Wang, Zhengsheng Wang",
        "published": "2023-4",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.measurement.2023.112614"
    },
    {
        "id": 20945,
        "title": "Transformer le handicap",
        "authors": "Anne-Lyse Chabert",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/eres.chabe.2017.01"
    },
    {
        "id": 20946,
        "title": "Impact of Transformer Leakage Inductance on the Soft-Switching Solid-State Transformer",
        "authors": "Liran Zheng, Karthik Kandasamy, Rajendra Prasad Kandula, Deepak Divan",
        "published": "2018-9",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2018.8557980"
    },
    {
        "id": 20947,
        "title": "Transformer in galaxy: a study of galaxy classification based on the Swin transformer",
        "authors": "Zhenyu Huang",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2667869"
    },
    {
        "id": 20948,
        "title": "Investigation of model transformer insulation behavior during PD activity in Di-Benzo-Di-Sulfide Sulphur contaminated transformer oil using online tan delta measurement",
        "authors": "L. Sanjeev Kumar,  ",
        "published": "2022-9-6",
        "citations": 0,
        "abstract": "Sulphur can be present in mineral insulating oil and can manifest in stable, highly reactive and corrosive form. The corrosive sulphur reacts with the copper conductor in transformer and forms semi conductive copper sulphide at the surface of the copper conductor. The paper insulation used on the copper conductor may get damaged due to copper sulfide deposits which also affect the partial discharge activities prevalent in the transformer paper-oil insulation. The affect of Sulphur on model transformer paper-oil insulation is studied with online measurement of Dissipation factor (tanδ) and partial discharges (PD). The paper presents results of the investigatory work carried out with on-line tanδ and PD measurements made during experiments with Paper covered copper conductor (PCCC) in presence of Di-Benzo-Di-Sulfide contamination in transformer oil.",
        "link": "http://dx.doi.org/10.47191/etj/v7i9.02"
    },
    {
        "id": 20949,
        "title": "Visual Surveillance Transformer",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2021-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5302/j.icros.2021.21.0143"
    },
    {
        "id": 20950,
        "title": "SparseSwin: Swin transformer with sparse transformer block",
        "authors": "Krisna Pinasthika, Blessius Sheldo Putra Laksono, Riyandi Banovbi Putera Irsal, Syifa’ Hukma Shabiyya, Novanto Yudistira",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127433"
    },
    {
        "id": 20951,
        "title": "Window Token Transformer: Can learnable window token help window-based transformer build better long-range interactions?",
        "authors": "Jiawei Mao, Yuanqi Chang, Xuesong Yin",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126793"
    },
    {
        "id": 20952,
        "title": "Monitoring of Transformer and Detection of Faults in Transformer Using IOT and Cloud Computing",
        "authors": "Minal Rajput, Prof. Pratibha Shingare",
        "published": "2022-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46254/in02.20220349"
    },
    {
        "id": 20953,
        "title": "DA-Transformer: Distance-aware Transformer",
        "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.naacl-main.166"
    },
    {
        "id": 20954,
        "title": "HRV-Transformer: Transformer-based non-contact heart rate detection",
        "authors": "Chengliang Geng, Chuanjiang Wang, Xiujuan Sun, Meng Liu, Hao Song",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327461"
    },
    {
        "id": 20955,
        "title": "Rotational Control Method with Transformer Loss Savings in Solid-State Transformer based Converter System",
        "authors": "Dong Dong, Ravisekhar Raju, Govardhan Ganireddy, Mohammed Agamy",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2018.8557959"
    },
    {
        "id": 20956,
        "title": "Voltage control of solid-state transformer to guarantee smart transformer functionalities",
        "authors": "Marco Liserre, Zhixiang Zou, Rongwu Zhu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-323-85622-5.00017-1"
    },
    {
        "id": 20957,
        "title": "CoBertTC: Covid-19 Text Classification Using Transformer-Based Language Models",
        "authors": "Md. Rajib Hossain, Mohammed Moshiul Hoque",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-50327-6_19"
    },
    {
        "id": 20958,
        "title": "ATICVis: A Visual Analytics System for Asymmetric Transformer Models Interpretation and Comparison",
        "authors": "Jian-Lin Wu, Pei-Chen Chang, Chao Wang, Ko-Chih Wang",
        "published": "2023-1-26",
        "citations": 1,
        "abstract": "In recent years, natural language processing (NLP) technology has made great progress. Models based on transformers have performed well in various natural language processing problems. However, a natural language task can be carried out by multiple different models with slightly different architectures, such as different numbers of layers and attention heads. In addition to quantitative indicators such as the basis for selecting models, many users also consider the language understanding ability of the model and the computing resources it requires. However, comparing and deeply analyzing two transformer-based models with different numbers of layers and attention heads are not easy because it lacks the inherent one-to-one match between models, so comparing models with different architectures is a crucial and challenging task when users train, select, or improve models for their NLP tasks. In this paper, we develop a visual analysis system to help machine learning experts deeply interpret and compare the pros and cons of asymmetric transformer-based models when the models are applied to a user’s target NLP task. We propose metrics to evaluate the similarity between layers or attention heads to help users to identify valuable layers and attention head combinations to compare. Our visual tool provides an interactive overview-to-detail framework for users to explore when and why models behave differently. In the use cases, users use our visual tool to find out and explain why a large model does not significantly outperform a small model and understand the linguistic features captured by layers and attention heads. The use cases and user feedback show that our tool can help people gain insight and facilitate model comparison tasks.",
        "link": "http://dx.doi.org/10.3390/app13031595"
    },
    {
        "id": 20959,
        "title": "Health-Related Content in Transformer-Based Language Models: Exploring Bias in Domain General vs. Domain Specific Training Sets",
        "authors": "Giuseppe Samo, Caterina Bonan",
        "published": "2023-5-18",
        "citations": 1,
        "abstract": "In this communication, we demonstrate that the bias observed in domain general training sets with health-related content is not improved in domain specific health-communication corpora, contra.",
        "link": "http://dx.doi.org/10.3233/shti230252"
    },
    {
        "id": 20960,
        "title": "BubbleFormer: Bubble Diagram Generation via Dual Transformer Models",
        "authors": "Jiahui Sun, Liping Zheng, Gaofeng Zhang, Wenming Wu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "AbstractBubble diagrams serve as a crucial tool in the field of architectural planning and graphic design. With the surge of Artificial Intelligence Generated Content (AIGC), there has been a continuous emergence of research and development efforts focused on utilizing bubble diagrams for layout design and generation. However, there is a lack of research efforts focused on bubble diagram generation. In this paper, we propose a novel generative model, BubbleFormer, for generating diverse and plausible bubble diagrams. BubbleFormer consists of two improved Transformer networks: NodeFormer and EdgeFormer. These networks generate nodes and edges of the bubble diagram, respectively. To enhance the generation diversity, a VAE module is incorporated into BubbleFormer, allowing for the sampling and generation of numerous high‐quality bubble diagrams. BubbleFormer is trained end‐to‐end and evaluated through qualitative and quantitative experiments. The results demonstrate that BubbleFormer can generate convincing and diverse bubble diagrams, which in turn drive downstream tasks to produce high‐quality layout plans. The model also shows generalization capabilities in other layout generation tasks and outperforms state‐of‐the‐art techniques in terms of quality and diversity. In previous work, bubble diagrams as input are provided by users, and as a result, our bubble diagram generative model fills a significant gap in automated layout generation driven by bubble diagrams, thereby enabling an end‐to‐end layout design and generation. Code for this paper is at https://github.com/cgjiahui/BubbleFormer.",
        "link": "http://dx.doi.org/10.1111/cgf.14984"
    },
    {
        "id": 20961,
        "title": "Empirical Study of Tweets Topic Classification Using Transformer-Based Language Models",
        "authors": "Ranju Mandal, Jinyan Chen, Susanne Becken, Bela Stantic",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-73280-6_27"
    },
    {
        "id": 20962,
        "title": "Comparative Analysis of CNN Models with Vision Transformer on Lung Infection Classification",
        "authors": "G. S. S. V. Badrish, K. G. N. Prabhanjali, A. Raghuvira Pratap",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-6690-5_12"
    },
    {
        "id": 20963,
        "title": "Detection of Medication Mentions and Medication Change Events in Clinical Notes Using Transformer-Based Models",
        "authors": "Yuting Guo, Yao Ge, Abeed Sarker",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "In this paper, we address the related tasks of medication extraction, event classification, and context classification from clinical text. The data for the tasks were obtained from the National Natural Language Processing (NLP) Clinical Challenges (n2c2) Track 1. We developed a named entity recognition (NER) model based on BioClinicalBERT and applied a dictionary-based fuzzy matching mechanism to identify the medication mentions in clinical notes. We developed a unified model architecture for event classification and context classification. The model used two pre-trained models—BioClinicalBERT and RoBERTa to predict the class, separately. Additionally, we applied an ensemble mechanism to combine the predictions of BioClinicalBERT and RoBERTa. For event classification, our best model achieved 0.926 micro-averaged F1-score, 5% higher than the baseline model. The shared task released the data in different stages during the evaluation phase. Our system consistently ranked among the top 10 for Releases 1 and 2.",
        "link": "http://dx.doi.org/10.3233/shti231052"
    },
    {
        "id": 20964,
        "title": "Novel Transformer-based Fusion Models for Aero-engine Remaining Useful Life Estimation",
        "authors": "Qiankun Hu, Yongping Zhao, Lihua Ren",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3277730"
    },
    {
        "id": 20965,
        "title": "Application of Transformer-Based Language Models to Detect Hate Speech in Social Media",
        "authors": "Swapnanil Mukherjee,  , Sujit Das,  ",
        "published": "2021-12-17",
        "citations": 3,
        "abstract": "Detecting and removing hateful speech in various online social media is a challenging task. Researchers tried to solve this problem by using both classical and deep learning methods, which are found to have limitations in terms of the requirement of extensive hand-crafted features, model architecture design, and pretrained embeddings, that are not very proficient in capturing semantic relations between words. Therefore, in this paper, we tackle the problem using Transformer-based pretrained language models which are specially designed to produce contextual embeddings of text sequences. We have evaluated two such models—RoBERTa and XLNet—using four publicly available datasets from different social media platforms and compared them to the existing baselines. Our investigation shows that the Transformer-based models either surpass or match all of the existing baseline scores by significant margins obtained by previously used models such as 1-dimensional convolutional neural network (1D-CNN) and long short-term memory (LSTM). The Transformer-based models proved to be more robust by achieving native performance when trained and tested on two different datasets. Our investigation also revealed that variations in the characteristics of the data produce significantly different results with the same model. From the experimental observations, we are able to establish that Transformer-based language models exhibit superior performance than their conventional counterparts at a fraction of the computation cost and minimal need for complex model engineering.",
        "link": "http://dx.doi.org/10.47852/bonviewjcce2022010102"
    },
    {
        "id": 20966,
        "title": "Advancement in Bangla Sentiment Analysis: A Comparative Study of Transformer-Based and Transfer Learning Models for E-commerce Sentiment Classification",
        "authors": "Zishan Ahmed, Shakib Sadat Shanto, Akinul Islam Jony",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Background: As a direct result of the Internet's expansion, the quantity of information shared by Internet users across its numerous platforms has increased. Sentiment analysis functions at a higher level when there are more available perspectives and opinions. However, the lack of labeled data significantly complicates sentiment analysis utilizing Bangla natural language processing (NLP). In recent years, nevertheless, due to the development of more effective deep learning models, Bangla sentiment analysis has improved significantly.\nObjective: This article presents a curated dataset for Bangla e-commerce sentiment analysis obtained solely from the \"Daraz\" platform. We aim to conduct sentiment analysis in Bangla for binary and understudied multiclass classification tasks.\nMethods: Transfer learning (LSTM, GRU) and Transformers (Bangla-BERT) approaches are compared for their effectiveness on our dataset. To enhance the overall performance of the models, we fine-tuned them.\nResults: The accuracy of Bangla-BERT was highest for both binary and multiclass sentiment classification tasks, with 94.5% accuracy for binary classification and 88.78% accuracy for multiclass sentiment classification.\nConclusion: Our proposed method performs noticeably better classifying multiclass sentiments in Bangla than previous deep learning techniques.\nKeywords: Bangla-BERT, Deep Learning, E-commerce, NLP, Sentiment Analysis",
        "link": "http://dx.doi.org/10.20473/jisebi.9.2.181-194"
    },
    {
        "id": 20967,
        "title": "Domain Adaptation in Transformer Models: Question Answering of Dutch Government Policies",
        "authors": "Berry Blom, João L. M. Pereira",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-48232-8_19"
    },
    {
        "id": 20968,
        "title": "Real-Time Monitoring and Adaptive Protection of Power Transformer",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6_7"
    },
    {
        "id": 20969,
        "title": "DeepGene Transformer: Transformer for the gene expression-based classification of cancer subtypes",
        "authors": "Anwar Khan, Boreom Lee",
        "published": "2023-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120047"
    },
    {
        "id": 20970,
        "title": "EEG-Transformer: Self-attention from Transformer Architecture for Decoding EEG of Imagined Speech",
        "authors": "Young-Eun Lee, Seo-Hyun Lee",
        "published": "2022-2-21",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bci53720.2022.9735124"
    },
    {
        "id": 20971,
        "title": "CASwin Transformer: A Hierarchical Cross Attention Transformer for Depth Completion",
        "authors": "Chunyu Feng, Xiaonian Wang, Yangyang Zhang, Chengfeng Zhao, Mengxuan Song",
        "published": "2022-10-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc55140.2022.9922273"
    },
    {
        "id": 20972,
        "title": "Ertim at SemEval-2023 Task 2: Fine-tuning of Transformer Language Models and External Knowledge Leveraging for NER in Farsi, English, French and Chinese",
        "authors": "Kevin Deturck, Pierre Magistry, Bénédicte Diot-Parvaz Ahmad, Ilaine Wang, Damien Nouvel, Hugo Lafayette",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.306"
    },
    {
        "id": 20973,
        "title": "Structure and properties of the hybrid and topological transformer models",
        "authors": "Jianhui Zhao, Sergey E. Zirka, Yuriy I. Moroz, Cesare M. Arturi",
        "published": "2020-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2019.105785"
    },
    {
        "id": 20974,
        "title": "A Targeted Assessment of the Syntactic Abilities of Transformer Models for Galician-Portuguese",
        "authors": "Marcos Garcia, Alfredo Crespo-Otero",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-98305-5_5"
    },
    {
        "id": 20975,
        "title": "The natural language processing of radiology requests and reports of chest imaging: Comparing five transformer models’ multilabel classification and a proof-of-concept study",
        "authors": "Allard W Olthof, Peter MA van Ooijen, Ludo J Cornelissen",
        "published": "2022-10",
        "citations": 0,
        "abstract": "Background Radiology requests and reports contain valuable information about diagnostic findings and indications, and transformer-based language models are promising for more accurate text classification. Methods In a retrospective study, 2256 radiologist-annotated radiology requests (8 classes) and reports (10 classes) were divided into training and testing datasets (90% and 10%, respectively) and used to train 32 models. Performance metrics were compared by model type (LSTM, Bertje, RobBERT, BERT-clinical, BERT-multilingual, BERT-base), text length, data prevalence, and training strategy. The best models were used to predict the remaining 40,873 cases’ categories of the datasets of requests and reports. Results The RobBERT model performed the best after 4000 training iterations, resulting in AUC values ranging from 0.808 [95% CI (0.757–0.859)] to 0.976 [95% CI (0.956–0.996)] for the requests and 0.746 [95% CI (0.689–0.802)] to 1.0 [95% CI (1.0–1.0)] for the reports. The AUC for the classification of normal reports was 0.95 [95% CI (0.922–0.979)]. The predicted data demonstrated variability of both diagnostic yield for various request classes and request patterns related to COVID-19 hospital admission data. Conclusion Transformer-based natural language processing is feasible for the multilabel classification of chest imaging request and report items. Diagnostic yield varies with the information in the requests. ",
        "link": "http://dx.doi.org/10.1177/14604582221131198"
    },
    {
        "id": 20976,
        "title": "Transfer Learning of Transformer-Based Speech Recognition Models from Czech to Slovak",
        "authors": "Jan Lehečka, Josef V. Psutka, Josef Psutka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40498-6_29"
    },
    {
        "id": 20977,
        "title": "Incorporating Transformer Models for Sentiment Analysis and News Classification in Khmer",
        "authors": "Md Rifatul Islam Rifat, Abdullah Al Imran",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-91434-9_10"
    },
    {
        "id": 20978,
        "title": "Automated ICD coding using extreme multi-label long text transformer-based models",
        "authors": "Leibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, Louisa Jorm",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.artmed.2023.102662"
    },
    {
        "id": 20979,
        "title": "Optimizing Geophysical Workloads in High-Performance Computing: Leveraging Machine Learning and Transformer Models for Enhanced Parallelism and Processor Allocation",
        "authors": "André Brasil Vieira Wyzykowski, Gabriel Mascarenhas Costa De Sousa, Breno Spinelli Coelho, Lucas Batista Santos, Darlan Anschau",
        "published": "2024-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dchpc60845.2024.10454084"
    },
    {
        "id": 20980,
        "title": "Novel Approach to the Extraction of Delphi-like Boundary-Condition-Independent Compact Thermal Models of Planar Transformer Devices",
        "authors": "Valentin Bissuel, Lorenzo Codecasa, Eric Monier-Vinard, Brice Rogie, Abel Olivier, Arnaud Mahe, Najib Laraqi, Vincenzo d'Alessandro, Christophe Gougis",
        "published": "2018-9",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/therminic.2018.8593283"
    },
    {
        "id": 20981,
        "title": "Design and Implementation of a 12 kW Scalable Electronic-Embedded Transformer (EET)-based DC Transformer (DCX) with Trapezoidal Current",
        "authors": "Yuliang Cao, Dong Dong",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce53617.2023.10362177"
    },
    {
        "id": 20982,
        "title": "Un système alimentaire à transformer",
        "authors": "Laurent Delcourt",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04"
    },
    {
        "id": 20983,
        "title": "A Progression of Transformer Architectures towards Cognition",
        "authors": "Mohit Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We review the transformer architectures that have potentially paved the way toward providing it with a better cognitive ability. This has led these architectures to think and query themselves with higher levels of intelligent questions and provide relevant answers by evolving subject-context relationships in the transformer layers. This is very similar to the cognitive ability of the human mind which can potentially think creatively by asking more relevant questions to the problem at hand which we frequently call \"thinking out loud\". These transformer architectures are potentially thought for a more creative and deep thinking approach by doing repeated queries and monitoring generated responses for cognitive behavior. Cognition and thinking for creating better answers in a generative AI system are evolving rapidly. In this article, we are trying to summarize the research conducted and the systems that evolved over time for cognitive thinking (after the basic architecture was proposed).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24548053"
    },
    {
        "id": 20984,
        "title": "Thermal degradation of transformer pressboard impregnated with magnetic nanofluid based on transformer oil",
        "authors": "Roman Cimbala, Samuel Bucko, Lukas Kruzelak, Michal Kosterec",
        "published": "2017-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2017.7967335"
    },
    {
        "id": 20985,
        "title": "Potentials and Challenges of Multiwinding Transformer-Based DC-DC Converters for Solid-State Transformer",
        "authors": "Thiago Pereira, Felix Hoffmann, Marco Liserre",
        "published": "2021-10-13",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon48115.2021.9589324"
    },
    {
        "id": 20986,
        "title": "Transformer Design for Solid-State Transformer (SST)-Based EV Charging Station Applications",
        "authors": "Eslam Abdelhamid Younis, Omar Zayed, Ahmed Elezab, Mohamed Ibrahim, Mehdi Narimani",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itec55900.2023.10187097"
    },
    {
        "id": 20987,
        "title": "MEEG-Transformer: Transformer Network based on Multi-domain EEG for Emotion Recognition",
        "authors": "Haoxuan Sun, Liying Yang, Qiang Wang, Dunhui Liu, Pei Ni",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385996"
    },
    {
        "id": 20988,
        "title": "Conmw Transformer: A General Vision Transformer Backbone With Merged-Window Attention",
        "authors": "Ang Li, Jichao Jiao, Ning Li, Wangjing Qi, Wei Xu, Min Pang",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897179"
    },
    {
        "id": 20989,
        "title": "Power Transformer Design Practices",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865"
    },
    {
        "id": 20990,
        "title": "Power Transformer Diagnostics, Monitoring and Design Features",
        "authors": "",
        "published": "2018-12-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/books978-3-03897-442-0"
    },
    {
        "id": 20991,
        "title": "Power Transformer Online Monitoring Using Electromagnetic Waves",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/c2019-0-04255-0"
    },
    {
        "id": 20992,
        "title": "Analysis of Partial Discharge Test of Transformer Caused by Excitation Transformer Fault",
        "authors": "Yu Bing, Zhao Lin, Yang Zhi, Yang Yong",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicm48536.2019.8977148"
    },
    {
        "id": 20993,
        "title": "Attenuation of Transformer Inrush Current Using Controlled Switching System on Delta-Star Transformer",
        "authors": "Mohamed Hassan Hashem, Ahdab Mohamed Elmorshedy, Ahmed Mohamed Emam",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mepcon.2018.8635146"
    },
    {
        "id": 20994,
        "title": "Loss Analysis and Temperature Measurement of Middle Frequency Transformer Applied for Solid State Transformer",
        "authors": "Noriyuki Kimura, Kazushige Nakao, Toshimitsu Morizane",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrera47325.2019.8997091"
    },
    {
        "id": 20995,
        "title": "Non-invasive anaemia detection based on palm pallor video using tree-structured 3D CNN and vision transformer models",
        "authors": "Abhishek Kesarwani, Sunanda Das, Dakshina Ranjan Kisku, Mamata Dalui",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/0952813x.2023.2301401"
    },
    {
        "id": 20996,
        "title": "Parameter-Efficient Transfer Learning of Pre-Trained Transformer Models for Speaker Verification Using Adapters",
        "authors": "Junyi Peng, Themos Stafylakis, Rongzhi Gu, Oldřich Plchot, Ladislav Mošner, Lukáš Burget, Jan Černocký",
        "published": "2023-6-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10094795"
    },
    {
        "id": 20997,
        "title": "E.T.",
        "authors": "Shiyang Chen, Shaoyi Huang, Santosh Pandey, Bingbing Li, Guang R. Gao, Long Zheng, Caiwen Ding, Hang Liu",
        "published": "2021-11-14",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3458817.3476138"
    },
    {
        "id": 20998,
        "title": "Olive Disease Classification Based on Vision Transformer and CNN Models",
        "authors": "Hamoud Alshammari, Karim Gasmi, Ibtihel Ben Ltaifa, Moez Krichen, Lassaad Ben Ammar, Mahmood A. Mahmood",
        "published": "2022-7-31",
        "citations": 21,
        "abstract": "It has been noted that disease detection approaches based on deep learning are becoming increasingly important in artificial intelligence-based research in the field of agriculture. Studies conducted in this area are not at the level that is desirable due to the diversity of plant species and the regional characteristics of many of these species. Although numerous researchers have studied diseases on plant leaves, it is undeniable that timely diagnosis of diseases on olive leaves remains a difficult task. It is estimated that people have been cultivating olive trees for 6000 years, making it one of the most useful and profitable fruit trees in history. Symptoms that appear on infected leaves can vary from one plant to another or even between individual leaves on the same plant. Because olive groves are susceptible to a variety of pathogens, including bacterial blight, olive knot, Aculus olearius, and olive peacock spot, it has been difficult to develop an effective olive disease detection algorithm. For this reason, we developed a unique deep ensemble learning strategy that combines the convolutional neural network model with vision transformer model. The goal of this method is to detect and classify diseases that can affect olive leaves. In addition, binary and multiclassification systems based on deep convolutional models were used to categorize olive leaf disease. The results are encouraging and show how effectively CNN and vision transformer models can be used together. Our model outperformed the other models with an accuracy of about 96% for multiclass classification and 97% for binary classification, as shown by the experimental results reported in this study.",
        "link": "http://dx.doi.org/10.1155/2022/3998193"
    },
    {
        "id": 20999,
        "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
        "authors": "Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P. de Almeida, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, Thomas Pierrot",
        "published": "No Date",
        "citations": 25,
        "abstract": "AbstractClosing the gap between measurable genetic information and observable traits is a longstand-ing challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, rang-ing from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model rep-resentations can improve the prioritization of functional genetic variants. The training and ap-plication of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available at:https://github.com/instadeepai/nucleotide-transformerin Jax andhttps://huggingface.co/InstaDeepAIin Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace.",
        "link": "http://dx.doi.org/10.1101/2023.01.11.523679"
    },
    {
        "id": 21000,
        "title": "The Impedance of a Transmission Line Transformer",
        "authors": "Enzo Carpentieri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> Transmission-line transformers are useful circuits for impedance-matching applications due to their broad operating bandwidth. An equivalent model for transmission-line transformers supports accurate calculation of input impedance through a distributed-circuit approach.  </p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21647102"
    },
    {
        "id": 21001,
        "title": "Analysis of Partial Discharge Test of Transformer Caused by Excitation Transformer Fault",
        "authors": "Yu Bing, Zhao Lin, Yang Zhi, Yang Yong",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicm48536.2019.8977148"
    },
    {
        "id": 21002,
        "title": "Attenuation of Transformer Inrush Current Using Controlled Switching System on Delta-Star Transformer",
        "authors": "Mohamed Hassan Hashem, Ahdab Mohamed Elmorshedy, Ahmed Mohamed Emam",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mepcon.2018.8635146"
    },
    {
        "id": 21003,
        "title": "Loss Analysis and Temperature Measurement of Middle Frequency Transformer Applied for Solid State Transformer",
        "authors": "Noriyuki Kimura, Kazushige Nakao, Toshimitsu Morizane",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrera47325.2019.8997091"
    },
    {
        "id": 21004,
        "title": "Design and Implementation of a 12 kW Scalable Electronic-Embedded Transformer (EET)-based DC Transformer (DCX) with Trapezoidal Current",
        "authors": "Yuliang Cao, Dong Dong",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce53617.2023.10362177"
    },
    {
        "id": 21005,
        "title": "Context Transformer and Adaptive Method with Visual Transformer for Robust Facial Expression Recognition",
        "authors": "Lingxin Xiong, Jicun Zhang, Xiaojia Zheng, Yuxin Wang",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "In real-world scenarios, the facial expression recognition task faces several challenges, including lighting variations, image noise, face occlusion, and other factors, which limit the performance of existing models in dealing with complex situations. To cope with these problems, we introduce the CoT module between the CNN and ViT frameworks, which improves the ability to perceive subtle differences by learning the correlations between local area features at a fine-grained level, helping to maintain the consistency between the local area features and the global expression, and making the model more adaptable to complex lighting conditions. Meanwhile, we adopt an adaptive learning method to effectively eliminate the interference of noise and occlusion by dynamically adjusting the parameters of the Transformer Encoder’s self-attention weight matrix. Experiments demonstrate the accuracy of our CoT_AdaViT model in the Oulu-CASIA dataset as (NIR: 87.94%, VL: strong: 89.47%, weak: 84.76%, dark: 82.28%). As well as, CK+, RAF-DB, and FERPlus datasets achieved 99.20%, 91.07%, and 90.57% recognition results, which achieved excellent performance and verified that the model has strong recognition accuracy and robustness in complex scenes.",
        "link": "http://dx.doi.org/10.3390/app14041535"
    },
    {
        "id": 21006,
        "title": "Cas-VSwin transformer: A variant swin transformer for surface-defect detection",
        "authors": "Linfeng Gao, Jianxun Zhang, Changhui Yang, Yuechuan Zhou",
        "published": "2022-9",
        "citations": 40,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compind.2022.103689"
    },
    {
        "id": 21007,
        "title": "∞-former: Infinite Memory Transformer-former: Infinite Memory Transformer",
        "authors": "Pedro Henrique Martins, Zita Marinho, Andre Martins",
        "published": "2022",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.acl-long.375"
    },
    {
        "id": 21008,
        "title": "Sizing Transformer Considering Transformer Thermal Limits and Wind Farm Wake Effect",
        "authors": "Zhongtian Li, Kateryna Morozovska, Patrik Hilber, Tor Laneryd, Stefan Ivanell",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgtasia49270.2021.9715702"
    },
    {
        "id": 21009,
        "title": "Multiterminal Three-Phase Transformer Model",
        "authors": "Robert M. Del Vecchio, Bertrand Poulin, Pierre T. Feghali, Dilipkumar M. Shah, Rajendra Ahuja",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/ebk1439805824-8"
    },
    {
        "id": 21010,
        "title": "SD-Transformer: A System-Level Denoising Transformer for Encrypted Traffic Behavior Identification",
        "authors": "Yizhuo Zhao, Yukun Zhu, Xiong Li, Ruidong Chen, Mohammad S. Obaidat, Pandi Vijayakumar",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436868"
    },
    {
        "id": 21011,
        "title": "A Progression of Transformer Architectures towards Cognition",
        "authors": "Mohit Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We review the transformer architectures that have potentially paved the way toward providing it with a better cognitive ability. This has led these architectures to think and query themselves with higher levels of intelligent questions and provide relevant answers by evolving subject-context relationships in the transformer layers. This is very similar to the cognitive ability of the human mind which can potentially think creatively by asking more relevant questions to the problem at hand which we frequently call \"thinking out loud\". These transformer architectures are potentially thought for a more creative and deep thinking approach by doing repeated queries and monitoring generated responses for cognitive behavior. Cognition and thinking for creating better answers in a generative AI system are evolving rapidly. In this article, we are trying to summarize the research conducted and the systems that evolved over time for cognitive thinking (after the basic architecture was proposed).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24548053"
    },
    {
        "id": 21012,
        "title": "Un système alimentaire à transformer",
        "authors": "Laurent Delcourt",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04"
    },
    {
        "id": 21013,
        "title": "Transformer la violence des élèves",
        "authors": "Daniel Favre",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.favre.2019.01"
    },
    {
        "id": 21014,
        "title": "Enormous Thanks",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0022"
    },
    {
        "id": 21015,
        "title": "A Progression of Transformer Architectures towards Cognition",
        "authors": "Mohit Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We review the transformer architectures that have potentially paved the way toward providing it with a better cognitive ability. This has led these architectures to think and query themselves with higher levels of intelligent questions and provide relevant answers by evolving subject-context relationships in the transformer layers. This is very similar to the cognitive ability of the human mind which can potentially think creatively by asking more relevant questions to the problem at hand which we frequently call \"thinking out loud\". These transformer architectures are potentially thought for a more creative and deep thinking approach by doing repeated queries and monitoring generated responses for cognitive behavior. Cognition and thinking for creating better answers in a generative AI system are evolving rapidly. In this article, we are trying to summarize the research conducted and the systems that evolved over time for cognitive thinking (after the basic architecture was proposed).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24548053.v1"
    },
    {
        "id": 21016,
        "title": "The Impedance of a Transmission Line Transformer",
        "authors": "Enzo Carpentieri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> Transmission-line transformers are useful circuits for impedance-matching applications due to their broad operating bandwidth. An equivalent model for transmission-line transformers supports accurate calculation of input impedance through a distributed-circuit approach.  </p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21647102.v1"
    },
    {
        "id": 21017,
        "title": "Potentials and Challenges of Multiwinding Transformer-Based DC-DC Converters for Solid-State Transformer",
        "authors": "Thiago Pereira, Felix Hoffmann, Marco Liserre",
        "published": "2021-10-13",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon48115.2021.9589324"
    },
    {
        "id": 21018,
        "title": "Thermal degradation of transformer pressboard impregnated with magnetic nanofluid based on transformer oil",
        "authors": "Roman Cimbala, Samuel Bucko, Lukas Kruzelak, Michal Kosterec",
        "published": "2017-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2017.7967335"
    },
    {
        "id": 21019,
        "title": "Power Transformer Design Practices",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865"
    },
    {
        "id": 21020,
        "title": "IOT based real time monitoring of distribution transformer",
        "authors": "",
        "published": "2018-3-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23883/ijrter.2018.4120.wyr05"
    },
    {
        "id": 21021,
        "title": "Virtual Transformer Operation of Solid State Transformer (SST)",
        "authors": "Qian TAO, Jianjun MA, Miao ZHU, Shuli WEN, Qing DUAN, Guanglin SHA",
        "published": "2020-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon43393.2020.9254936"
    },
    {
        "id": 21022,
        "title": "Transformer Kodlayıcı ve Zaman-Frekans Görüntüleri Kullanarak Otomatik Uyku Evreleri Sınıflandırması",
        "authors": "Göksu Zekiye ÖZEN, Yunus ÖZEN",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "Bu çalışmada Polisomnografi (PSG) kayıtlarından alınan tek kanallı EEG verileri kullanarak otomatik uyku evreleri sınıflandırması yapan bir derin öğrenme modeli önerilmektedir. Önerilen model, EEG sinyallerinin kısa süreli Fourier dönüşümü (STFT) ile elde edilen zaman-frekans görüntülerinden öznitelik çıkarmak için Transformer kodlayıcı kullanmaktadır. Transformer kodlayıcının çok başlı dikkat mekanizması, zaman-frekans görüntülerindeki zaman bağımlılıklarını yakalayarak modelin uykunun sıralı doğasını anlama performansını artırmaktadır. Önerilen modelin performansı, SleepEDF Expanded adlı açık erişim veri seti üzerinde değerlendirilmiştir ve 0.84 F1 skoru ile yüksek doğruluk değerine sahip sonuç elde edilmiştir. Modelin zaman-frekans görüntüleri kullanması, EEG sinyallerinin temel zaman alanı ve frekans alanı özelliklerini yakalayarak doğru uyku evreleri sınıflandırmasına katkı sağlamaktadır. Gelecek çalışmalarda, diğer PSG kanalları da dâhil edilerek uygulamada kullanımı mümkün olabilecek bir model geliştirilebileceği değerlendirilmektedir.",
        "link": "http://dx.doi.org/10.53070/bbd.1370639"
    },
    {
        "id": 21023,
        "title": "Author response for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": " Maulik Raichura,  Nilesh Chothani,  Dharmesh Patel",
        "published": "2020-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v3/response1"
    },
    {
        "id": 21024,
        "title": "Front Matter",
        "authors": "",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_fm"
    },
    {
        "id": 21025,
        "title": "Lou the Failure",
        "authors": "",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0005"
    },
    {
        "id": 21026,
        "title": "Compressed-Transformer",
        "authors": "Yuan, Chen, Pan, Rong",
        "published": "2020-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3443279.3443302"
    },
    {
        "id": 21027,
        "title": "S-Swin Transformer: simplified Swin Transformer model for offline handwritten Chinese character recognition",
        "authors": "Yongping Dan, Zongnan Zhu, Weishou Jin, Zhuo Li",
        "published": "2022-9-20",
        "citations": 6,
        "abstract": "The Transformer shows good prospects in computer vision. However, the Swin Transformer model has the disadvantage of a large number of parameters and high computational effort. To effectively solve these problems of the model, a simplified Swin Transformer (S-Swin Transformer) model was proposed in this article for handwritten Chinese character recognition. The model simplifies the initial four hierarchical stages into three hierarchical stages. In addition, the new model increases the size of the window in the window attention; the number of patches in the window is larger; and the perceptual field of the window is increased. As the network model deepens, the size of patches becomes larger, and the perceived range of each patch increases. Meanwhile, the purpose of shifting the window’s attention is to enhance the information interaction between the window and the window. Experimental results show that the verification accuracy improves slightly as the window becomes larger. The best validation accuracy of the simplified Swin Transformer model on the dataset reached 95.70%. The number of parameters is only 8.69 million, and FLOPs are 2.90G, which greatly reduces the number of parameters and computation of the model and proves the correctness and validity of the proposed model.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1093"
    },
    {
        "id": 21028,
        "title": "Author response for \"Development of an adaptive differential protection scheme for transformer during &lt;scp&gt;current transformer&lt;/scp&gt; saturation and over‐fluxing condition\"",
        "authors": " Maulik Raichura,  Nilesh Chothani,  Dharmesh Patel",
        "published": "2020-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12751/v2/response1"
    },
    {
        "id": 21029,
        "title": "Critical Investigation of Heat Transfer in Oil Cooled Transformer",
        "authors": "",
        "published": "2017-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21275/art20164254"
    },
    {
        "id": 21030,
        "title": "Modular Hybrid Solid State Transformer (H-SST) for Next Generation Flexible and Adaptable Large Power Transformer (LPT) (Final Technical Report)",
        "authors": "Alex Huang, Sanjay Rajendran, Xiaonan Lu, Tianqi Hong, Zhicheng Guo, Jagadeesh Tangudu",
        "published": "2022-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1891437"
    },
    {
        "id": 21031,
        "title": "Hybrid Time Distributed CNN-transformer for Speech Emotion Recognition",
        "authors": "Anwer Slimi, Henri Nicolas, Mounir Zrigui",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011314900003266"
    },
    {
        "id": 21032,
        "title": "Çapraz Satışı Destekleyebilecek Transformer ile Geliştirilmiş Bir Öneri Sistemi",
        "authors": "İbrahim Erdem KALKAN, Cenk ŞAHİN",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "Öneri sistemlerinin, perakende sektöründe çapraz satış bağlamında bir ürün grubunu satış için hedeflemek veya hangi müşterilerin diğerlerine göre daha pazarlanabilir olduğunu tahmin edebilen yeteneklere sahip olduğu düşünülmektedir. Bu sayede mevcut müşterilerin bir sonraki seferde hangi ürün veya hizmeti satın alacaklarına ilişkin bir tahmin oluşturularak çapraz satış etkinliği arttırılabilecektir. Bu araştırmada temel amaç, çevrimiçi alışveriş endüstrisine, çapraz satış olanaklarını arttırabilmek bağlamında, belirli bir ürün ya da ürün grubu için, belli bir satın alma tarihçesi bulunan müşterilerinden hangilerinin diğerlerine göre daha uygun olduğunu tahmin etmek için bir öneri sistemi geliştirip sunmaktır. Bu kapsamda transformer kullanılarak probleme adapte edilmiş öneri sisteminin karşılaştırmalı bir çalışması yapılmış ve elde edilen sonuçlara göre önceki çalışmalarda sunulan modellere göre daha başarılı olduğu gözlenmiştir.",
        "link": "http://dx.doi.org/10.21605/cukurovaumfd.1334166"
    },
    {
        "id": 21033,
        "title": "TSF-transformer: a time series forecasting model for exhaust gas emission using transformer",
        "authors": "Zhenyu Li, Xikun Zhang, Zhenbiao Dong",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-04326-1"
    },
    {
        "id": 21034,
        "title": "Transformer Models for Activity Mining in Knowledge-Intensive Processes",
        "authors": "Faria Khandaker, Arik Senderovich, Eric Yu, Sebastian Carbajales, Allen Chan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-25383-6_2"
    },
    {
        "id": 21035,
        "title": "Automatic Short Answer Scoring on an Indian Dataset Using Transformer-Based Language Models",
        "authors": "Ganga Sanuvala, S. Sameen Fatima, Tanmayee Kambhampati, Rajeshwari Sanuvala",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-9707-7_27"
    },
    {
        "id": 21036,
        "title": "Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography",
        "authors": "Md Nazmul Islam, Mehedi Hasan, Md. Kabir Hossain, Md. Golam Rabiul Alam, Md Zia Uddin, Ahmet Soylu",
        "published": "2022-7-6",
        "citations": 55,
        "abstract": "AbstractRenal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI community’s research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
        "link": "http://dx.doi.org/10.1038/s41598-022-15634-4"
    },
    {
        "id": 21037,
        "title": "EG-TransUNet: a transformer-based U-Net with enhanced and guided models for biomedical image segmentation",
        "authors": "Shaoming Pan, Xin Liu, Ningdi Xie, Yanwen Chong",
        "published": "2023-3-7",
        "citations": 9,
        "abstract": "AbstractAlthough various methods based on convolutional neural networks have improved the performance of biomedical image segmentation to meet the precision requirements of medical imaging segmentation task, medical image segmentation methods based on deep learning still need to solve the following problems: (1) Difficulty in extracting the discriminative feature of the lesion region in medical images during the encoding process due to variable sizes and shapes; (2) difficulty in fusing spatial and semantic information of the lesion region effectively during the decoding process due to redundant information and the semantic gap. In this paper, we used the attention-based Transformer during the encoder and decoder stages to improve feature discrimination at the level of spatial detail and semantic location by its multihead-based self-attention. In conclusion, we propose an architecture called EG-TransUNet, including three modules improved by a transformer: progressive enhancement module, channel spatial attention, and semantic guidance attention. The proposed EG-TransUNet architecture allowed us to capture object variabilities with improved results on different biomedical datasets. EG-TransUNet outperformed other methods on two popular colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB) by achieving 93.44% and 95.26% on mDice. Extensive experiments and visualization results demonstrate that our method advances the performance on five medical segmentation datasets with better generalization ability.",
        "link": "http://dx.doi.org/10.1186/s12859-023-05196-1"
    },
    {
        "id": 21038,
        "title": "An Empirical Study on the Usage of Transformer Models for Code Completion",
        "authors": "Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, Gabriele Bavota",
        "published": "2021",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tse.2021.3128234"
    },
    {
        "id": 21039,
        "title": "Ctad: Contrastive Transformer Anomaly Detection",
        "authors": "Juan Li, Yiduo Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458020"
    },
    {
        "id": 21040,
        "title": "Transformer and Inductor Design Handbook",
        "authors": "Colonel Wm. T. McLyman",
        "published": "2017-12-19",
        "citations": 136,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865"
    },
    {
        "id": 21041,
        "title": "Effectiveness of Transformer Models on IoT Security Detection in StackOverflow Discussions",
        "authors": "Nibir Chandra Mandal, G. M. Shahariar, Md. Tanvir Rouf Shawon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7528-8_10"
    },
    {
        "id": 21042,
        "title": "Optimized Transformer Models for FAQ Answering",
        "authors": "Sonam Damani, Kedhar Nath Narahari, Ankush Chatterjee, Manish Gupta, Puneet Agrawal",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-47426-3_19"
    },
    {
        "id": 21043,
        "title": "Answer Sentence Selection Using Local and Global Context in Transformer Models",
        "authors": "Ivano Lauriola, Alessandro Moschitti",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-72113-8_20"
    },
    {
        "id": 21044,
        "title": "Investigating Bidimensional Downsampling in Vision Transformer Models",
        "authors": "Paolo Bruno, Roberto Amoroso, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-06430-2_24"
    },
    {
        "id": 21045,
        "title": "Transformer-based models for multimodal irony detection",
        "authors": "David Tomás, Reynier Ortega-Bueno, Guobiao Zhang, Paolo Rosso, Rossano Schifanella",
        "published": "2023-6",
        "citations": 5,
        "abstract": "AbstractIrony is nowadays a pervasive phenomenon in social networks. The multimodal functionalities of these platforms (i.e., the possibility to attach audio, video, and images to textual information) are increasingly leading their users to employ combinations of information in different formats to express their ironic thoughts. The present work focuses on the study of irony detection in social media posts involving image and text. To this end, a transformer architecture for the fusion of textual and image information is proposed. The model leverages disentangled text attention with visual transformers, improving F1-score up to 9% over previous existing works in the field and current state-of-the-art visio-linguistic transformers. The proposed architecture was evaluated in three different multimodal datasets gathered from Twitter and Tumblr. The results revealed that, in many situations, the text-only version of the architecture was able to capture the ironic nature of the message without using visual information. This phenomenon was further analysed, leading to the identification of linguistic patterns that could provide the context necessary for irony detection without the need for additional visual information.",
        "link": "http://dx.doi.org/10.1007/s12652-022-04447-y"
    },
    {
        "id": 21046,
        "title": "Trans-IDS: A Transformer-Based Intrusion Detection System",
        "authors": "El Mercha, El Mostapha Chakir, Mohammed Erradi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012085800003555"
    },
    {
        "id": 21047,
        "title": "Transformer Infrastructure for Power Grid",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_1"
    },
    {
        "id": 21048,
        "title": "Vision Transformer untuk Klasifikasi Kematangan Pisang",
        "authors": "Arya Pangestu, Bedy Purnama, Risnandar Risnandar",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Produksi pisang di Indonesia pada tahun 2022 mencapai 9,6 juta ton buah. Metode konvensional yang digunakan untuk menentukan tingkat kematangan pisang masih mengandalkan indera penglihatan manusia dengan memperhatikan perubahan warna kulit pisang. Namun, penentuan tingkat kematangan pisang dengan metode ini memiliki beberapa kekurangan, seperti waktu yang lama, penilaian yang bersifat subjektif dan dapat menghasilkan hasil yang berbeda-beda bagi setiap individu. Oleh karena itu, teknologi computer vision dapat menjadi solusi yang efektif dalam mengklasifikasikan kematangan buah pisang secara otomatis. Penelitian ini menggunakan metodologi Vision Transformer (ViT) untuk mengklasifikasikan tingkat kematangan pada buah pisang, dengan tingkatan yang dibagi menjadi empat kategori, yaitu mentah, setengah matang, matang, dan terlalu matang. Penelitian dilakukan dengan menggunakan lima model ViT yang sudah dilatih sebelumnya atau pre-trained, yaitu ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32, and ViT-H/14 pada ImageNet-21k dan ImageNet-1k. Kemudian, model ViT tersebut dievaluasi dan dibandingkan dengan model CNN. Evaluasi dilakukan menggunakan metode cross-dataset dengan 5.068 citra pisang yang berbeda dari dataset latih. Hasil evaluasi menunjukkan model ViTL/16-in21k memiliki akurasi tertinggi sebesar 91,61%. Model ViT menunjukkan kemampuan generalisasi yang lebih baik, sementara CNN memiliki ukuran model dan waktu pelatihan yang lebih efisien.",
        "link": "http://dx.doi.org/10.25126/jtiik.20241117389"
    },
    {
        "id": 21049,
        "title": "Latent Video Transformer",
        "authors": "Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, Evgeny Burnaev",
        "published": "2021",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010241801010112"
    },
    {
        "id": 21050,
        "title": "Empathy and Distress Prediction using Transformer Multi-output Regression and Emotion Analysis with an Ensemble of Supervised and Zero-Shot Learning Models",
        "authors": "Flor Miriam Del Arco, Jaime Collado-Montañez, L. Alfonso Ureña, María-Teresa Martín-Valdivia",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wassa-1.23"
    },
    {
        "id": 21051,
        "title": "Distractor Generation Through Text-to-Text Transformer Models",
        "authors": "David De-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jesus-Angel Del-Hoyo-Gabaldon, Antonio Moreno-Cediel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3361673"
    },
    {
        "id": 21052,
        "title": "FLatten Transformer: Vision Transformer using Focused Linear Attention",
        "authors": "Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, Gao Huang",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00548"
    },
    {
        "id": 21053,
        "title": "Transformer-based Automatic Post-Editing for Machine Translation",
        "authors": "Jaehun Shin, Youngkil Kim, Jong-hyeok Lee",
        "published": "2019-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2019.25.1.64"
    },
    {
        "id": 21054,
        "title": "A Review on Active Power Electronic Transformer",
        "authors": "",
        "published": "2017-3-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21884/ijmter.2017.4081.98hs3"
    },
    {
        "id": 21055,
        "title": "[Paper] PSp-Transformer: A Transformer with Data-level Probabilistic Sparsity for Action Representation Learning",
        "authors": "Jiaxin Zhou, Takashi Komuro",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3169/mta.12.123"
    },
    {
        "id": 21056,
        "title": "RM-Transformer: A Transformer-based Model for Mandarin Speech Recognition",
        "authors": "Xingyu Lu, Jianguo Hu, Shenhao Li, Yanyu Ding",
        "published": "2022-5-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccai55564.2022.9807706"
    },
    {
        "id": 21057,
        "title": "A High-Frequency PCB-Winding Transformer Design with Medium Voltage Insulation for Solid-State Transformer",
        "authors": "Zheqing Li, Feng Jin, Yi-Hsun Hsieh, Qiang Li",
        "published": "2023-3-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43580.2023.10131159"
    },
    {
        "id": 21058,
        "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
        "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo",
        "published": "2021-10",
        "citations": 8069,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00986"
    },
    {
        "id": 21059,
        "title": "Failure Analysis of Power Transformer Based on Transformer Turn Ratio Test and SFRA",
        "authors": "Agung Prasetyo, Eko Supriyanto",
        "published": "2021-3-1",
        "citations": 0,
        "abstract": "Abstract\nA power transformer is one of the most key components of energy transmission and distribution in an electric power system. Inter-turn disturbances or short circuit faults may occur because of insulation degradation between one or more successive turns of the windings. If the fault is not detected early, it may spread to the nearest winding turn and causes permanent damage to the winding. Therefore, it is necessary to detect inter-turn faults to prevent catastrophic failure of the transformer. In this study, failure analysis of a transformer that approaching the catastrophic stage is conducted using sweep frequency response analysis (SFRA) and transformer turns ratio (TTR) testing methods. Correlation between SFRA & TTR tests is analyzed. TTR test before rewinding is > 0.5%, while after rewinding is < 0.5%. 400 VAC voltage is applied, and the output voltage is measured. SFRA and TTR results show that there is a linear correlation for all phases. It can be concluded in this study that the TTR test can be used to detect transformer failure.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1817/1/012021"
    },
    {
        "id": 21060,
        "title": "A Transformer Design with PCB Litz Wire Concept for Solid State Transformer",
        "authors": "Zheqing Li, Feng Jin, Xin Lou, Yi-Hsun Hsieh, Qiang Li, Fred C. Lee",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce53617.2023.10362376"
    },
    {
        "id": 21061,
        "title": "Back Matter",
        "authors": "",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_bm"
    },
    {
        "id": 21062,
        "title": "Polarization Transformer for Satellite Antennas",
        "authors": "Xi Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "This article presents the design and characteristics of new wideband polarization transformer based on a square waveguide with diaphragms. Matching and polarization characteristics of the polarization transformer have been simulated and optimized. Frequency dependences of the simulated characteristics are presented. Developed polarization transformer can be applied in modern satellite antennas.",
        "link": "http://dx.doi.org/10.31219/osf.io/8rhf4"
    },
    {
        "id": 21063,
        "title": "Lou the Queer",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0004"
    },
    {
        "id": 21064,
        "title": "Le changement agile",
        "authors": "",
        "published": "2022-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.autis.2022.02"
    },
    {
        "id": 21065,
        "title": "Transformer and Rectifier",
        "authors": "Kuppusamy Thayalan",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5005/jp/books/13101_6"
    },
    {
        "id": 21066,
        "title": "The Man Himself",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0021"
    },
    {
        "id": 21067,
        "title": "Lou and Bowie",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0006"
    },
    {
        "id": 21068,
        "title": "Transformer la banque",
        "authors": "",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.roman.2020.01"
    },
    {
        "id": 21069,
        "title": "CTARNS: Improving Capacity Estimation of Lithium-Ion Battery by Using Convolutional Transformer with Nested Sequence Models",
        "authors": "Hung Bui, Thien Pham, Tho Quan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3236-8_50"
    },
    {
        "id": 21070,
        "title": "Using Deep Transformer Based Models to Predict Ozone Levels",
        "authors": "Manuel Méndez, Carlos Montero, Manuel Núñez",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-21743-2_14"
    },
    {
        "id": 21071,
        "title": "Investigating Vision Transformer Models for Low-Resolution Medical Image Recognition",
        "authors": "Isaac Adjei-Mensah, Xiaoling Zhang, Adu Asare Baffour, Isaac Osei Agyemang, Sophyani Banaamwini Yussif, Bless Lord Y. Agbley, Collins Sey",
        "published": "2021-12-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip53232.2021.9674065"
    },
    {
        "id": 21072,
        "title": "Fine-Tuning Transformer Models for Adverse Drug Event Identification and Extraction in Biomedical Corpora: A Comparative Study",
        "authors": "Chanaa Hiba, El Habib Nfaoui, Chakir Loqman",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29857-8_95"
    },
    {
        "id": 21073,
        "title": "Transformer-based models to deal with heterogeneous environments in Human Activity Recognition",
        "authors": "Sannara Ek, François Portet, Philippe Lalanda",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00779-023-01776-3"
    },
    {
        "id": 21074,
        "title": "Energy-efficient Inference Service of Transformer-based Deep Learning Models on GPUs",
        "authors": "Yuxin Wang, Qiang Wang, Xiaowen Chu",
        "published": "2020-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ithings-greencom-cpscom-smartdata-cybermatics50389.2020.00067"
    },
    {
        "id": 21075,
        "title": "Generalization of finetuned transformer language models to new clinical contexts",
        "authors": "Kevin Xie, Samuel W Terman, Ryan S Gallagher, Chloe E Hill, Kathryn A Davis, Brian Litt, Dan Roth, Colin A Ellis",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "Abstract\n\nObjective\nWe have previously developed a natural language processing pipeline using clinical notes written by epilepsy specialists to extract seizure freedom, seizure frequency text, and date of last seizure text for patients with epilepsy. It is important to understand how our methods generalize to new care contexts.\n\n\nMaterials and methods\nWe evaluated our pipeline on unseen notes from nonepilepsy-specialist neurologists and non-neurologists without any additional algorithm training. We tested the pipeline out-of-institution using epilepsy specialist notes from an outside medical center with only minor preprocessing adaptations. We examined reasons for discrepancies in performance in new contexts by measuring physical and semantic similarities between documents.\n\n\nResults\nOur ability to classify patient seizure freedom decreased by at least 0.12 agreement when moving from epilepsy specialists to nonspecialists or other institutions. On notes from our institution, textual overlap between the extracted outcomes and the gold standard annotations attained from manual chart review decreased by at least 0.11 F1 when an answer existed but did not change when no answer existed; here our models generalized on notes from the outside institution, losing at most 0.02 agreement. We analyzed textual differences and found that syntactic and semantic differences in both clinically relevant sentences and surrounding contexts significantly influenced model performance.\n\n\nDiscussion and conclusion\nModel generalization performance decreased on notes from nonspecialists; out-of-institution generalization on epilepsy specialist notes required small changes to preprocessing but was especially good for seizure frequency text and date of last seizure text, opening opportunities for multicenter collaborations using these outcomes.\n",
        "link": "http://dx.doi.org/10.1093/jamiaopen/ooad070"
    },
    {
        "id": 21076,
        "title": "A transformer condition recognition method based on dissolved gas analysis features selection and multiple models fusion",
        "authors": "Xiaohui Han, Song Huang, Xu Zhang, Yingkuo Zhu, Guoqing An, Zhenbin Du",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106518"
    },
    {
        "id": 21077,
        "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models",
        "authors": "Emma Yann Zhang, Adrian David Cheok, Zhigeng Pan, Jun Cai, Ying Yan",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "In recent years, generative transformers have become increasingly prevalent in the field of artificial intelligence, especially within the scope of natural language processing. This paper provides a comprehensive overview of these models, beginning with the foundational theories introduced by Alan Turing and extending to contemporary generative transformer architectures. The manuscript serves as a review, historical account, and tutorial, aiming to offer a thorough understanding of the models’ importance, underlying principles, and wide-ranging applications. The tutorial section includes a practical guide for constructing a basic generative transformer model. Additionally, the paper addresses the challenges, ethical implications, and future directions in the study of generative models.",
        "link": "http://dx.doi.org/10.3390/sci5040046"
    },
    {
        "id": 21078,
        "title": "AF-Transformer: Attention Fusion Transformer for Facial Expression Recognition",
        "authors": "Hanning Lu",
        "published": "2022-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvidliccea56201.2022.9824452"
    },
    {
        "id": 21079,
        "title": "A Zero-Shot Transformer Model For an Attribute-Guided Challenge",
        "authors": "Nicos Isaak",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011782000003393"
    },
    {
        "id": 21080,
        "title": "Transformer Design for Transformer- Coupled Multi-phase Voltage-controlled Oscillators (VCOs)",
        "authors": "Sheng-Lyang Jang, Sung-Yang Chen, Dan-Li Wang, Wen-Cheng Lai",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicm59499.2023.10365995"
    },
    {
        "id": 21081,
        "title": "Transformer",
        "authors": "Mukund R. Patel",
        "published": "2021-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003191513-6"
    },
    {
        "id": 21082,
        "title": "Discrete modeling of a transformer with ALEGRA",
        "authors": "None None",
        "published": "2021-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1855070"
    },
    {
        "id": 21083,
        "title": "Effects of Spatial Transformer Location on Segmentation Performance of a Dense Transformer Network",
        "authors": "David Abou-Chacra, John Zelek",
        "published": "2017-10-15",
        "citations": 0,
        "abstract": "Semantic segmentation solves the task of labelling every pixel inan image with its class label, and remains an important unsolvedproblem. While significant work has gone into using deep learningto solve this problem, almost all the existing research uses methodsthat do not make modifications on spatial context considered for thepixel being labelled. Spatial information is an important cue in taskssuch as segmentation, reusing the same spatial span for every pixeland every label may not be the best approach. Spatial TransformerNetworks have shown promising results in improving classificationperformance of existing networks by allowing networks to activelymanipulate their input data to achieve better performance. Our workshows the benefit of incorporating Spatial Transformer Networksand their corresponding decoders into networks tailored to semanticsegmentation. Our experiments show an improvement in performanceover baseline networks when using networks augmentedwith Spatial Transformers.",
        "link": "http://dx.doi.org/10.15353/vsnl.v3i1.169"
    },
    {
        "id": 21084,
        "title": "Asset management of power transformer: optimization of transformer liftime cost based on quality management strategy",
        "authors": "P. Xie, L. Li, Y. Hao",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2020.0017"
    },
    {
        "id": 21085,
        "title": "Calculation of voltage distribution along the transformer winding using the wide band transformer model",
        "authors": "Bruno Jurišić, Luka Bučar, Ivo Uglešić",
        "published": "2022-6-23",
        "citations": 0,
        "abstract": "Electrical devices in transmission and distribution networks are submitted to fast front transients. These transients are often a cause of a failure in the power system. To simulate these phenomena, it is necessary to detailly model all the components of the power system. This paper concentrates on studding transformer internal failures due to voltage stress that can occur when fast front electromagnetic wave travels through the transformer. To prevent a failure, an internal insulation of the transformer has to be dimensioned to sustain these transient phenomena. Therefore, it is necessary to use advanced wideband transformer models for its dimensioning. In this paper, a wide band transformer model based on limited transformer geometry is used to simulate evaluate the stresses on internal transformer insulation in the case of fast front transients. The model is validated for calculation of internal overvoltages with the test case from CIGRE brochure.",
        "link": "http://dx.doi.org/10.37798/2017661-495"
    },
    {
        "id": 21086,
        "title": "Combining Transformer and Reverse Attention Mechanism for Polyp Segmentation",
        "authors": "Jianzhuang Lin, Wenzhong Yang, Sixiang Tan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012014800003633"
    },
    {
        "id": 21087,
        "title": "Note: Development of a triple resonance pulse transformer based on magnetic core transformer",
        "authors": "Mingjia Li, Chuan Liang, Lin Zhou, Faqiang Zhang, Wenchuan Wang",
        "published": "2018-9-1",
        "citations": 0,
        "abstract": "In this paper, a triple resonance pulse transformer based on a magnetic core transformer is developed, which uses the first peak of the output voltage to charge the load capacitor. A compact magnetic core pulse transformer is developed, which uses an innovative bi-conical specific geometry. Based on this magnetic core pulse transformer, the triple resonance circuit is built by adding a tuning capacitor and a tuning inductor between this transformer and a pulse forming line (PFL). The tuning capacitor is designed to be an irregular coaxial capacitor with a capacitance of 70 pF and the tuning inductor is made as a single-layer air core cylindrical inductor. The experimental results indicate that the peak output voltage of the triple resonance pulse transformer is about 530 kV and the rise time is about 790 ns. The peak voltage across PFL is 1.6 times the peak voltage across the magnetic core transformer. It is feasible to develop a magnetic core pulse transformer into a triple resonance pulse transformer.",
        "link": "http://dx.doi.org/10.1063/1.5038793"
    },
    {
        "id": 21088,
        "title": "STR Transformer: A Cross-domain Transformer for Scene Text Recognition",
        "authors": "Xing Wu, Bin Tang, Ming Zhao, Jianjia Wang, Yike Guo",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-03728-5"
    },
    {
        "id": 21089,
        "title": "Active Fault Analysis of Distribution Transformer Service Area Based on Intelligent Transformer Terminal Unit",
        "authors": "Yang Zhichun, Shen Yu, Yang Fan, Su Lei, Lei Yang, Yang Fangbin",
        "published": "2019-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec48194.2019.8975728"
    },
    {
        "id": 21090,
        "title": "Application of statistical tools in power transformer FRA results interpretation: Transformer winding diagnosis based on frequency response analysis",
        "authors": "G. U. Nnachi, A. O. Akumu, C. G. Richards, D. V. Nicolae",
        "published": "2017-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica.2017.7991193"
    },
    {
        "id": 21091,
        "title": "WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation",
        "authors": "Yang Wang, Chenxing Li, Feng Deng, Shun Lu, Peng Yao, Jianchao Tan, Chengru Song, Xiaorui Wang",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-995"
    },
    {
        "id": 21092,
        "title": "Generalized Attention Mechanism and Relative Position for Transformer",
        "authors": "Raja Vikram Pandya",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2476"
    },
    {
        "id": 21093,
        "title": "On the Use of Real-Time Transformer Temperature Estimation for Improving Transformer Operational Tripping Schemes",
        "authors": "Yuhang Cong, Peter Wall, Mark Osborne, Qi Li, Yigui Li, Vladimir Terzija",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4079486"
    },
    {
        "id": 21094,
        "title": "The correction of instrument transformer ferroresonance model in IEC standard and study on resonance interval based on transformer damping",
        "authors": "Hongwen Liu, Chenchao Chai, Chunli Zhang, Jindong Yang",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "The traditional ferromagnetic resonance model of the inductive voltage transformer in the IEC/TR 61869-102 transformer standard ignores the DC resistance of the transformer. The author believes that ignoring the damping effect of the transformer itself is not justified. Based on this, the model is modified, and the corresponding differential equation is derived. Through simulation and field testing, it is found that the DC resistance of the transformer has a damping effect on resonant overvoltage and resonant overcurrent. Moreover, with an increase in DC resistance, the ferromagnetic resonance region decreases. By increasing the DC resistance of the voltage transformer, it helps to reduce the ferromagnetic resonance interval as well as the inrush current.",
        "link": "http://dx.doi.org/10.1063/5.0169513"
    },
    {
        "id": 21095,
        "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence",
        "authors": "Kelvin Lo, Yuan Jin, Weicong Tan, Ming Liu, Lan Du, Wray Buntine",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.283"
    },
    {
        "id": 21096,
        "title": "Attention! Transformer with Sentiment on Cryptocurrencies Price Prediction",
        "authors": "Huali Zhao, Martin Crane, Marija Bezbradica",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011103400003197"
    },
    {
        "id": 21097,
        "title": "Heat transfer comparison of nanofluid filled transformer and traditional oil-immersed transformer",
        "authors": "Yunpeng Zhang, Siu-lau Ho, Weinong Fu",
        "published": "2018-5-1",
        "citations": 12,
        "abstract": "Dispersing nanoparticles with high thermal conductivity into transformer oil is an innovative approach to improve the thermal performance of traditional oil-immersed transformers. This mixture, also known as nanofluid, has shown the potential in practical application through experimental measurements. This paper presents the comparisons of nanofluid filled transformer and traditional oil-immersed transformer in terms of their computational fluid dynamics (CFD) solutions from the perspective of optimal design. Thermal performance of transformers with the same parameters except coolants is compared. A further comparison on heat transfer then is made after minimizing the oil volume and maximum temperature-rise of these two transformers. Adaptive multi-objective optimization method is employed to tackle this optimization problem.",
        "link": "http://dx.doi.org/10.1063/1.5006749"
    },
    {
        "id": 21098,
        "title": "Forensic Examination of Seized Transformer Oil: Case Study from Delhi, India",
        "authors": "Sweta Sinha",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23880/ijfsc-16000227"
    },
    {
        "id": 21099,
        "title": "Online Polyglot Programming Education with LFT (Lingua Franca Transformer)",
        "authors": "Sokratis Karkalas, Filothei Chalvatza, Manolis Mavrikis",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011981400003470"
    },
    {
        "id": 21100,
        "title": "MTPA_Unet: Multi-Scale Transformer-Position Attention Retinal Vessel Segmentation Network Joint Transformer and CNN",
        "authors": "Yun Jiang, Jing Liang, Tongtong Cheng, Xin Lin, Yuan Zhang, Jinkun Dong",
        "published": "2022-6-17",
        "citations": 9,
        "abstract": "Retinal vessel segmentation is extremely important for risk prediction and treatment of many major diseases. Therefore, accurate segmentation of blood vessel features from retinal images can help assist physicians in diagnosis and treatment. Convolutional neural networks are good at extracting local feature information, but the convolutional block receptive field is limited. Transformer, on the other hand, performs well in modeling long-distance dependencies. Therefore, in this paper, a new network model MTPA_Unet is designed from the perspective of extracting connections between local detailed features and making complements using long-distance dependency information, which is applied to the retinal vessel segmentation task. MTPA_Unet uses multi-resolution image input to enable the network to extract information at different levels. The proposed TPA module not only captures long-distance dependencies, but also focuses on the location information of the vessel pixels to facilitate capillary segmentation. The Transformer is combined with the convolutional neural network in a serial approach, and the original MSA module is replaced by the TPA module to achieve finer segmentation. Finally, the network model is evaluated and analyzed on three recognized retinal image datasets DRIVE, CHASE DB1, and STARE. The evaluation metrics were 0.9718, 0.9762, and 0.9773 for accuracy; 0.8410, 0.8437, and 0.8938 for sensitivity; and 0.8318, 0.8164, and 0.8557 for Dice coefficient. Compared with existing retinal image segmentation methods, the proposed method in this paper achieved better vessel segmentation in all of the publicly available fundus datasets tested performance and results.",
        "link": "http://dx.doi.org/10.3390/s22124592"
    },
    {
        "id": 21101,
        "title": "Automatic Sleep Stage Classification Method based on Transformer-in-Transformer",
        "authors": "Moogyeong Kim, Koohong Jung, Wonzoo Chung",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bci57258.2023.10078607"
    },
    {
        "id": 21102,
        "title": "Analysis and Design of a Multiport Resonant DC-Transformer for Solid-State Transformer Applications",
        "authors": "Thiago Pereira, Yuqi Wei, H. Alan Mantooth, Marco Liserre",
        "published": "2022-10-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce50734.2022.9948144"
    },
    {
        "id": 21103,
        "title": "Implementation of Multipath Transformer in CMOS Voltage-Controlled Oscillator CMOS VCO Using Multi-Path Transformer",
        "authors": "Wen-Cheng Lai, Sheng-Lyang Jang, Yu-Wen Huang",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icam.2018.8596589"
    },
    {
        "id": 21104,
        "title": "Template Attack Based on Improved Transformer Model",
        "authors": "静 彭",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/aam.2023.122069"
    },
    {
        "id": 21105,
        "title": "Efficiency Optimization Strategies for Point Transformer Networks",
        "authors": "Jannis Unkrig, Markus Friedrich",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012325000003660"
    },
    {
        "id": 21106,
        "title": "A High Power High Frequency Transformer Design for Solid State Transformer Applications",
        "authors": "Ahmad El Shafei, Saban Ozdemir, Necmi Altin, Garry Jean-Pierre, Adel Nasiri",
        "published": "2019-11",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrera47325.2019.8996515"
    },
    {
        "id": 21107,
        "title": "3M-Transformer: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction",
        "authors": "Mehdi Fatan, Emanuele Mincato, Dimitra Pintzou, Mariella Dimiccoli",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448136"
    },
    {
        "id": 21108,
        "title": "“Vicious”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0008"
    },
    {
        "id": 21109,
        "title": "Copyright",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.12001-4"
    },
    {
        "id": 21110,
        "title": "Electric Power Transformer Engineering",
        "authors": "Leonard L. Grigsby",
        "published": "2017-12-19",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110"
    },
    {
        "id": 21111,
        "title": "DC Transformer Detection of Power Transformer Based on Characteristic Analysis of Vibration Signal",
        "authors": "Jinlong Wang, Jianyuan Xu, Chen Cao, Xin Lin, Sha Hao",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icepe-st.2019.8928786"
    },
    {
        "id": 21112,
        "title": "Transformer Engineering",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 46,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011"
    },
    {
        "id": 21113,
        "title": "Causes of Power Transformer Failure",
        "authors": "Vasily Ya. Ushakov, Alexey V. Mytnikov, Valeriy A. Lavrinovich, Alexey V. Lavrinovich",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83198-1_1"
    },
    {
        "id": 21114,
        "title": "TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution",
        "authors": "Armin Mehri, Parichehr Behjati, Angel Domingo Sappa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3241852"
    },
    {
        "id": 21115,
        "title": "A structural overview on transformer and transformer-less multi level inverters for renewable energy applications",
        "authors": "Dhanamjayulu C., P. Sanjeevikumar, S.M. Muyeen",
        "published": "2022-11",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egyr.2022.07.166"
    },
    {
        "id": 21116,
        "title": "Identification Method for Household-Transformer Relationship in Low-voltage Transformer Area Based on LCSS-DBSCAN",
        "authors": "Wenjin Zou, Shaofei Hao, Haoran Ge, Yu Xia, Gang Ma",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciycee55749.2022.9958988"
    },
    {
        "id": 21117,
        "title": "Circuit Model of a Two-Winding Transformer with Core",
        "authors": "Robert M. Del Vecchio, Bertrand Poulin, Pierre T. Feghali, Dilipkumar M. Shah, Rajendra Ahuja",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/ebk1439805824-3"
    },
    {
        "id": 21118,
        "title": "EDDIE-Transformer: Enriched Disease Embedding Transformer for X-Ray Report Generation",
        "authors": "Hoang T.N. Nguyen, Dong Nie, Taivanbat Badamdorj, Yujie Liu, Lingzi Hong, Jason Truong, Li Cheng",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi52829.2022.9761459"
    },
    {
        "id": 21119,
        "title": "High-Frequency Transformer Design with Hollow Core for Solid State Transformer",
        "authors": "Haitham A Obaid, Yasir M Y Ameen",
        "published": "2021-8-1",
        "citations": 2,
        "abstract": "Abstract\nSolid-state transformer (SST) is one of the new technologies that has kept pace with the development of renewable energy sources such as the solar energy and wind turbines. The SST consists of a high-frequency (HF) transformer and power electronic converter at both ends of the HF transformer. Despite the high efficiency of the traditional transformers, is they are quite large in size and also very heavy. In order to reduce the size and weight of the transformers, SST is used as an alternative for the conventional transformer which also brings along other advantages. The type of core material, the type of wire used, and the method of winding the coils on the HF transformer core affects the core losses, copper losses, the cost, efficiency, and power density of the HF transformer. In this paper a new method has been proposed to form the core of the HF transformer used in SST where a hollow core is used, thus reducing the size and weight of the HF transformer along with the reduction in the core loss. The proposed core is designed using ANSYS Maxwell 3D software in addition to calculating the inductance matrix and the coupling coefficient. Finally, the proposed model of a high-frequency transformer was simulated using MATLAB Simulink software.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1973/1/012088"
    },
    {
        "id": 21120,
        "title": "Preface",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.05001-1"
    },
    {
        "id": 21121,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2024-1-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v2/review2"
    },
    {
        "id": 21122,
        "title": "Flexible Large Power Solid State Transformer (FLP-SST)",
        "authors": "Subhasish Bhattacharya",
        "published": "2019-2-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1647580"
    },
    {
        "id": 21123,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v1/review1"
    },
    {
        "id": 21124,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v1/review3"
    },
    {
        "id": 21125,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v3/review1"
    },
    {
        "id": 21126,
        "title": "Graph Transformer for drug response prediction",
        "authors": "Thang Chu, Tuan Nguyen",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackgroundPrevious models have shown that learning drug features from their graph representation is more efficient than learning from their strings or numeric representations. Furthermore, integrating multi-omics data of cell lines increases the performance of drug response prediction. However, these models showed drawbacks in extracting drug features from graph representation and incorporating redundancy information from multi-omics data. This paper proposes a deep learning model, GraTransDRP, to better drug representation and reduce information redundancy. First, the Graph transformer was utilized to extract the drug representation more efficiently. Next, Convolutional neural networks were used to learn the mutation, meth, and transcriptomics features. However, the dimension of transcriptomics features is up to 17737. Therefore, KernelPCA was applied to transcriptomics features to reduce the dimension and transform them into a dense presentation before putting them through the CNN model. Finally, drug and omics features were combined to predict a response value by a fully connected network. Experimental results show that our model outperforms some state-of-the-art methods, including GraphDRP, GraOmicDRP.Availability of data and materialshttps://github.com/chuducthang77/GraTransDRP.",
        "link": "http://dx.doi.org/10.1101/2021.11.29.470386"
    },
    {
        "id": 21127,
        "title": "Transformer Networks of Human Conceptual Knowledge",
        "authors": "Sudeep Bhatia, Russell Richie",
        "published": "No Date",
        "citations": 4,
        "abstract": "We present a computational model capable of simulating aspects of human knowledge for thousands of real-world concepts. Our approach involves fine-tuning a transformer network for natural language understanding on participant-generated feature norms. We show that such a model can successfully extrapolate from its training dataset, and predict human knowledge for novel concepts and features. We also apply our model to stimuli from twenty-three previous experiments in semantic cognition research, and show that it reproduces fifteen classic findings involving semantic verification, concept typicality, feature distribution, and semantic similarity. We interpret these findings using established properties of classic connectionist networks. The success of our approach shows how the combination of natural language data and psychological data can be used to build cognitive models with rich world knowledge. Such models can be used in the service of new psychological applications, such as the cognitive process modeling of naturalistic semantic verification and knowledge retrieval, as well as the modeling of real-world categorization, decision making, and reasoning.",
        "link": "http://dx.doi.org/10.31234/osf.io/hs4ra"
    },
    {
        "id": 21128,
        "title": "Punctuation Prediction in Vietnamese ASRs Using Transformer-Based Models",
        "authors": "Viet The Bui, Oanh Thi Tran",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-89363-7_15"
    },
    {
        "id": 21129,
        "title": "Sentiment Analysis of Product Reviews Using Deep Learning and Transformer Models: A Comparative Study",
        "authors": "Sheetal Kusal, Shruti Patil, Aashna Gupta, Harsh Saple, Devashish Jaiswal, Vaishnavi Deshpande, Ketan Kotecha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-8476-3_15"
    },
    {
        "id": 21130,
        "title": "TransPatch: A Transformer-based Generator for Accelerating Transferable Patch Generation in Adversarial Attacks Against Object Detection Models",
        "authors": "Jinghao Wang, Chenling Cui, Xuejun Wen, Jie Shi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-25056-9_21"
    },
    {
        "id": 21131,
        "title": "Runoff predictions in new-gauged basins using two transformer-based models",
        "authors": "Hanlin Yin, Wu Zhu, Xiuwei Zhang, Yinghui Xing, Runliang Xia, Jifeng Liu, Yanning Zhang",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jhydrol.2023.129684"
    },
    {
        "id": 21132,
        "title": "Aging of transformer insulation — experimental transformers and laboratory models with different moisture contents: Part I — DP and furans aging profiles",
        "authors": "Valentina Vasovic, Jelena Lukic, Draginja Mihajlovic, Branko Pejovic, Zoran Radakovic, Uros Radoman, Aleksandar Orlovic",
        "published": "2019-12",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2019.008183"
    },
    {
        "id": 21133,
        "title": "OctaNLP: A Benchmark for Evaluating Multitask Generalization of Transformer-Based Pre-trained Language Models",
        "authors": "Zakaria Kaddari, Youssef Mellah, Jamal Berrich, Mohammed G. Belkasmi, Toumi Bouchentouf",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6893-4_21"
    },
    {
        "id": 21134,
        "title": "Framing and BERTology: A Data-Centric Approach to Integration of Linguistic Features into Transformer-Based Pre-trained Language Models",
        "authors": "Hayastan Avetisyan, Parisa Safikhani, David Broneske",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47718-8_6"
    },
    {
        "id": 21135,
        "title": "Decouple and Resolve: Transformer-Based Models for Online Anomaly Detection From Weakly Labeled Videos",
        "authors": "Tianshan Liu, Cong Zhang, Kin-Man Lam, Jun Kong",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tifs.2022.3216479"
    },
    {
        "id": 21136,
        "title": "Facilitating the Learning Engineering Process for Educational Conversational Modules Using Transformer-Based Language Models",
        "authors": "Behzad Mirzababaei, Viktoria Pammer-Schindler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tlt.2024.3367738"
    },
    {
        "id": 21137,
        "title": "Simplified models of three-phase, five-limb transformer for studying GIC effects",
        "authors": "S.E. Zirka, Y.I. Moroz, J. Elovaara, M. Lahtinen, R.A. Walling, H.Kr. Høidalen, D. Bonmann, C.M. Arturi, N. Chiesa",
        "published": "2018-12",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2018.05.035"
    },
    {
        "id": 21138,
        "title": "A Multiport Power Electronic Transformer with Shared Medium-frequency Transformer",
        "authors": "Dajun Ma, Wu Chen, Liangcai Shu",
        "published": "2020-10-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon43393.2020.9254602"
    },
    {
        "id": 21139,
        "title": "Introduction to Magnetic Inrush of Power Transformer",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_3"
    },
    {
        "id": 21140,
        "title": "Transformer Based Multilingual Grapheme-to-Phoneme Conversion",
        "authors": "亚停 张",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2023.133050"
    },
    {
        "id": 21141,
        "title": "MS-Transformer: Introduce multiple structural priors into a unified transformer for encoding sentences",
        "authors": "Le Qi, Yu Zhang, Qingyu Yin, Ting Liu",
        "published": "2022-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.csl.2021.101304"
    },
    {
        "id": 21142,
        "title": "Replacing the Grid Interface Transformer in Wind Energy Conversion System With Solid-State Transformer",
        "authors": "Imran Syed, Vinod Khadkikar",
        "published": "2017-5",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrs.2016.2614692"
    },
    {
        "id": 21143,
        "title": "Study of Speech Recognition System Based on Transformer and Connectionist Temporal Classification Models for Low Resource Language",
        "authors": "Shweta Bansal, Shambhu Sharan, Shyam S. Agrawal",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-20980-2_6"
    },
    {
        "id": 21144,
        "title": "ViT-DAE: Transformer-Driven Diffusion Autoencoder for Histopathology Image Analysis",
        "authors": "Xuan Xu, Saarthak Kapse, Rajarsi Gupta, Prateek Prasanna",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-53767-7_7"
    },
    {
        "id": 21145,
        "title": "Tackling the Infodemic: Analysis Using Transformer Based Models",
        "authors": "Anand Zutshi, Aman Raj",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-73696-5_10"
    },
    {
        "id": 21146,
        "title": "Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)–Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study",
        "authors": "José Alberto Benítez-Andrades, José-Manuel Alija-Pérez, Maria-Esther Vidal, Rafael Pastor-Vargas, María Teresa García-Ordás",
        "published": "2022-2-24",
        "citations": 14,
        "abstract": "\nBackground\nEating disorders affect an increasing number of people. Social networks provide information that can help.\n\n\nObjective\nWe aimed to find machine learning models capable of efficiently categorizing tweets about eating disorders domain.\n\n\nMethods\nWe collected tweets related to eating disorders, for 3 consecutive months. After preprocessing, a subset of 2000 tweets was labeled: (1) messages written by people suffering from eating disorders or not, (2) messages promoting suffering from eating disorders or not, (3) informative messages or not, and (4) scientific or nonscientific messages. Traditional machine learning and deep learning models were used to classify tweets. We evaluated accuracy, F1 score, and computational time for each model.\n\n\nResults\nA total of 1,058,957 tweets related to eating disorders were collected. were obtained in the 4 categorizations, with The bidirectional encoder representations from transformer–based models had the best score among the machine learning and deep learning techniques applied to the 4 categorization tasks (F1 scores 71.1%-86.4%).\n\n\nConclusions\nBidirectional encoder representations from transformer–based models have better performance, although their computational cost is significantly higher than those of traditional techniques, in classifying eating disorder–related tweets.\n",
        "link": "http://dx.doi.org/10.2196/34492"
    },
    {
        "id": 21147,
        "title": "Deciphering the dynamics of distorted turbulent flows: Lagrangian particle tracking and chaos prediction through transformer-based deep learning models",
        "authors": "R. Hassanian, H. Myneni, Á. Helgadóttir, M. Riedel",
        "published": "2023-7-1",
        "citations": 2,
        "abstract": "Turbulent flow is a complex and vital phenomenon in fluid dynamics, as it is the most common type of flow in both natural and artificial systems. Traditional methods of studying turbulent flow, such as computational fluid dynamics and experiments, have limitations such as high computational costs, experiment costs, and restricted problem scales and sizes. Recently, artificial intelligence has provided a new avenue for examining turbulent flow, which can help improve our understanding of its flow features and physics in various applications. Strained turbulent flow, which occurs in the presence of gravity in situations such as combustion chambers and shear flow, is one such case. This study proposes a novel data-driven transformer model to predict the velocity field of turbulent flow, building on the success of this deep sequential learning technique in areas such as language translation and music. The present study applied this model to experimental work by Hassanian et al., who studied distorted turbulent flow with a specific range of Taylor microscale Reynolds numbers 100&lt;Reλ&lt;120. The flow underwent a vertical mean strain rate of 8 s−1 in the presence of gravity. The Lagrangian particle tracking technique recorded every tracer particle's velocity field and displacement. Using this dataset, the transformer model was trained with different ratios of data and used to predict the velocity of the following period. The model's predictions significantly matched the experimental test data, with a mean absolute error of 0.002–0.003 and an R2 score of 0.98. Furthermore, the model demonstrated its ability to maintain high predictive performance with less training data, showcasing its potential to predict future turbulent flow velocity with fewer computational resources. To assess the model, it has been compared to the long short-term memory and gated recurrent units model. High-performance computing machines, such as JUWELS-DevelBOOSTER at the Juelich Supercomputing Center, were used to train and run the model for inference.",
        "link": "http://dx.doi.org/10.1063/5.0157897"
    },
    {
        "id": 21148,
        "title": "AcrTransAct: Pre-trained Protein Transformer Models for the Detection of Type I Anti-CRISPR Activities",
        "authors": "Moein Hasani, Chantel N. Trost, Nolen Timmerman, Lingling Jin",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3584371.3613007"
    },
    {
        "id": 21149,
        "title": "Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations",
        "authors": "Patrícia Pereira, Rui Ribeiro, Helena Moniz, Luisa Coheur, Joao Paulo Carvalho",
        "published": "2023-8-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fuzz52849.2023.10309719"
    },
    {
        "id": 21150,
        "title": "Multicenter Study of the Utility of Convolutional Neural Network and Transformer Models for the Detection and Segmentation of Meningiomas",
        "authors": "Xin Ma, Lingxiao Zhao, Shijie Dang, Yajing Zhao, Yiping Lu, Xuanxuan Li, Peng Li, Yibo Chen, Nan Mei, Bo Yin, Daoying Geng",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "\nPurpose\nThis study aimed to investigate the effectiveness and practicality of using models like convolutional neural network and transformer in detecting and precise segmenting meningioma from magnetic resonance images.\n\n\nMethods\nThe retrospective study on T1-weighted and contrast-enhanced images of 523 meningioma patients from 3 centers between 2010 and 2020. A total of 373 cases split 8:2 for training and validation. Three independent test sets were built based on the remaining 150 cases. Six convolutional neural network detection models trained via transfer learning were evaluated using 4 metrics and receiver operating characteristic analysis. Detected images were used for segmentation. Three segmentation models were trained for meningioma segmentation and were evaluated via 4 metrics. In 3 test sets, intraclass consistency values were used to evaluate the consistency of detection and segmentation models with manually annotated results from 3 different levels of radiologists.\n\n\nResults\nThe average accuracies of the detection model in the 3 test sets were 97.3%, 93.5%, and 96.0%, respectively. The model of segmentation showed mean Dice similarity coefficient values of 0.884, 0.834, and 0.892, respectively. Intraclass consistency values showed that the results of detection and segmentation models were highly consistent with those of intermediate and senior radiologists and lowly consistent with those of junior radiologists.\n\n\nConclusions\nThe proposed deep learning system exhibits advanced performance comparable with intermediate and senior radiologists in meningioma detection and segmentation. This system could potentially significantly improve the efficiency of the detection and segmentation of meningiomas.\n",
        "link": "http://dx.doi.org/10.1097/rct.0000000000001565"
    },
    {
        "id": 21151,
        "title": "Automated Diagnosis for Colon Cancer Diseases Using Stacking Transformer Models and Explainable Artificial Intelligence",
        "authors": "Lubna Abdelkareim Gabralla, Ali Mohamed Hussien, Abdulaziz AlMohimeed, Hager Saleh, Deema Mohammed Alsekait, Shaker El-Sappagh, Abdelmgeid A. Ali, Moatamad Refaat Hassan",
        "published": "2023-9-13",
        "citations": 1,
        "abstract": "Colon cancer is the third most common cancer type worldwide in 2020, almost two million cases were diagnosed. As a result, providing new, highly accurate techniques in detecting colon cancer leads to early and successful treatment of this disease. This paper aims to propose a heterogenic stacking deep learning model to predict colon cancer. Stacking deep learning is integrated with pretrained convolutional neural network (CNN) models with a metalearner to enhance colon cancer prediction performance. The proposed model is compared with VGG16, InceptionV3, Resnet50, and DenseNet121 using different evaluation metrics. Furthermore, the proposed models are evaluated using the LC25000 and WCE binary and muticlassified colon cancer image datasets. The results show that the stacking models recorded the highest performance for the two datasets. For the LC25000 dataset, the stacked model recorded the highest performance accuracy, recall, precision, and F1 score (100). For the WCE colon image dataset, the stacked model recorded the highest performance accuracy, recall, precision, and F1 score (98). Stacking-SVM achieved the highest performed compared to existing models (VGG16, InceptionV3, Resnet50, and DenseNet121) because it combines the output of multiple single models and trains and evaluates a metalearner using the output to produce better predictive results than any single model. Black-box deep learning models are represented using explainable AI (XAI).",
        "link": "http://dx.doi.org/10.3390/diagnostics13182939"
    },
    {
        "id": 21152,
        "title": "UNITOR @ DANKMEMES: Combining Convolutional Models and Transformer-based architectures for accurate MEME management",
        "authors": "Claudia Breazzano, Edoardo Rubino, Danilo Croce, Roberto Basili",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.aaccademia.7420"
    },
    {
        "id": 21153,
        "title": "“We Must Protect the Transformers”: Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models",
        "authors": "Rohit Raj, Biplab Roy, Abir Das, Mainack Mondal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-51583-5_14"
    },
    {
        "id": 21154,
        "title": "Diabetic Foot Ulcer Segmentation Using Convolutional and Transformer-Based Models",
        "authors": "Mariam Hassib, Maram Ali, Amina Mohamed, Marwan Torki, Mohamed Hussein",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-26354-5_7"
    },
    {
        "id": 21155,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v2/review1"
    },
    {
        "id": 21156,
        "title": "HEMP Transformer Defense Through Power Electronics",
        "authors": "Klaehn Burkes, Vincent Cessyens",
        "published": "2019-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1570350"
    },
    {
        "id": 21157,
        "title": "A Transformer Model for Retrosynthesis",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 5,
        "abstract": "<div><div><div><p>We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45 033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at T=1.3 the accuracy of models up to 1-2%.</p></div></div></div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8058464"
    },
    {
        "id": 21158,
        "title": "Transformer Models for Question Answering at BioASQ 2019",
        "authors": "Michele Resta, Daniele Arioli, Alessandro Fagnani, Giuseppe Attardi",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-43887-6_63"
    },
    {
        "id": 21159,
        "title": "Sentiment Analysis of Steam Reviews Using Transformer Models",
        "authors": "Raghunath Reddy, Ahmed Abdul Naoman, Gollapudi Venkata Sriram Charan, Syed Naveed Fazal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-7137-4_70"
    },
    {
        "id": 21160,
        "title": "Improving the Accuracy of Tracker by Linearized Transformer",
        "authors": "Thang Dinh, Kien Trung, Thanh Chi, Long Quoc",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011900900003411"
    },
    {
        "id": 21161,
        "title": "Porosity prediction through well logging data: A combined approach of convolutional neural network and transformer model (CNN-transformer)",
        "authors": "Youzhuang Sun, Shanchen Pang, Junhua Zhang, Yongan Zhang",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "Porosity, as a key parameter to describe the properties of rock reservoirs, is essential for evaluating the permeability and fluid migration performance of underground rocks. In order to overcome the limitations of traditional logging porosity interpretation methods in the face of geological complexity and nonlinear relationships, this study introduces a CNN (convolutional neural network)-transformer model, which aims to improve the accuracy and generalization ability of logging porosity prediction. CNNs have excellent spatial feature capture capabilities. The convolution operation of CNNs can effectively learn the mapping relationship of local features, so as to better capture the local correlation in the well log. Transformer models are able to effectively capture complex sequence relationships between different depths or time points. This enables the model to better integrate information from different depths or times, and improve the porosity prediction accuracy. We trained the model on the well log dataset to ensure that it has good generalization ability. In addition, we comprehensively compare the performance of the CNN-transformer model with other traditional machine learning models to verify its superiority in logging porosity prediction. Through the analysis of experimental results, the CNN-transformer model shows good superiority in the task of logging porosity prediction. The introduction of this model will bring a new perspective to the development of logging technology and provide a more efficient and accurate tool for the field of geoscience.",
        "link": "http://dx.doi.org/10.1063/5.0190078"
    },
    {
        "id": 21162,
        "title": "Wild Animal Detection Based on Swin-Transformer",
        "authors": "福豪 姜",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/airr.2021.104028"
    },
    {
        "id": 21163,
        "title": "Transformer-Based Models for Automatic Identification of Argument Relations: A Cross-Domain Evaluation",
        "authors": "Ramon Ruiz-Dolz, Jose Alemany, Stella M. Heras Barbera, Ana Garcia-Fornes",
        "published": "2021-11-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mis.2021.3073993"
    },
    {
        "id": 21164,
        "title": "Hybrid transformer convolutional neural network-based radiomics models for osteoporosis screening in routine CT",
        "authors": "Jiachen Liu, Huan Wang, Xiuqi Shan, Lei Zhang, Shaoqian Cui, Zelin Shi, Yunpeng Liu, Yingdi Zhang, Lanbo Wang",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "Abstract\nObjective\nEarly diagnosis of osteoporosis is crucial to prevent osteoporotic vertebral fracture and complications of spine surgery. We aimed to conduct a hybrid transformer convolutional neural network (HTCNN)-based radiomics model for osteoporosis screening in routine CT.\n\nMethods\nTo investigate the HTCNN algorithm for vertebrae and trabecular segmentation, 92 training subjects and 45 test subjects were employed. Furthermore, we included 283 vertebral bodies and randomly divided them into the training cohort (n = 204) and test cohort (n = 79) for radiomics analysis. Area receiver operating characteristic curves (AUCs) and decision curve analysis (DCA) were applied to compare the performance and clinical value between radiomics models and Hounsfield Unit (HU) values to detect dual-energy X-ray absorptiometry (DXA) based osteoporosis.\n\nResults\nHTCNN algorithm revealed high precision for the segmentation of the vertebral body and trabecular compartment. In test sets, the mean dice scores reach 0.968 and 0.961. 12 features from the trabecular compartment and 15 features from the entire vertebral body were used to calculate the radiomics score (rad score). Compared with HU values and trabecular rad-score, the vertebrae rad-score suggested the best efficacy for osteoporosis and non-osteoporosis discrimination (training group: AUC = 0.95, 95%CI 0.91–0.99; test group: AUC = 0.97, 95%CI 0.93–1.00) and the differences were significant in test group according to the DeLong test (p < 0.05).\n\nConclusions\nThis retrospective study demonstrated the superiority of the HTCNN-based vertebrae radiomics model for osteoporosis discrimination in routine CT.\n",
        "link": "http://dx.doi.org/10.1186/s12880-024-01240-5"
    },
    {
        "id": 21165,
        "title": "Difference Between 50 and 60 Hz Transformer No-Load Noise Levels",
        "authors": "Miha Pirnat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_2"
    },
    {
        "id": 21166,
        "title": "A Novel Technique for Differential Protection for Resistance Grounded Three Phase Transformer Using Wavelet Packet Transformer",
        "authors": "R. Gomathi, R. Arunkumar, S.T. Preethi, P.V. Arunprakash, G. Karkuzhali",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscna58489.2023.10370331"
    },
    {
        "id": 21167,
        "title": "Probing Transformer Windings with Nanosecond Pulses",
        "authors": "Vasily Ya. Ushakov, Alexey V. Mytnikov, Valeriy A. Lavrinovich, Alexey V. Lavrinovich",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83198-1_4"
    },
    {
        "id": 21168,
        "title": "Comparative Analysis of Oil-Filled Transformer and Solid-State Transformer for Electric Arc Furnace",
        "authors": "O. J. Soto-Marin, E. A. Cano-Plata, Juan Carlos Balda, A. J. Ustariz-Farfan",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias.2019.8912446"
    },
    {
        "id": 21169,
        "title": "A Transformer Based Semantic Analysis of (non-English) Danish Jobads",
        "authors": "Morten Mathiasen, Jacob Nielsen, Simon Laub",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012008400003470"
    },
    {
        "id": 21170,
        "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
        "authors": "Sitong Wu, Tianyi Wu, Haoru Tan, Guodong Guo",
        "published": "2022-6-28",
        "citations": 16,
        "abstract": "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224x224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i3.20176"
    },
    {
        "id": 21171,
        "title": "Core",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-3"
    },
    {
        "id": 21172,
        "title": "Testing",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-12"
    },
    {
        "id": 21173,
        "title": "Bibliographie",
        "authors": "",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.roman.2020.01.0285"
    },
    {
        "id": 21174,
        "title": "nsαnkaw",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.23"
    },
    {
        "id": 21175,
        "title": "nóli",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.19"
    },
    {
        "id": 21176,
        "title": "Robust transformer tap estimation",
        "authors": "Yuzhang Lin, Ali Abur",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ptc.2017.7980919"
    },
    {
        "id": 21177,
        "title": "nəkʷətαnkaw",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.21"
    },
    {
        "id": 21178,
        "title": "Transformer",
        "authors": "Manu Bazzano",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003280262-8"
    },
    {
        "id": 21179,
        "title": "A Transformer Model for Retrosynthesis",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 1,
        "abstract": "We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45 033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at T=1.3 the accuracy of models up to 1-2%.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8058464.v1"
    },
    {
        "id": 21180,
        "title": "Path-Augmented Graph Transformer Network",
        "authors": "Benson Chen, Regina Barzilay, Tommi S Jaakkola",
        "published": "No Date",
        "citations": 15,
        "abstract": "<div>Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graphstructured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistently</div><div>outperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP)2.</div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8214422"
    },
    {
        "id": 21181,
        "title": "“Make Up”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0015"
    },
    {
        "id": 21182,
        "title": "Comparative Analysis of Solid State Transformer and Conventional Transformer for Doubly Fed Induction Generator fed Wind Energy Conversion System",
        "authors": "Komal V. Autkar, Sanjay S. Dhamse",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icgciot.2018.8752972"
    },
    {
        "id": 21183,
        "title": "RS Transformer: A Two-Stage Region Proposal Using the Swin Transformer for Few-Shot Pest Detection in Automated Agricultural Monitoring Systems",
        "authors": "Tengyue Wu, Liantao Shi, Lei Zhang, Xingkai Wen, Jianjun Lu, Zhengguo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Agriculture is pivotal in national economies, with pest detection significantly influencing food quality and quantity. Pest classification remains challenging in automated agriculture monitoring systems, exacerbated by the non-uniform pest scales and the scarcity of high-quality datasets. In this study, we constructed a pest dataset by acquiring domain-agnostic images from the Internet and resizing them to a standardized 299x299 pixel format. Additionally, we employed diffusion models to generate supplementary data. While Convolutional Neural Networks (CNNs) are prevalent for prediction and classification, they often lack effective global information integration and discriminative feature representation. To address these limitations, we propose the RS Transformer, an innovative model that combines elements like the Region Proposal Network, Swin Transformer, and ROI Align. Additionally, we introduce the Randomly Generated Stable Diffusion Dataset (RGSDD) to augment the availability of high-quality pest datasets. Extensive experimental evaluations demonstrate the superiority of our approach compared to both two-stage models (SSD and Faster R-CNN) and one-stage models (YOLOv3, YOLOv4, YOLOv5m, YOLOv8, and DETR). We rigorously assess performance using metrics such as mean Average Precision (mAP), F1Score, Recall, and mean Detection Time (mDT). Our research contributes to advancing pest detection methodologies in automated agriculture systems, promising improved food production and quality.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1162.v1"
    },
    {
        "id": 21184,
        "title": "A Hybrid Solid State Transformer (HSST) Based on a Two Stage Isolated Solid State Transformer (SST): Control and Modulation",
        "authors": "Sanjay Rajendran, Wei Xu, Zhicheng Guo, Alex Q. Huang",
        "published": "2023-3-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43580.2023.10131531"
    },
    {
        "id": 21185,
        "title": "Current Direction Comparison-Based Transformer Protection Using Kalman Filtering",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_10"
    },
    {
        "id": 21186,
        "title": "Optimized lightweight CA-transformer: Using transformer for fine-grained visual categorization",
        "authors": "Haiqing Wang, Shuqi Shang, Dongwei Wang, Xiaoning He, Kai Feng, Hao Zhu, Chengpeng Li, Yuetao Wang",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ecoinf.2022.101827"
    },
    {
        "id": 21187,
        "title": "Reliable Power Transformer Efficiency Tests",
        "authors": "Gert Rietveld, Ernest Houtzager, Dennis Hoogenboom, Gu Ye",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_10"
    },
    {
        "id": 21188,
        "title": "Design and Experimental Verification of a DAB Medium Frequency Transformer for a 6.6kV/200V Solid State Transformer",
        "authors": "Rene Barrera-Cardenas, Takanori Isobe, Terazono Katsushi, Tadano Hiroshi",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ipec.2018.8507595"
    },
    {
        "id": 21189,
        "title": "Investigation on the Occurrence of Ferroresonance with the Variation of Core Loss of a Transformer using Nonlinear Dynamic Model of the Transformer",
        "authors": "Rajat Shubhra Pal, Madhab Roy",
        "published": "2021-2-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icees51510.2021.9383708"
    },
    {
        "id": 21190,
        "title": "Recognition of Facial Expressions Using Vision Transformer",
        "authors": "Paula Ivone Rodríguez-Azar, José Manuel Mejía-Muñoz, Carlos Alberto Ochoa-Zezzatti",
        "published": "2022-12",
        "citations": 0,
        "abstract": "The identification of emotions through the reading of non-verbal signals, such as gestures and facial expressions, has generated a new application in the field of Facial Expression Recognition (FER) and human-computer interaction. Through the recognition of facial expressions, it would be possible to improve industrial equipment by making it safer through social intelligence that has excellent applications in the area of industrial security. That is why this research proposes to classify a series of images from the database called FER-2013, which contains data on seven different emotions, which are anger, disgust, fear, joy, sadness, surprise, neutral. For the recognition of expressions, a Vision Transformer architecture was implemented, of which 87% precision was obtained, while the top test accuracy was 99%.",
        "link": "http://dx.doi.org/10.46842/ipn.cien.v26n2a02"
    },
    {
        "id": 21191,
        "title": "Theme Transformer: Symbolic Music Generation With Theme-Conditioned Transformer",
        "authors": "Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard Müller, Yi-Hsuan Yang",
        "published": "2023",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tmm.2022.3161851"
    },
    {
        "id": 21192,
        "title": "CHARACTERISTICS OF DIELECTRIC FLUIDS IN POWER TRANSFORMER APPLICATIONS – A REVIEW",
        "authors": "",
        "published": "2017-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23883/ijrter.2017.3032.ihoat"
    },
    {
        "id": 21193,
        "title": "Calculation of internal overvoltages using a wide band transformer model based on limited information about transformer design",
        "authors": "Bruno Jurišić, Paul Poujade, Alain Xemard, Françoise Paladian",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.716"
    },
    {
        "id": 21194,
        "title": "Total Harmonic Distortion-Based Improved Transformer Protective Scheme",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_6"
    },
    {
        "id": 21195,
        "title": "STDM-transformer: Space-time dual multi-scale transformer network for skeleton-based action recognition",
        "authors": "Zhifu Zhao, Ziwei Chen, Jianan Li, Xuemei Xie, Kai Chen, Xiaotian Wang, Guangming Shi",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126903"
    },
    {
        "id": 21196,
        "title": "“Perfect Day”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0010"
    },
    {
        "id": 21197,
        "title": "“Andy’s Chest”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0009"
    },
    {
        "id": 21198,
        "title": "nisαnkaw",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.22"
    },
    {
        "id": 21199,
        "title": "Prada Transformer Seoul, KR",
        "authors": "",
        "published": "2019-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11129/9783955534998-010"
    },
    {
        "id": 21200,
        "title": "Software Development for Transformer Model Supporting Significant Learning Electrical Machines",
        "authors": "Ciaddy Gina Rodriguez Borges",
        "published": "2020-2-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37200/ijpr/v24i2/pr200373"
    },
    {
        "id": 21201,
        "title": "Index",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-22"
    },
    {
        "id": 21202,
        "title": "Also available in the series",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0023"
    },
    {
        "id": 21203,
        "title": "tὰpawαs",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.17"
    },
    {
        "id": 21204,
        "title": "nəkʷətαs",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.16"
    },
    {
        "id": 21205,
        "title": "mətala",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.20"
    },
    {
        "id": 21206,
        "title": "Index",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.26"
    },
    {
        "id": 21207,
        "title": "Autotransformers",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-11"
    },
    {
        "id": 21208,
        "title": "Introduction",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-2"
    },
    {
        "id": 21209,
        "title": "Introduction",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.7"
    },
    {
        "id": 21210,
        "title": "Adjustable RF photonic fractional Hilbert transformer based on Kerr microcombs",
        "authors": "David Moss",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We demonstrate an RF photonic fractional Hilbert transformer based on an integrated Kerr\nmicro-comb source featuring a record low free spectral range of 49 GHz. By\nprogramming and shaping the comb lines according to calculated tap weights for\nup to 39 wavelengths across the C-band, we achieve tunable bandwidths ranging\nfrom 1.2 to 15.3 GHz as well as variable center frequencies from baseband to\n9.5 GHz, for both standard integral and arbitrary fractional orders. We\nexperimentally characterize the RF amplitude and phase response of the tunable\nbandpass and lowpass Hilbert transformers with 90 and 45-degree phase shifts. The\nexperimental results show good agreement with theory, confirming the\neffectiveness of our approach as a powerful way to implement standard and fractional\norder Hilbert transformers with broad and\nvariable bandwidths and center frequencies, with high reconfigurability and\ngreatly reduced size and complexity.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14374490.v1"
    },
    {
        "id": 21211,
        "title": "Financial News Summarisation using Transformer Neural Network",
        "authors": "Parth Mihir Patel",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nTransformer architecture, which is based on self-attention mechanism, revolutionised the field of NLP in 2017. It overcame many of the limitations of sequential and iterative approach of the previous popular architectures like LSTM. Generative Pretrained Transformer (GPT), which is a type of transformer model, is one of the most powerful neural network architectures for the purpose of text summarisation. This paper elaborates how and why GPT-2 can be used for financial news summarisation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2132871/v1"
    },
    {
        "id": 21212,
        "title": "Hydraulic Analysis of Oil Spill Control Systems in Transformer Stations",
        "authors": "Celia Fan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Electrical transformer stations use transformer oil to increase the efficiency of the electrical voltage transfer and to reduce the moisture and air in an electrical transformer. Each year, there is a high probability of spilling the transformer oil accidentally into the environment. Some spill events contain large volume of transformer oil. The objective of this thesis is to investigate oil spill control systems for spilled transformer oil during all operating and weathering conditions at a Hydro One's transformer station near the city of Burlington. This thesis examines the design of (1) oil trap systems which trap the spilled transformer oil and (2) the oil back-up systems thick back up the transformer oil spills to the transformer station. This research focuses primarily on Hydro One's transformer stations and the normal operation conditions in Ontario.",
        "link": "http://dx.doi.org/10.32920/ryerson.14648940"
    },
    {
        "id": 21213,
        "title": "Transformer-Rectifier Unit For Centrifugal Cleaning Of Transformer Oil",
        "authors": "Andrii Hnatov, Shchasiana Arhun, Andrii Nechaus, Tarasova Valentina, Nadezhda Kunicina, Yelena Caiko",
        "published": "2021-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rtucon53541.2021.9711722"
    },
    {
        "id": 21214,
        "title": "Windings",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-4"
    },
    {
        "id": 21215,
        "title": "Cooling",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-8"
    },
    {
        "id": 21216,
        "title": "Index",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.20001-3"
    },
    {
        "id": 21217,
        "title": "Front Matter",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.01001-6"
    },
    {
        "id": 21218,
        "title": "References",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-21"
    },
    {
        "id": 21219,
        "title": "Video Captioning Using Transformer-Based Gan",
        "authors": "Mohammad Reza Babavalian, Kourosh Kiani",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4511115"
    },
    {
        "id": 21220,
        "title": "Knowledge Distillation for Streaming Transformer&amp;#8211;Transducer",
        "authors": "Atsushi Kojima",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-175"
    },
    {
        "id": 21221,
        "title": "“Wagon Wheel”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0017"
    },
    {
        "id": 21222,
        "title": "“Hangin’ ’Round”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0011"
    },
    {
        "id": 21223,
        "title": "Power Transformer Condition Monitoring and Diagnosis",
        "authors": "",
        "published": "2018-4-30",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e"
    },
    {
        "id": 21224,
        "title": "Core",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-3"
    },
    {
        "id": 21225,
        "title": "Testing",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-12"
    },
    {
        "id": 21226,
        "title": "Bibliographie",
        "authors": "",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.roman.2020.01.0285"
    },
    {
        "id": 21227,
        "title": "nsαnkaw",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.23"
    },
    {
        "id": 21228,
        "title": "nóli",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.19"
    },
    {
        "id": 21229,
        "title": "Robust transformer tap estimation",
        "authors": "Yuzhang Lin, Ali Abur",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ptc.2017.7980919"
    },
    {
        "id": 21230,
        "title": "nəkʷətαnkaw",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.21"
    },
    {
        "id": 21231,
        "title": "“Make Up”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0015"
    },
    {
        "id": 21232,
        "title": "Transformer",
        "authors": "Manu Bazzano",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003280262-8"
    },
    {
        "id": 21233,
        "title": "A Transformer Model for Retrosynthesis",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 1,
        "abstract": "We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45 033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at T=1.3 the accuracy of models up to 1-2%.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8058464.v1"
    },
    {
        "id": 21234,
        "title": "Path-Augmented Graph Transformer Network",
        "authors": "Benson Chen, Regina Barzilay, Tommi S Jaakkola",
        "published": "No Date",
        "citations": 15,
        "abstract": "<div>Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graphstructured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistently</div><div>outperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP)2.</div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8214422"
    },
    {
        "id": 21235,
        "title": "Comparative Analysis of Solid State Transformer and Conventional Transformer for Doubly Fed Induction Generator fed Wind Energy Conversion System",
        "authors": "Komal V. Autkar, Sanjay S. Dhamse",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icgciot.2018.8752972"
    },
    {
        "id": 21236,
        "title": "RS Transformer: A Two-Stage Region Proposal Using the Swin Transformer for Few-Shot Pest Detection in Automated Agricultural Monitoring Systems",
        "authors": "Tengyue Wu, Liantao Shi, Lei Zhang, Xingkai Wen, Jianjun Lu, Zhengguo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Agriculture is pivotal in national economies, with pest detection significantly influencing food quality and quantity. Pest classification remains challenging in automated agriculture monitoring systems, exacerbated by the non-uniform pest scales and the scarcity of high-quality datasets. In this study, we constructed a pest dataset by acquiring domain-agnostic images from the Internet and resizing them to a standardized 299x299 pixel format. Additionally, we employed diffusion models to generate supplementary data. While Convolutional Neural Networks (CNNs) are prevalent for prediction and classification, they often lack effective global information integration and discriminative feature representation. To address these limitations, we propose the RS Transformer, an innovative model that combines elements like the Region Proposal Network, Swin Transformer, and ROI Align. Additionally, we introduce the Randomly Generated Stable Diffusion Dataset (RGSDD) to augment the availability of high-quality pest datasets. Extensive experimental evaluations demonstrate the superiority of our approach compared to both two-stage models (SSD and Faster R-CNN) and one-stage models (YOLOv3, YOLOv4, YOLOv5m, YOLOv8, and DETR). We rigorously assess performance using metrics such as mean Average Precision (mAP), F1Score, Recall, and mean Detection Time (mDT). Our research contributes to advancing pest detection methodologies in automated agriculture systems, promising improved food production and quality.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1162.v1"
    },
    {
        "id": 21237,
        "title": "Current Direction Comparison-Based Transformer Protection Using Kalman Filtering",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_10"
    },
    {
        "id": 21238,
        "title": "A Hybrid Solid State Transformer (HSST) Based on a Two Stage Isolated Solid State Transformer (SST): Control and Modulation",
        "authors": "Sanjay Rajendran, Wei Xu, Zhicheng Guo, Alex Q. Huang",
        "published": "2023-3-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43580.2023.10131531"
    },
    {
        "id": 21239,
        "title": "Optimized lightweight CA-transformer: Using transformer for fine-grained visual categorization",
        "authors": "Haiqing Wang, Shuqi Shang, Dongwei Wang, Xiaoning He, Kai Feng, Hao Zhu, Chengpeng Li, Yuetao Wang",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ecoinf.2022.101827"
    },
    {
        "id": 21240,
        "title": "nahs",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.13"
    },
    {
        "id": 21241,
        "title": "Impedances",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-6"
    },
    {
        "id": 21242,
        "title": "The Transformer",
        "authors": "Mohamed E. El-Hawary",
        "published": "2018-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351071635-4"
    },
    {
        "id": 21243,
        "title": "nis",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.12"
    },
    {
        "id": 21244,
        "title": "Insulation",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-5"
    },
    {
        "id": 21245,
        "title": "pesəkʷ",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.11"
    },
    {
        "id": 21246,
        "title": "Autotransformer Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-22"
    },
    {
        "id": 21247,
        "title": "“Goodnight Ladies”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0020"
    },
    {
        "id": 21248,
        "title": "Review for \"Towards next generation power grid transformer for renewables: Technology review\"",
        "authors": "",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12848/v1/review2"
    },
    {
        "id": 21249,
        "title": "Transformer and graph transformer-based prediction of drug-target interactions",
        "authors": "Weizhong Lu, Meiling Qian, Yu Zhang, Junkai Liu, Hongjie Wu, Yaoyao Lu, Haiou Li, Qiming Fu, Jiyun Shen, Yongbiao Xiao",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "\nBackground:\nAs we all know, finding new pharmaceuticals requires a lot of time and money, which has compelled people to think about adopting more effective approaches to locate drugs. Researchers have made significant progress recently when it comes to using Deep Learning (DL) to create DTI..\n\n\nMethods:\nTherefore, we propose a deep learning model that applies Transformer to DTI prediction. The model uses a Transformer and Graph Transformer to extract the feature information of protein and compound molecules, respectively, and combines their respective representations to predict interactions.\n\n\nResults:\nWe used Human and C.elegans, the two benchmark datasets, evaluated the proposed method in different experimental settings and compared it with the latest DL model.\n\n\nConclusion:\nThe results show that the proposed model based on DL is an effective method for the classification and recognition of DTI prediction, and its performance on the two data sets is significantly better than other DL based methods.\n",
        "link": "http://dx.doi.org/10.2174/1574893618666230825121841"
    },
    {
        "id": 21250,
        "title": "Influence of Conductor Transposition on Transformer Winding RLC Parameters",
        "authors": "Ana Drandić, Bojan Trkulja, Željko Štih",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_19"
    },
    {
        "id": 21251,
        "title": "Research on Assembly Automation Technology of New Prefabricated Transformer Prefabricated Transformer",
        "authors": "Peifeng Pan, Zhiyi Yang, Longda Li, Xiaoying Zhu, Tao Wu",
        "published": "2020-10-1",
        "citations": 0,
        "abstract": "Abstract\nThe technical field of transformers discussed in the paper relates to the production of lead wires, especially a factory-made prefabricated transformer with a new lead wire; its transformer includes a transformer main body and a new transformer lead wire connected to the main body of the transformer. The transformer lead wire includes a transformer lead wire, and a copper-aluminium terminal, a heat shrinkable tube, a composite insulator, a grounding test ring, a lightning arrester lead assembly, and a C-clamp are arranged in order from one end to the other end. The design of the new type lead wire transformer assembly can completely realize the factory prefabrication of the transformer lead wire, the manufacturing process is simple, and the construction time on site can be greatly reduced and the construction efficiency can be improved.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1650/2/022012"
    },
    {
        "id": 21252,
        "title": "Comparison between piezoelectric transformer and electromagnetic transformer used in electronic circuits",
        "authors": "Wedian Hadi Abd Al Ameer, Mustafa A. Fadel Al-Qaisi, Ammar Al-Gizi",
        "published": "2020-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12928/telkomnika.v18i3.14334"
    },
    {
        "id": 21253,
        "title": "Current Transformer Infrastructure and Its Application to Power System Protection",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_4"
    },
    {
        "id": 21254,
        "title": "Localizing in-domain adaptation of transformer-based biomedical language models",
        "authors": "Tommaso Mario Buonocore, Claudio Crema, Alberto Redolfi, Riccardo Bellazzi, Enea Parimbelli",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbi.2023.104431"
    },
    {
        "id": 21255,
        "title": "Intervention Prediction in MOOCs Based on Learners’ Comments: A Temporal Multi-input Approach Using Deep Learning and Transformer Models",
        "authors": "Laila Alrajhi, Ahmed Alamri, Alexandra I. Cristea",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09680-8_22"
    },
    {
        "id": 21256,
        "title": "A Comparison of Transformer-Based Language Models on NLP Benchmarks",
        "authors": "Candida Maria Greco, Andrea Tagarelli, Ester Zumpano",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-08473-7_45"
    },
    {
        "id": 21257,
        "title": "Classification of Cleft Lip and Palate Speech Using Fine-Tuned Transformer Pretrained Models",
        "authors": "Susmita Bhattacharjee, H. S. Shekhawat, S. R. M. Prasanna",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-53827-8_6"
    },
    {
        "id": 21258,
        "title": "YOLO-T: Multitarget Intelligent Recognition Method for X-ray Images Based on the YOLO and Transformer Models",
        "authors": "Mingxun Wang, Baolu Yang, Xin Wang, Cheng Yang, Jie Xu, Baozhong Mu, Kai Xiong, Yanyi Li",
        "published": "2022-11-21",
        "citations": 5,
        "abstract": "X-ray security inspection processes have a low degree of automation, long detection times, and are subject to misjudgment due to occlusion. To address these problems, this paper proposes a multi-objective intelligent recognition method for X-ray images based on the YOLO deep learning network and an optimized transformer structure (YOLO-T). We also construct the GDXray-Expanded X-ray detection dataset, which contains multiple types of dangerous goods. Using this dataset, we evaluated several versions of the YOLO deep learning network model and compared the results to those of the proposed YOLO-T model. The proposed YOLO-T method demonstrated higher accuracy for multitarget and hidden-target detection tasks. On the GDXray-Expanded dataset, the maximum mAP of the proposed YOLO-T model was 97.73%, which is 7.66%, 16.47%, and 7.11% higher than that obtained by the YOLO v2, YOLO v3, and YOLO v4 models, respectively. Thus, we believe that the proposed YOLO-T network has good application prospects in X-ray security inspection technologies. In all kinds of security detection scenarios using X-ray security detectors, the model proposed in this paper can quickly and accurately identify dangerous goods, which has broad application value.",
        "link": "http://dx.doi.org/10.3390/app122211848"
    },
    {
        "id": 21259,
        "title": "Fine-Tuning Multimodal Transformer Models for Generating Actions in Virtual and Real Environments",
        "authors": "Aleksei Staroverov, Andrey S. Gorodetsky, Andrei S. Krishtopik, Uliana A. Izmesteva, Dmitry A. Yudin, Alexey K. Kovalev, Aleksandr I. Panov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3334791"
    },
    {
        "id": 21260,
        "title": "Role of Punctuation in Semantic Mapping Between Brain and Transformer Models",
        "authors": "Zenon Lamprou, Frank Pollick, Yashar Moshfeghi",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-25891-6_35"
    },
    {
        "id": 21261,
        "title": "Revisiting the Efficiency-Accuracy Tradeoff in Adapting Transformer Models via Adversarial Fine-Tuning",
        "authors": "Minjia Zhang, Niranjan Uma Naresh, Yuxiong He",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "Adversarial fine-tuning (i.e., training on adversarial perturbed inputs) has demonstrated promising results in improving the accuracy of natural language understanding tasks. However, the improved accuracy does not come for free but is accompanied by a significantly prolonged training time, limiting their applicability to larger and more complex models. This work revisits the efficiency-accuracy trade-off in adversarial fine-tuning by systematically analyzing if adversarial fine-tuning methods, in conjunction with several efficiency optimizations, are suitable for adapting pre-trained Transformer models for natural language understanding tasks. Our results show that multiple design choices are crucial in determining the efficiency-accuracy trade-off, and we introduce a method, ScaLA, that achieves better accuracy-vs-speed trade-off than prior methods. We show in experiments that our proposed method attains up to 14.7× adaptation speedups on BERT, RoBERTa, and T5, while achieving comparable accuracy to existing methods.",
        "link": "http://dx.doi.org/10.3233/faia230619"
    },
    {
        "id": 21262,
        "title": "PolypDEQ: Towards Effective Transformer-Based Deep Equilibrium Models for Colon Polyp Segmentation",
        "authors": "Nguyen Minh Chau, Le Truong Giang, Dinh Viet Sang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-20713-6_35"
    },
    {
        "id": 21263,
        "title": "GTGMM: geometry transformer and Gaussian Mixture Models for robust point cloud registration",
        "authors": "Haibo Zhang, Linqi Hai, Haoran Sun, Xu Wang, Ruoxue Li, Guohua Geng, Mingquan Zhou",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18660-8"
    },
    {
        "id": 21264,
        "title": "Comparing and Improving Active Learning Uncertainty Measures for Transformer Models",
        "authors": "Julius Gonsior, Christian Falkenberg, Silvio Magino, Anja Reusch, Claudio Hartmann, Maik Thiele, Wolfgang Lehner",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-42914-9_9"
    },
    {
        "id": 21265,
        "title": "Modal Interaction-Enhanced Prompt Learning by Transformer Decoder for Vision-Language Models",
        "authors": "Mingyue Liu, Honggang Zhao, Longfei Ma, Xiang Li, Yucheng Ji, Mingyong Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40292-0_14"
    },
    {
        "id": 21266,
        "title": "A comparative study of YOLO models and a transformer-based YOLOv5 model for mass detection in mammograms",
        "authors": "DAMLA COŞKUN, DERVİŞ KARABOĞA, ALPER BAŞTÜRK, BAHRİYE AKAY, ÖZKAN UFUK NALBANTOĞLU, SERAP DOĞAN, İSHAK PAÇAL, MERYEM ALTIN KARAGÖZ",
        "published": "2023-11-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55730/1300-0632.4048"
    },
    {
        "id": 21267,
        "title": "Mitigating the Position Bias of Transformer Models in Passage Re-ranking",
        "authors": "Sebastian Hofstätter, Aldo Lipani, Sophia Althammer, Markus Zlabinger, Allan Hanbury",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-72113-8_16"
    },
    {
        "id": 21268,
        "title": "An Effective Med-VQA Method Using a Transformer with Weights Fusion of Multiple Fine-Tuned Models",
        "authors": "Suheer Al-Hadhrami, Mohamed El Bachir Menai, Saad Al-Ahmadi, Ahmad Alnafessah",
        "published": "2023-8-28",
        "citations": 1,
        "abstract": "Visual question answering (VQA) is a task that generates or predicts an answer to a question in human language about visual images. VQA is an active field combining two AI branches: NLP and computer vision. VQA in the medical field is still at an early stage, and it needs vast efforts and exploration to reach practical usage. This paper proposes two models that are utilized in the latest vision and NLP transformers that outperform the SOTA and have not yet been utilized in medical VQA. The ELECTRA-base transformer is used for textual feature extraction, whereas SWIN is used for visual feature extraction. In the SOTA medical VQA, selecting the model is based on the model that achieves the highest validation accuracy or the last model in training. The first proposed model, the best-value-based model, is selected based on the highest validation accuracy. The second model, the greedy-soup-based model, uses a greedy soup technique based on the fusion of multiple fine-tuned models to set the model parameters. The greedy soup selects the model parameters by fusing the model parameters that have significant performance on the validation accuracy in training. The greedy-soup-based model outperforms the best-value-based model, and both proposed models outperform the SOTA, which has an accuracy of 83.49%. The greedy-soup-based model is optimized with batch size and learning rate. During the optimization, seven extra models exceed the SOTA accuracy. The best model trained with a learning rate of 1.0×10−4 and batch size 16 achieves an accuracy of 87.41%.",
        "link": "http://dx.doi.org/10.3390/app13179735"
    },
    {
        "id": 21269,
        "title": "Combination of Existing Transformer and Newly Placed Transformer for Optimal Electric Power Distribution System",
        "authors": "Hadi Sutanto Saragi, Iswanti Sihaloho, Wesly Siagian, Reynold Pardede, Santi Agustina Manalu",
        "published": "2022-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismode56940.2022.10181002"
    },
    {
        "id": 21270,
        "title": "Depth Map Estimation of Focus Objects Using Vision Transformer",
        "authors": "Chae-rim Park, Kwang-il Lee, Seok-je Cho",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011908700003612"
    },
    {
        "id": 21271,
        "title": "Mal-operation Phenomenon of Converter Transformer and Lead Wire Differential Protection during the Energization of \"12-Pulse\" Converter Transformer Group",
        "authors": "Yi Wan, Hanli Weng, Sheng Wang",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igbsg.2019.8886305"
    },
    {
        "id": 21272,
        "title": "Malware Family Classification Based on Vision Transformer",
        "authors": "Jing Li Jing Li, Xueping Luo Jing Li",
        "published": "2023-2",
        "citations": 0,
        "abstract": "\n                        <p>Cybersecurity worries intensify as Big Data, the Internet of Things, and 5G technologies develop. Based on code reuse technologies, malware creators are producing new malware quickly, and new malware is continually endangering the effectiveness of existing detection methods. We propose a vision transformer-based approach for malware picture identification because, in contrast to CNN, Transformer’s self-attentive process is not constrained by local interactions and can simultaneously compute long-range mine relationships. We use ViT-B/16 weights pre-trained on the ImageNet21k dataset to improve model generalization capability and fine-tune them for the malware image classification task. This work demonstrates that (i) a pure attention mechanism applies to malware recognition, and (ii) the Transformer can be used instead of traditional CNN for malware image recognition. We train and assess our models using the MalImg dataset and the BIG2015 dataset in this paper. Our experimental evaluation found that the recognition accuracy of transfer learning-based ViT for MalImg samples and BIG2015 samples is 99.14% and 98.22%, respectively. This study shows that training ViT models using transfer learning can perform better than CNN in malware family classification.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023023401007"
    },
    {
        "id": 21273,
        "title": "Vision Transformer Interpretability via Prediction of Image Reflected Relevance Among Tokens",
        "authors": "Kento Sago, Kazuhiro Hotta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012419200003654"
    },
    {
        "id": 21274,
        "title": "A Nested Vision Transformer in Vision Transformer Framework for Intracranial Hemorrhage",
        "authors": "Gangdong Zhang, Yonghao Shi, Qian Zhang, Haohuai Lin, Liuqing Yang, Ping Lan",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/auteee60196.2023.10407273"
    },
    {
        "id": 21275,
        "title": "Insulation Design on High-Frequency Transformer for Solid-State Transformer",
        "authors": "Zheqing Li, Yi-Hsun Hsieh, Qiang Li, Fred C. Lee, Chunyang Zhao",
        "published": "2021-10-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce47101.2021.9595657"
    },
    {
        "id": 21276,
        "title": "Electronic-Embedded Transformer Based DC Transformer With Trapezoidal Current and Natural Current Sharing",
        "authors": "Yuliang Cao, Dong Dong",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2023.3295130"
    },
    {
        "id": 21277,
        "title": "Impact of Transitory Excessive Fluxing Condition on Power Transformer Protection",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_5"
    },
    {
        "id": 21278,
        "title": "A Study of Developing a Job Recommendation Model Using Transformer Models",
        "authors": "Jung Woo Jeon, Ji Ho Song, Tae Ho Ahn,  ",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "With the rapid technological advancements brought about by the Fourth Industrial Revolution, the industrial ecosystem is undergoing changes with new professions emerging and existing ones disappearing at an unprecedented rate. This societal shift is leading to an employment market dominated by professionals with experience, with an increasing number of individuals working in roles unrelated to their major. Additionally, the discrepancy between one's major and the job requirements demanded by industries is leading to a rising number of young job seekers abandoning their job search. In light of this, the purpose of this study is to develop job recommendation models based on job seekers' resumes, evaluate the performance of these models, and propose ways to enhance the applicability of the recommended jobs. To achieve this, two experiments were conducted. The first experiment evaluated the performance of a job recommendation model that applied the Transformer architecture, using a training dataset composed of resumes with 52 features and set hyperparameters. The second experiment evaluated the performance of a deep learning model, which applied fine-tuning techniques on a training dataset constructed from 34 features after removing any feature with more than 30% missing values from the original 52 features used in the first experiment. The increase in the epoch value of the Transformer model has significant implications.\r\nThis research demonstrates that the optimization of training data and the application of fine-tuning in deep learning model design significantly impact the performance improvement of job recommendation models. Given the rate of increase in epoch values observed in this study, it is expected that applying high-quality resume data to the Transformer model for additional model training in the future will further enhance the performance of career-based job recommendation models for both students preparing for employment based on their major and for experienced job seekers.",
        "link": "http://dx.doi.org/10.38115/asgba.2024.21.1.23"
    },
    {
        "id": 21279,
        "title": "CardioBERTpt: Transformer-based Models for Cardiology Language Representation in Portuguese",
        "authors": "Elisa Terumi Rubel Schneider, Yohan Bonescki Gumiel, João Vitor Andrioli de Souza, Lilian Mie Mukai, Lucas Emanuel Silva e Oliveira, Marina de Sa Rebelo, Marco Antonio Gutierrez, Jose Eduardo Krieger, Douglas Teodoro, Claudia Moro, Emerson Cabrera Paraiso",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cbms58004.2023.00247"
    },
    {
        "id": 21280,
        "title": "Transformers-sklearn: a toolkit for medical language understanding with transformer-based models",
        "authors": "Feihong Yang, Xuwen Wang, Hetong Ma, Jiao Li",
        "published": "2021-7",
        "citations": 15,
        "abstract": "Abstract\nBackground\nTransformer is an attention-based architecture proven the state-of-the-art model in natural language processing (NLP). To reduce the difficulty of beginning to use transformer-based models in medical language understanding and expand the capability of the scikit-learn toolkit in deep learning, we proposed an easy to learn Python toolkit named transformers-sklearn. By wrapping the interfaces of transformers in only three functions (i.e., fit, score, and predict), transformers-sklearn combines the advantages of the transformers and scikit-learn toolkits.\n\nMethods\nIn transformers-sklearn, three Python classes were implemented, namely, BERTologyClassifier for the classification task, BERTologyNERClassifier for the named entity recognition (NER) task, and BERTologyRegressor for the regression task. Each class contains three methods, i.e., fit for fine-tuning transformer-based models with the training dataset, score for evaluating the performance of the fine-tuned model, and predict for predicting the labels of the test dataset. transformers-sklearn is a user-friendly toolkit that (1) Is customizable via a few parameters (e.g., model_name_or_path and model_type), (2) Supports multilingual NLP tasks, and (3) Requires less coding. The input data format is automatically generated by transformers-sklearn with the annotated corpus. Newcomers only need to prepare the dataset. The model framework and training methods are predefined in transformers-sklearn.\n\nResults\nWe collected four open-source medical language datasets, including TrialClassification for Chinese medical trial text multi label classification, BC5CDR for English biomedical text name entity recognition, DiabetesNER for Chinese diabetes entity recognition and BIOSSES for English biomedical sentence similarity estimation.\nIn the four medical NLP tasks, the average code size of our script is 45 lines/task, which is one-sixth the size of transformers’ script. The experimental results show that transformers-sklearn based on pretrained BERT models achieved macro F1 scores of 0.8225, 0.8703 and 0.6908, respectively, on the TrialClassification, BC5CDR and DiabetesNER tasks and a Pearson correlation of 0.8260 on the BIOSSES task, which is consistent with the results of transformers.\n\nConclusions\nThe proposed toolkit could help newcomers address medical language understanding tasks using the scikit-learn coding style easily. The code and tutorials of transformers-sklearn are available at https://doi.org/10.5281/zenodo.4453803. In future, more medical language understanding tasks will be supported to improve the applications of transformers_sklearn.\n",
        "link": "http://dx.doi.org/10.1186/s12911-021-01459-0"
    },
    {
        "id": 21281,
        "title": "Fine-Tuning Transformer Models Using Transfer Learning for Multilingual Threatening Text Identification",
        "authors": "Muhammad Rehan, Muhammad Shahid Iqbal Malik, Mona Mamdouh Jamjoom",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3320062"
    },
    {
        "id": 21282,
        "title": "Query Focused Abstractive Summarization via Incorporating Query Relevance and Transfer Learning with Transformer Models",
        "authors": "Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Huang",
        "published": "2020",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-47358-7_35"
    },
    {
        "id": 21283,
        "title": "Aging of transformer insulation of experimental transformers and laboratory models with different moisture contents: Part II — moisture distribution and aging kinetics",
        "authors": "Valentina Vasovic, Jelena Lukic, Draginja Mihajlovic, Branko Pejovic, Milovan Milovanovic, Uros Radoman, Zoran Radakovic",
        "published": "2019-12",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2019.008184"
    },
    {
        "id": 21284,
        "title": "Appropriate Modelling of Transformer High Current Leads in 3D FEM",
        "authors": "Karlo Petrović, Bruno Jurišić, Tomislav Župan",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_20"
    },
    {
        "id": 21285,
        "title": "Calculation of Eddy Current Losses in Iron Core of Transformer",
        "authors": "Stjepan Frljić, Bojan Trkulja, Željko Štih",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_21"
    },
    {
        "id": 21286,
        "title": "Image Denoising Algorithm Based on Improved Transformer Network",
        "authors": "勇 陈",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2022.1212280"
    },
    {
        "id": 21287,
        "title": "A Survey on: Application of Transformer in Computer Vision",
        "authors": "Zhenghua Zhang, Zhangjie Gong, Qingqing Hong",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2021.006"
    },
    {
        "id": 21288,
        "title": "Studies of Electrical and Thermal Characteristics of Natural Ester Immersed Transformer Compared with Mineral Oil Immersed and Palm Oil Immersed Transformer",
        "authors": "S. Maneerot, K. Jariyanurat, P. Nimsanong, C. Bunlaksananusor, A. Kunakorn, N. Pattanadech",
        "published": "2018-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535807"
    },
    {
        "id": 21289,
        "title": "Improving the cooling performance of electrical distribution transformer using transformer oil – Based MEPCM suspension",
        "authors": "Mushtaq Ismael Hasan",
        "published": "2017-4",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jestch.2016.12.003"
    },
    {
        "id": 21290,
        "title": "Instrument Transformers",
        "authors": "Randy Mullikin",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-7"
    },
    {
        "id": 21291,
        "title": "Bag-of-Words Algorithms Can Supplement Transformer Sequence Classification &amp; Improve Model Interpretability",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7249/wra1719-1"
    },
    {
        "id": 21292,
        "title": "Transformer Protection",
        "authors": "Y. G. Paithankar",
        "published": "2017-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780203735176-9"
    },
    {
        "id": 21293,
        "title": "Extra Large Sequence Transformer Model for Chinese Word Segment",
        "authors": "Dezhou Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nChinese word segment is widely studied in document analysis.\nThe accuracy of the current popular word segment model, LSTM+CRF, is still not satisfactory.\nModels trained by the popular dataset often fails in the out-domain situation.\nIn this paper, combining the Transformer-XL layer, the Fully-Connect layer, and the Conditional Random Field layer, the proposed model improved 3.23% in the macro-F1 score, comparing to the BERT+CRF model, on the MSR2005 Chinese word segment test dataset.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-149338/v1"
    },
    {
        "id": 21294,
        "title": "The Single-Phase Transformer",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315219295-10"
    },
    {
        "id": 21295,
        "title": "AC Inductor Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-10"
    },
    {
        "id": 21296,
        "title": "High Gain Transformer Less Inverter using Buck Boost Converter",
        "authors": "Abhinav Vinod Deshpande",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this research paper, a high gain transformer less inverter modeling which is used for the smart grid technology or as the stand alone for home appliances is proposed. The proposed transformer less topology provides a higher voltage gain, low cost, small size and simple control stand alone transformer less inverter for home appliances. The above inverter is modeled by using software which is known as MATLAB/ Simulimk.",
        "link": "http://dx.doi.org/10.20944/preprints202104.0720.v1"
    },
    {
        "id": 21297,
        "title": "Improving Transformer Protection",
        "authors": "Wayne Hartmann",
        "published": "2019-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2019.8765860"
    },
    {
        "id": 21298,
        "title": "Front Matter",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.1"
    },
    {
        "id": 21299,
        "title": "Piezoelectric Transformer",
        "authors": "Kenji Uchino",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315219172-14"
    },
    {
        "id": 21300,
        "title": "Bibliographie",
        "authors": "",
        "published": "2017-1-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/eres.chabe.2017.01.0145"
    },
    {
        "id": 21301,
        "title": "- Transformer Protection",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-139"
    },
    {
        "id": 21302,
        "title": "A Molecular Transformer: A π-Conjugated Macrocycle as an Adaptable Host",
        "authors": "Junting Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/scimeetings.1c00081"
    },
    {
        "id": 21303,
        "title": "A Novel Transformer-Based Fully Trainable Point Process",
        "authors": "Hirotaka Hachiya, Fumiya Nishizawa",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342060"
    },
    {
        "id": 21304,
        "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
        "authors": "Fanfei Meng, David Demeter",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24638745.v1"
    },
    {
        "id": 21305,
        "title": "Review for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v2/review2"
    },
    {
        "id": 21306,
        "title": "Sequential Spatial Transformer Networks for Salient Object Classification",
        "authors": "David Dembinsky, Fatemeh Azimi, Federico Raue, Jörn Hees, Sebastian Palacio, Andreas Dengel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011667100003411"
    },
    {
        "id": 21307,
        "title": "Transformer Parameter Monitoring and Controlling",
        "authors": "Pooja M. Bansod, Dinesh V. Rojatkar,  ",
        "published": "2017-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd4632"
    },
    {
        "id": 21308,
        "title": "Transformer based 6DoF Pose Estimation for Visual SLAM",
        "authors": "Jae-Min Chae, Soo-Chahn Lee",
        "published": "2021-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieie.2021.58.12.49"
    },
    {
        "id": 21309,
        "title": "Development of a Text Classification Framework using Transformer-based Embeddings",
        "authors": "Sumona Yeasmin, Nazia Afrin, Kashfia Saif, Mohammad Huq",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011268000003269"
    },
    {
        "id": 21310,
        "title": "Convolution Neural Network and XGBoost-Based Fault Identification in Power Transformer",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_8"
    },
    {
        "id": 21311,
        "title": "Design and Implementation of a 150kW High Frequency Transformer in High Voltage Large Power Solid State Transformer Consist of Multi-DABs",
        "authors": "Jinping He, Kai Ji, Nianzhou Liu, Derong Lin",
        "published": "2019-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8922179"
    },
    {
        "id": 21312,
        "title": "Analysis of a Solid-State Transformer Employing Capacitively Isolated Series-Stacked Converter Cells and a Single Medium-Frequency Transformer",
        "authors": "D. Neuner, M. Hartmann, J. W. Kolar, J. Huber",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compel52896.2023.10221174"
    },
    {
        "id": 21313,
        "title": "Comparativa entre RESNET-50, VGG-16, Vision Transformer y Swin Transformer para el reconocimiento facial con oclusión de una mascarilla",
        "authors": "Brenda Xiomara Tafur Acenjo, Martin Alexis Tello Pariona, Edwin Jhonatan Escobedo Cárdenas",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "En la búsqueda de soluciones sin contacto físico en espacios cerrados para la verificación de identidad en el contexto de la pandemia por el SARS-CoV-2, el reconocimiento facial ha tomado relevancia. Uno de los retos en este ámbito es la oclusión por mascarilla, ya que oculta más del 50 % del rostro. La presente investigación evaluó cuatro modelos preentrenados por aprendizaje por transferencia: VGG-16, RESNET-50, Vision Transformer (ViT) y Swin Transformer, los cuales se entrenaron en sus capas superiores con un conjunto de datos propio. Para el entrenamiento sin mascarilla, se obtuvo un accuracy de 24 % (RESNET-50), 25 % (VGG-16), 96 % (ViT) y 91 % (Swin). En cambio, con mascarilla se obtuvo un accuracy de 32 % (RESNET-50), 53 % (VGG-16), 87 % (ViT) y 61 % (Swin). Estos porcentajes de testing accuracy indican que las arquitecturas más modernas como los transformers arrojan mejores resultados en el reconocimiento con mascarilla que las CNN (VGG-16 y RESNET-50). El aporte de la investigación recae en la experimentación con dos tipos de arquitecturas: CNN y transformers, así como en la creación del conjunto de datos público que se comparte a la comunidad científica. Este trabajo robustece el estado del arte de la visión computacional en el reconocimiento facial por oclusión de una mascarilla, ya que ilustra con experimentos la variación del accuracy con distintos escenarios y arquitecturas.",
        "link": "http://dx.doi.org/10.26439/interfases2023.n017.6361"
    },
    {
        "id": 21314,
        "title": "MS-Transformer: Masked and Sparse Transformer for Point Cloud Registration",
        "authors": "Qingyuan Jia, Guiyang Luo, Quan Yuan, Jinglin Li, Congzhang Shao, Ziyue Chen",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394424"
    },
    {
        "id": 21315,
        "title": "H3D-Transformer: A Heterogeneous 3D (H3D) Computing Platform for Transformer Model Acceleration on Edge Devices",
        "authors": "Yandong Luo, Shimeng Yu",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "Prior hardware accelerator designs primarily focused on single-chip solutions for 10MB-class computer vision models. The GB-class transformer models for natural language processing (NLP) impose challenges on existing accelerator design due to the massive number of parameters and the diverse matrix multiplication (MatMul) workloads involved. This work proposes a heterogeneous 3D-based accelerator design for transformer models, which adopts an interposer substrate with multiple 3D memory/logic hybrid cubes optimized for accelerating different MatMul workloads. An approximate computing scheme is proposed to take advantage of heterogeneous computing paradigms of mixed-signal compute-in-memory (CIM) and digital tensor processing units (TPU). From the system-level evaluation results, 10 TOPS/W energy efficiency is achieved for the Bert and GPT2 model, which is about 2.6 × ∼ 3.1 × higher than the baseline with 7nm TPU and stacked FeFET memory.",
        "link": "http://dx.doi.org/10.1145/3649219"
    },
    {
        "id": 21316,
        "title": "Smart transformer/large flexible transformer",
        "authors": "Rongwu Zhu, Markus Andresen, Marius Langwasser, Marco Liserre, Joao Pecas Lopes, Carlos Moreira, Justino Rodrigues, Mario Couto",
        "published": "2020-12",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.30941/cestems.2020.00033"
    },
    {
        "id": 21317,
        "title": "A Novel Renal Parenchyma Segmentation Network Based on Transformer",
        "authors": "容祥 张",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/airr.2022.112014"
    },
    {
        "id": 21318,
        "title": "Assessing DP value of a power transformer considering thermal ageing and paper moisture",
        "authors": "Ricardo David Medina Velecela, Andres Arturo Romero Quete, Enrique Esteban Mombello, Giuseppe Ratta, Diego Xavier Morales Jadan",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch4"
    },
    {
        "id": 21319,
        "title": "Classification of Alzheimer's Disease via Vision Transformer",
        "authors": "Yanjun Lyu, Xiaowei Yu, Dajiang Zhu, Lu Zhang",
        "published": "2022-6-29",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529190.3534754"
    },
    {
        "id": 21320,
        "title": "Industrial Process Fault Diagnosis Based on Two Stream Swinc Transformer",
        "authors": "磊 徐",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/mos.2023.122074"
    },
    {
        "id": 21321,
        "title": "Transformer l'agriculture",
        "authors": "Christophe Rymarski",
        "published": "2017-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/sh.288.0032"
    },
    {
        "id": 21322,
        "title": "Bibliographie",
        "authors": "",
        "published": "2019-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.favre.2019.01.0345"
    },
    {
        "id": 21323,
        "title": "- Transformer Connections",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-49"
    },
    {
        "id": 21324,
        "title": "Input Filter Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-15"
    },
    {
        "id": 21325,
        "title": "Mechanical Design",
        "authors": "",
        "published": "2017-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-11"
    },
    {
        "id": 21326,
        "title": "Job runtime prediction of HPC cluster basedon PC-Transformer",
        "authors": "fengxian chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nJob scheduling of high performance cluster has been generally achieved using the job scheduling system. The efficiency of job scheduling can be significantly increased by filling accurate job runtime. To increase the accuracy of prediction, it is necessary to use job category information that is acquired by clustering job logs. After the data are sorted out, Transformer with plain connection in accordance with attention mechanism is adopted to predict job runtime. For data processing, 6-dimensional features are selected from the historical log datasets, inaccordance with the correlation with job runtime, the integrity of the features and the validity of data. The datasets are divided into multiple job sets in accordance with the length of the job runtime, each job set will be trained and predicted respectively. As revealed by the results, the proposed method exhibits better prediction performance and achieves, an average accuracy of 0.892 on the HPC2N, with 15.2% MAPE. Furthermore, the proposed time embedding method shows obvious advantages in training time and prediction performance, suggesting that the proposed model can be employed in the actual scheduling environment.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2567464/v1"
    },
    {
        "id": 21327,
        "title": "Multi-Modal Instruction based Reinforcement Learning using MoME Transformer",
        "authors": "Avish Kadakia, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> In this research, we present a model architecture for employing reinforcement learning with transformers for multimodal tasks. Using transformers allows us to take advantage of the transformer architecture’s simplicity and scalability, as well as developments in language and vision modelling such as ViT, GPT-x, and BERT. Specifically we have trained the model to recognize various digits from the MNIST dataset from their associated word labels and instructions provided. The approach is similar to how an infant would learn to associate pictorial representation of digits to their corresponding words. We have used a MoME transformer in conjunction with Deep Q Learning to train our model. The image inputs have been embedded using pre-trained ResNet18 and the instructions have been embedded using GLoVe before passing them to the model for prediction and training. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19566040.v1"
    },
    {
        "id": 21328,
        "title": "Medical BigBERTa: An Optimized Transformer for Long Medical Document",
        "authors": "Chérubin Mugisha, Incheon Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Specific-domain language models (LMs) are ubiquitous in natural language processing. Effective biomedical text mining requires in-domain models of biomedical and clinical documents, which involve particular challenges. Most existing biomedical LMs rely on self-attention mechanisms, which do not scale to clinical-document length because of space complexity and quadratic execution times. In addition, in-domain LMs ignore the importance of a dedicated tokenizer and of learning optimal hyperparameter values for downstream tasks. In this paper, we introduce a model that addresses these three main issues. Inspired by the BigBird architecture model, we extend the embedding size through an attention mechanism that scales linearly with the sequence length to enable the processing of longer sequences, by a factor of up to eight. Our model was trained on biomedical and raw clinical corpora, and the same data were used to build a domain-specific sentence-piece tokenizer. In our experiments comparing our tokenizer with those in existing models, it was found to have the lowest fertility rate. To optimize the model performance, additional data-oriented hyperparameter fine-tuning was performed to enhance the accuracy and reproducibility of the model. Using the BLURB benchmarks, we evaluated our model with respect to named-entity recognition, relation extraction, sentence similarity, and question-answering tasks. Our model demonstrated competitive results, outperforming other strong self-attention baseline models for several datasets.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21673598.v2"
    },
    {
        "id": 21329,
        "title": "Review for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v1/review1"
    },
    {
        "id": 21330,
        "title": "Comprendre/Transformer",
        "authors": "",
        "published": "2020-5-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3726/b16767"
    },
    {
        "id": 21331,
        "title": "Transformer-based Denoising Adversarial Variational Entity Resolution",
        "authors": "Shuaichao Li, Huaiguang Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nEntity resolution (ER), precisely identifying different representations of the same real-world entities, is critical for data integration. The ER question has been studied for many years, and many methods have been proposed to solve it. Although deep learning has achieved good performance in ER tasks, there are some challenges regarding manual labelling and model transfer. This paper proposes a novel ER model, Transformer-based Denoising Adversarial Variational Entity Resolution (TdavER). For entity embedding, we develop an unsupervised entity embedding model based on denoising autoencoders and pre-trained language models, which takes corrupted input as training data to motivate the encoder to generate rather stable and robust high-quality entity representations. Furthermore, we propose an unsupervised entity feature transformation model based on adversarial variational autoencoders to remove the constraints on entity representations from training data. This transformation model converts low-level entity embeddings to high-level probability distributions, which are not constrained by the source data and contain deep similarity features. To better implement the feature transformation, we adopt adversarial networks to optimize the variational autoencoder's training process and help it learn the correct posterior distribution. Extensive experiments confirm that the performance of our proposed TdavER is comparable with the current state-of-the-art ER methods and that its entity feature transformation model is transferable.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2160930/v1"
    },
    {
        "id": 21332,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v1"
    },
    {
        "id": 21333,
        "title": "palenəskʷ | nαn",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.15"
    },
    {
        "id": 21334,
        "title": "Back Matter",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.27"
    },
    {
        "id": 21335,
        "title": "Convolutional Vision Transformer for Handwritten Digit Recognition",
        "authors": "Vanita Agrawal, Jayant Jagtap",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nHandwritten digit recognition is an essential step in understanding handwritten documents. The state-of-the-art convolutional neural networks (CNN) methods are mostly preferred for recognizing handwritten digits. Though the recognition accuracy is high, CNN filter weights don’t change even after training. Hence the process cannot adjust dynamically to changes in input. Recently the interest of researchers of computer vision has been on Vision Transformers (ViTs) and Multilayer Perceptrons (MLPs). The use of transformer architecture enabled substantial parallelization and translation quality improvement. The inadequacies of CNNs sparked a hybrid model revolution, which combines the best of both disciplines. This paper is written to view the impact of the hybrid model on handwritten digit recognition. The testing is done on the available benchmark datasets, the Extended Modified National institute of standards and technology (EMNIST) digits dataset, and the most significant historical handwritten digit dataset (DIDA). The 10-fold cross-validation accuracy achieved on EMNIST and DIDA is 99.89% and 99.73%, respectively. The results show that the proposed method achieves the highest accuracy compared to existing methodologies. The proposed method is robust, feasible, and effective on clean and uncleaned images.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1984839/v1"
    },
    {
        "id": 21336,
        "title": "Front Matter",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-1"
    },
    {
        "id": 21337,
        "title": "A Concept Smart Switch for Single-Phase Transformer and Reactor Control",
        "authors": "Michael Everton",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p><b>Introduced\nto electrical power networks at the turn of the century, the use of power\nelectronic devices to control and regulate single-phase networks has trailed\nbehind. A likely reason for this outcome is the relative cost of 'smart\ndevices' in a system of low earned revenue.</b></p>\n\n<p><b>Now a feature of grid modernisation projects, interest\nin 'smart devices' as a means to extend the useful life of distribution assets,\ndelay capital expenditure, lower operating costs and to improve the supply\nreliability, is growing. </b></p>\n\n<p><b>Described in this paper for the control of\nsingle-phase transformers and reactors is a concept 'smart switch' that uses a\nlow voltage low power thyristor. Built with a novel magnetic core and winding\narrangement, the disconnection and reconnection of a transformer or reactor is\ncontrolled by the semiconductor switch. The concept design is demonstrated in\nthis paper for a thermal overload and switched shunt reactor transformer\napplications. </b></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12815561.v1"
    },
    {
        "id": 21338,
        "title": "Capacitance Calculations",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-13"
    },
    {
        "id": 21339,
        "title": "Path-Augmented Graph Transformer Network",
        "authors": "Benson Chen, Regina Barzilay, Tommi S Jaakkola",
        "published": "No Date",
        "citations": 2,
        "abstract": "Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graphstructured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistentlyoutperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP)2.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8214422.v1"
    },
    {
        "id": 21340,
        "title": "TransPPG: Two-stream Transformer for Remote Heart Rate Estimate",
        "authors": " Ruiqi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Non-contact facial video-based heart rate estima- tion using remote photoplethysmography (rPPG) has shown great potential in many applications (e.g., remote health care) and achieved creditable results in constrained scenarios. However, practi- cal applications require results to be accurate even under complex environment with head movement and unstable illumination. Therefore, improving the performance of rPPG in complex environment has become a key challenge. In this paper, we pro- pose a novel video embedding method that embedseach facial video sequence into a feature map re- ferred to as Multi-scale Adaptive Spatial and Tem- poral Map with Overlap (MAST Mop), which con- tains not only vital information but also surround- ing information as reference, which acts as the mir- ror to figure out the homogeneous perturbations imposed on foreground and background simulta- neously, such as illumination instability. Corre- spondingly, we propose a two-stream Transformer model to map the MAST Mop into heart rate (HR), where one stream follows the pulse signal in the facial area while the other figures out the perturba- tion signal from the surrounding region such that the difference of the two channels leads to adap- tive noise cancellation. Our approach significantly outperforms all current state-of-the-art methods on two public datasets MAHNOB-HCI and VIPL-HR. As far as we know, it is the first work with Trans- former as backbone to capture the temporal depen- dencies in rPPGs and apply the two stream scheme to figure out the interference from backgrounds as mirror of the corresponding perturbation on fore- ground signals for noise tolerating.",
        "link": "http://dx.doi.org/10.31219/osf.io/z7bjc"
    },
    {
        "id": 21341,
        "title": "Transformer-CNN Hybrid Network for Improving PET Time Of Flight Prediction",
        "authors": "Xuhui Feng",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In Positron Emission Tomography (PET) reconstruction, utilizing Time of Flight (TOF) information can significantly enhance the signal-to-noise (SNR) ratio, posing a greater challenge for the precision of TOF. To address this, we employed two distinct waveform datasets for training our developed network. One dataset comprises simulated waveform data obtained through a comprehensive simulation process established using Geant4 and GosSip. The other dataset consists of real waveform data collected from lutetium yttrium orthosilicate (LYSO) scintillators and silicon photomultiplier (SiPM) detectors placed at various positions.  Our network, a combination of Transformer and Convolutional Neural Network (CNN), was developed for predicting the TOF of coincidence events based on waveform data from PET detectors. Our network achieved average full width at half maximum (FWHM) of 189 ps, with reductions of 82 ps and 13 ps compared to constant fraction discriminator (CFD) and CNN, across multiple positions.  Additionally, there was an average bias reduction of 10.3 ps compared to CNN. We visualized the attention map, revealing the remarkable enhancement of Transformer on the rising edge of waveforms. We also demonstrated the robustness of our proposed network by including waveforms with scattered events in the real training dataset.  Data augmentation through translation and flip was investigated and resulted in an improvement of 5 ps.  Furthermore, we analyzed the characteristic differences between real and simulated waveform data, providing valuable insights for generating more realistic simulated data in the future. Our network improved the average FWHM and bias, leading to enhanced SNR and clearer imaging. Data augmentation effectively expanded the dataset and facilitated the data collection process.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24188961"
    },
    {
        "id": 21342,
        "title": "Bibliographie",
        "authors": "",
        "published": "2022-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.autis.2022.02.0179"
    },
    {
        "id": 21343,
        "title": "Asynchronous Functional Brain Network Construction with  Spatiotemporal Transformer for MCI Classification",
        "authors": "Xiang Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Construction and analysis of functional brain networks (FBNs) with rs-fMRI is a promising method to diagnose functional brain diseases. Nevertheless, the existing methods suffer from several limitations. First, the functional connectivities (FCs) of the FBN are usually measured by the temporal co-activation level between rs-fMRI time series from regions of interest (ROIs). While enjoying simplicity, the existing approach implicitly assumes simultaneous co-activation of all the ROIs, and models only their synchronous dependencies. However, the functional coactivation is not necessarily always synchronous due to the time lag of information flow and cross-time interactions between ROIs. Therefore, it is desirable to model the asynchronous functional interactions. Second, the traditional methods usually construct FBNs at individual level for feature extraction and classification, leading to large variability and degraded diagnosis accuracy when modeling asynchronous FBN. Third, the FBN construction and analysis are conducted in two independent steps without joint alignment for the target diagnosis task. To address the first limitation, this paper proposes an effective sliding-window-based method to model spatiotemporal FCs in Transformer. Regarding the second limitation, we propose to learn common and individual FBNs adaptively with the common FBN as prior knowledge, thus alleviating the variability and enabling the network to focus on the individual disease-specific asynchronous FCs. To address the third limitation, the common and individual asynchronous FBNs are built and analyzed by an integrated network, enabling end-to-end training and improving the flexibility and discriminativity. The effectiveness of the proposed method is consistently demonstrated on three data sets for MCI diagnosis.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24634881.v1"
    },
    {
        "id": 21344,
        "title": "Science and Sound in Nineteenth-Century Britain",
        "authors": "Edward J. Gillin",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/b23409"
    },
    {
        "id": 21345,
        "title": "Review for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v2/review1"
    },
    {
        "id": 21346,
        "title": "AST: Audio Spectrogram Transformer",
        "authors": "Yuan Gong, Yu-An Chung, James Glass",
        "published": "2021-8-30",
        "citations": 210,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-698"
    },
    {
        "id": 21347,
        "title": "Sound Levels",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-10"
    },
    {
        "id": 21348,
        "title": "Faultformer: Transformer-Based Prediction of Bearing Faults",
        "authors": "Anthony Zhou, Amir Barati Farimani",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4620618"
    },
    {
        "id": 21349,
        "title": "Fundamentals of Magnetics",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-1"
    },
    {
        "id": 21350,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v2"
    },
    {
        "id": 21351,
        "title": "Transformer-CNN Hybrid Network for Improving PET Time Of Flight Prediction",
        "authors": "Xuhui Feng",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In Positron Emission Tomography (PET) reconstruction, utilizing Time of Flight (TOF) information can significantly enhance the signal-to-noise (SNR) ratio, posing a greater challenge for the precision of TOF. To address this, we employed two distinct waveform datasets for training our developed network. One dataset comprises simulated waveform data obtained through a comprehensive simulation process established using Geant4 and GosSip. The other dataset consists of real waveform data collected from lutetium yttrium orthosilicate (LYSO) scintillators and silicon photomultiplier (SiPM) detectors placed at various positions.  Our network, a combination of Transformer and Convolutional Neural Network (CNN), was developed for predicting the TOF of coincidence events based on waveform data from PET detectors. Our network achieved average full width at half maximum (FWHM) of 189 ps, with reductions of 82 ps and 13 ps compared to constant fraction discriminator (CFD) and CNN, across multiple positions.  Additionally, there was an average bias reduction of 10.3 ps compared to CNN. We visualized the attention map, revealing the remarkable enhancement of Transformer on the rising edge of waveforms. We also demonstrated the robustness of our proposed network by including waveforms with scattered events in the real training dataset.  Data augmentation through translation and flip was investigated and resulted in an improvement of 5 ps.  Furthermore, we analyzed the characteristic differences between real and simulated waveform data, providing valuable insights for generating more realistic simulated data in the future. Our network improved the average FWHM and bias, leading to enhanced SNR and clearer imaging. Data augmentation effectively expanded the dataset and facilitated the data collection process.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24188961.v1"
    },
    {
        "id": 21352,
        "title": "Slavery and Utopia",
        "authors": "",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436"
    },
    {
        "id": 21353,
        "title": "Asynchronous Functional Brain Network Construction with  Spatiotemporal Transformer for MCI Classification",
        "authors": "Xiang Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Construction and analysis of functional brain networks (FBNs) with rs-fMRI is a promising method to diagnose functional brain diseases. Nevertheless, the existing methods suffer from several limitations. First, the functional connectivities (FCs) of the FBN are usually measured by the temporal co-activation level between rs-fMRI time series from regions of interest (ROIs). While enjoying simplicity, the existing approach implicitly assumes simultaneous co-activation of all the ROIs, and models only their synchronous dependencies. However, the functional coactivation is not necessarily always synchronous due to the time lag of information flow and cross-time interactions between ROIs. Therefore, it is desirable to model the asynchronous functional interactions. Second, the traditional methods usually construct FBNs at individual level for feature extraction and classification, leading to large variability and degraded diagnosis accuracy when modeling asynchronous FBN. Third, the FBN construction and analysis are conducted in two independent steps without joint alignment for the target diagnosis task. To address the first limitation, this paper proposes an effective sliding-window-based method to model spatiotemporal FCs in Transformer. Regarding the second limitation, we propose to learn common and individual FBNs adaptively with the common FBN as prior knowledge, thus alleviating the variability and enabling the network to focus on the individual disease-specific asynchronous FCs. To address the third limitation, the common and individual asynchronous FBNs are built and analyzed by an integrated network, enabling end-to-end training and improving the flexibility and discriminativity. The effectiveness of the proposed method is consistently demonstrated on three data sets for MCI diagnosis.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24634881"
    },
    {
        "id": 21354,
        "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
        "authors": "Fanfei Meng, David Demeter",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24638745"
    },
    {
        "id": 21355,
        "title": "Medical BigBERTa: An Optimized Transformer for Long Medical Document",
        "authors": "Chérubin Mugisha, Incheon Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Specific-domain language models (LMs) are ubiquitous in natural language processing. Effective biomedical text mining requires in-domain models of biomedical and clinical documents, which involve particular challenges. Most existing biomedical LMs rely on self-attention mechanisms, which do not scale to clinical-document length because of space complexity and quadratic execution times. In addition, in-domain LMs ignore the importance of a dedicated tokenizer and of learning optimal hyperparameter values for downstream tasks. In this paper, we introduce a model that addresses these three main issues. Inspired by the BigBird architecture model, we extend the embedding size through an attention mechanism that scales linearly with the sequence length to enable the processing of longer sequences, by a factor of up to eight. Our model was trained on biomedical and raw clinical corpora, and the same data were used to build a domain-specific sentence-piece tokenizer. In our experiments comparing our tokenizer with those in existing models, it was found to have the lowest fertility rate. To optimize the model performance, additional data-oriented hyperparameter fine-tuning was performed to enhance the accuracy and reproducibility of the model. Using the BLURB benchmarks, we evaluated our model with respect to named-entity recognition, relation extraction, sentence similarity, and question-answering tasks. Our model demonstrated competitive results, outperforming other strong self-attention baseline models for several datasets.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21673598"
    },
    {
        "id": 21356,
        "title": "Bibliographie",
        "authors": "",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0187"
    },
    {
        "id": 21357,
        "title": "POLARIZATION TRANSFORMER WITH COMBINATION OF DIAPHRAGMS AND PINS",
        "authors": "Stepan Piltyay",
        "published": "No Date",
        "citations": 0,
        "abstract": "POLARIZATION TRANSFORMER WITH COMBINATION OF DIAPHRAGMS AND PINS",
        "link": "http://dx.doi.org/10.31219/osf.io/9t5mu"
    },
    {
        "id": 21358,
        "title": "Probabilistic Reasoning with Transformer Networks: An Application to Predicate Judgment",
        "authors": "Sudeep Bhatia",
        "published": "No Date",
        "citations": 0,
        "abstract": "Transformer networks that are trained on participant-generated feature norms have been shown to replicate several behavioral patterns observed in prior research on human semantic cognition. In this paper, I compare the predictions of these networks with participant evaluations of sentences sampled from a vast domain of discourse, with thousands of naturalistic predicates. I find that existing transformer networks make human-like predictions when given atomic sentences. However, they fail to capture participant evaluations of compound sentences with conjunctions, disjunctions, negations, and conditionals. Instead, participant responses are best captured by a Probabilistic Reasoner, which aggregates transformer judgments according to the rules of probability theory.  These results suggest that human semantic cognition may also rely on a combination of associative processes and deliberative processes when judging complex statements about the world. In doing so they show how probabilistic cognitive modeling and neural network modeling can be integrated to study naturalistic high-level cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/s3d74"
    },
    {
        "id": 21359,
        "title": "Load Loss",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-7"
    },
    {
        "id": 21360,
        "title": "Short-term Hebbian learning can implement transformer-like attention",
        "authors": "Ian T. Ellwood",
        "published": "No Date",
        "citations": 0,
        "abstract": "Transformers have revolutionized machine learning models of language and vision, but their connection with neuroscience remains tenuous. Built from attention layers, they require a mass comparison of queries and keys that is difficult to perform using traditional neural circuits. Here, we show that neurons can implement attention-like computations using short-term, Hebbian synaptic potentiation. We call our mechanism the match-and-control principle and it proposes that when activity in an axon is synchronous, or matched, with the somatic activity of a neuron that it synapses onto, the synapse can be briefly strongly potentiated, allowing the axon to take over, or control, the activity of the downstream neuron for a short time. In our scheme, the keys and queries are represented as spike trains and comparisons between the two are performed in individual spines allowing for hundreds of key comparisons per query and roughly as many keys and queries as there are neurons in the network.",
        "link": "http://dx.doi.org/10.1101/2023.05.31.543109"
    },
    {
        "id": 21361,
        "title": "Improvement of Vision Transformer Using Word Patches",
        "authors": "Ayato Takama, Sota Kato, Satoshi Kamiya, Kazuhiro Hotta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011732900003417"
    },
    {
        "id": 21362,
        "title": "Time Series Forecasting of Air Pollutant PM2.5 Using Transformer Architecture",
        "authors": "K. Azhahudurai, V. Veeramanikandan",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21275/sr231125192357"
    },
    {
        "id": 21363,
        "title": "TrajViViT: A Trajectory Video Vision Transformer Network for Trajectory Forecasting",
        "authors": "Gauthier Rotsart de Hertaing, Dani Manjah, Benoit Macq",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012372000003654"
    },
    {
        "id": 21364,
        "title": "HEMP Transformer Defense Through Power Electronics",
        "authors": "Klaehn Burkes,  , Vincent Cyssens",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1658841"
    },
    {
        "id": 21365,
        "title": "Multi-Modal Instruction based Reinforcement Learning using MoME Transformer",
        "authors": "Avish Kadakia, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> In this research, we present a model architecture for employing reinforcement learning with transformers for multimodal tasks. Using transformers allows us to take advantage of the transformer architecture’s simplicity and scalability, as well as developments in language and vision modelling such as ViT, GPT-x, and BERT. Specifically we have trained the model to recognize various digits from the MNIST dataset from their associated word labels and instructions provided. The approach is similar to how an infant would learn to associate pictorial representation of digits to their corresponding words. We have used a MoME transformer in conjunction with Deep Q Learning to train our model. The image inputs have been embedded using pre-trained ResNet18 and the instructions have been embedded using GLoVe before passing them to the model for prediction and training. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19566040"
    },
    {
        "id": 21366,
        "title": "Quiet Converter Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-18"
    },
    {
        "id": 21367,
        "title": "- Transformer Testing",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-50"
    },
    {
        "id": 21368,
        "title": "A Concept Smart Switch for Single-Phase Transformer and Reactor Control",
        "authors": "Michael Everton",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p><b>Introduced\nto electrical power networks at the turn of the century, the use of power\nelectronic devices to control and regulate single-phase networks has trailed\nbehind. A likely reason for this outcome is the relative cost of 'smart\ndevices' in a system of low earned revenue.</b></p>\n\n<p><b>Now a feature of grid modernisation projects, interest\nin 'smart devices' as a means to extend the useful life of distribution assets,\ndelay capital expenditure, lower operating costs and to improve the supply\nreliability, is growing. </b></p>\n\n<p><b>Described in this paper for the control of\nsingle-phase transformers and reactors is a concept 'smart switch' that uses a\nlow voltage low power thyristor. Built with a novel magnetic core and winding\narrangement, the disconnection and reconnection of a transformer or reactor is\ncontrolled by the semiconductor switch. The concept design is demonstrated in\nthis paper for a thermal overload and switched shunt reactor transformer\napplications. </b></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12815561"
    },
    {
        "id": 21369,
        "title": "Thermal Design",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-18"
    },
    {
        "id": 21370,
        "title": "Semantic segmentation algorithm based on transformer In Mobile Edge Computing",
        "authors": "XiBei Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "The semantic segmentation task is a basic task in the field of Mobile\nEdge Computing, which requires the classification of each pixel in the\nimage, which has higher requirements for classification accuracy than\nthe image classification task. Fine-grained classification tasks\nrequires more detailed information, in addition to classifying according\nto the semantic information and spatial information of each pixel unit\nand the surrounding pixels, it is also necessary to distinguish from\nadjacent pixels, which is one of the main difficulties of the current\nsegmentation task. However, high-resolution input images can bring more\ndetailed information, but they are often accompanied by expensive\ncomputing costs, so smaller resolution images will be put in practical\napplications to ensure computing speed. As another task of computer\nvision, super-resolution recovery focuses on extracting information from\nlow-resolution pictures and reasoning into higher-resolution feature\nmaps. Its recovered detail features contribute to the high-precision\nclassification of semantic segmentation tasks. Considering the\ncomplementarity of the two tasks, considering the use of transformer as\na feature extractor, the design algorithm realizes semantic segmentation\nand super-resolution recovery tasks at the same time, multi-task\nlearning can ensure that the backbone network obtains more common\nhigh-dimensional information, and then we use the results of\nsuper-resolution recovery branches to guide the semantic segmentation\ntask to provide more detailed information and finally obtain an\neffective improvement on the original baseline.",
        "link": "http://dx.doi.org/10.22541/au.167845518.89258745/v1"
    },
    {
        "id": 21371,
        "title": "EMP-Resilient Electric Grid Transformer Analysis",
        "authors": "Paul Clem, Ellie Wang, Joseph Kotulski",
        "published": "2020-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1684647"
    },
    {
        "id": 21372,
        "title": "Medical BigBERTa: An Optimized Transformer for Long Medical Document",
        "authors": "Chérubin Mugisha, Incheon Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>For this research, we propose a biomedical language model trained on biomedical publicly available datasets from Kaggle, Pubmed abstract baseline 2019, and MIMIC III.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21673598.v1"
    },
    {
        "id": 21373,
        "title": "“Satellite of Love”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0016"
    },
    {
        "id": 21374,
        "title": "“I’m So Free”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0019"
    },
    {
        "id": 21375,
        "title": "Review for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v1/review2"
    },
    {
        "id": 21376,
        "title": "Synthetic Ester Impact on Power Transformer Design, Manufacturing and Testing",
        "authors": "Dario Šegović, Ana Orešković, Žarko Janić",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_5"
    },
    {
        "id": 21377,
        "title": "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning",
        "authors": "Kashu Yamazaki, Khoa Vo, Quang Sang Truong, Bhiksha Raj, Ngan Le",
        "published": "2023-6-26",
        "citations": 4,
        "abstract": "Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. \nFollowing the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github.com/UARK-AICV/VLTinT.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i3.25412"
    },
    {
        "id": 21378,
        "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
        "authors": "Zeyu Yun, Yubei Chen, Bruno Olshausen, Yann LeCun",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.deelio-1.1"
    },
    {
        "id": 21379,
        "title": "Rectifier Transformers",
        "authors": "Sheldon P. Kennedy",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-5"
    },
    {
        "id": 21380,
        "title": "BibliographIe",
        "authors": "",
        "published": "2019-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.mado.2019.01.0193"
    },
    {
        "id": 21381,
        "title": "Structural Design",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-10"
    },
    {
        "id": 21382,
        "title": "Series Saturable Reactor Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-24"
    },
    {
        "id": 21383,
        "title": "Vision Transformer based System for Fruit Quality Evaluation",
        "authors": "Tanushri Kumar, Shivani R",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nPurpose Fruit quality assessment is one of the most pressing issues in the farming industry. Agriculturists would benefit significantly from the capacity to recognize the freshness of fruits, as it will allow them to optimize the harvesting stage and avoid reaping either underdeveloped or overdeveloped natural products. Productivity has decreased due to a lack of low-cost technology and equitable access. Despite large-scale agricultural mechanization in some parts of the country, most agricultural operations are still carried out by hand with simple instruments. The goal of this research is to automate the task of evaluating fruit quality. Methods Transformers were first presented in the field of natural language processing, and they offer dramatic performance improvements over existing models in NLP like as LSTMs and GRU. The Vision Transformer, or ViT, is an image classification model that uses a Transformer-like design over image patches. Results Vision transformer exhibited far superior results compared to CNN models. The models were evaluated on various metrics and the vision transformer model generated the highest accuracy compared to convolutional neural network models such as InceptionNet and EfficientNet. Conclusions Vision Transformers exhibited superior performance in fruit quality evaluation compared to traditional CNN model approaches. For future works, this model can be used to develop an efficient quality control system Automation of the quality prediction process can greatly reduce food and agricultural produce wastage.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1526586/v1"
    },
    {
        "id": 21384,
        "title": "Magnetic Characteristics",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-2"
    },
    {
        "id": 21385,
        "title": "Novel transformer networks for improved sequence labeling in genomics",
        "authors": "Jim Clauwaert, Willem Waegeman",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractIn genomics, a wide range of machine learning methodologies have been investigated to annotate biological sequences for positions of interest such as transcription start sites, translation initiation sites, methylation sites, splice sites and promoter start sites. In recent years, this area has been dominated by convolutional neural networks, which typically outperform previously-designed methods as a result of automated scanning for influential sequence motifs. However, those architectures do not allow for the efficient processing of the full genomic sequence. As an improvement, we introduce transformer architectures for whole genome sequence labeling tasks. We show that these architectures, recently introduced for natural language processing, are better suited for processing and annotating long DNA sequences. We apply existing networks and introduce an optimized method for the calculation of attention from input nucleotides. To demonstrate this, we evaluate our architecture on several sequence labeling tasks, and find it to achieve state-of-the-art performances when comparing it to specialized models for the annotation of transcription start sites, translation initiation sites and 4mC methylation inE. coli.",
        "link": "http://dx.doi.org/10.1101/836163"
    },
    {
        "id": 21386,
        "title": "Transformer-based Convolutional Forgetting Knowledge Tracking",
        "authors": "Tieyuan Liu, Meng Zhang, Liang Chang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nKnowledge tracking is essential for online education. It can analyze a user's learning history to obtain their current knowledge mastery level and provide suggestions for their subsequent learning. When compared with the current field of knowledge tracking, some scholars found that the Transformer model is useful in time series prediction. However, through our research, we discovered that due to the context-awareness insensitivity of the Transformer dot product operation, as well as real data concentration, a lot of learning methods have the problem of training a large number of repeated knowledge points in continuous time, which leads to the ignorance of the correlation between different knowledge points. Therefore, we added a convolutional attention mechanism so that the model can better perceive contextual information. In comparison with the brain, memory is actively forgetting, passive, and people will unconsciously forget previous knowledge whilst they are learning. But as far as we know, most of the current research does not use forgetting as a factor to analyze students' status, so we will further analyze students' forgetting behavior and fuse it with the weight matrix generated by the model to further improve the accuracy of the model. Therefore, we propose a Transformer-based convolutional forgetting knowledge tracking (TCFKT). We verify the model’s effect on several real learning record data sets. The experimental results show that our model performs better than other knowledge tracking models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2076621/v1"
    },
    {
        "id": 21387,
        "title": "Peer Review #1 of \"Homologous point transformer for multi-modality prostate image registration (v0.1)\"",
        "authors": "",
        "published": "2022-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1155v0.1/reviews/1"
    },
    {
        "id": 21388,
        "title": "Sequential Component-Based Improvement in Percentage Biased Differential Protection of a Power Transformer",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4_9"
    },
    {
        "id": 21389,
        "title": "Through fault current effects on distribution transformer and prevention actions using backup protection : Case study of Kelapa Gading transformer",
        "authors": "Ira Mardya Sari, Azzahraninna Tryollinna, Anna Dwita Paulus Sudin, Dahlia Deka Permata",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichveps.2017.8225951"
    },
    {
        "id": 21390,
        "title": "Unsupervised Domain Adaptation Image Classification Based on Swin Transformer",
        "authors": "博文 范",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/mos.2023.123281"
    },
    {
        "id": 21391,
        "title": "Detecting Anomalies in Textured Images Using Modified Transformer Masked Autoencoder",
        "authors": "Afshin Dini, Esa Rahtu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012416400003660"
    },
    {
        "id": 21392,
        "title": "Learning Side Channel Attack Model Based on Vision Transformer",
        "authors": "杨杰 廖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/aam.2023.124163"
    },
    {
        "id": 21393,
        "title": "Finite Element Study of Core Materials of a Planar Transformer and Comparison of its Electromagnetic Properties with a High Frequency Transformer",
        "authors": "Arya Venugopal, Femi Robert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25103/jestr.163.16"
    },
    {
        "id": 21394,
        "title": "Prediction Model for the Distribution Transformer Failure Using Correlation of Weather Data",
        "authors": "Eun Hui Ko, Tatjana Dokic, Mladen Kezunovic",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_12"
    },
    {
        "id": 21395,
        "title": "Transformer-Based Two-level Approach for Music-driven Dance Choreography",
        "authors": "Yanbo Cheng, Yingying Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012434500003660"
    },
    {
        "id": 21396,
        "title": "Diagnostics of Transformer Windings Condition by Probing Pulses of Microsecond Duration",
        "authors": "Vasily Ya. Ushakov, Alexey V. Mytnikov, Valeriy A. Lavrinovich, Alexey V. Lavrinovich",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-83198-1_3"
    },
    {
        "id": 21397,
        "title": "Comparing RNN and Transformer Context Representations in the Czech Answer Selection Task",
        "authors": "Marek Medved, Radoslav Sabol, Aleš Horák",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010827000003116"
    },
    {
        "id": 21398,
        "title": "Assessing Voltage Sags Caused by Transformer Energisation",
        "authors": "H. Abdull Halim, M. Hatta, M. Amirruddin",
        "published": "2019-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5373/jardcs/v11sp12/20193287"
    },
    {
        "id": 21399,
        "title": "Leveraging Transformer and Graph Neural Networks for Variable Misuse Detection",
        "authors": "Vitaly Romanov, Gcinizwe Dlamini, Aidar Valeev, Vladimir Ivanov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011997300003464"
    },
    {
        "id": 21400,
        "title": "Influence of PV Penetration on Distribution Transformer Aging",
        "authors": "Banu Uçar,  , Mustafa Bağrıyanık, Güven Kömürgöz",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/jocet.2017.5.2.357"
    },
    {
        "id": 21401,
        "title": "Calliope - A Polyphonic Music Transformer",
        "authors": "Andrea Valenti, Stefano Berti, Davide Bacciu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-63"
    },
    {
        "id": 21402,
        "title": "Table of Contents",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.2"
    },
    {
        "id": 21403,
        "title": "- Power Transformer Protection",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-52"
    },
    {
        "id": 21404,
        "title": "Power Transformer Acceptance Tests",
        "authors": "Ryszard Malewski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-9"
    },
    {
        "id": 21405,
        "title": "WormSwin: Instance segmentation of C. elegans using vision transformer",
        "authors": "Maurice Deserno, Katarzyna Bozek",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe possibility to extract motion of a single organism from video recordings at a large-scale provides means for the quantitative study of its behavior, both individual and collective. This task is particularly difficult for organisms that interact with one another, overlap, and occlude parts of their bodies in the recording. Here we propose WormSwin - an approach to extract single animal postures of Caenorhabditis elegans (C. elegans) from recordings of many organisms in a single microscope well. Based on transformer neural network architecture our method segments individual worms across a range of videos and images generated in different labs. Our solutions offers accuracy of 0.990 average precision (AP0.50) and comparable results on the benchmark image dataset BBBC010. Finally, it allows to segment challenging overlapping postures of mating worms with an accuracy sufficient to track the organisms with a simple tracking heuristic. An accurate and efficient method forC. eleganssegmentation opens up new opportunities for studying of its behaviors previously inaccessible due to the difficulty in the worm extraction from the video frames.",
        "link": "http://dx.doi.org/10.1101/2023.04.10.536324"
    },
    {
        "id": 21406,
        "title": "Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks",
        "authors": "Yuliang Cai, Mohammad Rostami",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4713352"
    },
    {
        "id": 21407,
        "title": "Zotero 6 : transformer votre flux de travail de recherche",
        "authors": " ",
        "published": "No Date",
        "citations": 0,
        "abstract": "Nous traduisons ici le dernier billet publié sur le blog Zotero : Stillman Dan, <em> Zotero 6: Your research workflow, transformed </em> , https://www.zotero.org/blog/zotero-6/, 17 mars 2022. Vous trouverez en supplément à la fin de ce billet l’avis de la rédaction, soit les nouveautés préférées de chacun d’entre nous.",
        "link": "http://dx.doi.org/10.59350/8ynth-3mv40"
    },
    {
        "id": 21408,
        "title": "Predicting Chemical Reaction Outcomes: A Grammar Ontology-based Transformer Framework",
        "authors": "Vipul Mann, Venkat Venkatasubramanian",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Discovering and designing novel materials is a challenging problem as it often requires searching a combinatorially large space of potential candidates. Evaluation of all  candidates experimentally is typically infeasible as it requires great amounts of effort, time, expertise, and money. The ability to predict reaction outcomes without performing extensive experiments is, therefore, important. Towards that goal, we report an approach that uses context-free grammar (CFG) based representations of molecules in a neural machine translation framework. We formulate the reaction-prediction task as a machine translation problem that involves discovering the transformations from the source sequence (comprising the reactants and agents) to the target sequence (comprising the major product) in the reaction. The grammar ontology-based representation of molecules hierarchically incorporates rich molecular structure information that, in principle, should be valuable for modeling chemical reactions. We achieve an accuracy of 80.1% on a standard reaction dataset using a model characterized by only a fraction of the number of training parameters in other sequence-to-sequence models based works in this area. Moreover, 99% of the predictions made on the same reaction dataset were found to be syntactically valid. We conclude that CFGs-based ontological representations could be an efficient way of incorporating structural information, ensuring chemically valid predictions, and overcoming overfitting in complex machine learning architectures employed in reaction prediction tasks.<br></p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12985892"
    },
    {
        "id": 21409,
        "title": "Transformer-Based Two-level Approach for Music-driven Dance Choreography",
        "authors": "Yanbo Cheng, Yingying Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012434500003660"
    },
    {
        "id": 21410,
        "title": "Learning Side Channel Attack Model Based on Vision Transformer",
        "authors": "杨杰 廖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/aam.2023.124163"
    },
    {
        "id": 21411,
        "title": "Finite Element Study of Core Materials of a Planar Transformer and Comparison of its Electromagnetic Properties with a High Frequency Transformer",
        "authors": "Arya Venugopal, Femi Robert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25103/jestr.163.16"
    },
    {
        "id": 21412,
        "title": "Prediction Model for the Distribution Transformer Failure Using Correlation of Weather Data",
        "authors": "Eun Hui Ko, Tatjana Dokic, Mladen Kezunovic",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_12"
    },
    {
        "id": 21413,
        "title": "Pulse Transformer-Frequency Response",
        "authors": "",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315215402-62"
    },
    {
        "id": 21414,
        "title": "Predicting Enzymatic Reactions with a Molecular Transformer",
        "authors": "David Kreutter, Philippe Schwaller, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>The use of enzymes for organic synthesis allows for simplified, more economical\nand selective synthetic routes not accessible to conventional reagents. However,\npredicting whether a particular molecule might undergo a specific enzyme\ntransformation is very difficult. <a>Here we used\nmulti-task transfer learning to train the Molecular Transformer, a\nsequence-to-sequence machine learning model, with one million reactions from\nthe US Patent Office (USPTO) database combined with 32,181 enzymatic\ntransformations annotated with a text description of the enzyme. The resulting Enzymatic\nTransformer model predicts the structure and stereochemistry of\nenzyme-catalyzed reaction products with remarkable accuracy. One of the key\nnovelties is that we combined the reaction SMILES language of only 405 atomic\ntokens with thousands of human language tokens describing the enzymes, such\nthat our Enzymatic Transformer not only learned to interpret SMILES, but also the\nnatural language as used by human experts to describe enzymes and their\nmutations.</a></p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13161359"
    },
    {
        "id": 21415,
        "title": "Map of kətahkina",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.6"
    },
    {
        "id": 21416,
        "title": "kči-wəliwəni (Acknowledgments)",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.4"
    },
    {
        "id": 21417,
        "title": "A Unified Transformer-based Network for Multimodal Emotion Recognition",
        "authors": "Kamran Ali, Charles E. Hughes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The development of transformer-based models has resulted in significant advances in addressing various vision and NLP-based research challenges. However, the progress made in transformer-based methods has not been effectively applied to biosensing research. This paper presents a novel Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify emotions in an arousal-valence space by combining a 2D representation of an ECG/PPG signal with the face information. To achieve this goal, we first investigate and compare the unimodal emotion recognition performance of three image-based representations of the ECG/PPG signal. We then present our UBVMT network which is trained to perform emotion recognition by combining the 2D image-based representation of the ECG/PPG signal and the facial expression features. Our unified transformer model consists of homogeneous transformer blocks that take as an input the 2D representation of the ECG/PPG signal and the corresponding face frame for emotion representation learning with minimal modality-specific design. Our UBVMT model is trained by reconstructing masked patches of video frames and 2D images of ECG/PPG signals, and contrastive modeling to align face and ECG/PPG data. Extensive experiments on the MAHNOB-HCI and DEAP datasets show that our Unified UBVMT-based model produces comparable results to the state-of-the-art techniques.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23916123.v1"
    },
    {
        "id": 21418,
        "title": "Load Tap Changers",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-19"
    },
    {
        "id": 21419,
        "title": "Quels systèmes alimentaires… demain ?",
        "authors": "Laurent Delcourt",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0007"
    },
    {
        "id": 21420,
        "title": "Short-Circuit Obligation",
        "authors": "Fang Zhu, Baitun Yang",
        "published": "2021-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816865-9"
    },
    {
        "id": 21421,
        "title": "List of Illustrations",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.3"
    },
    {
        "id": 21422,
        "title": "Cooling Systems",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-9"
    },
    {
        "id": 21423,
        "title": "Transformer",
        "authors": "Gábor Gergely",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-06951-2_3"
    },
    {
        "id": 21424,
        "title": "Improved MHC–peptide class I interaction prediction with pretrained transformer",
        "authors": "",
        "published": "2022-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18699/sbb-2022-669"
    },
    {
        "id": 21425,
        "title": "ATST: Audio Representation Learning with Teacher-Student Transformer",
        "authors": "Xian LI, Xiaofei Li",
        "published": "2022-9-18",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10126"
    },
    {
        "id": 21426,
        "title": "Decision letter for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v1/decision1"
    },
    {
        "id": 21427,
        "title": "Decision letter for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v2/decision1"
    },
    {
        "id": 21428,
        "title": "Phase-Shifting Transformers",
        "authors": "Gustav Preininger",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-4"
    },
    {
        "id": 21429,
        "title": "Transformer-based tool recommendation system in Galaxy",
        "authors": "Anup Kumar, Björn Grüning, Rolf Backofen",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractGalaxy is a web-based open-source platform for scientific analyses. Researchers use thousands of high-quality tools and workflows for their respective analyses. Tool recommender system predicts a collection of tools that can be used to extend an analysis. In this work, a tool recommender system is developed by training a Transformer-based neural network on workflows available on Galaxy Europe. Compared to the existing tool recommender system on Galaxy Europe that trains a recurrent neural network, the transformer-based neural network achieves two times faster convergence, has a four times lower model usage (model loading + prediction) time and shows a better generalisation that goes beyond training workflows. The scripts to create the recommendation model are available under MIT licence athttps://github.com/anuprulez/galaxy_tool_recommendation_transformers.",
        "link": "http://dx.doi.org/10.1101/2022.12.16.520746"
    },
    {
        "id": 21430,
        "title": "GSM based Transformer Condition Monitoring System",
        "authors": "Ms.Swati R. Wandhare, Ms.Bhagyashree Shikkewal",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24001/ijaems.icsesd2017.92"
    },
    {
        "id": 21431,
        "title": "Transformer sans rompre ni exlcure ",
        "authors": "Mouloud Madoun, David Autissier, Jean-Marie Peretti",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.mado.2019.01"
    },
    {
        "id": 21432,
        "title": "Decision letter for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v3/decision1"
    },
    {
        "id": 21433,
        "title": "Self-Saturating, Magnetic Amplifiers",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-25"
    },
    {
        "id": 21434,
        "title": "Impedance Characteristics",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-3"
    },
    {
        "id": 21435,
        "title": "A Unified Transformer-based Network for Multimodal Emotion Recognition",
        "authors": "Kamran Ali, Charles E. Hughes",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>The development of transformer-based models has resulted in significant advances in addressing various vision and NLP-based research challenges. However, the progress made in transformer-based methods has not been effectively applied to biosensing research. This paper presents a novel Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify emotions in an arousal-valence space by combining a 2D representation of an ECG/PPG signal with the face information. To achieve this goal, we first investigate and compare the unimodal emotion recognition performance of three image-based representations of the ECG/PPG signal. We then present our UBVMT network which is trained to perform emotion recognition by combining the 2D image-based representation of the ECG/PPG signal and the facial expression features. Our unified transformer model consists of homogeneous transformer blocks that take as an input the 2D representation of the ECG/PPG signal and the corresponding face frame for emotion representation learning with minimal modality-specific design. Our UBVMT model is trained by reconstructing masked patches of video frames and 2D images of ECG/PPG signals, and contrastive modeling to align face and ECG/PPG data. Extensive experiments on the MAHNOB-HCI and DEAP datasets show that our Unified UBVMT-based model produces comparable results to the state-of-the-art techniques.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23916123"
    },
    {
        "id": 21436,
        "title": "Transformer-CNN: Fast and Reliable Tool for QSAR",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 1,
        "abstract": "We present SMILES-embeddings derived from internal encoder state of a Transformer model trained to canonize SMILES as a Seq2Seq problem. Using CharNN architecture upon\nthe embeddings results in a higher quality QSAR/QSPR models on diverse benchmark datasets\nincluding regression and classification tasks. The proposed Transformer-CNN method uses\nSMILES augmentation for training and inference, and thus the prognosis grounds on an internal\nconsensus. Both the augmentation and transfer learning based on embedding allows the\nmethod to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code\nand the embeddings are available on https://github.com/bigchem/transformer-cnn, whereas the\nOCHEM environment (https://ochem.eu) hosts its on-line implementation.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.9961787"
    },
    {
        "id": 21437,
        "title": "Peer Review #3 of \"Homologous point transformer for multi-modality prostate image registration (v0.1)\"",
        "authors": "",
        "published": "2022-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1155v0.1/reviews/3"
    },
    {
        "id": 21438,
        "title": "Peer Review #2 of \"Homologous point transformer for multi-modality prostate image registration (v0.1)\"",
        "authors": "",
        "published": "2022-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1155v0.1/reviews/2"
    },
    {
        "id": 21439,
        "title": "U2former: U-Square Transformer for Generating Chinese Landscape Paintings",
        "authors": "Yunfa Li, Jiale Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740664"
    },
    {
        "id": 21440,
        "title": "Electric Field Calculations",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-12"
    },
    {
        "id": 21441,
        "title": "Special Transformers",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-11"
    },
    {
        "id": 21442,
        "title": "Engineering Electrodynamics",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373"
    },
    {
        "id": 21443,
        "title": "Vision Transformer Pays Attention to Facial Morphological Features when Predicting Attractiveness",
        "authors": "Takanori Sano",
        "published": "No Date",
        "citations": 0,
        "abstract": "Many studies have been conducted to predict facial beauty and attractiveness using facial images. Although convolutional neural networks (CNNs) have been the mainstay of these analyses, the Vision Transformer (ViT) model has recently attracted considerable attention. In this study, face attractiveness prediction models were constructed using CNN or ViT, and the face regions to which models pay attention were extracted. The results showed that the CNN-based model has a relatively large region as an important feature for prediction, while the ViT model has the face morphology region as an important feature. Correlation analysis revealed that the ViT model showed a positive correlation between the degree of these activations and attractiveness scores. The results suggest that the CNN-based model and the ViT model focus on different regions of the face and that there is correspondence with psychological findings in each.",
        "link": "http://dx.doi.org/10.31234/osf.io/qjsng"
    },
    {
        "id": 21444,
        "title": "Transformer Circuit",
        "authors": "Teruo Matsushita",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44002-1_10"
    },
    {
        "id": 21445,
        "title": "Review for \"Towards next generation power grid transformer for renewables: Technology review\"",
        "authors": "Premkumar Manoharan",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12848/v1/review1"
    },
    {
        "id": 21446,
        "title": "The Smart Transformer: A solid-state transformer tailored to provide ancillary services to the distribution grid",
        "authors": "Levy Ferreira Costa, Giovanni De Carne, Giampaolo Buticchi, Marco Liserre",
        "published": "2017-6",
        "citations": 218,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mpel.2017.2692381"
    },
    {
        "id": 21447,
        "title": "Electrical Bushings",
        "authors": "Loren B. Wagenaar",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-13"
    },
    {
        "id": 21448,
        "title": "Electromagnetics of Time Varying Complex Media",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113"
    },
    {
        "id": 21449,
        "title": "Fault Current Analysis",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-7"
    },
    {
        "id": 21450,
        "title": "Common-Mode Inductor Design",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-23"
    },
    {
        "id": 21451,
        "title": "High-Frequency Transformer Materials",
        "authors": "Ralph Lucke",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-19"
    },
    {
        "id": 21452,
        "title": "Transformer Design",
        "authors": "K.R.M. Nair",
        "published": "2021-2-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003088578-1"
    },
    {
        "id": 21453,
        "title": "Toroidal vector-potential transformer",
        "authors": "Masahiro Daibo",
        "published": "2017-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsenst.2017.8304422"
    },
    {
        "id": 21454,
        "title": "Active transformer-based equalizer",
        "authors": "",
        "published": "2021-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo198e_ch9"
    },
    {
        "id": 21455,
        "title": "Swinsod: Salient Object Detection Using Swin Transformer",
        "authors": "Shuang Wu, Guangjian Zhang, Xuefeng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4556674"
    },
    {
        "id": 21456,
        "title": "Modelling of magnetisation processes in transformer steel sheets",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2023.147415"
    },
    {
        "id": 21457,
        "title": "Review for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": "Dr. Angshu Sinha",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v1/review2"
    },
    {
        "id": 21458,
        "title": "Efficient Planar Integrated Transformer-Inductor with High PCB Utilization and Optimized Core",
        "authors": "Ahmed Nabih",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper addresses the challenges associated with the high winding and core loss in the Integrated Transformer-Inductor (ITL). To overcome these challenges, we propose an improved winding design of the ITL by utilizing idle shielding layers for inductor integration within the matrix transformer. This method offers full printed circuit board (PCB) utilization, where all layers are consumed as winding, resulting in a significant reduction in the winding loss of the ITL.</p>\n<p>Moreover, we propose an improved core structure of the ITL that offers better flux distribution of the leakage flux within the magnetic core. This method reduces the core loss by more than 50\\% compared to the conventional core structure.</p>\n<p>We demonstrate the effectiveness of our proposed concepts by presenting the design of the ITL used in a high-efficiency, high-power-density 3-kW 400-V-to-48-V LLC module. The proposed converter achieves a peak efficiency of 98.7\\% and a power density of 1050 W/in3.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22213780"
    },
    {
        "id": 21459,
        "title": "Efficient Planar Integrated Transformer-Inductor with High PCB Utilization and Optimized Core",
        "authors": "Ahmed Nabih",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper addresses the challenges associated with the high winding and core loss in the Integrated Transformer-Inductor (ITL). To overcome these challenges, we propose an improved winding design of the ITL by utilizing idle shielding layers for inductor integration within the matrix transformer. This method offers full printed circuit board (PCB) utilization, where all layers are consumed as winding, resulting in a significant reduction in the winding loss of the ITL.</p>\n<p>Moreover, we propose an improved core structure of the ITL that offers better flux distribution of the leakage flux within the magnetic core. This method reduces the core loss by more than 50\\% compared to the conventional core structure.</p>\n<p>We demonstrate the effectiveness of our proposed concepts by presenting the design of the ITL used in a high-efficiency, high-power-density 3-kW 400-V-to-48-V LLC module. The proposed converter achieves a peak efficiency of 98.7\\% and a power density of 1050 W/in3.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22213780.v1"
    },
    {
        "id": 21460,
        "title": "Bayesian Transformer Using Disentangled Mask Attention",
        "authors": "Jen-Tzung Chien, Yu-Han Huang",
        "published": "2022-9-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10457"
    },
    {
        "id": 21461,
        "title": "Power Transformer Efficiency—Survey Results and Assessment of Efficiency Implementation",
        "authors": "Žarko Janić, Anthony Walsh, Adesh Singh, Yordan Botev",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_9"
    },
    {
        "id": 21462,
        "title": "Determination of Water in Transformer by Water Solubility and Percent Moisture by Dry Weight in Comparison with ppm Water Value for Justification Transformer Maintenance",
        "authors": "E. Wannapring, T. Sirisithichote, N. Pattanadech, M. Leelajindakrairerk",
        "published": "2020-10-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd48350.2020.9287295"
    },
    {
        "id": 21463,
        "title": "Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss",
        "authors": "Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, Shankar Kumar",
        "published": "2020-5",
        "citations": 173,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9053896"
    },
    {
        "id": 21464,
        "title": "Learned Fusion: 3D Object Detection Using Calibration-Free Transformer Feature Fusion",
        "authors": "Michael Fürst, Rahul Jakkamsetty, René Schuster, Didier Stricker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012311400003654"
    },
    {
        "id": 21465,
        "title": "STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition",
        "authors": "Dasom Ahn, Sangwon Kim, Hyunsu Hong, Byoung Chul Ko",
        "published": "2023-1",
        "citations": 37,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00333"
    },
    {
        "id": 21466,
        "title": "RecViT: Enhancing Vision Transformer with Top-Down Information Flow",
        "authors": "Štefan Pócoš, Iveta Bečková, Igor Farkaš",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012464700003660"
    },
    {
        "id": 21467,
        "title": "Can Transformer Models Effectively Detect Software Aspects in StackOverflow Discussion?",
        "authors": "Nibir Chandra Mandal, Tashreef Muhammad, G. M. Shahariar",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-34622-4_18"
    },
    {
        "id": 21468,
        "title": "Investigating transformer‐based models for spatial downscaling and correcting biases of near‐surface temperature and wind‐speed forecasts",
        "authors": "Xiaohui Zhong, Fei Du, Lei Chen, Zhibin Wang, Hao Li",
        "published": "2024-1",
        "citations": 0,
        "abstract": "AbstractHigh‐resolution and accurate prediction of near‐surface weather parameters based on numerical weather prediction (NWP) models is essential for many downstream and real‐world applications. Traditional dynamical or statistical downscaling methods are insufficient to derive high‐resolution data from operational NWP forecasts, making it essential to devise new approaches. In recent years, an increasing number of researchers have explored the implementations of deep learning (DL) based models for spatial downscaling, motivated by the similarity between the super‐resolution (SR) problem in computer vision (CV) and downscaling. Furthermore, while transformer‐based models have become state‐of‐the‐art models for many SR tasks, they are rarely applied for downscaling of weather forecasts or climate projections. This study adapted transformer‐based models such as SwinIR and Uformer to downscale the temperature at 2 m () and wind speed at 10 m () over Eastern Inner Mongolia, encompassing the area from 39.6–46°N latitude and 111.6–118°E longitude. We used high‐resolution forecast (HRES) data from the European Centre for Medium‐range Weather Forecast (ECMWF) with a spatial resolution of 0.1° as the input and gridded observation data from the China Meteorological Administration (CMA) Land Data Assimilation System (CLDAS) at a spatial resolution of 0.01° as the target. Given that the models use observation data rather than a coarse‐grained version of forecast data as the target, they accomplish both bias correction and spatial downscaling. The results demonstrate that the performance of SwinIR and Uformer is superior to that of two convolutional neural network (CNN) based models (UNet and RCAN). Additionally, we introduced a novel module to extract features of varying resolution from the high‐resolution topography data and applied a multiscale feature fusion module to merge features of different scales, contributing to further enhancement of Uformer's performance.",
        "link": "http://dx.doi.org/10.1002/qj.4596"
    },
    {
        "id": 21469,
        "title": "AgroML: An Open-Source Repository to Forecast Reference Evapotranspiration in Different Geo-Climatic Conditions Using Machine Learning and Transformer-Based Models",
        "authors": "Juan Antonio Bellido-Jiménez, Javier Estévez, Joaquin Vanschoren, Amanda Penélope García-Marín",
        "published": "2022-3-8",
        "citations": 9,
        "abstract": "Accurately forecasting reference evapotranspiration (ET0) values is crucial to improve crop irrigation scheduling, allowing anticipated planning decisions and optimized water resource management and agricultural production. In this work, a recent state-of-the-art architecture has been adapted and deployed for multivariate input time series forecasting (transformers) using past values of ET0 and temperature-based parameters (28 input configurations) to forecast daily ET0 up to a week (1 to 7 days). Additionally, it has been compared to standard machine learning models such as multilayer perceptron (MLP), random forest (RF), support vector machine (SVM), extreme learning machine (ELM), convolutional neural network (CNN), long short-term memory (LSTM), and two baselines (historical monthly mean value and a moving average of the previous seven days) in five locations with different geo-climatic characteristics in the Andalusian region, Southern Spain. In general, machine learning models significantly outperformed the baselines. Furthermore, the accuracy dramatically dropped when forecasting ET0 for any horizon longer than three days. SVM, ELM, and RF using configurations I, III, IV, and IX outperformed, on average, the rest of the configurations in most cases. The best NSE values ranged from 0.934 in Córdoba to 0.869 in Tabernas, using SVM. The best RMSE, on average, ranged from 0.704 mm/day for Málaga to 0.883 mm/day for Conil using RF. In terms of MBE, most models and cases performed very accurately, with a total average performance of 0.011 mm/day. We found a relationship in performance regarding the aridity index and the distance to the sea. The higher the aridity index at inland locations, the better results were obtained in forecasts. On the other hand, for coastal sites, the higher the aridity index, the higher the error. Due to the good performance and the availability as an open-source repository of these models, they can be used to accurately forecast ET0 in different geo-climatic conditions, helping to increase efficiency in tasks of great agronomic importance, especially in areas with low rainfall or where water resources are limiting for the development of crops.",
        "link": "http://dx.doi.org/10.3390/agronomy12030656"
    },
    {
        "id": 21470,
        "title": "New Graph-Based and Transformer Deep Learning Models for River Dissolved Oxygen Forecasting",
        "authors": "Paulo Alexandre Costa Rocha, Victor Oliveira Santos, Jesse Van Griensven Thé, Bahram Gharabaghi",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "Dissolved oxygen (DO) is a key indicator of water quality and the health of an aquatic ecosystem. Aspiring to reach a more accurate forecasting approach for DO levels of natural streams, the present work proposes new graph-based and transformer-based deep learning models. The models were trained and validated using a network of real-time hydrometric and water quality monitoring stations for the Credit River Watershed, Ontario, Canada, and the results were compared with both benchmarking and state-of-the-art approaches. The proposed new Graph Neural Network Sample and Aggregate (GNN-SAGE) model was the best-performing approach, reaching coefficient of determination (R2) and root mean squared error (RMSE) values of 97% and 0.34 mg/L, respectively, when compared with benchmarking models. The findings from the Shapley additive explanations (SHAP) indicated that the GNN-SAGE benefited from spatiotemporal information from the surrounding stations, improving the model’s results. Furthermore, temperature has been found to be a major input attribute for determining future DO levels. The results established that the proposed GNN-SAGE model outperforms the accuracy of existing models for DO forecasting, with great potential for real-time water quality management in urban watersheds.",
        "link": "http://dx.doi.org/10.3390/environments10120217"
    },
    {
        "id": 21471,
        "title": "A Comparative Analysis on the Summarization of Legal Texts Using Transformer Models",
        "authors": "Daniel Núñez-Robinson, Jose Talavera-Montalto, Willy Ugarte",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-20319-0_28"
    },
    {
        "id": 21472,
        "title": "Critical assessment of transformer-based AI models for German clinical notes",
        "authors": "Manuel Lentzen, Sumit Madan, Vanessa Lage-Rupprecht, Lisa Kühnel, Juliane Fluck, Marc Jacobs, Mirja Mittermaier, Martin Witzenrath, Peter Brunecker, Martin Hofmann-Apitius, Joachim Weber, Holger Fröhlich",
        "published": "2022-10-4",
        "citations": 6,
        "abstract": "AbstractObjectiveHealthcare data such as clinical notes are primarily recorded in an unstructured manner. If adequately translated into structured data, they can be utilized for health economics and set the groundwork for better individualized patient care. To structure clinical notes, deep-learning methods, particularly transformer-based models like Bidirectional Encoder Representations from Transformers (BERT), have recently received much attention. Currently, biomedical applications are primarily focused on the English language. While general-purpose German-language models such as GermanBERT and GottBERT have been published, adaptations for biomedical data are unavailable. This study evaluated the suitability of existing and novel transformer-based models for the German biomedical and clinical domain.Materials and MethodsWe used 8 transformer-based models and pre-trained 3 new models on a newly generated biomedical corpus, and systematically compared them with each other. We annotated a new dataset of clinical notes and used it with 4 other corpora (BRONCO150, CLEF eHealth 2019 Task 1, GGPONC, and JSynCC) to perform named entity recognition (NER) and document classification tasks.ResultsGeneral-purpose language models can be used effectively for biomedical and clinical natural language processing (NLP) tasks, still, our newly trained BioGottBERT model outperformed GottBERT on both clinical NER tasks. However, training new biomedical models from scratch proved ineffective.DiscussionThe domain-adaptation strategy’s potential is currently limited due to a lack of pre-training data. Since general-purpose language models are only marginally inferior to domain-specific models, both options are suitable for developing German-language biomedical applications.ConclusionGeneral-purpose language models perform remarkably well on biomedical and clinical NLP tasks. If larger corpora become available in the future, domain-adapting these models may improve performances.",
        "link": "http://dx.doi.org/10.1093/jamiaopen/ooac087"
    },
    {
        "id": 21473,
        "title": "Transformer-based Unethical Sentence Detection",
        "authors": "Hyeonseo Yun, Sunyong Yoo",
        "published": "2021-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9728/dcs.2021.22.8.1289"
    },
    {
        "id": 21474,
        "title": "Research on Chinese Text Error Correction Based on Transformer Enhanced Architecture",
        "authors": "靖翔 杨",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2022.123057"
    },
    {
        "id": 21475,
        "title": "Development of 3 Phase Transformer Module Using Isolation Transformer for Simulation of Electrical Power System",
        "authors": "Adi Sutopo, Abdul Hakim Butar-butar, Azmi Riski Lubis",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4108/eai.11-10-2022.2325477"
    },
    {
        "id": 21476,
        "title": "Increasing cost efficiency through minimizing transformer losses: Design and performance analysis of a 250 kVA off-load tap changing step down transformer",
        "authors": "Sairatun Nesa Soheli, Md Saidul Hasan, Md Refat Uddin",
        "published": "2018-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29322/ijsrp.8.9.2018.p8188"
    },
    {
        "id": 21477,
        "title": "Transformer-based Korean to English translation light-weighting model",
        "authors": "Taejun Lee, Heeseok Chae, Jonggeun Song, Hoekyung Jung",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.6109/jkiice.2023.27.8.968"
    },
    {
        "id": 21478,
        "title": "On the Use of Visual Transformer for Image Complexity Assessment",
        "authors": "Luigi Celona, Gianluigi Ciocca, Raimondo Schettini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012426500003660"
    },
    {
        "id": 21479,
        "title": "Transformer-Based Models for the Automatic Indexing of Scientific Documents in French",
        "authors": "José Ángel González, Davide Buscaldi, Emilio Sanchis, Lluís-F. Hurtado",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-08473-7_6"
    },
    {
        "id": 21480,
        "title": "Optimizing Local Repository Search with Bidirectional Encoder Representations from Transformer Models",
        "authors": "John Binze B. Escol,  , Johanes L. Guibone, Wayan Klein E. Duenas, Cristopher C. Abalorio, James Cloyd M. Bustillo, Junell T. Bojocan",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "—In this research, we investigate the relationship between sentence transformer models and the accuracy and recall of asymmetric semantic search engines. Our results show that the chosen sentence transformer model has a significant impact on the performance of the search engine. Using a pre-trained model specific to the domain resulted in a high cosine similarity score of 0.64. Fine-tuning a pre-trained sentence transformer for asymmetric semantic search improved the score to 0.4, but a larger corpus was needed for optimal results. Our findings highlight the importance of careful model selection and training data in the development of asymmetric semantic search engines.",
        "link": "http://dx.doi.org/10.46338/ijetae0923_03"
    },
    {
        "id": 21481,
        "title": "On the Optimality of Passive and Symmetric High-Frequency n-Terminal Transformer Models",
        "authors": "Lucas P. R. K. Ihlenfeld, Gustavo H. C. Oliveira",
        "published": "2019-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrd.2018.2879448"
    },
    {
        "id": 21482,
        "title": "Transformer-based deep learning models for predicting permeability of porous media",
        "authors": "Yinquan Meng, Jianguo Jiang, Jichun Wu, Dong Wang",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.advwatres.2023.104520"
    },
    {
        "id": 21483,
        "title": "Transformer Based Models for Unsupervised Anomaly Segmentation in Brain MR Images",
        "authors": "Ahmed Ghorbel, Ahmed Aldahdooh, Shadi Albarqouni, Wassim Hamidouche",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-33842-7_3"
    },
    {
        "id": 21484,
        "title": "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models",
        "authors": "Jaewan Choi, Jaehyun Park, Kwanhee Kyung, Nam Sung Kim, Jung Ho Ahn",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lca.2023.3305386"
    },
    {
        "id": 21485,
        "title": "Pre-Trained Transformer-Based Models for Text Classification Using Low-Resourced Ewe Language",
        "authors": "Victor Kwaku Agbesi, Wenyu Chen, Sophyani Banaamwini Yussif, Md Altab Hossin, Chiagoziem C. Ukwuoma, Noble A. Kuadey, Colin Collinson Agbesi, Nagwan Abdel Samee, Mona M. Jamjoom, Mugahed A. Al-antari",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "Despite a few attempts to automatically crawl Ewe text from online news portals and magazines, the African Ewe language remains underdeveloped despite its rich morphology and complex \"unique\" structure. This is due to the poor quality, unbalanced, and religious-based nature of the crawled Ewe texts, thus making it challenging to preprocess and perform any NLP task with current transformer-based language models. In this study, we present a well-preprocessed Ewe dataset for low-resource text classification to the research community. Additionally, we have developed an Ewe-based word embedding to leverage the low-resource semantic representation. Finally, we have fine-tuned seven transformer-based models, namely BERT-based (cased and uncased), DistilBERT-based (cased and uncased), RoBERTa, DistilRoBERTa, and DeBERTa, using the preprocessed Ewe dataset that we have proposed. Extensive experiments indicate that the fine-tuned BERT-base-cased model outperforms all baseline models with an accuracy of 0.972, precision of 0.969, recall of 0.970, loss score of 0.021, and an F1-score of 0.970. This performance demonstrates the model’s ability to comprehend the low-resourced Ewe semantic representation compared to all other models, thus setting the fine-tuned BERT-based model as the benchmark for the proposed Ewe dataset.",
        "link": "http://dx.doi.org/10.3390/systems12010001"
    },
    {
        "id": 21486,
        "title": "Explainable Multivariate Deep Transformer for Stock Prediction",
        "authors": "Imam Waliyuddin, Novanto Yudistira, Wayan  Firdaus Mahmudy",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4713443"
    },
    {
        "id": 21487,
        "title": "Transformer-CNN: Fast and Reliable Tool for QSAR",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 1,
        "abstract": "We present SMILES-embeddings derived from internal encoder state of a Transformer model trained to canonize SMILES as a Seq2Seq problem. Using CharNN architecture upon the embeddings results in a higher quality QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed Transformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis grounds on an internal consensus. Both the augmentation and transfer learning based on embedding allows the method to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code and the embeddings are available on https://github.com/bigchem/transformer-cnn, whereas the OCHEM environment (https://ochem.eu) hosts its on-line implementation.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.9961787.v1"
    },
    {
        "id": 21488,
        "title": "Preface",
        "authors": "CAROL A. DANA",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.5"
    },
    {
        "id": 21489,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/review2"
    },
    {
        "id": 21490,
        "title": "Transformer Faults and Protection Devices",
        "authors": "Geoff Macangus-Gerrard",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-385499-5.00022-4"
    },
    {
        "id": 21491,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v2/review2"
    },
    {
        "id": 21492,
        "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
        "authors": "Jen-Tzung Chien, Shang-En Li",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1652"
    },
    {
        "id": 21493,
        "title": "Decision letter for \"Towards next generation power grid transformer for renewables: Technology review\"",
        "authors": "",
        "published": "2024-1-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12848/v2/decision1"
    },
    {
        "id": 21494,
        "title": "OLTC Transformer Model Connecting 3-Wire MV with 4-Wire Multigrounded LV Networks",
        "authors": "Evangelos Pompodakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<b>This letter presents a comprehensive model of on-load\ntap-changer (OLTC) transformers that connect 3-wire medium voltage (MV) with\n4-wire multigrounded low voltage (LV) networks. The proposed model enables the inclusion\nof the 3-wire MV network and the 4-wire multigrounded LV network into a single\nY<sub>BUS</sub> matrix without any assumption or simplification. Its distinct\nfeature is that the tap changer of the transformer is simulated outside the Y<sub>BUS</sub>\nmatrix, thus a refactorization of the Y<sub>BUS</sub> matrix is not required in\nevery tap change. The proposed transformer model has been validated in a 4-Bus\nnetwork, while its performance has been tested in the IEEE 8500-Node and IEEE\n906-Bus test networks.     </b>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12612443"
    },
    {
        "id": 21495,
        "title": "OLTC Transformer Model Connecting 3-Wire MV with 4-Wire Multigrounded LV Networks",
        "authors": "Evangelos Pompodakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<b>This letter presents a comprehensive model of on-load\ntap-changer (OLTC) transformers that connect 3-wire medium voltage (MV) with\n4-wire multigrounded low voltage (LV) networks. The proposed model enables the inclusion\nof the 3-wire MV network and the 4-wire multigrounded LV network into a single\nY<sub>BUS</sub> matrix without any assumption or simplification. Its distinct\nfeature is that the tap changer of the transformer is simulated outside the Y<sub>BUS</sub>\nmatrix, thus a refactorization of the Y<sub>BUS</sub> matrix is not required in\nevery tap change. The proposed transformer model has been validated in a 4-Bus\nnetwork, while its performance has been tested in the IEEE 8500-Node and IEEE\n906-Bus test networks.     </b>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12612443.v1"
    },
    {
        "id": 21496,
        "title": "Detection of Power Transformer Fault Conditions using Optical Characteristics of Transformer Oil",
        "authors": "Nur Afini Fauzi, Vimal Angela Thiviyanathan, Yang Sing Leong, Pin Jern Ker, M. Z. Jamaludin, Saiffuddin M. Nomanbhay, H. M. Looe, C. K. Lo",
        "published": "2018-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icp.2018.8533173"
    },
    {
        "id": 21497,
        "title": "Fake News Detection Based on Multi-Head Attention Convolution Transformer",
        "authors": "亚立 张",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/hjdm.2023.134029"
    },
    {
        "id": 21498,
        "title": "Application of Converter Transformer Defect Detection Based on Transformer Integrated Monitoring System",
        "authors": "Xiu Zhou, Qingping Zhang, Bo Wang, Yuhua Xu, Tian Tian, Yunlong Ma, Xiuguang Li, Ninghui He, Liang Li, Weiyan Sha",
        "published": "2021-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeero52475.2021.9708088"
    },
    {
        "id": 21499,
        "title": "The Diagnosis of Neonatal Lung Disease Based on Ultrasound Images Using Transformer",
        "authors": "磊 张",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/sea.2022.116123"
    },
    {
        "id": 21500,
        "title": "Modeling and new tuning of the distribution transformer-integrated passive power filter and its effects on the transformer performance and network power quality",
        "authors": "Omid Rahat, Mohsen Saniei, Seyyed Ghodratollah Seifossadat",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2022.108844"
    },
    {
        "id": 21501,
        "title": "Transformer le lait local en Afrique de l'Ouest",
        "authors": "Cécile Broutin, Marie-Christine Goudiaby",
        "published": "2021-12-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35690/978-2-7592-3397-7"
    },
    {
        "id": 21502,
        "title": "Image2SMILES: Transformer-based Molecular Optical Recognition Engine",
        "authors": "Ivan Khokhlov, Lev Krasnov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 0,
        "abstract": "The rise of deep learning in various scientific and technology areas promotes the development of AI-based tools for information retrieval. Optical recognition of organic structures is a key part of the automated extraction of chemical information. However, this is a challenging task because there is a large variety of representation styles. In this research, we present a Transformer-based artificial neural network to convert images of organic structures to molecular structures. To train the model, we created a comprehensive data generator that stochastically simulates various drawing styles, functional groups, functional group placeholders (R-groups), and visual contamination. We demonstrate that the Transformer-based architecture can gather chemical insights from our generator with almost absolute confidence. That means that, with Transformer, one can fully concentrate on data simulation to build a good recognition model. A web demo of our optical recognition engine is available online at <i>Syntelly</i> platform.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14602716"
    },
    {
        "id": 21503,
        "title": "Transformer Design Optimization",
        "authors": "K.R.M. Nair",
        "published": "2021-2-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003088578-28"
    },
    {
        "id": 21504,
        "title": "Full-Set Transformer Protection Barrier Manufacturing and Technology",
        "authors": "Todd Johnson, Kenneth Bratton, Henry Chu",
        "published": "2020-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1692383"
    },
    {
        "id": 21505,
        "title": "Continuous-Time Sequential Recommendation with Transformer and Graph Neural Network",
        "authors": "Yuanbo Xu, Zichang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622847"
    },
    {
        "id": 21506,
        "title": "Transformer : des réseaux de neurones pour le traitement automatique des langues",
        "authors": "François YVON",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51257/a-v1-in195"
    },
    {
        "id": 21507,
        "title": "Déformer, transformer, traduire",
        "authors": "",
        "published": "2018-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/rfp.825.1233"
    },
    {
        "id": 21508,
        "title": "Temporal Transformer Networks for Acoustic Scene Classification",
        "authors": "Teng Zhang, Kailai Zhang, Ji Wu",
        "published": "2018-9-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-1152"
    },
    {
        "id": 21509,
        "title": "OLTC Transformer Model Connecting 3-Wire MV with 4-Wire Multigrounded LV Networks",
        "authors": "Evangelos Pompodakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<b>This letter presents a comprehensive model of on-load\ntap-changer (OLTC) transformers that connect 3-wire medium voltage (MV) with\n4-wire multigrounded low voltage (LV) networks. The proposed model enables the inclusion\nof the 3-wire MV network and the 4-wire multigrounded LV network into a single\nY<sub>BUS</sub> matrix without any assumption or simplification. Its distinct\nfeature is that the tap changer of the transformer is simulated outside the Y<sub>BUS</sub>\nmatrix, thus a refactorization of the Y<sub>BUS</sub> matrix is not required in\nevery tap change. The proposed transformer model has been validated in a 4-Bus\nnetwork, while its performance has been tested in the IEEE 8500-Node and IEEE\n906-Bus test networks.     </b>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12612443.v2"
    },
    {
        "id": 21510,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/review3"
    },
    {
        "id": 21511,
        "title": "Transformer BERT and GPT3",
        "authors": "Oswald Campesato",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973"
    },
    {
        "id": 21512,
        "title": "Research Review of Remanence Measurement Methods for Current Transformer Cores",
        "authors": "Cailing Huo, Yiming Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693737"
    },
    {
        "id": 21513,
        "title": "Winding Capacitance and Leakage Inductance",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-17"
    },
    {
        "id": 21514,
        "title": "Vision Transformer Based Photo Capturing System",
        "authors": "Abdülkadir Albayrak",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4154101"
    },
    {
        "id": 21515,
        "title": "Poultry Disease Identification In Fecal Images Using Vision Transformer",
        "authors": "",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55162/mcaes.06.150"
    },
    {
        "id": 21516,
        "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
        "authors": "Katerina Papadimitriou, Gerasimos Potamianos",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2198"
    },
    {
        "id": 21517,
        "title": "A-Prot: Protein structure modeling using MSA transformer",
        "authors": "Yiyu Hong, Juyong Lee, Junsu Ko",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn this study, we propose a new protein 3D structure modeling method, A-Prot, using MSA Transformer, one of the state-of-the-art protein language models. For a given MSA, an MSA feature tensor and row attention maps are extracted and converted into 2D residue-residue distance and dihedral angle predictions. We demonstrated that A-Prot predicts long-range contacts better than the existing methods. Additionally, we modeled the 3D structures of the free modeling and hard template-based modeling targets of CASP14. The assessment shows that the A-Prot models are more accurate than most top server groups of CASP14. These results imply that A-Prot captures evolutionary and structural information of proteins accurately with relatively low computational cost. Thus, A-Prot can provide a clue for the development of other protein property prediction methods.",
        "link": "http://dx.doi.org/10.1101/2021.09.10.459866"
    },
    {
        "id": 21518,
        "title": "Review for \"Identifying promising sequences for protein engineering using a deep transformer protein language model\"",
        "authors": "",
        "published": "2023-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26536/v2/review1"
    },
    {
        "id": 21519,
        "title": "Predicting Enzymatic Reactions with a Molecular Transformer",
        "authors": "David Kreutter, Philippe Schwaller, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "The use of enzymes for organic synthesis allows for simplified, more economical and selective synthetic routes not accessible to conventional reagents. However, predicting whether a particular molecule might undergo a specific enzyme transformation is very difficult. Here we used multi-task transfer learning to train the Molecular Transformer, a sequence-to-sequence machine learning model, with one million reactions from the US Patent Office (USPTO) database combined with 32,181 enzymatic transformations annotated with a text description of the enzyme. The resulting Enzymatic Transformer model predicts the structure and stereochemistry of enzyme-catalyzed reaction products with remarkable accuracy. One of the key novelties is that we combined the reaction SMILES language of only 405 atomic tokens with thousands of human language tokens describing the enzymes, such that our Enzymatic Transformer not only learned to interpret SMILES, but also the natural language as used by human experts to describe enzymes and their mutations.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13161359.v2"
    },
    {
        "id": 21520,
        "title": "Improving Small Objects Detection using Transformer",
        "authors": "Shikha Dubey, Farrukh Olimov, Muhammad Aasim Rafique, Moongu Jeon",
        "published": "No Date",
        "citations": 0,
        "abstract": "General artificial intelligence is a trade-off between the inductive bias of an algorithm and its out-of-distribution generalization performance. The conspicuous impact of inductive bias is an unceasing trend of improved predictions in various problems in computer vision like object detection. Although a recently introduced object detection technique, based on transformers (DETR), shows results competitive to the conventional and modern object detection models, its accuracy deteriorates for detecting small-sized objects (in perspective). This study examines the inductive bias of DETR and proposes a normalized inductive bias for object detection using a transformer (SOF-DETR). It uses a lazy-fusion of features to sustain deep contextual information of objects present in the image. The features from multiple subsequent deep layers are fused with element-wise-summation and input to a transformer network for object queries that learn the long and short-distance spatial association in the image by the attention mechanism.<br>SOF-DETR uses a global set-based prediction for object detection, which directly produces a set of bounding boxes. The experimental results on the MS COCO dataset show the effectiveness of the added normalized inductive bias and feature fusion techniques by detecting more small-sized objects than DETR. <br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16921000"
    },
    {
        "id": 21521,
        "title": "Transformer Based Grapheme-to-Phoneme Conversion",
        "authors": "Sevinj Yolchuyeva, Géza Németh, Bálint Gyires-Tóth",
        "published": "2019-9-15",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1954"
    },
    {
        "id": 21522,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2024-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v2/review1"
    },
    {
        "id": 21523,
        "title": "Solid-State Transformer",
        "authors": "K.R.M. Nair",
        "published": "2021-2-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003088578-27"
    },
    {
        "id": 21524,
        "title": "Experiments",
        "authors": "Igor Alexeff",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-16"
    },
    {
        "id": 21525,
        "title": "Monitoring and Diagnostics",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-14"
    },
    {
        "id": 21526,
        "title": "Linear Transformer",
        "authors": "Nassir H. Sabah",
        "published": "2017-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315402222-9"
    },
    {
        "id": 21527,
        "title": "Transformer Dielectric Liquid: A Review",
        "authors": "Deba Kumar Mahanta, Olyveena Andrew",
        "published": "2020-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica49420.2020.9219867"
    },
    {
        "id": 21528,
        "title": "Derivations for the Design Equations",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-21"
    },
    {
        "id": 21529,
        "title": "Transformer Analyzer BOT",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "In an automation world everything gets connect to the internet using the cloud, IoT, and various techniques but it does not reach up to the mark. In a transformer, the efficiency can improve by various methods but the greatest challenge is still continuous which includes fault analysis and energy saving so there is an ideal solution to that greatest challenge using automation, cloud computing, andmachinelearning.",
        "link": "http://dx.doi.org/10.54216/jchci.030202"
    },
    {
        "id": 21530,
        "title": "Transformer Data",
        "authors": "",
        "published": "2022-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119513100.app4"
    },
    {
        "id": 21531,
        "title": "Nowcasting Earthquakes with QuakeGPT An AI-Enhanced Earthquake Generative Pretrained Transformer",
        "authors": "John B. Rundle",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/essoar.170034944.42344050/v1"
    },
    {
        "id": 21532,
        "title": "Pour une révolution de la confiance",
        "authors": "",
        "published": "2020-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.herve.2020.01"
    },
    {
        "id": 21533,
        "title": "Der Siegeszug der Transformer",
        "authors": "Manuela Lenzen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17104/9783406815584-49-1"
    },
    {
        "id": 21534,
        "title": "Regional Flood Inundation Nowcast Using Double-Encoder Transformer",
        "authors": "Shao-Kun Shiu, Li-Chiu Chang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the context of rapid global population growth and extensive economic development, urbanization is expanding rapidly. The expansion of urbanization brings about increasingly complex challenges for cities, and flooding is one of the disasters faced. Climate change has led to a significant increase in extreme hydrological events, particularly a sharp rise in rainfall intensity, further elevating the risk of flooding in low-lying urban areas. The study area is located in Taipei City, characterized by low-lying terrain surrounded by mountains, and is influenced by subtropical climate. The frequent occurrence of heavy rainfall during the monsoon season and typhoons contributes to frequent flooding events, with the additional impact of climate change increasing the risk of intense rainfall. Therefore, the real-time prediction of regional flooding and its application in urban management becomes an imperative task, aiding in early warning, effective flood risk response, and ensuring sustainable urban development.\nThis study utilizes the Double-Encoder Transformer model for real-time flood forecasting leveraging dual-encoder architecture to process and analyze diverse data types relevant predicting floods. One encoder could be dedicated to interpreting meteorological data, such as rainfall spatial distribution. This encoder focuses on extracting and understanding the complex patterns in weather-related data, which are crucial for predicting the likelihood of flooding. The second encoder, on the other hand, could handle geographical and environmental data, including terrain topology, and land use patterns. This encoder is adept at understanding how environmental factors contribute to flood risk in specific areas. By concurrently processing these two streams of information, the Double-Encoder Transformer can create a more comprehensive prediction model. It can identify correlations between meteorological conditions and environmental responses, leading to more accurate and timely flood forecasts. This approach enhances the model's ability to predict not only when and where floods might occur but also their potential severity, aiding in disaster preparedness and resource allocation.\nOverall, the application of the Double-Encoder Transformer in flood forecasting represents a significant advancement in disaster management, leveraging AI's power to integrate and analyze complex, multi-faceted data for better, more informed decision-making in critical situations.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-16762"
    },
    {
        "id": 21535,
        "title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer",
        "authors": "Alan Baade, Puyuan Peng, David Harwath",
        "published": "2022-9-18",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10961"
    },
    {
        "id": 21536,
        "title": "Increasing cost efficiency through minimizing transformer losses: Design and performance analysis of a 250 kVA off-load tap changing step down transformer",
        "authors": "Sairatun Nesa Soheli, Md Saidul Hasan, Md Refat Uddin",
        "published": "2018-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29322/ijsrp.8.9.2018.p8188"
    },
    {
        "id": 21537,
        "title": "Research on Chinese Text Error Correction Based on Transformer Enhanced Architecture",
        "authors": "靖翔 杨",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2022.123057"
    },
    {
        "id": 21538,
        "title": "Transformer-based Unethical Sentence Detection",
        "authors": "Hyeonseo Yun, Sunyong Yoo",
        "published": "2021-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9728/dcs.2021.22.8.1289"
    },
    {
        "id": 21539,
        "title": "Development of 3 Phase Transformer Module Using Isolation Transformer for Simulation of Electrical Power System",
        "authors": "Adi Sutopo, Abdul Hakim Butar-butar, Azmi Riski Lubis",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4108/eai.11-10-2022.2325477"
    },
    {
        "id": 21540,
        "title": "Transformer-based Korean to English translation light-weighting model",
        "authors": "Taejun Lee, Heeseok Chae, Jonggeun Song, Hoekyung Jung",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.6109/jkiice.2023.27.8.968"
    },
    {
        "id": 21541,
        "title": "On the Use of Visual Transformer for Image Complexity Assessment",
        "authors": "Luigi Celona, Gianluigi Ciocca, Raimondo Schettini",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012426500003660"
    },
    {
        "id": 21542,
        "title": "Preface",
        "authors": "CAROL A. DANA",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.5"
    },
    {
        "id": 21543,
        "title": "Transformer-CNN: Fast and Reliable Tool for QSAR",
        "authors": "Pavel Karpov, Guillaume Godin, Igor Tetko",
        "published": "No Date",
        "citations": 1,
        "abstract": "We present SMILES-embeddings derived from internal encoder state of a Transformer model trained to canonize SMILES as a Seq2Seq problem. Using CharNN architecture upon the embeddings results in a higher quality QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed Transformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis grounds on an internal consensus. Both the augmentation and transfer learning based on embedding allows the method to provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for the development of the method. The source code and the embeddings are available on https://github.com/bigchem/transformer-cnn, whereas the OCHEM environment (https://ochem.eu) hosts its on-line implementation.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.9961787.v1"
    },
    {
        "id": 21544,
        "title": "Transformer Faults and Protection Devices",
        "authors": "Geoff Macangus-Gerrard",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-385499-5.00022-4"
    },
    {
        "id": 21545,
        "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
        "authors": "Jen-Tzung Chien, Shang-En Li",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1652"
    },
    {
        "id": 21546,
        "title": "OLTC Transformer Model Connecting 3-Wire MV with 4-Wire Multigrounded LV Networks",
        "authors": "Evangelos Pompodakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<b>This letter presents a comprehensive model of on-load\ntap-changer (OLTC) transformers that connect 3-wire medium voltage (MV) with\n4-wire multigrounded low voltage (LV) networks. The proposed model enables the inclusion\nof the 3-wire MV network and the 4-wire multigrounded LV network into a single\nY<sub>BUS</sub> matrix without any assumption or simplification. Its distinct\nfeature is that the tap changer of the transformer is simulated outside the Y<sub>BUS</sub>\nmatrix, thus a refactorization of the Y<sub>BUS</sub> matrix is not required in\nevery tap change. The proposed transformer model has been validated in a 4-Bus\nnetwork, while its performance has been tested in the IEEE 8500-Node and IEEE\n906-Bus test networks.     </b>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12612443"
    },
    {
        "id": 21547,
        "title": "OLTC Transformer Model Connecting 3-Wire MV with 4-Wire Multigrounded LV Networks",
        "authors": "Evangelos Pompodakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<b>This letter presents a comprehensive model of on-load\ntap-changer (OLTC) transformers that connect 3-wire medium voltage (MV) with\n4-wire multigrounded low voltage (LV) networks. The proposed model enables the inclusion\nof the 3-wire MV network and the 4-wire multigrounded LV network into a single\nY<sub>BUS</sub> matrix without any assumption or simplification. Its distinct\nfeature is that the tap changer of the transformer is simulated outside the Y<sub>BUS</sub>\nmatrix, thus a refactorization of the Y<sub>BUS</sub> matrix is not required in\nevery tap change. The proposed transformer model has been validated in a 4-Bus\nnetwork, while its performance has been tested in the IEEE 8500-Node and IEEE\n906-Bus test networks.     </b>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12612443.v1"
    },
    {
        "id": 21548,
        "title": "Explainable Multivariate Deep Transformer for Stock Prediction",
        "authors": "Imam Waliyuddin, Novanto Yudistira, Wayan  Firdaus Mahmudy",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4713443"
    },
    {
        "id": 21549,
        "title": "Decision letter for \"Towards next generation power grid transformer for renewables: Technology review\"",
        "authors": "",
        "published": "2024-1-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12848/v2/decision1"
    },
    {
        "id": 21550,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/review2"
    },
    {
        "id": 21551,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v2/review2"
    },
    {
        "id": 21552,
        "title": "Magnetic Materials and Their Characteristics",
        "authors": "",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-2"
    },
    {
        "id": 21553,
        "title": "Self-oscillating DC Current Transformer with Nanocrystalline Core",
        "authors": "Vaclav Grim, Pavel Ripka",
        "published": "2021-10-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sensors47087.2021.9639745"
    },
    {
        "id": 21554,
        "title": "Decision letter for \"Towards next generation power grid transformer for renewables: Technology review\"",
        "authors": "",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12848/v1/decision1"
    },
    {
        "id": 21555,
        "title": "Davit: Dual Attention Vision Transformer for Multimodal Image Fusion",
        "authors": "Shrida Kalamkar, Geetha Mary Amalanathan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458710"
    },
    {
        "id": 21556,
        "title": "Radar HRRP target recognition based on contraction Transformer",
        "authors": "Siyu Chen, Weibo Xu, Xiaohong Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Radar High Resolution Range Profile (HRRP), which can provide target\nstructure information with great potential for target recognition.\nHowever, the structural information is not fully exploited by most\nexisting deep learning methods, which focus only on local or sequence\ninformation. Furthermore, existing methods equalise target and\nnon-target regions in HRRP. This is not conducive to target feature\nextraction. In this letter, we propose a target recognition method using\nwavelet patch merging and contraction Transformer, called CT. CT can\nadaptively focus on the target region and efficiently extract local and\nsequence information. CT used convolution to extract local features and\ncontraction self-attention to extract sequential features. Wavelet patch\nmerging was used to avoid oversampling. Finally, the experimental\nresults show that the CT can effectively extract structural features in\nHRRP to improve target recognition performance. It is also robust under\nlow signal-to-noise conditions.",
        "link": "http://dx.doi.org/10.22541/au.169875927.73804425/v1"
    },
    {
        "id": 21557,
        "title": "Predicting Enzymatic Reactions with a Molecular Transformer",
        "authors": "David Kreutter, Philippe Schwaller, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 0,
        "abstract": "The use of enzymes for organic synthesis allows for simplified, more economical and selective synthetic routes not accessible to conventional reagents. However, predicting whether a particular molecule might undergo a specific enzyme transformation is very difficult. Here we exploited recent advances in computer assisted synthetic planning (CASP) by considering the Molecular Transformer, which is a sequence-to-sequence machine learning model that can be trained to predict the products of organic transformations, including their stereochemistry, from the structure of reactants and reagents. We used multi-task transfer learning to train the Molecular Transformer with one million reactions from the US Patent Office (USPTO) database as a source of general chemistry knowledge combined with 32,000 enzymatic transformations, each one annotated with a text description of the enzyme. We show that the resulting Enzymatic Transformer model predicts the products formed from a given substrate and enzyme with remarkable accuracy, including typical kinetic resolution processes.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13161359.v1"
    },
    {
        "id": 21558,
        "title": "Quality Confirmation Tests for Power Transformer Insulation Systems",
        "authors": "Behrooz Vahidi, Ashkan Teymouri",
        "published": "2019",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-19693-6"
    },
    {
        "id": 21559,
        "title": "Tapless Voltage Regulating Transformer. Final Report",
        "authors": "Zhi Li, Bailu Xiao, Yaosuo Xue, Philip Irminger",
        "published": "2020-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1770650"
    },
    {
        "id": 21560,
        "title": "“New York Telephone Conversation”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0018"
    },
    {
        "id": 21561,
        "title": "Improving Small Objects Detection using Transformer",
        "authors": "Shikha Dubey, Farrukh Olimov, Muhammad Aasim Rafique, Moongu Jeon",
        "published": "No Date",
        "citations": 0,
        "abstract": "General artificial intelligence is a trade-off between the inductive bias of an algorithm and its out-of-distribution generalization performance. The conspicuous impact of inductive bias is an unceasing trend of improved predictions in various problems in computer vision like object detection. Although a recently introduced object detection technique, based on transformers (DETR), shows results competitive to the conventional and modern object detection models, its accuracy deteriorates for detecting small-sized objects (in perspective). This study examines the inductive bias of DETR and proposes a normalized inductive bias for object detection using a transformer (SOF-DETR). It uses a lazy-fusion of features to sustain deep contextual information of objects present in the image. The features from multiple subsequent deep layers are fused with element-wise-summation and input to a transformer network for object queries that learn the long and short-distance spatial association in the image by the attention mechanism.<br>SOF-DETR uses a global set-based prediction for object detection, which directly produces a set of bounding boxes. The experimental results on the MS COCO dataset show the effectiveness of the added normalized inductive bias and feature fusion techniques by detecting more small-sized objects than DETR. <br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16921000.v1"
    },
    {
        "id": 21562,
        "title": "Review for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/review1"
    },
    {
        "id": 21563,
        "title": "The Annotated Transformer",
        "authors": "Alexander Rush",
        "published": "2018",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-2509"
    },
    {
        "id": 21564,
        "title": "Super-Resolution Reconstruction of Face Images Based on GAN and Transformer",
        "authors": "新晨 蒯",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/sea.2023.123044"
    },
    {
        "id": 21565,
        "title": "Magnetic Current Transformer-Based Protection of a Single-Phase Transformer from Electrical and Mechanical Failures",
        "authors": "Timofei A. NOVOZHILOV,  ",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24160/0013-5380-2017-6-65-70"
    },
    {
        "id": 21566,
        "title": "Combining GCN and Transformer for Chinese Grammatical Error Detection",
        "authors": "Jinhong Zhang Jinhong Zhang",
        "published": "2022-12",
        "citations": 0,
        "abstract": "\n                        <p>This paper describes our system at a task: Chinese Grammatical Error Diagnosis (CGED). The task is held by the Natural Language Processing Techniques for Educational Applications (NLP-TEA) to encourage the development of automatic grammatical error diagnosis in Chinese learning since 2014. The goal of CGED is to diagnose four types of grammatical errors: word selection (S), redundant words (R), missing words (M), and disordered words (W). The automatic CGED system contains two parts including error detection and error correction and our system is designed to solve the error detection problem. Our system is built on three models: 1) a BERT-based model leveraging syntactic information; 2) a BERT-based model leveraging contextual embeddings; 3) a lexicon-based graph neural network leveraging lexical information. We also design an ensemble mechanism to improve the single model&rsquo;s performance. Finally, our system achieves the highest F1 scores at detection level and identification level among all teams participating in the CGED 2020 task.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642022122307020"
    },
    {
        "id": 21567,
        "title": "Reclamation of Aged Transformer Oil Employing Combined Adsorbents Techniques Using Response Surface for Transformer Applications",
        "authors": "Sarathkumar Duraisamy, Srinivasan Murugesan, Karthikeyan Murugan, Raymon Antony Raj",
        "published": "2023-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2022.3226162"
    },
    {
        "id": 21568,
        "title": "Monitoring and Control of Operational Parameters of distribution transformer using IoT technology\nDistribution Transformer using IoT Technology",
        "authors": "Vishwanath  M. Soppimath, Pavitra Sheeri, Rajakumar Kalakaraddi, Chawan Sagar Kumar, Chawan Sagar Kumar",
        "published": "2018-8-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29294/ijase.5.1.2018.871-878"
    },
    {
        "id": 21569,
        "title": "Modeling the Temperature of the Distribution Transformer Oil Using Transformer Body Temperature and Power Quality Parameters Based on Artificial Neural Network",
        "authors": "Anang Tjahjono, Wahyu A. Septian,  Rosmaliati, Novita W. Rika, Taufik Taufik",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf48524.2019.9102485"
    },
    {
        "id": 21570,
        "title": "Stressed Oil Volume Theory in Transformer Winding Corner Stress Analysis",
        "authors": "Petar Gabrić, Ana Orešković, Vjenceslav Kuprešanin, Antun Mikulecky, Vladimir Podobnik",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_4"
    },
    {
        "id": 21571,
        "title": "Dynamic Model and Analysis of Three phase YD Transformer Based Dual Active Bridge Using Optimised Harmonic Number for Solid State Transformer in Distributed System",
        "authors": "Mohammad Tauquir Iqbal, AH Iftekhar Maswood, Kevin Yeo, Mohd Tariq",
        "published": "2018-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt-asia.2018.8467858"
    },
    {
        "id": 21572,
        "title": "PE-Transformer: Path enhanced transformer for improving underwater object detection",
        "authors": "Jinxiong Gao, Yonghui Zhang, Xu Geng, Hao Tang, Uzair Aslam Bhatti",
        "published": "2024-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123253"
    },
    {
        "id": 21573,
        "title": "Exploring vision transformer: classifying electron-microscopy pollen images with transformer",
        "authors": "Kaibo Duan, Shi Bao, Zhiqiang Liu, Shaodong Cui",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00521-022-07789-y"
    },
    {
        "id": 21574,
        "title": "Resonant inductive-coupling configurations with load-independent transfer-parameters for wireless power transfer – a perspective from the transformer equivalent-models",
        "authors": "Carlos Marques",
        "published": "2018-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/09398368.2017.1418048"
    },
    {
        "id": 21575,
        "title": "Comparing CNN-based and transformer-based models for identifying lung cancer: which is more effective?",
        "authors": "Lulu Gai, Mengmeng Xing, Wei Chen, Yi Zhang, Xu Qiao",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "AbstractLung cancer constitutes the most severe cause of cancer-related mortality. Recent evidence supports that early detection by means of computed tomography (CT) scans significantly reduces mortality rates. Given the remarkable progress of Vision Transformers (ViTs) in the field of computer vision, we have delved into comparing the performance of ViTs versus Convolutional Neural Networks (CNNs) for the automatic identification of lung cancer based on a dataset of 212 medical images. Importantly, neither ViTs nor CNNs require lung nodule annotations to predict the occurrence of cancer. To address the dataset limitations, we have trained both ViTs and CNNs with three advanced techniques: transfer learning, self-supervised learning, and sharpness-aware minimizer. Remarkably, we have found that CNNs achieve highly accurate prediction of a patient’s cancer status, with an outstanding recall (93.4%) and area under the Receiver Operating Characteristic curve (AUC) of 98.1%, when trained with self-supervised learning. Our study demonstrates that both CNNs and ViTs exhibit substantial potential with the three strategies. However, CNNs are more effective than ViTs with the insufficient quantities of dataset.",
        "link": "http://dx.doi.org/10.1007/s11042-023-17644-4"
    },
    {
        "id": 21576,
        "title": "Latent Transformer Models for out-of-distribution detection",
        "authors": "Mark S. Graham, Petru-Daniel Tudosiu, Paul Wright, Walter Hugo Lopez Pinaya, Petteri Teikari, Ashay Patel, Jean-Marie U-King-Im, Yee H. Mah, James T. Teo, Hans Rolf Jäger, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.media.2023.102967"
    },
    {
        "id": 21577,
        "title": "Topological Transient Models of Three-Phase, Three-Legged Transformer",
        "authors": "Jianhui Zhao, Sergey E. Zirka, Yuriy I. Moroz, Cesare M. Arturi, Reigh A. Walling, Nasser Tleis, Olexandr L. Tarchutkin",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2931311"
    },
    {
        "id": 21578,
        "title": "Automation of Vulnerability Information Extraction Using Transformer-Based Language Models",
        "authors": "Fateme Hashemi Chaleshtori, Indrakshi Ray",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-25460-4_37"
    },
    {
        "id": 21579,
        "title": "News Categorisation Based on Pre-Trained Transformer Models",
        "authors": "César Espin-Riofrio, Vanessa Murillo-Cepeda, David García-Zambrano, Verónica Mendoza Morán, Arturo Montejo-Ráez, Johanna Zumba Gamboa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18687/laccei2023.1.1.1076"
    },
    {
        "id": 21580,
        "title": "TrojBits: A Hardware Aware Inference-Time Attack on Transformer-Based Language Models",
        "authors": "Mansour Al Ghanim, Muhammad Santriaji, Qian Lou, Yan Solihin",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "Transformer-based language models demonstrate exceptional performance in Natural Language Processing (NLP) tasks but remain susceptible to backdoor attacks involving hidden input triggers. Trojan injection via hardware bitflips presents a significant challenge for contemporary language models. However, previous research overlooks practical hardware considerations, such as DRAM and cache memory structures, resulting in unrealistic attacks that demand the manipulation of an excessive number of parameters and bits. In this paper, we present TrojBits, a novel approach requiring minimal bit-flips to effectively insert Trojans into real-world Transformer language model systems. This is achieved through a three-module framework designed to efficiently target Transformer-based language models, consisting of Vulnerable Parameters Ranking (VPR), Hardware-aware Attack Optimization (HAO), and Vulnerable Bits Pruning (VBP). Within the VPR module, we are the first to employ Gradient-guided Fisher information to identify the most susceptible Transformer parameters, specifically in the word embedding layer. The HAO module then redistributes these parameters across multiple triggers, conforming to hardware constraints by incorporating a regularization term in the trojan optimization methodology. Finally, the VBP module aims to reduce the number of bit-flips by discarding less significant bits. We evaluate TrojBits on two representative NLP models, BERT and XLNE, on three classification tasks (SST2, OffensEval, and AG’s News). Our results demonstrate that our TrojBits successfully achieves the inference-time attack with only 64 parameters out of 116 million and 90-bit flips while maintaining the model performance.",
        "link": "http://dx.doi.org/10.3233/faia230254"
    },
    {
        "id": 21581,
        "title": "Simplifying Paragraph-Level Question Generation via Transformer Language Models",
        "authors": "Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, Charibeth Cheng",
        "published": "2021",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-89363-7_25"
    },
    {
        "id": 21582,
        "title": "Neutral Group Prediction with Bidirectional Encoder Representations from Transformer",
        "authors": "Tazizur Rahman, Yang Sok Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4382826"
    },
    {
        "id": 21583,
        "title": "- Transformer Installation and Maintenance",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-55"
    },
    {
        "id": 21584,
        "title": "Dry-Type Transformers",
        "authors": "Charles W. Johnson",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-6"
    },
    {
        "id": 21585,
        "title": "Residual Adaptive Sparse Hybrid Attention Transformer for Image Super Resolution",
        "authors": "Hai Huan, Mingxuan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4750908"
    },
    {
        "id": 21586,
        "title": "Step-Voltage Regulators",
        "authors": "Craig A. Colopy",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-8"
    },
    {
        "id": 21587,
        "title": "Review for \"New family of transformer‐less quadratic buck‐boost converters with wide conversion ratio\"",
        "authors": "",
        "published": "2021-7-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13061/v1/review1"
    },
    {
        "id": 21588,
        "title": "Review for \"Identifying promising sequences for protein engineering using a deep transformer protein language model\"",
        "authors": "",
        "published": "2023-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26536/v1/review1"
    },
    {
        "id": 21589,
        "title": "Human action Recognition with Transformer based on Convolutional Features",
        "authors": "Chengcheng Shi, Shuxin Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs one of the key research directions in the field of computer vision, human action recognition has a wide range of practical application values and prospects. In the fields of video surveillance, human-computer interaction, sports analysis, and healthcare, human action recognition technology shows a broad application prospect and potential. However, the diversity and complexity of human actions bring many challenges, such as handling complex actions, distinguishing similar actions, coping with changes in viewing angle, and overcoming occlusion problems. To address the challenges, this paper proposes an innovative framework for human action recognition. The framework combines the latest pose estimation algorithms, pre-trained CNN models, and a Vision Transformer to build an efficient system. The first step involves utilizing the latest pose estimation algorithm to accurately extract human pose information from real RGB image frames. Then, a pre-trained CNN model is used to perform feature extraction on the extracted pose information. Finally, the Vision Transformer model is applied for fusion and classification operations on the extracted features. Experimental validation is conducted on two benchmark datasets, UCF 50 and UCF 101, to demonstrate the effectiveness and efficiency of the proposed framework. The applicability and limitations of the framework in different scenarios are further explored through quantitative and qualitative experiments, providing valuable insights and inspiration for future research.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3855469/v1"
    },
    {
        "id": 21590,
        "title": "Predicting Chemical Reaction Outcomes: A Grammar Ontology-based Transformer Framework",
        "authors": "Vipul Mann, Venkat Venkatasubramanian",
        "published": "No Date",
        "citations": 1,
        "abstract": "Discovering and designing novel materials is a challenging problem as it often requires searching a combinatorially large space of potential candidates. Evaluation of all candidates experimentally is typically infeasible as it requires great amounts of effort, time, expertise, and money. The ability to predict reaction outcomes without performing extensive experiments is, therefore, important. Towards that goal, we report an approach that uses context-free grammar (CFG) based representations of molecules in a neural machine translation framework. We formulate the reaction-prediction task as a machine translation problem that involves discovering the transformations from the source sequence (comprising the reactants and agents) to the target sequence (comprising the major product) in the reaction. The grammar ontology-based representation of molecules hierarchically incorporates rich molecular structure information that, in principle, should be valuable for modeling chemical reactions. We achieve an accuracy of 80.1% on a standard reaction dataset using a model characterized by only a fraction of the number of training parameters in other sequence-to-sequence models based works in this area. Moreover, 99% of the predictions made on the same reaction dataset were found to be syntactically valid. We conclude that CFGs-based ontological representations could be an efficient way of incorporating structural information, ensuring chemically valid predictions, and overcoming overfitting in complex machine learning architectures employed in reaction prediction tasks.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12985892.v1"
    },
    {
        "id": 21591,
        "title": "Sindiff: Spoken-to-Sign Language Generation Based Transformer Diffusion Model",
        "authors": "Wuyan Liang, Xiaolong Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4611530"
    },
    {
        "id": 21592,
        "title": "Transforming the transformer",
        "authors": "Subhashish Bhattacharya",
        "published": "2017-7",
        "citations": 58,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mspec.2017.7951721"
    },
    {
        "id": 21593,
        "title": "Joint Time and Frequency Transformer for Chinese Opera Classification",
        "authors": "Qiang Li, Beibei Hu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1582"
    },
    {
        "id": 21594,
        "title": "Improving Small Objects Detection using Transformer",
        "authors": "Shikha Dubey, Farrukh Olimov, Muhammad Aasim Rafique, Moongu Jeon",
        "published": "No Date",
        "citations": 1,
        "abstract": "General artificial intelligence is a trade-off between the inductive bias of an algorithm and its out-of-distribution generalization performance. The conspicuous impact of inductive bias is an unceasing trend of improved predictions in various problems in computer vision like object detection. Although a recently introduced object detection technique, based on transformers (DETR), shows results competitive to the conventional and modern object detection models, its accuracy deteriorates for detecting small-sized objects (in perspective). This study examines the inductive bias of DETR and proposes a normalized inductive bias for object detection using a transformer (SOF-DETR). It uses a lazy-fusion of features to sustain deep contextual information of objects present in the image. The features from multiple subsequent deep layers are fused with element-wise-summation and input to a transformer network for object queries that learn the long and short-distance spatial association in the image by the attention mechanism.<br>SOF-DETR uses a global set-based prediction for object detection, which directly produces a set of bounding boxes. The experimental results on the MS COCO dataset show the effectiveness of the added normalized inductive bias and feature fusion techniques by detecting more small-sized objects than DETR. <br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16921000.v2"
    },
    {
        "id": 21595,
        "title": "Estimation of residual flux in single phase transformer using dynamic modeling of two winding single phase transformer",
        "authors": "I Made Yulistya Negara, Daniar Fahmi, Dimas Anton Asfani, I Gusti Ngurah Satriyadi Hernanda, Ayyub Dhimastara Aji",
        "published": "2017-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isitia.2017.8124072"
    },
    {
        "id": 21596,
        "title": "Research on Face Attractiveness Prediction Method Based on Visual Transformer",
        "authors": "建安 方",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2022.124117"
    },
    {
        "id": 21597,
        "title": "MOT: A Multi-Omics Transformer for Multiclass Classification Tumour Types Predictions",
        "authors": "Mazid Osseni, Prudencio Tossou, François Laviolette, Jacques Corbeil",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011780100003414"
    },
    {
        "id": 21598,
        "title": "Self-Modularized Transformer: Learn to Modularize Networks for Systematic Generalization",
        "authors": "Yuichi Kamata, Moyuru Yamada, Takayuki Okatani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011682100003417"
    },
    {
        "id": 21599,
        "title": "Emotion Transformer: Attention Model for Pose-Based Emotion Recognition",
        "authors": "Pedro Paiva, Josué Ramos, Marina Gavrilova, Marco Carvalho",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011791700003417"
    },
    {
        "id": 21600,
        "title": "The Development of Microcontroller-based Electrostatic Air Filter Device using Flyback Transformer",
        "authors": "Nasruddin M. Noer, Harris Fadhilla Said, Zikri Noer, Siti Utari Rahayu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010200500002775"
    },
    {
        "id": 21601,
        "title": "Colorformer: A Novel Colorization Method Based on a Transformer",
        "authors": "Hamza Shafiq, Truong Nguyen, Bumshik Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4703443"
    },
    {
        "id": 21602,
        "title": "Window Utilization, Magnet Wire and Insulation",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-4"
    },
    {
        "id": 21603,
        "title": "A new attempt at full-scale jump connectivity and Transformer",
        "authors": "luyan yin, haijun chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this research, we introduce SwinUnet3+, a pioneering algorithm that integrates Unet with Transformer, to facilitate the automatic segmentation of three primary tissues—subcutaneous fat layer, muscle, and intramuscular fat—in the thoracoabdominal region under challenging conditions, including subcutaneous soft tissue swelling, gas accumulation, artifacts, and fistulas. Our model showcases superior performance in body composition segmentation tasks, with improvements in DSC, IoU, sensitivity, and positive predictive value by 3.2%, 6.05%, 4.03%, and 2.34%, respectively. Notably, in segmenting subcutaneous fat, intramuscular fat, and muscle, SwinUnet3 + yielded the best outcomes. However, the model does exhibit certain limitations, such as a reliance on vast amounts of training data and potential challenges in handling certain image types. Additionally, high-resolution images may pose computational efficiency concerns. In conclusion, while SwinUnet3 + offers considerable advantages in complex medical image segmentation tasks, its limitations warrant acknowledgment. Future research will focus on addressing these challenges and enhancing the model's robustness and generalization capabilities.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3445505/v1"
    },
    {
        "id": 21604,
        "title": "Designing Inductors for a Given Resistance",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-26"
    },
    {
        "id": 21605,
        "title": "Emotion Recognition Using Transformer Network with Multiple 2d Photoplethysmogram Features",
        "authors": "Kamran Ali, Charles  E. Hughes",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4689294"
    },
    {
        "id": 21606,
        "title": "Decision letter for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v2/decision1"
    },
    {
        "id": 21607,
        "title": "Contents",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-toc"
    },
    {
        "id": 21608,
        "title": "Induced Heating In Steel-Beam Reinforced Ceilings Over Three-Phase Transformer Delta Closures",
        "authors": "Peter F Ryff",
        "published": "No Date",
        "citations": 0,
        "abstract": "Due to eddy currents, high temperatures often occur in structural parts adjacent to conductors carrying large currents. This paper deals with this problem and specifically with the minimum safe height and orientation of steel beams in a reinforced concrete ceiling above a three-phase transformer delta closure.",
        "link": "http://dx.doi.org/10.32920/14639661"
    },
    {
        "id": 21609,
        "title": "Transformer-based dimensionality reduction",
        "authors": "Ruisheng Ran, Tianyu Gao, Wenfeng Zhang, Shunshun Peng, Bin Fang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecently, Transformer is much popular and plays an important role in the fields of Machine Learning (ML), Natural Language Processing (NLP), and Computer Vision (CV), etc. In this paper, based on the Vision Transformer (ViT) model, a new dimensionality reduction (DR) model is proposed, named Transformer-DR. From data visualization, image reconstruction and face recognition, the representation ability of Transformer-DR after dimensionality reduction is studied, and it is compared with some representative DR methods to understand the difference between Transformer-DR and existing DR methods. The experimental results show that Transformer-DR is an effective dimensionality reduction method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2417990/v1"
    },
    {
        "id": 21610,
        "title": "Study and Implementation of High Frequency Cast Resin Transformer Applied for Medium-Voltage Solid-State Transformer",
        "authors": "Kuan-Ting Chen, Jiann-Fuh Chen, Tsung-Jen Wang, Hsuan Liao",
        "published": "2021-11-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ifeec53238.2021.9662005"
    },
    {
        "id": 21611,
        "title": "Smooth Embedding and Word Sampling Research Based on Transformer Pointer Generation Network",
        "authors": "Meiwei Zhang,  ",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2021.11.3.1045"
    },
    {
        "id": 21612,
        "title": "Number of Attention Heads vs. Number of Transformer-encoders in Computer Vision",
        "authors": "Tomas Hrycej, Bernhard Bermeitinger, Siegfried Handschuh",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011578000003335"
    },
    {
        "id": 21613,
        "title": "Bispectral Pedestrian Detection Augmented with Saliency Maps using Transformer",
        "authors": "Mohamed Marnissi, Ikram Hattab, Hajer Fradi, Anis Sahbani, Najoua Ben Amara",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010913000003124"
    },
    {
        "id": 21614,
        "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
        "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang",
        "published": "2021",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.acl-short.107"
    },
    {
        "id": 21615,
        "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
        "authors": "Xuran Pan, Tianzhu Ye, Zhuofan Xia, Shiji Song, Gao Huang",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00207"
    },
    {
        "id": 21616,
        "title": "A Scalable Electronic-Embedded Transformer, a New Concept Toward Ultra-High-Frequency High-Power Transformer in DC–DC Converters",
        "authors": "Yuliang Cao, Khai Ngo, Dong Dong",
        "published": "2023-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2023.3279259"
    },
    {
        "id": 21617,
        "title": "Magnetism and Related Core Issues",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-3"
    },
    {
        "id": 21618,
        "title": "Failure Analysis of Transformer Brackets",
        "authors": "Melissa Kurtz",
        "published": "2019-12-1",
        "citations": 0,
        "abstract": "Abstract\nSix transformer brackets failed in service, sending a group of three pole-mounted transformers to the ground below. The brackets were made from acrylonitrile-butadiene-styrene (ABS) resin and had been in service for more than 30 years. Remnants of the fractured brackets were analyzed using optical and scanning electron microscopy, Fourier transform infrared spectroscopy (FTIR), and thermogravimetric analysis (TGA). The exterior surfaces of all six brackets were alike and shared similar features, including witness marks, discoloration, mechanical deformation, and secondary cracking, along with crack networks. Both FTIR and TGA analyses indicated that the surface material was in a highly degraded state, likely due to weathering and thermal and ultraviolet exposure. This, in turn, led to the formation of cracks that propagated under the cyclic forces of vibration and wind. As the cracks grew larger, the weight of the transformer eventually overloaded the brackets, resulting in failure.",
        "link": "http://dx.doi.org/10.31399/asm.fach.v03.c9001767"
    },
    {
        "id": 21619,
        "title": "No-Load and Load Losses",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-16"
    },
    {
        "id": 21620,
        "title": "DC Inductor Design, Using Gapped Cores",
        "authors": "",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b10865-8"
    },
    {
        "id": 21621,
        "title": "Phase-Shifting and Zigzag Transformers",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-8"
    },
    {
        "id": 21622,
        "title": "Efficient Video Enhancement Transformer",
        "authors": "Florin Vasluianu, Radu Timofte",
        "published": "2022-10-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897883"
    },
    {
        "id": 21623,
        "title": "Converter Transformer Winding Mechanical Condition Detection Using Online Vibration Frequency Response Analysis Method",
        "authors": "Shuyu Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The harmonic current and vibration of the converter transformer are\neffective information for detecting winding mechanical faults. This\npaper proposes an online vibration frequency response analysis (VFRA)\nmethod and constructs a novel vibration frequency response function\n(VFRF), where the VFRF is defined as the ratio of vibration acceleration\nto current squared. A harmonic loading system simulating the actual\noperating situation of the converter transformer is established, and it\nis verified by numerical analysis and experimental methods that online\nVFRA is a simpler and more convenient method for winding mechanical\ncondition detection than offline VFRA. The effects of different factors\nsuch as current magnitude and harmonic content on the online VFRA are\ninvestigated, and the converter transformer winding mechanical condition\nis detected by combining the VFRF trace with two numerical indices of\nrelative factor and improved expectation. Laboratory experiments\ndemonstrate that the online VFRA has high stability and successfully\nidentify winding looseness faults of different degrees. Compared with\nthe traditional frequency response analysis (FRA) and short-circuit\nimpedance (SCI) methods, the online VFRA has higher sensitivity and\ndetection capability for winding mechanical faults, and is a better\nmethod for online monitoring of winding mechanical condition.",
        "link": "http://dx.doi.org/10.22541/au.167357499.94255422/v1"
    },
    {
        "id": 21624,
        "title": "Front Matter",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.fmatter"
    },
    {
        "id": 21625,
        "title": "CRE: Circle relationship embedding of patches in vision transformer",
        "authors": "Zhengyang Yu, Jochen Triesch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-75"
    },
    {
        "id": 21626,
        "title": "Transformer le périph’ en boulevard urbain ?",
        "authors": "",
        "published": "2019-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/turb.135.0001"
    },
    {
        "id": 21627,
        "title": "Three-Phase Transformer Winding Deformation Diagnostics using Terminal Capacitance Measurements",
        "authors": "Prasad Joshi, Shrikrishna V. Kulkarni",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00035"
    },
    {
        "id": 21628,
        "title": "The Development of Microcontroller-based Electrostatic Air Filter Device using Flyback Transformer",
        "authors": "Nasruddin M. Noer, Harris Fadhilla Said, Zikri Noer, Siti Utari Rahayu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010200500002775"
    },
    {
        "id": 21629,
        "title": "Multi-time Scale Frequency Regulation of a General Resonant DC Transformer in Hybrid AC/DC Microgrid",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_8"
    },
    {
        "id": 21630,
        "title": "Design, analysis and optimization for transformer windings in megawatt medium frequency DC transformer",
        "authors": "J. Hu, X. Zhou, B. Cui, M. Guo, Y. Ma, N. Jia, B. Zhao, B. Tang, M. Sun, L. Yin, L. Dong",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.1393"
    },
    {
        "id": 21631,
        "title": "IACT: Intensive Attention in Convolution-Transformer Network for Facial Landmark Localization",
        "authors": "Zhanyu Gao, Kai Chen, Dahai Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011740900003417"
    },
    {
        "id": 21632,
        "title": "A comparative review of different transformer modelling methods in TRV studies in case of transformer limited faults",
        "authors": "Ashkan Teymouri, Behrooz Vahidi, Morteza Eslamian",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jestch.2018.11.015"
    },
    {
        "id": 21633,
        "title": "Healthcare Transformer 2: Culture",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-14"
    },
    {
        "id": 21634,
        "title": "Token-Word Mixer Meets Object-Aware Transformer for Referring Image Segmentation",
        "authors": "Zhenliang Zhang, Zhu Teng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4695323"
    },
    {
        "id": 21635,
        "title": "Transformer Losses",
        "authors": "",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119866725.ch12"
    },
    {
        "id": 21636,
        "title": "Supplemental Images",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ins"
    },
    {
        "id": 21637,
        "title": "Transformer Behavior Under Lightning Surge",
        "authors": "Khalil Denno",
        "published": "2018-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351073219-6"
    },
    {
        "id": 21638,
        "title": "Conclusion",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-11"
    },
    {
        "id": 21639,
        "title": "Healthcare Transformer 1: Leadership",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-15"
    },
    {
        "id": 21640,
        "title": "Induced Heating In Steel-Beam Reinforced Ceilings Over Three-Phase Transformer Delta Closures",
        "authors": "Peter F Ryff",
        "published": "No Date",
        "citations": 0,
        "abstract": "Due to eddy currents, high temperatures often occur in structural parts adjacent to conductors carrying large currents. This paper deals with this problem and specifically with the minimum safe height and orientation of steel beams in a reinforced concrete ceiling above a three-phase transformer delta closure.",
        "link": "http://dx.doi.org/10.32920/14639661.v1"
    },
    {
        "id": 21641,
        "title": "Hierarchical Multi-Scale Learning Transformer for Video-Based Person Re-Identification",
        "authors": "Yingjie Zhu, Wenzhong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4632623"
    },
    {
        "id": 21642,
        "title": "Transformer les établissements en Afrique",
        "authors": " ",
        "published": "2018-2-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/4165cd7b-fr"
    },
    {
        "id": 21643,
        "title": "Signal as Token: Robust DOA Estimation in Complex Environments Aidded by Transformer",
        "authors": "Ziqi Wang, ZiHan Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Traditional DOA estimation methods include beamforming, maximum likelihood estimation, subspace-based methods and the sparsity-inducing methods, and DOA estimation is made by establishing the relationship between the received signal and the geometric characteristics of the array. However, factors such as low signal-to-noise ratio, low snapshot, array errors, coherent signals, and broadband signals can seriously affect the performance of these methods. Existing improved methods, such as spatial smoothing and compressed sensing to deal with coherent signal sources, and band division technology to deal with broadband signal sources, are often at the expense of resolution. Besides that, traditional methods tend to be poorly extrapolated and fail to make satisfactory estimates in complex situations. In order to deal with the above problems, some studies have proposed machine learning methods and deep learning methods to estimate DOA. However, the generalization ability of machine learning methods is weaker than that of deep learning methods, and most of them only use synthetic data for experiments, which cannot guarantee the performance in practical applications. Most deep learning methods model DOA estimation as a classification problem on grids, which limits the accuracy of estimation results. If the accuracy is to be increased, the grids have to be finer, which significantly increases the computational cost. Like the above machine learning methods, most deep learning methods do not give experimental results on measured data.</p>\n<p>This paper proposes a novel DOA estimation method based on the Transformer model to solve the DOA estimation problem. Firstly, compared with the traditional Transformer, the model in this paper adds a sensor-based attention mechanism specially designed for DOA estimation. This method abandons the previous grid classification, and directly regards the DOA estimation problem as a regression problem to minimize the error. It can be proved through strict mathematical derivation that its output can be decomposed by pseudo-singular value, and the eigenvalue matrix is the same as that of the MUSIC method, which means that the output of the proposed attention module is in the space spanned by the (projected) signal and noise eigenvectors. If the eigenvalue is large, the spanned space is dominated by the corresponding eigenvector, which forces the model to concentrate on the vital eigenvectors. Secondly, the complexity of the sensor-based attention mechanism is significantly reduced compared with the original attention mechanism, from O(<em>N2)</em> to O(<em>M2</em>), where <em>N</em> is the number of snapshots, <em>M</em> is the number of sensors. Thirdly, we conducted simulation experiments including low signal-to-noise ratio, low snapshot, array errors, coherent signal and broadband signal scenarios, and the results show that our method has good adaptability to various scenarios. Fourthly, in order to verify the practical application ability of our model, we carried out migration and testing on the measured data, and the results show that our method still has a good effect. Fifthly, in order to cope with possible environmental changes in practical applications, we specially set up a generalization setting experiment. This experiment mainly explores the generalization ability of the model for unknown scenarios, including the generalization situation under different signal-to-noise ratios and different array error strengths, and satisfactory results have been achieved. Finally, since our model needs to know the number of sources in advance, and the number of sources is sometimes unknown in reality, we slightly modify the DOA estimation model, changing the regression head to the classification head to realize the estimation of the number of sources. The results show that the average estimation accuracy is about 98%, which further enhance the application capabilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24166209"
    },
    {
        "id": 21644,
        "title": "TALE: Transformer-based protein function Annotation with joint sequence–Label Embedding",
        "authors": "Yue Cao, Yang Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotivationFacing the increasing gap between high-throughput sequence data and limited functional insights, computational protein function annotation provides a high-throughput alternative to experimental approaches. However, current methods can have limited applicability while relying on data besides sequences, or lack generalizability to novel sequences, species and functions.ResultsTo overcome aforementioned barriers in applicability and generalizability, we propose a novel deep learning model, named Transformer-based protein function Annotation through joint sequence–Label Embedding (TALE). For generalizbility to novel sequences we use self attention-based transformers to capture global patterns in sequences. For generalizability to unseen or rarely seen functions, we also embed protein function labels (hierarchical GO terms on directed graphs) together with inputs/features (sequences) in a joint latent space. Combining TALE and a sequence similarity-based method, TALE+ outperformed competing methods when only sequence input is available. It even outperformed a state-of-the-art method using network information besides sequence, in two of the three gene ontologies. Furthermore, TALE and TALE+ showed superior generalizability to proteins of low homology and never/rarely annotated novel species or functions compared to training data, revealing deep insights into the protein sequence–function relationship. Ablation studies elucidated contributions of algorithmic components toward the accuracy and the generalizability.AvailabilityThe data, source codes and models are available at https://github.com/Shen-Lab/TALEContactyshen@tamu.eduSupplementary informationSupplementary data are available at Bioinformatics online.",
        "link": "http://dx.doi.org/10.1101/2020.09.27.315937"
    },
    {
        "id": 21645,
        "title": "Surface Defect Classification with Vision Transformer",
        "authors": "Jihai Zhao",
        "published": "2022-10-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icid57362.2022.9969746"
    },
    {
        "id": 21646,
        "title": "Gaze Estimation using Transformer",
        "authors": "Yihua Cheng, Feng Lu",
        "published": "2022-8-21",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956687"
    },
    {
        "id": 21647,
        "title": "Surge Phenomena in Transformers",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-7"
    },
    {
        "id": 21648,
        "title": "Transformer-based Uplink Resource Prediction Model for 5G Dual Connectivity System",
        "authors": "Jewon Jung, Sugi Lee, Jaemin Shin, Yusung Kim",
        "published": "2023-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2023.29.1.44"
    },
    {
        "id": 21649,
        "title": "Parameter Design for Symmetrical CLLC-Type DC Transformer Considering Cascaded System Stability and Power Efficiency",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_6"
    },
    {
        "id": 21650,
        "title": "A Prediction Method for Institutional Reserve Based on LSTM, Transformer, and LightGBM",
        "authors": "乃庚 冀",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2024.142025"
    },
    {
        "id": 21651,
        "title": "PU-Transformer: Point Cloud Upsampling Transformer",
        "authors": "Shi Qiu, Saeed Anwar, Nick Barnes",
        "published": "2023",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-26319-4_20"
    },
    {
        "id": 21652,
        "title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
        "authors": "Dipesh Gyawali, Jian Zhang, Bijaya Karki",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012424500003660"
    },
    {
        "id": 21653,
        "title": "Problems",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-17"
    },
    {
        "id": 21654,
        "title": "Transformer Design",
        "authors": "Robert W. Erickson, Dragan Maksimović",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-43881-4_12"
    },
    {
        "id": 21655,
        "title": "Heartbeat Classification Method Combining Multi-Branch Convolutional Neural Networks and Transformer",
        "authors": "Feiyan Zhou, Jiannan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4540757"
    },
    {
        "id": 21656,
        "title": "Voltage Breakdown Theory and Practice",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-14"
    },
    {
        "id": 21657,
        "title": "Decision letter for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": "",
        "published": "2023-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v1/decision1"
    },
    {
        "id": 21658,
        "title": "On Transformer Autoregressive Decoding for Multivariate Time Series Forecasting",
        "authors": "Mohammed Aldosari, John Miller",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-171"
    },
    {
        "id": 21659,
        "title": "Multiscale Spectral-Spatial Convolutional Transformer for Hyperspectral Image Classification",
        "authors": "Zhiqiang Gong, Xian Zhou, Wen Yao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4668897"
    },
    {
        "id": 21660,
        "title": "Signal as Token: Robust DOA Estimation in Complex Environments Aidded by Transformer",
        "authors": "Ziqi Wang, ZiHan Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Traditional DOA estimation methods include beamforming, maximum likelihood estimation, subspace-based methods and the sparsity-inducing methods, and DOA estimation is made by establishing the relationship between the received signal and the geometric characteristics of the array. However, factors such as low signal-to-noise ratio, low snapshot, array errors, coherent signals, and broadband signals can seriously affect the performance of these methods. Existing improved methods, such as spatial smoothing and compressed sensing to deal with coherent signal sources, and band division technology to deal with broadband signal sources, are often at the expense of resolution. Besides that, traditional methods tend to be poorly extrapolated and fail to make satisfactory estimates in complex situations. In order to deal with the above problems, some studies have proposed machine learning methods and deep learning methods to estimate DOA. However, the generalization ability of machine learning methods is weaker than that of deep learning methods, and most of them only use synthetic data for experiments, which cannot guarantee the performance in practical applications. Most deep learning methods model DOA estimation as a classification problem on grids, which limits the accuracy of estimation results. If the accuracy is to be increased, the grids have to be finer, which significantly increases the computational cost. Like the above machine learning methods, most deep learning methods do not give experimental results on measured data.</p>\n<p>This paper proposes a novel DOA estimation method based on the Transformer model to solve the DOA estimation problem. Firstly, compared with the traditional Transformer, the model in this paper adds a sensor-based attention mechanism specially designed for DOA estimation. This method abandons the previous grid classification, and directly regards the DOA estimation problem as a regression problem to minimize the error. It can be proved through strict mathematical derivation that its output can be decomposed by pseudo-singular value, and the eigenvalue matrix is the same as that of the MUSIC method, which means that the output of the proposed attention module is in the space spanned by the (projected) signal and noise eigenvectors. If the eigenvalue is large, the spanned space is dominated by the corresponding eigenvector, which forces the model to concentrate on the vital eigenvectors. Secondly, the complexity of the sensor-based attention mechanism is significantly reduced compared with the original attention mechanism, from O(<em>N2)</em> to O(<em>M2</em>), where <em>N</em> is the number of snapshots, <em>M</em> is the number of sensors. Thirdly, we conducted simulation experiments including low signal-to-noise ratio, low snapshot, array errors, coherent signal and broadband signal scenarios, and the results show that our method has good adaptability to various scenarios. Fourthly, in order to verify the practical application ability of our model, we carried out migration and testing on the measured data, and the results show that our method still has a good effect. Fifthly, in order to cope with possible environmental changes in practical applications, we specially set up a generalization setting experiment. This experiment mainly explores the generalization ability of the model for unknown scenarios, including the generalization situation under different signal-to-noise ratios and different array error strengths, and satisfactory results have been achieved. Finally, since our model needs to know the number of sources in advance, and the number of sources is sometimes unknown in reality, we slightly modify the DOA estimation model, changing the regression head to the classification head to realize the estimation of the number of sources. The results show that the average estimation accuracy is about 98%, which further enhance the application capabilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24166209.v1"
    },
    {
        "id": 21661,
        "title": "Signal as Token: Robust DOA Estimation in Complex Environments Aidded by Transformer",
        "authors": "Ziqi Wang, ZiHan Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Traditional DOA estimation methods include beamforming, maximum likelihood estimation, subspace-based methods and the sparsity-inducing methods, and DOA estimation is made by establishing the relationship between the received signal and the geometric characteristics of the array. However, factors such as low signal-to-noise ratio, low snapshot, array errors, coherent signals, and broadband signals can seriously affect the performance of these methods. Existing improved methods, such as spatial smoothing and compressed sensing to deal with coherent signal sources, and band division technology to deal with broadband signal sources, are often at the expense of resolution. Besides that, traditional methods tend to be poorly extrapolated and fail to make satisfactory estimates in complex situations. In order to deal with the above problems, some studies have proposed machine learning methods and deep learning methods to estimate DOA. However, the generalization ability of machine learning methods is weaker than that of deep learning methods, and most of them only use synthetic data for experiments, which cannot guarantee the performance in practical applications. Most deep learning methods model DOA estimation as a classification problem on grids, which limits the accuracy of estimation results. If the accuracy is to be increased, the grids have to be finer, which significantly increases the computational cost. Like the above machine learning methods, most deep learning methods do not give experimental results on measured data.</p>\n<p>This paper proposes a novel DOA estimation method based on the Transformer model to solve the DOA estimation problem. Firstly, compared with the traditional Transformer, the model in this paper adds a sensor-based attention mechanism specially designed for DOA estimation. This method abandons the previous grid classification, and directly regards the DOA estimation problem as a regression problem to minimize the error. It can be proved through strict mathematical derivation that its output can be decomposed by pseudo-singular value, and the eigenvalue matrix is the same as that of the MUSIC method, which means that the output of the proposed attention module is in the space spanned by the (projected) signal and noise eigenvectors. If the eigenvalue is large, the spanned space is dominated by the corresponding eigenvector, which forces the model to concentrate on the vital eigenvectors. Secondly, the complexity of the sensor-based attention mechanism is significantly reduced compared with the original attention mechanism, from O(<em>N2)</em> to O(<em>M2</em>), where <em>N</em> is the number of snapshots, <em>M</em> is the number of sensors. Thirdly, we conducted simulation experiments including low signal-to-noise ratio, low snapshot, array errors, coherent signal and broadband signal scenarios, and the results show that our method has good adaptability to various scenarios. Fourthly, in order to verify the practical application ability of our model, we carried out migration and testing on the measured data, and the results show that our method still has a good effect. Fifthly, in order to cope with possible environmental changes in practical applications, we specially set up a generalization setting experiment. This experiment mainly explores the generalization ability of the model for unknown scenarios, including the generalization situation under different signal-to-noise ratios and different array error strengths, and satisfactory results have been achieved. Finally, since our model needs to know the number of sources in advance, and the number of sources is sometimes unknown in reality, we slightly modify the DOA estimation model, changing the regression head to the classification head to realize the estimation of the number of sources. The results show that the average estimation accuracy is about 98%, which further enhance the application capabilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24166209.v2"
    },
    {
        "id": 21662,
        "title": "Foreground-aware Transformer Network for Person Re-identification",
        "authors": "Guifang Zhang, Shijun Tan, Yuming Fang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer-based person re-identification (person ReID) technologies tend to capture global information, but only focus on global features and ignore the interference of irrelevant information. To the best of our knowledge, most foreground information corresponds to pedestrian(in the person ReID datasets), enhancing foreground information or weakening background information helps to distinguish the person from the background. From this insight, we proposed a foreground-aware transformer network to achieve the task of person ReID. To make the most of foreground information for person identification, we isolate the foreground by minimizing the impact of background interference and introduce a foreground-aware loss function. This loss function directs the attention of networks toward the primary foreground information in the image, optimizing its ability to identify pedestrian. To prove the effectiveness of our proposed foreground-aware transformer network, we conducted experiments on Market1501 and MSMT17 datasets. Our experimental results indicate that the proposed method can yield substantial improvements in person ReID accuracy, demonstrating the practical value of our foreground-aware transformer network in addressing real-world person ReID challenges.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3704700/v1"
    },
    {
        "id": 21663,
        "title": "The impact of smart transformer on different radial distribution systems",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2021.136983"
    },
    {
        "id": 21664,
        "title": "Image2SMILES: Transformer-based Molecular Optical Recognition Engine",
        "authors": "Ivan Khokhlov, Lev Krasnov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 2,
        "abstract": "The rise of deep learning in various scientific and technology areas promotes the development of AI-based tools for information retrieval. Optical recognition of organic structures is a key part of the automated extraction of chemical information. However, this is a challenging task because there is a large variety of representation styles. In this research, we present a Transformer-based artificial neural network to convert images of organic structures to molecular structures. To train the model, we created a comprehensive data generator that stochastically simulates various drawing styles, functional groups, functional group placeholders (R-groups), and visual contamination. We demonstrate that the Transformer-based architecture can gather chemical insights from our generator with almost absolute confidence. That means that, with Transformer, one can fully concentrate on data simulation to build a good recognition model. A web demo of our optical recognition engine is available online at Syntelly platform.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14602716.v1"
    },
    {
        "id": 21665,
        "title": "Anticipative Video Transformer",
        "authors": "Rohit Girdhar, Kristen Grauman",
        "published": "2021-10",
        "citations": 75,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01325"
    },
    {
        "id": 21666,
        "title": "Major Drivers of Long-Term Distribution Transformer Demand",
        "authors": "Killian McKenna, Sherin Ann Abraham, Wenbo Wang",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2309697"
    },
    {
        "id": 21667,
        "title": "Frontmatter",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-fm"
    },
    {
        "id": 21668,
        "title": "Index",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-012"
    },
    {
        "id": 21669,
        "title": "PL-Transformer: a POS-aware and layer ensemble transformer for text classification",
        "authors": "Yu Shi, Xi Zhang, Ning Yu",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00521-022-07872-4"
    },
    {
        "id": 21670,
        "title": "Salient Mask-Guided Vision Transformer for Fine-Grained Classification",
        "authors": "Dmitry Demidov, Muhammad Sharif, Aliakbar Abdurahimov, Hisham Cholakkal, Fahad Khan",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011611100003417"
    },
    {
        "id": 21671,
        "title": "Prosit Transformer: A transformer for Prediction of MS2 Spectrum Intensities",
        "authors": "Markus Ekvall, Patrick Truong, Wassim Gabriel, Mathias Wilhelm, Lukas Käll",
        "published": "2022-5-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/acs.jproteome.1c00870"
    },
    {
        "id": 21672,
        "title": "Healthcare Transformer 4: Transparency",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-12"
    },
    {
        "id": 21673,
        "title": "Fine-Grained Sentiment Analysis of College Email Texts Based on Transformer",
        "authors": "Jiangning Xie, Zhen Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4529274"
    },
    {
        "id": 21674,
        "title": "Traffic volume prediction on highway network with mix-of-expert Transformer",
        "authors": "Zhou (Eric) Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAccurate traffic volume prediction is essential for effective traffic management and urban planning. This paper introduces a novel approach, the Mix-of-Expert Transformer model, for traffic volume prediction on highway networks. Integrating the Mix-of-Expert concept within the Transformer architecture, our model leverages expert specialization and adaptive attention mechanisms to capture complex spatiotemporal dependencies in traffic data. Through comprehensive experiments, including comparisons with traditional time-series models, machine learning approaches, and state-of-the-art Transformer-based methods, we demonstrate the superior performance and computational efficiency of the proposed model. The Mix-of-Expert Transformer consistently outperforms baseline models, showcasing its potential as a valuable tool for optimizing traffic flow and urban transportation systems. The model's versatility and efficiency make it a promising solution for real-world applications in traffic prediction, paving the way for further advancements in intelligent transportation systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3753948/v1"
    },
    {
        "id": 21675,
        "title": "Long-term tracking with Transformer and Template update",
        "authors": "Zhang Hongying, Peng Xiaowen, Wang Xuyong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAiming at the tracking failure due to the disappearance of the target in the long-term target tracking process, this paper proposes a long-term target tracking network based on visual transformer and template update. First of all, Deit based on the transformer is utilized for feature extraction, and local features with their global dependencies are efficiently extracted by the distillation strategy. Secondly, the encoder of the modeling transformer is used to fully integrate the target features with the features in the search area, and the decoder is used to learn the location information in the target query. Then, target predictions are performed on the information from the encoder-decoder to obtain tracking results. Finally, the reliability of the dynamic template is judged by the score prediction head before tracking the next frame. We perform extensive experiments on the LaSOT、VOT2021-LT、TrackingNet and UAV123 datasets. The experimental results demonstrate the effectiveness of our model in long-term tracking.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1941223/v1"
    },
    {
        "id": 21676,
        "title": "Transformer performance enhancement by optimized charging strategy for electric vehicles",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2022.140196"
    },
    {
        "id": 21677,
        "title": "“Walk on the Wild Side”",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0012"
    },
    {
        "id": 21678,
        "title": "L'art martial du leadership",
        "authors": "",
        "published": "2022-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/inter.minde.2022.01"
    },
    {
        "id": 21679,
        "title": "Se former pour se transformer",
        "authors": "Brigitte Hérisson",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/s0038-0814(20)30123-7"
    },
    {
        "id": 21680,
        "title": "Reliability of an Automatic Adjustable Current Transformer",
        "authors": "Shavkat Mukhsimov",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3779246"
    },
    {
        "id": 21681,
        "title": "Inductive inference of novel protein-molecule interactions using Heterogeneous Graph Transformer (HGT) AutoEncoder",
        "authors": "Alberto Arrigoni",
        "published": "No Date",
        "citations": 0,
        "abstract": "1AbstractProtein-molecule interactions are promoted by the physicochemical characteristics of the actors involved, but structural information alone does not capture expression patterns, localization and pharmacokinetics. In this work we propose an integrative strategy for protein-molecule interaction discovery that combines different layers of information through the use of convolutional operators on graph, and frame the problem as missing link prediction task on an heterogeneous graph constituted by three node types: 1) molecules 2) proteins 3) diseases. Physicochemical information of the actors are encoded using shallow embedding techniques (SeqVec, Mol2Vec, Doc2Vec respectively) and are supplied as feature vectors to a Graph AutoEncoer (GAE) that uses a Heterogeneous Graph Transformer (HGT) in the encoder module. We show in this work that HGT Autoencoder can be used to accurately recapitulate the proteinmolecule interactions set and propose novel relationships in inductive settings that are grounded in biological and functional information extracted from the graph.",
        "link": "http://dx.doi.org/10.1101/2021.12.20.472904"
    },
    {
        "id": 21682,
        "title": "Transformer, Reacfor, and Capacitor Characteristics",
        "authors": "J. Lewis Blackburn",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222356-9"
    },
    {
        "id": 21683,
        "title": "Hierarchical Multi-Scale Learning Transformer for Video-Based Person Re-Identification",
        "authors": "Yingjie Zhu, Wenzhong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4598034"
    },
    {
        "id": 21684,
        "title": "Transformer Generator of High Current Pulses",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15372/pmtf20200217"
    },
    {
        "id": 21685,
        "title": "Efficient Transformer Fault Diagnosis Based on an Improved Slime Mold Algorithm",
        "authors": "Guomin Xie, Zhongbao Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4584693"
    },
    {
        "id": 21686,
        "title": "Challenges and Strategies in Transformer Design",
        "authors": "S.V. Kulkarni",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-5"
    },
    {
        "id": 21687,
        "title": "Reactance and Leakage Reactance Calculations",
        "authors": "",
        "published": "2017-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-5"
    },
    {
        "id": 21688,
        "title": "Systematischer Transformer",
        "authors": "Peter Hanser",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s41964-018-0196-4"
    },
    {
        "id": 21689,
        "title": "Fuck You Leave Me Alone Don’t Read My Book",
        "authors": "",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501323089.0003"
    },
    {
        "id": 21690,
        "title": "ITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations",
        "authors": "Shahabeddin Sotudian, Ioannis Ch. Paschalidis",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTPersonalized drug response prediction is an approach for tailoring effective therapeutic strategies for patients based on their tumors’ genomic characterization. The current study introduces a new listwise Learning-to-rank (LTR) model called Inversion Transformer-based Neural Ranking (ITNR). ITNR utilizes genomic features and a transformer architecture to decipher functional relationships and construct models that can predict patient-specific drug responses. Our experiments were conducted on three major drug response data sets, showing that ITNR reliably and consistently outperforms state-of-the-art LTR models.HighlightsThe proposed framework is a transformer-based model to predict drug responses using RNAseq gene expression profile, drug descriptors and drug fingerprints.ITNR utilizes a Context-Aware-Transformer architecture as its scoring function that ensures the modeling of inter-item dependencies.We introduced a novel loss function using the concept of Inversion and Approximate Permutation matrices.Our computational results indicated that our method leads to substantially improved performance when compared to the baseline methods across all performance metrics, which can lead to selecting highly effective personalized treatment.",
        "link": "http://dx.doi.org/10.1101/2023.03.16.533057"
    },
    {
        "id": 21691,
        "title": "A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images",
        "authors": "Yudong Zhang, Ge Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSingle particle tracking is an important image analysis technique widely used in biomedical sciences to follow the movement of sub-cellular structures, which typically appear as individual particles in fluorescence microscopy images. In practice, the low signal-to-noise ratio (SNR) of fluorescence microscopy images as well as the high density and complex movement of subcellular structures pose substantial technical challenges for accurate and robust tracking. In this paper, we propose a novel Transformer-based single particle tracking method called Motion Transformer Tracker (MoTT). By using its attention mechanism to learn complex particle behaviors from past and hypothetical future tracklets (i.e., fragments of trajectories), MoTT estimates the matching probabilities between each live/established tracklet and its multiple hypothesis tracklets simultaneously, as well as the existence probability and position of each live tracklet. Global optimization is then used to find the overall best matching for all live tracklets. For those tracklets with high existence probabilities but missing detections due to e.g., low SNRs, MoTT utilizes its estimated particle positions to substitute for the missed detections, a strategy we refer to as relinking in this study. Experiments have confirmed that this strategy substantially alleviates the impact of missed detections and enhances the robustness of our tracking method. Overall, our method substantially outperforms competing state-of-the-art methods on the ISBI Particle Tracking Challenge datasets. It provides a powerful tool for studying the complex spatiotemporal behavior of subcellular structures. The source code is publicly available athttps://github.com/imzhangyd/MoTT.git.",
        "link": "http://dx.doi.org/10.1101/2023.07.20.549804"
    },
    {
        "id": 21692,
        "title": "Rethinking position embedding methods in the Transformer architecture",
        "authors": "Xin Zhou, Zhaohui Ren, Tianzhuang Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the transformer architecture, as self-attention reads entire image patches at once, the context of the sequence between patches is omitted. Therefore, the position embedding method is employed to assist the self-attention layers in computing the ordering information of tokens. Although most works directly add the position vector into the corresponding token vector instead of concatenating, fewer papers give a detailed explanation except for dimensional reduction. Actually, the addition method makes no sense because token vectors and position vectors are different physical quantities and can not be simply added together. Thus, we investigate the discrepancy of learned position information in both embedding methods(concatenation and addition) and compare their performance for models. Experiments demonstrate that the concatenation method can learn more spatial information(horizontal, vertical, and angle) than the addition method and learn smoother position information in the multi-scale model. Also, an appropriate concatenating strategy can easily bring 0.1% to 0.4% performance gain for soft-of-the-art models without extra computation overhead.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2525471/v1"
    },
    {
        "id": 21693,
        "title": "Transformer Ageing",
        "authors": "",
        "published": "2017-8-7",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970"
    },
    {
        "id": 21694,
        "title": "Preface",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-001"
    },
    {
        "id": 21695,
        "title": "Performance evaluation of main transformer equipment suppliers for 500kV main transformer equipment",
        "authors": "Yinghan Jiang, litong Dong, yinghua Chen, yanqin Ge, mingxue Wang, jinliang Zhang",
        "published": "2021-2-1",
        "citations": 0,
        "abstract": "Abstract\nScientific evaluation of the main transformer equipment suppliers is the premise to ensure the cost control and safe operation of the power grid. Based on the analysis of various factors affecting the main transformer equipment supplier, the evaluation index of the main transformer equipment supplier is constructed. Considering the fuzziness of the evaluation index, the index weight is determined by AHP, and the evaluation model of main transformer equipment supplier based on fuzzy comprehensive evaluation is established. The example calculation shows that the fuzzy comprehensive evaluation method can fully reveal the evaluation index information and reflect the actual situation of main transformer equipment suppliers scientifically and reasonably.",
        "link": "http://dx.doi.org/10.1088/1755-1315/651/2/022047"
    },
    {
        "id": 21696,
        "title": "Design Methodology for Symmetric CLLC Resonant DC Transformer Considering Voltage Conversion Ratio, System Stability and Efficiency",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_7"
    },
    {
        "id": 21697,
        "title": "Design of Symmetrical CLLC Resonant DC Transformer Considering Voltage Transfer Ratio and Cascaded System Stability",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_5"
    },
    {
        "id": 21698,
        "title": "Determining Accuracy of Temperature Limit Change in Power Transformer Core Using Temperature-Time Parameter Method",
        "authors": "Nihat PAMUK",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "The efficient transmission of electrical energy depends on amplifying voltage values with power transformers. To obtain higher efficiency from transformers, the core and winding type of transformer, the geometric structure of the core, and the shaping techniques in the windings are changed. This requires modeling transformer windings with equivalent circuits and calculating the inductance and electrical parameters appropriately. In this study, two-dimensional (2D) finite element solutions with energy perturbation and flux-coupling methods are used. The correctness of the inductance values of transformer windings was established, and the design was performed, by considering the inductance and electrical parameter values, which are comparable to the energy perturbation and flux connection. However, when two-dimensional calculated fields are used, the flux coupling method requires less computation and gives numerically more accurate results than the energy perturbation method. So, it is concluded that the flux-coupling approach should be chosen as the preferred method for calculating the inductance and electrical parameters of transformer windings. The numerical properties and equivalence of energy perturbation and flux-connection methods, the “apparent” inductance value of the primary and secondary field windings of power transformer operating under transient conditions, using the temperature-time parameter method, are calculated and its accuracy is demonstrated.",
        "link": "http://dx.doi.org/10.34248/bsengineering.1239298"
    },
    {
        "id": 21699,
        "title": "The Effect of Transformer-Oil Aging on the Residual Service Life of the Transformer and Its Diagnostic Parameters",
        "authors": "A. S. Serebryakov, D. A. Semenov, E. A. Sbitnev, A. V. Sidorova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3103/s1068371223120143"
    },
    {
        "id": 21700,
        "title": "A Relative Positional Embedding Scheme for Transformer-Based Person Re-Identification",
        "authors": "Seong-Su Kim, Gyeonghwan Kim",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.9.1175"
    },
    {
        "id": 21701,
        "title": "Study and Implementation of High Frequency Cast Resin Transformer Applied for Medium-Voltage Solid-State Transformer",
        "authors": "Kuan-Ting Chen, Jiann-Fuh Chen, Tsung-Jen Wang, Hsuan Liao",
        "published": "2021-11-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ifeec53238.2021.9662005"
    },
    {
        "id": 21702,
        "title": "Smooth Embedding and Word Sampling Research Based on Transformer Pointer Generation Network",
        "authors": "Meiwei Zhang,  ",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2021.11.3.1045"
    },
    {
        "id": 21703,
        "title": "Number of Attention Heads vs. Number of Transformer-encoders in Computer Vision",
        "authors": "Tomas Hrycej, Bernhard Bermeitinger, Siegfried Handschuh",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011578000003335"
    },
    {
        "id": 21704,
        "title": "Transformer-based On-Offline Hybrid Reinforcement Learning for Locomotion Tasks",
        "authors": "Min-Kyo Kang, Jin-Hwan Kim, Jung-Hyeon Choi, Incheol Kim",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5302/j.icros.2022.22.0136"
    },
    {
        "id": 21705,
        "title": "The Proposed Robust Circuit Parameters Design for the CLLC-Type DC Transformer in the Hybrid AC/DC Microgrid",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_2"
    },
    {
        "id": 21706,
        "title": "Development of ultrahigh‐voltage standard voltage transformer based on series voltage transformer structure",
        "authors": "Zhou Feng, Jiang Chunyang, Lei Min, Lin Fuchang, Yang Shihai",
        "published": "2019-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/iet-smt.2018.5258"
    },
    {
        "id": 21707,
        "title": "Optimal sizing of the wind farm and wind farm transformer using MILP and dynamic transformer rating",
        "authors": "Andrea Molina Gómez, Kateryna Morozovska, Tor Laneryd, Patrik Hilber",
        "published": "2022-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2021.107645"
    },
    {
        "id": 21708,
        "title": "Miscellaneous Topics",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-10"
    },
    {
        "id": 21709,
        "title": "MI-CAT: A Transformer-Based Domain Adaptation Network for Motor",
        "authors": "Dongxue Zhang, Huiying Li, Jingmeng Xie",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4331172"
    },
    {
        "id": 21710,
        "title": "Ddformer:Deepfake Detection with Multimodal Fusion Transformer",
        "authors": "gao jiazhan, Deqi Huang, Jinlai Zhang, Eksan Firkat, Jihong Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673119"
    },
    {
        "id": 21711,
        "title": "ETEL Smart Distribution Transformer for Electric Vehicle Applications",
        "authors": "Bhaba Privo Das",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535795"
    },
    {
        "id": 21712,
        "title": "Differential expression of transformer 2 alpha homolog in cancers of the breast.",
        "authors": "Shahan Mamoor",
        "published": "No Date",
        "citations": 0,
        "abstract": "Breast cancer affects women at relatively high frequency (1).  We mined published microarray datasets (2, 3) to determine in an unbiased fashion and at the systems level genes most differentially expressed in the primary tumors of patients with breast cancer.  We report here significant differential expression of the gene encoding transformer 2 alpha homolog, TRA2A, when comparing primary tumors of the breast to the tissue of origin, the normal breast.  TRA2A was also differentially expressed in the tumor cells of patients with triple negative breast cancer.  TRA2A mRNA was present at significantly lower quantities in tumors of the breast as compared to normal breast tissue.  Analysis of human survival data revealed that expression of TRA2A in primary tumors of the breast was correlated with overall survival in patients with luminal B subtype cancer, demonstrating a relationship between primary tumor expression of a differentially expressed gene and patient survival outcomes influenced by PAM50 molecular subtype.  TRA2A may be of relevance to initiation, maintenance or progression of cancers of the female breast.",
        "link": "http://dx.doi.org/10.31219/osf.io/2ngvq"
    },
    {
        "id": 21713,
        "title": "A Generalized Dual-Frequency Transformer for Dual-Band Rf Rectifier",
        "authors": "Walid Zahra, Abdelkader Zerfaine, Tarek Djerafi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4623627"
    },
    {
        "id": 21714,
        "title": "Healthcare Transformer 3: Payment Reform",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-13"
    },
    {
        "id": 21715,
        "title": "Persistence of excitation in an online monitoring of transformer: A system identification perspective",
        "authors": "Shadab Nayyer Syed",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54985/peeref.2306p6632433"
    },
    {
        "id": 21716,
        "title": "BaSFormer: A Balanced Sparsity Regularized Attention Network for Transformer",
        "authors": "Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Attention networks often make decisions relying solely on a few pieces of tokens, even if those reliances are not truly indicative of the underlying meaning or intention of the full context. That can lead to over-fitting in Transformers and hinder their ability to generalize. Attention regularization and sparsity-based methods have been used to overcome this issue. However, these methods cannot guarantee that all tokens have sufficient receptive fields for global information inference. Thus, the impact of individual biases cannot be effectively reduced. As a result, the generalization of these approaches improved slightly from the training data to new data. To address these limitations, we proposed a balanced sparsity (BaS) regularized attention network on top of the Transformers, called BaSFormer. BaS regularization introduces the K-regular graph constraint on self-attention connections, which replaces SoftMax with SparseMax in the attention transformation. In BaS-regularized self-attentions, SparseMax assigns zero attention scores to low-scoring connections, highlighting influential and meaningful contexts. The K-regular graph constraint ensures that all tokens have an equal-sized receptive field to aggregate information, which facilitates the involvement of global tokens in the feature update of each layer and reduces the impact of individual biases. As no continuous loss can be used as the K-regular graph regularization, we proposed an exponential extremum loss with augmented Lagrangian. Experimental results show that BaSFormer improves debiasing effectiveness compared to the newest large language models, such as the chatGPT, GPT-4 and LLaMA. In addition, BaSFormer achieves new state-of-the-art results in text generation tasks. Interestingly, this paper also evaluates that BaSFormer can learn hierarchically linguistic dependencies in gradient attributions, which improves interpretability and adversarial robustness.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22824908"
    },
    {
        "id": 21717,
        "title": "Stray Losses in Structural Components",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-5"
    },
    {
        "id": 21718,
        "title": "Swintransentropy: A Swin-Transformer Based Entropy Model for Image Compression",
        "authors": "Yujin Zhang, Jixiang Cheng, Zhidan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4448834"
    },
    {
        "id": 21719,
        "title": "An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization",
        "authors": "Oguzhan Katar, Ozal Yildirim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Blood cell analysis is a crucial diagnostic process in medical practice. In particular, detecting white blood cells (WBCs) is essential for diagnosing of many diseases. The manual screening of blood films is a time-consuming and subjective process, which can lead to inconsistencies and errors. Therefore, automated detection of blood cells can improve the accuracy and efficiency of the screening process. In this study, an explainable Vision Transformer (ViT) model was proposed for the automatic detection of WBCs from blood films. The proposed model utilizes the self-attention mechanism to extract relevant features from the input images and leverages transfer learning by incorporating pre-trained model weights to improve its performance. The proposed model achieved a classification accuracy of 99.40% for five distinct types of WBCs and exhibited potential in reducing the time required for manual screening of blood films by pathologists. Upon examination of the misclassified test samples, it was observed that incorrect predictions were correlated with the presence or absence of granules in the cell samples. To validate this observation, the dataset was divided into two classes, namely Granulocytes and Agranulocytes, and a secondary training process was conducted. The resulting ViT model trained for binary classification achieved an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43% during the test phase. To ensure the reliability of the ViT model&#039;s multi-class classification of WBCs, the pixel areas that the model focuses on in its predictions are visualized through the Score-CAM algorithm.",
        "link": "http://dx.doi.org/10.20944/preprints202306.1106.v1"
    },
    {
        "id": 21720,
        "title": "A Driving Area Detection Algorithm Based on Improved Swin Transformer",
        "authors": "Ying Li, Shuang Liu, Huankun Sheng",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4425696"
    },
    {
        "id": 21721,
        "title": "Fonction achats et management des ressources externes",
        "authors": "",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.malm.2023.01"
    },
    {
        "id": 21722,
        "title": "BaSFormer: A Balanced Sparsity Regularized Attention Network for Transformer",
        "authors": "Shuoran Jiang, Qingcai Chen, Yang Xiang, Youcheng Pan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Attention networks often make decisions relying solely on a few pieces of tokens, even if those reliances are not truly indicative of the underlying meaning or intention of the full context. That can lead to over-fitting in Transformers and hinder their ability to generalize. Attention regularization and sparsity-based methods have been used to overcome this issue. However, these methods cannot guarantee that all tokens have sufficient receptive fields for global information inference. Thus, the impact of individual biases cannot be effectively reduced. As a result, the generalization of these approaches improved slightly from the training data to new data. To address these limitations, we proposed a balanced sparsity (BaS) regularized attention network on top of the Transformers, called BaSFormer. BaS regularization introduces the K-regular graph constraint on self-attention connections, which replaces SoftMax with SparseMax in the attention transformation. In BaS-regularized self-attentions, SparseMax assigns zero attention scores to low-scoring connections, highlighting influential and meaningful contexts. The K-regular graph constraint ensures that all tokens have an equal-sized receptive field to aggregate information, which facilitates the involvement of global tokens in the feature update of each layer and reduces the impact of individual biases. As no continuous loss can be used as the K-regular graph regularization, we proposed an exponential extremum loss with augmented Lagrangian. Experimental results show that BaSFormer improves debiasing effectiveness compared to the newest large language models, such as the chatGPT, GPT-4 and LLaMA. In addition, BaSFormer achieves new state-of-the-art results in text generation tasks. Interestingly, this paper also evaluates that BaSFormer can learn hierarchically linguistic dependencies in gradient attributions, which improves interpretability and adversarial robustness.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22824908.v1"
    },
    {
        "id": 21723,
        "title": "Enhancing Sentence Representation with Syntactically Structural Transformer",
        "authors": "Petrenko Elizabeth, Rodolfo Patel, Fomuso Jose",
        "published": "No Date",
        "citations": 0,
        "abstract": "The evolution of sentence representation learning, especially with parse tree encoders, has shown remarkable progress. Traditional approaches predominantly rely on recursive encoding of tree structures, which impedes parallel processing capabilities. Additionally, these methods often overlook the significance of dependency tree arc labels. To overcome these limitations, we introduce the Syntax-Enhanced Transformer (SET), incorporating a novel dual-attention mechanism that integrates relation-focused attention alongside traditional self-attention. This design effectively encodes both dependency and spatial positional relationships within sentence dependency trees. Our approach innovatively incorporates syntactic information into the Transformer framework without compromising its inherent parallelizability. The SET demonstrates superior or comparable performance to contemporary methods across various sentence representation tasks, significantly enhancing computational efficiency.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0600.v1"
    },
    {
        "id": 21724,
        "title": "Reviewer #2 (Public Review): Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.1.sa2"
    },
    {
        "id": 21725,
        "title": "eLife assessment: Transformer-based spatial–temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "Vitaly Ryu",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.3.sa0"
    },
    {
        "id": 21726,
        "title": "Reviewer #1 (Public Review): Transformer-based spatial–temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.3.sa1"
    },
    {
        "id": 21727,
        "title": "To enhance power quality in single phase system with critical loads using transformer-less active devices",
        "authors": "",
        "published": "2017-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23883/ijrter.2017.3535.vfdsw"
    },
    {
        "id": 21728,
        "title": "Visual Anomaly Detection and Localization with a Patch-Wise Transformer and Convolutional Model",
        "authors": "Afshin Dini, Esa Rahtu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011669400003417"
    },
    {
        "id": 21729,
        "title": "HAM-Transformer: A Hybrid Adaptive Multi-Scaled Transformer Net for Remote Sensing in Complex Scenes",
        "authors": "Keying Ren, Xiaoyan Chen, Zichen Wang, Xiwen Liang, Zhihui Chen, Xia Miao",
        "published": "2023-10-3",
        "citations": 1,
        "abstract": "The quality of remote sensing images has been greatly improved by the rapid improvement of unmanned aerial vehicles (UAVs), which has made it possible to detect small objects in the most complex scenes. Recently, learning-based object detection has been introduced and has gained popularity in remote sensing image processing. To improve the detection accuracy of small, weak objects in complex scenes, this work proposes a novel hybrid backbone composed of a convolutional neural network and an adaptive multi-scaled transformer, referred to as HAM-Transformer Net. HAM-Transformer Net firstly extracts the details of feature maps using convolutional local feature extraction blocks. Secondly, hierarchical information is extracted, using multi-scale location coding. Finally, an adaptive multi-scale transformer block is used to extract further features in different receptive fields and to fuse them adaptively. We implemented comparison experiments on a self-constructed dataset. The experiments proved that the method is a significant improvement over the state-of-the-art object detection algorithms. We also conducted a large number of comparative experiments in this work to demonstrate the effectiveness of this method.",
        "link": "http://dx.doi.org/10.3390/rs15194817"
    },
    {
        "id": 21730,
        "title": "Supervised Spatial Transformer Networks for Attention Learning in Fine-grained Action Recognition",
        "authors": "Dichao Liu, Yu Wang, Jien Kato",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007257803110318"
    },
    {
        "id": 21731,
        "title": "Impacts on DER: Transformer and Cable Reliability",
        "authors": "Ben Lanz, Alan Ross",
        "published": "2022-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic51169.2022.9833206"
    },
    {
        "id": 21732,
        "title": "Mécaniser et transformer l’agriculture en afrique",
        "authors": "Busani Bafana",
        "published": "2019-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/efa044e7-fr"
    },
    {
        "id": 21733,
        "title": "Sauvegarder et transformer l’école",
        "authors": "",
        "published": "2022-1-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/9789210012119c012"
    },
    {
        "id": 21734,
        "title": "Research and analysis based on transformer DC resistance measurement data",
        "authors": "Jing Wang",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5089082"
    },
    {
        "id": 21735,
        "title": "Chemformer: A Pre-Trained Transformer for Computational Chemistry",
        "authors": "Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Bjerrum",
        "published": "No Date",
        "citations": 1,
        "abstract": "Transformer models coupled with Simplified Molecular Line Entry System (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present Chemformer model – a Transformerbased model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top- 1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-v2pnn"
    },
    {
        "id": 21736,
        "title": "SYSTEM OF SMOOTH REGULATION BY VOLTAGE SUPPLEMENTARY TRANSFORMER",
        "authors": "Eldar Usmanov",
        "published": "2022-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32743/unitech.2022.104.11.14578"
    },
    {
        "id": 21737,
        "title": "Theory and Principles",
        "authors": "Dennis J. Allan, Harold Moore",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-1"
    },
    {
        "id": 21738,
        "title": "Comparative Analysis of Retinal Vessel Segmentation Utilising Convolutional and Transformer-Based Architectures",
        "authors": "Morteza Tavakol Sadrabadi, Hamed Agahi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/3439"
    },
    {
        "id": 21739,
        "title": "Power Transformers",
        "authors": "H. Jin Sim, Scott H. Digby",
        "published": "2017-12-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-2"
    },
    {
        "id": 21740,
        "title": "CFI-Former: Cross-Feature Interaction Transformer for Group Activity Recognition",
        "authors": "Xiaolin Zhu, dongli wang, Yan Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4595240"
    },
    {
        "id": 21741,
        "title": "Optimizing Vision Transformer Performance with Customizable Parameters",
        "authors": "E. Ibrahimovic",
        "published": "2023-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mipro57284.2023.10159761"
    },
    {
        "id": 21742,
        "title": "Continual Learning in Vision Transformer",
        "authors": "Mana Takeda, Keiji Yanai",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897851"
    },
    {
        "id": 21743,
        "title": "Dual-Transformer Head End-to-End Person Search Network",
        "authors": "Cheng Feng, Dezhi Han, Chongqing Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPerson Search mainly consists of two submissions, namely Person Detection and Person Re-identification (re-ID). Existing approaches are primarily based on Faster R-CNN and CNN (e.g., ResNet). While these structures may detect high-quality bounding boxes, they seem to degrade the performance of re-ID. To address this issue, this paper proposes a Dual-Transformer Head Network (DTHN) for end-to-end person search, which contains two independent Transformer heads, a box head for detecting the bounding box and extracting efficient bounding box feature, and a re-ID head for capturing high-quality re-ID features for the re-ID task. Specifically, after the image goes through the ResNet backbone network to extract features, the RPN proposes possible bounding boxes. The box head then extracts more efficient features within these bounding boxes for detection. Following this, the re-ID head computes the occluded attention of the features in these bounding boxes and distinguishes them from other persons or backgrounds. Extensive experiments on two widely used benchmark datasets, CUHK-SYSU and PRW, achieve state-of-the-art performance levels, 94.9 mAP and 95.3 top-1 score on the CUHK-SYSU dataset, and 51.6 mAP and 87.6 top-1 score on the PRW dataset, which demonstrates the advantages of this paper’s approach.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2989105/v1"
    },
    {
        "id": 21744,
        "title": "Panopticon: not a Transformer, but transforming ion trapping technology",
        "authors": "Anashe Bandari",
        "published": "2020-11-13",
        "citations": 0,
        "abstract": "A combination of a hemispherical mirror and unique ion trap design allows for the observation of quantum electrodynamics effects in complex systems.",
        "link": "http://dx.doi.org/10.1063/10.0002713"
    },
    {
        "id": 21745,
        "title": "Hierarchical Patch Aggregation Transformer For Motion Deblurring",
        "authors": "Yujie Wu, Liang Lei, Siyao Ling, Zhisheng Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe paradigm of harnessing encoder-decoder frameworks, underpinned by Transformer constituents, has emerged as an exemplar in the realm of image deblurring architectural designs. In this investigation, we critically reexamine this approach. Our analysis reveals that many current architectures focus heavily on limited local regions during the feature extraction phase. This narrow focus compromises the richness and diversity of features channeled to the encoder-decoder framework, resulting in an information bottleneck. Furthermore, these designs tend to rely excessively on global features, which can lead to the neglect of crucial local details in specific areas, adversely affecting image deblurring efficacy. To address these issues, we present the a novel hierarchical patch aggregation Transformer(HPAT) architecture. In the initial feature extraction phase, we incorporate cross-axis spatial Transformer blocks that exhibit linear complexity, complemented by an adaptive hierarchical attention fusion mechanism. These enhancements enable the model to adeptly capture spatial interrelationships among features and integrate insights from multiple hierarchical layers. Subsequently, we optimize the feedforward network within the Transformer blocks of the encoder-decoder framework, leading to the development of the Fusion Feedforward Network (F3N). This innovation streamlines the aggregation of token information, bolstering the model's ability to capture and retain local details. Our comprehensive experimental assessments, conducted across a variety of publicly available datasets, confirm the effectiveness of the HPAT model. Empirical results decisively prove that our HPAT model establishes a new benchmark in image deblurring tasks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3360147/v1"
    },
    {
        "id": 21746,
        "title": "eLife assessment: Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "Vitaly Ryu",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.2.sa2"
    },
    {
        "id": 21747,
        "title": "Based-Clip Early Fusion Transformer for Image Caption",
        "authors": "Jinyu Guo, Yuejia Li, Guanghui Cheng, Wenrui Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4748844"
    },
    {
        "id": 21748,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109.v2"
    },
    {
        "id": 21749,
        "title": "A Review on Transformer - Less UPFC",
        "authors": "Shaikh Mohammed Tauseef, S. M. Kulkarni,  ",
        "published": "2018-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd18750"
    },
    {
        "id": 21750,
        "title": "Multimodal Personality Recognition using Cross-attention Transformer and Behaviour Encoding",
        "authors": "Tanay Agrawal, Dhruv Agarwal, Michal Balazia, Neelabh Sinha, François Bremond",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010841400003124"
    },
    {
        "id": 21751,
        "title": "The Development of A Universal Transformer Housing of the Unit Transformer Substation 6-10 / 0.4 KV",
        "authors": "Dmitry Vladimirovich Malygin, Maksim Vladimirovich Borodin, Roman Pavlovich Belikov, Yulia Lyusievna Mikhaylova, Zumeyra Munirovna Shakurova",
        "published": "2021",
        "citations": 0,
        "abstract": "The company group of the PJSC “ROSSETI” pays great attention to reducing occupational hazard at the facilities of the power grid complex. The analysis of the accidents in the branch of PJSC “IGDC of the Center”– “Orelenergo” revealed that transformer housings installed at the mast-type transformer substations 6-10 / 0.4 kV can’t fully provide the required safety level as they can be slightly raised even without using a special tool, and therefore one can stick his hands or some objects to the current-carrying parts of the transformer substation. According to the statistics of the damages at the mast-type transformer substations 6-10 / 0.4 kV various small animals and birds can enter the electrical installation through small slits between the housing and the transformer, which will lead to different emergency situations. To prevent the aforementioned negative consequences, a universal transformer housing was developed for a mast-type unit transformer substation (UTS) 6-10 / 0.4 kV. The offered design of the housing is universal, since it can be used for the transformers of various capacities; for its manufacture tools and materials with different characteristics can be used. At the same time, the installation of the developed housing will allow power grid companies to reduce occupational hazard, reduce the undersupply of electricity and increase the reliability of power supply to the agricultural consumers. The technical solution presented in the article can be applied for the mast-type UTS 6-10 / 0.4 kV in the post-Soviet countries.",
        "link": "http://dx.doi.org/10.1051/e3sconf/202128801097"
    },
    {
        "id": 21752,
        "title": "Voice-to-voice conversion using transformer network*",
        "authors": "June-Woo Kim, Ho-Young Jung",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13064/ksss.2020.12.3.055"
    },
    {
        "id": 21753,
        "title": "Transformer Ageing Investigation and Correlation of Furanic Compound and Degree of Polymerization for Power Transformer Life Assessment",
        "authors": "Mohd Aizam Talib, Mohd Azhar Abdul Aziz, Yogendra Balasubramaniam, Young Zaidey Yang Ghazali, Mohd Rasyidi Zainal Abidin, Mohd Fairouz Mohd Yousof",
        "published": "2020-12-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pecon48942.2020.9314543"
    },
    {
        "id": 21754,
        "title": "Transformer-based Electric Quality Forecasting and Anomaly Detection for Predictive Maintenance",
        "authors": "Jehyuk Lee",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14801/jkiit.2023.21.12.35"
    },
    {
        "id": 21755,
        "title": "Comparative Study on the AC Breakdown Voltage of Transformer Mineral Oil with Transformer Oil-based Al &lt;inf&gt;2&lt;/inf&gt; O&lt;inf&gt;3&lt;/inf&gt; Nanofluids",
        "authors": "Usama Khaled, Abderrahmane Beroual",
        "published": "2018-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichve.2018.8642051"
    },
    {
        "id": 21756,
        "title": "Effective Convolution Mixed Transformer Siamese Network for Robust Visual Tracking",
        "authors": "Lin Chen, Yungang Liu, Yuan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4519320"
    },
    {
        "id": 21757,
        "title": "Three Phase Transformer: Connection and Configuration",
        "authors": "Saif Aldeen Saad Alkadhim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3647143"
    },
    {
        "id": 21758,
        "title": "Talk to Transformer",
        "authors": "Mark Amerika",
        "published": "2021-2-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429355875-21"
    },
    {
        "id": 21759,
        "title": "Exploring Transformer-based Language Recognition using Phonotactic Information",
        "authors": "David Romero, Luis Fernando D'Haro, Christian Salamea",
        "published": "2021-3-24",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/iberspeech.2021-53"
    },
    {
        "id": 21760,
        "title": "High-Voltage Impulse Analysis and Testing",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-15"
    },
    {
        "id": 21761,
        "title": "Transformer based Fingerprint Feature Extraction",
        "authors": "Saraansh Tandon, Anoop Namboodiri",
        "published": "2022-8-21",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956435"
    },
    {
        "id": 21762,
        "title": "Carbon Footprint Calculation of Transformer",
        "authors": "K.R.M. Nair",
        "published": "2021-2-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003088578-25"
    },
    {
        "id": 21763,
        "title": "Advanced Language Understanding with Syntax-Enhanced Transformer",
        "authors": "Wender Rine, Rodolfo Patel, Neo Steve",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we introduce Syntax-Enhanced Transformer Model (SET), a groundbreaking approach in the realm of Transformer-based language modeling that seeks to redefine the boundaries of linguistic analysis and comprehension. SET innovatively combine (i) the well-established high-level performance, scalability, and adaptability of traditional Transformers with (ii) a sophisticated analysis of syntactic structures. This synergy is enabled by a novel attention mechanism tailored to parse syntactic nuances and a deterministic process adept at transforming linearized parse trees into meaningful linguistic representations.  Our comprehensive experiments reveal that SET significantly advance the field by surpassing existing benchmarks in sentence-level language modeling perplexity. They exhibit exceptional proficiency in tasks that require an acute awareness of syntax, setting new standards for language models in understanding complex linguistic structures. Furthermore, SET demonstrate an enhanced capability to grasp nuanced linguistic patterns that have traditionally been challenging for standard Transformer models.  However, our studies also uncover a unique aspect of SET: while they excel in sentence-level tasks, their representation of sentences as singular vectors&mdash;owing to the syntactic composition constraints intrinsic to their design&mdash;introduces certain limitations in document-level language modeling. This observation points to an intriguing area for future exploration; it suggests the potential need for an alternative or complementary memory mechanism within Transformer models, one that functions independently from, yet in harmony with, syntactic structures. Such a mechanism could be pivotal in enhancing the model's ability to comprehend and process long-form texts effectively. In conclusion, SET mark a significant stride in the journey towards more sophisticated, syntax-aware language models. They offer promising insights into the integration of deep linguistic knowledge with cutting-edge machine learning techniques, potentially opening doors to a new era of natural language understanding and processing.",
        "link": "http://dx.doi.org/10.20944/preprints202312.1673.v1"
    },
    {
        "id": 21764,
        "title": "Conceptual Design: High-Voltage Transformer",
        "authors": "Jorge A. Gordillo",
        "published": "2022-4-4",
        "citations": 0,
        "abstract": "This paper shown and describe this behaviour an original conceptual design of an electrical transformer. The device it is constituted by an electrodynamic actuator and piezoelectric crystals.The input AC voltage generates an axial vibration in the electrodynamic actuator. The axial vibration is transmitted to a piezoelectric crystal which is polarized in the axial direction and generates the output voltage. In a reduced volumes and a single step, it would be possible to reach voltages of tens of MV and great transformation ratios-achieving these voltages is impossible with conventional systems-The transformer works at axial resonance of the piezoelectric crystal. This device operates to the frequency of order kHz; therefore could be used to generate electromagnetic waves. The capacitive and inductive at its output negligible respect conventional transformer. This transformer could be used in countless devices, such as gamma‐ray machines, electron microscope, solid-state propulsion system, Ion thruster, small particle accelerator etc.",
        "link": "http://dx.doi.org/10.4028/p-7p5c59"
    },
    {
        "id": 21765,
        "title": "FDTD Method",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-12"
    },
    {
        "id": 21766,
        "title": "Se transformer : éthique de vertu et spiritualité",
        "authors": "Richard J. Major",
        "published": "2021-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.voynn.2021.01.0180"
    },
    {
        "id": 21767,
        "title": "Transformer and Reactor Protection",
        "authors": "",
        "published": "2022-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119513100.ch17"
    },
    {
        "id": 21768,
        "title": "Pratiquer la thérapie des schémas",
        "authors": "",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.cousi.2023.01"
    },
    {
        "id": 21769,
        "title": "Evotuning protocols for Transformer-based variant effect prediction on multi-domain proteins",
        "authors": "Hideki Yamaguchi, Yutaka Saito",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAccurate variant effect prediction has broad impacts on protein engineering. Recent machine learning approaches toward this end are based on representation learning, by which feature vectors are learned and generated from unlabeled sequences. However, it is unclear how to effectively learn evolutionary properties of an engineering target protein from homologous sequences, taking into account the protein’s sequence-level structure called domain architecture (DA). Additionally, no optimal protocols are established for incorporating such properties into Transformer, the neural network well-known to perform the best in natural language processing research. This article proposes DA-aware evolutionary fine-tuning, or “evotuning”, protocols for Transformer-based variant effect prediction, considering various combinations of homology search, fine-tuning, and sequence vectorization strategies. We exhaustively evaluated our protocols on diverse proteins with different functions and DAs. The results indicated that our protocols achieved significantly better performances than previous DA-unaware ones. The visualizations of attention maps suggested that the structural information was incorporated by evotuning without direct supervision, possibly leading to better prediction accuracy.Availabilityhttps://github.com/dlnp2/evotuning_protocols_for_transformersSupplementary informationSupplementary data are available at Briefings in Bioinformatics online.",
        "link": "http://dx.doi.org/10.1101/2021.03.05.434175"
    },
    {
        "id": 21770,
        "title": "Chemformer: A Pre-Trained Transformer for Computational Chemistry",
        "authors": "Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Bjerrum",
        "published": "No Date",
        "citations": 0,
        "abstract": "Transformer models coupled with Simplified Molecular Line Entry System (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present Chemformer model – a Transformerbased model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top- 1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-v2pnn"
    },
    {
        "id": 21771,
        "title": "Human Fall Detection from Sequences of Skeleton Features using Vision Transformer",
        "authors": "Ali Raza, Muhammad Yousaf, Sergio Velastin, Serestina Viriri",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011678800003417"
    },
    {
        "id": 21772,
        "title": "Research on Lightweight Super-Resolution Network Based on CNN and Transformer",
        "authors": "光明 李",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2023.131010"
    },
    {
        "id": 21773,
        "title": "Spatial-Temporal Transformer Traffic Flow Prediction Based on Spatial Information Delay Awareness",
        "authors": "文翔 孔",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/sea.2023.122029"
    },
    {
        "id": 21774,
        "title": "High frequency transformer model derived from limited information about the transformer geometry",
        "authors": "Bruno Jurisic, Ivo Uglesic, Alain Xemard, Françoise Paladian",
        "published": "2018-1",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2017.07.017"
    },
    {
        "id": 21775,
        "title": "View-invariant 3D Skeleton-based Human Activity Recognition based on Transformer and Spatio-temporal Features",
        "authors": "Ahmed Snoun, Tahani Bouchrika, Olfa Jemai",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010895300003122"
    },
    {
        "id": 21776,
        "title": "Comment transformer une faillite en triomphe !",
        "authors": "Christian Chavagneux",
        "published": "2017-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ae.370.0022"
    },
    {
        "id": 21777,
        "title": "MF-UHF Transformer Design Techniques",
        "authors": "Abdullah Eroglu",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b14286-3"
    },
    {
        "id": 21778,
        "title": "Transformer For Image Quality Assessment",
        "authors": "Junyong You, Jari Korhonen",
        "published": "2021-9-19",
        "citations": 79,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506075"
    },
    {
        "id": 21779,
        "title": "Transformer le regard social sur l’addiction",
        "authors": "Michel Reynaud",
        "published": "2017-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.soin.2017.04.015"
    },
    {
        "id": 21780,
        "title": "Short-Circuit Stresses and Strength",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-6"
    },
    {
        "id": 21781,
        "title": "Special Tests Consideration for a Distributed Photovoltaic Grid Power Transformer",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16412-16"
    },
    {
        "id": 21782,
        "title": "Three phase transformer banks",
        "authors": "Thomas Ortmeyer",
        "published": "2018-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/978-0-7503-1662-0ch4"
    },
    {
        "id": 21783,
        "title": "TransPPG: Two-stream Transformer for Remote Heart Rate Estimate",
        "authors": "Jiaqi Liu, su yang, Weishan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Non-contact facial video-based heart rate estima- tion using remote photoplethysmography (rPPG) has shown great potential in many applications (e.g., remote health care) and achieved creditable results in constrained scenarios. However, practi- cal applications require results to be accurate even under complex environment with head movement and unstable illumination. Therefore, improving the performance of rPPG in complex environment has become a key challenge. In this paper, we pro- pose a novel video embedding method that embeds each facial video sequence into a feature map re- ferred to as Multi-scale Adaptive Spatial and Tem- poral Map with Overlap (MAST Mop), which con- tains not only vital information but also surround- ing information as reference, which acts as the mir- ror to figure out the homogeneous perturbations imposed on foreground and background simulta- neously, such as illumination instability. Corre- spondingly, we propose a two-stream Transformer model to map the MAST Mop into heart rate (HR), where one stream follows the pulse signal in the facial area while the other figures out the perturba- tion signal from the surrounding region such that the difference of the two channels leads to adap- tive noise cancellation. Our approach significantly outperforms all current state-of-the-art methods on two public datasets MAHNOB-HCI and VIPL-HR. As far as we know, it is the first work with Trans- former as backbone to capture the temporal depen- dencies in rPPGs and apply the two stream scheme to figure out the interference from backgrounds as mirror of the corresponding perturbation on fore- ground signals for noise tolerating.",
        "link": "http://dx.doi.org/10.31237/osf.io/pks7a"
    },
    {
        "id": 21784,
        "title": "Index",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.index"
    },
    {
        "id": 21785,
        "title": "RMTnet: Recurrence meet Transformer for Alzheimer's disease diagnosis using FDG-PET",
        "authors": "Uttam Khatri, Goo-Rak Kwon",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170594596.62106929/v1"
    },
    {
        "id": 21786,
        "title": "Learning Single and Multi-Scene Camera Pose with Transformer Encoders",
        "authors": "Yosi Keller, Yoli Shavit, Ron Ferens",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4435945"
    },
    {
        "id": 21787,
        "title": "Context-Aware Vision Transformer (Cavit) for Satellite Image Classification",
        "authors": "Himanshu Srivastava, Anuj  Kumar Bharti, Akansha Singh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673127"
    },
    {
        "id": 21788,
        "title": "Tolstoy’s Genius Explored by Deep Learning using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "Artificial Narrow Intelligence is in the phase of moving towards the AGN, which will attempt to decide as a human being. We are getting closer to it by each day, but AI actually is indefinite to many, although it is no different than any other set of mathematically defined computer operations in its core. Generating new data from a pre-trained model introduces new challenges to science & technology. In this work, the design of such an architecture from scratch, solving problems, and introducing alternative approaches are what has been conducted. Using a deep thinker, Tolstoy, as an object of study is a source of motivation for the entire research.",
        "link": "http://dx.doi.org/10.5121/csit.2023.132306"
    },
    {
        "id": 21789,
        "title": "Ensemble Transformer for cross lingual semantic textual similarity",
        "authors": "Poorya Piroozfar, Mohammad Abdous, Behrouz Minaei Bidgoli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nToday, it is particularly important to recognize the semantic similarity between texts in different languages due to the emergence of new natural language processing models like ChatGPT and Bard. These models can provide more accurate and comprehensive answers to users' questions by identifying semantic similarity between two texts in different languages. Cross-lingual semantic similarity refers to the process of calculating similarity between two pieces of text in different languages. This paper aims to present an improved method for finding similarities between sentences in different languages. Some of the current methods create the same vector space to achieve this, while others use machine translation to translate the text into another language and then determine similarity between the two sentences using monolingual sentence similarity methods. The degree of similarity is expressed as a number between 0 and 5. Over the past few years, the progress in language models based on transformers has paved the way for improvements in detecting text similarity. This article discusses the utilization of ensemble models with transformers to determine the semantic similarity of sentences in Persian and English languages utilizing the Persian-English corpus. According to our findings, this ensemble approach has a correlation rate of 95.28% in detecting the extent of semantic similarity between cross-lingual sentences. These results indicate that our method surpasses previous techniques for discovering similarities between sentences in different languages.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3860915/v1"
    },
    {
        "id": 21790,
        "title": "The MicroRNA 2000 Transformer",
        "authors": "Yoichi Robertus Fujii",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3165-1"
    },
    {
        "id": 21791,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable communication scholars to adopt modern computational methods for valid and reliable sentiment analysis, we propose a free and open web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates different sentiment analysis tools including transformer-based deep learning models and sentiment dictionaries. TLSA enables users with limited computational resources to use these tools and validate their results at once. Two cases showed that transformer-based models provided more accurate measurement of sentiments in tweets, compared to lexicon-based methods. The performance of different tools varied to a large extent based on the dataset, suggesting the importance to validate different sentiment tools in a specific study. TLSA is expected to empower communication scholars to reap the benefit of state-of-the-art computational methods for valid measurement and insight discovery. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109.v1"
    },
    {
        "id": 21792,
        "title": "Vision Transformer Prediction of Delirium",
        "authors": "Malissa Mulkey, Huyunting Huang, Thomas Albanese, Kim Sunghan, Baijian Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs many as 80% of critically ill patients develop delirium increasing the need for institutionalization and higher morbidity and mortality. Clinicians detect less than 40% of delirium when using a validated screening tool. EEG is the criterion standard but is resource intensive thus not feasible for widespread delirium monitoring. This study evaluated the use of limited-lead rapid-response EEG and supervised deep learning methods with vision transformer to predict delirium. This proof-of-concept study used a prospective design to evaluate use of supervised deep learning with vision transformer and a rapid-response EEG device for predicting delirium in mechanically ventilated critically ill older adults. Fifteen different models were analyzed. Using all available data, the vision transformer models provided 99.9%+ training and 97% testing accuracy across models. Vision transformer with rapid-response EEG is capable of predicting delirium. Such monitoring is feasible in critically ill older adults. Therefore, this method has strong potential for improving the accuracy of delirium detection, providing greater opportunity for individualized interventions. Such an approach may shorten hospital length of stay, increase discharge to home, decrease mortality, and reduce the financial burden associated with delirium.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2156449/v1"
    },
    {
        "id": 21793,
        "title": "Fusion of Transformer and ML-CNN-BiLSTM for Network Intrusion Detection",
        "authors": "Zelin Xiang, Xuwei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNetwork intrusion detection system (NIDS) can effectively sense network attacks, which is of great significance for maintaining the security of cyberspace. To meet the requirements of efficient and accurate network status monitoring, this paper proposes a NIDS model using deep learning network model. Firstly, GAN-Cross is used to expand minority class sample data, thereby alleviating the problem of minority class imbalance in the original dataset. Then, the Transformer module is used to adjust the ML-CNN-BiLSTM model to enhance the analysis ability of the intrusion model. Finally, the data enhancement model and feature enhancement model are integrated into the NIDS model, the detection model is optimized, the characteristics of network state data are extracted at a deeper level, and the generalization ability of the detection model is enhanced. The simulation experiments using UNSW-NB15 data sets shows that the proposed algorithm can achieve efficient analysis of complex network traffic data sets, with an accuracy of 0.903, and can effectively improve the detection accuracy of NIDS and the detection ability for unknown attacks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3033373/v1"
    },
    {
        "id": 21794,
        "title": "Analysis of Ferroresonance in a Power Transformer with Multiple Nonlinearities",
        "authors": "Goutham Kumar, Saravanaselvan Rajan, Ramanujam Rangarajan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00247"
    },
    {
        "id": 21795,
        "title": "Revolutionizing Wireless Traffic Usage Forecasting: Transformer with Attention Mechanism",
        "authors": "Bandu Uppalaiah, D. Mallikarjuna Reddy, A. Srilath",
        "published": "No Date",
        "citations": 0,
        "abstract": "Revolutionizing wireless traffic forecasting empowers proactive resource\nallocation, optimizing network performance and ensuring efficient\nutilization of resources in dynamic wireless environments. real-time\ntraffic data from a business network with There are 470 APs.), this\nresearch provides a thorough examination of the temporal and\ngeographical dynamics of network traffic. Time series data forecasting\nis given a new spin with the help of machine learning models built on\nthe Transformer framework. This approach uses the brain’s attentional\nprocesses to analyze time series data for hidden dynamics and complex\npatterns. Notably, the analysis identifies high-traffic-utilization AP\ngroups exhibiting robust seasonality patterns, alongside those devoid of\nsuch patterns. Several different types of forecasting methods are used\nand evaluated in this research, among them the Holt-Winters technique, a\nSARIMA model, a GRU model, a CNN model, and a model based on\nconvolutional neural networks. In conclusion, the research sheds light\non the complex patterns underlying network traffic and presents an\ninnovative forecasting approach, bolstering the potential for improved\nwireless network resource management.",
        "link": "http://dx.doi.org/10.22541/au.169769165.55793181/v1"
    },
    {
        "id": 21796,
        "title": "L’effet levier de l’énergétique pour transformer la construction",
        "authors": "Christophe GOBIN",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51257/a-v1-c8135"
    },
    {
        "id": 21797,
        "title": "End-to-End Neural Transformer Based Spoken Language Understanding",
        "authors": "Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann",
        "published": "2020-10-25",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1963"
    },
    {
        "id": 21798,
        "title": "Advanced Machine Translation with Linguistic-Enhanced Transformer",
        "authors": "Richard Patricia, Judith Sadok, Rodolfo Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recent advancements in neural language models, particularly attention-based architectures like the Transformer, have substantially surpassed traditional methods in various natural language processing tasks. These models adeptly generate nuanced token representations by considering contextual relations within a sequence. However, augmenting these models with explicit syntactic knowledge, such as part of speech tags, has been found to remarkably bolster their effectiveness, especially under constrained data scenarios. This study introduces the Linguistic Enhanced Transformer (LET), which integrates multiple syntactic features, showing a notable increase in translation accuracy, evidenced by an improvement of up to 1.99 BLEU points on subsets of the WMT '14 English-German dataset. Furthermore, this paper demonstrates that enriching BERT models with syntax-aware embeddings enhances their performance on several GLUE benchmark tasks.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0186.v1"
    },
    {
        "id": 21799,
        "title": "Reviewer #2 (Public Review): Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.2.sa0"
    },
    {
        "id": 21800,
        "title": "Reviewer #1 (Public Review): Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.2.sa1"
    },
    {
        "id": 21801,
        "title": "eLife assessment: Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "Vitaly Ryu",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.1.sa0"
    },
    {
        "id": 21802,
        "title": "Reviewer #1 (Public Review): Transformer-based spatial-temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.1.sa1"
    },
    {
        "id": 21803,
        "title": "Point Transformer",
        "authors": "Nico Engel, Vasileios Belagiannis, Klaus Dietmayer",
        "published": "2021",
        "citations": 104,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3116304"
    },
    {
        "id": 21804,
        "title": "Reviewer #2 (Public Review): Transformer-based spatial–temporal detection of apoptotic cell death in live-cell imaging",
        "authors": "",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.90502.3.sa2"
    },
    {
        "id": 21805,
        "title": "Healthcare Transformer 7: Patient-Centeredness",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-9"
    },
    {
        "id": 21806,
        "title": "mcU-TransNet: A more Comprehensive UNet-Transformer Network for Polyp Segmentation",
        "authors": "Lei Huang, Yun Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPurpose: Polyp segmentation is crucial for localizing areas that are at risk of early cancer. Traditional Convolutional Neural Networks (CNNs) have limitations in capturing the global context and long-range dependencies. In this paper, a more comprehensive UNet-Transformer network (mcU-TransNet) is proposed for polyp segmentation. \nMethods: mcU-TransNet combines the strengths of CNN-based approaches with Transformer-based approaches. To tackle the challenge of detecting different types of polyps at the same location, an attention-enhanced UNet backbone is introduced. This backbone integrates a stem block into the UNet encoder. Spatial and channel attention mechanisms in the UNet decoder are employed to learn specific polyp features, thereby enhancing contour extraction accuracy. To overcome the limited fine-grained perception of the Transformer model, a Transformer head with deformable separable kernel attention (DSKA) is utilized to capture global context information. Convolutional parallel dilated convolutions are employed to gather both local and distant context information, thereby addressing spatial perception deficiencies in the unfolding sequence. \nResults: Experimental results on the CVC-ClinicDB and Kvasir-SEG datasets demonstrate that mcU-TransNet outperforms existing state-of-the-art methods, showcasing its robustness and ease of training. \nConclusion: mcU-TransNet could comprehensively learn dataset features and enhance colonoscopy interpretability for polyp detection.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3991172/v1"
    },
    {
        "id": 21807,
        "title": "Inverter Circuit Coordination with a Distributed Photovoltaic Grid Power Transformer",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16412-12"
    },
    {
        "id": 21808,
        "title": "Self-Supervised Transformer Model Training for a Sleep-EEG Foundation Model",
        "authors": "Mattson Ogg, William G. Coon",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe American Academy of Sleep Medicine (AASM) recognizes five sleep/wake states (Wake, N1, N2, N3, REM), yet this classification schema provides only a high-level summary of sleep and likely overlooks important neurological or health information. New, data-driven approaches are needed to more deeply probe the information content of sleep signals. Here we present a self-supervised approach that learns the structure embedded in large quantities of neurophysiological sleep data. This masked transformer training procedure is inspired by high performing self-supervised methods developed for speech transcription. We show that self-supervised pre-training matches or outperforms supervised sleep stage classification, especially when labeled data or compute-power is limited. Perhaps more importantly, we also show that our pretrained model is flexible and can be fine-tuned to perform well on new tasks including distinguishing individuals and quantifying “brain age” (a potential health biomarker). This suggests that modern methods can automatically learn information that is potentially overlooked by the 5-class sleep staging schema, laying the groundwork for new schemas and further data-driven exploration of sleep.",
        "link": "http://dx.doi.org/10.1101/2024.01.18.576245"
    },
    {
        "id": 21809,
        "title": "Frequency response analysis",
        "authors": "Mehdi Bagheri,  Toan Phung",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch5"
    },
    {
        "id": 21810,
        "title": "1. First Natural Air Cooling On-roof Traction Transformer for railway rolling stock",
        "authors": "vulturescu bogdan, Chamaret Chamaret",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5e4fe9c06bc493207536f6fd"
    },
    {
        "id": 21811,
        "title": "Healthcare Transformer 6: Care Coordination",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-10"
    },
    {
        "id": 21812,
        "title": "GOProFormer: A Multi-modal Transformer Method for Gene Ontology Protein Function Prediction",
        "authors": "Anowarul Kabir, Amarda Shehu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractProtein Language Models (PLMs) are shown capable of learning sequence representations useful for various prediction tasks, from subcellular localization, evolutionary relationships, family membership, and more. They have yet to be demonstrated useful for protein function prediction. In particular, the problem of automatic annotation of proteins under the Gene Ontology (GO) framework remains open. This paper makes two key contributions. It debuts a novel method that leverages the transformer architecture in two ways. A sequence transformer encodes protein sequences in a task-agnostic feature space. A graph transformer learns a representation of GO terms while respecting their hierarchical relationships. The learned sequence and GO terms representations are combined and utilized for multi-label classification, with the labels corresponding to GO terms. The method is shown superior over recent representative GO prediction methods. The second major contribution in this paper is a deep investigation of different ways of constructing training and testing datasets. The paper shows that existing approaches under- or over-estimate the generalization power of a model. A novel approach is proposed to address these issues, resulting a new benchmark dataset to rigorously evaluate and compare methods and advance the state-of-the-art.",
        "link": "http://dx.doi.org/10.1101/2022.10.20.513033"
    },
    {
        "id": 21813,
        "title": "Dual Double Point Cloud Transformer",
        "authors": "Qi Zhong, Xian-Feng Han, Cheng-Yu Fang, Lu Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLong-range context relationship plays an important role in understanding point clouds. Inspired by the permutation invariance of the recent Transformer models, in this paper, we propose an end-to-end Dual Double Transformer (DDTFormer) architecture for 3D point cloud processing. Specifically, two core components, termed point-wise double transformer and channel-wise double transformer blocks, are well designed for explicitly modeling interdependencies among points and channels. And both blocks consist of two key operations: aggregation attention mapping the point-wise/channel-wise feature maps into a global space, and propagation attention diffusing the aggregated features back to the input points or channels. We illustrate the effectiveness and competitiveness via extensive quantitative and qualitative experiments on 3D shape classification and segmentation benchmark datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2756872/v1"
    },
    {
        "id": 21814,
        "title": "Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation",
        "authors": "Shashank Kotyan, Danilo  Vasconcellos Vargas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374280"
    },
    {
        "id": 21815,
        "title": "Ambient temperature as the reason for MV/LV power transformer damage",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2021.138273"
    },
    {
        "id": 21816,
        "title": "Bidirectional LSTM Autoencoder Transformer for Remaining Useful Life Estimation",
        "authors": "Zhengyang Fan, Wanru Li, Kuo-Chu Chang",
        "published": "No Date",
        "citations": 1,
        "abstract": "Estimating the Remaining Useful Life (RUL) of aircraft engines holds a pivotal role in enhancing safety, optimizing operations, and promoting sustainability, thus being a crucial component of modern aviation management. Precise RUL predictions offer valuable insights into an engine’s condition, enabling informed decisions regarding maintenance and crew scheduling. In this context, we propose a novel RUL prediction approach in this paper, harnessing the power of Bi-directional LSTM and Transformer architectures, known for their success in sequence modeling, such as natural languages. We adopt the encoder part of the full Transformer as the backbone of our framework, integrating it with a self-supervised denoising autoencoder that utilizes Bidirectional LSTM for improved feature extraction. Within our framework, a sequence of multivariate time series sensor measurements serves as the input, initially processed by the Bidirectional LSTM autoencoder to extract essential features. Subsequently, these feature values are fed into our Transformer encoder backbone for RUL prediction. Notably, our approach simultaneously trains the autoencoder and Transformer encoder, different from the naive sequential training method. Through a series of numerical experiments carried out on the C-MAPSS datasets, we demonstrate that the efficacy of our proposed models either surpasses or stands on par with that of other existing methods.",
        "link": "http://dx.doi.org/10.20944/preprints202311.1705.v1"
    },
    {
        "id": 21817,
        "title": "HiCDiffusion - diffusion-enhanced, transformer-based prediction of chromatin interactions from DNA sequences",
        "authors": "Mateusz Chiliński, Dariusz Plewczynski",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractPrediction of chromatin interactions from DNA sequence has been a significant research challenge in the last couple of years. Several solutions have been proposed, most of which are based on encoder-decoder architecture, where 1D sequence is convoluted, encoded into the latent representation, and then decoded using 2D convolutions into the Hi-C pairwise chromatin spatial proximity matrix. Those methods, while obtaining high correlation scores and improved metrics, produce Hi-C matrices that are artificial - they are blurred due to the deep learning model architecture. In our study, we propose the HiCDiffusion model that addresses this problem. We first train the encoder-decoder neural network and then use it as a component of the diffusion model - where we guide the diffusion using a latent representation of the sequence, as well as the final output from the encoder-decoder. That way, we obtain the high-resolution Hi-C matrices that not only better resemble the experimental results - improving the Fréchet inception distance by an average of 12 times, with the highest improvement of 35 times - but also obtain similar classic metrics to current state-of-the-art encoder-decoder architectures used for the task.",
        "link": "http://dx.doi.org/10.1101/2024.02.01.578389"
    },
    {
        "id": 21818,
        "title": "A New Adaptive Differential Protection Scheme for Tap Changing Power Transformer",
        "authors": "Ashesh Mukeshbhai Shah, Bhavesh Bhalja",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00394"
    },
    {
        "id": 21819,
        "title": "Drug-Drug Interaction Extraction Using Transformer-based Ensemble Model",
        "authors": "Sarina Sefidgarhoseini, Leila Safari, Zanyar Mohammady",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nPurpose Drug-drug interactions (DDIs), or the simultaneous use of several medications, can impair the effectiveness of the medications, have negative physiological consequences, raise the price of medical treatment, or even endanger the health and lives of patients. In order to increase patient safety when using drugs, it is crucial to do research in the area of automated extraction of DDIs. It benefits the medical industry and forms a foundation for DDI databases to be updated automatically.Methods This article's goal is to suggest a useful classification and identification scheme for DDIs. Based on a range of characteristics, including text, POS tags, distance, and SDP features, three deep learning-based models are presented: CNN, BiLSTM, and BiLSTM with attention. Three models based on transformers are also proposed: BERT, RoBERTa, and ELECTRA. In addition, an approach using an ensemble of transformer-based models is suggested for extracting DDIs from medical literature.Results All models were applied to the benchmark DDIExtraction2013 dataset. The results showed that the attention mechanism in deep learning models is very effective and has led to comparable results with the best existing models, but other features are not helpful. Also, with a 12% improvement in the F-measure score, the ensemble method showed indisputably supremacy over the existing methods and other proposed methods in this study.Conclusion Transformer-based models with the bidirectional processing ability and internal attention mechanism outperform deep learning models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2799750/v1"
    },
    {
        "id": 21820,
        "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
        "authors": "Yukun Qian, Xuyi Zhuang, Mingjiang Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1450"
    },
    {
        "id": 21821,
        "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model",
        "authors": "Chong-En Lin, Kuan-Yu Chen",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-863"
    },
    {
        "id": 21822,
        "title": "Former pour transformer",
        "authors": "Rémi Beau, Marie Drique",
        "published": "2020-11-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/pro.379.0086"
    },
    {
        "id": 21823,
        "title": "Polymeric transformer: Nanoantibiotics for drug-resistant infections",
        "authors": "Young Jik Kwon",
        "published": "2019-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32907/ro-108-2225"
    },
    {
        "id": 21824,
        "title": "Learning Single and Multi-Scene Camera Pose with Transformer Encoders",
        "authors": "Yosi Keller, Yoli Shavit, Ron Ferens",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4426054"
    },
    {
        "id": 21825,
        "title": "DeTformer: A Novel Efficient Transformer Framework for Image Deraining",
        "authors": "Thatikonda Ragini, Ramalingaswamy Cheruku, Kodali Prakash",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCaptured rainy images severely degrade outdoor vision systems performance, such as semi-autonomous or autonomous driving systems and video surveillance systems. Consequently, removing heavy and complex rain streaks i.e., undesirable rainy artifacts from a rainy image plays a crucial role for many high-level computer vision tasks and has drawn researchers’ attention from the past few years. The main drawbacks of Convolutional neural networks: have smaller receptive field, lack the model’s ability to capture long-range dependencies and complicated rainy artifacts, non-adaptive to input content and also computational complexity grows quadratically with input image size. These factors limit the deraining model performance improvement further. Recently, transformer has achieved better performance in both Natural language processing (NLP) and high-level computer vision (CV). We cannot adopt transformer directly to image deraining task as it has following limitations: a) although the transformer possesses powerful long-range computational capability, it lacks the ability to model local features b) to process input image, transformer uses fixed patch size, therefore pixels at the patch edges cannot use local features of surrounding pixels while removing heavy rain streaks. To address these issues, in single image deraining, we proposed a novel and efficient De-raining Transformer (DeTformer). In DeTformer, we designed a “Gated-Depth-wise Convolution Feed-forward Network” (GDWCFN) to address the first issue and applied depthwise convolution to improve the modelling capability of local features and suppress unnecessary features and allow only useful information further. Also, the second issue was addressed, by introducing multi-resolution features in our network, and we applied progressive learning in the transformer and thus it allows the edge pixels to utilize local features effectively. Furthermore, to integrate the extracted multi-scale features and provide feature interaction across channel dimensions, we introduced a “Multi-head Depth-wise Convolution Transposed Attention” (MDWCTA) module. The proposed model experimented with various de-rained datasets and compared with various state-of-the-art models. The experimental results show that DeTformer network achieves superior performance compared to state-of-the-art networks on synthetic and real-world rain datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2623687/v1"
    },
    {
        "id": 21826,
        "title": "Estimation of Transformer Winding Capacitances through Frequency Response Analysis – An Experimental Investigation",
        "authors": "Krupa Shah, K. Ragavan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00180"
    },
    {
        "id": 21827,
        "title": "AC Breakdown Analysis of Synthesized Nanofluids for Transformer Insulation",
        "authors": "SAMSON OPARANTI, A.A. Khaleed, A.A. Abdelmalik",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis work presents the effect of nanoparticles (Al2O3 and TiO2) on methyl ester synthesized from palm kernel oil for oil-filled power equipment. It investigates the loss tangent, AC conductivity and AC breakdown strength of the methyl ester-based nanofluid. The surface of the nanoparticles was functionalized with oleic acid before dispersing it into methyl ester to modify the stability of the mixture. Scanning electron microscopy coupled with electron dispersive x-ray was done on the two nanoparticles to know the morphology and elemental composition of the nanoparticles. The preparation of nanofluids was achieved through the dispersion of 0.2, 0.4, 0.6, 0.8 and 1wt. % of nanoparticles into the ester. It was observed that the loading of the two nanoparticles reduces the loss tangent and the AC conductivity of methyl ester but with a pronounced enhancement in Al2O3 nanofluid. The Weibull statistical analysis of the breakdown data shows that the dispersion of the nanoparticles in the base fluid increases the characteristic breakdown strength of the ester with an optimum performance at 0.6 wt. %. The characteristic breakdown strength for TiO2 and Al2O3 nanofluid is 60.6 kV and 64.1 kV respectively. The result revealed that Al2O3 nanofluid possesses the highest dielectric properties with low loss, low conductivity and high characteristic breakdown strength which makes it a better replacement for mineral insulating oil.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-453749/v1"
    },
    {
        "id": 21828,
        "title": "Fusion of Transformer Attention and Cnn Features for Skin Cancer Detection",
        "authors": "Hatice Catal Reis, Veysel Turk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4654126"
    },
    {
        "id": 21829,
        "title": "Transformer la loi en lien social",
        "authors": "Francesco Dendena",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.psorbonne.109466"
    },
    {
        "id": 21830,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109"
    },
    {
        "id": 21831,
        "title": "Review for \"New family of transformer‐less quadratic buck‐boost converters with wide conversion ratio\"",
        "authors": "Armando Cordeiro",
        "published": "2021-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13061/v1/review2"
    },
    {
        "id": 21832,
        "title": "Assessing the Effectiveness of Multilingual Transformer-based Text Embeddings for Named Entity Recognition in Portuguese",
        "authors": "Diego Santos, Frederico Dutra, Fernando Parreiras, Wladmir Brandão",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010443204730483"
    },
    {
        "id": 21833,
        "title": "Understanding of Feature Representation in Convolutional Neural Networks and Vision Transformer",
        "authors": "Hiroaki Minoura, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011621300003417"
    },
    {
        "id": 21834,
        "title": "Hie-Transformer: A Hierarchical Hybrid Transformer for Abstractive Article Summarization",
        "authors": "Xuewen Zhang, Kui Meng, Gongshen Liu",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-36718-3_21"
    },
    {
        "id": 21835,
        "title": "Two-Stream Spatial–Temporal Transformer Networks For Driver Drowsiness Detection",
        "authors": "Qianyi Jiang Qianyi Jiang, Huahu Xu Qianyi Jiang, Chen Cheng Huahu Xu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "\n                        <p>For driver drowsiness detection in the real world, the existing methods have good performance in general. However, when the face is blocked, the light is dim, and the driver’s head posture changes, the performance will deteriorate significantly. In this paper, a two-stream Spatial-Temporal transformer network intended to perform diver drowsiness detection task is proposed to solve the above problems. The spatial-temporal graph is extracted from the video and then the results are obtained from 2s-STTN. The model is a two-stream transformer network model. In our model, the Spatial Self-Attention module is used to learn the embedding of different facial landmarks, and the Temporal Self-Attention module is used to learn the correlation between the frames of facial feature points. Different activated facial landmarks are separated and recognized by class activation mapping technology. Each flow recognizes different activated facial features, extracts spatial or temporal features, and integrates the information about facial features, so as to improve the performance of the system. 2s-STTN can not only mine the long-term dependence of driver behavior from video, but also mine the driver drowsiness information provided by the unobstructed facial signs when the face is blocked. By conducting experiments and comparing our model with other models, it is demonstrated that the proposed model has good performance in driver drowsiness detection.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023103405008"
    },
    {
        "id": 21836,
        "title": "Graph Transformer SlowFast Model-based Child Abnormal Behavior Analysis System",
        "authors": "Chang-seob Yun, Sang-hyun Park, Yun-ha Park",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieie.2023.60.4.71"
    },
    {
        "id": 21837,
        "title": "Startup Strategy With Constant Peak Transformer Current for Solid-State Transformer in Distribution Network",
        "authors": "Jianning Sun, Liqiang Yuan, Qing Gu, Zhengming Zhao",
        "published": "2019-3",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tia.2018.2883012"
    },
    {
        "id": 21838,
        "title": "STA-YOLOv7: Swin-Transformer-Enabled YOLOv7 for Road Damage Detection",
        "authors": "冬梅 张",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2023.135113"
    },
    {
        "id": 21839,
        "title": "Supervised Spatial Transformer Networks for Attention Learning in Fine-grained Action Recognition",
        "authors": "Dichao Liu, Yu Wang, Jien Kato",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007257800002108"
    },
    {
        "id": 21840,
        "title": "Cooperative overload control strategy of power grid-transformer considering dynamic security margin of transformer in emergencies",
        "authors": "Jian Wang, Yi Wan, Zhangmin Xiong, Xiaofu Xiong, Jinxin Ouyang",
        "published": "2022-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2022.108098"
    },
    {
        "id": 21841,
        "title": "An ensemble learning integration of multiple <scp>CNN</scp> with improved vision transformer models for pest classification",
        "authors": "Wanshang Xia, Dezhi Han, Dun Li, Zhongdai Wu, Bing Han, Junxiang Wang",
        "published": "2023-3",
        "citations": 10,
        "abstract": "AbstractPests are the main threats to crop growth, and the precision classification of pests is conducive to formulating effective prevention and governance strategies. In response to the problems of low efficiency and inadaptability to the large‐scale environment of existing pest classification methods, this paper proposes a new pest classification method based on a convolutional neural network (CNN) and an improved Vision Transformer model. First, the MMAlNet is designed to extract the characteristics of the identification object from different scales and finer granularity. Then, a classification model called DenseNet Vision Transformer (DNVT) combining a CNN and an improved vision transformer model is proposed. The proposed DNVT captures both long distance dependencies and local characteristic modelling capabilities, which can effectively improve pest classification accuracy. Finally, the ensemble learning algorithm is used to learn MMAlNet and DNVT classification forecasts for soft voting, further enhancing the classification accuracy of pests. The simulation experiment results on the D0 and IP102 datasets show that the proposed method attained a maximum classification of 99.89 and 74.20%, respectively, which is better than other state‐of‐the‐art methods and has a high practical application value.",
        "link": "http://dx.doi.org/10.1111/aab.12804"
    },
    {
        "id": 21842,
        "title": "Digital transformer differential protection setting using its mathematical models",
        "authors": "Mikhail Andreev,  , Aleksey Suvorov, Anton Kievets, Vladimir Rudnik,  ,  ,  ",
        "published": "2020-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21285/1814-3520-2020-1-85-96"
    },
    {
        "id": 21843,
        "title": "Financial Fraud Detection Based on Deep Learning: Towards Large-Scale Pre-training Transformer Models",
        "authors": "Haitao Wang, Jiale Zheng, Ivan E. Carvajal-Roca, Linghui Chen, Mengqiu Bai",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-7224-1_13"
    },
    {
        "id": 21844,
        "title": "Masking and Transformer-based Models for Hyperpartisanship Detection in News",
        "authors": "Javier Sánchez-Junquera,  , Paolo Rosso, Manuel Montes-y-Gómez, Simone P. Ponzetto,  ,  ,  ",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-072-4_140"
    },
    {
        "id": 21845,
        "title": "Evaluation of Transformer-Based Models for Punctuation and Capitalization Restoration in Spanish and Portuguese",
        "authors": "Ronghao Pan, José Antonio García-Díaz, Rafael Valencia-García",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-35320-8_17"
    },
    {
        "id": 21846,
        "title": "Multi-task Swin Transformer for Motion Artifacts Classification and Cardiac Magnetic Resonance Image Segmentation",
        "authors": "Michal K. Grzeszczyk, Szymon Płotka, Arkadiusz Sitek",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-23443-9_38"
    },
    {
        "id": 21847,
        "title": "Comparing Transformer-Based Machine Translation Models for Low-Resource Languages of Colombia and Mexico",
        "authors": "Jason Angel, Abdul Gafar Manuel Meque, Christian Maldonado-Sifuentes, Grigori Sidorov, Alexander Gelbukh",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47640-2_8"
    },
    {
        "id": 21848,
        "title": "Thermal Performance Study of Power Transformer Filled with Biodegradable Oil Using the Effective Approach for Electromagnetic and Computational Fluid Dynamics Models Multiple Coupling",
        "authors": "Michal Stebel, Andrzej J. Nowak, Bartlomiej Melka, Michal Palacz, Michal Haida, Jakub Bodys, Jacek Smolka, Krzysztof Kubiczek, Pawel Lasek, Mariusz Stepien, Gustavo Rios Rodriguez, Luciano Garelli, Mario Storti, Francisco Pessolani, Mauro Amadei, Daniel Granata",
        "published": "2022-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl49583.2022.9830957"
    },
    {
        "id": 21849,
        "title": "Damage Detection and Localization at the Jacket Support of an Offshore Wind Turbine Using Transformer Models",
        "authors": "Héctor Triviño, Cisne Feijóo, Hugo Lugmania, Yolanda Vidal, Christian Tutivén",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Early detection of damage in the support structure (submerged part) of an offshore wind turbine is crucial as it can help to prevent emergency shutdowns and extend the lifespan of the turbine. To this end, a promising proof-of-concept is stated, based on a transformer network, for the detection and localization of damage at the jacket-type support of an offshore wind turbine. To the best of the authors’ knowledge, this is the first time transformer-based models have been used for offshore wind turbine structural health monitoring. The proposed strategy employs a transformer-based framework for learning multivariate time series representation. The framework is based on the transformer architecture, which is a neural network architecture that has been shown to be highly effective for natural language processing tasks. A down-scaled laboratory model of an offshore wind turbine that simulates the different regions of operation of the wind turbine is employed to develop and validate the proposed methodology. The vibration signals collected from 8 accelerometers are used to analyze the dynamic behavior of the structure. The results obtained show a significant improvement compared to other approaches previously proposed in the literature. In particular, the stated methodology achieves an accuracy of 99.96% with an average training time of only 6.13 minutes due to the high parallelizability of the transformer network. In fact, as it is computationally highly efficient, it has the potential to be a useful tool for implementation in real-time monitoring systems.",
        "link": "http://dx.doi.org/10.1155/2023/6646599"
    },
    {
        "id": 21850,
        "title": "Ensemble Model for Predicting Alzheimer's Disease and Disease Stages with CNN and Transformer Models",
        "authors": "Wei Chen, Fan Sun, Yi Luo, Xiaomin Wang",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3625403.3625414"
    },
    {
        "id": 21851,
        "title": "On the Use of Transformer-Based Models for Intent Detection Using Clustering Algorithms",
        "authors": "André Moura, Pedro Lima, Fábio Mendonça, Sheikh Shanawaz Mostafa, Fernando Morgado-Dias",
        "published": "2023-4-21",
        "citations": 3,
        "abstract": "Chatbots are becoming increasingly popular and require the ability to interpret natural language to provide clear communication with humans. To achieve this, intent detection is crucial. However, current applications typically need a significant amount of annotated data, which is time-consuming and expensive to acquire. This article assesses the effectiveness of different text representations for annotating unlabeled dialog data through a pipeline that examines both classical approaches and pre-trained transformer models for word embedding. The resulting embeddings were then used to create sentence embeddings through pooling, followed by dimensionality reduction, before being fed into a clustering algorithm to determine the user’s intents. Therefore, various pooling, dimension reduction, and clustering algorithms were evaluated to determine the most appropriate approach. The evaluation dataset contains a variety of user intents across different domains, with varying intent taxonomies within the same domain. Results demonstrate that transformer-based models perform better text representation than classical approaches. However, combining several clustering algorithms and embeddings from dissimilar origins through ensemble clustering considerably improves the final clustering solution. Additionally, applying the uniform manifold approximation and projection algorithm for dimension reduction can substantially improve performance (up to 20%) while using a much smaller representation.",
        "link": "http://dx.doi.org/10.3390/app13085178"
    },
    {
        "id": 21852,
        "title": "Few-Shot Learning for Identification of COVID-19 Symptoms Using Generative Pre-trained Transformer Language Models",
        "authors": "Keyuan Jiang, Minghao Zhu, Gordon R. Bernard",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-23633-4_21"
    },
    {
        "id": 21853,
        "title": "Exploring the Feasibility of Transformer Based Models on Question Relatedness",
        "authors": "Honglin Shu, Pei Gao, Ziwei Yang, Chen Li, Man Wu",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hpcc-dss-smartcity-dependsys57074.2022.00136"
    },
    {
        "id": 21854,
        "title": "CardiacSeg: Customized Pre-training Volumetric Transformer with Scaling Pyramid for 3D Cardiac Segmentation",
        "authors": "Zhiyu Ye, Hairong Zheng, Tong Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-52448-6_1"
    },
    {
        "id": 21855,
        "title": "Modal interaction-enhanced prompt learning by transformer decoder for vision-language models",
        "authors": "Mingyue Liu, Honggang Zhao, Longfei Ma, Mingyong Li",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13735-023-00287-4"
    },
    {
        "id": 21856,
        "title": "Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis",
        "authors": "Xiangzhe Xu, Shiwei Feng, Yapeng Ye, Guangyu Shen, Zian Su, Siyuan Cheng, Guanhong Tao, Qingkai Shi, Zhuo Zhang, Xiangyu Zhang",
        "published": "2023-7-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3597926.3598121"
    },
    {
        "id": 21857,
        "title": "CLASSIFICATION OF BREAST CANCER ON MAMMOGRAM IMAGES USING DEEP LEARNING MODELS AND VISION TRANSFORMER",
        "authors": "Phan Anh Cang, Vo Dinh Nghia, Tran Ho Dat, Phan Thuong Cang",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15625/vap.2023.0020"
    },
    {
        "id": 21858,
        "title": "Transformer-PSS: A High-Efficiency Prosodic Speech Synthesis Model based on Transformer",
        "authors": "Yutian Wang, Kai Cui, Hui Wang, Jingling Wang, Qin Zhang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciba50161.2020.9277162"
    },
    {
        "id": 21859,
        "title": "DETERMINING THE AMOUNT OF ELECTRONIC SCINTILLATIONS OF ELECTRO – OPTIC TRANSFORMER OF IMAGES",
        "authors": "Petar Getsov,  ",
        "published": "2018-11-15",
        "citations": 0,
        "abstract": "One of the basic requirements when using electro - optical transformer (EOT) of images in space research [1, 2, 5, 7, 8] is the low level of bright light flashes on the screen, called multi - electronic scintillations. The existence of multi - electronic scintillations leads to significant expansion of the possibility for diminishing the utmost sensitivity of the EOT [4, 6, 9, and 10]. The definition of the amount of scintillations per definite time could be done with the help of the method for scintillations' calculation while using photo - electronic multiplier [3]. If the number of scintillations' distribution over the active surface of the screen is determined and experimentally are evaluated the distribution parameters for a certain EOT, it is a matter of simpler methods required for the scintillation evaluation without the need for development of complex scanning devices. The current research is about evaluating the law of scintillations' amount distribution over the active surface of the screen, evaluation of their amplitude spectrum and development of method for defining the amount of multi - electronic scintillations over the active surface of the EOT's screen.",
        "link": "http://dx.doi.org/10.46687/jsar.v14i1.243"
    },
    {
        "id": 21860,
        "title": "Partial Discharge Signal Measurement based on Stand-alone and Hybrid Detection Technique for Power Transformer",
        "authors": "Jalil M.A.A",
        "published": "2020-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5373/jardcs/v12sp7/20202230"
    },
    {
        "id": 21861,
        "title": "Dissolved gas in transformer oil forecasting for transformer fault evaluation based on HATT-RLSTM",
        "authors": "Mingwei Zhong, Yunfei Cao, Guanglin He, Lutao Feng, Zhichao Tan, Wenjun Mo, Jingmin Fan",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2023.109431"
    },
    {
        "id": 21862,
        "title": "Silencing transformer and transformer-2 in Zeugodacus cucurbitae causes defective sex determination with inviability of most pseudomales",
        "authors": "Qin Ma, Zizhen Fan, Ping Wang, Siya Ma, Jian Wen, Fengqin Cao, Xianwu Lin, Rihui Yan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jia.2023.06.019"
    },
    {
        "id": 21863,
        "title": "Design Methodology for Inductor-Integrated Litz-Wired High-Power Medium-Frequency Transformer With the Nanocrystalline Core Material for Isolated DC-Link Stage of Solid-State Transformer",
        "authors": "Bin Chen, Xu Liang, Nina Wan",
        "published": "2020-11",
        "citations": 52,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2020.2987944"
    },
    {
        "id": 21864,
        "title": "Transformer Technician in Pop Art (Study in techniques of show)",
        "authors": "",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35560/jcofarts89/5-22"
    },
    {
        "id": 21865,
        "title": "Switched Magnetoplasma Medium",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-7"
    },
    {
        "id": 21866,
        "title": "Transformer: Principles and Practices",
        "authors": "Md. Abdus Salam",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-3212-2_2"
    },
    {
        "id": 21867,
        "title": "Reader Guide to the Penobscot Language:",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.10"
    },
    {
        "id": 21868,
        "title": "Unusual Efficiency of the Resonance Transformer",
        "authors": "Osamu Ide",
        "published": "2022-7-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecet55527.2022.9873087"
    },
    {
        "id": 21869,
        "title": "Resizer Swin Transformer based Classification using sMRI for Alzheimer’s Disease",
        "authors": "Yihang Huang, Wan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high definition and noninvasive. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimer’s disease (AD). Due to the excellent performance of Transformer in computer vision, Vision Transformer (ViT) has been employed for AD classification in recent years. ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the pre-processing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose Resizer Swin Transformer (RST), a deep learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. The experimental results of ADNI and AIBL datasets prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
        "link": "http://dx.doi.org/10.20944/preprints202307.0799.v1"
    },
    {
        "id": 21870,
        "title": "Transformer: More Than Meets the I/Eye",
        "authors": "Fetaui Iosefo",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-47527-1_12"
    },
    {
        "id": 21871,
        "title": "Categorizing transformer faults via dissolved gas analysis",
        "authors": "Randy Cox",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124733"
    },
    {
        "id": 21872,
        "title": "Phasors, 3-Phase Connections, and Symmetrical Components",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-6"
    },
    {
        "id": 21873,
        "title": "Transformer Modifications",
        "authors": "Uday Kamath, Kenneth L. Graham, Wael Emara",
        "published": "2022-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003170082-5"
    },
    {
        "id": 21874,
        "title": "Smartformer: An Intelligent Model Compression Framework for Transformer",
        "authors": "Xiaojian Wang, Yinan Wang, Jin Yang, Ying Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer, as one of the latest popular deep neural networks (DNNs), has achieved outstanding performance in various areas. However, this model usually requires massive amounts of parameters to fit. Over-parameterization not only brings storage challenges in a resource-limited setting but also inevitably results in the model over-fitting. Even though literature works introduced several ways to reduce the parameter size of Transformers, none of them have focused on addressing the over-parameterization issue by concurrently considering the following three objectives: preserving the model architecture, maintaining the model performance, and reducing the model complexity (number of parameters) to improve its convergence property. In this study, we propose an intelligent model compression framework, Smartformer, by incorporating reinforcement learning and CP-decomposition techniques. This framework has two advantages: 1) from the perspective of convenience, researchers can directly exploit this framework to train their designed Transformer models as usual without fine-tuning the original model training hyperparameters; 2) from the perspective of effectiveness, researchers can simultaneously fulfill the aforementioned three objectives by using this framework. We conduct two basic experiments and three practical experiments to demonstrate the proposed framework using various datasets and existing Transformer models. The results demonstrate that the Smartformer is able to improve the existing Transformers regarding the model performance and model complexity by intelligently incorporating the model decomposition and compression in the training process.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1780688/v1"
    },
    {
        "id": 21875,
        "title": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
        "authors": "Johannes Schneider",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892250"
    },
    {
        "id": 21876,
        "title": "Scaled Quantization for the Vision Transformer",
        "authors": "Yangyang Chang, Gerald E. Sobelman",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4397294"
    },
    {
        "id": 21877,
        "title": "Transformer Model for Cause-Specific Hazard Prediction",
        "authors": "Matthieu Oliver, Nicolas Allou, Marjolaine Devineau, Jérôme Allyn, Cyril Ferdynus",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4333614"
    },
    {
        "id": 21878,
        "title": "Chapter 2: Tokenization",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-003"
    },
    {
        "id": 21879,
        "title": "Empowering Dysarthric Communication: Hybrid Transformer-CTC based Speech Recognition System",
        "authors": "Vinotha R, Hepsiba D, Vijay Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPeople suffering from speech disorders face various challenges in their ability to communicate effectively, and one of the conditions they may experience is dysarthria. Dysarthria is a motor speech disorder that impacts an individual's ability to speak due to difficulties in controlling the muscles responsible for speech production. People with dysarthria may experience difficulties with articulation, pronunciation, intonation, rhythm, and pace, resulting in a slow or slurred speech pattern that can be difficult to understand. Augmentative and Alternative Communication (AAC) aids that utilize speech recognition technology have emerged as an appealing solution to support communication for individuals with dysarthria. Automatic Speech Recognition (ASR) systems trained solely on normal speech data may not accurately recognize dysarthric speech due to variations in their speech patterns and accent differences. However, a significant challenge in training ASR systems for dysarthric speech is the limited availability of data. To overcome these challenges, a hybrid architecture using the Transformer and Connectionist Temporal Classification (CTC) approach is proposed in this work. The transformer architecture is effective in learning speech patterns using limited data due to its self-attention mechanism, while the CTC approach allows for direct mapping between input speech features and output character sequences without requiring explicit alignment information. This approach is especially beneficial for speech recognition in situations involving variations in speech patterns. A hybrid architecture is trained using UA speech corpus that allows it to focus on important features of the speech and capture the relationships between them, leading to more accurate speech recognition. The performance of the proposed ASR system shows a remarkable decrease in Word Error Rate (WER) up to 2.78% and 15.67% for individuals with dysarthria who have low and very low intelligibility, respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3736965/v1"
    },
    {
        "id": 21880,
        "title": "Simulation design of residential smart-grid based on solid-state transformer",
        "authors": "Kristian Takacs, Michal Frivaldsky",
        "published": "2022-5-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elektro53996.2022.9803792"
    },
    {
        "id": 21881,
        "title": "Multi-Channel Transformer Transducer for Speech Recognition",
        "authors": "Feng-Ju Chang, Martin Radfar, Athanasios Mouchtaris, Maurizio Omologo",
        "published": "2021-8-30",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-655"
    },
    {
        "id": 21882,
        "title": "Peer Review #1 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.2)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.2/reviews/1"
    },
    {
        "id": 21883,
        "title": "Design Selection of Transformer on Based Load Capacities for Industrial Zone",
        "authors": "Aye Myo Thant,  ",
        "published": "2019-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd23983"
    },
    {
        "id": 21884,
        "title": "Effect of Permittivity on Breakdown characteristic of Transformer oil based Nanofluid",
        "authors": "Mihir A Bhatt,  ",
        "published": "2018-4-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd10885"
    },
    {
        "id": 21885,
        "title": "Evolution in Solid-State Transformer and Power Electronic Transformer for Distribution and Traction System",
        "authors": "Shivam Sharma, Ruhul Amin Chaudhary,  Kamalpreet Singh",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-5802-9_115"
    },
    {
        "id": 21886,
        "title": "Fake News Detection Based on Emotional Multi-Head Attention Convolution Transformer + CNN",
        "authors": "亚立 张",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/hjdm.2023.134030"
    },
    {
        "id": 21887,
        "title": "Harmful Cyanobacterial Blooms forecasting based on improved CNN-Transformer and Temporal Fusion Transformer",
        "authors": "Jung Min Ahn, Jungwook Kim, Hongtae Kim, Kyunghyun Kim",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eti.2023.103314"
    },
    {
        "id": 21888,
        "title": "Eddy Currents and Winding Stray Losses",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-4"
    },
    {
        "id": 21889,
        "title": "Application of electrical steels in transformer cores",
        "authors": "",
        "published": "2019-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo157g_ch5"
    },
    {
        "id": 21890,
        "title": "Next-Generation Modular Flexible Low-Cost SiC-based High-Frequency-Link Transformer (Final Technical Report)",
        "authors": "Sudip Mazumder",
        "published": "2023-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1974885"
    },
    {
        "id": 21891,
        "title": "Tolstoy’s Genius Explored by Deep Learning Using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670984"
    },
    {
        "id": 21892,
        "title": "High-Voltage, High-Frequency Transformer Design*",
        "authors": "Paul Lefley, Philip Devine",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-22"
    },
    {
        "id": 21893,
        "title": "Modeling movie-evoked human brain activity using motion-energy and space-time vision transformer features",
        "authors": "Shinji Nishimoto",
        "published": "No Date",
        "citations": 0,
        "abstract": "SummaryIn this paper, the process of building a model for predicting human brain activity under video viewing conditions was described as a part of an entry into the Algonauts Project 2021 Challenge. The model was designed to predict brain activity measured using functional MRI (fMRI) by weighted linear summations of the spatiotemporal visual features that appear in the video stimuli (video features). Two types of video features were used: (1) motion-energy features designed based on neurophysiological findings, and (2) features derived from a space-time vision transformer (TimeSformer). To utilize the features of various video domains, the features of the TimeSformer models pre-trained using several different movie sets were combined. Through these model building and validation processes, results showed that there is a certain correspondence between the hierarchical representation of the TimeSformer model and the hierarchical representation of the visual system in the brain. The motion-energy features are effective in predicting brain activity in the early visual areas, while TimeSformer-derived features are effective in higher-order visual areas, and a hybrid model that uses motion energy and TimeSformer features is effective for predicting whole brain activity.",
        "link": "http://dx.doi.org/10.1101/2021.08.22.457251"
    },
    {
        "id": 21894,
        "title": "Problem and Failure Investigation",
        "authors": "Wallace B. Binder, Harold Moore",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-23"
    },
    {
        "id": 21895,
        "title": "Multistage Temporal Convolution Transformer for Action Segmentation",
        "authors": "Nicolas Aziere, Sinisa Todorovic",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4217347"
    },
    {
        "id": 21896,
        "title": "TIS Transformer: Remapping the Human Proteome Using Deep Learning",
        "authors": "Jim Clauwaert, Ramneek Gupta, Zahra McVey, Gerben Menschaert",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe correct mapping of the proteome is an important step towards advancing our understanding of biological systems and cellular mechanisms. Methods that provide better mappings can fuel important processes such as drug discovery and disease understanding. Currently, true determination of translation initiation sites is primarily achieved byin vivoexperiments. Here we propose TIS Transformer, a deep learning model for the determination of translation start sites solely utilizing the information embedded in the transcript nucleotide sequence. The method is built upon deep learning techniques first designed for natural language processing. We prove this approach to be best suited for learning the semantics of translation, outperforming previous approaches by a large margin. We demonstrate that limitations in the model performance is primarily due to the presence of low quality annotations against which the model is evaluated against. Advantages of the method are its ability to detect key features of the translation process and multiple coding sequences on a transcript. These include micropeptides encoded by short Open Reading Frames, either alongside a canonical coding sequence or within long non-coding RNAs. To demonstrate the use of our methods, we applied TIS Transformer to remap the full human proteome.",
        "link": "http://dx.doi.org/10.1101/2021.11.18.468957"
    },
    {
        "id": 21897,
        "title": "Chromatic Analysis of High-Voltage Transformer Oils Data",
        "authors": "E. Elzagzoug, G. R. Jones",
        "published": "2020-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367815202-20"
    },
    {
        "id": 21898,
        "title": "Predicting Bone Degradation Using Vision Transformer and Synthetic Cellular Microstructures Dataset",
        "authors": "Moahmmad Saber Hashemi, Azadeh Sheidaei",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBone degradation, especially for astronauts in microgravity conditions, is crucial for space exploration missions since the lower applied external forces accelerate the diminution in bone stiffness and strength substantially. Although existing computational models help us understand this phenomenon and possibly restrict its effect in the future, they are time-consuming to simulate the changes in the bones, not just the bone microstructures, of each individual in detail. In this study, a robust yet fast computational method to predict and visualize bone degradation has been developed. Our deep-learning method, TransVNet, can take in different 3D voxelized images and predict their evolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer autoencoder architecture. Because of limited available experimental data and challenges of obtaining new samples, a digital twin dataset of diverse and initial bone-like microstructures was generated to train our TransVNet on the evolution of the 3D images through a previously developed degradation model for microgravity. \nArticle Highlights:\n• Long-term manned space missions face the challenges of microgravity-accelerated bone degradation.\n• We developed a suitable AI generalizing and accelerating the bone degradation prediction using our synthetic dataset.\n• The results show the high accuracy and scalability of our proposed TransVNet compared with conventional models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3974175/v1"
    },
    {
        "id": 21899,
        "title": "Impact of TiO2 Nanoparticles on Physical and  Electrical Properties of Mustard Oil for Power  Transformer",
        "authors": "Muhammad Furqan Hameed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vegetable oils are an effective alternative to mineral oil for power transformers due to their high biodegradability and excellent dielectric strength. Moreover, adding a small number of nanoparticles (NPs) to vegetable oils can cause a significant increase in the dielectric strength of vegetable oils. This paper presents an experimental study of the physical and electrical properties of Mustard oil and its titanium dioxide (TiO2) based nanofluids (NFs). Physical properties include viscosity, water content, specific gravity, acidic content, flash point, and pour point whereas electrical properties include the AC dielectric  strength and partial discharge analysis. All the properties were determined according to IEC/ISO standards. Nanofluids were  characterized using scanning electron microscopy and x-ray  dispersion analyzer. Titanium dioxide based nanofluids were added to pure Mustard oil in varying quantities of 0.02 g/liter,  0.05 g/liter, and 0.1 g/liter to make NFs. The experimental  results indicate that 0.05 g/L NF has best properties among all  the four samples, and it has the ability to substitute conventional  mineral oil. Experimental study showed that there was an  increase of 40% in AC dielectric strength, 3 times increase in the  partial discharge inception and extinction voltage when 0.05 g/L  TiO2 was added in Mineral oil as compared to the experimental  study of pure Mineral oil. The comparison of physical and  electrical properties of the said sample was compared with  mineral oil and other vegetable oils. It has been observed that 0.05 g/L TiO2 is also cost effective alternative to mineral oil and  other vegetable oils. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20419629"
    },
    {
        "id": 21900,
        "title": "News Topic Prediction Via Transformer",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2022.06.023"
    },
    {
        "id": 21901,
        "title": "TRANSFORMER PROTECTION",
        "authors": "",
        "published": "2020-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119709787.ch11"
    },
    {
        "id": 21902,
        "title": "Flexformer: Flexible Transformer for Efficient Visual Recognition",
        "authors": "Xinyi Fan, Huajun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4184550"
    },
    {
        "id": 21903,
        "title": "End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network",
        "authors": "Binghuai Lin, Liyuan Wang",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-774"
    },
    {
        "id": 21904,
        "title": "Diffusion Transformer for Adaptive Text-to-Speech",
        "authors": "Haolin Chen, Philip N. Garner",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/ssw.2023-25"
    },
    {
        "id": 21905,
        "title": "This robot is a real-life transformer, but not like any you’ve seen before",
        "authors": "",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1126/science.adj4447"
    },
    {
        "id": 21906,
        "title": "Comparative Analysis of Transformer Solid Insulation Drying Methods",
        "authors": "Mazur Oksana, Dannik Alexander",
        "published": "2022-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic51169.2022.9833182"
    },
    {
        "id": 21907,
        "title": "Transformer Based Period Spatial-Temporal Graph Convolutional Network for Traffic Forecasting",
        "authors": "Jun Yin, Bo Li, Zhiheng Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4572179"
    },
    {
        "id": 21908,
        "title": "Automatic Modulation Classification Based on Improved R-Transformer",
        "authors": "Xueyuan Liu",
        "published": "2021-6-28",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc51323.2021.9498878"
    },
    {
        "id": 21909,
        "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
        "authors": "Axel Berg, Mark O’Connor, Miguel Tairum Cruz",
        "published": "2021-8-30",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1286"
    },
    {
        "id": 21910,
        "title": "Experiment 13 Load Test on a Single-Phase Transformer",
        "authors": "",
        "published": "2017-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683922797-013"
    },
    {
        "id": 21911,
        "title": "Peer Review #3 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.2)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.2/reviews/3"
    },
    {
        "id": 21912,
        "title": "Peer Review #3 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.1)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.1/reviews/3"
    },
    {
        "id": 21913,
        "title": "Rtrl: Relation-Aware Transformer with Reinforcement Learning for Deep Question Generation",
        "authors": "Hongwei Zeng, Bifan Wei, Jun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4730126"
    },
    {
        "id": 21914,
        "title": "La reconsolidation thérapeutique de la mémoire",
        "authors": "",
        "published": "2022-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dunod.cote.2022.01"
    },
    {
        "id": 21915,
        "title": "Decision letter for \"New family of transformer‐less quadratic buck‐boost converters with wide conversion ratio\"",
        "authors": "",
        "published": "2021-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13061/v1/decision1"
    },
    {
        "id": 21916,
        "title": "Vineyard detection from multitemporal Sentinel-2 images with a Transformer model",
        "authors": "Weiying Zhao, Alexey Unagaev, Natalia Efremova",
        "published": "No Date",
        "citations": 0,
        "abstract": "This study introduces an innovative method for vineyard detection by integrating advanced machine learning techniques with high-resolution satellite imagery, particularly focusing on the use of preprocessed multitemporal Sentinel-2 images combined with a Transformer-based model.\nWe collected a series of Sentinel-2 images over an entire seasonal cycle from eight distinct locations in Oregon, United States, all within similar climatic zones. The training and validation database sizes are 403612 and 100903, respectively. To reduce the cloud effect, we used the monthly median band values derived from initially cloud-filtered images. &#160;The multispectral (12 bands) and multiscale (10m, 20m, and 60m) time series were effective in capturing both the phenological patterns of the land covers and the overall management activities.\nThe Transformer model, primarily recognized for its successes in natural language processing tasks, was adapted for our time series identification scenario. Then, we transferred the object detection into a binary classification task. Our findings demonstrate that the Transformer model significantly surpasses traditional 1D convolutional neural networks (CNNs) in detecting vineyards across 16 new areas within similar climatic zones, boasting an impressive accuracy of 87.77% and an F1 score of 0.876. In the majority of these new test locations, the accuracy exceeded 92%, except for two areas that experienced significant cloud interference and presented numerous missing values in their time series data. This model proved its capability to differentiate between land covers with similar characteristics during various stages of growth throughout the season. Compared with attention LSTM and BiLSTM, it has less trainable parameters when getting a similar performance. The model was especially adept at handling temporal variations, elucidating the dynamic changes in vineyard phenology over time. This research underscores the potential of combining advanced machine learning techniques with high-resolution satellite imagery for crop type detection and suggests broader applications in land cover classification tasks. Future research will pay more attention to the missing value problem.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-20025"
    },
    {
        "id": 21917,
        "title": "CpG Transformer for imputation of single-cell methylomes",
        "authors": "Gaetan De Waele, Jim Clauwaert, Gerben Menschaert, Willem Waegeman",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotivationThe adoption of current single-cell DNA methylation sequencing protocols is hindered by incomplete coverage, outlining the need for effective imputation techniques. The task of imputing single-cell (methylation) data requires models to build an understanding of underlying biological processes.ResultsWe adapt the transformer neural network architecture to operate on methylation matrices through combining axial attention with sliding window self-attention. The obtained CpG Transformer displays state-of-the-art performances on a wide range of scBS-seq and scRRBS-seq datasets. Further-more, we demonstrate the interpretability of CpG Transformer and illustrate its rapid transfer learning properties, allowing practitioners to train models on new datasets with a limited computational and time budget.Availability and ImplementationCpG Transformer is freely available at https://github.com/gdewael/cpg-transformer.",
        "link": "http://dx.doi.org/10.1101/2021.06.08.447547"
    },
    {
        "id": 21918,
        "title": "Determination of Dielectric Losses in a Power Transformer",
        "authors": "Zbigniew Nadolny",
        "published": "2022-1-28",
        "citations": 7,
        "abstract": "The article presents a method of determining dielectric losses that occur in insulating materials in a power transformer. These losses depend mainly on the electric field stress, pulsation, dielectric loss coefficient, and electrical permittivity of insulating materials. These losses were determined by integrating an expression describing unit losses. The determined dielectric losses were compared with the total losses of the transformer. It turned out that dielectric losses are a fraction of a percent of the total losses. The influence of the electrical permittivity of the insulating liquid and paper insulation on the value of dielectric losses was investigated. This influence was ambiguous, which is characteristic of stratified systems made of materials with different permittivity. An analysis of the influence of the dielectric loss coefficient tan(delta) on the value of dielectric losses in the transformer was carried out. The impact of this coefficient on the amount of dielectric losses turned out to be directly proportional.",
        "link": "http://dx.doi.org/10.3390/en15030993"
    },
    {
        "id": 21919,
        "title": "Three-phase Transformer Modeling",
        "authors": "Steven H. Low",
        "published": "2023-9-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313400"
    },
    {
        "id": 21920,
        "title": "Image Super-Resolution Reconstruction Based on Transformer and Non-Separable Additive Wavelet",
        "authors": "斌 刘",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/jisp.2023.121005"
    },
    {
        "id": 21921,
        "title": "A Model of Pole-Mounted Distribution Transformer for Lightning Failure Analysis",
        "authors": "Kenichi Kanatani, Susumu Matsuura, Hirofumi Fujita, Koji Michishita",
        "published": "2023-2-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1541/ieejpes.143.165"
    },
    {
        "id": 21922,
        "title": "Modelling of Piezoelectric Sensor based on ZnO Material for Partial Discharge Detection on Power Transformer",
        "authors": "Akashah N.A",
        "published": "2020-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5373/jardcs/v12sp7/20202228"
    },
    {
        "id": 21923,
        "title": "Enhancing Summarization Performance Through Transformer-Based Prompt Engineering in Automated Medical Reporting",
        "authors": "Daphne van Zandvoort, Laura Wiersema, Tom Huibers, Sandra van Dulmen, Sjaak Brinkkemper",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012422600003657"
    },
    {
        "id": 21924,
        "title": "Analysis of Transformer attention in EEG signal classification",
        "authors": "Philipp Thölke, Karim Jerbi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1240-0"
    },
    {
        "id": 21925,
        "title": "Works Consulted and Suggestions for Further Reading",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.25"
    },
    {
        "id": 21926,
        "title": "- Load-Tap-Change Control and Transformer Paralleling",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-51"
    },
    {
        "id": 21927,
        "title": "World’s first microscale ‘transformer’ robot",
        "authors": "Tian-Yun Huang, Huiling Duan",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25250/thescbr.brk424"
    },
    {
        "id": 21928,
        "title": "Stray Losses from 3D Finite Element Analysis",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-17"
    },
    {
        "id": 21929,
        "title": "Vector analysis of transformer tank vibration",
        "authors": "Eugeniusz Kornatowski",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imitel.2018.8370475"
    },
    {
        "id": 21930,
        "title": "Advertisement [ENPAY TRANSFORMER COMPONENTS]",
        "authors": "",
        "published": "2018-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mei.2018.8300454"
    },
    {
        "id": 21931,
        "title": "Technical Notes on the Newell Lyon Text",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.24"
    },
    {
        "id": 21932,
        "title": "6Former: Transformer-Based IPv6 Address Generation",
        "authors": "Qiankun Liu, Xing Li",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscc58397.2023.10218311"
    },
    {
        "id": 21933,
        "title": "Demystifying Hybrid DC-DC Converters Using an Improved Transformer Model",
        "authors": "MO HUANG, tingxu hu, yan lu, rui martins",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Multiple hybrid DC-DC converter topologies emerged to achieve better efficiency and power density for various applications. However, it is unclear how these topologies evolve from the original DC model. Therefore, it would be difficult to intuitively understand the pros and cons of the topologies only from the circuit level point-of-view. In this work, we propose an improved transformer model for hybrid converters. The proposed model demystifies the converters, explaining how they evolve, and how such evolution affects efficiency and other performances. Meanwhile, we can directly predict the important features of the converters from the proposed model, such as the working principles, conversion ratio, switch stress, inductor currents balance/equalization, switch loss, switching node voltage changing times, and flying capacitor voltage balance. To verify the effectiveness of the proposed model we utilize it in six popular hybrid converters. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22002272"
    },
    {
        "id": 21934,
        "title": "Demystifying Hybrid DC-DC Converters Using an Improved Transformer Model",
        "authors": "MO HUANG, tingxu hu, yan lu, rui martins",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Multiple hybrid DC-DC converter topologies emerged to achieve better efficiency and power density for various applications. However, it is unclear how these topologies evolve from the original DC model. Therefore, it would be difficult to intuitively understand the pros and cons of the topologies only from the circuit level point-of-view. In this work, we propose an improved transformer model for hybrid converters. The proposed model demystifies the converters, explaining how they evolve, and how such evolution affects efficiency and other performances. Meanwhile, we can directly predict the important features of the converters from the proposed model, such as the working principles, conversion ratio, switch stress, inductor currents balance/equalization, switch loss, switching node voltage changing times, and flying capacitor voltage balance. To verify the effectiveness of the proposed model we utilize it in six popular hybrid converters. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22002272.v1"
    },
    {
        "id": 21935,
        "title": "Robust Transformer Tracking with Minimum Entropy Criterion Model Update",
        "authors": "Zhiyong An, geng chen, Qifeng Liang, Zhongliang Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4504064"
    },
    {
        "id": 21936,
        "title": "Transformer Insulation Paper Moisture Measurement Using Optical Fiber Sensor",
        "authors": "Deba Kumar Mahanta",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp50017.2020.9230830"
    },
    {
        "id": 21937,
        "title": "Performer la visite d'un quartier et transformer la perception des lieux",
        "authors": "Clémence Canet",
        "published": "2023-8-8",
        "citations": 0,
        "abstract": "\r\n\r\n\r\nIn July 2022, the artist Anne-Sophie Turion realizes the tour-performance Grandeur Nature in two districts of Rennes. Instead of commenting on the built heritage, as one might have expected in the context of an urban tour, the performer restores the lived stories of the inhabitants of the neighborhoods. Based on the analysis of this artistic proposal, the article seeks to understand how the device contributes to reconfiguring the perception of the urban territory covered. We are interested in how performance reveals spaces as they are occupied and perceived by users ; then we try to show the links that the proposal weaves between the protagonists and we try to understand to what extent the common experience of the neighborhoods crossed contributes to create social ties.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.54103/2039-9251/20817"
    },
    {
        "id": 21938,
        "title": "Welch Allyn Wall Transformer, Blood Pressure Device",
        "authors": "",
        "published": "2021-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1097/01.bmsas.0000766976.32830.6c"
    },
    {
        "id": 21939,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "Herein we present a new retrosynthesis prediction method, viz. RetroTRAE, which uses fragment-based tokenization combined with the Transformer architecture. RetroTRAE mimics chemical reasoning, and predicts reactant candidates by learning the changes of atomic environments associated with the chemical reaction. Atom environments stand as ideal, chemically meaningful building blocks, which together produce a high-resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to the conservation of atoms in the reactions. Our model achieved a top-1 accuracy of 68.1% within the bioactively similar range for the USPTO test dataset, outperforming other state-of-the-art translation methods. Besides yielding a high level of overall accuracy, the proposed method solves the translation issues arising from the SMILES-based retrosynthesis planning methods effectively. Through careful inspection of reactant candidates, we demonstrated atom environments as promising descriptors for studying reaction route prediction and discovery. RetroTRAE provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed. Our methodology offers a novel way of devising a retrosynthetic planning model using fragmental and topological descriptors as natural inputs for chemical translation tasks.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-9g274-v2"
    },
    {
        "id": 21940,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a new single-step retrosynthesis prediction method, viz. RetroTRAE, using fragment-based tokenization and the Transformer architecture. RetroTRAE mimics chemical reasoning, and predicts reactant candidates by learning the changes of atom environments (AEs) associated with the chemical reaction. AEs are the ideal stand-alone chemically meaningful building blocks providing a high-resolution molecular representation. Describing a molecule with a set of AEs establishes a clear relationship between translated product-reactant pairs due to the conservation of atoms in the reactions. Our model achieved a top-1 accuracy of 58.3% on the USPTO test dataset. When highly similar analogs were considered the accuracy increased to 61.6%. These results outperform other state-of-the-art neural machine translation-based methods. Besides yielding a high level of overall accuracy, the proposed method does not suffer from the SMILES-based translation issues such as invalid SMILES. Additionally, the attention matrices of RetroTRAE are shown to capture chemical changes around reaction sites successfully. Through careful inspection of reactant candidates, we demonstrated that AEs are promising descriptors for studying reaction route prediction and discovery, which has been underexplored yet. Our methodology offers a novel way of devising a retrosynthetic planning model using fragmental and topological descriptors as natural inputs for chemical translation tasks, and opens new possibilities for developing other sequence-based machine-learning methods in chemistry.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-9g274-v3"
    },
    {
        "id": 21941,
        "title": "Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer",
        "authors": "Huu Phong Nguyen, Bernardete Ribeiro",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nRecognizing human actions in video sequences, known as Human Action Recognition (HAR), is a challenging task in pattern recognition. While Convolutional Neural Networks (ConvNets) have shown remarkable success in image recognition, they are not always directly applicable to HAR, as temporal features are critical for accurate classification. In this paper, we propose a novel dynamic PSO-ConvNet model for learning actions in videos, building on our recent work in image recognition. Our approach leverages a framework where the weight vector of each neural network represents the position of a particle in phase space, and particles share their current weight vectors and gradient estimates of the Loss function. To extend our approach to video, we integrate ConvNets with state-of-the-art temporal methods such as Transformer and Recurrent Neural Networks. Our experimental results on the UCF-101 dataset demonstrate substantial improvements of up to 9% in accuracy, which confirms the effectiveness of our proposed method. Overall, our dynamic PSO-ConvNet model provides a promising direction for improving HAR by better capturing the spatio-temporal dynamics of human actions in videos. The code is available at https://github.com/leonlha/Video-Action-Recognition-Collaborative-Learning-with-Dynamics-via-PSO-ConvNet-Transformer.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2600609/v1"
    },
    {
        "id": 21942,
        "title": "Transformer la culture de l’habiter",
        "authors": "Flavien Menu",
        "published": "2020-11-4",
        "citations": 1,
        "abstract": "Y a-t-il une alternative au mode d’habitation hérité de la société industrielle et qui tienne compte des mutations sociétales de nos modes de vie ? Des initiatives ont émergé, en Europe et aux États-Unis, adoptant une démarche collective, comme le community-led housing et d’autres formes de co-housing . Elles ont toutes pour principe de mettre le foncier en commun et de faire de tous les habitants des acteurs du changement.",
        "link": "http://dx.doi.org/10.3917/tu.030.0022"
    },
    {
        "id": 21943,
        "title": "Transformer – Textgenerierung, Industrieanwendungen und Grenzen",
        "authors": "Roland Kunz",
        "published": "2024-1-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783111351490-028"
    },
    {
        "id": 21944,
        "title": "Transformer Real Time Health Monitoring System",
        "authors": "Prachi Chandel, Harshal Sao, Pankaj Chandankhede",
        "published": "2022-2-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/delcon54057.2022.9753348"
    },
    {
        "id": 21945,
        "title": "Improving Classification of Remotely Sensed Images with the Swin Transformer",
        "authors": "Fatema-E Jannat, Andrew R. Willis",
        "published": "2022-3-26",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon48659.2022.9764016"
    },
    {
        "id": 21946,
        "title": "WITHDRAWN: Long-Term Prediction of Network Security Situation Through the Use of the Transformer-Based Model",
        "authors": "",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn, as it was submitted in error. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1553264/v2"
    },
    {
        "id": 21947,
        "title": "Rdtn: Residual Densely Transformer Network for Hyperspectral Image Classification",
        "authors": "Yan Li, Xiaofei YANG, Dong Tang, Zheng Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4749748"
    },
    {
        "id": 21948,
        "title": "Chapter 7: Transformer, BERT, and GPT",
        "authors": "",
        "published": "2021-6-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683926566-008"
    },
    {
        "id": 21949,
        "title": "DETERMINING THE AMOUNT OF ELECTRONIC SCINTILLATIONS OF ELECTRO – OPTIC TRANSFORMER OF IMAGES",
        "authors": "Petar Getsov,  ",
        "published": "2018-11-15",
        "citations": 0,
        "abstract": "One of the basic requirements when using electro - optical transformer (EOT) of images in space research [1, 2, 5, 7, 8] is the low level of bright light flashes on the screen, called multi - electronic scintillations. The existence of multi - electronic scintillations leads to significant expansion of the possibility for diminishing the utmost sensitivity of the EOT [4, 6, 9, and 10]. The definition of the amount of scintillations per definite time could be done with the help of the method for scintillations' calculation while using photo - electronic multiplier [3]. If the number of scintillations' distribution over the active surface of the screen is determined and experimentally are evaluated the distribution parameters for a certain EOT, it is a matter of simpler methods required for the scintillation evaluation without the need for development of complex scanning devices. The current research is about evaluating the law of scintillations' amount distribution over the active surface of the screen, evaluation of their amplitude spectrum and development of method for defining the amount of multi - electronic scintillations over the active surface of the EOT's screen.",
        "link": "http://dx.doi.org/10.46687/jsar.v14i1.243"
    },
    {
        "id": 21950,
        "title": "Partial Discharge Signal Measurement based on Stand-alone and Hybrid Detection Technique for Power Transformer",
        "authors": "Jalil M.A.A",
        "published": "2020-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5373/jardcs/v12sp7/20202230"
    },
    {
        "id": 21951,
        "title": "Dissolved gas in transformer oil forecasting for transformer fault evaluation based on HATT-RLSTM",
        "authors": "Mingwei Zhong, Yunfei Cao, Guanglin He, Lutao Feng, Zhichao Tan, Wenjun Mo, Jingmin Fan",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2023.109431"
    },
    {
        "id": 21952,
        "title": "Transformer-PSS: A High-Efficiency Prosodic Speech Synthesis Model based on Transformer",
        "authors": "Yutian Wang, Kai Cui, Hui Wang, Jingling Wang, Qin Zhang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciba50161.2020.9277162"
    },
    {
        "id": 21953,
        "title": "Design Methodology for Inductor-Integrated Litz-Wired High-Power Medium-Frequency Transformer With the Nanocrystalline Core Material for Isolated DC-Link Stage of Solid-State Transformer",
        "authors": "Bin Chen, Xu Liang, Nina Wan",
        "published": "2020-11",
        "citations": 52,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2020.2987944"
    },
    {
        "id": 21954,
        "title": "Silencing transformer and transformer-2 in Zeugodacus cucurbitae causes defective sex determination with inviability of most pseudomales",
        "authors": "Qin Ma, Zizhen Fan, Ping Wang, Siya Ma, Jian Wen, Fengqin Cao, Xianwu Lin, Rihui Yan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jia.2023.06.019"
    },
    {
        "id": 21955,
        "title": "FakeRevealer: A Multimodal Framework for Revealing the Falsity of Online Tweets Using Transformer-Based Architectures",
        "authors": "Sakshi Kalra, Yashvardhan Sharma, Priyansh Vyas, Gajendra Chauhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011889800003411"
    },
    {
        "id": 21956,
        "title": "Transformer-DW: A Transformer Network with Dynamic and Weighted Head",
        "authors": "Ruxin Tan, Jiahui Sun, Bo Su, Gongshen Liu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-36711-4_42"
    },
    {
        "id": 21957,
        "title": "Analysis of Overvoltages on Power Transformer Recorded by Transient Overvoltage Monitoring System",
        "authors": "Bozidar Filipovic-Grcic, Bruno Jurišić, Samir Keitoue, Ivan Murat, Dalibor Filipovic-Grcic, Alan Zupan",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_8"
    },
    {
        "id": 21958,
        "title": "Analytical discrete Fourier transformer‐based phasor estimation method for reducing transient impact of capacitor voltage transformer",
        "authors": "Mohsen Tajdinian, Mehdi Allahbakhshi, Ali Reza Seifi, Alireza Bagheri",
        "published": "2017-6",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/iet-gtd.2016.1784"
    },
    {
        "id": 21959,
        "title": "A bidirectional three‐phase triple‐voltage LLC (T\n            <sup>2</sup>\n            ‐LLC) resonant converter for DC/DC stage in solid state transformer and DC transformer applications",
        "authors": "Liangcai Shu, Wu Chen, Haozhe Jin",
        "published": "2022-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pel2.12243"
    },
    {
        "id": 21960,
        "title": "Vision-Perceptual Transformer Network for Semantic Scene Understanding",
        "authors": "Mohamad Alansari, Hamad AlRemeithi, Bilal Hassan, Sara Alansari, Jorge Dias, Majid Khonji, Naoufel Werghi, Sajid Javed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012313800003660"
    },
    {
        "id": 21961,
        "title": "Transformer Technician in Pop Art (Study in techniques of show)",
        "authors": "",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35560/jcofarts89/5-22"
    },
    {
        "id": 21962,
        "title": "Switched Magnetoplasma Medium",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-7"
    },
    {
        "id": 21963,
        "title": "Transformer: Principles and Practices",
        "authors": "Md. Abdus Salam",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-3212-2_2"
    },
    {
        "id": 21964,
        "title": "Reader Guide to the Penobscot Language:",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.10"
    },
    {
        "id": 21965,
        "title": "Unusual Efficiency of the Resonance Transformer",
        "authors": "Osamu Ide",
        "published": "2022-7-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecet55527.2022.9873087"
    },
    {
        "id": 21966,
        "title": "Transformer: More Than Meets the I/Eye",
        "authors": "Fetaui Iosefo",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-47527-1_12"
    },
    {
        "id": 21967,
        "title": "Categorizing transformer faults via dissolved gas analysis",
        "authors": "Randy Cox",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124733"
    },
    {
        "id": 21968,
        "title": "Phasors, 3-Phase Connections, and Symmetrical Components",
        "authors": "",
        "published": "2017-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920-6"
    },
    {
        "id": 21969,
        "title": "Transformer Modifications",
        "authors": "Uday Kamath, Kenneth L. Graham, Wael Emara",
        "published": "2022-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003170082-5"
    },
    {
        "id": 21970,
        "title": "Simulation design of residential smart-grid based on solid-state transformer",
        "authors": "Kristian Takacs, Michal Frivaldsky",
        "published": "2022-5-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elektro53996.2022.9803792"
    },
    {
        "id": 21971,
        "title": "Smartformer: An Intelligent Model Compression Framework for Transformer",
        "authors": "Xiaojian Wang, Yinan Wang, Jin Yang, Ying Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer, as one of the latest popular deep neural networks (DNNs), has achieved outstanding performance in various areas. However, this model usually requires massive amounts of parameters to fit. Over-parameterization not only brings storage challenges in a resource-limited setting but also inevitably results in the model over-fitting. Even though literature works introduced several ways to reduce the parameter size of Transformers, none of them have focused on addressing the over-parameterization issue by concurrently considering the following three objectives: preserving the model architecture, maintaining the model performance, and reducing the model complexity (number of parameters) to improve its convergence property. In this study, we propose an intelligent model compression framework, Smartformer, by incorporating reinforcement learning and CP-decomposition techniques. This framework has two advantages: 1) from the perspective of convenience, researchers can directly exploit this framework to train their designed Transformer models as usual without fine-tuning the original model training hyperparameters; 2) from the perspective of effectiveness, researchers can simultaneously fulfill the aforementioned three objectives by using this framework. We conduct two basic experiments and three practical experiments to demonstrate the proposed framework using various datasets and existing Transformer models. The results demonstrate that the Smartformer is able to improve the existing Transformers regarding the model performance and model complexity by intelligently incorporating the model decomposition and compression in the training process.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1780688/v1"
    },
    {
        "id": 21972,
        "title": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
        "authors": "Johannes Schneider",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892250"
    },
    {
        "id": 21973,
        "title": "Scaled Quantization for the Vision Transformer",
        "authors": "Yangyang Chang, Gerald E. Sobelman",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4397294"
    },
    {
        "id": 21974,
        "title": "Transformer Model for Cause-Specific Hazard Prediction",
        "authors": "Matthieu Oliver, Nicolas Allou, Marjolaine Devineau, Jérôme Allyn, Cyril Ferdynus",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4333614"
    },
    {
        "id": 21975,
        "title": "Resizer Swin Transformer based Classification using sMRI for Alzheimer’s Disease",
        "authors": "Yihang Huang, Wan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high definition and noninvasive. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimer’s disease (AD). Due to the excellent performance of Transformer in computer vision, Vision Transformer (ViT) has been employed for AD classification in recent years. ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the pre-processing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose Resizer Swin Transformer (RST), a deep learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. The experimental results of ADNI and AIBL datasets prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
        "link": "http://dx.doi.org/10.20944/preprints202307.0799.v1"
    },
    {
        "id": 21976,
        "title": "Multi-Channel Transformer Transducer for Speech Recognition",
        "authors": "Feng-Ju Chang, Martin Radfar, Athanasios Mouchtaris, Maurizio Omologo",
        "published": "2021-8-30",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-655"
    },
    {
        "id": 21977,
        "title": "Empowering Dysarthric Communication: Hybrid Transformer-CTC based Speech Recognition System",
        "authors": "Vinotha R, Hepsiba D, Vijay Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPeople suffering from speech disorders face various challenges in their ability to communicate effectively, and one of the conditions they may experience is dysarthria. Dysarthria is a motor speech disorder that impacts an individual's ability to speak due to difficulties in controlling the muscles responsible for speech production. People with dysarthria may experience difficulties with articulation, pronunciation, intonation, rhythm, and pace, resulting in a slow or slurred speech pattern that can be difficult to understand. Augmentative and Alternative Communication (AAC) aids that utilize speech recognition technology have emerged as an appealing solution to support communication for individuals with dysarthria. Automatic Speech Recognition (ASR) systems trained solely on normal speech data may not accurately recognize dysarthric speech due to variations in their speech patterns and accent differences. However, a significant challenge in training ASR systems for dysarthric speech is the limited availability of data. To overcome these challenges, a hybrid architecture using the Transformer and Connectionist Temporal Classification (CTC) approach is proposed in this work. The transformer architecture is effective in learning speech patterns using limited data due to its self-attention mechanism, while the CTC approach allows for direct mapping between input speech features and output character sequences without requiring explicit alignment information. This approach is especially beneficial for speech recognition in situations involving variations in speech patterns. A hybrid architecture is trained using UA speech corpus that allows it to focus on important features of the speech and capture the relationships between them, leading to more accurate speech recognition. The performance of the proposed ASR system shows a remarkable decrease in Word Error Rate (WER) up to 2.78% and 15.67% for individuals with dysarthria who have low and very low intelligibility, respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3736965/v1"
    },
    {
        "id": 21978,
        "title": "Peer Review #1 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.2)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.2/reviews/1"
    },
    {
        "id": 21979,
        "title": "Chapter 2: Tokenization",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-003"
    },
    {
        "id": 21980,
        "title": "Operation Maintenance and Fault Analysis of Distribution Transformer",
        "authors": "",
        "published": "2021-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i8.305"
    },
    {
        "id": 21981,
        "title": "Enhancing Voxel Transformer with Spatial-Semantic Feature Aggregation and Optimized Rpn for 3d Object Detection",
        "authors": "Yingfei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4579060"
    },
    {
        "id": 21982,
        "title": "Vision transformer for skin cancer classification",
        "authors": "Vladyslav Nikitin, Nataliia Shapoval",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "This paper investigates the use of vision transformers (ViT) for skin cancer classification tasks, compared to convolutional models. We propose a novel ViT architecture that effectively classifies skin cancer images. Our findings suggest that ViT models have the potential to outperform convolutional models, especially with larger datasets.",
        "link": "http://dx.doi.org/10.51582/interconf.19-20.05.2023.039"
    },
    {
        "id": 21983,
        "title": "3D core VPI transformer",
        "authors": "K.C. Lee, J.C. Duart, K. Xu",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/insucon.2017.8097202"
    },
    {
        "id": 21984,
        "title": "Predictive Model of Stroke Risk with Vision Transformer-Based Doppler Features",
        "authors": "Chung-Ming Lo, Peng-Hsiang Hung",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4356364"
    },
    {
        "id": 21985,
        "title": "- Causes and Effects of Transformer Sound Levels",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-53"
    },
    {
        "id": 21986,
        "title": "A cross-attention transformer encoder for paired sequence data",
        "authors": "Ceder Dens, Kris Laukens, Pieter Meysman, Wout Bittremieux",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractTransformer-based sequence encoding architectures are often limited to a single-sequence input while some tasks require a multi-sequence input. For example, the peptide–MHCII binding prediction task where the input consists of two protein sequences. Current workarounds to solve this input-type mismatch lack resemblance with the biological mechanisms behind the task. As a solution, we propose a novel cross-attention transformer encoder that creates a cross-attended embedding of both input sequences. We compare its classification performance on the peptide–MHCII binding prediction task to a baseline logistic regression model and a default transformer encoder. Finally, we make visualizations of the attention layers to show how the different models learn different patterns.",
        "link": "http://dx.doi.org/10.1101/2023.12.11.571066"
    },
    {
        "id": 21987,
        "title": "Healthcare Transformer 9: Disseminate Best Practices",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-7"
    },
    {
        "id": 21988,
        "title": "Transformer Protection",
        "authors": "",
        "published": "2021-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119847397.ch8"
    },
    {
        "id": 21989,
        "title": "Impact of TiO2 Nanoparticles on Physical and  Electrical Properties of Mustard Oil for Power  Transformer",
        "authors": "Muhammad Furqan Hameed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vegetable oils are an effective alternative to mineral oil for power transformers due to their high biodegradability and excellent dielectric strength. Moreover, adding a small number of nanoparticles (NPs) to vegetable oils can cause a significant increase in the dielectric strength of vegetable oils. This paper presents an experimental study of the physical and electrical properties of Mustard oil and its titanium dioxide (TiO2) based nanofluids (NFs). Physical properties include viscosity, water content, specific gravity, acidic content, flash point, and pour point whereas electrical properties include the AC dielectric  strength and partial discharge analysis. All the properties were determined according to IEC/ISO standards. Nanofluids were  characterized using scanning electron microscopy and x-ray  dispersion analyzer. Titanium dioxide based nanofluids were added to pure Mustard oil in varying quantities of 0.02 g/liter,  0.05 g/liter, and 0.1 g/liter to make NFs. The experimental  results indicate that 0.05 g/L NF has best properties among all  the four samples, and it has the ability to substitute conventional  mineral oil. Experimental study showed that there was an  increase of 40% in AC dielectric strength, 3 times increase in the  partial discharge inception and extinction voltage when 0.05 g/L  TiO2 was added in Mineral oil as compared to the experimental  study of pure Mineral oil. The comparison of physical and  electrical properties of the said sample was compared with  mineral oil and other vegetable oils. It has been observed that 0.05 g/L TiO2 is also cost effective alternative to mineral oil and  other vegetable oils. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20419629.v1"
    },
    {
        "id": 21990,
        "title": "Vision Transformer Framework Approach For Melanoma Skin Disease Identification",
        "authors": "Vikas Kumar Roy, Vasu Thakur, Nupur Goyal",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the past few decades, skin diseases have been a hazardous issue because of more sophisticated and high-cost treatments. Identifying skin disease is still a challenging task for dermatologists. In reference to severe diseases like Melanoma, therapy in the initial stages is very important and effective to avoid skin cancer. This paper proposes an effective approach by using Vision Transformers (ViT) to detect Melanoma, which gives the accuracy of 99% on the test images. Authors considered the dataset, which is publicly available on Kaggle that comprises 1000 images and did the comprehensive study to get better results using ViT. The obtained results are compared with other state-of-the-art algorithms (VGG-19 and Inception-V3) to analyze the distinction between the proposed approach and other Convolutional Neural Network (CNN) Models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2536632/v1"
    },
    {
        "id": 21991,
        "title": "Visual Tracking with Layer-Wise Transformer Fusion",
        "authors": "Xiaoyan Qian, Feng Zhang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3985289"
    },
    {
        "id": 21992,
        "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 7,
        "abstract": "<p>Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.</p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14561901"
    },
    {
        "id": 21993,
        "title": "Streaming model for Acoustic to Articulatory Inversion with transformer networks",
        "authors": "Sathvik Udupa, Aravind Illa, Prasanta Ghosh",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10159"
    },
    {
        "id": 21994,
        "title": "Peer Review #1 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.1)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.1/reviews/1"
    },
    {
        "id": 21995,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "Herein we present a new retrosynthesis prediction method, viz. RetroTRAE, which uses fragment-based tokenization combined with the Transformer architecture. RetroTRAE mimics chemical reasoning, and predicts reactant candidates by learning the changes of atomic environments associated with the chemical reaction. Atom environments stand as ideal, chemically meaningful building blocks, which together produce a high-resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to the conservation of atoms in the reactions. Our model achieved a top-1 accuracy of 68.1% within the bioactively similar range for the USPTO test dataset, outperforming other state-of-the-art translation methods. Besides yielding a high level of overall accuracy, the proposed method solves the translation issues arising from the SMILES-based retrosynthesis planning methods effectively. Through careful inspection of reactant candidates, we demonstrated atom environments as promising descriptors for studying reaction route prediction and discovery. RetroTRAE provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed. Our methodology offers a novel way of devising a retrosynthetic planning model using fragmental and topological descriptors as natural inputs for chemical translation tasks.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-9g274-v2"
    },
    {
        "id": 21996,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present the new retrosynthesis prediction method RetroTRAE using fragment-based tokenization combined with the Transformer architecture. RetroTRAE represents chemical reactions by using the changes of fragment sets of molecules using the atomic environment fragmentation scheme. Atom environments stand as an ideal, chemically meaningful building blocks together producing a high resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to conservation of atoms in reactions. Our model achieved a top-1 accuracy of 67.1% within the bioactively similar range for USPTO test dataset, outperforming the other state of the art, translation methods. We investigated the effect of different encoding scenarios on predicting the reactant candidates. We also critically assessed the retrieval process that converts a set of fragments into a molecule with respect to coverage, degeneracy and resolution. Our new template-free model for retrosynthetic prediction provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-9g274"
    },
    {
        "id": 21997,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present the new retrosynthesis prediction method RetroTRAE using fragment-based tokenization combined with the Transformer architecture. RetroTRAE represents chemical reactions by using the changes of fragment sets of molecules using the atomic environment fragmentation scheme. Atom environments stand as an ideal, chemically meaningful building blocks together producing a high resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to conservation of atoms in reactions. Our model achieved a top-1 accuracy of 67.1% within the bioactively similar range for USPTO test dataset, outperforming the other state of the art, translation methods. We investigated the effect of different encoding scenarios on predicting the reactant candidates. We also critically assessed the retrieval process that converts a set of fragments into a molecule with respect to coverage, degeneracy and resolution. Our new template-free model for retrosynthetic prediction provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-9g274"
    },
    {
        "id": 21998,
        "title": "Resonance Overvoltages in Transformer Windings - Part 4: Determination of Oscillation Frequencies in «Feeding Cable — Transformer» System",
        "authors": "Vasily S. LARIN,  ",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24160/0013-5380-2018-9-22-28"
    },
    {
        "id": 21999,
        "title": "Intelligent distribution transformer using a fractionally rated converter based on an existing distribution transformer",
        "authors": "Hyun-Jun Lee, Seung-Cheol Choi, Young-Doo Yoon",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s43236-023-00609-z"
    },
    {
        "id": 22000,
        "title": "Analysis of Key Technologies of Transformer Fault Treatment",
        "authors": "",
        "published": "2022-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i2.360"
    },
    {
        "id": 22001,
        "title": "Predictive Model of Stroke Risk with Vision Transformer-Based Doppler Features",
        "authors": "Chung-Ming Lo, Peng-Hsiang Hung",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4356364"
    },
    {
        "id": 22002,
        "title": "- Causes and Effects of Transformer Sound Levels",
        "authors": "",
        "published": "2018-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315222363-53"
    },
    {
        "id": 22003,
        "title": "Healthcare Transformer 9: Disseminate Best Practices",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-7"
    },
    {
        "id": 22004,
        "title": "Transformer Protection",
        "authors": "",
        "published": "2021-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119847397.ch8"
    },
    {
        "id": 22005,
        "title": "Impact of TiO2 Nanoparticles on Physical and  Electrical Properties of Mustard Oil for Power  Transformer",
        "authors": "Muhammad Furqan Hameed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vegetable oils are an effective alternative to mineral oil for power transformers due to their high biodegradability and excellent dielectric strength. Moreover, adding a small number of nanoparticles (NPs) to vegetable oils can cause a significant increase in the dielectric strength of vegetable oils. This paper presents an experimental study of the physical and electrical properties of Mustard oil and its titanium dioxide (TiO2) based nanofluids (NFs). Physical properties include viscosity, water content, specific gravity, acidic content, flash point, and pour point whereas electrical properties include the AC dielectric  strength and partial discharge analysis. All the properties were determined according to IEC/ISO standards. Nanofluids were  characterized using scanning electron microscopy and x-ray  dispersion analyzer. Titanium dioxide based nanofluids were added to pure Mustard oil in varying quantities of 0.02 g/liter,  0.05 g/liter, and 0.1 g/liter to make NFs. The experimental  results indicate that 0.05 g/L NF has best properties among all  the four samples, and it has the ability to substitute conventional  mineral oil. Experimental study showed that there was an  increase of 40% in AC dielectric strength, 3 times increase in the  partial discharge inception and extinction voltage when 0.05 g/L  TiO2 was added in Mineral oil as compared to the experimental  study of pure Mineral oil. The comparison of physical and  electrical properties of the said sample was compared with  mineral oil and other vegetable oils. It has been observed that 0.05 g/L TiO2 is also cost effective alternative to mineral oil and  other vegetable oils. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20419629.v1"
    },
    {
        "id": 22006,
        "title": "Vision Transformer Framework Approach For Melanoma Skin Disease Identification",
        "authors": "Vikas Kumar Roy, Vasu Thakur, Nupur Goyal",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the past few decades, skin diseases have been a hazardous issue because of more sophisticated and high-cost treatments. Identifying skin disease is still a challenging task for dermatologists. In reference to severe diseases like Melanoma, therapy in the initial stages is very important and effective to avoid skin cancer. This paper proposes an effective approach by using Vision Transformers (ViT) to detect Melanoma, which gives the accuracy of 99% on the test images. Authors considered the dataset, which is publicly available on Kaggle that comprises 1000 images and did the comprehensive study to get better results using ViT. The obtained results are compared with other state-of-the-art algorithms (VGG-19 and Inception-V3) to analyze the distinction between the proposed approach and other Convolutional Neural Network (CNN) Models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2536632/v1"
    },
    {
        "id": 22007,
        "title": "Visual Tracking with Layer-Wise Transformer Fusion",
        "authors": "Xiaoyan Qian, Feng Zhang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3985289"
    },
    {
        "id": 22008,
        "title": "A cross-attention transformer encoder for paired sequence data",
        "authors": "Ceder Dens, Kris Laukens, Pieter Meysman, Wout Bittremieux",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractTransformer-based sequence encoding architectures are often limited to a single-sequence input while some tasks require a multi-sequence input. For example, the peptide–MHCII binding prediction task where the input consists of two protein sequences. Current workarounds to solve this input-type mismatch lack resemblance with the biological mechanisms behind the task. As a solution, we propose a novel cross-attention transformer encoder that creates a cross-attended embedding of both input sequences. We compare its classification performance on the peptide–MHCII binding prediction task to a baseline logistic regression model and a default transformer encoder. Finally, we make visualizations of the attention layers to show how the different models learn different patterns.",
        "link": "http://dx.doi.org/10.1101/2023.12.11.571066"
    },
    {
        "id": 22009,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present the new retrosynthesis prediction method RetroTRAE using fragment-based tokenization combined with the Transformer architecture. RetroTRAE represents chemical reactions by using the changes of fragment sets of molecules using the atomic environment fragmentation scheme. Atom environments stand as an ideal, chemically meaningful building blocks together producing a high resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to conservation of atoms in reactions. Our model achieved a top-1 accuracy of 67.1% within the bioactively similar range for USPTO test dataset, outperforming the other state of the art, translation methods. We investigated the effect of different encoding scenarios on predicting the reactant candidates. We also critically assessed the retrieval process that converts a set of fragments into a molecule with respect to coverage, degeneracy and resolution. Our new template-free model for retrosynthetic prediction provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-9g274"
    },
    {
        "id": 22010,
        "title": "Peer Review #1 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.1)\"",
        "authors": "",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.1/reviews/1"
    },
    {
        "id": 22011,
        "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 7,
        "abstract": "<p>Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.</p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14561901"
    },
    {
        "id": 22012,
        "title": "Streaming model for Acoustic to Articulatory Inversion with transformer networks",
        "authors": "Sathvik Udupa, Aravind Illa, Prasanta Ghosh",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10159"
    },
    {
        "id": 22013,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present the new retrosynthesis prediction method RetroTRAE using fragment-based tokenization combined with the Transformer architecture. RetroTRAE represents chemical reactions by using the changes of fragment sets of molecules using the atomic environment fragmentation scheme. Atom environments stand as an ideal, chemically meaningful building blocks together producing a high resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to conservation of atoms in reactions. Our model achieved a top-1 accuracy of 67.1% within the bioactively similar range for USPTO test dataset, outperforming the other state of the art, translation methods. We investigated the effect of different encoding scenarios on predicting the reactant candidates. We also critically assessed the retrieval process that converts a set of fragments into a molecule with respect to coverage, degeneracy and resolution. Our new template-free model for retrosynthetic prediction provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-9g274"
    },
    {
        "id": 22014,
        "title": "RetroTRAE: retrosynthetic translation of atomic environments with Transformer",
        "authors": "Umit Volkan Ucak, Islambek Ashyrmamatov, Junsu Ko, Juyong Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "Herein we present a new retrosynthesis prediction method, viz. RetroTRAE, which uses fragment-based tokenization combined with the Transformer architecture. RetroTRAE mimics chemical reasoning, and predicts reactant candidates by learning the changes of atomic environments associated with the chemical reaction. Atom environments stand as ideal, chemically meaningful building blocks, which together produce a high-resolution molecular representation. Describing a molecule with a set of atom environments establishes a clear relationship between translated product-reactant pairs due to the conservation of atoms in the reactions. Our model achieved a top-1 accuracy of 68.1% within the bioactively similar range for the USPTO test dataset, outperforming other state-of-the-art translation methods. Besides yielding a high level of overall accuracy, the proposed method solves the translation issues arising from the SMILES-based retrosynthesis planning methods effectively. Through careful inspection of reactant candidates, we demonstrated atom environments as promising descriptors for studying reaction route prediction and discovery. RetroTRAE provides fast and reliable retrosynthetic route planning for substances whose fragmentation patterns are revealed. Our methodology offers a novel way of devising a retrosynthetic planning model using fragmental and topological descriptors as natural inputs for chemical translation tasks.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-9g274-v2"
    },
    {
        "id": 22015,
        "title": "FakeRevealer: A Multimodal Framework for Revealing the Falsity of Online Tweets Using Transformer-Based Architectures",
        "authors": "Sakshi Kalra, Yashvardhan Sharma, Priyansh Vyas, Gajendra Chauhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011889800003411"
    },
    {
        "id": 22016,
        "title": "Transformer-DW: A Transformer Network with Dynamic and Weighted Head",
        "authors": "Ruxin Tan, Jiahui Sun, Bo Su, Gongshen Liu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-36711-4_42"
    },
    {
        "id": 22017,
        "title": "Analysis of Overvoltages on Power Transformer Recorded by Transient Overvoltage Monitoring System",
        "authors": "Bozidar Filipovic-Grcic, Bruno Jurišić, Samir Keitoue, Ivan Murat, Dalibor Filipovic-Grcic, Alan Zupan",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_8"
    },
    {
        "id": 22018,
        "title": "A bidirectional three‐phase triple‐voltage LLC (T\n            <sup>2</sup>\n            ‐LLC) resonant converter for DC/DC stage in solid state transformer and DC transformer applications",
        "authors": "Liangcai Shu, Wu Chen, Haozhe Jin",
        "published": "2022-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pel2.12243"
    },
    {
        "id": 22019,
        "title": "Analytical discrete Fourier transformer‐based phasor estimation method for reducing transient impact of capacitor voltage transformer",
        "authors": "Mohsen Tajdinian, Mehdi Allahbakhshi, Ali Reza Seifi, Alireza Bagheri",
        "published": "2017-6",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/iet-gtd.2016.1784"
    },
    {
        "id": 22020,
        "title": "Vision-Perceptual Transformer Network for Semantic Scene Understanding",
        "authors": "Mohamad Alansari, Hamad AlRemeithi, Bilal Hassan, Sara Alansari, Jorge Dias, Majid Khonji, Naoufel Werghi, Sajid Javed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012313800003660"
    },
    {
        "id": 22021,
        "title": "Classification of Smartphone Application Reviews Using Small Corpus Based on Bidirectional LSTM Transformer",
        "authors": "Kazuyuki Matsumoto, Seiji Tsuchiya, Takumi Kojima, Hiroya Kondo, Minoru Yoshida, Kenji Kita",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2020.10.1.912"
    },
    {
        "id": 22022,
        "title": "Operation and Control of the Smart Transformer in Meshed and Hybrid Grids: Choosing the Appropriate Smart Transformer Control and Operation Scheme",
        "authors": "Rongwu Zhu, Marco Liserre, Marius Langwasser, Chandan Kumar",
        "published": "2021-3",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mie.2020.3005357"
    },
    {
        "id": 22023,
        "title": "EHA-Transformer: Efficient and Haze-Adaptive Transformer for Single Image Dehazing",
        "authors": "Yu Zhou, Zhihua Chen, Ran Li, Bin Sheng, Lei Zhu, Ping Li",
        "published": "2022-12-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3574131.3574429"
    },
    {
        "id": 22024,
        "title": "A MMC-Based Multiport Power Electronic Transformer With Shared Medium-Frequency Transformer",
        "authors": "Dajun Ma, Wu Chen, Liangcai Shu, Xiaohui Qu, Kai Hou",
        "published": "2021-2",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tcsii.2020.3012293"
    },
    {
        "id": 22025,
        "title": "The Investigation of Detect Position of Partial Discharge in Cast-Resin Transformer Using High-Frequency Current Transformer Sensor and Acoustic Emission Sensor",
        "authors": "Nuttee Thungsuk, Narong Mungkung, Apidat Songruk, Khanchai Tunlasakun, Kittimasak Tikakosol, Siriwut Nilawat, Kanitphan Boonsomchuae, Toshifumi Yuji, Somchai Arunrungrusmi, Hiroyuki Kinoshita",
        "published": "2022-1-26",
        "citations": 7,
        "abstract": "The lifetime of a cast-resin transformer mainly depends on the condition of insulation material. Partial discharge (PD) is an important reason for insulation deterioration in cast-resin transformers. Identifying the position of PD is very necessary for damage assessment while the transformer is still operating, and the transformer is covered by housing. This paper proposes the investigation of a cast-resin transformer using an AE sensor and HFCT sensor to specify the precise source of PD. In this study, four AE sensors were used to find PD sources, and the high-frequency current transducer (HFCT) technique was used to identify the PD source and the criteria level. The experiment, in the first two parts, identified the possibility of PD, which includes the position of PD. The final part of the experiment verified the position of the PD source of a cast-resin transformer and confirmed the inspection results. AE and HFCT sensors can be used to detect the location of PD sources, confirming the position of the PD source by sensor detection. In addition, the evident partial discharge picture on the insulator surface of high voltage side. The process successfully and accurately identifies and locates the PD source.",
        "link": "http://dx.doi.org/10.3390/app12031310"
    },
    {
        "id": 22026,
        "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
        "authors": "Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, Joey Tianyi Zhou",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00854"
    },
    {
        "id": 22027,
        "title": "基于Transformer-GANs生成有风格调节的音乐",
        "authors": "Weining Wang, Jiahui Li, Yifan Li, Xiaofen Xing",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300359"
    },
    {
        "id": 22028,
        "title": "Unveiling the Thematic Landscape of Generative Pre-trained Transformer (GPT) Through Bibliometric Analysis",
        "authors": "Carlos Alberto Gómez Cano, Verenice Sánchez Castillo, Tulio Andrés Clavijo Gallego",
        "published": "2023-4-2",
        "citations": 9,
        "abstract": "Introduction: the Generative Pre-trained Transformer (GPT) is a deep learning language model architecture developed by OpenAI.\nAim: to describe the knowledge networks (both at the theoretical and country levels) of the Generative Pre-trained Transformer (GPT) as an emerging technology.\nResults: 222 Documents were identified, of which 69 were articles, 50 were conference papers, 36 were editorials, 29 were notes, 19 were letters, 14 were reviews, 3 were conference reviews, and 2 were short surveys. In terms of the number of documents per year, 2 were found in 2019, 10 in 2020, 22 in 2021, 44 in 2022, and 144 in 2023. The year-on-year growth rate was over 100% in all years. The subject area with the highest number of documents was Computer Science with 90 documents. The most productive countries in relation to GPT were the United States with 60 documents, followed by China with 19, the United Kingdom with 18, India with 15, and Australia with 12. Co-occurrence illustrated the centrality of Artificial Intelligence, Natural Language Processing, Deep Learning, and the term Human around ChatGPT and GPT.\nConclusions: this bibliometric study aimed to describe the knowledge networks of the Generative Pre-trained Transformer (GPT) as an emerging technology. Although only 222 documents were found, this study revealed a high level of international scientific collaboration in the field. The results suggest that GPT is a highly relevant technology with a wide range of potential applications in natural language processing, artificial intelligence, and deep learning.\nMoreover, the study was able to qualitatively characterize the main thematic areas surrounding GPT, including its applications in chatbots, text generation, machine translation, sentiment analysis, and more.",
        "link": "http://dx.doi.org/10.56294/mr202333"
    },
    {
        "id": 22029,
        "title": "PCTU-Net: Multi-Scale Pooling and Transformer Collaborative Unmixing Network for Hyperspectral Images",
        "authors": "建锋 陈",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/mos.2023.126506"
    },
    {
        "id": 22030,
        "title": "NGD-Transformer: Navigation Geodesic Distance Positional Encoding with Self-Attention Pooling for Graph Transformer on 3D Triangle Mesh",
        "authors": "Jiafu Zhuang, Xiaofeng Liu, Wei Zhuang",
        "published": "2022-10-1",
        "citations": 1,
        "abstract": "Following the significant success of the transformer in NLP and computer vision, this paper attempts to extend it to 3D triangle mesh. The aim is to determine the shape’s global representation using the transformer and capture the inherent manifold information. To this end, this paper proposes a novel learning framework named Navigation Geodesic Distance Transformer (NGD-Transformer) for 3D mesh. Specifically, this approach combined farthest point sampling with the Voronoi segmentation algorithm to spawn uniform and non-overlapping manifold patches. However, the vertex number of these patches was inconsistent. Therefore, self-attention graph pooling is employed for sorting the vertices on each patch and screening out the most representative nodes, which were then reorganized according to their scores to generate tokens and their raw feature embeddings. To better exploit the manifold properties of the mesh, this paper further proposed a novel positional encoding called navigation geodesic distance positional encoding (NGD-PE), which encodes the geodesic distance between vertices relatively and spatial symmetrically. Subsequently, the raw feature embeddings and positional encodings were summed as input embeddings fed to the graph transformer encoder to determine the global representation of the shape. Experiments on several datasets were conducted, and the experimental results show the excellent performance of our proposed method.",
        "link": "http://dx.doi.org/10.3390/sym14102050"
    },
    {
        "id": 22031,
        "title": "An Object Pseudo-Label Generation Technique based on Self-Supervised Vision Transformer for Improving Dataset Quality",
        "authors": "Dohyun Kim, Jiwoong Jeon, Seongtaek Lim, Hongchul Lee",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/jok.2024.51.1.49"
    },
    {
        "id": 22032,
        "title": "Pvt2dnet:Polyp Segmentation with Vision Transformer and Dual Decoder Refinement Strategy",
        "authors": "Yibiao Hu, Yan Jin, Zhiwei Jiang, Qiufu Zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458919"
    },
    {
        "id": 22033,
        "title": "Analysis of Excitation Transformer Fault Problem in Power Plant",
        "authors": "",
        "published": "2022-2-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i2.548"
    },
    {
        "id": 22034,
        "title": "Review of Hybrid Transformer Topology",
        "authors": "Xin Wan, Man-Chung Wong",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syps59767.2023.10268179"
    },
    {
        "id": 22035,
        "title": "Isotropic Plasma: Dispersive Medium",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-1"
    },
    {
        "id": 22036,
        "title": "Transformer with Bidirectional Decoder for Speech Recognition",
        "authors": "Xi Chen, Songyang Zhang, Dandan Song, Peng Ouyang, Shouyi Yin",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2677"
    },
    {
        "id": 22037,
        "title": "Screening of Constructional Parts",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-4"
    },
    {
        "id": 22038,
        "title": "The Art and Science of Transformer Ratio Measurement",
        "authors": "Oleh W. Iwanusiw, P. Eng",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481036"
    },
    {
        "id": 22039,
        "title": "Proposal of Miniaturization of Rectifier Circuit Using Multiphase Transformer",
        "authors": "Akinori Kato",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vppc46532.2019.8952345"
    },
    {
        "id": 22040,
        "title": "Determination Key Indicator in Transformer Oil by Infrared Equipment",
        "authors": "Zhang Hua",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced56215.2022.9928819"
    },
    {
        "id": 22041,
        "title": "Data Expansion and Relabeling For Improving the vision transformer-based Micro-expression Recognition",
        "authors": "He Zhang, Hanling Zhang, Lu Yin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer-based Micro-expression Recognition Improved by Data Expansion and Relabeling</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21842520"
    },
    {
        "id": 22042,
        "title": "TCCL-Net: Transformer-Convolution Collaborative Learning Network for Omnidirectional Image Super-Resolution",
        "authors": "Xiongli Chai, Feng Shao, Qiuping Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4362667"
    },
    {
        "id": 22043,
        "title": "Design of Solid State Transformer",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21172/ijiet.83.027"
    },
    {
        "id": 22044,
        "title": "Transient-Voltage Response of Coils and Windings",
        "authors": "Robert C. Degeneff",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-21"
    },
    {
        "id": 22045,
        "title": "Un discours peut transformer le monde",
        "authors": "Barbara Cassin, Alizée Vincent",
        "published": "2019-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/sh.312.0004"
    },
    {
        "id": 22046,
        "title": "Wearable Sensor Based Human Activity Recognition with Transformer",
        "authors": "Iveta Dirgová Luptáková, Martin Kubovčík, Jiří Pospíchal",
        "published": "No Date",
        "citations": 7,
        "abstract": "This paper describes the successful application of the Transformer model used in the natural language processing and vision tasks as a means of processing the time series of signals from gyroscope and accelerometer sensors for the classification of human activities. The Transformer model is based on deep neural networks with many layers which can generalize well on signals. All measured signals come from a smartphone placed in a waist bag. Activity prediction is sequence-to-sequence, each time step of the signal is assigned a designation of the performed activity. Emphasis is placed on attention mechanisms, which express individual dependencies between signal values within a time series. In comparison with another recent result, the recognition precision was improved from 89.67 percent to 99.2 percent. The transformer model should in the future be included among the top options in machine learning methods for human activity recognition.",
        "link": "http://dx.doi.org/10.20944/preprints202202.0111.v1"
    },
    {
        "id": 22047,
        "title": "A Method to Embed Resonant Inductor within PCB Matrix Transformer for High-Density Resonant Converters",
        "authors": "Ahmed Nabih, Qiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The power electronics industry seeks more efficient power delivery and higher power density year-by-year. Resonant converters, such as the three-element resonant LLC converters, have been intensively studied and applied as DC-DC converters in many applications. One of the most demanding applications for LLC converters is data centers. Data centers consume a significant amount of power. According to some estimates\\cite{jones2018stop}, data centers around the world consume approximately 200 terawatt-hours of electricity per year, equivalent to the annual electricity consumption of a country like the Netherlands. This high power consumption is primarily due to the growing number of servers and other computing equipment used in data centers and the need for cooling systems. As a result, reducing the power consumption of data centers is a pivotal goal to save costs and reduce their environmental impact. Thus, due to the high power consumption and large-scale server farms, the pursuit of efficient and dense power architecture is ongoing.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24274867"
    },
    {
        "id": 22048,
        "title": "A Review of Research Progress on Residual Magnetism Detection Methods for Power Transformer Cores",
        "authors": "Cailing Huo, Yiming Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4421802"
    },
    {
        "id": 22049,
        "title": "ConvFormer: Tracking by Fusing Convolution and Transformer Features",
        "authors": "Chao Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3293592"
    },
    {
        "id": 22050,
        "title": "\"Still They Remember Me\"",
        "authors": "CAROL A. DANA, MARGO LUKENS, CONOR M. QUINN",
        "published": "2021-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52"
    },
    {
        "id": 22051,
        "title": "Efficient Structured Prediction with Transformer Encoders",
        "authors": "Ali Basirat",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "Finetuning is a useful method for adapting Transformer-based text encoders to new tasks but can be computationally expensive for structured prediction tasks that require tuning at the token level. Furthermore, finetuning is inherently inefficient in updating all base model parameters, which prevents parameter sharing across tasks. To address these issues, we propose a method for efficient task adaptation of frozen Transformer encoders based on the local contribution of their intermediate layers to token representations. Our adapter uses a novel attention mechanism to aggregate intermediate layers and tailor the resulting representations to a target task. Experiments on several structured prediction tasks demonstrate that our method outperforms previous approaches, retaining over 99% of the finetuning performance at a fraction of the training cost. Our proposed method offers an efficient solution for adapting frozen Transformer encoders to new tasks, improving performance and enabling parameter sharing across different tasks.",
        "link": "http://dx.doi.org/10.3384/nejlt.2000-1533.2024.4932"
    },
    {
        "id": 22052,
        "title": "Transformer regards et politiques sur le grand âge",
        "authors": "Monique Iborra",
        "published": "2019-4-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/const.053.0080"
    },
    {
        "id": 22053,
        "title": "Aged transformer oil analysis through laser induced breakdown spectroscopy",
        "authors": "Amir Hossein Farhadian, Morteza Mikhak-Beyranvand, Seyed Sajad Mousavi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169842966.62021440/v1"
    },
    {
        "id": 22054,
        "title": "Forecasting COVID-19 New Cases Using Transformer Deep Learning Model",
        "authors": "Saurabh Patil, Parisa Mollaei, Amir Barati Farimani",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractMaking accurate forecasting of COVID-19 cases is essential for healthcare systems, with more than 650 million cases as of 4 January,1making it one of the worst in history. The goal of this research is to improve the precision of COVID-19 case predictions in Russia, India, and Brazil, a transformer-based model was developed. Several researchers have implemented a combination of CNNs and LSTMs, Long Short-Term Memory (LSTMs), and Convolutional Neural Networks (CNNs) to calculate the total number of COVID-19 cases. In this study, an effort was made to improve the correctness of the models by incorporating recent advancements in attention-based models for time-series forecasting. The resulting model was found to perform better than other existing models and showed improved accuracy in forecasting. Using the data from different countries and adapting it to the model will enhance its ability to support the worldwide effort to combat the pandemic by giving more precise projections of cases.",
        "link": "http://dx.doi.org/10.1101/2023.11.02.23297976"
    },
    {
        "id": 22055,
        "title": "3T-Net: Transformer Encoders for Destination Prediction",
        "authors": "Jing Zhang, Daniel Nikovski, Takuro Kojima",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240616"
    },
    {
        "id": 22056,
        "title": "Racing with Vision Transformer Architecture",
        "authors": "Chengwen Tian, Liang Song",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ishc56805.2022.00055"
    },
    {
        "id": 22057,
        "title": "Enhancing Cervical Cancer Diagnosis: Integrated Attention-Transformer System with Weakly Supervised Learning",
        "authors": "Ashfaque Khowaja, Beiji Zou, Xiaoyan Kui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4661793"
    },
    {
        "id": 22058,
        "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
        "authors": "Ziyi Chen, Pengyuan Zhang",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1301"
    },
    {
        "id": 22059,
        "title": "Target Detection Algorithm Based on Efficient Self-Attention-Convolution Enhanced Transformer",
        "authors": "Fengping An, Jianrong Wang, Ruijun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSince the target detection algorithm based on convolutional neural network suffers from limited convolutional kernel receptive field, it leads to the model's inability to perceive the remote semantic information in the image. Because the Transformer model does not have the limitation of local receptive fields, it is introduced into the field of target detection, and many scholars have proposed target detection algorithms based on Transformer and its variants. However, the Transformer model has the difficulties of not being able to extract the deep feature information and the high computational complexity of the standard self-attention mechanism in the process of target detection and recognition applications. Aiming at the above two core problems, we have carried out in-depth analysis and research, and proposed an encoder-decoder model consisting of a convolutional layer and a Transformer module. And then, we constructed the efficient multi-head self-attention mechanism, which can capture both local and remote contextual information of target image features. Then, we design efficient convolutional module-enhanced cross-window connectivity, which can significantly improve the characterization and global modeling capabilities of Transformer model. In addition, we propose the convolution-enhanced Transformer learning framework, which improves the adaptability to different datasets, which also integrates the sparse sampling strategy. It can significantly reduce the memory and computational requirements in large-scale image processing. Finally, we propose a target detection algorithm based on a new Transformer framework. We conducted ablation experiments and computational performance comparison experiments on several HRRS scenes and natural scene datasets. The experimental results confirm that our proposed method obtains optimal results in terms of weighted F-measure, average F-measure and MAE. Moreover, our proposed method has clearer edge information and more accurate target localization information in the visual effect of detection results.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3980415/v1"
    },
    {
        "id": 22060,
        "title": "Power Transformer Diagnostics, Monitoring and Design Features",
        "authors": "Issouf Fofana, Yazid Hadjadj",
        "published": "2018-11-22",
        "citations": 7,
        "abstract": "The reliability of the power grid system directly contributes to the economic well-being and the quality of life of citizens in any country. [...]",
        "link": "http://dx.doi.org/10.3390/en11123248"
    },
    {
        "id": 22061,
        "title": "Transformer Leakage Inductance Design Methodology",
        "authors": "Angshuman Sharma, Jonathan W. Kimball",
        "published": "2023-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43580.2023.10131409"
    },
    {
        "id": 22062,
        "title": "Protein sequence profile prediction using ProtAlbert transformer",
        "authors": "Armin Behjati, Fatemeh Zare-Mirakabad, Seyed Shahriar Arab, Abbas Nowzari-Dalini",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractProtein profiles have many applications in bioinformatics. To construct the profile from a protein sequence, the sequence is aligned with database. However, sometimes there are no similar sequences with the query. This paper proposes a method based on pre-trained ProtAlbert transformer to predict the profile for a single protein sequence without alignment. The performance of transformers on natural languages is impressive. Protein sequences can be viewed as a language; therefore, we can benefit from using these models. We analyze the attention heads in different layers of ProtAlbert to show that the transformer can capture five essential protein characteristics of the family from a single protein sequence. These assessments are performed on the CASP13 dataset to find representative heads for each of five protein characteristics. Then, these heads are investigated on one thermophilic and two mesophilic proteins as case studies. The results show the significant attention heads for protein family properties extracted from a single protein sequence. This analysis led us to propose an algorithm called PA_SPP for profile prediction using only a single protein sequence as input. In our algorithm, we apply the masked language modeling method of ProtAlbert. The results display high similarity between the predicted profiles and HSSP profiles.",
        "link": "http://dx.doi.org/10.1101/2021.09.23.461475"
    },
    {
        "id": 22063,
        "title": "Customer Requirements Extraction Based on Transformer",
        "authors": "Guangyu Huang",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceace60673.2023.10442158"
    },
    {
        "id": 22064,
        "title": "LaneFormer: An Efficient Transformer-based Network for Fast Lane Detection",
        "authors": "Yifan Yao, Huilin Xiong",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055124"
    },
    {
        "id": 22065,
        "title": "Decision letter for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/decision1"
    },
    {
        "id": 22066,
        "title": "Chapitre VII. Transformer la classe et former les enseignants",
        "authors": "Philippe Veyrunes",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.pumi.6420"
    },
    {
        "id": 22067,
        "title": "Fruit Ripeness Identification Using Transformer Model",
        "authors": "Bingjie Xiao, Minh Nguyen, Weiqi Yan",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4129908"
    },
    {
        "id": 22068,
        "title": "Design Considerations—Inside/Outside Windings for a Distributed Photovoltaic Grid Power Transformer",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16412-15"
    },
    {
        "id": 22069,
        "title": "GSM BASED SINGLE PHASE DISTRIBUTION TRANSFORMER MONITORING AND CONTROL",
        "authors": "",
        "published": "2020-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31838/jcr.07.12.115"
    },
    {
        "id": 22070,
        "title": "On-Line Monitoring of Liquid-Immersed Transformers",
        "authors": "Gary R. Hoffman",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-24"
    },
    {
        "id": 22071,
        "title": "Loading and Thermal Performance",
        "authors": "Robert F. Tillman, Don A. Duckett",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-15"
    },
    {
        "id": 22072,
        "title": "Distribution system loss reduction by automatic transformer load balancing",
        "authors": "Basit Ali, Imran Siddique",
        "published": "2017-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inmic.2017.8289456"
    },
    {
        "id": 22073,
        "title": "Transformer less Railway Traction Drive using Cascaded Multilevel Converter",
        "authors": "",
        "published": "2020-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37896/jxu14.5/079"
    },
    {
        "id": 22074,
        "title": "Methods of Experimental Investigations",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-10"
    },
    {
        "id": 22075,
        "title": "Hypnose",
        "authors": "Betty Mamane",
        "published": "2017-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/cerpsy.092.0090"
    },
    {
        "id": 22076,
        "title": "State of Charge Prediction of Lead Acid Battery using Transformer Neural Network for Solar Smart Dome 4.0",
        "authors": "Iwan Agustono",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2606"
    },
    {
        "id": 22077,
        "title": "Measuring system for analysis of transformer moisture",
        "authors": "Miroslav Gutten, Daniel Korenciak, Richard Janura, Tomasz Koltunowicz, Pawel Zukowski",
        "published": "2018-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elektro.2018.8398271"
    },
    {
        "id": 22078,
        "title": "Multilingual Transformer Architectures",
        "authors": "Uday Kamath, Kenneth L. Graham, Wael Emara",
        "published": "2022-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003170082-4"
    },
    {
        "id": 22079,
        "title": "XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples",
        "authors": "Shorya Consul, John Robertson, Haris Vikalo",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTIt is estimated that approximately 15% of cancers world-wide can be linked to viral infections. The viruses that can cause or increase the risk of cancer include human papillomavirus, hepatitis B and C viruses, Epstein-Barr virus, and human immunodeficiency virus, to name a few. The computational analysis of the massive amounts of tumor DNA data, whose collection is enabled by the recent advancements in sequencing technologies, have allowed studies of the potential association between cancers and viral pathogens. However, the high diversity of oncoviral families makes reliable detection of viral DNA difficult and thus, renders such analysis challenging. In this paper, we introduce XVir, a data pipeline that relies on a transformer-based deep learning architecture to reliably identify viral DNA present in human tumors. In particular, XVir is trained on genomic sequencing reads from viral and human genomes and may be used with tumor sequence information to find evidence of viral DNA in human cancers. Results on semi-experimental data demonstrate that XVir is capable of achieving high detection accuracy, generally outperforming state-of-the-art competing methods while being more compact and less computationally demanding.CCS CONCEPTS•Computer systems organization→Embedded systems;Redundancy; Robotics; •Networks→ Network reliability.ACM Reference FormatShorya Consul, John Robertson, and Haris Vikalo. 2023. XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples. InProceedings of The Eighth International Workshop on Computational Network Biology: Modeling, Analysis, and Control (CNB-MAC ’23). ACM, New York, NY, USA, 8 pages.",
        "link": "http://dx.doi.org/10.1101/2023.08.28.555020"
    },
    {
        "id": 22080,
        "title": "Editing Principles and Design Choices, Transcription, and Translation",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.8"
    },
    {
        "id": 22081,
        "title": "TransPPG: Two-stream Transformer for Remote Heart Rate Estimate",
        "authors": "Karan Singh, Jiaqi Wang, su yang, Weishan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Non-contact facial video-based heart rate estima- tion using remote photoplethysmography (rPPG) has shown great potential in many applications (e.g., remote health care) and achieved creditable results in constrained scenarios. However, practi- cal applications require results to be accurate even under complex environment with head movement and unstable illumination. Therefore, improving the performance of rPPG in complex environment has become a key challenge. In this paper, we pro- pose a novel video embedding method that embeds each facial video sequence into a feature map re- ferred to as Multi-scale Adaptive Spatial and Tem- poral Map with Overlap (MAST Mop), which con- tains not only vital information but also surround- ing information as reference, which acts as the mir- ror to figure out the homogeneous perturbations imposed on foreground and background simulta- neously, such as illumination instability. Corre- spondingly, we propose a two-stream Transformer model to map the MAST Mop into heart rate (HR), where one stream follows the pulse signal in the facial area while the other figures out the perturba- tion signal from the surrounding region such that the difference of the two channels leads to adap- tive noise cancellation. Our approach significantly outperforms all current state-of-the-art methods on two public datasets MAHNOB-HCI and VIPL-HR. As far as we know, it is the first work with Trans- former as backbone to capture the temporal depen- dencies in rPPGs and apply the two stream scheme to figure out the interference from backgrounds as mirror of the corresponding perturbation on fore- ground signals for noise tolerating.",
        "link": "http://dx.doi.org/10.31237/osf.io/h952n"
    },
    {
        "id": 22082,
        "title": "L’enfance, machine à transformer la nature en culture",
        "authors": "Laurent Bachler",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/spi.106.0172"
    },
    {
        "id": 22083,
        "title": "Hierarchical Decision Transformer",
        "authors": "André Correia, Luis A. Alexandre",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342230"
    },
    {
        "id": 22084,
        "title": "Mod-Yolo: Multispectral Object Detection Based on Transformer Dual-Stream",
        "authors": "Yanhua Shao, Qimeng huang, yanying Mei, Hongyu Chu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4469854"
    },
    {
        "id": 22085,
        "title": "Score Transformer: Generating Musical Score from Note-level Representation",
        "authors": "Masahiro Suzuki",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3469877.3490612"
    },
    {
        "id": 22086,
        "title": "ST-GCN AltFormer: GestureRecognition with Spatial temporal Alternating Transformer",
        "authors": "Qing Pan, Jintao Zhu, Gangmin Ning, Lingwei Zhang, Luping Fang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Recently, with the rapid development of 3D motion estimation technology and the continuous improvement of low-cost depth camera performance, skeleton-based dynamic gesture recognition technology has received increasing attention. This paper proposes a novel neural network architecture — spatial-temporal alternating graph convolutional Transformer (ST-GCN-AltFormer), which combines spatial-temporal graph convolution and in-parallel spatial-temporal alternating Transformer, for dynamic gesture recognition. We first input the joint stream to the spatial-temporal graph convolution (ST-GCN) to extract the low-level features of gesture actions. Secondly, the spatial and temporal correlations between joints are modelled by an in-parallel spatial-temporal alternating Transformer (ST-TS) module, in which a spatial Transformer (STR) extracts the spatial correlations of joints within each frame, and a temporal Transformer (TTR) extracts the inter-frame correlations between joints. STR and TTR are alternately fused to obtain the final gesture prediction results. The performance of the proposed method is validated through experiments on three public dynamic gesture datasets (SHREC'17 Track(two evaluation protocols), DHG-14/28(two evaluation protocols), and LMDHG). Compared with the state-of-the-art methods, our method achieves the highest recognition accuracy with $97.3\\%$, $95.8\\%$, $94.3\\%$, $92.8\\%$, and $98.03\\%$ on the SHREC'17 Track, DHG-14/28, and LMDHG dynamic gesture datasets, respectively.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22809329.v1"
    },
    {
        "id": 22087,
        "title": "ST-GCN AltFormer: GestureRecognition with Spatial temporal Alternating Transformer",
        "authors": "Qing Pan, Jintao Zhu, Gangmin Ning, Lingwei Zhang, Luping Fang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Recently, with the rapid development of 3D motion estimation technology and the continuous improvement of low-cost depth camera performance, skeleton-based dynamic gesture recognition technology has received increasing attention. This paper proposes a novel neural network architecture — spatial-temporal alternating graph convolutional Transformer (ST-GCN-AltFormer), which combines spatial-temporal graph convolution and in-parallel spatial-temporal alternating Transformer, for dynamic gesture recognition. We first input the joint stream to the spatial-temporal graph convolution (ST-GCN) to extract the low-level features of gesture actions. Secondly, the spatial and temporal correlations between joints are modelled by an in-parallel spatial-temporal alternating Transformer (ST-TS) module, in which a spatial Transformer (STR) extracts the spatial correlations of joints within each frame, and a temporal Transformer (TTR) extracts the inter-frame correlations between joints. STR and TTR are alternately fused to obtain the final gesture prediction results. The performance of the proposed method is validated through experiments on three public dynamic gesture datasets (SHREC'17 Track(two evaluation protocols), DHG-14/28(two evaluation protocols), and LMDHG). Compared with the state-of-the-art methods, our method achieves the highest recognition accuracy with $97.3\\%$, $95.8\\%$, $94.3\\%$, $92.8\\%$, and $98.03\\%$ on the SHREC'17 Track, DHG-14/28, and LMDHG dynamic gesture datasets, respectively.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22809329"
    },
    {
        "id": 22088,
        "title": "FluxFormer: Upscaled global carbon fluxes from eddy covariance data with multivariate timeseries Transformer",
        "authors": "Anh Phan, Hiromichi Fukui",
        "published": "No Date",
        "citations": 0,
        "abstract": "We provided a monthly global gross primary production (GPP) and ecosystem respiration (RECO) dataset from 1990 to 2019 at 0.25° × 0.25° spatial resolution named FluxFormer by utilizing the new plant function type dataset in combination with multivariate timeseries Transformer-based model. FluxFormer outperforms other satellite-derived upscaled products when comparing the correlation at site-level and seasonal pattern with FLUXNET 2015, especially in tropical regions. Additionally, our dataset shows the highest positive trend in GPP from 2001 to 2019, aligning with trends derived from dynamic global vegetation models that account for the CO2 fertilization effect. Notably, FluxFormer captures positive long-term trends that are not replicated by some existing products. FluxFormer could be used to validate terrestrial biosphere models and serve asa tool for cross-checking other datasets. The FluxFormer GPP and RECO product is available at https://doi.org/10.5281/zenodo.10258644",
        "link": "http://dx.doi.org/10.31223/x5bq2h"
    },
    {
        "id": 22089,
        "title": "Digital Protective Schemes for Power Transformer",
        "authors": "Dharmesh Patel, Nilesh Chothani",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-6763-6"
    },
    {
        "id": 22090,
        "title": "Predicting Binding from Screening Assays with Transformer Network Embeddings",
        "authors": "Paul Morris, Rachel St Clair, Elan Barenholtz, William Edward Hahn",
        "published": "No Date",
        "citations": 0,
        "abstract": "Cheminformatics aims to assist in chemistry applications that depend on molecular interactions, structural characteristics, and functional properties. The arrival of deep learning and the abundance of easily accessible chemical data from repositories like PubChem have enabled advancements in computer-aided drug discovery. Virtual High-Throughput Screening (vHTS) is one such technique that integrates chemical domain knowledge to perform in silico biomolecular simulations, but prediction of binding affinity is restricted due to limited availability of ground-truth binding assay results. Here, text representations of 83,000,000 molecules are leveraged to enable single-target binding affinity prediction directly on the outcome of screening assays. The embedding of an end-to-end Transformer neural network, trained to encode the structural characteristics of a molecule via a text-based translation task, is repurposed through transfer learning to classify binding affinity to a single target. Classifiers trained on the embedding outperform those trained on SMILES strings for multiple tasks, receiving between 0.67-0.99 AUC. Visualization reveals organization of structural and functional properties in the learned embedding useful for binding prediction. The proposed model is suitable for parallel computing, enabling rapid screening as a complement to virtual screening techniques when limited data is available.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.11625885"
    },
    {
        "id": 22091,
        "title": "Carbohydrate Transformer: Predicting Regio- and Stereoselective Reactions Using Transfer Learning",
        "authors": "Giorgio Pesciullesi, Philippe Schwaller, Teodoro Laino, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 0,
        "abstract": "Organic chemistry is central to society because it enables the synthesis of complex molecules and materials used in all fields of science and technology. The synthetic methods represent a vast body of accumulated knowledge optimally suited for deep learning. Indeed, most organic reactions involve distinct functional groups and can readily be learned by deep learning models and chemists alike. The task is, however, much more challenging for regio- and stereoselective transformations because their outcome also depends on functional group surroundings in subtle ways. Here, we challenge the Molecular Transformer model to predict reactions in carbohydrate chemistry, a field of central importance in the life sciences and for vaccine development and where regio- and stereoselectivity are notoriously difficult to predict even for experienced chemists. We show that transfer learning of the general USPTO model with a small set of carbohydrate reactions produces a specialized Carbohydrate Transformer model, returning predictions for carbohydrate reactions with remarkable accuracy. We validate these predictions experimentally with a previously unpublished synthesis of a lipid-linked oligosaccharide, involving regioselective protecting group operations and stereoselective glycosylations that are typical for complex carbohydrate synthesis. The chemical reaction transfer learning methods presented in this work are generally applicable to any reaction class of interest.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.11935635"
    },
    {
        "id": 22092,
        "title": "A Transformer-Based Model for Effective Representation of Geospatial Data and Context",
        "authors": "Rui Deng, Ziqi Li, Mingshu Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Machine learning (ML) and Artificial Intelligence (AI) models have been increasingly adopted for geospatial tasks. However, geospatial data (such as points and raster cells) are often influenced by underlying spatial effects, and current model designs often lack adequate consideration of these effects. Determining the efficient model structure for representing geospatial data and capturing the underlying complex spatial and contextual effects still needs to be explored. To address this gap, we propose a Transformer-like encoder-decoder architecture to first represent geospatial data with respect to their corresponding geospatial context, and then decode the representation for task-specific inferences. The encoder consists of embedding layers that transform the input location and attributes of geospatial data into meaningful embedding vectors. The decoder comprises task-specific neural network layers that map the encoder outputs to the final output. Spatial contextual effects are measured using explainable artificial intelligence (XAI) methods. We evaluate and compare the performance of our model with other model structures on both synthetic and real-world datasets for spatial regression and interpolation tasks. This work proposes a generalizable approach to better modeling and measuring complex spatial contextual effects, potentially contribute to efficient and reliable urban analytic applications that require geo-context information.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-1003"
    },
    {
        "id": 22093,
        "title": "Using Pre-Trained Transformer for Better Lay Summarization",
        "authors": "Seungwon Kim",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.sdp-1.38"
    },
    {
        "id": 22094,
        "title": "A solid-state wind-energy transformer",
        "authors": "Richard I. Epstein",
        "published": "2019-8-19",
        "citations": 5,
        "abstract": "We show that a solid-state apparatus with no moving parts can harvest electrical power from the wind. This apparatus, a Solid-state Wind-Energy Transformer (SWET), uses coronal discharge to create negative air ions, which the wind carries away from the SWET. The SWET harnesses the wind-induced currents and voltages to produce electrical power. We report on the operation of a low-power, proof-of-concept SWET. This device consists of a number of parallel electrical wires: “emitter wires,” which have numerous, sharp coronal emitters attached to it, and bare “attractor wires.” When a negative bias voltage is applied to the emitter wires relative to the attractor wires, the coronal emitters generate negative ions. The wind carries off these ions, which eventually settle to ground. The power imparted to the ions by the wind is extracted from the current returning to the SWET from the ground. This proof-of-concept SWET demonstrates that it is possible to generate net electrical power from the wind using only air ions. We estimate that SWET can be scaled up to commercially interesting powers by increasing the number and length of emitter and attractor wires and by controlling the bias voltage. SWETs have the potential to produce large amounts of electrical power at low costs with little negative environmental impact.",
        "link": "http://dx.doi.org/10.1063/1.5109776"
    },
    {
        "id": 22095,
        "title": "A Transformer Model for Ionospheric TEC Prediction Using GNSS Observations",
        "authors": "Maria Kaselimi, Nikolaos Doulamis, Anastasios Doulamis, Demitris Delikaraoglou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Precise modeling of the ionospheric Total Electron Content (TEC) is critical for reliable and accurate GNSS applications. TEC is the integral of the location-dependent electron density along the signal path and is a crucial parameter that is often used to describe ionospheric variability, as it is strongly affected by solar activity. TEC is highly depended on local time (temporal variability), latitude, longitude (spatial variability), solar and geomagnetic conditions. The propagation of the signals from GNSS (Global Navigation Satellite System) satellites throughout the ionosphere is strongly influenced by temporal changes and ionospheric regular or irregular variations. Here, we leverage transformer as an effective and scalable structure with self-attention mechanisms, for modeling long-range temporal dependencies for ionospheric TEC modelling based on GNSS data.&#160;The proposed transformer model is capable of learning long-range temporal dependencies. In seq2seq models, learning temporal dependencies is a demanding task, and often the model forgets the first part, once it completes processing the whole sequence input. Our model utilizes attention mechanisms and identifies complex dependencies between input sequence elements throughout the whole sequence.Our model handles imbalanced datasets. Our work demonstrates that combining the unsupervised pre-training process with downstream task fine-tuning, offers a practical solution for ionospheric TEC modelling. This is a comparative advantage against the existing state-of-the-art works which, in most cases, fail to sufficiently model intense ionospheric variability conditions.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-4303"
    },
    {
        "id": 22096,
        "title": "Development of Prototype Power Supply for Ohmic Transformer System of Ssst",
        "authors": "Urmil Thaker, Vaibhav Ranjan, Supriya A. Nair",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4396491"
    },
    {
        "id": 22097,
        "title": "Building Occupancy Number Prediction: A Transformer Approach",
        "authors": "Kailai Sun, Irfan Qaisar, Muhammad  Arslan Khan, Tian Xing, Qianchuan Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4517384"
    },
    {
        "id": 22098,
        "title": "Analysis on Connection Method of Transformer Differential Protection Test",
        "authors": "",
        "published": "2021-9-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i9.147"
    },
    {
        "id": 22099,
        "title": "Eddy Current and Stray Loss Calculations of Distributed Photovoltaic Grid Power Transformer",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16412-14"
    },
    {
        "id": 22100,
        "title": "A Study of Biodegradable Oil as Transformer Insulating Material",
        "authors": "",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pecon.2018.8684106"
    },
    {
        "id": 22101,
        "title": "Mstformer: Motion Inspired Spatial-Temporal Transformer for Vessel Trajectory Prediction",
        "authors": "Huimin Qiang, Zhiyuan Guo, Shiyuan Xie, Xiaodong Peng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4585933"
    },
    {
        "id": 22102,
        "title": "xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography",
        "authors": "Arnab Kumar Mondal, Arnab Bhattacharjee, Parag Singla, Prathosh AP",
        "published": "No Date",
        "citations": 1,
        "abstract": "Since its outbreak, the rapid growth of COrona VIrus Disease 2019 (COVID-19) across the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is imperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain the spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19 screening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several researchers have demonstrated the use of  Deep Learning (DL) methods on chest radiography (such as X-ray and CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context due to their inherent image-specific inductive bias. Motivated by this, in this work, we propose the use of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray and CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity. Furthermore, we show that the features learned by our transformer networks are explainable. We demonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on meaningful regions in the images for detection (as confirmed by Radiologists), aiding not only in accurate diagnosis of COVID-19 but also in localization of the infected area.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14912367"
    },
    {
        "id": 22103,
        "title": "Grid Ready, Flexible Large Power Transformer",
        "authors": "Ibrahima Ndiaye, Enrique Betancourt Ramírez, Jesus Avila Montes, Yazhou Jiang, Ahmed Elasser",
        "published": "2019-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1527031"
    },
    {
        "id": 22104,
        "title": "Transformer ma mal-entendance en bien-écoutance",
        "authors": "Cécile Merrer-Chakir",
        "published": "2020-7-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/gest.054.0057"
    },
    {
        "id": 22105,
        "title": "Expformer: Audio-Driven One-Shot Talking Face Generation Based On  Expression Transformer",
        "authors": "Kangwei Liu, Xiaowei Yi, Xianfeng Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4698118"
    },
    {
        "id": 22106,
        "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
        "authors": "Keulbit Kim, Namhyun Cho",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-555"
    },
    {
        "id": 22107,
        "title": "Dielectric Response Measurements",
        "authors": "",
        "published": "2017-8-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch3"
    },
    {
        "id": 22108,
        "title": "Transformer-less Medium Voltage EV Chargers",
        "authors": "Muhammad H. Alvi, Giri Venkataramanan",
        "published": "2019-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8913060"
    },
    {
        "id": 22109,
        "title": "Review for \"Stochastic investigation for solid‐state transformer integration in distributed energy resources integrated active distribution network\"",
        "authors": "Daniel Lima",
        "published": "2021-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13056/v1/review1"
    },
    {
        "id": 22110,
        "title": "Author response for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": " Cooper Lorsung,  Zijie Li,  Amir Barati Farimani",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v2/response1"
    },
    {
        "id": 22111,
        "title": "Author response for \"Physics Informed Token Transformer for Solving Partial Differential Equations\"",
        "authors": " Cooper Lorsung,  Zijie Li,  Amir Barati Farimani",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad27e3/v3/response1"
    },
    {
        "id": 22112,
        "title": "Operational stream water temperature forecasting with a temporal fusion transformer model",
        "authors": "Ryan S. Padrón, Massimiliano Zappa, Konrad Bogner",
        "published": "No Date",
        "citations": 0,
        "abstract": "Stream water temperatures influence aquatic biodiversity, agriculture, tourism, electricity production, and water quality. Therefore, stakeholders would benefit from an operational forecasting service that would support timely action. Deep Learning methods are well-suited for this task as they can provide probabilistic forecasts at individual stations of a monitoring network. Here we train and evaluate several state-of-the-art models using 10 years of data from 55 stations across Switzerland. Static features (e.g. station coordinates, catchment mean elevation, area, and glacierized fraction), time indices, meteorological and/or hydrological observations from the past 64 days, and their ensemble forecasts for the following 32 days are included as predictors in the models to estimate daily maximum water temperature for the next 32 days. We find that the Temporal Fusion Transformer (TFT) model performs best for all lead times with a cumulative rank probability score (CRPS) of 0.73 &#186;C averaged over all stations, lead times and 90 forecasts distributed over 1 full year. The TFT is followed by the Recurrent Neural Network (CRPS = 0.77 &#186;C), Neural Hierarchical Interpolation for Time Series (CRPS = 0.80 &#186;C), and Multi-layer Perceptron (CRPS = 0.85 &#186;C). All models outperform the benchmark ARX model. When factoring out the uncertainty stemming from the meteorological ensemble forecasts by using observations instead, the TFT improves to a CRPS of 0.43 &#186;C, and it remains the best of all models. In addition, the TFT model identifies air temperature and time of the year as the most relevant predictors. Furthermore, its attention feature suggests a dominant response to more recent information in the summer, and to information from the previous month during spring and autumn. Currently, daily maximum water temperature probabilistic forecasts are produced twice per week and made available at https://drought.ch/de/allgemeine-lage/wassertemperatur/fliessgewaesser-1-1.html.&#160;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-18073"
    },
    {
        "id": 22113,
        "title": "Ewt: Efficient Wavelet-Transformer for Single Image Denoising",
        "authors": "Juncheng Li, Bodong Cheng, Ying Chen, Guangwei Gao, Tieyong Zeng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4733709"
    },
    {
        "id": 22114,
        "title": "Colorectal Cancer Prevention and Chat Generative Pretrained Transformer (ChatGPT)",
        "authors": "Hinpetch Daungsupawong, Viroj Wiwanitkit",
        "published": "2024-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1097/mcg.0000000000001989"
    },
    {
        "id": 22115,
        "title": "Contextualized Embeddings and Transformer Networks",
        "authors": "",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009026222.013"
    },
    {
        "id": 22116,
        "title": "Transformer Neural Network Interpretable Prediction of Diverse Environmental Time Series Using Weather Forecasts",
        "authors": "Enrique Orozco Lopez, David Kaplan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Transformer Neural Networks (TNNs) have caused a paradigm shift in deep learning domains like natural language processing and gathered immense interest due to their versatility in other fields such as time series forecasting (TSF). Most current TSF applications of TNNs use only historic observations to predict future events, ignoring information available in weather forecasts to inform better predictions, and with little attention given to the interpretability of the model&#8217;s use of environmental input factors. This work explores the potential for TNNs to perform TSF across multiple environmental variables (streamflow, stage, water temperature, and salinity) in two ecologically important regions: the Peace River watershed (Florida) and the northern Gulf of Mexico (Louisiana). The TNN was tested (and uncertainty quantified) for each response variable from one- to fourteen-day-ahead forecasts using past observations and spatially distributed weather forecasts. Additionally, a sensitivity analysis was performed on the trained TNNs&#8217; attention weights to identify the relative influence of each input variable on each response variable&#8217;s prediction. Overall model performance ranged from good to very good (0.78<NSE<0.99 for all variables and forecast horizons). Through the sensitivity analysis, we found that the TNN was able to learn the physical patterns behind the data, adapt the use of the input variables to each forecast, and increasingly use weather forecast information as forecasting windows increased. The TNN excellent performance and flexibility, along with the intuitive interpretability highlighting the logic behind the models&#8217; forecasting decision-making process, provide evidence for the applicability of this architecture to other TSF variables and locations.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-13702"
    },
    {
        "id": 22117,
        "title": "Spatial-Aware Transformer (SAT): Enhancing Global Modeling in Transformer Segmentation for Remote Sensing Images",
        "authors": "Duolin Wang, Yadang Chen, Bushra Naz, Le Sun, Baozhu Li",
        "published": "2023-7-19",
        "citations": 1,
        "abstract": "In this research, we present the Spatial-Aware Transformer (SAT), an enhanced implementation of the Swin Transformer module, purposed to augment the global modeling capabilities of existing transformer segmentation mechanisms within remote sensing. The current landscape of transformer segmentation techniques is encumbered by an inability to effectively model global dependencies, a deficiency that is especially pronounced in the context of occluded objects. Our innovative solution embeds spatial information into the Swin Transformer block, facilitating the creation of pixel-level correlations, and thereby significantly elevating the feature representation potency for occluded subjects. We have incorporated a boundary-aware module into our decoder to mitigate the commonly encountered shortcoming of inaccurate boundary segmentation. This component serves as an innovative refinement instrument, fortifying the precision of boundary demarcation. After these strategic enhancements, the Spatial-Aware Transformer achieved state-of-the-art performance benchmarks on the Potsdam, Vaihingen, and Aerial datasets, demonstrating its superior capabilities in recognizing occluded objects and distinguishing unique features, even under challenging conditions. This investigation constitutes a significant advancement toward optimizing transformer segmentation algorithms in remote sensing, opening a wealth of opportunities for future research and development.",
        "link": "http://dx.doi.org/10.3390/rs15143607"
    },
    {
        "id": 22118,
        "title": "Integral Paraphrase of Physical Parameters of Non-uniformly Induced Medium in Optical Current Transformer",
        "authors": "Dong Yin, Zhizhong Guo, Guoqing Zhang, Wenbin Yu, Guizhong Wang, Caiyun Mo",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008384102800287"
    },
    {
        "id": 22119,
        "title": "The Effects of PWM With High <i>dv/dt</i> on Partial Discharge and Lifetime of Medium-Frequency Transformer for Medium-Voltage (MV) Solid State Transformer Applications",
        "authors": "Rachit Agarwal, Hui Li, Zhehui Guo, Peter Cheetham",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tie.2022.3174243"
    },
    {
        "id": 22120,
        "title": "Classification of the Top-cited Literature by Fusing Linguistic and Citation Information with the Transformer Model",
        "authors": "Masanao Ochi, Masanori Shiro, Jun’ichiro Mori, Ichiro Sakata",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011542200003318"
    },
    {
        "id": 22121,
        "title": "Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates",
        "authors": "Jimeng Shi, Vitalii Stebliankin, Zhaonan Wang, Shaowen Wang, Giri Narasimhan",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5703/1288284317672"
    },
    {
        "id": 22122,
        "title": "TSignal: A transformer model for signal peptide prediction",
        "authors": "Alexandru Dumitrescu, Emmi Jokinen, Juho Kellosalo, Ville Paavilainen, Harri Lähdesmäki",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSignal peptides are short amino acid segments present at the N-terminus of newly synthesized proteins that facilitate protein translocation into the lumen of the endoplasmic reticulum, after which they are cleaved off. Specific regions of signal peptides influence the efficiency of protein translocation, and small changes in their primary structure can abolish protein secretion altogether. The lack of conserved motifs across signal peptides, sensitivity to mutations, and variability in the length of the peptides, make signal peptide prediction a challenging task that has been extensively pursued over the years. We introduce TSignal, a deep transformer-based neural network architecture that utilizes BERT language models (LMs) and dot-product attention techniques. TSignal predicts the presence of signal peptides (SPs) and the cleavage site between the SP and the translocated mature protein. We show improved accuracy in terms of cleavage site and SP presence prediction for most of the SP types and organism groups. We further illustrate that our fully data-driven trained model identifies useful biological information on heterogeneous test sequences.",
        "link": "http://dx.doi.org/10.1101/2022.06.02.493958"
    },
    {
        "id": 22123,
        "title": "Interrompre le cycle des violences, transformer la communauté",
        "authors": "Emma Bigé",
        "published": "2022-9-27",
        "citations": 0,
        "abstract": "La justice transformatrice opère à trois niveaux interconnectés : macropolitiquement, elle œuvre à l’abolition du capitalisme carcéral ; micropolitiquement, elle développe des formes d’entraide communautaire pour prévenir les violences ; et nanopolitiquement, elle invente une forme de justice à ras du sol pour médier les relations interpersonnelles. Ces trois niveaux se répondent de manière fractale et créent un sens de la communauté/sous-communalité comme cet endroit, précaire, bricolé, en constant effort de réinvention, où nous nous rendons capables de rendre des comptes de ce que nous nous faisons.",
        "link": "http://dx.doi.org/10.3917/mult.088.0057"
    },
    {
        "id": 22124,
        "title": "Bibliographie",
        "authors": "",
        "published": "2022-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/inter.minde.2022.01.0217"
    },
    {
        "id": 22125,
        "title": "Electromagnetic Fields in Transformers: Theory and Computations",
        "authors": "S.V. Kulkarni, S.A. Khaparde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b13011-12"
    },
    {
        "id": 22126,
        "title": "The Wye-Delta Center Tapped Transformer Bank",
        "authors": "W. H. Kersting, Wayne Carr",
        "published": "2018-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/repc.2018.00019"
    },
    {
        "id": 22127,
        "title": "Chemical Indicators",
        "authors": "Behrooz Vahidi, Ashkan Teymouri",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-19693-6_3"
    },
    {
        "id": 22128,
        "title": "A modified Transformer-based network for seismic processing tasks",
        "authors": "R. Harsuko, T. Alkhalifah",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310331"
    },
    {
        "id": 22129,
        "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
        "authors": "Viraj Bagal, Rishal Aggarwal, P. K. Vinod, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 5,
        "abstract": "Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14561901.v1"
    },
    {
        "id": 22130,
        "title": "Other Tests",
        "authors": "Behrooz Vahidi, Ashkan Teymouri",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-19693-6_5"
    },
    {
        "id": 22131,
        "title": "Distribution Transformers",
        "authors": "Dudley L. Galloway, Dan Mulkey, Alan L. Wilks",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-3"
    },
    {
        "id": 22132,
        "title": "On the characterization results of toroidal transformer and microtransformer under different applied voltages",
        "authors": "Fellype Nascimento, Antonio Telles, Ricardo Teixeira",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work is part of a study regarding an issue found in the characterization of a microtransformer when different voltage values were applied to measure the inductance and resistance values of the windings. Under such conditions it was found a variation on those parameters, for both primary and secondary sides.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12005844.v1"
    },
    {
        "id": 22133,
        "title": "Forces in Electrodynamic Systems",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-8"
    },
    {
        "id": 22134,
        "title": "PCB Spiral Winding Transformer Design",
        "authors": "",
        "published": "2017-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23977/acsat.2017.1017"
    },
    {
        "id": 22135,
        "title": "A Bpnn-Transformer for Air Quality Index Prediction",
        "authors": "Xiankui Wu, Penghua Li, Xinyu Gu, K.W. See, Junhong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4416025"
    },
    {
        "id": 22136,
        "title": "Cross-View Image Transformer from a Single Image",
        "authors": "Xiaoquan Hua, Weixi Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4310616"
    },
    {
        "id": 22137,
        "title": "A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining",
        "authors": "Cheng Wang, Wei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nImage deraining is a challenging task that involves restoring degraded images affected by rain streaks. While Convolutional Neural Networks (CNNs) have been commonly used for this task, existing approaches often rely on stacked convolutional basic blocks with limited performance and compromised spatial detail. Furthermore, the limited receptive field of convolutional layers leads to incomplete processing of non-uniform rain streaks. To address these concerns, we propose a novel image deraining network that combines CNNs and trans- formers. Our network comprises two stages: an encoder-decoder architecture with a triple attention mechanism to capture valuable features and residual dual branch transformer blocks that enhance local information modeling. To address the transformer’s lack of local information modeling capability, we intro- duce convolution in the self-attentive mechanism of the transformer block and feed-forward network. Additionally, we employ a frequency domain contrastive learning method to enhance contrastive sample information, ensuring that the restored image closely resembles the clear image  in the frequency domain space, while still retaining a distinction from the rainy image. Extensive quantitative and qualitative experiments demonstrate that our proposed deraining network outperforms existing methods on public datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3240803/v1"
    },
    {
        "id": 22138,
        "title": "Brayton-Moser Modeling of Solid State Transformer",
        "authors": "Sonal Gedam, Vivek Pal, Ragini Meshram",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535957"
    },
    {
        "id": 22139,
        "title": "Salient Object Detection Based on Pyramid Vision Transformer Gated Network",
        "authors": "Lina Huo, wei Wang, xiaoli zhou, zhiyi cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4512541"
    },
    {
        "id": 22140,
        "title": "Multiple Templates Transformer for Visual Object Tracking",
        "authors": "Haibo Pang, Jie Su, Rongqi Ma, Tingting Li, Cheng-ming Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496392"
    },
    {
        "id": 22141,
        "title": "Intelligent Universal Transformer",
        "authors": "L. Ashok Kumar",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003203810-8"
    },
    {
        "id": 22142,
        "title": "An investigation on the electrical ageing properties of fibre\nsheathing compounds in power transformer oil",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29121/web/v18i5/40"
    },
    {
        "id": 22143,
        "title": "Machine Learning for Power Transformer Sfra Based Fault Detection",
        "authors": "Miloš Bjelić, Bogdan Brković, Mileta Žarković, Tatjana Miljković",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper presents machine learning methods for health assessment of power transformer based on sweep frequency response analysis. The paper presents an overview of monitoring and diagnostics based on statistical Sweep Frequency Response Analysis (SFRA) based indicators that are used to evaluate the state of the power transformer. Experimental data obtained from power transformers with internal short-circuit faults is used as a database for applying machine learning. Machine learning is implemented to achieve more precise asset management and condition-based maintenance. Unsupervised machine learning was applied through the k-means cluster method for classifying and dividing the examined power transformer state into groups with similar state and probability of failure. Artificial neural network (ANN) and Adaptive Neuro Fuzzy Inference System (ANFIS) as part of supervised machine learning are created in order to detect fault severity in tested power transformers of different lifetime. The presented machine learning methods can be used to improve health assessment of power transformers.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2193409/v1"
    },
    {
        "id": 22144,
        "title": "Transformer",
        "authors": "Andreas Helfrich-Schkarbanenko",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-68209-8_1"
    },
    {
        "id": 22145,
        "title": "Representation Aggregation and Propagation Transformer for 3D Point Cloud Analysis",
        "authors": "Qi Zhong, Lu Yang, Xian-Feng Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe unstructured property challenges the applications of deep learning methods to understand 3D point clouds. While the recent appearance of Transformer architecture provides an ideal solution to address the aforementioned problem. Therefore, based on Transformer's permutation invariance and capability of building long-range context relationship, in this paper we propose an end-to-end Representation Aggregation and Propagation Transformer (RAPFormer) architecture for 3D point cloud analysis. Specifically, two core components, termed Point-wise Double Transformer and Channel-wise Double Transformer modules, are well designed for explicitly capturing interdependencies among points and channels, respectively. And both models consist of two key operations: aggregation attention mapping the point-wise/channel-wise feature maps into a global space, and propagation attention diffusing the aggregated features back to the input points or channels. We illustrate the effectiveness and competitiveness via extensive quantitative and qualitative experiments on 3D shape classification and segmentation benchmark datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3298300/v1"
    },
    {
        "id": 22146,
        "title": "Adaptive Parking Slot Occupancy Detection Using Vision Transformer and LLIE",
        "authors": "Karthick Pannerselvam",
        "published": "2021-9-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isc253183.2021.9562955"
    },
    {
        "id": 22147,
        "title": "A Method to Embed Resonant Inductor within PCB Matrix Transformer for High-Density Resonant Converters",
        "authors": "Ahmed Nabih, Qiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The power electronics industry seeks more efficient power delivery and higher power density year-by-year. Resonant converters, such as the three-element resonant LLC converters, have been intensively studied and applied as DC-DC converters in many applications. One of the most demanding applications for LLC converters is data centers. Data centers consume a significant amount of power. According to some estimates\\cite{jones2018stop}, data centers around the world consume approximately 200 terawatt-hours of electricity per year, equivalent to the annual electricity consumption of a country like the Netherlands. This high power consumption is primarily due to the growing number of servers and other computing equipment used in data centers and the need for cooling systems. As a result, reducing the power consumption of data centers is a pivotal goal to save costs and reduce their environmental impact. Thus, due to the high power consumption and large-scale server farms, the pursuit of efficient and dense power architecture is ongoing.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24274867.v1"
    },
    {
        "id": 22148,
        "title": "Gas and Optical Sensing Technology for the Field Assessment of Transformer Oil",
        "authors": "Yusuf Amrulloh, Udantha Abeyratne, Chandima Ekanayake",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00077"
    },
    {
        "id": 22149,
        "title": "PIsToN: Evaluating Protein Binding Interfaces with Transformer Networks",
        "authors": "Vitalii Stebliankin, Azam Shirali, Prabin Baral, Prem Chapagain, Giri Narasimhan",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe computational studies of protein binding are widely used to investigate fundamental biological processes and facilitate the development of modern drugs, vaccines, and therapeutics. Scoring functions aim to predict complexes that would be formed by the binding of two biomolecules and to assess and rank the strength of the binding at the interface. Despite past efforts, the accurate prediction and scoring of protein binding interfaces remain a challenge. The physics-based methods are computationally intensive and often have to trade accuracy for computational cost. The possible limitations of current machine learning (ML) methods are ineffective data representation, network architectures, and limited training data. Here, we propose a novel approach called PIsToN (evaluatingProtein bindingInterfaceswithTransformerNetworks) that aim to distinguish native-like protein complexes from decoys. Each protein interface is transformed into a collection of 2D images (interface maps), where each image corresponds to a geometric or biochemical property in which pixel intensity represents the feature values. Such a data representation provides atomic-level resolution of relevant protein characteristics. To buildhybridmachine learning models, additional empirical-based energy terms are computed and provided as inputs to the neural network. The model is trained on thousands of native and computationally-predicted protein complexes that contain challenging examples. The multi-attention transformer network is also endowed with explainability by highlighting the specific features and binding sites that were the most important for the classification decision. The developed PIsToN model significantly outperforms existing state-of-the-art scoring functions on well-known datasets.",
        "link": "http://dx.doi.org/10.1101/2023.01.03.522623"
    },
    {
        "id": 22150,
        "title": "Decision letter for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v2/decision1"
    },
    {
        "id": 22151,
        "title": "Layer Sparse Transformer for Speech Recognition",
        "authors": "Peng Wang,  ZhiyuanGuo, Fei Xie",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ickg59574.2023.00039"
    },
    {
        "id": 22152,
        "title": "Decision letter for \"Identifying promising sequences for protein engineering using a deep transformer protein language model\"",
        "authors": "",
        "published": "2023-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26536/v2/decision1"
    },
    {
        "id": 22153,
        "title": "Team Papelo: Transformer Networks at FEVER",
        "authors": "Christopher Malon",
        "published": "2018",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5517"
    },
    {
        "id": 22154,
        "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
        "authors": "Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, Jinghong Zheng, Zhiguo Cao, Joey Tianyi Zhou",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00854"
    },
    {
        "id": 22155,
        "title": "基于Transformer-GANs生成有风格调节的音乐",
        "authors": "Weining Wang, Jiahui Li, Yifan Li, Xiaofen Xing",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300359"
    },
    {
        "id": 22156,
        "title": "Unveiling the Thematic Landscape of Generative Pre-trained Transformer (GPT) Through Bibliometric Analysis",
        "authors": "Carlos Alberto Gómez Cano, Verenice Sánchez Castillo, Tulio Andrés Clavijo Gallego",
        "published": "2023-4-2",
        "citations": 9,
        "abstract": "Introduction: the Generative Pre-trained Transformer (GPT) is a deep learning language model architecture developed by OpenAI.\nAim: to describe the knowledge networks (both at the theoretical and country levels) of the Generative Pre-trained Transformer (GPT) as an emerging technology.\nResults: 222 Documents were identified, of which 69 were articles, 50 were conference papers, 36 were editorials, 29 were notes, 19 were letters, 14 were reviews, 3 were conference reviews, and 2 were short surveys. In terms of the number of documents per year, 2 were found in 2019, 10 in 2020, 22 in 2021, 44 in 2022, and 144 in 2023. The year-on-year growth rate was over 100% in all years. The subject area with the highest number of documents was Computer Science with 90 documents. The most productive countries in relation to GPT were the United States with 60 documents, followed by China with 19, the United Kingdom with 18, India with 15, and Australia with 12. Co-occurrence illustrated the centrality of Artificial Intelligence, Natural Language Processing, Deep Learning, and the term Human around ChatGPT and GPT.\nConclusions: this bibliometric study aimed to describe the knowledge networks of the Generative Pre-trained Transformer (GPT) as an emerging technology. Although only 222 documents were found, this study revealed a high level of international scientific collaboration in the field. The results suggest that GPT is a highly relevant technology with a wide range of potential applications in natural language processing, artificial intelligence, and deep learning.\nMoreover, the study was able to qualitatively characterize the main thematic areas surrounding GPT, including its applications in chatbots, text generation, machine translation, sentiment analysis, and more.",
        "link": "http://dx.doi.org/10.56294/mr202333"
    },
    {
        "id": 22157,
        "title": "Pvt2dnet:Polyp Segmentation with Vision Transformer and Dual Decoder Refinement Strategy",
        "authors": "Yibiao Hu, Yan Jin, Zhiwei Jiang, Qiufu Zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458919"
    },
    {
        "id": 22158,
        "title": "Analysis of Excitation Transformer Fault Problem in Power Plant",
        "authors": "",
        "published": "2022-2-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i2.548"
    },
    {
        "id": 22159,
        "title": "Forecasting COVID-19 New Cases Using Transformer Deep Learning Model",
        "authors": "Saurabh Patil, Parisa Mollaei, Amir Barati Farimani",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractMaking accurate forecasting of COVID-19 cases is essential for healthcare systems, with more than 650 million cases as of 4 January,1making it one of the worst in history. The goal of this research is to improve the precision of COVID-19 case predictions in Russia, India, and Brazil, a transformer-based model was developed. Several researchers have implemented a combination of CNNs and LSTMs, Long Short-Term Memory (LSTMs), and Convolutional Neural Networks (CNNs) to calculate the total number of COVID-19 cases. In this study, an effort was made to improve the correctness of the models by incorporating recent advancements in attention-based models for time-series forecasting. The resulting model was found to perform better than other existing models and showed improved accuracy in forecasting. Using the data from different countries and adapting it to the model will enhance its ability to support the worldwide effort to combat the pandemic by giving more precise projections of cases.",
        "link": "http://dx.doi.org/10.1101/2023.11.02.23297976"
    },
    {
        "id": 22160,
        "title": "Isotropic Plasma: Dispersive Medium",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-1"
    },
    {
        "id": 22161,
        "title": "The Art and Science of Transformer Ratio Measurement",
        "authors": "Oleh W. Iwanusiw, P. Eng",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481036"
    },
    {
        "id": 22162,
        "title": "Proposal of Miniaturization of Rectifier Circuit Using Multiphase Transformer",
        "authors": "Akinori Kato",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vppc46532.2019.8952345"
    },
    {
        "id": 22163,
        "title": "Determination Key Indicator in Transformer Oil by Infrared Equipment",
        "authors": "Zhang Hua",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced56215.2022.9928819"
    },
    {
        "id": 22164,
        "title": "A Method to Embed Resonant Inductor within PCB Matrix Transformer for High-Density Resonant Converters",
        "authors": "Ahmed Nabih, Qiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The power electronics industry seeks more efficient power delivery and higher power density year-by-year. Resonant converters, such as the three-element resonant LLC converters, have been intensively studied and applied as DC-DC converters in many applications. One of the most demanding applications for LLC converters is data centers. Data centers consume a significant amount of power. According to some estimates\\cite{jones2018stop}, data centers around the world consume approximately 200 terawatt-hours of electricity per year, equivalent to the annual electricity consumption of a country like the Netherlands. This high power consumption is primarily due to the growing number of servers and other computing equipment used in data centers and the need for cooling systems. As a result, reducing the power consumption of data centers is a pivotal goal to save costs and reduce their environmental impact. Thus, due to the high power consumption and large-scale server farms, the pursuit of efficient and dense power architecture is ongoing.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24274867"
    },
    {
        "id": 22165,
        "title": "Transient-Voltage Response of Coils and Windings",
        "authors": "Robert C. Degeneff",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-21"
    },
    {
        "id": 22166,
        "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
        "authors": "Ziyi Chen, Pengyuan Zhang",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1301"
    },
    {
        "id": 22167,
        "title": "Screening of Constructional Parts",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-4"
    },
    {
        "id": 22168,
        "title": "Review of Hybrid Transformer Topology",
        "authors": "Xin Wan, Man-Chung Wong",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syps59767.2023.10268179"
    },
    {
        "id": 22169,
        "title": "Data Expansion and Relabeling For Improving the vision transformer-based Micro-expression Recognition",
        "authors": "He Zhang, Hanling Zhang, Lu Yin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer-based Micro-expression Recognition Improved by Data Expansion and Relabeling</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21842520"
    },
    {
        "id": 22170,
        "title": "Design of Solid State Transformer",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21172/ijiet.83.027"
    },
    {
        "id": 22171,
        "title": "Transformer regards et politiques sur le grand âge",
        "authors": "Monique Iborra",
        "published": "2019-4-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/const.053.0080"
    },
    {
        "id": 22172,
        "title": "Un discours peut transformer le monde",
        "authors": "Barbara Cassin, Alizée Vincent",
        "published": "2019-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/sh.312.0004"
    },
    {
        "id": 22173,
        "title": "Transformer with Bidirectional Decoder for Speech Recognition",
        "authors": "Xi Chen, Songyang Zhang, Dandan Song, Peng Ouyang, Shouyi Yin",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2677"
    },
    {
        "id": 22174,
        "title": "Aged transformer oil analysis through laser induced breakdown spectroscopy",
        "authors": "Amir Hossein Farhadian, Morteza Mikhak-Beyranvand, Seyed Sajad Mousavi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169842966.62021440/v1"
    },
    {
        "id": 22175,
        "title": "Enhancing Cervical Cancer Diagnosis: Integrated Attention-Transformer System with Weakly Supervised Learning",
        "authors": "Ashfaque Khowaja, Beiji Zou, Xiaoyan Kui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4661793"
    },
    {
        "id": 22176,
        "title": "TCCL-Net: Transformer-Convolution Collaborative Learning Network for Omnidirectional Image Super-Resolution",
        "authors": "Xiongli Chai, Feng Shao, Qiuping Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4362667"
    },
    {
        "id": 22177,
        "title": "Wearable Sensor Based Human Activity Recognition with Transformer",
        "authors": "Iveta Dirgová Luptáková, Martin Kubovčík, Jiří Pospíchal",
        "published": "No Date",
        "citations": 7,
        "abstract": "This paper describes the successful application of the Transformer model used in the natural language processing and vision tasks as a means of processing the time series of signals from gyroscope and accelerometer sensors for the classification of human activities. The Transformer model is based on deep neural networks with many layers which can generalize well on signals. All measured signals come from a smartphone placed in a waist bag. Activity prediction is sequence-to-sequence, each time step of the signal is assigned a designation of the performed activity. Emphasis is placed on attention mechanisms, which express individual dependencies between signal values within a time series. In comparison with another recent result, the recognition precision was improved from 89.67 percent to 99.2 percent. The transformer model should in the future be included among the top options in machine learning methods for human activity recognition.",
        "link": "http://dx.doi.org/10.20944/preprints202202.0111.v1"
    },
    {
        "id": 22178,
        "title": "A Review of Research Progress on Residual Magnetism Detection Methods for Power Transformer Cores",
        "authors": "Cailing Huo, Yiming Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4421802"
    },
    {
        "id": 22179,
        "title": "ConvFormer: Tracking by Fusing Convolution and Transformer Features",
        "authors": "Chao Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3293592"
    },
    {
        "id": 22180,
        "title": "\"Still They Remember Me\"",
        "authors": "CAROL A. DANA, MARGO LUKENS, CONOR M. QUINN",
        "published": "2021-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52"
    },
    {
        "id": 22181,
        "title": "3T-Net: Transformer Encoders for Destination Prediction",
        "authors": "Jing Zhang, Daniel Nikovski, Takuro Kojima",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240616"
    },
    {
        "id": 22182,
        "title": "Racing with Vision Transformer Architecture",
        "authors": "Chengwen Tian, Liang Song",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ishc56805.2022.00055"
    },
    {
        "id": 22183,
        "title": "Power Transformer Diagnostics, Monitoring and Design Features",
        "authors": "Issouf Fofana, Yazid Hadjadj",
        "published": "2018-11-22",
        "citations": 7,
        "abstract": "The reliability of the power grid system directly contributes to the economic well-being and the quality of life of citizens in any country. [...]",
        "link": "http://dx.doi.org/10.3390/en11123248"
    },
    {
        "id": 22184,
        "title": "LaneFormer: An Efficient Transformer-based Network for Fast Lane Detection",
        "authors": "Yifan Yao, Huilin Xiong",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055124"
    },
    {
        "id": 22185,
        "title": "Transformer Leakage Inductance Design Methodology",
        "authors": "Angshuman Sharma, Jonathan W. Kimball",
        "published": "2023-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43580.2023.10131409"
    },
    {
        "id": 22186,
        "title": "Customer Requirements Extraction Based on Transformer",
        "authors": "Guangyu Huang",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceace60673.2023.10442158"
    },
    {
        "id": 22187,
        "title": "Protein sequence profile prediction using ProtAlbert transformer",
        "authors": "Armin Behjati, Fatemeh Zare-Mirakabad, Seyed Shahriar Arab, Abbas Nowzari-Dalini",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractProtein profiles have many applications in bioinformatics. To construct the profile from a protein sequence, the sequence is aligned with database. However, sometimes there are no similar sequences with the query. This paper proposes a method based on pre-trained ProtAlbert transformer to predict the profile for a single protein sequence without alignment. The performance of transformers on natural languages is impressive. Protein sequences can be viewed as a language; therefore, we can benefit from using these models. We analyze the attention heads in different layers of ProtAlbert to show that the transformer can capture five essential protein characteristics of the family from a single protein sequence. These assessments are performed on the CASP13 dataset to find representative heads for each of five protein characteristics. Then, these heads are investigated on one thermophilic and two mesophilic proteins as case studies. The results show the significant attention heads for protein family properties extracted from a single protein sequence. This analysis led us to propose an algorithm called PA_SPP for profile prediction using only a single protein sequence as input. In our algorithm, we apply the masked language modeling method of ProtAlbert. The results display high similarity between the predicted profiles and HSSP profiles.",
        "link": "http://dx.doi.org/10.1101/2021.09.23.461475"
    },
    {
        "id": 22188,
        "title": "Target Detection Algorithm Based on Efficient Self-Attention-Convolution Enhanced Transformer",
        "authors": "Fengping An, Jianrong Wang, Ruijun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSince the target detection algorithm based on convolutional neural network suffers from limited convolutional kernel receptive field, it leads to the model's inability to perceive the remote semantic information in the image. Because the Transformer model does not have the limitation of local receptive fields, it is introduced into the field of target detection, and many scholars have proposed target detection algorithms based on Transformer and its variants. However, the Transformer model has the difficulties of not being able to extract the deep feature information and the high computational complexity of the standard self-attention mechanism in the process of target detection and recognition applications. Aiming at the above two core problems, we have carried out in-depth analysis and research, and proposed an encoder-decoder model consisting of a convolutional layer and a Transformer module. And then, we constructed the efficient multi-head self-attention mechanism, which can capture both local and remote contextual information of target image features. Then, we design efficient convolutional module-enhanced cross-window connectivity, which can significantly improve the characterization and global modeling capabilities of Transformer model. In addition, we propose the convolution-enhanced Transformer learning framework, which improves the adaptability to different datasets, which also integrates the sparse sampling strategy. It can significantly reduce the memory and computational requirements in large-scale image processing. Finally, we propose a target detection algorithm based on a new Transformer framework. We conducted ablation experiments and computational performance comparison experiments on several HRRS scenes and natural scene datasets. The experimental results confirm that our proposed method obtains optimal results in terms of weighted F-measure, average F-measure and MAE. Moreover, our proposed method has clearer edge information and more accurate target localization information in the visual effect of detection results.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3980415/v1"
    },
    {
        "id": 22189,
        "title": "Decision letter for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v1/decision1"
    },
    {
        "id": 22190,
        "title": "Efficient Structured Prediction with Transformer Encoders",
        "authors": "Ali Basirat",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "Finetuning is a useful method for adapting Transformer-based text encoders to new tasks but can be computationally expensive for structured prediction tasks that require tuning at the token level. Furthermore, finetuning is inherently inefficient in updating all base model parameters, which prevents parameter sharing across tasks. To address these issues, we propose a method for efficient task adaptation of frozen Transformer encoders based on the local contribution of their intermediate layers to token representations. Our adapter uses a novel attention mechanism to aggregate intermediate layers and tailor the resulting representations to a target task. Experiments on several structured prediction tasks demonstrate that our method outperforms previous approaches, retaining over 99% of the finetuning performance at a fraction of the training cost. Our proposed method offers an efficient solution for adapting frozen Transformer encoders to new tasks, improving performance and enabling parameter sharing across different tasks.",
        "link": "http://dx.doi.org/10.3384/nejlt.2000-1533.2024.4932"
    },
    {
        "id": 22191,
        "title": "Flux-based turn-to-turn fault protection for power transformer",
        "authors": "Aarati Rajput, Mr. C. R. Lakade",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24001/icsesd2017.1"
    },
    {
        "id": 22192,
        "title": "On the characterization results of toroidal transformer and microtransformer under different applied voltages",
        "authors": "Fellype Nascimento, Antonio Telles, Ricardo Teixeira",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work is part of a study regarding an issue found in the characterization of a microtransformer when different voltage values were applied to measure the inductance and resistance values of the windings. Under such conditions it was found a variation on those parameters, for both primary and secondary sides.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12005844"
    },
    {
        "id": 22193,
        "title": "Discussion on Application of Power Transformer Relay Protection Technology",
        "authors": "",
        "published": "2022-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i8(02).29"
    },
    {
        "id": 22194,
        "title": "Application of Energy Saving Technology in Power Transformer Design",
        "authors": "",
        "published": "2022-5-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i5(01).41"
    },
    {
        "id": 22195,
        "title": "Discussion on Safe Operation Management of Power Supply Transformer",
        "authors": "",
        "published": "2021-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i11.93"
    },
    {
        "id": 22196,
        "title": "MULTIMODAL TRANSFORMER-BASED IMAGE CAPTION GENERATION",
        "authors": "",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets46115"
    },
    {
        "id": 22197,
        "title": "Analysis of Some Problems in Condition Maintenance of Transformer",
        "authors": "",
        "published": "2021-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i9.64"
    },
    {
        "id": 22198,
        "title": "Transformer la République",
        "authors": "Bruno Perreau, Joan Wallach Scott",
        "published": "2017-1-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/scpo.perre.2017.01.0013"
    },
    {
        "id": 22199,
        "title": "Thermal Analysis For Distribution Transformer With ONAN Cooling System",
        "authors": "ابتسام أحمد الحسن أحمد",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36458/1253-000-029-010"
    },
    {
        "id": 22200,
        "title": "Data Expansion and Relabeling For Improving the vision transformer-based Micro-expression Recognition",
        "authors": "He Zhang, Hanling Zhang, Lu Yin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer-based Micro-expression Recognition Improved by Data Expansion and Relabeling</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21842520.v1"
    },
    {
        "id": 22201,
        "title": "Decision letter for \"Identifying promising sequences for protein engineering using a deep transformer protein language model\"",
        "authors": "",
        "published": "2023-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/prot.26536/v1/decision1"
    },
    {
        "id": 22202,
        "title": "Thermal simulation of a power transformer",
        "authors": "Lizeta Popescu, Lucian Diodiu",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mps.2019.8759748"
    },
    {
        "id": 22203,
        "title": "A Transformer-Convolutional Neural Network Based Framework for Predicting Ionic Liquid Properties",
        "authors": "Guzhong Chen, Zhen Song, Zhiwen Qi",
        "published": "No Date",
        "citations": 1,
        "abstract": "Of central importance to evaluate the suitability of ionic liquids (ILs)\nfor a process is the accurate estimation of IL properties related to\ntarget performances. In this work, a versatile deep learning method for\npredicting IL properties is developed. Molecular fingerprints are\nderived from the encoder state of a Transformer model pre-trained on the\nPubChem database, which allows transfer learning from large-scale\nunlabeled data and significantly improves generalization performance for\ndeveloping models with small datasets. Employing the pre-trained\nmolecular fingerprints, convolutional neural network (CNN) models for IL\nproperties prediction are trained and tested on 11 databases. The\nobtained Transformer-CNN models present superior performance to\nstate-of-the-art models in all cases and enable property prediction of\nmillions of ILs shortly. The application of the proposed models is\nexemplified by searching CO2 absorbent from a huge database of 8,333,096\nsynthetically feasible ILs, which is by far the most high-throughput IL\nscreening in literature.",
        "link": "http://dx.doi.org/10.22541/au.166311194.46628611/v1"
    },
    {
        "id": 22204,
        "title": "Meta-Model Based Scaling Laws Of A Two-Winding Transformer",
        "authors": "Ahmed Tahir",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37376/2402-001-001-003"
    },
    {
        "id": 22205,
        "title": "Transformer la cité de transit de Beutre : pour une conception ouverte en architecture",
        "authors": "Marion Howa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56698/metropolitiques.1943"
    },
    {
        "id": 22206,
        "title": "An extremely ultrathin flexible Huygens’s transformer",
        "authors": "Alireza Ghaneizadeh, Khalil Mafinezhad, Mojtaba Joodaki",
        "published": "2020-10-1",
        "citations": 3,
        "abstract": "The current study aims to present the physical perception of a meta-surface energy harvester’s (MEH’s) design based on space-time physics of a traveling wave. Regarding the relation between the wave-velocity and field-impedance, the balance condition in Huygens’s meta-atoms is provided. Accordingly, it was demonstrated that MEH behaves as a transformer at far-field. It was observed that the location of the metallic-via is mimicked by the number of loop coils in the secondary of the transformer in the unit-cell. In addition, the impedance matching between the wave impedance in a lossless medium and MEH’s load was to be tuned by adjusting the size parameters of the unit-cell at a desired resonance frequency. For this purpose, the present study developed a simple design framework to achieve the resonance frequency at a more optimum pace based on surrogate modeling. The theoretical analyses are validated by the results of full-wave and circuit simulations. Finally, a recently developed flexible MEH was further extended to a multi-polarization structure using more compact cells. The fabricated flexible MEH has 10 × 10 number of deep subwavelength thick cells (≈0.004λ0), while traditional MEH was basically designed only to fit on the planar surface. The new design paves the way for the multi-polarized MEH to wrap around the cylindrical surface as a 2D-isotropic MEH. The results of the data analyses show that the simulation and experimental results enjoy an acceptable agreement.",
        "link": "http://dx.doi.org/10.1063/5.0016373"
    },
    {
        "id": 22207,
        "title": "Empowering Dysarthric Communication: Hybrid Transformer-Ctc Based Speech Recognition System",
        "authors": "R Vinotha, Hepsiba D, L. D. Vijay Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4565143"
    },
    {
        "id": 22208,
        "title": "Transformer Leakage Inductance Design Methodology",
        "authors": "Angshuman Sharma, Jonathan Kimball",
        "published": "2023-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1991825"
    },
    {
        "id": 22209,
        "title": "STGT: Forecasting Pedestrian Motion Using Spatio-Temporal Graph Transformer",
        "authors": "Arsal Syed, Brendan Morris",
        "published": "2021-7-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv48863.2021.9575498"
    },
    {
        "id": 22210,
        "title": "Three-Phase Transformer-less Hybrid-Bypass Inverter",
        "authors": "Liwei Zhou, Matthias Preindl",
        "published": "2019-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8912939"
    },
    {
        "id": 22211,
        "title": "Defort: Deformable Transformer for Visual Tracking",
        "authors": "Kai Yang, Qun Li, Chunwei Tian, Haijun Zhang, Aiwu Shi, Jinkai Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4691095"
    },
    {
        "id": 22212,
        "title": "Does Transformer Deep Learning Yield More Accurate Sores on Written Explanations Than Traditional Machine Learning?",
        "authors": "Holly Amerman",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3102/ip.23.2014053"
    },
    {
        "id": 22213,
        "title": "Fault diagnosis of power transformer based on improved particle swarm optimization OS-ELM",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2019.125987"
    },
    {
        "id": 22214,
        "title": "Abstractive Text Summarization Based on Long-Short Transformer",
        "authors": "Shibo Ji, Bo Yang",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiiot58121.2023.10174260"
    },
    {
        "id": 22215,
        "title": "Spectral-Spatial-Frequency Transformer Network for Hyperspectral Image Classification",
        "authors": "Xin Qiao, Weimin Huang",
        "published": "2023-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sas58821.2023.10254115"
    },
    {
        "id": 22216,
        "title": "Reducing Electricity Losses at 10/0.4 kV Transformer Substations by Replacing one Transformer With two and Using Photovoltaic Arrays",
        "authors": "A.V. Vinogradov,  , A.V. Vinogradova, A.A. Lansberg, D.V. Konkin,  ,  ,  ",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "Using the example of rural electrical networks in the Oryol region, it was revealed that transformers are loaded at 20-25%. The use of photovoltaic arrays (PV) in the grid structure leads to an additional seasonal reduction in load. A method has been proposed to reduce electricity losses in transformer substations by replacing one transformer with two of lower power. An assessment was made of the reduction in losses and their cost when using the proposed method, including the use of photovoltaic arrays.",
        "link": "http://dx.doi.org/10.33267/2072-9642-2023-11-40-44"
    },
    {
        "id": 22217,
        "title": "Hybrid Distribution Transformer Based on an Existing Distribution Transformer and a Series-Connected Power Converter",
        "authors": "Hyun-Jun Lee, Sang Won Yoon, Young-Doo Yoon",
        "published": "2022-10",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrd.2022.3147820"
    },
    {
        "id": 22218,
        "title": "Quantifying the Bias of Transformer-Based Language Models for African American English in Masked Language Modeling",
        "authors": "Flavia Salutari, Jerome Ramos, Hossein A. Rahmani, Leonardo Linguaglossa, Aldo Lipani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-33374-3_42"
    },
    {
        "id": 22219,
        "title": "Transformer Models and Convolutional Networks with Different Activation Functions for Swallow Classification Using Depth Video Data",
        "authors": "Derek Ka-Hei Lai, Ethan Shiu-Wang Cheng, Bryan Pak-Hei So, Ye-Jiao Mao, Sophia Ming-Yan Cheung, Daphne Sze Ki Cheung, Duo Wai-Chi Wong, James Chung-Wai Cheung",
        "published": "2023-7-12",
        "citations": 1,
        "abstract": "Dysphagia is a common geriatric syndrome that might induce serious complications and death. Standard diagnostics using the Videofluoroscopic Swallowing Study (VFSS) or Fiberoptic Evaluation of Swallowing (FEES) are expensive and expose patients to risks, while bedside screening is subjective and might lack reliability. An affordable and accessible instrumented screening is necessary. This study aimed to evaluate the classification performance of Transformer models and convolutional networks in identifying swallowing and non-swallowing tasks through depth video data. Different activation functions (ReLU, LeakyReLU, GELU, ELU, SiLU, and GLU) were then evaluated on the best-performing model. Sixty-five healthy participants (n = 65) were invited to perform swallowing (eating a cracker and drinking water) and non-swallowing tasks (a deep breath and pronouncing vowels: “/eɪ/”, “/iː/”, “/aɪ/”, “/oʊ/”, “/u:/”). Swallowing and non-swallowing were classified by Transformer models (TimeSFormer, Video Vision Transformer (ViViT)), and convolutional neural networks (SlowFast, X3D, and R(2+1)D), respectively. In general, convolutional neural networks outperformed the Transformer models. X3D was the best model with good-to-excellent performance (F1-score: 0.920; adjusted F1-score: 0.885) in classifying swallowing and non-swallowing conditions. Moreover, X3D with its default activation function (ReLU) produced the best results, although LeakyReLU performed better in deep breathing and pronouncing “/aɪ/” tasks. Future studies shall consider collecting more data for pretraining and developing a hyperparameter tuning strategy for activation functions and the high dimensionality video data for Transformer models.",
        "link": "http://dx.doi.org/10.3390/math11143081"
    },
    {
        "id": 22220,
        "title": "NÔM TEXT RECOGNITION USING AN END-TO-END TRANSFORMER-BASED ARCHITECTURE WITH PRE-TRAINED MODELS",
        "authors": "Nguyen Xuan Quang, Nguyen Quang Tan, Le Thi Thuy Hang, Dinh Dien",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15625/vap.2023.0084"
    },
    {
        "id": 22221,
        "title": "A Hardware-Friendly Tiled Singular-Value Decomposition-Based Matrix Multiplication for Transformer-Based Models",
        "authors": "Hailong Li, Jaewan Choi, Yongsuk Kwon, Jung Ho Ahn",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lca.2023.3323482"
    },
    {
        "id": 22222,
        "title": "Comparison of LSTM, GRU and Transformer Neural Network Architecture for Prediction of Wind Turbine Variables",
        "authors": "Pablo-Andrés Buestán-Andrade, Matilde Santos, Jesús-Enrique Sierra-García, Juan-Pablo Pazmiño-Piedra",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-42536-3_32"
    },
    {
        "id": 22223,
        "title": "Improved YOLOv7 models based on modulated deformable convolution and swin transformer for object detection in fisheye images",
        "authors": "Jie Zhou, Degang Yang, Tingting Song, Yichen Ye, Xin Zhang, Yingze Song",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.imavis.2024.104966"
    },
    {
        "id": 22224,
        "title": "Hybrid Autoregressive and Non-Autoregressive Transformer Models for Speech Recognition",
        "authors": "Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Shuai Zhang, Zhengqi Wen",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsp.2022.3152128"
    },
    {
        "id": 22225,
        "title": "Evaluation of computational models for electromagnetic force calculation in transformer windings using finite-element method",
        "authors": "Arthur F. Andrade, Edson G. Costa, João P.C. Souza, Filipe L.M. Andrade, Jalberth F. Araujo",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2023.109744"
    },
    {
        "id": 22226,
        "title": "AccTFM: An Effective Intra-Layer Model Parallelization Strategy for Training Large-Scale Transformer-Based Models",
        "authors": "Zihao Zeng, Chubo Liu, Zhuo Tang, Kenli Li, Keqin Li",
        "published": "2022-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpds.2022.3187815"
    },
    {
        "id": 22227,
        "title": "Serial KinderMiner (SKiM) discovers and annotates biomedical knowledge using co-occurrence and transformer models",
        "authors": "Robert J. Millikin, Kalpana Raja, John Steill, Cannon Lock, Xuancheng Tu, Ian Ross, Lam C. Tsoi, Finn Kuusisto, Zijian Ni, Miron Livny, Brian Bockelman, James Thomson, Ron Stewart",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Abstract\nBackground\nThe PubMed archive contains more than 34 million articles; consequently, it is becoming increasingly difficult for a biomedical researcher to keep up-to-date with different knowledge domains. Computationally efficient and interpretable tools are needed to help researchers find and understand associations between biomedical concepts. The goal of literature-based discovery (LBD) is to connect concepts in isolated literature domains that would normally go undiscovered. This usually takes the form of an A–B–C relationship, where A and C terms are linked through a B term intermediate. Here we describe Serial KinderMiner (SKiM), an LBD algorithm for finding statistically significant links between an A term and one or more C terms through some B term intermediate(s). The development of SKiM is motivated by the observation that there are only a few LBD tools that provide a functional web interface, and that the available tools are limited in one or more of the following ways: (1) they identify a relationship but not the type of relationship, (2) they do not allow the user to provide their own lists of B or C terms, hindering flexibility, (3) they do not allow for querying thousands of C terms (which is crucial if, for instance, the user wants to query connections between a disease and the thousands of available drugs), or (4) they are specific for a particular biomedical domain (such as cancer). We provide an open-source tool and web interface that improves on all of these issues.\n\nResults\nWe demonstrate SKiM’s ability to discover useful A–B–C linkages in three control experiments: classic LBD discoveries, drug repurposing, and finding associations related to cancer. Furthermore, we supplement SKiM with a knowledge graph built with transformer machine-learning models to aid in interpreting the relationships between terms found by SKiM. Finally, we provide a simple and intuitive open-source web interface (https://skim.morgridge.org) with comprehensive lists of drugs, diseases, phenotypes, and symptoms so that anyone can easily perform SKiM searches.\n\nConclusions\nSKiM is a simple algorithm that can perform LBD searches to discover relationships between arbitrary user-defined concepts. SKiM is generalized for any domain, can perform searches with many thousands of C term concepts, and moves beyond the simple identification of an existence of a relationship; many relationships are given relationship type labels from our knowledge graph.\n",
        "link": "http://dx.doi.org/10.1186/s12859-023-05539-y"
    },
    {
        "id": 22228,
        "title": "Design and Construction of an Electric Arc Welding Machine’s Transformer",
        "authors": "Asiwe Uchechukwu. M, Edema Anthony, Edeafeadhe Godspower,  ",
        "published": "2018-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd18784"
    },
    {
        "id": 22229,
        "title": "A Review on Various Methods of Transformer Protection",
        "authors": "Anand Sharma, Dr. Deepika Chauhan, Vijay Kumar Mahawar,  ",
        "published": "2018-4-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd11267"
    },
    {
        "id": 22230,
        "title": "Tunable Broadband RF Photonic Fractional Hilbert Transformer Based on a Soliton Crystal Microcomb",
        "authors": "Mengxi Tan, Xingyuan Xu, David Moss",
        "published": "No Date",
        "citations": 0,
        "abstract": "We demonstrate an RF photonic fractional Hilbert transformer based on an integrated Kerr micro-comb source featuring a record low free spectral range of 49 GHz. By programming and shaping the comb lines according to calculated tap weights for up to 39 wavelengths across the C-band, we achieve tunable bandwidths ranging from 1.2 to 15.3 GHz as well as variable center frequencies from baseband to 9.5 GHz, for both standard integral and arbitrary fractional orders. We experimentally characterize the RF amplitude and phase response of the tunable bandpass and lowpass Hilbert transformers with 90 and 45-degree phase shifts. The experimental results show good agreement with theory, confirming the effectiveness of our approach as a powerful way to implement standard and fractional order Hilbert transformers with broad and variable bandwidths and center frequencies, with high reconfigurability and greatly reduced size and complexity.  Tan, and D. J. Moss are with the Optical Sciences Centre, Swinburne University of Technology, Hawthorn, VIC 3122, Australia. (Corresponding e-mail: dmoss@swin.edu.au).  Xu is with the Electro-Photonics Laboratory, Department of Electrical and Computer System Engineering, Monash University, Clayton, 3800 VIC, Australia ",
        "link": "http://dx.doi.org/10.20944/preprints202104.0162.v1"
    },
    {
        "id": 22231,
        "title": "Design of High Power Density Transformer",
        "authors": "Xiao-Dong DING",
        "published": "2017-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/dtetr/iceeac2017/10722"
    },
    {
        "id": 22232,
        "title": "Molecular Transformer-aided Biocatalysed Synthesis Planning",
        "authors": "Daniel Probst, Matteo Manica, Yves Gaëtan Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, Teodoro Laino",
        "published": "No Date",
        "citations": 0,
        "abstract": "Enzyme catalysts are an integral part of green chemistry strategies towards a more sustainable and resource-efficient chemical synthesis. However, the use of enzymes on unreported substrates and their specific stereo- and regioselectivity  are domain-specific knowledge factors that require decades of field experience to master. This makes the retrosynthesis of given targets with biocatalysed reactions a significant challenge. Here, we use the molecular transformer architecture to capture the latent knowledge about enzymatic activity from a large data set of publicly available biochemical reactions, extending forward reaction and retrosynthetic pathway prediction to the domain of biocatalysis. We introduce the use of a class token based on the EC classification scheme that allows to capture catalysis patterns among different enzymes belonging to the same hierarchical families. The forward prediction model achieves an accuracy of 49.6% and 62.7%, top-1 and top-5 respectively, while the single-step retrosynthetic model shows a round-trip accuracy of 39.6% and 42.6%, top-1 and top-10 respectively. Trained models and curated data are made publicly available with the hope of promoting enzymatic catalysis and making green chemistry more accessible through the use of digital technologies.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14639007"
    },
    {
        "id": 22233,
        "title": "Reducing Transformer Oil Leakages in Projects",
        "authors": "Hani K. Hajjar, Abdulraman M. Alsomali",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeegcc.2017.8448205"
    },
    {
        "id": 22234,
        "title": "Semantic Augmentation Transformer Model for Ad-hoc Retrieval",
        "authors": "Chongyang Li",
        "published": "2022-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec53115.2022.9734522"
    },
    {
        "id": 22235,
        "title": "Comprehensive Analysis of Transformer Networks in Identifying Informative Sentences Containing Customer Needs",
        "authors": "Mehrshad Kashi, Salim Lahmiri, Otmane Ait-Mohamed",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4719403"
    },
    {
        "id": 22236,
        "title": "Notes",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-014"
    },
    {
        "id": 22237,
        "title": "Transformer flies, drives and squeezes past obstacles",
        "authors": "Yvaine Ye",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/s0262-4079(19)30971-6"
    },
    {
        "id": 22238,
        "title": "The Reference Wideband Inductive Current Transformer",
        "authors": "Michal Kaczmarek, Piotr Kaczmarek, Ernest Stano",
        "published": "2023-10-28",
        "citations": 0,
        "abstract": "The aim of this paper is to show that the developed inductive current transformer may ensure the required wideband transformation accuracy and it may be applied, as the reference source, in the measuring system for the evaluation of the transformation accuracy of inductive current transformers for harmonics of distorted current. This device ensures 5 A and 1 A RMS secondary currents to provide the opportunity to use the differential measuring setup. Such solutions are characterized by the significantly reduced measurement uncertainty in relation to the comparative measurements made between two current/voltage channels. The problems required to ensure the high wideband transformation accuracy, including the self-generation phenomenon of the low order higher harmonics to the secondary current and a too-low frequency range of operation, were overcome in the design process. The values of its ratio error and the phase displacement of the developed reference wideband inductive current transformer did not exceed ±0.2%/° up to 1 kHz, ±0.4%/° from 1 kHz up to 1.5 kHz and ±0.5%/° from 1.5 kHz up to 3 kHz, as is required to perform the test procedure in accordance with the optional requirements for the inductive current transformers defined in the new edition of the standard IEC 61869-1.",
        "link": "http://dx.doi.org/10.3390/en16217307"
    },
    {
        "id": 22239,
        "title": "Dual-Stage Network Combining Transformer and Hybrid Convolutions for Stereo Image Super-Resolution",
        "authors": "Jintao Zeng, Aiwen Jiang, Feiqiang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740504"
    },
    {
        "id": 22240,
        "title": "Editor's evaluation: Early stage NSCLS patients’ prognostic prediction with multi-information using transformer and graph neural network model",
        "authors": "Caigang Liu",
        "published": "2022-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.80547.sa0"
    },
    {
        "id": 22241,
        "title": "Decision letter: Early stage NSCLS patients’ prognostic prediction with multi-information using transformer and graph neural network model",
        "authors": "",
        "published": "2022-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.80547.sa1"
    },
    {
        "id": 22242,
        "title": "Transformer la violence en conflit ?",
        "authors": "Karine Hendriks, Virginie Gazon",
        "published": "2023-5-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/epar.647.0054"
    },
    {
        "id": 22243,
        "title": "Simulation Analysis of Transformer Short Circuit Fault",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23977/jeeem.2023.060409"
    },
    {
        "id": 22244,
        "title": "Transformer Design Review: A Link between Design and Maintenance Stages",
        "authors": "Ryszard Sobocki",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-7"
    },
    {
        "id": 22245,
        "title": "Multi-Granularity Cross Transformer Network for Person Re-Identification",
        "authors": "Yanping Li, Duoqian Miao, Hongyun Zhang, Jie Zhou, Cairong Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4438944"
    },
    {
        "id": 22246,
        "title": "SEMANTIC TAGGING OF REQUIREMENT DESCRIPTIONS: A TRANSFORMER-BASED APPROACH",
        "authors": "",
        "published": "2020-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33965/ac2020_202013c015"
    },
    {
        "id": 22247,
        "title": "MAGNETORESISTIVE TRANSFORMER ON THIN FILMS",
        "authors": "Yu. F. Zinkovsky, A. I. Vytiaganets",
        "published": "2018-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18372/1990-5548.56.12929"
    },
    {
        "id": 22248,
        "title": "Fundamental Equations of Electromagnetic Field",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-2"
    },
    {
        "id": 22249,
        "title": "Epilogue",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-012"
    },
    {
        "id": 22250,
        "title": "An Improved Framework for Scaling Party Positions from Texts using Transformer and Supervised Dimension Reduction",
        "authors": "Hung Hoang Viet Nguyen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Previous research in both Natural Language Processing (NLP) and political science has proven the superiority of the Transformer architecture compared to word frequencies and word embeddings on multiple text analysis tasks. In this article, I introduce a novel framework for scaling party positions from texts using Transformer. Besides a sizable boost in text representation, I demonstrate how the scaling framework also benefits from a supervised dimension reduction technique, one which reduces complex contextual embeddings produced from Transformer into meaningful position scores for political science research. This Transformer-based scaling framework is scalable, reproducible, and extensible across languages, political domains, and theoretical models. A dataset of party positions for seventeen Western democratic societies is released along with the paper.",
        "link": "http://dx.doi.org/10.31235/osf.io/8sha3"
    },
    {
        "id": 22251,
        "title": "TOSQ: Transparent object segmentation through query based transformer",
        "authors": "Bin Ma, Ming Ma, Ruiguang Li, Jiawei zheng, Deping Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSensing transparent objects has many applications in human daily life, including robot navigation and grasping. However, this task presents significant challenges due to the unpredictable nature of scenes that lay beyond transparent objects. This paper aims to solve the transparent object segmentation problem based Transformer. We design a Query Parsing Module (QPM) that formulates the transparent object segmentation task into a dictionary look-up problem and a set of learnable class prototypes as query inputs. Based QPM, we propose a high-performance transformer-based end-to-end segmentation model Transparent Object Segmentation through Query (TOSQ). TOSQ’s encoder is based on the Segformer’s backbone, and its decoder consists of a series of QPM modules. On the Trans10K-V2 dataset, TOSQ significantly outperforms almost all CNN-based and transformer-based methods, fully demonstrating the unique advantages and great potential of TOSQ to solve the semantic segmentation problem of transparent objects in daily human life. The code is publicly available at https://github.com/ldepn/tosq.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3881956/v1"
    },
    {
        "id": 22252,
        "title": "Case Study: Amesbury #5 Substation Power Transformer Emergency Relocation",
        "authors": "Carli Gavin",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10253383"
    },
    {
        "id": 22253,
        "title": "Aspect of Power Transformer Design and Optimization",
        "authors": "Chirag Parekh, Chetan Kotwal, Hiral Upadhyay",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37591/.v7i1.3242"
    },
    {
        "id": 22254,
        "title": "Data Efficient Video Transformer for Violence Detection",
        "authors": "Almamon Rasool Abdali",
        "published": "2021-7-17",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comnetsat53002.2021.9530829"
    },
    {
        "id": 22255,
        "title": "Poincare Cross Section Classification Based on Vision Transformer",
        "authors": "Yan Sun",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmiii58949.2023.00038"
    },
    {
        "id": 22256,
        "title": "Predicting Neighbourhood Wealthiness Based on Street View Images and Swin Transformer",
        "authors": "Yang Qiu, Meiliu Wu, Qunying Huang, Yuhao Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4597219"
    },
    {
        "id": 22257,
        "title": "Transformer Based Punctuation Restoration for Turkish",
        "authors": "Uygar Kurt, Aykut Çayır",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk59864.2023.10286690"
    },
    {
        "id": 22258,
        "title": "Subharmonic ferroresonance mitigation in capacitive voltage transformer using meminductor",
        "authors": "S. Poornima",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00202-023-02229-z"
    },
    {
        "id": 22259,
        "title": "Travailler avec la répétition : l’accueillir, la supporter, la transformer",
        "authors": "Elysé Linde",
        "published": "2017-12-1",
        "citations": 0,
        "abstract": "Les patients hospitalisés dans les institutions de soins vont inévitablement déployer leurs problématiques dans les différentes scènes transférentielles, répétant par des mises en acte la dimension traumatique de leur histoire. L’auteur propose, par l’intermédiaire du travail réalisé auprès d’une patiente régulièrement hospitalisée en institution psychiatrique, de penser les effets de cette répétition. Il soutient l’idée que même si la répétition a souvent pour effet d’immobiliser le travail de pensée, elle offre également la possibilité d’une voie vers un remaniement pulsionnel à la condition qu’elle soit entendue comme l’expression d’un fragment de la vie psychique inaccessible pour le sujet.",
        "link": "http://dx.doi.org/10.3917/clini.014.0122"
    },
    {
        "id": 22260,
        "title": "Jeux de mots",
        "authors": "Vincent Di Rocco",
        "published": "2017-12-4",
        "citations": 0,
        "abstract": "Les travaux de recherche récents sur les médiations thérapeutiques, le processus créateur et le développement psychique de l’enfant soulignent le rôle du jeu dans les processus de transformation psychique et d’appropriation subjective. Cette exploration des fonctions psychiques du jeu permet de dégager progressivement un véritable modèle de compréhension de certains aspects des processus engagés dans les dispositifs thérapeutiques, permettant de soutenir la mise en place d’un fonctionnement associatif dans la clinique des états psychotiques de l’adulte.",
        "link": "http://dx.doi.org/10.3917/psys.174.0225"
    },
    {
        "id": 22261,
        "title": "Schweizerdeutsche Telefonanrufe und Meetings mit GPT (Generative Pre-trained Transformer) automatisieren",
        "authors": "Mark Bosshard",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.38023/927d5adf-652b-4477-8fe9-4ef4bee420ff"
    },
    {
        "id": 22262,
        "title": "Designing of Transformer for DC Flyback Converter",
        "authors": "Soumyarupa Saha, Rajanikanta Sahoo, Molay Roy",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec57805.2023.10384977"
    },
    {
        "id": 22263,
        "title": "Optimizing Transformer for Large-Hole Image Inpainting",
        "authors": "Zixuan Li, Yuan-Gen Wang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222218"
    },
    {
        "id": 22264,
        "title": "Generating Music with Emotion Using Transformer",
        "authors": "M. Aqmal Pangestu, Suyanto Suyanto",
        "published": "2021-11-16",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2se52832.2021.9791928"
    },
    {
        "id": 22265,
        "title": "GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples",
        "authors": "Tian Gao, Cheng-Zhong Xu, Le Zhang, Hui Kong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vision Transformer (ViT) has performed remarkably in various computer vision tasks. Nonetheless, affected by the massive amount of parameters, ViT usually suffers from serious overfitting problems with a relatively limited number of training samples. In addition, ViT generally demands heavy computing resources, which limit its deployment on resource-constrained devices. As a type of model-compression method,  model binarization is potentially a good choice to solve the above problems. Compared with the full-precision one, the model with the binarization method replaces complex tensor multiplication with simple bit-wise binary operations and represents full-precision model parameters and activations with only 1-bit ones, which potentially solves the problem of model size and computational complexity, respectively. In this paper, we investigate a binarized ViT model. Empirically, we observe that the existing binarization technology designed for Convolutional Neural Networks (CNN) cannot migrate well to a ViT's binarization task. We also find that the decline of the accuracy of the binary ViT model is mainly due to the information loss of the \\textbf{Attention} module and the \\textbf{Value} vector. Therefore, we propose a novel model binarization technique, called \\textbf{G}roup \\textbf{S}uperposition \\textbf{B}inarization (\\textbf{GSB}), to deal with these issues. Furthermore, in order to further improve the performance of the binarization model, we have investigated the gradient calculation procedure in the binarization process and derived more proper gradient calculation equations for GSB to reduce the influence of gradient mismatch. Then, the knowledge distillation technique is introduced to alleviate the performance degradation caused by model binarization. Analytically, model binarization can limit the parameter’s search space during parameter updates while training a model. Therefore, the binarization process can actually play an implicit regularization role and help solve the problem of overfitting in the case of insufficient training data. Experiments on three datasets with limited numbers of training samples demonstrate that the proposed GSB model achieves state-of-the-art performance among the binary quantization schemes and exceeds its full-precision counterpart on some indicators. Code and models are available at https://github.com/IMRL/GSB-Vision-Transformer.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22817249.v1"
    },
    {
        "id": 22266,
        "title": "Reinforcement Learning Based Time-Adaptive Power Transformer Differential Protection",
        "authors": "xiaopeng wang, Anyang He, Zongbo Li, Zaibin Jiao, Na Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685791"
    },
    {
        "id": 22267,
        "title": "A compact transformer-based GAN vocoder",
        "authors": "Chenfeng Miao, Ting Chen, Minchuan Chen, Jun Ma, Shaojun Wang, Jing Xiao",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11254"
    },
    {
        "id": 22268,
        "title": "Predicting Binding from Screening Assays with Transformer Network Embeddings",
        "authors": "Paul Morris, Rachel St Clair, Elan Barenholtz, William Edward Hahn",
        "published": "No Date",
        "citations": 0,
        "abstract": "Cheminformatics aims to assist in chemistry applications that depend on molecular interactions, structural characteristics, and functional properties. The arrival of deep learning and the abundance of easily accessible chemical data from repositories like PubChem have enabled advancements in computer-aided drug discovery. Virtual High-Throughput Screening (vHTS) is one such technique that integrates chemical domain knowledge to perform in silico biomolecular simulations, but prediction of binding affinity is restricted due to limited availability of ground-truth binding assay results. Here, text representations of 83,000,000 molecules are leveraged to enable single-target binding affinity prediction directly on the outcome of screening assays. The embedding of an end-to-end Transformer neural network, trained to encode the structural characteristics of a molecule via a text-based translation task, is repurposed through transfer learning to classify binding affinity to a single target. Classifiers trained on the embedding outperform those trained on SMILES strings for multiple tasks, receiving between 0.67-0.99 AUC. Visualization reveals organization of structural and functional properties in the learned embedding useful for binding prediction. The proposed model is suitable for parallel computing, enabling rapid screening as a complement to virtual screening techniques when limited data is available.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.11625885.v1"
    },
    {
        "id": 22269,
        "title": "Knowledge Distillation of Multi-scale Dense Prediction Transformer for Self-supervised Depth Estimation",
        "authors": "Jimin Song, Sang Jun Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDepth estimation is an inverse projection problem that estimates pixel-level distances from a single image. Although, supervised methods have shown promising results, it has intrinsic limitations in requiring ground truth depth from an external sensor. On the other hand, self-supervised depth estimation relieves the burden for collecting calibrated training data, while there is still a large performance gap between supervised and self-supervised methods. The objective of this study is to reduce the performance gap in supervised and self-supervised depth estimation. The loss function of previous self-supervised methods is mainly based on a photometric error, which is indirectly computed from synthesized images using depth and pose estimates. In this paper, we argue that direct depth cue is more effective to train a depth estimation network. To obtain the direct depth cue, we employed a knowledge distillation technique, which is a teacher-student learning framework. The teacher network was trained in a self-supervised manner based on a photometric error, and its predictions were utilized to train a student network. We constructed a multi-scale dense prediction transformer with Monte Carlo dropout, and multi-scale distillation loss was proposed to train the student network based on the ensemble of stochastic estimates. Experiments were conducted on the KITTI and Make3D datasets, and our proposed method achieved the state-of-the-art accuracy in self-supervised depth estimation. Our code is publicly available at https://github.com/ji-min-song/KD-of-MS-DPT.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2937548/v1"
    },
    {
        "id": 22270,
        "title": "Feedback Decision Transformer: Offline Reinforcement Learning With Feedback",
        "authors": "Liad Giladi, Gilad Katz",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00120"
    },
    {
        "id": 22271,
        "title": "An End-to-end Oxford Nanopore Basecaller Using Convolution-augmented Transformer",
        "authors": "Xuan Lv, Zhiguang Chen, Yutong Lu, Yuedong Yang",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractOxford Nanopore sequencing is fastly becoming an active field in genomics, and it’s critical to basecall nucleotide sequences from the complex electrical signals. Many efforts have been devoted to developing new basecalling tools over the years. However, the basecalled reads still suffer from a high error rate and slow speed. Here, we developed an open-source basecalling method, CATCaller, by simultaneously capturing global context through Attention and modeling local dependencies through dynamic convolution. The method was shown to consistently outper-form the ONT default basecaller Albacore, Guppy, and a recently developed attention-based method SACall in read accuracy. More importantly, our method is fast through a heterogeneously computational model to integrate both CPUs and GPUs. When compared to SACall, the method is nearly 4 times faster on a single GPU, and is highly scalable in parallelization with a further speedup of 3.3 on a four-GPU node.",
        "link": "http://dx.doi.org/10.1101/2020.11.09.374165"
    },
    {
        "id": 22272,
        "title": "Multi-task predictions of the Arctic sea ice by a transformer-based deep learning model",
        "authors": "Yibin Ren, Xiaofeng Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "The Arctic sea ice has been retreating dramatically in recent years in summer and fall. The navigation season for open water vessels along the Northeast Passage has lengthened to sub-seasonal scales. Accurate perditions of Arctic sea ice in sub-seasonal scales are essential for planning shipping activities. The numerical model cannot achieve a high accuracy of daily sea ice predictions on a sub-seasonal scale. The advanced deep learning brings new solutions for the data-driven-based sea ice prediction.\nThis study proposed a transformer-based deep learning model to predict multiple sea ice parameters, including sea ice concentration (SIC), sea ice thickness (SIT), and sea ice drift (SID), in the Pan-Arctic in a sub-seasonal scale, 90 days&#8217; lead. An encoder and decoder are constructed based on transformer modules to extract spatio-temporal dependencies from daily SIC, SIT, and SID sequences. The spatio-temporal dependencies at different scales are fused to form the final feature maps. Three SIC, SIT, and SID output modules are designed based on the final feature maps to output different parameters for the next 90 days. The satellite-observed sea ice data from the National Sea Ice Data Center (NSIDC) are employed to train the proposed model. We compared our model with anomaly persistence and the European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble predictions to demonstrate the model&#8217;s prediction skill.\nFurther, based on the proposed model, we discuss the effects of typical thermal and dynamic factors on sub-seasonal scale daily sea ice prediction. The selected factors include surface air temperature (SAT), sea surface temperature (SST), surface solar radiation downwards (SSRD), and geopotential height. Finally, we conclude with some scientific guidelines for the sub-seasonal sea ice predictability of the Arctic.&#160;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-13556"
    },
    {
        "id": 22273,
        "title": "Unsupervised Transformer-Based Anomaly Detection in ECG Signals",
        "authors": "Abrar Alamr, Abdelmonim Artoli",
        "published": "2023-3-9",
        "citations": 10,
        "abstract": "Anomaly detection is one of the basic issues in data processing that addresses different problems in healthcare sensory data. Technology has made it easier to collect large and highly variant time series data; however, complex predictive analysis models are required to ensure consistency and reliability. With the rise in the size and dimensionality of collected data, deep learning techniques, such as autoencoder (AE), recurrent neural networks (RNN), and long short-term memory (LSTM), have gained more attention and are recognized as state-of-the-art anomaly detection techniques. Recently, developments in transformer-based architecture have been proposed as an improved attention-based knowledge representation scheme. We present an unsupervised transformer-based method to evaluate and detect anomalies in electrocardiogram (ECG) signals. The model architecture comprises two parts: an embedding layer and a standard transformer encoder. We introduce, implement, test, and validate our model in two well-known datasets: ECG5000 and MIT-BIH Arrhythmia. Anomalies are detected based on loss function results between real and predicted ECG time series sequences. We found that the use of a transformer encoder as an alternative model for anomaly detection enables better performance in ECG time series data. The suggested model has a remarkable ability to detect anomalies in ECG signal and outperforms deep learning approaches found in the literature on both datasets. In the ECG5000 dataset, the model can detect anomalies with 99% accuracy, 99% F1-score, 99% AUC score, 98.1% recall, and 100% precision. In the MIT-BIH Arrhythmia dataset, the model achieved an accuracy of 89.5%, F1 score of 92.3%, AUC score of 93%, recall of 98.2%, and precision of 87.1%.",
        "link": "http://dx.doi.org/10.3390/a16030152"
    },
    {
        "id": 22274,
        "title": "The Proposed Two-Stage Parameter Design Methodology of a Generalized Resonant DC Transformer in Hybrid AC/DC Microgrid with Optimum Active Power Transmission",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_4"
    },
    {
        "id": 22275,
        "title": "The Proposed Simplified Resonant Parameters Design of the Asymmetrical CLLC-Type DC Transformer in the Renewable Energy System via Semi-Artificial Intelligent Optimal Scheme",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_3"
    },
    {
        "id": 22276,
        "title": "Transformer oil-based magnetic nanofluid with high dielectric losses tested for cooling of a model transformer",
        "authors": "Michal Rajnak, Milan Timko, Peter Kopcansky, Katarina Paulovicova, Jozef Kuchta, Marek Franko, Juraj Kurimsky, Bystrik Dolnik, Roman Cimbala",
        "published": "2019-8",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2019.008047"
    },
    {
        "id": 22277,
        "title": "UATR-MSG-Transformer: A Deep Learning Network for Underwater Acoustic Target Recognition Based on Spectrogram Feature Fusion and Transformer with Messenger Tokens",
        "authors": "Hao Zhou, Xuening Wang, Peishun Liu, Liang Wang, Ruichun Tang",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604078.3604096"
    },
    {
        "id": 22278,
        "title": "Experimental thermal investigation of an ONAN distribution transformer by fiber optic sensors",
        "authors": "Ahmet Yigit Arabul, Fatma Keskin Arabul, Ibrahim Senol",
        "published": "2018-2",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2017.11.007"
    },
    {
        "id": 22279,
        "title": "A Method for Diagnosing Factory Facility Abnormality in CNN-transformer Network using Thermal Imaging",
        "authors": "Dong-Hyun Kim, Ho-Seong Hwang, Ho-Chul Kim",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieie.2023.60.3.53"
    },
    {
        "id": 22280,
        "title": "BD-Transformer: A Transformer-Based Approach for Bipolar Disorder Classification Using Audio",
        "authors": "Mohamed Ramadan, Hazem Abdelkawy,  Mustaqueem, Alice Othmani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-6775-6_27"
    },
    {
        "id": 22281,
        "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
        "authors": "Takaaki Hori, Niko Moritz, Chiori Hori, Jonathan Le Roux",
        "published": "2020-10-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2928"
    },
    {
        "id": 22282,
        "title": "High-temperature coplanar transformer",
        "authors": "Maxime Semard, Christian Martin, Cyril Buttay, Charles Joubert",
        "published": "2018-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2018.8352268"
    },
    {
        "id": 22283,
        "title": "Healthcare Transformer 8: Build Organizational Quality Improvement Capability",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-8"
    },
    {
        "id": 22284,
        "title": "Smart transformer control of the electrical grid",
        "authors": "",
        "published": "2022-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo217e_ch13"
    },
    {
        "id": 22285,
        "title": "Forecasting on solid state transformer applications",
        "authors": "Vijayakrishna Satyamsetti, Andreas Michealides, Antonis Hadjiantonis",
        "published": "2017-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iss1.2017.8389425"
    },
    {
        "id": 22286,
        "title": "Transformer inrush - Harmonics in the current",
        "authors": "Bermann Jiri, Prochazka Martin",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2018.8396014"
    },
    {
        "id": 22287,
        "title": "STCovidNet: Automatic Detection Model of Novel Coronavirus Pneumonia Based on Swin Transformer",
        "authors": "Boyuan Wang, Du Zhang, Zonggui Tian",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nThe novel coronavirus disease 2019 (COVID-19) has emerged as an enormous challenge facing China today. Preventive Medicine physicians and Artificial Intelligence (AI) researchers try to improve the ability to early automatic warning of coronavirus infections, promote epidemic prevention, and reduce medical costs using deep learning methods. In this work, we build an extensive database of chest computed tomography (CT) scans with image data from domestic and international open-source medical datasets. Swin Transformer is chosen as the backbone network to establish a model (STCovidNet) for the prediction of COVID-19. We then compare the performance of our technique against that of Vision Transformer (ViT) and Convolutional Neural Network (CNN). Next, to visualize our model's high-dimensional outputs in 2-dimensional space, we apply t-distributed stochastic neighbor embedding (t-SNE) as the dimension-reduction strategy. Finally, we employ gradient-weighted class activation mapping (Grad-CAM) to present a class activation map. The results indicate that STCovidNet’s performance surpasses ViT and CNN with a 0.9811 AUC and 0.9858 accuracy score. Our network outperforms previous techniques to reduce intra-class variability and generate well-separated feature embedding. The CAM figure illustrates that the decision region corresponds to radiologists' detecting spots. The suggested method can be an effective way of catching COVID-19 instances.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1401026/v1"
    },
    {
        "id": 22288,
        "title": "Thermal modelling of power transformer",
        "authors": "R. Bouhaddiche, S. Bouazabia, I. Fofana",
        "published": "2017-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124676"
    },
    {
        "id": 22289,
        "title": "Prologue",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-003"
    },
    {
        "id": 22290,
        "title": "Power Transformer Fault Diagnosis Based on MPSO-SVM",
        "authors": "Zhiqiang Yang",
        "published": "2020-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5013/ijssst.a.16.02.06"
    },
    {
        "id": 22291,
        "title": "Research on Transformer Relay Protection in 10kV Power Supply System",
        "authors": "",
        "published": "2022-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i1.033"
    },
    {
        "id": 22292,
        "title": "Agriculture : adapter les pratiques, transformer la communication",
        "authors": "André Barlier",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/pes.400.0045"
    },
    {
        "id": 22293,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247878"
    },
    {
        "id": 22294,
        "title": "Data &amp; Channel Efficient Vision Transformer",
        "authors": "Li Changhong",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccet59170.2023.10335121"
    },
    {
        "id": 22295,
        "title": "Illustrations",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-001"
    },
    {
        "id": 22296,
        "title": "Automatic gas detection using seismic data and Transformer neural networks",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22564/17cisbgf2021.058"
    },
    {
        "id": 22297,
        "title": "Transformer-based Environmental Sound Classification Modeling by Jointing Multi-class Classification and Similarity Clustering",
        "authors": "Xing Zhou, Ming Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs environmental sound signal is not as regular as speech, it has varying temporal structures and is more difficult to distinguish. Previous research on Environmental Sound Classiﬁcation (ESC) has designed sophisticated methods to extract feature from raw waveforms directly, but this may not be generalized well across different ESC tasks. We proposed an end-to-end audio scene classification network which is only based on the log-mel feature. First, we used Transformer network to encode signals that can capture crucial temporal information from self-attention. Then we combined Multi-class classifier with similarity clustering so as to maximize the distance between different classes. At last, we visualized the Transformer’s ability to locate important temporal information. The performance on ESC10 and ESC50 showed that our architecture reached an average accuracy of 95.3% and 84.2%, respectively. That was an achievement of new state-of-the-art performance with only log-mel input. Meanwhile, that is nearly equivalent to the best performance of the model based on raw waveform or combined feature method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2158071/v1"
    },
    {
        "id": 22298,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4245289"
    },
    {
        "id": 22299,
        "title": "Contents",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-toc"
    },
    {
        "id": 22300,
        "title": "Detection Fuse Blown of Transformer Using GSM",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2017.06.192"
    },
    {
        "id": 22301,
        "title": "Transformer Model Based Soft Actor-Critic Learning for HEMS",
        "authors": "Ulrich Ludolfinger, Vedran S. Peric, Thomas Hamacher, Sascha Hauke, Maren Martens",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The transition to weather dependent renewable energy generators requires the electric loads to be adjusted to generation. This is made possible by demand response programs and home energy management systems. However, practically easy to use rule-based control systems often miss many optimization potentials. Self-learning alternatives employing reinforcement learning often ignore the partial observability of the building control problem and consequently neglect the importance of the observation history. Adaptive control systems that do consider that history often rely on policies that suffer from catastrophic forgetting, which makes them unable to fully grasp long histories.</p>\n<p>As an alternative, we present a new reinforcement learning method for autonomous building energy management control based on the soft actor-critic method and the transformer deep neural network architecture. For the control of a heat pump and an the inlet port of a thermal storage, under consideration of photovoltaic generations and dynamic electricity prices, we formulate the problem as partially observable and use the history of observations to determine the control signals. We show, based on a validated building simulation, that our method outperforms rule-based as well as reinforcement learning methods that use multi layer perceptrons or recurrent neural networks as policy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23264429"
    },
    {
        "id": 22302,
        "title": "Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer",
        "authors": "Shrutina Agarwal, Naoya Takahashi, Sriram Ganapathy",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11256"
    },
    {
        "id": 22303,
        "title": "St-Gtrans: Spatio-Temporal Graph Transformer with Road Network Semantic Awareness for Traffic Flow Prediction",
        "authors": "Pingping Dong, Xiaoning Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4494656"
    },
    {
        "id": 22304,
        "title": "Transformer ? Déformer ? Bion et / ou Freud ?",
        "authors": "Guy Cabrol",
        "published": "2018-12-17",
        "citations": 0,
        "abstract": "La théorie, issue des propres transformations en l’analyste dans la séance, court toujours le danger d’être un instrument de pouvoir plus que de savoir. Comment favoriser et en tout cas ne pas entraver, les processus de subjectivation et d’auto-organisation, comme la créativité primaire de l’analysant, demeure l’enjeu de l’analyste à chaque séance.",
        "link": "http://dx.doi.org/10.3917/rfp.825.1418"
    },
    {
        "id": 22305,
        "title": "Crowdtrans: Learning Top-Down Visual Perception for Crowd Counting by Transformer",
        "authors": "Weiyu Guo, Shaopeng Yang, Yuheng Ren, Yongzhen Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706202"
    },
    {
        "id": 22306,
        "title": "Empirical Study of Transformer and Graph-Based Architectures for Class-Imbalanced Road Scene Recognition",
        "authors": "Suman Chowdhury, Rajesh  P. Barnwal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4718551"
    },
    {
        "id": 22307,
        "title": "Online Compressive Transformer for End-to-End Speech Recognition",
        "authors": "Chi-Hang Leong, Yu-Han Huang, Jen-Tzung Chien",
        "published": "2021-8-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-545"
    },
    {
        "id": 22308,
        "title": "Identifying Errors in Service Transformer Connections",
        "authors": "Logan Blakely, Matthew J. Reno",
        "published": "2020-8-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9282035"
    },
    {
        "id": 22309,
        "title": "Millimeter-Wave Dual-Frequency Transformer-based Rectifier",
        "authors": "Yun Fang, Hao Gao",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicdt59917.2023.10332409"
    },
    {
        "id": 22310,
        "title": "Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition",
        "authors": "Lucile Gelin, Thomas Pellegrini, Julien Pinquier, Morgane Daniel",
        "published": "2021-8-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-2202"
    },
    {
        "id": 22311,
        "title": "Convolutional Transformer-Based Image Compression",
        "authors": "Bouzid Arezki, Fangchen Feng, Anissa Mokraoui",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/spa59660.2023.10274433"
    },
    {
        "id": 22312,
        "title": "Molecule Generation for Drug Discovery with New Transformer Architecture",
        "authors": "Yu-Bin Hong, Kyung-Jun Lee, DongNyeong Heo, Heeyoul Choi",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4195528"
    },
    {
        "id": 22313,
        "title": "Lightweight Context-aware Feature Transformer Network for Human Pose Estimation",
        "authors": "Yanli Ma, Qingxuan Shi, Fan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "We propose Context-aware Feature Transformer Network (CaFTNet), a novel network for human pose estimation. To address the issue of limited modeling of global dependencies in convolutional neural networks, we design Transformerneck to strengthen the expressive power of features. Transformerneck directly substitutes the 3×3 convolution in bottleneck of HRNet with Contextual Transformer (CoT) block, while reducing the complexity of the network. Specifically, CoT first produces keys with static contextual information through 3×3 convolution. Then, relying on the query and contextualization keys, the dynamic contexts are generated through two concatenated 1×1 convolutions. Static and dynamic contexts are eventually fused as an output. Additionally, for the multi-scale networks, in order to further refine the features of the fusion output, we propose an Attention Feature Aggregation Module(AFAM). Technically, given an intermediate input, AFAM successively deduces attention maps along channel and spatial dimensions. Then, Adaptive refinement module(ARM) is exploited to activate the obtained attention maps. Finally, the input undergoes adaptive feature refinement through multiplication with the activated attention maps. Through the above studies, our lightweight network provides a powerful clue for detection of keypoints. Experiments are implemented on the COCO and MPII datasets. The model achieves 76.2 AP on the COCO val2017. Compared to other methods with the CNN as the backbone, CaFTNet reduces the number of parameters by 72.9 %. On the MPII, our method uses only 60.7% of the number of parameters, acquiring semblable results to other methods with the CNN as the backbone.",
        "link": "http://dx.doi.org/10.20944/preprints202401.0836.v1"
    },
    {
        "id": 22314,
        "title": "SAFIT: Segmentation-Aware Scene Flow with Improved Transformer",
        "authors": "Yukang Shi, Kaisheng Ma",
        "published": "2022-5-23",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811747"
    },
    {
        "id": 22315,
        "title": "Debunking Rumors on Twitter with Tree Transformer",
        "authors": "Jing Ma, Wei Gao",
        "published": "2020",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.476"
    },
    {
        "id": 22316,
        "title": "Research on Structure Loss Separation of Power Transformer",
        "authors": "Cui Xiao, Chen Dezhi, Bai Baodong",
        "published": "2021-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intermag42984.2021.9579631"
    },
    {
        "id": 22317,
        "title": "Untied Positional Encodings for Efficient Transformer-Based Speech Recognition",
        "authors": "Lahiru Samarakoon, Ivan Fung",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10023097"
    },
    {
        "id": 22318,
        "title": "Weighted Transformer for Dialect Speech Recognition",
        "authors": "Minghan Zhang, Fei Xie, Fuliang Weng",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ickg55886.2022.00055"
    },
    {
        "id": 22319,
        "title": "Utranspa: Transformer-Based Network for Sparsely Viewed Photoacoustic Tomography",
        "authors": "Zhengyan He, Qiuping Liu, Yuehua Ye, yuan zhao, Tianqi Shan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4751751"
    },
    {
        "id": 22320,
        "title": "The Proposed Two-Stage Parameter Design Methodology of a Generalized Resonant DC Transformer in Hybrid AC/DC Microgrid with Optimum Active Power Transmission",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_4"
    },
    {
        "id": 22321,
        "title": "The Proposed Simplified Resonant Parameters Design of the Asymmetrical CLLC-Type DC Transformer in the Renewable Energy System via Semi-Artificial Intelligent Optimal Scheme",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_3"
    },
    {
        "id": 22322,
        "title": "Transformer oil-based magnetic nanofluid with high dielectric losses tested for cooling of a model transformer",
        "authors": "Michal Rajnak, Milan Timko, Peter Kopcansky, Katarina Paulovicova, Jozef Kuchta, Marek Franko, Juraj Kurimsky, Bystrik Dolnik, Roman Cimbala",
        "published": "2019-8",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2019.008047"
    },
    {
        "id": 22323,
        "title": "UATR-MSG-Transformer: A Deep Learning Network for Underwater Acoustic Target Recognition Based on Spectrogram Feature Fusion and Transformer with Messenger Tokens",
        "authors": "Hao Zhou, Xuening Wang, Peishun Liu, Liang Wang, Ruichun Tang",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604078.3604096"
    },
    {
        "id": 22324,
        "title": "Experimental thermal investigation of an ONAN distribution transformer by fiber optic sensors",
        "authors": "Ahmet Yigit Arabul, Fatma Keskin Arabul, Ibrahim Senol",
        "published": "2018-2",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2017.11.007"
    },
    {
        "id": 22325,
        "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
        "authors": "Takaaki Hori, Niko Moritz, Chiori Hori, Jonathan Le Roux",
        "published": "2020-10-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2928"
    },
    {
        "id": 22326,
        "title": "High-temperature coplanar transformer",
        "authors": "Maxime Semard, Christian Martin, Cyril Buttay, Charles Joubert",
        "published": "2018-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2018.8352268"
    },
    {
        "id": 22327,
        "title": "Healthcare Transformer 8: Build Organizational Quality Improvement Capability",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-8"
    },
    {
        "id": 22328,
        "title": "Smart transformer control of the electrical grid",
        "authors": "",
        "published": "2022-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo217e_ch13"
    },
    {
        "id": 22329,
        "title": "Forecasting on solid state transformer applications",
        "authors": "Vijayakrishna Satyamsetti, Andreas Michealides, Antonis Hadjiantonis",
        "published": "2017-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iss1.2017.8389425"
    },
    {
        "id": 22330,
        "title": "Transformer inrush - Harmonics in the current",
        "authors": "Bermann Jiri, Prochazka Martin",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2018.8396014"
    },
    {
        "id": 22331,
        "title": "STCovidNet: Automatic Detection Model of Novel Coronavirus Pneumonia Based on Swin Transformer",
        "authors": "Boyuan Wang, Du Zhang, Zonggui Tian",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nThe novel coronavirus disease 2019 (COVID-19) has emerged as an enormous challenge facing China today. Preventive Medicine physicians and Artificial Intelligence (AI) researchers try to improve the ability to early automatic warning of coronavirus infections, promote epidemic prevention, and reduce medical costs using deep learning methods. In this work, we build an extensive database of chest computed tomography (CT) scans with image data from domestic and international open-source medical datasets. Swin Transformer is chosen as the backbone network to establish a model (STCovidNet) for the prediction of COVID-19. We then compare the performance of our technique against that of Vision Transformer (ViT) and Convolutional Neural Network (CNN). Next, to visualize our model's high-dimensional outputs in 2-dimensional space, we apply t-distributed stochastic neighbor embedding (t-SNE) as the dimension-reduction strategy. Finally, we employ gradient-weighted class activation mapping (Grad-CAM) to present a class activation map. The results indicate that STCovidNet’s performance surpasses ViT and CNN with a 0.9811 AUC and 0.9858 accuracy score. Our network outperforms previous techniques to reduce intra-class variability and generate well-separated feature embedding. The CAM figure illustrates that the decision region corresponds to radiologists' detecting spots. The suggested method can be an effective way of catching COVID-19 instances.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1401026/v1"
    },
    {
        "id": 22332,
        "title": "Thermal modelling of power transformer",
        "authors": "R. Bouhaddiche, S. Bouazabia, I. Fofana",
        "published": "2017-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124676"
    },
    {
        "id": 22333,
        "title": "Prologue",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-003"
    },
    {
        "id": 22334,
        "title": "Power Transformer Fault Diagnosis Based on MPSO-SVM",
        "authors": "Zhiqiang Yang",
        "published": "2020-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5013/ijssst.a.16.02.06"
    },
    {
        "id": 22335,
        "title": "Research on Transformer Relay Protection in 10kV Power Supply System",
        "authors": "",
        "published": "2022-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i1.033"
    },
    {
        "id": 22336,
        "title": "Agriculture : adapter les pratiques, transformer la communication",
        "authors": "André Barlier",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/pes.400.0045"
    },
    {
        "id": 22337,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247878"
    },
    {
        "id": 22338,
        "title": "Data &amp; Channel Efficient Vision Transformer",
        "authors": "Li Changhong",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccet59170.2023.10335121"
    },
    {
        "id": 22339,
        "title": "Illustrations",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-001"
    },
    {
        "id": 22340,
        "title": "Automatic gas detection using seismic data and Transformer neural networks",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22564/17cisbgf2021.058"
    },
    {
        "id": 22341,
        "title": "Transformer-based Environmental Sound Classification Modeling by Jointing Multi-class Classification and Similarity Clustering",
        "authors": "Xing Zhou, Ming Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs environmental sound signal is not as regular as speech, it has varying temporal structures and is more difficult to distinguish. Previous research on Environmental Sound Classiﬁcation (ESC) has designed sophisticated methods to extract feature from raw waveforms directly, but this may not be generalized well across different ESC tasks. We proposed an end-to-end audio scene classification network which is only based on the log-mel feature. First, we used Transformer network to encode signals that can capture crucial temporal information from self-attention. Then we combined Multi-class classifier with similarity clustering so as to maximize the distance between different classes. At last, we visualized the Transformer’s ability to locate important temporal information. The performance on ESC10 and ESC50 showed that our architecture reached an average accuracy of 95.3% and 84.2%, respectively. That was an achievement of new state-of-the-art performance with only log-mel input. Meanwhile, that is nearly equivalent to the best performance of the model based on raw waveform or combined feature method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2158071/v1"
    },
    {
        "id": 22342,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4245289"
    },
    {
        "id": 22343,
        "title": "Contents",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-toc"
    },
    {
        "id": 22344,
        "title": "Detection Fuse Blown of Transformer Using GSM",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2017.06.192"
    },
    {
        "id": 22345,
        "title": "A research on submersible transformer",
        "authors": "Chunbao Liu",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2638582"
    },
    {
        "id": 22346,
        "title": "CUNI Transformer Neural MT System for WMT18",
        "authors": "Martin Popel",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6424"
    },
    {
        "id": 22347,
        "title": "UNDERSTANDING THE LOCATION OF HOT SPOTS IN TRANSFORMER WINDINGS",
        "authors": "Tor Laneryd, Andreas Gustafsson, Sead Travancic",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1615/ihtc16.ctm.023498"
    },
    {
        "id": 22348,
        "title": "Transformer model: Explainability and prospectiveness",
        "authors": "Gengpan Nuobu",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The purpose of Artificial Intelligence(AI) is to simulate learning process of human brain by strong computing power and appropriate algorithm, so that the machine can develop judging ability at work as human. Current AI mainly relies on Deep Learning model which is based on artificial neural network, like Convolutional Neural Network(CNN) in computer visualization, but that also takes with some defects. This paper introduces defects of CNN and discusses Transformer model in solving unexplainability of traditional CNN algorithm. To discuss why the Transformer model and attention mechanism are considered as the way to AI intelligibility.",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231079"
    },
    {
        "id": 22349,
        "title": "Los programas generativos “Transformer” AI, entre los que está ChatGPT, ¿una oportunidad para la evaluación formativa?",
        "authors": "Miguel Zapata-Ros",
        "published": "No Date",
        "citations": 2,
        "abstract": "Cada vez hay más evidencia de que las improvisaciones estadísticas de programas como ChatGPT producen textos aparentemente creíbles pero que no tienen ninguna sustentación. El problema no es tanto que permitan el fraude en proyectos y artículos, como que hagan pasar por correctos lo que sólo son generación de textos \"sin alma\", sin beingness, o simplemente escritos sin validar teórica o empíricamente, que se aceptan como ciertos acríticamente.El aprendizaje no es dar cuenta sólo mediante exámenes y proyectos, de datos o de textos describiendo conceptos o hechos. Supone según los clásicos (Gagne, Merrill, Reigeluth) comprender, atribuir sentido, aplicar autónomamente lo aprendido y transferirlo a entornos distintos y cambiantes.Para eso tiene que haber procesos de interacción y de retroalimentación. Y, si se utilizan proyectos, deben, en la evaluación, ser supervisados en su realización, en la comprensión y en la atribución de sentido que el prendiz hace.Toda esta constelación de ideas, métodos y prácticas constituye, en el contexto del diseño instruccional, la evaluación formativa.La evaluación formativa (formative assesment) es evaluación continua + feedback. En ese sentido debería haber actividades de evaluación continua que ofrezcan a los estudiantes comentarios formativos sobre su progreso o para ayudarles a autoevaluar el desarrollo y la calidad del diseñoLa evaluación para el aprendizaje (comúnmente conocida como evaluación formativa se centra en la retroalimentación tanto para profesores como para estudiantes.En un mundo donde la IA aprende y nos supera en una lista creciente de dominios cognitivos, *Beingness* es el dominio final del esfuerzo intelectual humano y debe ser el núcleo de la educación. Y la evaluación formativa debe ser un instrumento para ello.",
        "link": "http://dx.doi.org/10.35542/osf.io/k2eps"
    },
    {
        "id": 22350,
        "title": "Deep Transformer Network for Hyperspectral Image Classification",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2021.040703"
    },
    {
        "id": 22351,
        "title": "Cascading Transformer Failure Probability Model Under Geomagnetic Disturbances",
        "authors": "Pratishtha Shukla, James Nutaro, Srikanth Yoginath",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408098"
    },
    {
        "id": 22352,
        "title": "Calibration of Instrument Current Transformer Test Sets",
        "authors": "Karel Draxler, Jan Hlavacek, Renata Styblikova",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ae.2019.8866993"
    },
    {
        "id": 22353,
        "title": "Se transformer de voyeur en voyant",
        "authors": "Jean-Jacques Lebel, Anne Coppel",
        "published": "2017-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/chime.091.0243"
    },
    {
        "id": 22354,
        "title": "Multilingual Cyber Abuse Detection using Advanced Transformer Architecture",
        "authors": "Aditya Malte, Pratik Ratadiya",
        "published": "2019-10",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929493"
    },
    {
        "id": 22355,
        "title": "Cnn-Transformer Blend Pyramid Network for Underwater Image",
        "authors": "Shibai Yin, Dongyang Hong, Yibin Wang, Weixing Wang, Yee-hong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4564021"
    },
    {
        "id": 22356,
        "title": "GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples",
        "authors": "Tian Gao, Cheng-Zhong Xu, Le Zhang, Hui Kong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vision Transformer (ViT) has performed remarkably in various computer vision tasks. Nonetheless, affected by the massive amount of parameters, ViT usually suffers from serious overfitting problems with a relatively limited number of training samples. In addition, ViT generally demands heavy computing resources, which limit its deployment on resource-constrained devices. As a type of model-compression method,  model binarization is potentially a good choice to solve the above problems. Compared with the full-precision one, the model with the binarization method replaces complex tensor multiplication with simple bit-wise binary operations and represents full-precision model parameters and activations with only 1-bit ones, which potentially solves the problem of model size and computational complexity, respectively. In this paper, we investigate a binarized ViT model. Empirically, we observe that the existing binarization technology designed for Convolutional Neural Networks (CNN) cannot migrate well to a ViT's binarization task. We also find that the decline of the accuracy of the binary ViT model is mainly due to the information loss of the \\textbf{Attention} module and the \\textbf{Value} vector. Therefore, we propose a novel model binarization technique, called \\textbf{G}roup \\textbf{S}uperposition \\textbf{B}inarization (\\textbf{GSB}), to deal with these issues. Furthermore, in order to further improve the performance of the binarization model, we have investigated the gradient calculation procedure in the binarization process and derived more proper gradient calculation equations for GSB to reduce the influence of gradient mismatch. Then, the knowledge distillation technique is introduced to alleviate the performance degradation caused by model binarization. Analytically, model binarization can limit the parameter’s search space during parameter updates while training a model. Therefore, the binarization process can actually play an implicit regularization role and help solve the problem of overfitting in the case of insufficient training data. Experiments on three datasets with limited numbers of training samples demonstrate that the proposed GSB model achieves state-of-the-art performance among the binary quantization schemes and exceeds its full-precision counterpart on some indicators. Code and models are available at https://github.com/IMRL/GSB-Vision-Transformer.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22817249"
    },
    {
        "id": 22357,
        "title": "Transpose: 6d Object Pose Estimation with Geometry-Aware Transformer",
        "authors": "Xiao Lin, Deming Wang, Guangliang Zhou, Chengju Liu, Qijun Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670638"
    },
    {
        "id": 22358,
        "title": "MetaLLM: Residue-wise Metal ion Prediction Using Deep Transformer Model",
        "authors": "Fairuz Shadmani Shishir, Bishnu Sarker, Farzana Rahman, Sumaiya Shomaji",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProteins bind to metals such as copper, zinc, magnesium, etc., serving various purposes such as importing, exporting, or transporting metal in other parts of the cell as ligands and maintaining stable protein structure to function properly. A metal binding site indicates the single amino acid position where a protein binds a metal ion. Manually identifying metal binding sites is expensive, laborious, and time-consuming. A tiny fraction of the millions of proteins in UniProtKB – the most comprehensive protein database – are annotated with metal binding sites, leaving many millions of proteins waiting for metal binding site annotation. Developing a computational pipeline is thus essential to keep pace with the growing number of proteins. A significant shortcoming of the existing computational methods is the consideration of the long-term dependency of the residues. Other weaknesses include low accuracy, absence of positional information, hand-engineered features, and a pre-determined set of residues and metal ions. In this paper, we propose MetaLLM, a metal binding site prediction technique, by leveraging the recent progress in self-supervised attention-based (e.g. Transformer) large language models (LLMs) and a considerable amount of protein sequences publicly available. LLMs are capable of modelling long residual dependency in a sequence. The proposed MetaLLM uses a transformer pre-trained on an extensive database of protein sequences and later fine-tuned on metal-binding proteins for multi-label metal ions prediction. A 10-fold cross-validation shows more than 90% precision for the most prevalent metal ions.",
        "link": "http://dx.doi.org/10.1101/2023.03.20.533488"
    },
    {
        "id": 22359,
        "title": "Multiple Plant Tracking Using a Vision Transformer for Image Matching as a Spatial Association Strategy",
        "authors": "Byron Hernandez, Henry Medeiros",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693600"
    },
    {
        "id": 22360,
        "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
        "authors": "Markéta Řezáčková, Jan Švec, Daniel Tihelka",
        "published": "2021-8-30",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-546"
    },
    {
        "id": 22361,
        "title": "Better Sign Language Translation with STMC-Transformer",
        "authors": "Kayo Yin, Jesse Read",
        "published": "2020",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.525"
    },
    {
        "id": 22362,
        "title": "SyntaLinker: Automatic Fragment Linking with Deep Conditional Transformer Neural Networks",
        "authors": "Yuyao Yang, Shuangjia Zheng, Shimin Su, Jun Xu, Hongming Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Fragment\nbased drug design represents a promising drug discovery paradigm complimentary\nto the traditional HTS based lead generation strategy. How to link fragment\nstructures to increase compound affinity is remaining a challenge task in this\nparadigm. Hereby a novel deep generative model (SyntaLinker) for linking fragments\nis developed with the potential for applying in the fragment-based lead\ngeneration scenario. The state-of-the-art transformer architecture was employed\nto learn the linker grammar and generate novel linker. Our results show that,\ngiven starting fragments and user customized linker constraints, our SyntaLinker model can design abundant drug-like molecules fulfilling these constraints and\nits performance was superior to other reference models. Moreover, several\nexamples were showcased that SyntaLinkercan be useful tools for carrying out\ndrug design tasks such as fragment linking, lead optimization and scaffold\nhopping.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12271508"
    },
    {
        "id": 22363,
        "title": "Aggregation Transformer for Human Pose Estimation",
        "authors": "Hao Dong, Guodong Wang, Xinyue Zhang",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956315"
    },
    {
        "id": 22364,
        "title": "Research advances of bubbles dynamics in power transformer",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.04130"
    },
    {
        "id": 22365,
        "title": "GENERATIVE PRE-TRAINED TRANSFORMER 3",
        "authors": "Олександр Іванович ГОЛУБЕНКО, Олександр Олександрович ПІДМОГИЛЬНИЙ",
        "published": "2022-12-30",
        "citations": 0,
        "abstract": "GPT (Generative Pre-training Transformer) — це тип штучного інтелекту (AI), який використовує алгоритми машинного навчання для створення тексту природною мовою. Перша версія GPT, випущена в 2018 році, стала революційним досягненням у сфері ШІ та обробки природної мови (NLP). Однак він також мав деякі обмеження та проблеми, які були розглянуті в наступних версіях моделі.\r\nОднією з головних проблем першої версії GPT була відсутність контролю над контентом, який вона генерувала. Модель було навчено на великому наборі даних тексту, створеного людиною, і вона змогла створити зв’язний і, здавалося б, людиноподібний текст на широкий спектр тем. Однак він часто створював текст, який був упередженим, образливим або іншим чином недоречним, оскільки він не міг повністю зрозуміти контекст або значення використаних слів.\r\nІншою проблемою першої версії GPT була її нездатність виконувати складніші завдання NLP, такі як переклад або конспектування. Хоча він міг створити зв’язний текст, він не міг зрозуміти значення чи структуру тексту так, як це може зробити людина.\r\nПодальші версії GPT, такі як GPT-2 і GPT-3, вирішували ці проблеми та додавали нові можливості, такі як здатність виконувати складніші завдання NLP і генерувати більш зв’язний і відповідний контексту текст. Однак вони все ще мають обмеження і можуть давати необ’єктивні або невідповідні результати, якщо не використовувати їх відповідально.",
        "link": "http://dx.doi.org/10.53920/its-2022-2-2"
    },
    {
        "id": 22366,
        "title": "Noise and Vibration Analysis of a Distribution Transformer",
        "authors": "Daniel Marcsa",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ptze.2019.8781735"
    },
    {
        "id": 22367,
        "title": "Transformer Health Monitoring System Using Android Device",
        "authors": "K Suja, T Yuvaraj",
        "published": "2021-2-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icees51510.2021.9383679"
    },
    {
        "id": 22368,
        "title": "A Transformer model for predicting near-surface temperature and relative humidity",
        "authors": "Emy Alerskans, Joachim Nyborg, Morten Birk, Eigil Kaas",
        "published": "No Date",
        "citations": 0,
        "abstract": "&lt;p&gt;It is a well-known fact that numerical weather prediction (NWP) models exhibit systematic errors, especially for near-surface variables. Reasons for this are, among other, the inability of these models to successfully handle sub-grid phenomena and shortcomings in the physical formulation of the model dynamics. Even though high-resolution regional NWP models usually have a spatial resolution of a few kilometers (or even finer) they generally exhibit local biases due to unresolved topography and obstacles. In order to obtain more local and site-specific forecasts post-processing methods can be used. Here, we have implemented a Transformer Neural Network model for post-processing 48-hour forecasts of 2 m temperature and relative humidity. The observational data used in this study consist of observations of 2 m air temperature and relative humidity from a network of private weather stations (PWSs). All in all, data from more than 1,000 locations are used. Forecast data from the Global Forecast System (GFS) model &amp;#8211; such as temperature, relative humidity, wind speed and direction, radiation fluxes and upper level model fields &amp;#8211; are also used as input to the model. The model is trained on 1.5 years of observational and forecast data and the performance is evaluated using an independent validation dataset of PWSs. We find that the Transformer post-processing model reduces the bias and standard deviation compared to the raw NWP forecast for a majority of stations. Furthermore, the model is validated on completely independent data from the Danish Meteorological Institute&amp;#8217;s (DMI&amp;#8217;s) observational network, where good results were obtained. Overall, the Transformer model produces forecasts that better match the locally observed weather.&lt;/p&gt;",
        "link": "http://dx.doi.org/10.5194/ems2021-198"
    },
    {
        "id": 22369,
        "title": "Identifying Promising Sequences For Protein Engineering Using A Deep Transformer Protein Language Model",
        "authors": "Trevor S. Frisby, Christopher James Langmead",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTProtein engineers aim to discover and design novel sequences with targeted, desirable properties. Given the near limitless size of the protein sequence landscape, it is no surprise that these desirable sequences are often a relative rarity. This makes identifying such sequences a costly and time-consuming endeavor. In this work, we show how to use a deep Transformer Protein Language Model to identify sequences that have the mostpromise. Specifically, we use the model’s self-attention map to calculate a PROMISE SCORE that weights the relative importance of a given sequence according to predicted interactions with a specified binding partner. This PROMISE SCORE can then be used to identify strong binders worthy of further study and experimentation. We use the PROMISE SCORE within two protein engineering contexts— Nanobody (Nb) discovery and protein optimization. With Nb discovery, we show how the PROMISE SCORE provides an effective way to select lead sequences from Nb repertoires. With protein optimization, we show how to use the PROMISE SCORE to select site-specific mutagenesis experiments that identify a high percentage of improved sequences. In both cases, we also show how the self-attention map used to calculate the PROMISE SCORE can indicate which regions of a protein are involved in intermolecular interactions that drive the targeted property. Finally, we describe how to fine-tune the Transformer Protein Language Model to learn a predictive model for the targeted property, and discuss the capabilities and limitations of fine-tuning with and without knowledge transfer within the context of protein engineering.",
        "link": "http://dx.doi.org/10.1101/2023.02.15.528697"
    },
    {
        "id": 22370,
        "title": "Improved Thermal Management in Dry Transformer",
        "authors": "Sandeep Dhar, Grupesh Tapiawala",
        "published": "2019-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i-pact44901.2019.8960112"
    },
    {
        "id": 22371,
        "title": "GPA-TUNet: Transformer and GPA Attention Co-Encoder for Medical Image Segmentation",
        "authors": "Chaoqun Li, Liejun Wang, Yongming Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nU-Net has become baseline standard in the medical image segmentation tasks, but it has limitations in explicitly modeling long-term dependencies. Transformer has the ability to capture long-term relevance through its internal self-attention. However, Transformer is committed to modeling the correlation of all elements, but its awareness of local foreground information is not significant. Since medical images are often presented as regional blocks, local information is equally important. In this paper, we propose the GPA-TUNet by considering local and global information synthetically. Specifically, we propose a new attention mechanism to highlight local foreground information, called group parallel axial attention (GPA). Furthermore, we effectively combine GPA with Transformer in encoder part of model. It can not only highlight the foreground information of sample, but also reduce the negative influence of background information on the segmentation results. Meanwhile, we introduce the sMLP block to improve the global modeling capability of network. Sparse connectivity and weight sharing are well achieved by applying it. Extensive experiments on public datasets confirm the excellent performance of our proposed GPA-TUNet. In particular, on Synapse and ACDC datasets, mean DSC reached 80.37% and 90.37% respectively, mean HD95 reached 20.55% and 1.23% respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1607219/v1"
    },
    {
        "id": 22372,
        "title": "Acknowledgments",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-002"
    },
    {
        "id": 22373,
        "title": "Digitalisation of Power Transformer Monitoring System",
        "authors": "Anjan Mitra, Mousam Dutta, Arpan Pramanick",
        "published": "2022-7-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indiscon54605.2022.9862843"
    },
    {
        "id": 22374,
        "title": "IoT Based Smart Transformer Oil level monitoring and purity check",
        "authors": "",
        "published": "2020-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37896/jxu14.7/029"
    },
    {
        "id": 22375,
        "title": "The Construction Method of Fan Box Transformer Foundation is Discussed",
        "authors": "",
        "published": "2021-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i12.318"
    },
    {
        "id": 22376,
        "title": "Innovative Materials for Transformer of the Future",
        "authors": "Remya Krishnan, K. R. M. Nair",
        "published": "2021-3-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieecon51072.2021.9440238"
    },
    {
        "id": 22377,
        "title": "Implements of Transformer in NLP and DKT",
        "authors": "Haotong Gong",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiam57466.2022.00163"
    },
    {
        "id": 22378,
        "title": "Generating Synthetic Population Using Transformer Based Networks",
        "authors": "Phattranit Phattharajiranan, Veera Muangsin",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsec59635.2023.10329722"
    },
    {
        "id": 22379,
        "title": "Conclusions and Future Research",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch10"
    },
    {
        "id": 22380,
        "title": "Acoustic Imaging based Covariance Matrix Fitting Algorithm for Transformer Fault Diagnosis",
        "authors": "yue Feng, Yongliang Qian, Kunlun Wang, Zhixian Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "With the vigorous development of power system, fault diagnosis of power\ntransformers is very important to ensure the normal operation of the\npower system. However, the traditional transformer monitoring methods\nhave blind spots, and the diagnosis results depend heavily on the\nexperience of the managers. At present, it is urgent to develop new\ntechnologies to improve the accuracy of transformer fault diagnosis. How\nto diagnose transformer faults quickly, accurately, and effectively has\nbecome a difficult problem. Encouragingly, acoustic imaging, as a visual\ntechnology of sound field, promotes the development of transformer fault\ndiagnosis. In this paper, the principles and technical status of\nacoustic imaging are summarized. Meanwhile, the covariance matrix\nfitting (CMF) beamforming algorithm is compared with the traditional\nbeamforming algorithm, and the main factors affecting the performance of\nthe array beamforming algorithm are simulated and analyzed. Finally, an\nacoustic imaging technology for transformer fault diagnosis based on CMF\nalgorithm is proposed. This technology can accurately diagnose the\ntransformer fault state according to the distribution of the internal\nsound field of the transformer, improve the efficiency of fault\ndiagnosis, and promote the construction of smart grid. Through the\nanalysis and processing of the acoustic test data of seven power\ntransformers with different voltage levels and loads, including 500kV\nYanshan, 220kV Tinghu and Laoshan transformers in Wenshan Power Supply\nBureau, the test results prove that the acoustic imaging technology\nbased on CMF algorithm can accurately and conveniently diagnose\ntransformer faults.",
        "link": "http://dx.doi.org/10.22541/au.167948830.08070947/v1"
    },
    {
        "id": 22381,
        "title": "Crowdtrans: Learning Top-Down Visual Perception for Crowd Counting by Transformer",
        "authors": "Weiyu Guo, Shaopeng Yang, Yuheng Ren, Yongzhen Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4419853"
    },
    {
        "id": 22382,
        "title": "Comment on “Transformer impedance reflection demonstration”",
        "authors": "Leonid Minkin",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1119/5.0141878"
    },
    {
        "id": 22383,
        "title": "Brightformer: A Transformer to Brighten the Image",
        "authors": "Yong Wang, Bo Li, Xinlin Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4196724"
    },
    {
        "id": 22384,
        "title": "Les systèmes alimentaires locaux ou comment transformer les villes par l’alimentation ?",
        "authors": "Béatrice Lefebvre",
        "published": "2022-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/vertigo.37038"
    },
    {
        "id": 22385,
        "title": "Carbohydrate Transformer: Predicting Regio- and Stereoselective Reactions Using Transfer Learning",
        "authors": "Giorgio Pesciullesi, Philippe Schwaller, Teodoro Laino, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "Organic chemistry is central to society because it enables the synthesis of complex molecules and materials used in all fields of science and technology. The synthetic methods represent a vast body of accumulated knowledge optimally suited for deep learning. Indeed, most organic reactions involve distinct functional groups and can readily be learned by deep learning models and chemists alike. The task is, however, much more challenging for regio- and stereoselective transformations because their outcome also depends on functional group surroundings in subtle ways. Here, we challenge the Molecular Transformer model to predict reactions in carbohydrate chemistry, a field of central importance in the life sciences and for vaccine development and where regio- and stereoselectivity are notoriously difficult to predict even for experienced chemists. We show that transfer learning of the general USPTO model with a small set of carbohydrate reactions produces a specialized Carbohydrate Transformer model, returning predictions for carbohydrate reactions with remarkable accuracy. We validate these predictions experimentally with a previously unpublished synthesis of a lipid-linked oligosaccharide, involving regioselective protecting group operations and stereoselective glycosylations that are typical for complex carbohydrate synthesis. The chemical reaction transfer learning methods presented in this work are generally applicable to any reaction class of interest.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.11935635.v1"
    },
    {
        "id": 22386,
        "title": "Cross-Attention Multi-Scale Spatial Temporal Transformer for Skeleton-based Action Recognition",
        "authors": "Zhehuang Lin, Yichang Gao, Dong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn recent years, TransFormer has made remarkable achievements in a variety of tasks in computer vision. However, the Transformer-based methods have limitations in learning multi-scale features of skeleton data, while the multi-scale spatial temporal features contain potential both global and local information, which is crucial for skeleton-based action recognition.In this work, we explore the multi-scale feature representation of skeleton sequence in both the spatial and temporal dimensions, and propose an efficient cross-attention mechanism for cross-scale feature fusion. Moreover, we propose a Multi-scale Feature Extraction and Fusion Transformer (MFEF-Former) , which can be divided into two types: (1) MFEF-SFormer for spatial modeling, which captures the inter-joint and inter-part correlations with self-attention, then performs multi-scale spatial feature fusion with cross-attention to model the correlations between joints and body parts. (2) MFEF-TFormer for temporal modeling, which captures the multi-scale temporal feature with self-attention and fuses the multi-scale feature with cross-attention. These two components are combined in a two-stream network, which is evaluated on two large-scale datasets, NTU RGB+D and NTU RGB+D 120. The experiments show that our proposed method outperforms other Transformer-based methods on skeleton-based action recognition and achieves state-of-the-art performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3368402/v1"
    },
    {
        "id": 22387,
        "title": "Transformer Design",
        "authors": "",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119674658.ch7"
    },
    {
        "id": 22388,
        "title": "Time-domain Transformer-based Audiovisual Speaker Separation",
        "authors": "Vahid Ahmadi Kalkhorani, Anurag Kumar, Ke Tan, Buye Xu, DeLiang Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2098"
    },
    {
        "id": 22389,
        "title": "Self-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition",
        "authors": "Mohan Li, Rama Sanand Doddipatla, Catalin Zorila",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10682"
    },
    {
        "id": 22390,
        "title": "Fourier Image Transformer",
        "authors": "Tim-Oliver Buchholz, Florian Jug",
        "published": "2022-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw56347.2022.00201"
    },
    {
        "id": 22391,
        "title": "Conductors for power transformer windings",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo237e_ch3"
    },
    {
        "id": 22392,
        "title": "End-to-End Neural Diarization: From Transformer to Conformer",
        "authors": "Yi Chieh Liu, Eunjung Han, Chul Lee, Andreas Stolcke",
        "published": "2021-8-30",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1909"
    },
    {
        "id": 22393,
        "title": "Efficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes",
        "authors": "Danilo de Oliveira, Tal Peer, Timo Gerkmann",
        "published": "2022-9-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10781"
    },
    {
        "id": 22394,
        "title": "Explaining Cyberbullying Trait Detection Through High Accuracy Transformer Ensemble",
        "authors": "Bruce Goldfeder, Igor Griva",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00116"
    },
    {
        "id": 22395,
        "title": "基于双分支U形Transformer的遥感图像融合",
        "authors": "范文盛 FAN Wensheng, 刘帆 LIU Fan, 李明 LI Ming",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/gzxb20235204.0428002"
    },
    {
        "id": 22396,
        "title": "RoMP-transformer: Rotational bounding box with multi-level feature pyramid transformer for object detection",
        "authors": "Joonhyeok Moon, Munsu Jeon, Siheon Jeong, Ki-Yong Oh",
        "published": "2024-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.110067"
    },
    {
        "id": 22397,
        "title": "Transformer-based deep learning models for adsorption capacity prediction of heavy metal ions toward biochar-based adsorbents",
        "authors": "Zeeshan Haider Jaffari, Ather Abbas, Chang-Min Kim, Jaegwan Shin, Jinwoo Kwak, Changgil Son, Yong-Gu Lee, Sangwon Kim, Kangmin Chon, Kyung Hwa Cho",
        "published": "2024-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jhazmat.2023.132773"
    },
    {
        "id": 22398,
        "title": "The Electrotonic State in a Power Transformer",
        "authors": "M. Shakirov",
        "published": "2020-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271587"
    },
    {
        "id": 22399,
        "title": "On-line gas monitoring for increased transformer protection",
        "authors": "Mickel Saad, Ed teNyenhuis",
        "published": "2017-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epec.2017.8286169"
    },
    {
        "id": 22400,
        "title": "Decision letter for \"Stochastic investigation for solid‐state transformer integration in distributed energy resources integrated active distribution network\"",
        "authors": "",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13056/v1/decision1"
    },
    {
        "id": 22401,
        "title": "Transformer with Local-feature Extractor for Relation Extraction",
        "authors": "Lihan Liu, Pengfei Li",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534183"
    },
    {
        "id": 22402,
        "title": "Reactors",
        "authors": "Richard F. Dudley, Michael Sharp, Antonio Castanheira, Behdad B. Biglar",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-11"
    },
    {
        "id": 22403,
        "title": "STTRE: A Spatio-Temporal Transformer with Relative Embeddings for Multivariate Time Series Forecasting",
        "authors": "Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4404879"
    },
    {
        "id": 22404,
        "title": "Evaluation of Effectiveness of Pre-Training Method in Chest X-Ray Imaging Using Vision Transformer",
        "authors": "Kuniki Imagawa, Kohei Shiomoto",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4507834"
    },
    {
        "id": 22405,
        "title": "Rescue the “Parishing”: Henry Budd – Constructive Transformer or Colonial Tool?",
        "authors": "J. Keith Hyde",
        "published": "2017-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25071/0848-1563.39692"
    },
    {
        "id": 22406,
        "title": "Struct2IUPAC -- Transformer-Based Artificial Neural Network for the Conversion Between Chemical Notations",
        "authors": "Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 1,
        "abstract": "Providing IUPAC chemical names is necessary for chemical information exchange. We developed a Transformer-based artificial neural architecture to translate between SMILES and IUPAC chemical notations: <i>Struct2IUPAC</i> and <i>IUPAC2Struct</i>. Our models demonstrated the performance that is comparable to rule-based solutions. We proved that both accuracy, speed of computations, and the model's robustness allow us to use it in production. Our showcase demonstrates that a neural-based solution can encourage rapid development keeping the same performance. We believe that our findings will inspire other developers to reduce development costs by replacing complex rule-based solutions with neural-based ones. The demonstration of  <i>Struct2IUPAC</i> model is available online on <i>Syntelly</i> platform <i>https://app.syntelly.com/smiles2iupac</i>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13274732"
    },
    {
        "id": 22407,
        "title": "A Normal Cloud Model for Transformer Insulation Condition Prognosis with Optimal Weights",
        "authors": "Lei Yang",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535746"
    },
    {
        "id": 22408,
        "title": "Optimizing and Integrating Transformer / Feeder Protection &amp; Control Including Operations",
        "authors": "Benton Vandiver",
        "published": "2019-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2019.8765879"
    },
    {
        "id": 22409,
        "title": "High Voltage Circuit Breaker and Power Transformer Failure Modes and Their Detection",
        "authors": "Charbel Antoun",
        "published": "2018-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535655"
    },
    {
        "id": 22410,
        "title": "Transformer-Based Approach to Variable Typing",
        "authors": "Charles Arthel Rey, Jose  Lorenzo Danguilan, Karl  Patrick Mendoza, Miguel Francisco  Miravite Remolona",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4415221"
    },
    {
        "id": 22411,
        "title": "Competition Scenario of Taiwan Transformer Industry",
        "authors": "En-hou ZU",
        "published": "2017-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/dtmse/mmme2016/10145"
    },
    {
        "id": 22412,
        "title": "Vision Transformer-Based Image Inpainting Method",
        "authors": "Rui Lin",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icetci57876.2023.10176576"
    },
    {
        "id": 22413,
        "title": "Electromagnetic Phenomena in Ferromagnetic Bodies",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-7"
    },
    {
        "id": 22414,
        "title": "The Power Transformer Winding Input Admittance and Frequency Response",
        "authors": "Vasiliy LARIN",
        "published": "2022-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24160/0013-5380-2022-8-33-39"
    },
    {
        "id": 22415,
        "title": "THREE-PHASE WITHOUT TRANSFORMER DIRECT FREQUENCY CONVERTER",
        "authors": "V.G. Sugakov, N.S. Varlamov, Yu.S. Malyshev",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46960/47355565_2022_37"
    },
    {
        "id": 22416,
        "title": "CALCULATION AND OPTIMIZATION OF MAGNETORHEOLOGICAL TRANSFORMER ELECTROMAGNET OF SEMIACTIVE HYDROMOUNT",
        "authors": "A.I. Ermolaev",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46960/2658-6754_2022_1_16"
    },
    {
        "id": 22417,
        "title": "Présentation",
        "authors": "Benoît Meyronin, Marie-Louis Jullien, Stéphane Bourrier",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0011"
    },
    {
        "id": 22418,
        "title": "Conclusion",
        "authors": "Benoît Meyronin, Marie-Louis Jullien, Stéphane Bourrier",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0181"
    },
    {
        "id": 22419,
        "title": "Overview of Insulation Diagnostics",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch2"
    },
    {
        "id": 22420,
        "title": "Application of New Generation Transformer Harmonic Filter Hybrid Solution",
        "authors": "Thomas Orlowski, Leszek Jasinski",
        "published": "2019-11-11",
        "citations": 1,
        "abstract": "Abstract\nIn recent years, Electrical Submersible Pump operators are facing increasing demands to provide more economical ways of producing crude oil at the well site. One of the ways to decrease the cost of production is to reduce the cost of electrical energy used for the ESP system. The limiting of harmonics produced by ESP electrical systems to IEEE-519 recommended levels is widely used, yet greatly misunderstood, as there are a variety of methods and tools that must be made compatible with the application in order to achieve successful mitigation of the harmonics.\nIncreased horsepower, larger electrical ESP systems, and greater energy consumption require more space at the well site. On offshore installations, that space has always been at a premium. The footprint expansion of this ESP electrical equipment would create unacceptable costs considering the current cost-saving market conditions.\nThis paper presents an innovative approach to tackle harmonics mitigation. This significantly smaller and cheaper approach delivers the required IEEE-519 THDI in ESP electrical system whilst incorporating new technologies that present characteristics capable of increasing the run life of an ESP electrical system.\nA typical ESP electrical system installation has usually contained the following: SDT – Step Down Transformer, HF – Harmonic Filter [or some other means of controlling harmonics], VSD- Variable Speed drive, SWF – Sine Wave Filter, SUT – Step Up Transformer, and ESP Motor. Combining the Step Down Transformer with the Harmonic Filter and designing it as a one-piece unit, cooled in oil, creates a smaller and therefore more cost-effective device, that achieves more desirable harmonics characteristics. Including not only efficient cooling, but the use of parasitic inductances of transformer windings as a useful component of the integrated harmonic filter gives an advantage on the technical market. This paper provides further discussion and details on this concept's advantages as a solution to reducing operating costs.",
        "link": "http://dx.doi.org/10.2118/197234-ms"
    },
    {
        "id": 22421,
        "title": "Analysis of different transformer variants",
        "authors": "Yisu Li",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Efficient self-attention models are crucial in applications where long sequences are modeled, such as documents, images, and videos, are often composed of relatively large numbers of pixels or tokens. Therefore, the efficiency of dealing with long sequences is crucial for the broad adoption of Transformers. This paper talks about four transformer variants: Longformer, Transformer-XL, Big Bird, and Star transformer. This paper makes a comparative analysis of the four varieties theoretical methods, and obtains the advantages of the four methods and their applicable fields. Longformer has lower complexity and can be used for various document-level tasks. Transformer-XL improves the accuracy by addressing context fragmentation. BigBird replaces the Bert-like whole attention mechanism with Block Sparse Attention, which has achieved the SOTA effect on many tasks with long text sequences, such as long text summaries, long text questions, and answers. Star-Transformer reduces the time complexity to and performs well in the synthetic data set.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/20230432"
    },
    {
        "id": 22422,
        "title": "A Lightweight Transformer with Convolutional Attention",
        "authors": "Kungan Zeng, Incheon Paik",
        "published": "2020-12-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icast51195.2020.9319489"
    },
    {
        "id": 22423,
        "title": "Experimental Investigation of Changes in Iron and Copper Losses in the Transformer",
        "authors": "Mehmet Ali Ozcelik",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf58372.2023.10177493"
    },
    {
        "id": 22424,
        "title": "Mitigation Transformer Inrush Current Using Modified Transient Current Limiter",
        "authors": "",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5829/ije.2019.32.05b.12"
    },
    {
        "id": 22425,
        "title": "Comparison of Transformer-based Architectures for Product Categorization",
        "authors": "Cherine Mohamed, Abeer Hamdy",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icca59364.2023.10401734"
    },
    {
        "id": 22426,
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modelling - Paper Review (Presentation Slides)",
        "authors": "Eric Benhamou",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3971444"
    },
    {
        "id": 22427,
        "title": "PLANAR FLYBACK TRANSFORMER DESIGN FOR PV POWERED LED ILLUMINATION",
        "authors": "",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20508/ijrer.v11i1.11695.g8155"
    },
    {
        "id": 22428,
        "title": "Load Alleviation in Transmission System by Using Phase Shifting Transformer",
        "authors": "Thatarad Srikwamcharoen, Kiatiyuth Kveeyarn",
        "published": "2018-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieecon.2018.8712121"
    },
    {
        "id": 22429,
        "title": "EndoDepthL: Lightweight Endoscopic Monocular Depth Estimation with CNN-Transformer",
        "authors": "Yangke Li",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10386008"
    },
    {
        "id": 22430,
        "title": "Dedustnet: A Frequency-Dominated Swin Transformer-Based Wavelet Network for Agricultural Dust Removal",
        "authors": "Shengli Zhang, Zhiyong Tao, Sen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4701006"
    },
    {
        "id": 22431,
        "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
        "authors": "Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-359"
    },
    {
        "id": 22432,
        "title": "CT-SAT: Contextual Transformer for Sequential Audio Tagging",
        "authors": "Yuanbo Hou, Zhaoyi Liu, Bo Kang, Yun Wang, Dick Botteldooren",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-196"
    },
    {
        "id": 22433,
        "title": "Types of transformer overload protection",
        "authors": "Numonjonov Shakhzod Dilshodjon Ugli",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5958/2278-4853.2021.00278.0"
    },
    {
        "id": 22434,
        "title": "Robust Tokenizer for Vision Transformer",
        "authors": "Rinka Kiriyama, Akio Sashima, Ikuko Shimizu",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315472"
    },
    {
        "id": 22435,
        "title": "HTC-Grasp: A Hybrid Transformer-CNN Architecture for Robotic Grasp Detection",
        "authors": "Qiang Zhang, Jianwei Zhu, Xueying Sun, Mingmin Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "We introduce a novel hybrid Transformer-CNN architecture for robotic grasp detection, designed to enhance the accuracy of grasping unknown objects. Our proposed architecture has two key designs. Firstly, we develop a hierarchical transformer as the encoder, incorporating the external attention to effectively capture the correlation features across the data. Secondly, the decoder is constructed with cross-layer connections to efficiently fuse multi-scale features. Channel attention is introduced in the decoder to model the correlation between channels and to adaptively recalibrate the channel correlation feature response, thereby increasing the weight of the effective channels. Our method is evaluated on the Cornell and Jacquard public datasets, achieving an image-wise detection accuracy of 98.3% and 95.8% on each dataset, respectively. Additionally, we achieve object-wise detection accuracy of 96.9% and 92.4% on the same datasets. A physical experiment is also performed using the Elite 6Dof robot, with a grasping accuracy rate of 93.3%, demonstrating the proposed method's ability to grasp unknown objects in real-world scenarios. The results of this study show that our proposed method outperforms other state-of-the-art methods.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0382.v2"
    },
    {
        "id": 22436,
        "title": "Dielectric strength of nanofluid-impregnated  transformer solid insulation",
        "authors": "Daniel Pérez-Rosa, Andrés Montero, Belén García, Juan Carlos Burgos",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThe interest in developing new fluids that can be used as dielectric liquids for transformers has driven the research on dielectric nanofluids in the last years. A number of authors have reported promising results on the electrical and thermal properties of dielectric nanofluids. Less attention has been paid to the interaction of these fluids with the cellulose materials that constitute the solid insulation of the transformers. In the present study, the dielectric strength of cellulose insulation is investigated, comparing its behaviour when it is impregnated with transformer mineral oil and when it is impregnated with a dielectric nanofluid. The study includes the analysis of the AC breakdown voltage and the impulse breakdown voltage of the samples. Large improvements were observed on the AC breakdown voltages of the specimens impregnated with nanofluids while the enhancements were lower in the case of the impulse tests. The reasons for the increase in AC breakdown voltage were investigated considering the dielectric properties of the nanofluids used to impregnate the samples of cellulose. The analysis was completed with a finite element study that revealed the effect of the nanoparticles on the electric field distribution within the test cell, and its role in the observed enhancement.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1898337/v1"
    },
    {
        "id": 22437,
        "title": "Investigation of the Influence of Direct Current Bias on Transformer Vibration",
        "authors": "Jun Wang, Hongshun Liu, Kaicheng Niu, Zemin Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00559"
    },
    {
        "id": 22438,
        "title": "Matières à former-conformer-transformer",
        "authors": "Céline Rosselin-Bareille",
        "published": "2017-6-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/socio-anthropologie.2517"
    },
    {
        "id": 22439,
        "title": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer Neural Networks",
        "authors": "Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, Yuedong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Synthesis planning is the process of recursively decomposing target molecules into available precursors. Computer-aided retrosynthesis can potentially assist chemists in designing synthetic routes, but at present it is cumbersome and provides results of dissatisfactory quality. In this study, we develop a template-free self-corrected retrosynthesis predictor (SCROP) to perform a retrosynthesis prediction task trained by using the Transformer neural network architecture. In the method, the retrosynthesis planning is converted as a machine translation problem between molecular linear notations of reactants and the products. Coupled with a neural network-based syntax corrector, our method achieves an accuracy of 59.0% on a standard benchmark dataset, which increases >21% over other deep learning methods, and >6% over template-based methods. More importantly, our method shows an accuracy 1.7 times higher than other state-of-the-art methods for compounds not appearing in the training set.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.8427776.v1"
    },
    {
        "id": 22440,
        "title": "Transformer Text Classification Model for Arabic Dialects that Utilizes Inductive Transfer",
        "authors": "Laith H. Baniata, Sangwoo Kang",
        "published": "No Date",
        "citations": 3,
        "abstract": "In the realm of the five-category classification endeavor, there has been limited exploration into applied techniques for classifying Arabic text. These methods have primarily leaned on singletask learning, incorporating manually crafted features that lack robust sentence representations. Recently, the Transformer paradigm has emerged as a highly promising alternative. However, when these models are trained using single-task learning, they often face challenges in achieving outstanding performance and generating robust latent feature representations, especially when dealing with small datasets. This issue is particularly pronounced in the context of the Arabic dialect, which has a scarcity of available resources. Given these constraints, this study introduces an innovative approach to dissecting sentiment in Arabic text. This approach combines Inductive Transfer (INT) with the Transformer paradigm to augment the adaptability of the model and refine the representation of sentences. By employing self-attention SE-A and feed-forward sub-layers as a shared Transformer encoder for both the five-category and three-category Arabic text classification tasks, this proposed model adeptly discerns sentiment in Arabic dialect sentences. The empirical findings underscore the commendable performance of the proposed model, as demonstrated in assessments of the Hotel Arabic-Reviews Dataset, the Book Reviews Arabic Dataset, and the LARB dataset. ",
        "link": "http://dx.doi.org/10.20944/preprints202311.0818.v1"
    },
    {
        "id": 22441,
        "title": "Inverse Compositional Spatial Transformer Networks",
        "authors": "Chen-Hsuan Lin, Simon Lucey",
        "published": "2017-7",
        "citations": 91,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2017.242"
    },
    {
        "id": 22442,
        "title": "Video Multitask Transformer Network",
        "authors": "Hongje Seong, Junhyuk Hyun, Euntai Kim",
        "published": "2019-10",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw.2019.00194"
    },
    {
        "id": 22443,
        "title": "CTGAN : Cloud Transformer Generative Adversarial Network",
        "authors": "Gi-Luen Huang, Pei-Yuan Wu",
        "published": "2022-10-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897229"
    },
    {
        "id": 22444,
        "title": "The Proposed Two-Stage Parameter Design Methodology of a Generalized Resonant DC Transformer in Hybrid AC/DC Microgrid with Optimum Active Power Transmission",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_4"
    },
    {
        "id": 22445,
        "title": "The Proposed Simplified Resonant Parameters Design of the Asymmetrical CLLC-Type DC Transformer in the Renewable Energy System via Semi-Artificial Intelligent Optimal Scheme",
        "authors": "Xin Zhang, Fanfan Lin, Hao Ma, Bin Zhao, Jingjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-9115-8_3"
    },
    {
        "id": 22446,
        "title": "Transformer oil-based magnetic nanofluid with high dielectric losses tested for cooling of a model transformer",
        "authors": "Michal Rajnak, Milan Timko, Peter Kopcansky, Katarina Paulovicova, Jozef Kuchta, Marek Franko, Juraj Kurimsky, Bystrik Dolnik, Roman Cimbala",
        "published": "2019-8",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tdei.2019.008047"
    },
    {
        "id": 22447,
        "title": "Automatic gas detection using seismic data and Transformer neural networks",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22564/17cisbgf2021.058"
    },
    {
        "id": 22448,
        "title": "High-temperature coplanar transformer",
        "authors": "Maxime Semard, Christian Martin, Cyril Buttay, Charles Joubert",
        "published": "2018-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2018.8352268"
    },
    {
        "id": 22449,
        "title": "Healthcare Transformer 8: Build Organizational Quality Improvement Capability",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-8"
    },
    {
        "id": 22450,
        "title": "SyntaLinker: Automatic Fragment Linking with Deep Conditional Transformer Neural Networks",
        "authors": "Yuyao Yang, Shuangjia Zheng, Shimin Su, Jun Xu, Hongming Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Fragment\nbased drug design represents a promising drug discovery paradigm complimentary\nto the traditional HTS based lead generation strategy. How to link fragment\nstructures to increase compound affinity is remaining a challenge task in this\nparadigm. Hereby a novel deep generative model (SyntaLinker) for linking fragments\nis developed with the potential for applying in the fragment-based lead\ngeneration scenario. The state-of-the-art transformer architecture was employed\nto learn the linker grammar and generate novel linker. Our results show that,\ngiven starting fragments and user customized linker constraints, our SyntaLinker model can design abundant drug-like molecules fulfilling these constraints and\nits performance was superior to other reference models. Moreover, several\nexamples were showcased that SyntaLinkercan be useful tools for carrying out\ndrug design tasks such as fragment linking, lead optimization and scaffold\nhopping.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12271508"
    },
    {
        "id": 22451,
        "title": "Transformer inrush - Harmonics in the current",
        "authors": "Bermann Jiri, Prochazka Martin",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2018.8396014"
    },
    {
        "id": 22452,
        "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
        "authors": "Markéta Řezáčková, Jan Švec, Daniel Tihelka",
        "published": "2021-8-30",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-546"
    },
    {
        "id": 22453,
        "title": "Prologue",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-003"
    },
    {
        "id": 22454,
        "title": "Power Transformer Fault Diagnosis Based on MPSO-SVM",
        "authors": "Zhiqiang Yang",
        "published": "2020-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5013/ijssst.a.16.02.06"
    },
    {
        "id": 22455,
        "title": "GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples",
        "authors": "Tian Gao, Cheng-Zhong Xu, Le Zhang, Hui Kong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vision Transformer (ViT) has performed remarkably in various computer vision tasks. Nonetheless, affected by the massive amount of parameters, ViT usually suffers from serious overfitting problems with a relatively limited number of training samples. In addition, ViT generally demands heavy computing resources, which limit its deployment on resource-constrained devices. As a type of model-compression method,  model binarization is potentially a good choice to solve the above problems. Compared with the full-precision one, the model with the binarization method replaces complex tensor multiplication with simple bit-wise binary operations and represents full-precision model parameters and activations with only 1-bit ones, which potentially solves the problem of model size and computational complexity, respectively. In this paper, we investigate a binarized ViT model. Empirically, we observe that the existing binarization technology designed for Convolutional Neural Networks (CNN) cannot migrate well to a ViT's binarization task. We also find that the decline of the accuracy of the binary ViT model is mainly due to the information loss of the \\textbf{Attention} module and the \\textbf{Value} vector. Therefore, we propose a novel model binarization technique, called \\textbf{G}roup \\textbf{S}uperposition \\textbf{B}inarization (\\textbf{GSB}), to deal with these issues. Furthermore, in order to further improve the performance of the binarization model, we have investigated the gradient calculation procedure in the binarization process and derived more proper gradient calculation equations for GSB to reduce the influence of gradient mismatch. Then, the knowledge distillation technique is introduced to alleviate the performance degradation caused by model binarization. Analytically, model binarization can limit the parameter’s search space during parameter updates while training a model. Therefore, the binarization process can actually play an implicit regularization role and help solve the problem of overfitting in the case of insufficient training data. Experiments on three datasets with limited numbers of training samples demonstrate that the proposed GSB model achieves state-of-the-art performance among the binary quantization schemes and exceeds its full-precision counterpart on some indicators. Code and models are available at https://github.com/IMRL/GSB-Vision-Transformer.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22817249"
    },
    {
        "id": 22456,
        "title": "Transpose: 6d Object Pose Estimation with Geometry-Aware Transformer",
        "authors": "Xiao Lin, Deming Wang, Guangliang Zhou, Chengju Liu, Qijun Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670638"
    },
    {
        "id": 22457,
        "title": "Research on Transformer Relay Protection in 10kV Power Supply System",
        "authors": "",
        "published": "2022-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i1.033"
    },
    {
        "id": 22458,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4247878"
    },
    {
        "id": 22459,
        "title": "Illustrations",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-001"
    },
    {
        "id": 22460,
        "title": "Smart transformer control of the electrical grid",
        "authors": "",
        "published": "2022-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo217e_ch13"
    },
    {
        "id": 22461,
        "title": "Los programas generativos “Transformer” AI, entre los que está ChatGPT, ¿una oportunidad para la evaluación formativa?",
        "authors": "Miguel Zapata-Ros",
        "published": "No Date",
        "citations": 2,
        "abstract": "Cada vez hay más evidencia de que las improvisaciones estadísticas de programas como ChatGPT producen textos aparentemente creíbles pero que no tienen ninguna sustentación. El problema no es tanto que permitan el fraude en proyectos y artículos, como que hagan pasar por correctos lo que sólo son generación de textos \"sin alma\", sin beingness, o simplemente escritos sin validar teórica o empíricamente, que se aceptan como ciertos acríticamente.El aprendizaje no es dar cuenta sólo mediante exámenes y proyectos, de datos o de textos describiendo conceptos o hechos. Supone según los clásicos (Gagne, Merrill, Reigeluth) comprender, atribuir sentido, aplicar autónomamente lo aprendido y transferirlo a entornos distintos y cambiantes.Para eso tiene que haber procesos de interacción y de retroalimentación. Y, si se utilizan proyectos, deben, en la evaluación, ser supervisados en su realización, en la comprensión y en la atribución de sentido que el prendiz hace.Toda esta constelación de ideas, métodos y prácticas constituye, en el contexto del diseño instruccional, la evaluación formativa.La evaluación formativa (formative assesment) es evaluación continua + feedback. En ese sentido debería haber actividades de evaluación continua que ofrezcan a los estudiantes comentarios formativos sobre su progreso o para ayudarles a autoevaluar el desarrollo y la calidad del diseñoLa evaluación para el aprendizaje (comúnmente conocida como evaluación formativa se centra en la retroalimentación tanto para profesores como para estudiantes.En un mundo donde la IA aprende y nos supera en una lista creciente de dominios cognitivos, *Beingness* es el dominio final del esfuerzo intelectual humano y debe ser el núcleo de la educación. Y la evaluación formativa debe ser un instrumento para ello.",
        "link": "http://dx.doi.org/10.35542/osf.io/k2eps"
    },
    {
        "id": 22462,
        "title": "Contents",
        "authors": "",
        "published": "2018-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7560/316436-toc"
    },
    {
        "id": 22463,
        "title": "Detection Fuse Blown of Transformer Using GSM",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2017.06.192"
    },
    {
        "id": 22464,
        "title": "Forecasting on solid state transformer applications",
        "authors": "Vijayakrishna Satyamsetti, Andreas Michealides, Antonis Hadjiantonis",
        "published": "2017-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iss1.2017.8389425"
    },
    {
        "id": 22465,
        "title": "A research on submersible transformer",
        "authors": "Chunbao Liu",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2638582"
    },
    {
        "id": 22466,
        "title": "Cascading Transformer Failure Probability Model Under Geomagnetic Disturbances",
        "authors": "Pratishtha Shukla, James Nutaro, Srikanth Yoginath",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408098"
    },
    {
        "id": 22467,
        "title": "Thermal modelling of power transformer",
        "authors": "R. Bouhaddiche, S. Bouazabia, I. Fofana",
        "published": "2017-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124676"
    },
    {
        "id": 22468,
        "title": "STCovidNet: Automatic Detection Model of Novel Coronavirus Pneumonia Based on Swin Transformer",
        "authors": "Boyuan Wang, Du Zhang, Zonggui Tian",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nThe novel coronavirus disease 2019 (COVID-19) has emerged as an enormous challenge facing China today. Preventive Medicine physicians and Artificial Intelligence (AI) researchers try to improve the ability to early automatic warning of coronavirus infections, promote epidemic prevention, and reduce medical costs using deep learning methods. In this work, we build an extensive database of chest computed tomography (CT) scans with image data from domestic and international open-source medical datasets. Swin Transformer is chosen as the backbone network to establish a model (STCovidNet) for the prediction of COVID-19. We then compare the performance of our technique against that of Vision Transformer (ViT) and Convolutional Neural Network (CNN). Next, to visualize our model's high-dimensional outputs in 2-dimensional space, we apply t-distributed stochastic neighbor embedding (t-SNE) as the dimension-reduction strategy. Finally, we employ gradient-weighted class activation mapping (Grad-CAM) to present a class activation map. The results indicate that STCovidNet’s performance surpasses ViT and CNN with a 0.9811 AUC and 0.9858 accuracy score. Our network outperforms previous techniques to reduce intra-class variability and generate well-separated feature embedding. The CAM figure illustrates that the decision region corresponds to radiologists' detecting spots. The suggested method can be an effective way of catching COVID-19 instances.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1401026/v1"
    },
    {
        "id": 22469,
        "title": "UNDERSTANDING THE LOCATION OF HOT SPOTS IN TRANSFORMER WINDINGS",
        "authors": "Tor Laneryd, Andreas Gustafsson, Sead Travancic",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1615/ihtc16.ctm.023498"
    },
    {
        "id": 22470,
        "title": "Transformer-based Environmental Sound Classification Modeling by Jointing Multi-class Classification and Similarity Clustering",
        "authors": "Xing Zhou, Ming Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs environmental sound signal is not as regular as speech, it has varying temporal structures and is more difficult to distinguish. Previous research on Environmental Sound Classiﬁcation (ESC) has designed sophisticated methods to extract feature from raw waveforms directly, but this may not be generalized well across different ESC tasks. We proposed an end-to-end audio scene classification network which is only based on the log-mel feature. First, we used Transformer network to encode signals that can capture crucial temporal information from self-attention. Then we combined Multi-class classifier with similarity clustering so as to maximize the distance between different classes. At last, we visualized the Transformer’s ability to locate important temporal information. The performance on ESC10 and ESC50 showed that our architecture reached an average accuracy of 95.3% and 84.2%, respectively. That was an achievement of new state-of-the-art performance with only log-mel input. Meanwhile, that is nearly equivalent to the best performance of the model based on raw waveform or combined feature method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2158071/v1"
    },
    {
        "id": 22471,
        "title": "Simple Vision Transformer Fallen Person Detection for the Elderly",
        "authors": "Tinglong Liu, Haiyan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4245289"
    },
    {
        "id": 22472,
        "title": "Deep Transformer Network for Hyperspectral Image Classification",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2021.040703"
    },
    {
        "id": 22473,
        "title": "MetaLLM: Residue-wise Metal ion Prediction Using Deep Transformer Model",
        "authors": "Fairuz Shadmani Shishir, Bishnu Sarker, Farzana Rahman, Sumaiya Shomaji",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProteins bind to metals such as copper, zinc, magnesium, etc., serving various purposes such as importing, exporting, or transporting metal in other parts of the cell as ligands and maintaining stable protein structure to function properly. A metal binding site indicates the single amino acid position where a protein binds a metal ion. Manually identifying metal binding sites is expensive, laborious, and time-consuming. A tiny fraction of the millions of proteins in UniProtKB – the most comprehensive protein database – are annotated with metal binding sites, leaving many millions of proteins waiting for metal binding site annotation. Developing a computational pipeline is thus essential to keep pace with the growing number of proteins. A significant shortcoming of the existing computational methods is the consideration of the long-term dependency of the residues. Other weaknesses include low accuracy, absence of positional information, hand-engineered features, and a pre-determined set of residues and metal ions. In this paper, we propose MetaLLM, a metal binding site prediction technique, by leveraging the recent progress in self-supervised attention-based (e.g. Transformer) large language models (LLMs) and a considerable amount of protein sequences publicly available. LLMs are capable of modelling long residual dependency in a sequence. The proposed MetaLLM uses a transformer pre-trained on an extensive database of protein sequences and later fine-tuned on metal-binding proteins for multi-label metal ions prediction. A 10-fold cross-validation shows more than 90% precision for the most prevalent metal ions.",
        "link": "http://dx.doi.org/10.1101/2023.03.20.533488"
    },
    {
        "id": 22474,
        "title": "Se transformer de voyeur en voyant",
        "authors": "Jean-Jacques Lebel, Anne Coppel",
        "published": "2017-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/chime.091.0243"
    },
    {
        "id": 22475,
        "title": "Data &amp; Channel Efficient Vision Transformer",
        "authors": "Li Changhong",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccet59170.2023.10335121"
    },
    {
        "id": 22476,
        "title": "Transformer-Based Long-Context End-to-End Speech Recognition",
        "authors": "Takaaki Hori, Niko Moritz, Chiori Hori, Jonathan Le Roux",
        "published": "2020-10-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2928"
    },
    {
        "id": 22477,
        "title": "CUNI Transformer Neural MT System for WMT18",
        "authors": "Martin Popel",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6424"
    },
    {
        "id": 22478,
        "title": "Better Sign Language Translation with STMC-Transformer",
        "authors": "Kayo Yin, Jesse Read",
        "published": "2020",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.525"
    },
    {
        "id": 22479,
        "title": "Cnn-Transformer Blend Pyramid Network for Underwater Image",
        "authors": "Shibai Yin, Dongyang Hong, Yibin Wang, Weixing Wang, Yee-hong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4564021"
    },
    {
        "id": 22480,
        "title": "Agriculture : adapter les pratiques, transformer la communication",
        "authors": "André Barlier",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/pes.400.0045"
    },
    {
        "id": 22481,
        "title": "Transformer model: Explainability and prospectiveness",
        "authors": "Gengpan Nuobu",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The purpose of Artificial Intelligence(AI) is to simulate learning process of human brain by strong computing power and appropriate algorithm, so that the machine can develop judging ability at work as human. Current AI mainly relies on Deep Learning model which is based on artificial neural network, like Convolutional Neural Network(CNN) in computer visualization, but that also takes with some defects. This paper introduces defects of CNN and discusses Transformer model in solving unexplainability of traditional CNN algorithm. To discuss why the Transformer model and attention mechanism are considered as the way to AI intelligibility.",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231079"
    },
    {
        "id": 22482,
        "title": "Multilingual Cyber Abuse Detection using Advanced Transformer Architecture",
        "authors": "Aditya Malte, Pratik Ratadiya",
        "published": "2019-10",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929493"
    },
    {
        "id": 22483,
        "title": "Calibration of Instrument Current Transformer Test Sets",
        "authors": "Karel Draxler, Jan Hlavacek, Renata Styblikova",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ae.2019.8866993"
    },
    {
        "id": 22484,
        "title": "Multiple Plant Tracking Using a Vision Transformer for Image Matching as a Spatial Association Strategy",
        "authors": "Byron Hernandez, Henry Medeiros",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693600"
    },
    {
        "id": 22485,
        "title": "Multi-Port Power Electronic Transformer",
        "authors": "Ereola J. Aladesanmi, Remmy Musumpuka, D. G. Dorrell",
        "published": "2021-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec52827.2021.9709474"
    },
    {
        "id": 22486,
        "title": "Dissolved Gas Analysis (DGA)",
        "authors": "Behrooz Vahidi, Ashkan Teymouri",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-19693-6_4"
    },
    {
        "id": 22487,
        "title": "Analysis of Influence of Main Transformer Remanence on Generator Starting Process",
        "authors": "",
        "published": "2021-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i12.89"
    },
    {
        "id": 22488,
        "title": "An Algorithm for transformer hot spot temperature determining",
        "authors": "Srdjan JOKIC",
        "published": "2017-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2017.06.32"
    },
    {
        "id": 22489,
        "title": "Analyzing EMWs using wavelet transform",
        "authors": "Gevork B. Gharehpetian, Hossein Karami",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00011-2"
    },
    {
        "id": 22490,
        "title": "Space-Varying Time-Invariant Isotropic Medium",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-2"
    },
    {
        "id": 22491,
        "title": "Use of amorphous ribbon and nano-materials in transformer cores",
        "authors": "",
        "published": "2019-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo157g_ch9"
    },
    {
        "id": 22492,
        "title": "XeroAlign: Zero-shot cross-lingual transformer alignment",
        "authors": "Milan Gritta, Ignacio Iacobacci",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.32"
    },
    {
        "id": 22493,
        "title": "Tap Changers and Smart Intelligent Controls",
        "authors": "Dieter Dohnal, Axel Kraemer, Karsten Viereck",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-14"
    },
    {
        "id": 22494,
        "title": "Space Vector Analysis of Three-Phase Transformer Transients",
        "authors": "Diego Bellan",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cscc49995.2020.00046"
    },
    {
        "id": 22495,
        "title": "Improved DC Transformer Submodule with High Input voltage",
        "authors": "Liangcai Shu, Wu Chen",
        "published": "2020-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec39645.2020.9124552"
    },
    {
        "id": 22496,
        "title": "Practical Study of Mixed-Core High Frequency Power Transformer",
        "authors": "Arun Kumar Paul",
        "published": "2022-9-1",
        "citations": 3,
        "abstract": "The design of medium- to high-frequency power electronics transformer aims not only to minimize the power loss in the windings and the core, but its heat removal features should also allow optimal use of both core and copper. The heat removal feature (e.g., thermal conduction) of a transformer is complex because there exist multiple loss centers. The bulk of total power loss is concentrated around a small segment of the core assembly where windings are overlaid. The primary winding is most constrained thermally. For superior use of core and copper, the temperature rise in different segments of the transformer should be well below their respective safe operating limits. In practice, cores of same soft-magnetic materials are traditionally used. To achieve superior temperature profile and for better long-term performance, this article proposes to use the mixed-core configuration. The new core(s) would replace the parent ones from the segment where windings are laid. The characteristic features of new cores would share increased burden of heat removal from the transformer. To obtain the qualitative insight of magnetic and thermal performance, the proposed mixed-core transformer would be thoroughly validated practically in two different high-power applications. In the first case, the core is always energized to its rated value, and in the second one, windings are always energized at respective rated current capacity.",
        "link": "http://dx.doi.org/10.3390/magnetism2030022"
    },
    {
        "id": 22497,
        "title": "Cross Test Comparison in Transformer Windings Frequency Response Analysis",
        "authors": "Szymon Banaszak, Wojciech Szoka",
        "published": "2018-5-25",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/en11061349"
    },
    {
        "id": 22498,
        "title": "Feature Aggregation with Transformer for Rgb-T Salient Object Detection",
        "authors": "Mengnan Xu, Ping Zhang, Ziyan Zhang, Pan Gao, jing zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342104"
    },
    {
        "id": 22499,
        "title": "Transformer les perceptions avant d’engager l’action",
        "authors": "Marie Jasmine Hyppolite",
        "published": "2021-10-29",
        "citations": 0,
        "abstract": "Les transformations perceptives sont à la fois des produits de l’interaction et des outils pour l’interaction. Les changements de perceptions et des cadres perceptifs contribuent à la construction de nouveaux possibles d’activités. Les acteurs disposent de nouveaux canevas prêts à agir dans leur environnement « ici et maintenant » mais également pour « demain et ailleurs » Engagés dans des situations de crise, d’incertitude, les transformations perceptives se mobilisent. Notre étude « se préparer à une action de négociation collective » éclaire la façon dont les acteurs se construisent afin d’anticiper la pression inhérente à la singularité de cette situation. Une approche anthropologique des activités permet l’analyse de l’interaction des acteurs présents. Emergent des « mouvements conversationnels » mettant en lumière le processus de transformations des activités et de transformations des acteurs en activité. Les ruptures, les articulations et les continuums sont repérés. Les résultats manifestent une catégorisation des activités de transformation.",
        "link": "http://dx.doi.org/10.18778/2450-4491.13.09"
    },
    {
        "id": 22500,
        "title": "Healthcare Transformer 10: Reliably Implement the Tried and True",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439805077-6"
    },
    {
        "id": 22501,
        "title": "Virtual Synchronous Machine Control Applied to Solid State Transformer",
        "authors": "Yushi Miura, Junya Higuchi",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce50734.2022.9948189"
    },
    {
        "id": 22502,
        "title": "Inception of Jump resonance in Single phase transformer",
        "authors": "S. Poornima",
        "published": "2021-1-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnte51185.2021.9487654"
    },
    {
        "id": 22503,
        "title": "An Optimized Modeling Method for Transformer Design",
        "authors": "Yingying Liang, Xiaoming Liu, Jing Jin",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asicon47005.2019.8983686"
    },
    {
        "id": 22504,
        "title": "Patch Features Reconstruction Transformer for Occluded Person Re-Identification",
        "authors": "Yunbin Zhao, Songhao Zhu, Zhiwei Liang",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901670"
    },
    {
        "id": 22505,
        "title": "Task-Specific Alignment and Multiple-Level Transformer for Few-Shot Action Recognition",
        "authors": "Fei Guo, Li Zhu, YiKang Wang, Jing Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4600142"
    },
    {
        "id": 22506,
        "title": "Excitation Transformer",
        "authors": "",
        "published": "2019-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781118841006.ch12"
    },
    {
        "id": 22507,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/review1"
    },
    {
        "id": 22508,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/review2"
    },
    {
        "id": 22509,
        "title": "Transformer-Based Turkish Automatic Speech Recognition",
        "authors": "Davut Taşar, Kutan Koruyan, Cihan Çılgın",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26650/acin.1338604"
    },
    {
        "id": 22510,
        "title": "SiGra: Single-cell spatial elucidation through image-augmented graph transformer",
        "authors": "Ziyang Tang, Tonglin Zhang, Baijian Yang, Jing Su, Qianqian Song",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTThe recent advances in high-throughput molecular imaging push the spatial transcriptomics technologies to the subcellular resolution, which breaks the limitations of both single-cell RNA-seq and array-based spatial profiling. The latest released single-cell spatial transcriptomics data from NanoString CosMx and MERSCOPE platforms contains multi-channel immunohistochemistry images with rich information of cell types, functions, and morphologies of cellular compartments. In this work, we developed a novel method, Single-cell spatial elucidation through image-augmented Graph transformer (SiGra), to reveal spatial domains and enhance the substantially sparse and noisy transcriptomics data. SiGra applies hybrid graph transformers over a spatial graph that comprises high-content images and gene expressions of individual cells. SiGra outperformed state-of-the-art methods on both single-cell spatial profiles and spot-level spatial transcriptomics data from complex tissues. The inclusion of immunohistochemistry images improved the model performance by 37% (95%CI: 27% – 50%). SiGra improves the characterization of intratumor heterogeneity and intercellular communications in human lung cancer samples, meanwhile recovers the known microscopic anatomy in both human brain and mouse liver tissues. Overall, SiGra effectively integrates different spatial modality data to gain deep insights into the spatial cellular ecosystems.",
        "link": "http://dx.doi.org/10.1101/2022.08.18.504464"
    },
    {
        "id": 22511,
        "title": "Compact Transformer-Based Matching Structures for Ka-Band Power Amplifiers",
        "authors": "Valdrin Qunaj, Patrick Reynaert",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc46564.2019.9038687"
    },
    {
        "id": 22512,
        "title": "Research on Stress Reduction Model Based on Transformer",
        "authors": "",
        "published": "2022-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.12.009"
    },
    {
        "id": 22513,
        "title": "The development of transformer in vision",
        "authors": "Jialong Shao",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Transformer is a deep neural network that utilizes a self-attentive technique to process data in parallel. The area of vision was neural network based before Transformer, with great frameworks like faster-RNN, YOLO, etc. Transformer, which was developed to stop this phenomena, is discussed in this article together with its accomplishments in the field of vision during the past few years and its projections for the future in order to provide some references for more research.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/2023312"
    },
    {
        "id": 22514,
        "title": "Alternative Transformer Theory Based on Poynting’s Theorem",
        "authors": "Mansur Shakirov, Anton Tkachuk",
        "published": "2020-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icieam48468.2020.9111920"
    },
    {
        "id": 22515,
        "title": "Methods of Investigation and Constructional Materials",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-1"
    },
    {
        "id": 22516,
        "title": "Characterization of Amorphous Metal Materials for High-Frequency High-Power-Density Transformer",
        "authors": "Anas Bashir-U-Din",
        "published": "2019-1-7",
        "citations": 0,
        "abstract": "In many applications such as power electronic devices, it is very desirable to employ high power-density transformers, because the available space and allowed weight are very limited. In general, operating at higher frequency would lead to smaller volume and weight of electromagnetic devices, but the core loss could increase significantly. With very low specific core loss and relatively high saturation magnetic flux density, amorphous metal (AM) materials offer great potential. This scientific research aims to study and model the AM properties for developing high performance transformers such as high efficiency and high power density. The use of Amorphous metals as a core material enables high frequency transformers to attain optimum and higher level of efficiencies. This scientific paper discusses: (i) theoretical understanding of the process of magnetization and a discussion of AM magnetic properties as are useful for design of electrical devices (ii). Characterize the AM materials and to distinguish them on the basis of their distinctive magnetic properties and their usefulness for High frequency High Power Density (HFHPD) Transformers.",
        "link": "http://dx.doi.org/10.29007/mjhr"
    },
    {
        "id": 22517,
        "title": "Research Direction of Oil-immersed Transformer",
        "authors": "Yanbo Huang",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": " With the continuous development of the electric power industry in the world, the application of oil-immersed transformer in all walks of life has become more and more extensive. But the safety problem of oil-immersed transformer, environmental protection problem, heat energy recovery and utilization problem are always the three mountains in front of the power system researchers. Therefore, it is particularly important to develop a new type of transformer that can adapt to the green, innovative and sustainable development of the power system. This paper will discuss the new ideas and new directions of oil-immersed transformer research in the future from three aspects: the study of transformer oil, the recovery and utilization of transformer heat energy, and the noise reduction of transformer.",
        "link": "http://dx.doi.org/10.54097/ajst.v6i3.10164"
    },
    {
        "id": 22518,
        "title": "Efficient Seismic Facies Classification Using Transformer-based Masked Autoencoders",
        "authors": "M. Alfarhan, C. Birnie, T. Alkhalifah",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310319"
    },
    {
        "id": 22519,
        "title": "Molecular Transformer-aided Biocatalysed Synthesis Planning",
        "authors": "Daniel Probst, Matteo Manica, Yves Gaëtan Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, Teodoro Laino",
        "published": "No Date",
        "citations": 4,
        "abstract": "Enzyme catalysts are an integral part of green chemistry strategies towards a more sustainable and resource-efficient chemical synthesis. However, the use of enzymes on unreported substrates and their specific stereo- and regioselectivity are domain-specific knowledge factors that require decades of field experience to master. This makes the retrosynthesis of given targets with biocatalysed reactions a significant challenge. Here, we use the molecular transformer architecture to capture the latent knowledge about enzymatic activity from a large data set of publicly available biochemical reactions, extending forward reaction and retrosynthetic pathway prediction to the domain of biocatalysis. We introduce the use of a class token based on the EC classification scheme that allows to capture catalysis patterns among different enzymes belonging to the same hierarchical families. The forward prediction model achieves an accuracy of 49.6% and 62.7%, top-1 and top-5 respectively, while the single-step retrosynthetic model shows a round-trip accuracy of 39.6% and 42.6%, top-1 and top-10 respectively. Trained models and curated data are made publicly available with the hope of promoting enzymatic catalysis and making green chemistry more accessible through the use of digital technologies.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14639007.v1"
    },
    {
        "id": 22520,
        "title": "A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies",
        "authors": "shivacharan oruganti, Deepa Karuppaiah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4317584"
    },
    {
        "id": 22521,
        "title": "Real-Time Transformer Oil Monitoring Using Planar Resonator Based Sensor",
        "authors": "Rajat Srivastava, Sangeeta Kale",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4174867"
    },
    {
        "id": 22522,
        "title": "Msqat: A Multi-Dimension Non-Intrusive Speech Quality Assessment Transformer Utilizing Self-Supervised Representations",
        "authors": "Kailai Shen, Diqun Yan, Li Dong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466764"
    },
    {
        "id": 22523,
        "title": "Images Classification Integrating Transformer with Convolutional Neural Network",
        "authors": "Yulin Peng",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Convolutional neural networks (CNN) are one of the most widely used deep learning methods in computer vision, which can effectively extract local spatial information from images, but lack global understanding and dependency modelling of image features. As a result, contextual information cannot be fully utilized by the network. For example, on coordinate modelling tasks (such as object detection, image generation, etc.), CNN may not be able to accurately locate or reconstruct the position and shape of objects. In contrast to traditional CNN models such as ResNet, Transformers rely on their global attention mechanism to capture long-distance dependencies between patches. The thesis presents an enhanced lightweight method which integrates Transformer with five convolutional neural layers. Model based on CNN and Transformer is tested on the two benchmark datasets MNIST and CIFAR-10. After a few epochs, the model is convergent and reaches high accuracy of 99.34% in MNIST and 92.04% in CIFAR-10. This model outperforms the single CNN and some state-of-the-art models in classifying both datasets, especially in distinguishing similar images like ‘6’ and ‘9’, ‘bird’ and ‘plane’. These results indicate the model's good robustness and generality.",
        "link": "http://dx.doi.org/10.56028/aetr.6.1.621.2023"
    },
    {
        "id": 22524,
        "title": "Novel Transformer with Variable Leakage and Magnetizing Inductances",
        "authors": "Angshuman Sharma, Jonathan Kimball",
        "published": "2021-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1991834"
    },
    {
        "id": 22525,
        "title": "End-to-end 3D Human Pose Estimation with Transformer",
        "authors": "Bowei Zhang, Peng Cui",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956693"
    },
    {
        "id": 22526,
        "title": "Classification of Seismic Events Using Density Based Clustering and Transformer Neural Networks",
        "authors": "Luis Delgado, Billy Peralta, Orietta Nicolis, Mailiu Díaz",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760943"
    },
    {
        "id": 22527,
        "title": "Convtransgan: A Memory-Friendly Combination of Convolution and Transformer for 3d Ct/Mri Translation",
        "authors": "Ji Ma, Yetao Xie, Jinjin Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760354"
    },
    {
        "id": 22528,
        "title": "Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity",
        "authors": "Joel Ye, Jennifer L. Collinger, Leila Wehbe, Robert Gaunt",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe neural population spiking activity recorded by intracortical brain-computer interfaces (iBCIs) contain rich structure. Current models of such spiking activity are largely prepared for individual experimental contexts, restricting data volume to that collectable within a single session and limiting the effectiveness of deep neural networks (DNNs). The purported challenge in aggregating neural spiking data is the pervasiveness of context-dependent shifts in the neural data distributions. However, large scale unsupervised pretraining by nature spans heterogeneous data, and has proven to be a fundamental recipe for successful representation learning across deep learning. We thus develop Neural Data Transformer 2 (NDT2), a spatiotemporal Transformer for neural spiking activity, and demonstrate that pretraining can leverage motor BCI datasets that span sessions, subjects, and experimental tasks. NDT2 enables rapid adaptation to novel contexts in downstream decoding tasks and opens the path to deployment of pretrained DNNs for iBCI control. Code:https://github.com/joel99/context_general_bci",
        "link": "http://dx.doi.org/10.1101/2023.09.18.558113"
    },
    {
        "id": 22529,
        "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit",
        "authors": "Fanfei Meng, Lele Zhang, Yu Chen, Yuxin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound algorithm while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system. According to the experiment results, we achieve up to 74\\% computational savings for both training and inference with a minimal loss of accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24680943"
    },
    {
        "id": 22530,
        "title": "Field Validated Dynamic Thermal Model for Power Transformer Insulation System Assessment",
        "authors": "Mohamed Ryadi, Alain Tanguy",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481065"
    },
    {
        "id": 22531,
        "title": "TRANSFORMER BUSHINGS – FAILURE CASE STUDIES",
        "authors": " Antun Mikulecky",
        "published": "2022-7-19",
        "citations": 1,
        "abstract": "Relationship between bushing failure and transformer failure is discussed and, in regard of that, two bushing failure types are recognized: incipient bushing failure that does not result in transformer damage and terminal bushing failure having transformer failure as a consequence. It can be seen, that without applying the diagnostics, all bushing failures are terminal. Thirteen bushing failures have been analyzed regarding their cause, failure mechanism and consequences. In that sense, the ability and limitation of off line and on line diagnostics are discussed and some improvements are proposed. Some switchyard properties in the aspect of fire protection are indicated and, especially, the possible influence of rigid tubular connections on bushing failures. Beside mentioned design, service, condition diagnostics and other properties of all three condenser types of bushings are described in the paper.",
        "link": "http://dx.doi.org/10.37798/2012611-4249"
    },
    {
        "id": 22532,
        "title": "Saccade Inspired Attentive Visual Patch Transformer for Image Sentiment Analysis",
        "authors": "Jing Zhang, Jiangpei Liu, Xinzhou Zhang, Han Sun, Zhe Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685795"
    },
    {
        "id": 22533,
        "title": "Black start in distribution grids through solid-state transformer",
        "authors": "M. Couto, A. Coccia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0555"
    },
    {
        "id": 22534,
        "title": "OBJECT DETECTION VIA ALTERNATIVE TRANSFORMER",
        "authors": "Neychev Radoslav, Arman Stepanyan",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "Transformer is a great building block for state-of-the-art models for\nevery direction in artificial intelligence (AI). In natural language processing,\nGPT3 is one of the leading language models, and its fine-tuning leads to the\ncreation of automation products in various fields (chatting, analysis of data,\ncontent generation, etc.). In computer vision, it is DALLEE-2 with its fantastic\ncapability to generate realistic art images with the desired style. In\nreinforcement learning (RL) decision transformers [1] are widely used and\nachieve great results in such RL baseline games as ATARI, Key-To-Door tasks,\netc. Even though modern transformer blocks for end-to-end object detection\ntasks converge very slowly, which makes the training process\ncomputationally hard. We introduce alternative transformers improving\narchitecture and training process, reducing convergence and training time\nwith achieving same results in object detection tasks. Training process is\nparallelized, and a loss function is modified to increase model’s capability in\nmultiple tasks. Finally, this architecture can be used as a building block in\nother models, improving their performance",
        "link": "http://dx.doi.org/10.56246/18294480-2023.15-12"
    },
    {
        "id": 22535,
        "title": "Exploring transformer fault detection using RFID technology",
        "authors": "Xiaomeng Li",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Real-time monitoring and fault diagnosis of transformers are essential for the stable power system operation. This paper presents an RFID-based transformer fault feature extraction and classification algorithm. Experiments show that monitored current signals are stable while the temperature peak is 356°C. Hilbert decomposition reveals regular current and voltage patterns that can be used as fault indicators. Signal strength classification accuracy reached 80% . At rated load, the transformer temperature soared to 186°C, indicating overheating issues. The monitoring during a sample day showed that overload events were concentrated from 16:00-20:00, which required attention. The approach helps accurately identify transformer fault types from real-time RFID data for proactive maintenance. Compared to reactive repairs after failures, this not only improves employee productivity but also reduces costs. Based on customized RFID deployment, the algorithm contributes to the stability and economy of power infrastructure.",
        "link": "http://dx.doi.org/10.3233/rft-230056"
    },
    {
        "id": 22536,
        "title": "Grouptransnet: Group Transformer Network for Rgb-D Salient Object Detection",
        "authors": "Xian Fang, Jiang Mingfeng, Jinchao Zhu, Xiuli Shao, Hongpeng Wang",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4585918"
    },
    {
        "id": 22537,
        "title": "Faster Boundary-aware Transformer for Breast Cancer Segmentation",
        "authors": "Xin Zhou, Xiaoxia Yin",
        "published": "2023-5-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaci58115.2023.10146176"
    },
    {
        "id": 22538,
        "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
        "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura",
        "published": "2020-10-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3033"
    },
    {
        "id": 22539,
        "title": "Peer Review #2 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.1)\"",
        "authors": "DS Venuji Renuka",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.1/reviews/2"
    },
    {
        "id": 22540,
        "title": "Struct2IUPAC -- Transformer-Based Artificial Neural Network for the Conversion Between Chemical Notations",
        "authors": "Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 1,
        "abstract": "Providing IUPAC chemical names is necessary for chemical information exchange. We developed a Transformer-based artificial neural architecture to translate between SMILES and IUPAC chemical notations: Struct2IUPAC and IUPAC2Struct. Our models demonstrated the performance that is comparable to rule-based solutions. We proved that both accuracy, speed of computations, and the model's robustness allow us to use it in production. Our showcase demonstrates that a neural-based solution can encourage rapid development keeping the same performance. We believe that our findings will inspire other developers to reduce development costs by replacing complex rule-based solutions with neural-based ones. The demonstration of Struct2IUPAC model is available online on Syntelly platform https://app.syntelly.com/smiles2iupac",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13274732.v1"
    },
    {
        "id": 22541,
        "title": "Violation detection based on a modified transformer encoder network with Bi-LSTM",
        "authors": "Yuyu Zeng, Yu Zhang, XinSheng Zhu, Lan Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Due to the rapid growth of China’s security market, the early detection\nof accounting violations by listed companies has become increasingly\nurgent. However, the current lack of regulatory efficiency means that\nsuch problems are often not detected and solved. To solve these issues,\nresearchers have proposed traditional methods and artificial\nintelligence methods in recent decades. However, some existing methods\nhave high requirements for setting indicators and parameters, and their\ndetection accuracy could be improved. To take advantage of the concept\nof the self-attention mechanism, we built a modified transformer encoder\nnetwork with Bi-LSTM to analyze accounting data and identify whether\nthere are violations by listed companies. This model attempts to enhance\nthe long-term feature capture capability for processing structural data,\nsuch as accounting data, through Bi-LSTM and stacking multiple\ntransformer encoder networks while reducing the speed of computation due\nto the parallel power of the self-attention mechanism. Several\nexperiments were conducted on the accounting dataset released by the\nopen-source China Stock Market & Accounting Research (CSMAR). The\nresults show that the proposed model, by the composition of Bi-LSTM as\nthe input layer and three layers of transformer encoder network, has the\nbest detection performance in comparison to the deep neural network\n(DNN), random forest models, etc., with a testing accuracy of 0.9758 and\nan AUC of 0.98. Furthermore, we embedded the pretrained model in a\nWeChat miniprogram to provide a better interactive experience for users\nduring the preliminary detection of a single file with the required\naccounting data.",
        "link": "http://dx.doi.org/10.22541/au.167059997.74750948/v1"
    },
    {
        "id": 22542,
        "title": "Transformer monitoring using Kalman filtering",
        "authors": "Subramanian V. Shastri, Emma Stewart, Ciaran Roberts",
        "published": "2017-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta.2017.8062692"
    },
    {
        "id": 22543,
        "title": "Identifying Moraine-Dammed Glacial Lakes Using Moraine Accumulation Characteristics and Vision Transformer&amp;#160;",
        "authors": "Jinhao Xu, Min Feng, Yijie Sui",
        "published": "No Date",
        "citations": 0,
        "abstract": "Moraine-dammed glacial lakes are naturally formed by the accumulation of moraine debris in high mountain glacier environments. Due to their remote locations and the challenges in identification, these lakes often elude systematic and comprehensive surveys. However, under the influences of glacier melting and climate change, they can potentially cause catastrophic outburst floods, threatening the safety of downstream communities and the stability of ecosystems. Therefore, precise identification and monitoring of these lakes are crucial for disaster early warning and risk management.The aim of this study is to develop a novel method based on multi-source remote sensing data and Vision Transformer technology for effectively identifying moraine-dammed glacial lakes. Traditional remote sensing methods face numerous challenges in these high mountain environments, such as confusion with similar water bodies and the impact of complex terrain. Our proposed method focuses on utilizing moraine accumulation characteristics, a key factor in the formation of moraine-dammed lakes. By analyzing the relationship between glacier movement and lake formation, we aim to more accurately identify potential dammed lakes, thereby reducing misidentifications.We are using high-resolution satellite imagery and terrain data, combined with the Vision Transformer model for feature extraction. This model is capable of efficiently processing a large amount of complex spatial data and identifying specific geographical and geomorphological features. We are focusing on changes at the glacier front and terrain changes related to lake formation. Through this approach, we aim to extract key features directly related to the formation of moraine-dammed glacial lakes, thus improving the accuracy of identification.Additionally, we are establishing a database containing samples of known moraine-dammed glacial lakes to train and validate our model. By comparing it with existing databases of moraine-dammed glacial lakes, we aim to further test the effectiveness and reliability of our method. We are anticipating that this research will provide a new technological approach for monitoring moraine-dammed glacial lakes, with significant scientific importance and practical value in understanding the mechanisms of lake formation, assessing potential risks, and developing effective disaster prevention measures.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-14909"
    },
    {
        "id": 22544,
        "title": "Speaker-Aware Speech-Transformer",
        "authors": "Zhiyun Fan, Jie Li, Shiyu Zhou, Bo Xu",
        "published": "2019-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9003844"
    },
    {
        "id": 22545,
        "title": "Recent Developments in the Modelling of Transformer Windings",
        "authors": "Konstanty M. Gawrylczyk, Szymon Banaszak",
        "published": "2021-5-13",
        "citations": 6,
        "abstract": "The paper provides a review of the modelling techniques used to simulate the frequency response of transformer windings. The aim of the research and development of modelling methods was to analyze the influence of deformations and faults in the windings on the changes in the frequency response. All described methods are given with examples of the modelling results performed by the authors of this paper and from literature sources. The research is prefaced with a thorough literature review. There are described models based on lumped parameters with input data coming from direct calculations based on the winding geometry and obtained from FEM modelling software and models considering the wave phenomena in the windings. The analysis was also performed for practical problems in winding modelling: the influence of windings other than the modelled one and the influence of parallel wires in a winding.",
        "link": "http://dx.doi.org/10.3390/en14102798"
    },
    {
        "id": 22546,
        "title": "Sensitivity Analysis of Medium Frequency Transformer Design",
        "authors": "Marko Mogorovic, Drazen Dujic",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ipec.2018.8507855"
    },
    {
        "id": 22547,
        "title": "Sq-Swin: Siamese Quadratic Swin Transformer for Lettuce Browning Prediction",
        "authors": "Dayang Wang, Boce Zhang, Yongshun Xu, Yaguang Luo, Hengyong Yu",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374498"
    },
    {
        "id": 22548,
        "title": "Éduquer les patients, transformer les soignants",
        "authors": "Cécile Fournier",
        "published": "2018-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/gdsh.053.0018"
    },
    {
        "id": 22549,
        "title": "Impact of Battery Energy Storage System on Distribution Transformer Insulation Life",
        "authors": "Ram Krishan, Mukesh Kumar",
        "published": "2023-4-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/greentech56823.2023.10173803"
    },
    {
        "id": 22550,
        "title": "C10.1 Integrated Differential Transformer on a Single Printed Circuit Board",
        "authors": "M. Berger, A. Zygmanowski, S. Zimmermann",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5162/smsi2021/c10.1"
    },
    {
        "id": 22551,
        "title": "A Novel Method for Calculating and Weakening Residual Magnetism of Transformer Cores Considering No-Load Variables",
        "authors": "Cailing Huo, Yiming Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4671281"
    },
    {
        "id": 22552,
        "title": "Chapitre 7. Une typologie des clients réclamants",
        "authors": "Laurent Garnier",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0129"
    },
    {
        "id": 22553,
        "title": "Repenser les droits humains dans une perspective émancipatrice",
        "authors": "Sofía Monsalve Suárez, Guillaume Lejeune",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0135"
    },
    {
        "id": 22554,
        "title": "Vision Transformer Based Photo Capturing System",
        "authors": "Abdülkadir ALBAYRAK",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "Portrait photo is one of the most crucial documents that many people need for official transactions in many public and private organizations. Despite the developing technologies and high resolution imaging devices, people need such photographer offices to fulfil their needs to take photos. In this study, a Photo Capturing System has been developed to provide infrastructure for web and mobile applications. After the system detects the person's face, facial orientation and facial expression, it automatically takes a photo and sends it to a graphical user interface developed for this purpose. Then, with the help of the user interface of the photo taken by the system, it is automatically printed out. The proposed study is a unique study that uses imaging technologies, deep learning and vision transformer algorithms, which are very popular image processing techniques in several years. Within the scope of the study, face detection and facial expression recognition are performed with a success rate of close to 100\\% and 95.52\\%, respectively. In the study, the performances of Vision Transformer algorithm is also compared with the state of art algorithms in facial expression recognition.",
        "link": "http://dx.doi.org/10.17694/bajece.1345993"
    },
    {
        "id": 22555,
        "title": "Boosting English-Amharic machine translation using corpus augmentation and Transformer",
        "authors": "Yohannes Biadgligne, Kamel Smaili",
        "published": "2024",
        "citations": 0,
        "abstract": "The Transformer-based neural machine translation (NMT) model has been very successful in recent years and has become a new mainstream method. However, using them in lowresourced languages requires large amounts of data and efficient model configuration (hyperparameter tuning) mechanisms. The scarcity of parallel texts is a bottleneck for high quality (N) MTs, especially for under resourced languages like Amharic. As a result, this paper presents an attempt to improve English-Amharic MT by introducing three different vanilla Transformer architectures, with different hyper-parameter values. To obtain additional training material, offline token level corpus augmentation was applied to the previously collected English-Amharic parallel corpus. Compared to previous work on Amharic MT, the best of the three Transformer models have achieved state-of-the-art BLEU scores. In fact, we were able to achieve this result by employing corpus augmentation techniques and hyper-parameter tuning.",
        "link": "http://dx.doi.org/10.59671/mbulj"
    },
    {
        "id": 22556,
        "title": "Transformer Modeling",
        "authors": "",
        "published": "2019-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119487470.ch6"
    },
    {
        "id": 22557,
        "title": "Jusqu’où la santé numérique va-t-elle transformer l’organisation des soins ?",
        "authors": "Hervé Dumez, Étienne Minvielle",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.editionscnrs.45537"
    },
    {
        "id": 22558,
        "title": "Mesh Transformer: 3d Temperature Reconstruction for Various Tanks Via a Single Image",
        "authors": "Jincheng Chen, Feiding Zhu, Yuge Han, Dengfeng Ren",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4379461"
    },
    {
        "id": 22559,
        "title": "An Approach for Small Image Object Detection Based on Transformer-Cnn",
        "authors": "Chun-Liang Lin, Yan-Lin Chen, Yu-Chen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4718375"
    },
    {
        "id": 22560,
        "title": "Design of Transformer Based English French Translation Model",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060505"
    },
    {
        "id": 22561,
        "title": "Text Simplification Using Transformer and BERT",
        "authors": "Sarah Alissa, Mike Wald",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.033647"
    },
    {
        "id": 22562,
        "title": "Automatic Design System with Generative Adversarial Network and Vision Transformer for Efficiency Optimization of Interior Permanent Magnet Synchronous Motor",
        "authors": "Yuki Shimizu",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Interior permanent magnet synchronous motors are becoming increasingly popular as traction motors in environmentally friendly vehicles. These motors, which offer a wide range of design options, require time-consuming finite element analysis to verify their performance, thereby extending design times. To address this problem, we propose a deep learning model that can accurately predict the iron loss characteristics of different rotor topologies under various speed and current conditions, resulting in an automatic design system for the IPMSM rotor core. Using this system, the computation time for efficiency maps is reduced to less than 1/3000 of the time required for finite element analysis. The system also shows efficiency optimization results similar to the best results of previous research, while reducing the computational time for optimization by one or two orders of magnitude. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23650284.v1"
    },
    {
        "id": 22563,
        "title": "Transformer Protection",
        "authors": "Samir I. Abood, John Fuller",
        "published": "2023-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003394389-8"
    },
    {
        "id": 22564,
        "title": "Construction material classification on imbalanced datasets using Vision Transformer architecture (ViT)",
        "authors": "Maryam Soleymani, Mahdi Bonyani, Hadi Mahami, Farnad Nasirzadeh",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThis research proposes a reliable model for identifying different construction materials with the highest accuracy, which is exploited as an advantageous tool for a wide range of construction applications such as automated progress monitoring. In this study, a novel deep learning architecture called Vision Transformer (ViT) is used for detecting and classifying construction materials. The robustness of the proposed method is assessed by utilizing different image datasets. For this purpose, the model is trained and tested on two large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD). A third dataset is also generated by combining CML and BMD to create a more imbalanced dataset and assess the capabilities of the proposed method. The achieved results reveal an accuracy of 100 percent in evaluation metrics such as accuracy, precision, recall, and f1-score for each material category of three different datasets. It is believed that the suggested model accomplishes a novel and robust tool for detecting and classifying different material types. To date, a number of studies have attempted to automatically classify a variety of building materials, which still have some errors. This research will address the mentioned shortcoming and proposes a model to detect the material type with higher accuracy. The proposed model is also capable of being generalized to different datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1948162/v1"
    },
    {
        "id": 22565,
        "title": "Generating Radiology Reports via Multi-feature Optimization Transformer",
        "authors": "",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2023.10.010"
    },
    {
        "id": 22566,
        "title": "Optimizing Transformer for Low-Resource Neural Machine Translation",
        "authors": "Ali Araabi, Christof Monz",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.304"
    },
    {
        "id": 22567,
        "title": "A Novel Transformer-Based Model for State of Charge Estimation of Electrical Vehicle Batteries",
        "authors": "Metin Yılmaz, Eyüp Çinar, Ahmet Yazici",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4736082"
    },
    {
        "id": 22568,
        "title": "Using transformer in stock trend prediction",
        "authors": "Zhichen Liu",
        "published": "2023-10-23",
        "citations": 2,
        "abstract": "Large transformer model had achieved good results in many tasks, such as computer vision (CV) and natural language processing (NLP). However, in financial domains, the application of large deep learning models is rarely observed. Stock Trend Prediction (STP) is a task that using Limit Order Books (LOBs) to predict the future stock price trend by the sequence of historical limit order information, the trend can be Current works are mostly based on the structure of Convolutional Neural Network (CNN) + Recurrent Neural Networks (RNN). This structure is hard to parallel and cannot make full use of GPU resources. It is also difficult to increase the dimension to fit more complex data and performs poor when time sequence is long. Recently, some works proposed that CNN + Transformer model can also work is solving this task. This paper verifies that Transformer can be directly used into STP task and gain a good result, and proposes a novel Transformer-based model, Transformer-LOB, to enhance the basic transformer model performance. This model uses attention mechanisms to extract temporal information rather than using RNN, which utilizes the GPU effectively. Since all the feature extractions are based on transformer modules, the model is scalable and easy to parallel. Transformer-LOB is tested on FI-2010 LOB dataset and SZ-2015 LOB dataset, and outputs ideal results on both datasets.",
        "link": "http://dx.doi.org/10.54254/2755-2721/22/20231212"
    },
    {
        "id": 22569,
        "title": "CryoTransformer: A Transformer Model for Picking Protein Particles from Cryo-EM Micrographs",
        "authors": "Ashwin Dhakal, Rajan Gyawali, Liguo Wang, Jianlin Cheng",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractCryo-electron microscopy (cryo-EM) is a powerful technique for determining the structures of large protein complexes. Picking single protein particles from cryo-EM micrographs (images) is a crucial step in reconstructing protein structures from them. However, the widely used template-based particle picking process requires some manual particle picking and is labor-intensive and time-consuming. Though machine learning and artificial intelligence (AI) can potentially automate particle picking, the current AI methods pick particles with low precision or low recall. The erroneously picked particles can severely reduce the quality of reconstructed protein structures, especially for the micrographs with low signal-to-noise (SNR) ratios. To address these shortcomings, we devised CryoTransformer based on transformers, residual networks, and image processing techniques to accurately pick protein particles from cryo-EM micrographs. CryoTransformer was trained and tested on the largest labelled cryo-EM protein particle dataset - CryoPPP. It outperforms the current state-of-the-art machine learning methods of particle picking in terms of the resolution of 3D density maps reconstructed from the picked particles as well as F1-score and is poised to facilitate the automation of the cryo-EM protein particle picking.",
        "link": "http://dx.doi.org/10.1101/2023.10.19.563155"
    },
    {
        "id": 22570,
        "title": "Mesh Transformer: 3d Temperature Reconstruction for Various Tanks Via a Single Image",
        "authors": "Jincheng Chen, Feiding Zhu, Yuge Han, Dengfeng Ren",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683756"
    },
    {
        "id": 22571,
        "title": "Asymptotic Multilayer Pooled Transformer Strategy in Medical Assisted Decision-Making Systems",
        "authors": "Jia WU, Keke HE, Jun Zhu, Limiao Li, Fangfang Gou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Medical image analysis is one of the most important tools used by medical professionals in clinical diagnosis to diagnose patients' conditions. Digital pathology images contain rich information that is more conducive to identifying tissue structure. However, the boundaries between tissues in pathology images are sometimes too blurred, resulting in a high rate of misdiagnosis. Medical decision-making systems are able to assist in the recognition of medical images, thereby improving diagnostic accuracy. Transformer-based models have made excellent progress in the field of segmentation. However, the computational complexity of self-focused modules is high. Single-layer pooling is often used to reduce computational expenditure, but this strategy is prone to feature loss. Even worse, a large number of self-attention operations may exacerbate distraction and negatively impact segmentation performance. Therefore, this study proposes an asymptotic multilayer pooling transformer strategy in medical decision-making systems. First, we denoise and contrast-enhance the pathology images, and then segment them using the asymptotic pyramid structure based on multilayer pooling transformer (PMPSNet). In this, multilayer pooling is applied to the self-attention module to simplify the sequence while capturing the contextual aspects of the image sequence more efficiently. Meanwhile, the pyramid encoder is used to come to acquire multi-scale feature maps and the novel MLP structure is used to limit the distraction. Experimental results show that our proposed method not only obtains high DSC and IOU, but also has fewer model parameters and computational complexity.",
        "link": "http://dx.doi.org/10.1364/opticaopen.25108307"
    },
    {
        "id": 22572,
        "title": "Asymptotic Multilayer Pooled Transformer Strategy in Medical Assisted Decision-Making Systems",
        "authors": "Jia WU, Keke HE, Jun Zhu, Limiao Li, Fangfang Gou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Medical image analysis is one of the most important tools used by medical professionals in clinical diagnosis to diagnose patients' conditions. Digital pathology images contain rich information that is more conducive to identifying tissue structure. However, the boundaries between tissues in pathology images are sometimes too blurred, resulting in a high rate of misdiagnosis. Medical decision-making systems are able to assist in the recognition of medical images, thereby improving diagnostic accuracy. Transformer-based models have made excellent progress in the field of segmentation. However, the computational complexity of self-focused modules is high. Single-layer pooling is often used to reduce computational expenditure, but this strategy is prone to feature loss. Even worse, a large number of self-attention operations may exacerbate distraction and negatively impact segmentation performance. Therefore, this study proposes an asymptotic multilayer pooling transformer strategy in medical decision-making systems. First, we denoise and contrast-enhance the pathology images, and then segment them using the asymptotic pyramid structure based on multilayer pooling transformer (PMPSNet). In this, multilayer pooling is applied to the self-attention module to simplify the sequence while capturing the contextual aspects of the image sequence more efficiently. Meanwhile, the pyramid encoder is used to come to acquire multi-scale feature maps and the novel MLP structure is used to limit the distraction. Experimental results show that our proposed method not only obtains high DSC and IOU, but also has fewer model parameters and computational complexity.",
        "link": "http://dx.doi.org/10.1364/opticaopen.25108307.v1"
    },
    {
        "id": 22573,
        "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
        "authors": "Waradon Phokhinanan, Nicolas Obin, Sylvain Argentieri",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2015"
    },
    {
        "id": 22574,
        "title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation",
        "authors": "Ali Aroudi, Stefan Uhlich, Marc Ferras Font",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11386"
    },
    {
        "id": 22575,
        "title": "Multimodal Co-attention Transformer for Video-Based Personality Understanding",
        "authors": "Mingwei Sun, Kunpeng Zhang",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386376"
    },
    {
        "id": 22576,
        "title": "A Lightweight Multi-Head Attention Transformer for Stock Price Forecasting",
        "authors": "Anh Nguyen, Son Ha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4729648"
    },
    {
        "id": 22577,
        "title": "Modelling and Analysis of a Three-Phase to Five-Phase Transformer",
        "authors": "M. Rizwan Khan, Rashid Alammari, Shaikh Moinoddin, Atif Iqbal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00353"
    },
    {
        "id": 22578,
        "title": "Solid State Transformer Interface Based on Multilevel Inverter for Fuel Cell Power Generation and Management",
        "authors": "M. R. Banaei, E. Salary",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00359"
    },
    {
        "id": 22579,
        "title": "Video Transformer Network",
        "authors": "Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann",
        "published": "2021-10",
        "citations": 208,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00355"
    },
    {
        "id": 22580,
        "title": "Spatial Temporal Block Transformer Network for Skeleton-Based Action Recognition",
        "authors": "Fan Yang, Dewei Li, Gang Wang",
        "published": "2022-11-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055641"
    },
    {
        "id": 22581,
        "title": "Transformer Networks for Trajectory Forecasting",
        "authors": "Francesco Giuliari, Irtiza Hasan, Marco Cristani, Fabio Galasso",
        "published": "2021-1-10",
        "citations": 154,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9412190"
    },
    {
        "id": 22582,
        "title": "Une opportunité se chiffrant à plusieurs milliards de dollars – Réorienter le soutien au secteur agricole pour transformer les systèmes alimentaires",
        "authors": "",
        "published": "2021-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4060/cb6683fr"
    },
    {
        "id": 22583,
        "title": "Oil and Paper Insulation for DC Converter Transformer",
        "authors": "Boxue Du",
        "published": "2021",
        "citations": 0,
        "abstract": "Transformer oil and oil-impregnated paper, serve as the essential parts of converter transformer, suffering various electric fields. The accumulation of surface charge on the paper would lead to flashover. When the power flow of the HVDC system is reversed, the charge field will easily lead to discharge. Direct-fluorination is a method which could affect the material property without alternating the bulk property. Besides, a new type of nano-modified transformer oil is a method to improve properties. This chapter presents a study of the effect of fluorination on surface charge behavior, the effect of polarity reversal voltages on interface charge behavior and the effect of Boron nitride (BN) nanoparticles on the high thermal conductivity of transformer oil. Results show that fluorination had an influence on the chemical property of the paper and BN nanoparticles has improvements in heat transfer process. In the polarity reversal test, the dissipation rate becomes smaller as the reversal time gets longer. ",
        "link": "http://dx.doi.org/10.4018/978-1-7998-8591-7.ch024"
    },
    {
        "id": 22584,
        "title": "Transformer and seq2seq model for Paraphrase Generation",
        "authors": "Elozino Egonmwan, Yllias Chali",
        "published": "2019",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5627"
    },
    {
        "id": 22585,
        "title": "DESTR: Object Detection with Split Transformer",
        "authors": "Liqiang He, Sinisa Todorovic",
        "published": "2022-6",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00916"
    },
    {
        "id": 22586,
        "title": "Self-Calibrating High Precision Current Transformer",
        "authors": "D. Slomovitz, A. Santos, R. Sandler, G. Barreto",
        "published": "2020-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpem49742.2020.9191806"
    },
    {
        "id": 22587,
        "title": "Statistical analysis of Australian and New Zealand Power Transformer Catastrophic Fires",
        "authors": "D. Martin, N.R. Watson",
        "published": "2018-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535670"
    },
    {
        "id": 22588,
        "title": "Transformer Reliability: A Key Issue for Both Manufacturers and Users",
        "authors": "Adolf J. Kachler",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12275-13"
    },
    {
        "id": 22589,
        "title": "Smart Transformer: Design, Control and Impact on Future Distribution Grids",
        "authors": "",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2018.8591260"
    },
    {
        "id": 22590,
        "title": "Carcinogenicity of dioxin-like polychlorinated biphenyls in transformer soil in vicinity of University of Port Harcourt, Choba, Nigeria",
        "authors": "Chem Int",
        "published": "No Date",
        "citations": 0,
        "abstract": "Polychlorinated Biphenyls (PCBs) in the vicinity of transformers soils at main campus of university of Port Harcourt, Choba, Nigeria was monitored. Evaluation was done for both total PCBs (Aroclor) and congener’s form using Gas Chromatography at four designated sites; A, B, C, and D with geographical co-ordinates for site A-Donald Ekong Library, latitude 4°54’, 32’’N and longitude 6°55:05’’E, site B-Senate building with latitude 4°54’14’’N and longitude 6°55’, 23’’N, site C-Transformer close to music department with latitude 4°54’, 01’’N and longitude 6°55; 56’’E and finally site D-Gana-Ma Lecturers residential quarters with latitude 4°54’.23’’ N and longitude 6°55’, 74’’E. All the sites are polluted with PCBs that exceeds the maximum limit of 2.0 mg/kg as per United States Toxic Substances Control Act (TSCA). The order of total PCBs was site D site &gt; site A &gt; site B &gt; site C, which also corresponds with the order of sites carcinogenicity of dioxin-like PCBs, calculated as Total toxicity Equivalence concentration (TTEC). The TTEC for site A, B, C and D corresponds to 0.000012, 0.000035, 0.0000185 and 0.00039 (mg/kg), respectively, which exceeded the method B clean up levels for 2, 3, 7, 8-tetrachloro dibenzo-p-dioxin levels of 1.3×10-5 mg/kg and need of massive cleanup for carcinogenic dioxin-like PCBs. We also found out there is high loadings of PCBs congeners with little or no biodegradability across the four sites. To mitigate the known human health risks posed by PCBs toxicity, non-PCBs transformers should be replaced with current ones and extensive soil remediation is needed to clean up the PCBs to avoid negative impact.",
        "link": "http://dx.doi.org/10.31221/osf.io/cwepm"
    },
    {
        "id": 22591,
        "title": "Deep Diffeomorphic Transformer Networks",
        "authors": "Nicki Skafte Detlefsen, Oren Freifeld, Soren Hauberg",
        "published": "2018-6",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00463"
    },
    {
        "id": 22592,
        "title": "Transformer Core-Vibration Analysis: Coupling Paths",
        "authors": "Andre Wurde, Jannis Nikolas Kahlen, Nils Langenberg, Albert Moser",
        "published": "2022-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic51169.2022.9833177"
    },
    {
        "id": 22593,
        "title": "Professional knowledge-based convolutional neural network for transformer protection",
        "authors": "",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.04480"
    },
    {
        "id": 22594,
        "title": "Phase-Shifting Transformer Efficiency Analysis Based on Low-Voltage Laboratory Units",
        "authors": "Paweł Albrechtowicz",
        "published": "2021-8-17",
        "citations": 3,
        "abstract": "Phase-shifting transformers are effective elements used to control power flows in many power systems. Their positive influence on power flows has been proved in the literature. However, the efficiency of phase-shifting transformers has not been analyzed, especially not with regard for their various types. This study is therefore focused on the efficiency question with respect to electrical energy parameters. Research was performed on a laboratory phase-shifter unit with longitudinal and quadrature voltage regulation, and then these results were correlated to the simulation model equivalent. Laboratory transformer parameter data were used to prepare asymmetrical and symmetrical phase-shifting transformer simulation models. Simulation results were then used to compare the electrical properties and efficiency of all the types of phase-shifting transformer considered. All phase-shifting transformer types had a significant impact on the transmitted active power, but each type had different features. The symmetrical unit had the lowest power losses and a stable output voltage level compared to the asymmetrical one, which increased the output voltage, while the quadrature voltage also grew. These features must be considered, taking into account power system conditions such as the voltage variability profile and active power transfer demand. In this study, we propose the construction of an asymmetrical controllable phase-shifting transformer in order to achieve flexible control.",
        "link": "http://dx.doi.org/10.3390/en14165049"
    },
    {
        "id": 22595,
        "title": "Transformer Tracker based on Multi-level Residual Perception Structure",
        "authors": "Hui Chen, Zhenhai Wang, Lutao Yuan, Ying Ren, Hongyu Tian",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecently, Transformer networks have been used for feature extraction and calculation of similarity in object tracking, replacing the previous use of CNN for feature extraction and cross-attention for feature fusion in Transformer based object tracking algorithms. This new structure is called one stream structure, and has achieved good results. However, the one stream structure of the Transformer tracker has too many network parameters, which limits the tracking speed of the network. For this reason, we have designed a pure one stream Transformer structure that uses soft segmentation operations to significantly reduce model parameters and computational complexity. In order to further improve the accuracy of tracking, we propose a multi-level residual perception structure to enhance the feature information of the target and reduce the background feature information, thereby enhancing the foreground and background discrimination ability of the model. In order to prove the speed and accuracy of our method, we not only compared it with algorithms using deep neural network models, but also compared it with UAV tracking algorithms using shallow networks. Compared with these algorithms, it can be proved that our algorithm not only has fast reasoning speed, but also has very high reasoning accuracy. Experimentally, the UAV123 dataset reached the level of SOTA; the inference speed can reach 130 FPS.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2799061/v1"
    },
    {
        "id": 22596,
        "title": "Convolutional Transformer for Fast and Accurate Gravitational Wave Detection",
        "authors": "Letian Jiang, Yuan Luo",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956104"
    },
    {
        "id": 22597,
        "title": "Accelerating materials property prediction via a hybrid structure-composition transformer-graph framework that leverages four body interactions",
        "authors": "Mohammad Madani, Anna Tarakanova",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.64c26778632e9539aa87d934"
    },
    {
        "id": 22598,
        "title": "Analysis, Simulation and Comparison between H6 Transformer-less Inverter Topologies",
        "authors": "Essam Hendawi",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mepcon47431.2019.9007922"
    },
    {
        "id": 22599,
        "title": "Music Audio Sentiment Classification Based on Improvied Vision Transformer",
        "authors": "Chen Zhen, Liu Changhui",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11648/j.ajcst.20230601.16"
    },
    {
        "id": 22600,
        "title": "The single-phase transformer as a source of current harmonics",
        "authors": "V. L. Fedorov",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dynamics.2017.8239450"
    },
    {
        "id": 22601,
        "title": "Construction material classification on imbalanced datasets using Vision Transformer architecture (ViT)",
        "authors": "Maryam Soleymani, Mahdi Bonyani, Hadi Mahami, Farnad Nasirzadeh",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThis research proposes a reliable model for identifying different construction materials with the highest accuracy, which is exploited as an advantageous tool for a wide range of construction applications such as automated progress monitoring. In this study, a novel deep learning architecture called Vision Transformer (ViT) is used for detecting and classifying construction materials. The robustness of the proposed method is assessed by utilizing different image datasets. For this purpose, the model is trained and tested on two large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD). A third dataset is also generated by combining CML and BMD to create a more imbalanced dataset and assess the capabilities of the proposed method. The achieved results reveal an accuracy of 100 percent in evaluation metrics such as accuracy, precision, recall, and f1-score for each material category of three different datasets. It is believed that the suggested model accomplishes a novel and robust tool for detecting and classifying different material types. To date, a number of studies have attempted to automatically classify a variety of building materials, which still have some errors. This research will address the mentioned shortcoming and proposes a model to detect the material type with higher accuracy. The proposed model is also capable of being generalized to different datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1948162/v1"
    },
    {
        "id": 22602,
        "title": "Generating Radiology Reports via Multi-feature Optimization Transformer",
        "authors": "",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2023.10.010"
    },
    {
        "id": 22603,
        "title": "Optimizing Transformer for Low-Resource Neural Machine Translation",
        "authors": "Ali Araabi, Christof Monz",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.304"
    },
    {
        "id": 22604,
        "title": "A Novel Transformer-Based Model for State of Charge Estimation of Electrical Vehicle Batteries",
        "authors": "Metin Yılmaz, Eyüp Çinar, Ahmet Yazici",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4736082"
    },
    {
        "id": 22605,
        "title": "Using transformer in stock trend prediction",
        "authors": "Zhichen Liu",
        "published": "2023-10-23",
        "citations": 2,
        "abstract": "Large transformer model had achieved good results in many tasks, such as computer vision (CV) and natural language processing (NLP). However, in financial domains, the application of large deep learning models is rarely observed. Stock Trend Prediction (STP) is a task that using Limit Order Books (LOBs) to predict the future stock price trend by the sequence of historical limit order information, the trend can be Current works are mostly based on the structure of Convolutional Neural Network (CNN) + Recurrent Neural Networks (RNN). This structure is hard to parallel and cannot make full use of GPU resources. It is also difficult to increase the dimension to fit more complex data and performs poor when time sequence is long. Recently, some works proposed that CNN + Transformer model can also work is solving this task. This paper verifies that Transformer can be directly used into STP task and gain a good result, and proposes a novel Transformer-based model, Transformer-LOB, to enhance the basic transformer model performance. This model uses attention mechanisms to extract temporal information rather than using RNN, which utilizes the GPU effectively. Since all the feature extractions are based on transformer modules, the model is scalable and easy to parallel. Transformer-LOB is tested on FI-2010 LOB dataset and SZ-2015 LOB dataset, and outputs ideal results on both datasets.",
        "link": "http://dx.doi.org/10.54254/2755-2721/22/20231212"
    },
    {
        "id": 22606,
        "title": "CryoTransformer: A Transformer Model for Picking Protein Particles from Cryo-EM Micrographs",
        "authors": "Ashwin Dhakal, Rajan Gyawali, Liguo Wang, Jianlin Cheng",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractCryo-electron microscopy (cryo-EM) is a powerful technique for determining the structures of large protein complexes. Picking single protein particles from cryo-EM micrographs (images) is a crucial step in reconstructing protein structures from them. However, the widely used template-based particle picking process requires some manual particle picking and is labor-intensive and time-consuming. Though machine learning and artificial intelligence (AI) can potentially automate particle picking, the current AI methods pick particles with low precision or low recall. The erroneously picked particles can severely reduce the quality of reconstructed protein structures, especially for the micrographs with low signal-to-noise (SNR) ratios. To address these shortcomings, we devised CryoTransformer based on transformers, residual networks, and image processing techniques to accurately pick protein particles from cryo-EM micrographs. CryoTransformer was trained and tested on the largest labelled cryo-EM protein particle dataset - CryoPPP. It outperforms the current state-of-the-art machine learning methods of particle picking in terms of the resolution of 3D density maps reconstructed from the picked particles as well as F1-score and is poised to facilitate the automation of the cryo-EM protein particle picking.",
        "link": "http://dx.doi.org/10.1101/2023.10.19.563155"
    },
    {
        "id": 22607,
        "title": "Mesh Transformer: 3d Temperature Reconstruction for Various Tanks Via a Single Image",
        "authors": "Jincheng Chen, Feiding Zhu, Yuge Han, Dengfeng Ren",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683756"
    },
    {
        "id": 22608,
        "title": "Asymptotic Multilayer Pooled Transformer Strategy in Medical Assisted Decision-Making Systems",
        "authors": "Jia WU, Keke HE, Jun Zhu, Limiao Li, Fangfang Gou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Medical image analysis is one of the most important tools used by medical professionals in clinical diagnosis to diagnose patients' conditions. Digital pathology images contain rich information that is more conducive to identifying tissue structure. However, the boundaries between tissues in pathology images are sometimes too blurred, resulting in a high rate of misdiagnosis. Medical decision-making systems are able to assist in the recognition of medical images, thereby improving diagnostic accuracy. Transformer-based models have made excellent progress in the field of segmentation. However, the computational complexity of self-focused modules is high. Single-layer pooling is often used to reduce computational expenditure, but this strategy is prone to feature loss. Even worse, a large number of self-attention operations may exacerbate distraction and negatively impact segmentation performance. Therefore, this study proposes an asymptotic multilayer pooling transformer strategy in medical decision-making systems. First, we denoise and contrast-enhance the pathology images, and then segment them using the asymptotic pyramid structure based on multilayer pooling transformer (PMPSNet). In this, multilayer pooling is applied to the self-attention module to simplify the sequence while capturing the contextual aspects of the image sequence more efficiently. Meanwhile, the pyramid encoder is used to come to acquire multi-scale feature maps and the novel MLP structure is used to limit the distraction. Experimental results show that our proposed method not only obtains high DSC and IOU, but also has fewer model parameters and computational complexity.",
        "link": "http://dx.doi.org/10.1364/opticaopen.25108307"
    },
    {
        "id": 22609,
        "title": "Asymptotic Multilayer Pooled Transformer Strategy in Medical Assisted Decision-Making Systems",
        "authors": "Jia WU, Keke HE, Jun Zhu, Limiao Li, Fangfang Gou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Medical image analysis is one of the most important tools used by medical professionals in clinical diagnosis to diagnose patients' conditions. Digital pathology images contain rich information that is more conducive to identifying tissue structure. However, the boundaries between tissues in pathology images are sometimes too blurred, resulting in a high rate of misdiagnosis. Medical decision-making systems are able to assist in the recognition of medical images, thereby improving diagnostic accuracy. Transformer-based models have made excellent progress in the field of segmentation. However, the computational complexity of self-focused modules is high. Single-layer pooling is often used to reduce computational expenditure, but this strategy is prone to feature loss. Even worse, a large number of self-attention operations may exacerbate distraction and negatively impact segmentation performance. Therefore, this study proposes an asymptotic multilayer pooling transformer strategy in medical decision-making systems. First, we denoise and contrast-enhance the pathology images, and then segment them using the asymptotic pyramid structure based on multilayer pooling transformer (PMPSNet). In this, multilayer pooling is applied to the self-attention module to simplify the sequence while capturing the contextual aspects of the image sequence more efficiently. Meanwhile, the pyramid encoder is used to come to acquire multi-scale feature maps and the novel MLP structure is used to limit the distraction. Experimental results show that our proposed method not only obtains high DSC and IOU, but also has fewer model parameters and computational complexity.",
        "link": "http://dx.doi.org/10.1364/opticaopen.25108307.v1"
    },
    {
        "id": 22610,
        "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
        "authors": "Waradon Phokhinanan, Nicolas Obin, Sylvain Argentieri",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2015"
    },
    {
        "id": 22611,
        "title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation",
        "authors": "Ali Aroudi, Stefan Uhlich, Marc Ferras Font",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11386"
    },
    {
        "id": 22612,
        "title": "Multimodal Co-attention Transformer for Video-Based Personality Understanding",
        "authors": "Mingwei Sun, Kunpeng Zhang",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386376"
    },
    {
        "id": 22613,
        "title": "A Lightweight Multi-Head Attention Transformer for Stock Price Forecasting",
        "authors": "Anh Nguyen, Son Ha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4729648"
    },
    {
        "id": 22614,
        "title": "Modelling and Analysis of a Three-Phase to Five-Phase Transformer",
        "authors": "M. Rizwan Khan, Rashid Alammari, Shaikh Moinoddin, Atif Iqbal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00353"
    },
    {
        "id": 22615,
        "title": "Solid State Transformer Interface Based on Multilevel Inverter for Fuel Cell Power Generation and Management",
        "authors": "M. R. Banaei, E. Salary",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00359"
    },
    {
        "id": 22616,
        "title": "Video Transformer Network",
        "authors": "Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann",
        "published": "2021-10",
        "citations": 208,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00355"
    },
    {
        "id": 22617,
        "title": "Spatial Temporal Block Transformer Network for Skeleton-Based Action Recognition",
        "authors": "Fan Yang, Dewei Li, Gang Wang",
        "published": "2022-11-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055641"
    },
    {
        "id": 22618,
        "title": "Transformer Networks for Trajectory Forecasting",
        "authors": "Francesco Giuliari, Irtiza Hasan, Marco Cristani, Fabio Galasso",
        "published": "2021-1-10",
        "citations": 154,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9412190"
    },
    {
        "id": 22619,
        "title": "A Wavelet-Based Transformer Differential Protection With Differential Current Transformer Saturation and Cross-Country Fault Detection",
        "authors": "Rodrigo Prado Medeiros, Flavio Bezerra Costa",
        "published": "2018-4",
        "citations": 85,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrd.2017.2764062"
    },
    {
        "id": 22620,
        "title": "Medium frequency transformer leakage inductance modeling and experimental verification",
        "authors": "Marko Mogorovic, Drazen Dujic",
        "published": "2017-10",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2017.8095813"
    },
    {
        "id": 22621,
        "title": "Contactless Energy Transmission using a Transformer with Movable Secondary",
        "authors": "Mariusz Stepien",
        "published": "2021-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/edpe53134.2021.9604088"
    },
    {
        "id": 22622,
        "title": "User-Guided Image Inpatinting with Transformer",
        "authors": "Jingjun Qiu, Yan Gao",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai52525.2021.00174"
    },
    {
        "id": 22623,
        "title": "Self Oscilating Transformer-Based Current Transducer",
        "authors": "Daniil Shevtsov, Dmitry Shishov, Dmitry Sukhov",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoecs46375.2019.8949944"
    },
    {
        "id": 22624,
        "title": "Switched Plasma Slab: B Wave Pulses*",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-5"
    },
    {
        "id": 22625,
        "title": "Automatic Design System with Generative Adversarial Network and Vision Transformer for Efficiency Optimization of Interior Permanent Magnet Synchronous Motor",
        "authors": "Yuki Shimizu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Interior permanent magnet synchronous motors are becoming increasingly popular as traction motors in environmentally friendly vehicles. These motors, which offer a wide range of design options, require time-consuming finite element analysis to verify their performance, thereby extending design times. To address this problem, we propose a deep learning model that can accurately predict the iron loss characteristics of different rotor topologies under various speed and current conditions, resulting in an automatic design system for the IPMSM rotor core. Using this system, the computation time for efficiency maps is reduced to less than 1/3000 of the time required for finite element analysis. The system also shows efficiency optimization results similar to the best results of previous research, while reducing the computational time for optimization by one or two orders of magnitude. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23650284"
    },
    {
        "id": 22626,
        "title": "A Time Series Driven Model for Early Sepsis Prediction Based on Transformer Module",
        "authors": "Yan Tang, Yu Zhang, Jiaxi Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSepsis remains a critical concern in intensive care units due to its high mortality rate. Early identification and intervention are paramount to improving patient outcomes. In this study, we have proposed predictive models for early sepsis prediction based on time-series data, utilizing both CNN-Transformer and LSTM-Transformer architectures. By collecting time-series data from patients at 4, 8, and 12 hours prior to sepsis diagnosis and subjecting it to various network models for analysis and comparison. In contrast to traditional recurrent neural networks, our model exhibited a substantial improvement of approximately 20%. On average, our model demonstrated an accuracy of 0.964 (± 0.018), a precision of 0.956 (± 0.012), a recall of 0.967 (± 0.012), and an F1 score of 0.959 (± 0.014). Furthermore, by adjusting the time window, it was observed that the Transformer-based model demonstrated exceptional predictive capabilities, particularly within the earlier time window (i.e., 12 hours before onset), thus holding significant promise for early clinical diagnosis and intervention. Besides, we employed the SHAP algorithm to visualize the weight distribution of different features, enhancing the interpretability of our model and facilitating early clinical diagnosis and intervention.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3341174/v1"
    },
    {
        "id": 22627,
        "title": "Breakdown voltage based transformer oil analysis using optical fiber as sensor",
        "authors": "Deba Kumar Mahanta, Shakuntala Laskar",
        "published": "2017-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/moc.2017.8244582"
    },
    {
        "id": 22628,
        "title": "Maximum overlap discrete wavelet based transformer differential protection",
        "authors": "Okan Ozgonenel, Serap Karagol",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu.2017.7960187"
    },
    {
        "id": 22629,
        "title": "Transformer: Linking Atom Mapping and Neural Machine Translation",
        "authors": "Chengyun Zhang, Ling Wang, Yejian Wu, Yun Zhang, An Su, Hongliang Duan",
        "published": "No Date",
        "citations": 1,
        "abstract": " Atom mapping reveals the corresponding relationship between reactant and product atoms in chemical reactions, which is important for drug design, exploration for underlying chemical mechanism, reaction classification and so on. Here, we present a new method that links atom mapping and neural machine translation using the transformer model. In contrast to the previous algorithms, our method runs reaction prediction and captures the information of corresponding atoms in parallel. Meanwhile, we use a set of approximately 360K reactions without atom mapping information for obtaining general chemical knowledge and transfer it to atom mapping task on another dataset which contains 50K atom-mapped reactions. With manual evaluation, the top-1 accuracy of the transformer model in atom mapping reaches 91.4%. we hope our work can provide an important step toward solving the challenge problem of atom mapping in a linguistic perspective.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13173674.v1"
    },
    {
        "id": 22630,
        "title": "Electric Transformer and Coupled Inductors",
        "authors": "Sergey N. Makarov, Reinhold Ludwig, Stephen J. Bitar",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-96692-2_12"
    },
    {
        "id": 22631,
        "title": "Power Transformer Differential Protection For Three Eskom Feeders",
        "authors": "Philani N Ngema, Innocent E Davidson, Elutinji Buraimoh",
        "published": "2022-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica53997.2022.9905297"
    },
    {
        "id": 22632,
        "title": "WOMAN AS A ‘TRANSFORMER’ IN PAULO COELHO’S THE ZAHIR",
        "authors": "Jagdish Joshi, Neha Hariyani",
        "published": "2020-6-30",
        "citations": 0,
        "abstract": "Paulo Coelho is an eminent Brazilian writer of the present times. His novels reflect his deep understanding of mythology and mysticism and are a blend of tradition and modernity. Woman plays a significant role in the metamorphosis of the protagonist in Coelho’s novels. This paper intends to study the role of woman in his novel The Zahir, in the light of Campbell’s theory of Initiation in his seminal work The Hero with a Thousand Faces. Campbell’s theory of Initiation accords a central position to woman. She performs various roles and becomes instrumental in bringing about the transformation of the hero. The more the hero initiates into his adventure, the more fascinating does he find the woman, changing herself in accordance with the needs of the hero as dictated by destiny Initially, woman in The Zahir seems to hold a subordinate position. She is even regarded as a mere commodity by the hero. But as the narrative develops, the role of the woman, true to Campbell’s remark undergoes a series of transfigurations, the deeper the hero initiates into his inner quest the subtler the woman becomes. From a mere commodity the woman transfigures into a guide, from a guide into a divine energy, capable of bringing about the inner transformation of the hero.",
        "link": "http://dx.doi.org/10.37867/te120318"
    },
    {
        "id": 22633,
        "title": "PAFormer: Pyramid Attention Transformer for Polyp Segmentation",
        "authors": "Zhiyong Huang, Fang Xie, WenCheng Qing, MengYao Wang, Man Liu, Daming Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAdenomatous polyps are usually the most common tumor in clinical screening for colorectal cancer. Early detection and removal of these precursor lesions have been proved to be effective in preventing many cancers and reducing the mortality rate. Therefore, intelligent polyp detection is an urgent matter, which could assist clinicians to quickly identify detected polyps at an early stage. This paper proposes a polyp segmentation method based on sequential self-attentive networks with the features such as the color and texture of images and the relationship features between them. The color and texture features of the region image are extracted and fused with the original image. The resulting feature maps are serially input to the Transformer model to learn high-level spatial and attentional features. The loss of spatial information is reduced by multiscale local aggregation. By aggregating the models from a global perspective, the transformer model can further reduce the loss of context information caused by deep convolution. Controlled experiments between the PAFormer network and the mainstream polyp segmentation networks are conducted on five datasets, including ColonDB, ETIS, CVC-ColonDB, CVC-612 and Kvasir. The results show that the segmentation accuracy of PAFormer has great improvement on the datasets and PAFormer is superior to existing mainstream baseline networks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2215007/v1"
    },
    {
        "id": 22634,
        "title": "Power Converters with Transformer Isolation",
        "authors": "Yim-Shu Lee",
        "published": "2017-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315140087-4"
    },
    {
        "id": 22635,
        "title": "Nucleic Transformer: Deep Learning on Nucleic Acids with Self-attention and Convolutions",
        "authors": "Shujun He, Baizhen Gao, Rushant Sabnis, Qing Sun",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractMuch work has been done to apply machine learning and deep learning to genomics tasks, but these applications usually require extensive domain knowledge and the resulting models provide very limited interpretability. Here we present the Nucleic Transformer, a conceptually simple but effective and interpretable model architecture that excels in a variety of DNA/RNA tasks. The Nucleic Transformer processes nucleic acid sequences with self-attention and convolutions, two deep learning techniques that have proved dominant in the fields of computer vision and natural language processing. We demonstrate that the Nucleic Transformer can be trained in both supervised and unsupervised fashion without much domain knowledge to achieve high performance with limited amounts of data in Escherichia coli promoter classification, viral genome identification, and degradation properties of COVID-19 mRNA vaccine candidates. Additionally, we showcase extraction of promoter motifs from learned attention and how direct visualization of self-attention maps assists informed decision making using deep learning models.",
        "link": "http://dx.doi.org/10.1101/2021.01.28.428629"
    },
    {
        "id": 22636,
        "title": "Low EMI Planar Transformer for an Isolated, Cascaded Buck-LLC Converter",
        "authors": "Thomas V. Cook, Brandon M. Grainger",
        "published": "2023-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero55745.2023.10115545"
    },
    {
        "id": 22637,
        "title": "Deformable Mesh Transformer for 3D Human Mesh Recovery",
        "authors": "Yusuke Yoshiyasu",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01631"
    },
    {
        "id": 22638,
        "title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering",
        "authors": "Saurabh Adya, Vineet Garg, Siddharth Sigtia, Pramod Simha, Chandra Dhir",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1330"
    },
    {
        "id": 22639,
        "title": "Efficient forward discrete wavelet transformer",
        "authors": "Goran Savic, Milan Prokin, Vladimir Rajovic, Dragana Prokin",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/meco.2017.7977186"
    },
    {
        "id": 22640,
        "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
        "authors": "Jian Luo, Jianzong Wang, Ning Cheng, Jing Xiao",
        "published": "2021-8-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1066"
    },
    {
        "id": 22641,
        "title": "Power Transformer in a Power Grid",
        "authors": "",
        "published": "2021-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119765325.ch1"
    },
    {
        "id": 22642,
        "title": "Conserver, adapter, transformer : Marcel Jousse à l’épreuve du présent",
        "authors": "Jean-Rémi Lapaire",
        "published": "2021-4-15",
        "citations": 0,
        "abstract": "Marcel Jousse (1886-1961) partage avec le linguiste français Gustave Guillaume (1883-1960) une époque, des lieux, ainsi qu’un goût marqué pour l’enseignement oral (la présence physique d’un public fidèle et attentif les aidant à formuler leurs idées). Mais surtout, les deux hommes ont le désir brûlant de mettre au jour les mécanismes – dynamiques, cohérents et intégrés – de la pensée et du langage humains. Ils y consacrent le plus clair de leur temps, inventant l’un et l’autre des discours très particuliers sur le langage. Leurs disciples se retrouvent aujourd’hui avec deux œuvres volumineuses et inclassables, difficiles à transmettre en l’état. La mise en regard de ces deux destins est utile pour nous positionner face à l’héritage complexe légué par Jousse, qu’on s’intéresse à la théorie anthropologique du style oral global ou à la technique des récitatifs. Deux options s’offrent à nous : la première, de nature patrimoniale, privilégie la conservation et l’exégèse, tandis que la seconde, évolutive, adapte les idées et modifie les pratiques originales pour les accorder au monde présent et fertiliser d’autres disciplines. En nous appuyant sur deux exemples réussis d’association de ces stratégies – la pédagogie du théâtre de Jacques Lecoq et la sémiotique du geste de Geneviève Calbris –, nous démontrons que les héritiers intellectuels de Jousse ont intérêt à combiner, non à opposer, conservation et évolution, comme l’attestent les expérimentations que nous avons pu mener avec nos propres étudiants, lors des séminaires-ateliers d’interprétation par-corps de textes littéraires en langue étrangère.",
        "link": "http://dx.doi.org/10.3917/trans.157.0049"
    },
    {
        "id": 22643,
        "title": "Digital Twin of Scott-T Connection Special Transformer",
        "authors": "Alexey Stulov, Andrey Tikhonov, Andrey Karzhevin",
        "published": "2023-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartindustrycon57312.2023.10110837"
    },
    {
        "id": 22644,
        "title": "Forecasting Stock Prices with Stack Transformer",
        "authors": "Vaishnavi Tevare, P. S. Revankar",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccpct58313.2023.10245652"
    },
    {
        "id": 22645,
        "title": "Enhancing Monotonicity for Robust Autoregressive Transformer TTS",
        "authors": "Xiangyu Liang, Zhiyong Wu, Runnan Li, Yanqing Liu, Sheng Zhao, Helen Meng",
        "published": "2020-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1751"
    },
    {
        "id": 22646,
        "title": "LLamol: A Dynamic Multi-Conditional Generative Transformer for De Novo Molecular Design",
        "authors": "Niklas David Dobberstein, Astrid Maass, Jan Hamaekers",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nGenerative models have demonstrated substantial promise in Natural Language Processing (NLP) and have found application in designing molecules, as seen in General Pretrained Transformer (GPT) models. In our efforts to develop such a tool for exploring the organic chemical space in search of potentially electro-active compounds, we present LLamol, a single novel generative transformer model based on the LLama 2 architecture, which was trained on a 13M superset of organic compounds drawn from diverse public sources. To allow for a maximum flexibility in usage and robustness in view of potentially incomplete data, we introduce Stochastic Context Learning as a new training procedure. We demonstrate that the resulting model adeptly handles single- and multi-conditional organic molecule generation with up to four conditions, yet more are possible. The model generates valid molecular structures in SMILES notation while flexibly incorporating three numerical and/or one token sequence into the generative process, just as requested. The generated compounds are very satisfactory in all scenarios tested. In detail, we showcase the model's capability to utilize token sequences for conditioning, either individually or in combination with numerical properties, making LLamol a potent tool for de novo molecule design, easily expandable with new properties. \nScientific Contribution: We developed a novel generative transformer model, LLamol, based on the LLama 2 architecture that was trained on a diverse set of 13M organic compounds. It introduces Stochastic Context Learning (SCL) as a new training procedure, allowing for flexible and robust generation of valid organic molecules with up to multiple conditions that can be combined in various ways, making it a potent tool for de novo molecular design.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3847727/v1"
    },
    {
        "id": 22647,
        "title": "Transformer-based Hierarchical Encoder for Document Classification",
        "authors": "Harsh Sakhrani, Saloni Parekh, Pratik Ratadiya",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw53433.2021.00109"
    },
    {
        "id": 22648,
        "title": "Mode de vie : de quoi parle-t-on ? Peut-on le transformer ?",
        "authors": "Bruno Maresca",
        "published": "2017-7-31",
        "citations": 2,
        "abstract": "Le mode de vie est une prénotion sociologique fortement mobilisée du fait des mutations technologiques et économiques et des débats sur la transition écologique. Sa transformation est-elle un enjeu collectif ou plutôt une responsabilité individuelle ? On éclaire cette question par une mise à plat de la polysémie des termes « genre de vie », « mode de vie », « style de vie », et l’on remonte leur généalogie pour clarifier ce concept. Le mode de vie est un système structurant et différenciant à l’intérieur duquel les styles de vie sont une dynamique de renouvellement des manières d’être. La transition écologique ne prendra un tournant décisif que si le changement se trouve engagé au cœur de la structure du mode de vie par une révolution de l’architecture du vivre-ensemble.",
        "link": "http://dx.doi.org/10.3917/lpe.pr1.0013"
    },
    {
        "id": 22649,
        "title": "Transformer-based NLoS detection in UWB localization systems",
        "authors": "Slavica Tomovic, Klemen Bregar, Tomaz Javornik, Igor Radusinovic",
        "published": "2022-11-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/telfor56187.2022.9983765"
    },
    {
        "id": 22650,
        "title": "SVG Vector Font Generation for Chinese Characters with Transformer",
        "authors": "Haruka Aoki, Kiyoharu Aizawa",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897633"
    },
    {
        "id": 22651,
        "title": "Surface Transformer for 3D Object Detection",
        "authors": "Yanfei Liu, Kanglin Ning",
        "published": "2022-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccgiv57403.2022.00038"
    },
    {
        "id": 22652,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v2/review1"
    },
    {
        "id": 22653,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v2/review2"
    },
    {
        "id": 22654,
        "title": "Transformer-based power system energy prediction model",
        "authors": "Zhuyi Rao, Yunxiang Zhang",
        "published": "2020-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec49072.2020.9141649"
    },
    {
        "id": 22655,
        "title": "Rumor Detection on Social Networks Based on Temporal Tree Transformer",
        "authors": "Sirong Wu, Yuhui Deng, Junjie Liu, Xi Luo, Gengchen Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4741613"
    },
    {
        "id": 22656,
        "title": "SyntaLinker: Automatic Fragment Linking with Deep Conditional Transformer Neural Networks",
        "authors": "Yuyao Yang, Shuangjia Zheng, Shimin Su, Jun Xu, Hongming Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Fragment based drug design represents a promising drug discovery paradigm complimentary to the traditional HTS based lead generation strategy. How to link fragment structures to increase compound affinity is remaining a challenge task in this paradigm. Hereby a novel deep generative model (SyntaLinker) for linking fragments is developed with the potential for applying in the fragment-based lead generation scenario. The state-of-the-art transformer architecture was employed to learn the linker grammar and generate novel linker. Our results show that, given starting fragments and user customized linker constraints, our SyntaLinker model can design abundant drug-like molecules fulfilling these constraints and its performance was superior to other reference models. Moreover, several examples were showcased that SyntaLinkercan be useful tools for carrying out drug design tasks such as fragment linking, lead optimization and scaffold hopping.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12271508.v4"
    },
    {
        "id": 22657,
        "title": "Text Summarization using Transformer Model",
        "authors": "Jaishree Ranganathan, Gloria Abuka",
        "published": "2022-11-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/snams58071.2022.10062698"
    },
    {
        "id": 22658,
        "title": "Attention Augmented Convolutional Transformer for Tabular Time-series",
        "authors": "Sharath M Shankaranarayana, Davor Runje",
        "published": "2021-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw53433.2021.00071"
    },
    {
        "id": 22659,
        "title": "Chapter 10: Visualization with Generative AI",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-011"
    },
    {
        "id": 22660,
        "title": "Chapter 5: The BERT Family Introduction",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-006"
    },
    {
        "id": 22661,
        "title": "Point Transformer",
        "authors": "Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun",
        "published": "2021-10",
        "citations": 662,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01595"
    },
    {
        "id": 22662,
        "title": "MSA Transformer",
        "authors": "Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",
        "published": "No Date",
        "citations": 134,
        "abstract": "AbstractUnsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evo lutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.",
        "link": "http://dx.doi.org/10.1101/2021.02.12.430858"
    },
    {
        "id": 22663,
        "title": "W‐Transformer: Accurate Cobb angles estimation by using a transformer‐based hybrid structure",
        "authors": "Yifan Yao, Wenjun Yu, Yongbin Gao, Jiuqing Dong, Qiangqiang Xiao, Bo Huang, Zhicai Shi",
        "published": "2022-5",
        "citations": 7,
        "abstract": "AbstractBackgroundScoliosis is a type of spinal deformity, which is harmful to a person's health. In severe cases, it can trigger paralysis or death. The measurement of Cobb angle plays an essential role in assessing the severity of scoliosis.PurposeThe aim of this paper is to propose an automatic system for landmark detection and Cobb angle estimation, which can effectively help clinicians diagnose and treat scoliosis.MethodsA novel hybrid framework was proposed to measure Cobb angle precisely for clinical diagnosis, which was referred as W‐Transformer due to its w‐shaped architecture. First, a convolutional neural network of cascade residual blocks as our backbone was designed. Then a transformer was fused to learn the dependency information between spine and landmarks. In addition, a reinforcement branch was designed to improve the overlap of landmarks, and an improved prediction module was proposed to fine‐tune the final coordinates of landmarks in Cobb angles estimation. Besides, the public Accurate Automated Spinal Curvature Estimation (AASCE) MICCAI 2019 challenge was served as data set. It supplies 609 manually labeled spine anterior–posterior (AP) X‐ray images, each of which contains a total of 68 landmark labels and three Cobb Angles tags.ResultsFrom the perspective of the AASCE MICCAI 2019 challenge, we achieved a lower symmetric mean absolute percentage error (SMAPE) of 8.26% for all Cobb angles and the lowest averaged detection error of 50.89 in terms of landmark detection, compared with many state‐of‐the‐art methods. We also provided the SMAPEs for the Cobb angles of the proximal‐thoracic (PT), the main‐thoracic (MT), and the thoracic‐lumbar (TL) area, which are 5.27%, 14.59%, and 20.97% respectively, however, these data were not covered in most previous studies. Statistical analysis demonstrates that our model has obtained a high level of Pearson correlation coefficient of 0.9398 (), which shows excellent reliability of our model. Our model can yield 0.9489 (), 0.8817 (), and 0.9149 () for PT, MT, and TL, respectively. The overall variability of Cobb angle measurement is less than 4, implying clinical value. And the mean absolute deviation (standard deviation) for three regions is 3.64 (4.13), 3.84 (4.66), and 3.80 (4.19). The results of Student paired ‐test indicate that no statistically significant differences are observed between manual measurement and our automatic approach (‐value is always 0.05). Regarding the diagnosis of scoliosis (Cobb angle 10), the proposed method achieves a high sensitivity of 0.9577 and a specificity of 0.8475 for all spinal regions.ConclusionsThis study offers a brand‐new automatic approach that is potentially of great benefit of the complex task of landmark detection and Cobb angle evaluation, which can provide helpful navigation information about the early diagnosis of scoliosis.",
        "link": "http://dx.doi.org/10.1002/mp.15561"
    },
    {
        "id": 22664,
        "title": "ALSI-Transformer: Transformer-Based Code Comment Generation With Aligned Lexical and Syntactic Information",
        "authors": "Youngmi Park, Ahjeong Park, Chulyun Kim",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3268638"
    },
    {
        "id": 22665,
        "title": "Remote Sensing Image Road Extraction Algorithm Based on U-Type Neural Network and Transformer Combined with CNN",
        "authors": "琳 高",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2024.141015"
    },
    {
        "id": 22666,
        "title": "Spatial-temporal transformer network for protecting person-of-interest from deepfaking",
        "authors": "Dingyu Lu, Dongming Zhang, Jing Zhang, Guoqing Jin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe rampant use of forgery techniques poses a significant threat to the security of celebrities' identities. Although current deepfake detection methods have shown effectiveness when dealing with specific public face forgery datasets, their reliability diminishes when applied to open data. Moreover, these methods are susceptible to re-compression and mainly rely on pixel-level abnormalities in forgery faces.\n\nIn this study, we present a novel approach to detecting face forgery by leveraging individual speaking patterns of facial expressions and head movements. Our method utilizes potential motion patterns and inter-frame variations to effectively differentiate between fake and real videos. We propose an end-to-end dual-branch detection network, named the spatial-temporal transformer (STT), which aims to safeguard the identity of the person-of-interest (POI) from deepfaking. The STT incorporates the spatial transformer (ST) to establish the connection between facial expressions and head movements, while the temporal transformer (TT) exploits inconsistencies in facial attribute changes. Additionally, we introduce a central compression loss to enhance the detection performance.\n\nExtensive experiments are conducted to evaluate the effectiveness of the STT, and the results demonstrate its superiority over other SOTA methods in detecting forgery videos involving POIs. Furthermore, our network exhibits resilience to pixel-level re-compression perturbations, making it a robust solution in the face of evolving forgery techniques.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2988689/v1"
    },
    {
        "id": 22667,
        "title": "Moisture analysis for power transformers",
        "authors": "Belen Garcia, Alexander Cespedes, Diego Garcia",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch3"
    },
    {
        "id": 22668,
        "title": "Struct2IUPAC -- Transformer-Based Artificial Neural Network for the Conversion Between Chemical Notations",
        "authors": "Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 1,
        "abstract": "Providing IUPAC chemical names is necessary for chemical information exchange. We developed a Transformer-based artificial neural architecture to translate between SMILES and IUPAC chemical notations: Struct2IUPAC and IUPAC2Struct. Our models demonstrated the performance that is comparable to rule-based solutions. We proved that both accuracy, speed of computations, and the model's robustness allow us to use it in production. Our showcase demonstrates that a neural-based solution can encourage rapid development keeping the same performance. We believe that our findings will inspire other developers to reduce development costs by replacing complex rule-based solutions with neural-based ones. The demonstration of Struct2IUPAC model is available online on Syntelly platform https://app.syntelly.com/smiles2iupac",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13274732.v1"
    },
    {
        "id": 22669,
        "title": "Saccade Inspired Attentive Visual Patch Transformer for Image Sentiment Analysis",
        "authors": "Jing Zhang, Jiangpei Liu, Xinzhou Zhang, Han Sun, Zhe Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685795"
    },
    {
        "id": 22670,
        "title": "TRANSFORMER BUSHINGS – FAILURE CASE STUDIES",
        "authors": " Antun Mikulecky",
        "published": "2022-7-19",
        "citations": 1,
        "abstract": "Relationship between bushing failure and transformer failure is discussed and, in regard of that, two bushing failure types are recognized: incipient bushing failure that does not result in transformer damage and terminal bushing failure having transformer failure as a consequence. It can be seen, that without applying the diagnostics, all bushing failures are terminal. Thirteen bushing failures have been analyzed regarding their cause, failure mechanism and consequences. In that sense, the ability and limitation of off line and on line diagnostics are discussed and some improvements are proposed. Some switchyard properties in the aspect of fire protection are indicated and, especially, the possible influence of rigid tubular connections on bushing failures. Beside mentioned design, service, condition diagnostics and other properties of all three condenser types of bushings are described in the paper.",
        "link": "http://dx.doi.org/10.37798/2012611-4249"
    },
    {
        "id": 22671,
        "title": "End-to-end 3D Human Pose Estimation with Transformer",
        "authors": "Bowei Zhang, Peng Cui",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956693"
    },
    {
        "id": 22672,
        "title": "Numerical model of the 10 kVA superconducting transformer",
        "authors": "Pawel Surdacki, Leszek Jaroszynski, Lukasz Wozniak",
        "published": "2017-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paee.2017.8009005"
    },
    {
        "id": 22673,
        "title": "Approximation of Hysteresis Loops of Transformer Steel Sheets",
        "authors": "Witold Mazgaj, Zbigniew Szular, Michal Sierzega",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem.2018.8442629"
    },
    {
        "id": 22674,
        "title": "An Optimized Modeling Method for Transformer Design",
        "authors": "Yingying Liang, Xiaoming Liu, Jing Jin",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asicon47005.2019.8983686"
    },
    {
        "id": 22675,
        "title": "Alternative Transformer Theory Based on Poynting’s Theorem",
        "authors": "Mansur Shakirov, Anton Tkachuk",
        "published": "2020-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icieam48468.2020.9111920"
    },
    {
        "id": 22676,
        "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit",
        "authors": "Fanfei Meng, Lele Zhang, Yu Chen, Yuxin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound algorithm while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system. According to the experiment results, we achieve up to 74\\% computational savings for both training and inference with a minimal loss of accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24680943"
    },
    {
        "id": 22677,
        "title": "Methods of Investigation and Constructional Materials",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-1"
    },
    {
        "id": 22678,
        "title": "Research Direction of Oil-immersed Transformer",
        "authors": "Yanbo Huang",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": " With the continuous development of the electric power industry in the world, the application of oil-immersed transformer in all walks of life has become more and more extensive. But the safety problem of oil-immersed transformer, environmental protection problem, heat energy recovery and utilization problem are always the three mountains in front of the power system researchers. Therefore, it is particularly important to develop a new type of transformer that can adapt to the green, innovative and sustainable development of the power system. This paper will discuss the new ideas and new directions of oil-immersed transformer research in the future from three aspects: the study of transformer oil, the recovery and utilization of transformer heat energy, and the noise reduction of transformer.",
        "link": "http://dx.doi.org/10.54097/ajst.v6i3.10164"
    },
    {
        "id": 22679,
        "title": "Characterization of Amorphous Metal Materials for High-Frequency High-Power-Density Transformer",
        "authors": "Anas Bashir-U-Din",
        "published": "2019-1-7",
        "citations": 0,
        "abstract": "In many applications such as power electronic devices, it is very desirable to employ high power-density transformers, because the available space and allowed weight are very limited. In general, operating at higher frequency would lead to smaller volume and weight of electromagnetic devices, but the core loss could increase significantly. With very low specific core loss and relatively high saturation magnetic flux density, amorphous metal (AM) materials offer great potential. This scientific research aims to study and model the AM properties for developing high performance transformers such as high efficiency and high power density. The use of Amorphous metals as a core material enables high frequency transformers to attain optimum and higher level of efficiencies. This scientific paper discusses: (i) theoretical understanding of the process of magnetization and a discussion of AM magnetic properties as are useful for design of electrical devices (ii). Characterize the AM materials and to distinguish them on the basis of their distinctive magnetic properties and their usefulness for High frequency High Power Density (HFHPD) Transformers.",
        "link": "http://dx.doi.org/10.29007/mjhr"
    },
    {
        "id": 22680,
        "title": "Molecular Transformer-aided Biocatalysed Synthesis Planning",
        "authors": "Daniel Probst, Matteo Manica, Yves Gaëtan Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, Teodoro Laino",
        "published": "No Date",
        "citations": 4,
        "abstract": "Enzyme catalysts are an integral part of green chemistry strategies towards a more sustainable and resource-efficient chemical synthesis. However, the use of enzymes on unreported substrates and their specific stereo- and regioselectivity are domain-specific knowledge factors that require decades of field experience to master. This makes the retrosynthesis of given targets with biocatalysed reactions a significant challenge. Here, we use the molecular transformer architecture to capture the latent knowledge about enzymatic activity from a large data set of publicly available biochemical reactions, extending forward reaction and retrosynthetic pathway prediction to the domain of biocatalysis. We introduce the use of a class token based on the EC classification scheme that allows to capture catalysis patterns among different enzymes belonging to the same hierarchical families. The forward prediction model achieves an accuracy of 49.6% and 62.7%, top-1 and top-5 respectively, while the single-step retrosynthetic model shows a round-trip accuracy of 39.6% and 42.6%, top-1 and top-10 respectively. Trained models and curated data are made publicly available with the hope of promoting enzymatic catalysis and making green chemistry more accessible through the use of digital technologies.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14639007.v1"
    },
    {
        "id": 22681,
        "title": "Patch Features Reconstruction Transformer for Occluded Person Re-Identification",
        "authors": "Yunbin Zhao, Songhao Zhu, Zhiwei Liang",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901670"
    },
    {
        "id": 22682,
        "title": "Inception of Jump resonance in Single phase transformer",
        "authors": "S. Poornima",
        "published": "2021-1-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnte51185.2021.9487654"
    },
    {
        "id": 22683,
        "title": "SiGra: Single-cell spatial elucidation through image-augmented graph transformer",
        "authors": "Ziyang Tang, Tonglin Zhang, Baijian Yang, Jing Su, Qianqian Song",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTThe recent advances in high-throughput molecular imaging push the spatial transcriptomics technologies to the subcellular resolution, which breaks the limitations of both single-cell RNA-seq and array-based spatial profiling. The latest released single-cell spatial transcriptomics data from NanoString CosMx and MERSCOPE platforms contains multi-channel immunohistochemistry images with rich information of cell types, functions, and morphologies of cellular compartments. In this work, we developed a novel method, Single-cell spatial elucidation through image-augmented Graph transformer (SiGra), to reveal spatial domains and enhance the substantially sparse and noisy transcriptomics data. SiGra applies hybrid graph transformers over a spatial graph that comprises high-content images and gene expressions of individual cells. SiGra outperformed state-of-the-art methods on both single-cell spatial profiles and spot-level spatial transcriptomics data from complex tissues. The inclusion of immunohistochemistry images improved the model performance by 37% (95%CI: 27% – 50%). SiGra improves the characterization of intratumor heterogeneity and intercellular communications in human lung cancer samples, meanwhile recovers the known microscopic anatomy in both human brain and mouse liver tissues. Overall, SiGra effectively integrates different spatial modality data to gain deep insights into the spatial cellular ecosystems.",
        "link": "http://dx.doi.org/10.1101/2022.08.18.504464"
    },
    {
        "id": 22684,
        "title": "Compact Transformer-Based Matching Structures for Ka-Band Power Amplifiers",
        "authors": "Valdrin Qunaj, Patrick Reynaert",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc46564.2019.9038687"
    },
    {
        "id": 22685,
        "title": "Research on Stress Reduction Model Based on Transformer",
        "authors": "",
        "published": "2022-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.12.009"
    },
    {
        "id": 22686,
        "title": "The development of transformer in vision",
        "authors": "Jialong Shao",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Transformer is a deep neural network that utilizes a self-attentive technique to process data in parallel. The area of vision was neural network based before Transformer, with great frameworks like faster-RNN, YOLO, etc. Transformer, which was developed to stop this phenomena, is discussed in this article together with its accomplishments in the field of vision during the past few years and its projections for the future in order to provide some references for more research.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/2023312"
    },
    {
        "id": 22687,
        "title": "Images Classification Integrating Transformer with Convolutional Neural Network",
        "authors": "Yulin Peng",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Convolutional neural networks (CNN) are one of the most widely used deep learning methods in computer vision, which can effectively extract local spatial information from images, but lack global understanding and dependency modelling of image features. As a result, contextual information cannot be fully utilized by the network. For example, on coordinate modelling tasks (such as object detection, image generation, etc.), CNN may not be able to accurately locate or reconstruct the position and shape of objects. In contrast to traditional CNN models such as ResNet, Transformers rely on their global attention mechanism to capture long-distance dependencies between patches. The thesis presents an enhanced lightweight method which integrates Transformer with five convolutional neural layers. Model based on CNN and Transformer is tested on the two benchmark datasets MNIST and CIFAR-10. After a few epochs, the model is convergent and reaches high accuracy of 99.34% in MNIST and 92.04% in CIFAR-10. This model outperforms the single CNN and some state-of-the-art models in classifying both datasets, especially in distinguishing similar images like ‘6’ and ‘9’, ‘bird’ and ‘plane’. These results indicate the model's good robustness and generality.",
        "link": "http://dx.doi.org/10.56028/aetr.6.1.621.2023"
    },
    {
        "id": 22688,
        "title": "Real-Time Transformer Oil Monitoring Using Planar Resonator Based Sensor",
        "authors": "Rajat Srivastava, Sangeeta Kale",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4174867"
    },
    {
        "id": 22689,
        "title": "Excitation Transformer",
        "authors": "",
        "published": "2019-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781118841006.ch12"
    },
    {
        "id": 22690,
        "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
        "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura",
        "published": "2020-10-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3033"
    },
    {
        "id": 22691,
        "title": "Msqat: A Multi-Dimension Non-Intrusive Speech Quality Assessment Transformer Utilizing Self-Supervised Representations",
        "authors": "Kailai Shen, Diqun Yan, Li Dong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466764"
    },
    {
        "id": 22692,
        "title": "A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies",
        "authors": "shivacharan oruganti, Deepa Karuppaiah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4317584"
    },
    {
        "id": 22693,
        "title": "Novel Transformer with Variable Leakage and Magnetizing Inductances",
        "authors": "Angshuman Sharma, Jonathan Kimball",
        "published": "2021-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1991834"
    },
    {
        "id": 22694,
        "title": "Virtual Synchronous Machine Control Applied to Solid State Transformer",
        "authors": "Yushi Miura, Junya Higuchi",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce50734.2022.9948189"
    },
    {
        "id": 22695,
        "title": "Task-Specific Alignment and Multiple-Level Transformer for Few-Shot Action Recognition",
        "authors": "Fei Guo, Li Zhu, YiKang Wang, Jing Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4600142"
    },
    {
        "id": 22696,
        "title": "Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity",
        "authors": "Joel Ye, Jennifer L. Collinger, Leila Wehbe, Robert Gaunt",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe neural population spiking activity recorded by intracortical brain-computer interfaces (iBCIs) contain rich structure. Current models of such spiking activity are largely prepared for individual experimental contexts, restricting data volume to that collectable within a single session and limiting the effectiveness of deep neural networks (DNNs). The purported challenge in aggregating neural spiking data is the pervasiveness of context-dependent shifts in the neural data distributions. However, large scale unsupervised pretraining by nature spans heterogeneous data, and has proven to be a fundamental recipe for successful representation learning across deep learning. We thus develop Neural Data Transformer 2 (NDT2), a spatiotemporal Transformer for neural spiking activity, and demonstrate that pretraining can leverage motor BCI datasets that span sessions, subjects, and experimental tasks. NDT2 enables rapid adaptation to novel contexts in downstream decoding tasks and opens the path to deployment of pretrained DNNs for iBCI control. Code:https://github.com/joel99/context_general_bci",
        "link": "http://dx.doi.org/10.1101/2023.09.18.558113"
    },
    {
        "id": 22697,
        "title": "Grouptransnet: Group Transformer Network for Rgb-D Salient Object Detection",
        "authors": "Xian Fang, Jiang Mingfeng, Jinchao Zhu, Xiuli Shao, Hongpeng Wang",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4585918"
    },
    {
        "id": 22698,
        "title": "Faster Boundary-aware Transformer for Breast Cancer Segmentation",
        "authors": "Xin Zhou, Xiaoxia Yin",
        "published": "2023-5-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaci58115.2023.10146176"
    },
    {
        "id": 22699,
        "title": "Exploring transformer fault detection using RFID technology",
        "authors": "Xiaomeng Li",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Real-time monitoring and fault diagnosis of transformers are essential for the stable power system operation. This paper presents an RFID-based transformer fault feature extraction and classification algorithm. Experiments show that monitored current signals are stable while the temperature peak is 356°C. Hilbert decomposition reveals regular current and voltage patterns that can be used as fault indicators. Signal strength classification accuracy reached 80% . At rated load, the transformer temperature soared to 186°C, indicating overheating issues. The monitoring during a sample day showed that overload events were concentrated from 16:00-20:00, which required attention. The approach helps accurately identify transformer fault types from real-time RFID data for proactive maintenance. Compared to reactive repairs after failures, this not only improves employee productivity but also reduces costs. Based on customized RFID deployment, the algorithm contributes to the stability and economy of power infrastructure.",
        "link": "http://dx.doi.org/10.3233/rft-230056"
    },
    {
        "id": 22700,
        "title": "Black start in distribution grids through solid-state transformer",
        "authors": "M. Couto, A. Coccia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0555"
    },
    {
        "id": 22701,
        "title": "Simple Exponential Smoothing for Forecasting the Numbers of Pole-Mounted Transformer Failures",
        "authors": "Nhlanhla Mbuli, Jan-Harm C. Pretorius",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/africon55910.2023.10293359"
    },
    {
        "id": 22702,
        "title": "Waveglove: Transformer-Based Hand Gesture Recognition Using Multiple Inertial Sensors",
        "authors": "Matej Kralik, Marek Suppa",
        "published": "2021-8-23",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco54536.2021.9616000"
    },
    {
        "id": 22703,
        "title": "Sparse Mix-Attention Transformer for Multispectral Image and Hyperspectral Image Fusion",
        "authors": "Shihai Yu, Xu Zhang, Huihui Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "Multispectral image (MSI) and hyperspectral image (HSI) fusion (MHIF) aims to address the challenge of acquiring high-resolution (HR) HSI images. This field combines a low-resolution (LR) HSI with an HR-MSI to reconstruct HR-HSI. Existing methods directly utilize transformers to perform feature extraction and fusion. Despite the demonstrated success, there exist two limitations: 1) Employing the entire transformer model for feature extraction and fusion fails to fully harness the transformer’s potential in integrating the spectral of the HSI and spatial information of the MSI. 2) HSI has a strong spectral correlation and exhibits sparsity in the spatial domain. Existing transformer-based models do not optimize this physical property, which makes their methods prone to spectral distortion. To accomplish these issues, this paper introduces a novel framework for MHIF called Sparse Mix-Attention Transformer (SAMformer). Specifically, to fully harness the advantages of the Transformer architecture, we propose a Spectral Mix Attention Block (SMAB), which concatenates the keys and values extracted from LR-HSI and HR-MSI to create a new multi-head attention module. This design facilitates the extraction of detailed long-range information across spatial and spectral dimensions. Besides, to address the spatial sparsity inherent in HSI, we incorporated a sparse mechanism within the core of SMAB called Sparse Spectral Mix Attention Block (SSMAB). In the SSMAB, we compute attention maps from queries and keys and select the K highly correlated values as the sparse attention map. This approach enables us to achieve a sparse representation of spatial information while eliminating spatially disruptive noise. Extensive experiments conducted on three benchmark datasets, namely Cave, Harvard, and Pavia Center, demonstrate the SMAformer method outperforms state-of-the-art methods.",
        "link": "http://dx.doi.org/10.20944/preprints202311.0343.v1"
    },
    {
        "id": 22704,
        "title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation",
        "authors": "Keqi Deng, Shinji Watanabe, Jiatong Shi, Siddhant Arora",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-933"
    },
    {
        "id": 22705,
        "title": "Position Transformer for Image Retrieval",
        "authors": "Hao Tian, Liufan Wu",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009832"
    },
    {
        "id": 22706,
        "title": "Transformer-Based Monocular Depth Estimation with Hybrid Attention Fusion and Progressive Regression",
        "authors": "Peng Liu, Zonghua Zhang, Zhaozong Meng, Nan Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4710730"
    },
    {
        "id": 22707,
        "title": "An Improved LA-Transformer Machine Translation Model",
        "authors": "Zumin Wang, Chengye Zhang, Fengbo Bai, Yingjie Wang",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10448708"
    },
    {
        "id": 22708,
        "title": "Smart Transformer Connected to Medium Voltage Grid Through LCL Filter",
        "authors": "Hiba Helali, Adel Khedher",
        "published": "2020-10-29",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irec48820.2020.9310435"
    },
    {
        "id": 22709,
        "title": "A Transformer-Based Audio Captioning Model with Keyword Estimation",
        "authors": "Yuma Koizumi, Ryo Masumura, Kyosuke Nishida, Masahiro Yasuda, Shoichiro Saito",
        "published": "2020-10-25",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2087"
    },
    {
        "id": 22710,
        "title": "Transformer-Based Weed Segmentation for Grass Management",
        "authors": "Kan Jiang, Usman Afzaal, Joonwhoan Lee",
        "published": "2022-12-21",
        "citations": 11,
        "abstract": "Weed control is among the most challenging issues for crop cultivation and turf grass management. In addition to hosting various insects and plant pathogens, weeds compete with crop for nutrients, water and sunlight. This results in problems such as the loss of crop yield, the contamination of food crops and disruption in the field aesthetics and practicality. Therefore, effective and efficient weed detection and mapping methods are indispensable. Deep learning (DL) techniques for the rapid recognition and localization of objects from images or videos have shown promising results in various areas of interest, including the agricultural sector. Attention-based Transformer models are a promising alternative to traditional constitutional neural networks (CNNs) and offer state-of-the-art results for multiple tasks in the natural language processing (NLP) domain. To this end, we exploited these models to address the aforementioned weed detection problem with potential applications in automated robots. Our weed dataset comprised of 1006 images for 10 weed classes, which allowed us to develop deep learning-based semantic segmentation models for the localization of these weed classes. The dataset was further augmented to cater for the need of a large sample set of the Transformer models. A study was conducted to evaluate the results of three types of Transformer architectures, which included Swin Transformer, SegFormer and Segmenter, on the dataset, with SegFormer achieving final Mean Accuracy (mAcc) and Mean Intersection of Union (mIoU) of 75.18% and 65.74%, while also being the least computationally expensive, with just 3.7 M parameters.",
        "link": "http://dx.doi.org/10.3390/s23010065"
    },
    {
        "id": 22711,
        "title": "Hierarchical Transformer for Brain Computer Interface",
        "authors": "Permana Deny, Kae Won Choi",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bci57258.2023.10078473"
    },
    {
        "id": 22712,
        "title": "Depth Inpainting via Vision Transformer",
        "authors": "Ilya Makarov, Gleb Borisenko",
        "published": "2021-10",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismar-adjunct54149.2021.00065"
    },
    {
        "id": 22713,
        "title": "W‐Transformer: Accurate Cobb angles estimation by using a transformer‐based hybrid structure",
        "authors": "Yifan Yao, Wenjun Yu, Yongbin Gao, Jiuqing Dong, Qiangqiang Xiao, Bo Huang, Zhicai Shi",
        "published": "2022-5",
        "citations": 7,
        "abstract": "AbstractBackgroundScoliosis is a type of spinal deformity, which is harmful to a person's health. In severe cases, it can trigger paralysis or death. The measurement of Cobb angle plays an essential role in assessing the severity of scoliosis.PurposeThe aim of this paper is to propose an automatic system for landmark detection and Cobb angle estimation, which can effectively help clinicians diagnose and treat scoliosis.MethodsA novel hybrid framework was proposed to measure Cobb angle precisely for clinical diagnosis, which was referred as W‐Transformer due to its w‐shaped architecture. First, a convolutional neural network of cascade residual blocks as our backbone was designed. Then a transformer was fused to learn the dependency information between spine and landmarks. In addition, a reinforcement branch was designed to improve the overlap of landmarks, and an improved prediction module was proposed to fine‐tune the final coordinates of landmarks in Cobb angles estimation. Besides, the public Accurate Automated Spinal Curvature Estimation (AASCE) MICCAI 2019 challenge was served as data set. It supplies 609 manually labeled spine anterior–posterior (AP) X‐ray images, each of which contains a total of 68 landmark labels and three Cobb Angles tags.ResultsFrom the perspective of the AASCE MICCAI 2019 challenge, we achieved a lower symmetric mean absolute percentage error (SMAPE) of 8.26% for all Cobb angles and the lowest averaged detection error of 50.89 in terms of landmark detection, compared with many state‐of‐the‐art methods. We also provided the SMAPEs for the Cobb angles of the proximal‐thoracic (PT), the main‐thoracic (MT), and the thoracic‐lumbar (TL) area, which are 5.27%, 14.59%, and 20.97% respectively, however, these data were not covered in most previous studies. Statistical analysis demonstrates that our model has obtained a high level of Pearson correlation coefficient of 0.9398 (), which shows excellent reliability of our model. Our model can yield 0.9489 (), 0.8817 (), and 0.9149 () for PT, MT, and TL, respectively. The overall variability of Cobb angle measurement is less than 4, implying clinical value. And the mean absolute deviation (standard deviation) for three regions is 3.64 (4.13), 3.84 (4.66), and 3.80 (4.19). The results of Student paired ‐test indicate that no statistically significant differences are observed between manual measurement and our automatic approach (‐value is always 0.05). Regarding the diagnosis of scoliosis (Cobb angle 10), the proposed method achieves a high sensitivity of 0.9577 and a specificity of 0.8475 for all spinal regions.ConclusionsThis study offers a brand‐new automatic approach that is potentially of great benefit of the complex task of landmark detection and Cobb angle evaluation, which can provide helpful navigation information about the early diagnosis of scoliosis.",
        "link": "http://dx.doi.org/10.1002/mp.15561"
    },
    {
        "id": 22714,
        "title": "ALSI-Transformer: Transformer-Based Code Comment Generation With Aligned Lexical and Syntactic Information",
        "authors": "Youngmi Park, Ahjeong Park, Chulyun Kim",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3268638"
    },
    {
        "id": 22715,
        "title": "Remote Sensing Image Road Extraction Algorithm Based on U-Type Neural Network and Transformer Combined with CNN",
        "authors": "琳 高",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2024.141015"
    },
    {
        "id": 22716,
        "title": "Spatial-temporal transformer network for protecting person-of-interest from deepfaking",
        "authors": "Dingyu Lu, Dongming Zhang, Jing Zhang, Guoqing Jin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe rampant use of forgery techniques poses a significant threat to the security of celebrities' identities. Although current deepfake detection methods have shown effectiveness when dealing with specific public face forgery datasets, their reliability diminishes when applied to open data. Moreover, these methods are susceptible to re-compression and mainly rely on pixel-level abnormalities in forgery faces.\n\nIn this study, we present a novel approach to detecting face forgery by leveraging individual speaking patterns of facial expressions and head movements. Our method utilizes potential motion patterns and inter-frame variations to effectively differentiate between fake and real videos. We propose an end-to-end dual-branch detection network, named the spatial-temporal transformer (STT), which aims to safeguard the identity of the person-of-interest (POI) from deepfaking. The STT incorporates the spatial transformer (ST) to establish the connection between facial expressions and head movements, while the temporal transformer (TT) exploits inconsistencies in facial attribute changes. Additionally, we introduce a central compression loss to enhance the detection performance.\n\nExtensive experiments are conducted to evaluate the effectiveness of the STT, and the results demonstrate its superiority over other SOTA methods in detecting forgery videos involving POIs. Furthermore, our network exhibits resilience to pixel-level re-compression perturbations, making it a robust solution in the face of evolving forgery techniques.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2988689/v1"
    },
    {
        "id": 22717,
        "title": "Moisture analysis for power transformers",
        "authors": "Belen Garcia, Alexander Cespedes, Diego Garcia",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch3"
    },
    {
        "id": 22718,
        "title": "Struct2IUPAC -- Transformer-Based Artificial Neural Network for the Conversion Between Chemical Notations",
        "authors": "Lev Krasnov, Ivan Khokhlov, Maxim Fedorov, Sergey Sosnin",
        "published": "No Date",
        "citations": 1,
        "abstract": "Providing IUPAC chemical names is necessary for chemical information exchange. We developed a Transformer-based artificial neural architecture to translate between SMILES and IUPAC chemical notations: Struct2IUPAC and IUPAC2Struct. Our models demonstrated the performance that is comparable to rule-based solutions. We proved that both accuracy, speed of computations, and the model's robustness allow us to use it in production. Our showcase demonstrates that a neural-based solution can encourage rapid development keeping the same performance. We believe that our findings will inspire other developers to reduce development costs by replacing complex rule-based solutions with neural-based ones. The demonstration of Struct2IUPAC model is available online on Syntelly platform https://app.syntelly.com/smiles2iupac",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13274732.v1"
    },
    {
        "id": 22719,
        "title": "Saccade Inspired Attentive Visual Patch Transformer for Image Sentiment Analysis",
        "authors": "Jing Zhang, Jiangpei Liu, Xinzhou Zhang, Han Sun, Zhe Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685795"
    },
    {
        "id": 22720,
        "title": "TRANSFORMER BUSHINGS – FAILURE CASE STUDIES",
        "authors": " Antun Mikulecky",
        "published": "2022-7-19",
        "citations": 1,
        "abstract": "Relationship between bushing failure and transformer failure is discussed and, in regard of that, two bushing failure types are recognized: incipient bushing failure that does not result in transformer damage and terminal bushing failure having transformer failure as a consequence. It can be seen, that without applying the diagnostics, all bushing failures are terminal. Thirteen bushing failures have been analyzed regarding their cause, failure mechanism and consequences. In that sense, the ability and limitation of off line and on line diagnostics are discussed and some improvements are proposed. Some switchyard properties in the aspect of fire protection are indicated and, especially, the possible influence of rigid tubular connections on bushing failures. Beside mentioned design, service, condition diagnostics and other properties of all three condenser types of bushings are described in the paper.",
        "link": "http://dx.doi.org/10.37798/2012611-4249"
    },
    {
        "id": 22721,
        "title": "End-to-end 3D Human Pose Estimation with Transformer",
        "authors": "Bowei Zhang, Peng Cui",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956693"
    },
    {
        "id": 22722,
        "title": "Numerical model of the 10 kVA superconducting transformer",
        "authors": "Pawel Surdacki, Leszek Jaroszynski, Lukasz Wozniak",
        "published": "2017-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paee.2017.8009005"
    },
    {
        "id": 22723,
        "title": "Approximation of Hysteresis Loops of Transformer Steel Sheets",
        "authors": "Witold Mazgaj, Zbigniew Szular, Michal Sierzega",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem.2018.8442629"
    },
    {
        "id": 22724,
        "title": "An Optimized Modeling Method for Transformer Design",
        "authors": "Yingying Liang, Xiaoming Liu, Jing Jin",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asicon47005.2019.8983686"
    },
    {
        "id": 22725,
        "title": "Alternative Transformer Theory Based on Poynting’s Theorem",
        "authors": "Mansur Shakirov, Anton Tkachuk",
        "published": "2020-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icieam48468.2020.9111920"
    },
    {
        "id": 22726,
        "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit",
        "authors": "Fanfei Meng, Lele Zhang, Yu Chen, Yuxin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound algorithm while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system. According to the experiment results, we achieve up to 74\\% computational savings for both training and inference with a minimal loss of accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24680943"
    },
    {
        "id": 22727,
        "title": "Methods of Investigation and Constructional Materials",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-1"
    },
    {
        "id": 22728,
        "title": "Research Direction of Oil-immersed Transformer",
        "authors": "Yanbo Huang",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": " With the continuous development of the electric power industry in the world, the application of oil-immersed transformer in all walks of life has become more and more extensive. But the safety problem of oil-immersed transformer, environmental protection problem, heat energy recovery and utilization problem are always the three mountains in front of the power system researchers. Therefore, it is particularly important to develop a new type of transformer that can adapt to the green, innovative and sustainable development of the power system. This paper will discuss the new ideas and new directions of oil-immersed transformer research in the future from three aspects: the study of transformer oil, the recovery and utilization of transformer heat energy, and the noise reduction of transformer.",
        "link": "http://dx.doi.org/10.54097/ajst.v6i3.10164"
    },
    {
        "id": 22729,
        "title": "Characterization of Amorphous Metal Materials for High-Frequency High-Power-Density Transformer",
        "authors": "Anas Bashir-U-Din",
        "published": "2019-1-7",
        "citations": 0,
        "abstract": "In many applications such as power electronic devices, it is very desirable to employ high power-density transformers, because the available space and allowed weight are very limited. In general, operating at higher frequency would lead to smaller volume and weight of electromagnetic devices, but the core loss could increase significantly. With very low specific core loss and relatively high saturation magnetic flux density, amorphous metal (AM) materials offer great potential. This scientific research aims to study and model the AM properties for developing high performance transformers such as high efficiency and high power density. The use of Amorphous metals as a core material enables high frequency transformers to attain optimum and higher level of efficiencies. This scientific paper discusses: (i) theoretical understanding of the process of magnetization and a discussion of AM magnetic properties as are useful for design of electrical devices (ii). Characterize the AM materials and to distinguish them on the basis of their distinctive magnetic properties and their usefulness for High frequency High Power Density (HFHPD) Transformers.",
        "link": "http://dx.doi.org/10.29007/mjhr"
    },
    {
        "id": 22730,
        "title": "Molecular Transformer-aided Biocatalysed Synthesis Planning",
        "authors": "Daniel Probst, Matteo Manica, Yves Gaëtan Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, Teodoro Laino",
        "published": "No Date",
        "citations": 4,
        "abstract": "Enzyme catalysts are an integral part of green chemistry strategies towards a more sustainable and resource-efficient chemical synthesis. However, the use of enzymes on unreported substrates and their specific stereo- and regioselectivity are domain-specific knowledge factors that require decades of field experience to master. This makes the retrosynthesis of given targets with biocatalysed reactions a significant challenge. Here, we use the molecular transformer architecture to capture the latent knowledge about enzymatic activity from a large data set of publicly available biochemical reactions, extending forward reaction and retrosynthetic pathway prediction to the domain of biocatalysis. We introduce the use of a class token based on the EC classification scheme that allows to capture catalysis patterns among different enzymes belonging to the same hierarchical families. The forward prediction model achieves an accuracy of 49.6% and 62.7%, top-1 and top-5 respectively, while the single-step retrosynthetic model shows a round-trip accuracy of 39.6% and 42.6%, top-1 and top-10 respectively. Trained models and curated data are made publicly available with the hope of promoting enzymatic catalysis and making green chemistry more accessible through the use of digital technologies.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14639007.v1"
    },
    {
        "id": 22731,
        "title": "Patch Features Reconstruction Transformer for Occluded Person Re-Identification",
        "authors": "Yunbin Zhao, Songhao Zhu, Zhiwei Liang",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901670"
    },
    {
        "id": 22732,
        "title": "Inception of Jump resonance in Single phase transformer",
        "authors": "S. Poornima",
        "published": "2021-1-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnte51185.2021.9487654"
    },
    {
        "id": 22733,
        "title": "SiGra: Single-cell spatial elucidation through image-augmented graph transformer",
        "authors": "Ziyang Tang, Tonglin Zhang, Baijian Yang, Jing Su, Qianqian Song",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTThe recent advances in high-throughput molecular imaging push the spatial transcriptomics technologies to the subcellular resolution, which breaks the limitations of both single-cell RNA-seq and array-based spatial profiling. The latest released single-cell spatial transcriptomics data from NanoString CosMx and MERSCOPE platforms contains multi-channel immunohistochemistry images with rich information of cell types, functions, and morphologies of cellular compartments. In this work, we developed a novel method, Single-cell spatial elucidation through image-augmented Graph transformer (SiGra), to reveal spatial domains and enhance the substantially sparse and noisy transcriptomics data. SiGra applies hybrid graph transformers over a spatial graph that comprises high-content images and gene expressions of individual cells. SiGra outperformed state-of-the-art methods on both single-cell spatial profiles and spot-level spatial transcriptomics data from complex tissues. The inclusion of immunohistochemistry images improved the model performance by 37% (95%CI: 27% – 50%). SiGra improves the characterization of intratumor heterogeneity and intercellular communications in human lung cancer samples, meanwhile recovers the known microscopic anatomy in both human brain and mouse liver tissues. Overall, SiGra effectively integrates different spatial modality data to gain deep insights into the spatial cellular ecosystems.",
        "link": "http://dx.doi.org/10.1101/2022.08.18.504464"
    },
    {
        "id": 22734,
        "title": "Compact Transformer-Based Matching Structures for Ka-Band Power Amplifiers",
        "authors": "Valdrin Qunaj, Patrick Reynaert",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc46564.2019.9038687"
    },
    {
        "id": 22735,
        "title": "Research on Stress Reduction Model Based on Transformer",
        "authors": "",
        "published": "2022-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.12.009"
    },
    {
        "id": 22736,
        "title": "The development of transformer in vision",
        "authors": "Jialong Shao",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Transformer is a deep neural network that utilizes a self-attentive technique to process data in parallel. The area of vision was neural network based before Transformer, with great frameworks like faster-RNN, YOLO, etc. Transformer, which was developed to stop this phenomena, is discussed in this article together with its accomplishments in the field of vision during the past few years and its projections for the future in order to provide some references for more research.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/2023312"
    },
    {
        "id": 22737,
        "title": "Images Classification Integrating Transformer with Convolutional Neural Network",
        "authors": "Yulin Peng",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Convolutional neural networks (CNN) are one of the most widely used deep learning methods in computer vision, which can effectively extract local spatial information from images, but lack global understanding and dependency modelling of image features. As a result, contextual information cannot be fully utilized by the network. For example, on coordinate modelling tasks (such as object detection, image generation, etc.), CNN may not be able to accurately locate or reconstruct the position and shape of objects. In contrast to traditional CNN models such as ResNet, Transformers rely on their global attention mechanism to capture long-distance dependencies between patches. The thesis presents an enhanced lightweight method which integrates Transformer with five convolutional neural layers. Model based on CNN and Transformer is tested on the two benchmark datasets MNIST and CIFAR-10. After a few epochs, the model is convergent and reaches high accuracy of 99.34% in MNIST and 92.04% in CIFAR-10. This model outperforms the single CNN and some state-of-the-art models in classifying both datasets, especially in distinguishing similar images like ‘6’ and ‘9’, ‘bird’ and ‘plane’. These results indicate the model's good robustness and generality.",
        "link": "http://dx.doi.org/10.56028/aetr.6.1.621.2023"
    },
    {
        "id": 22738,
        "title": "Real-Time Transformer Oil Monitoring Using Planar Resonator Based Sensor",
        "authors": "Rajat Srivastava, Sangeeta Kale",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4174867"
    },
    {
        "id": 22739,
        "title": "Excitation Transformer",
        "authors": "",
        "published": "2019-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781118841006.ch12"
    },
    {
        "id": 22740,
        "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
        "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura",
        "published": "2020-10-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3033"
    },
    {
        "id": 22741,
        "title": "Msqat: A Multi-Dimension Non-Intrusive Speech Quality Assessment Transformer Utilizing Self-Supervised Representations",
        "authors": "Kailai Shen, Diqun Yan, Li Dong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466764"
    },
    {
        "id": 22742,
        "title": "A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies",
        "authors": "shivacharan oruganti, Deepa Karuppaiah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4317584"
    },
    {
        "id": 22743,
        "title": "Novel Transformer with Variable Leakage and Magnetizing Inductances",
        "authors": "Angshuman Sharma, Jonathan Kimball",
        "published": "2021-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1991834"
    },
    {
        "id": 22744,
        "title": "Virtual Synchronous Machine Control Applied to Solid State Transformer",
        "authors": "Yushi Miura, Junya Higuchi",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce50734.2022.9948189"
    },
    {
        "id": 22745,
        "title": "Task-Specific Alignment and Multiple-Level Transformer for Few-Shot Action Recognition",
        "authors": "Fei Guo, Li Zhu, YiKang Wang, Jing Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4600142"
    },
    {
        "id": 22746,
        "title": "Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity",
        "authors": "Joel Ye, Jennifer L. Collinger, Leila Wehbe, Robert Gaunt",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe neural population spiking activity recorded by intracortical brain-computer interfaces (iBCIs) contain rich structure. Current models of such spiking activity are largely prepared for individual experimental contexts, restricting data volume to that collectable within a single session and limiting the effectiveness of deep neural networks (DNNs). The purported challenge in aggregating neural spiking data is the pervasiveness of context-dependent shifts in the neural data distributions. However, large scale unsupervised pretraining by nature spans heterogeneous data, and has proven to be a fundamental recipe for successful representation learning across deep learning. We thus develop Neural Data Transformer 2 (NDT2), a spatiotemporal Transformer for neural spiking activity, and demonstrate that pretraining can leverage motor BCI datasets that span sessions, subjects, and experimental tasks. NDT2 enables rapid adaptation to novel contexts in downstream decoding tasks and opens the path to deployment of pretrained DNNs for iBCI control. Code:https://github.com/joel99/context_general_bci",
        "link": "http://dx.doi.org/10.1101/2023.09.18.558113"
    },
    {
        "id": 22747,
        "title": "Grouptransnet: Group Transformer Network for Rgb-D Salient Object Detection",
        "authors": "Xian Fang, Jiang Mingfeng, Jinchao Zhu, Xiuli Shao, Hongpeng Wang",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4585918"
    },
    {
        "id": 22748,
        "title": "Faster Boundary-aware Transformer for Breast Cancer Segmentation",
        "authors": "Xin Zhou, Xiaoxia Yin",
        "published": "2023-5-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaci58115.2023.10146176"
    },
    {
        "id": 22749,
        "title": "Exploring transformer fault detection using RFID technology",
        "authors": "Xiaomeng Li",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Real-time monitoring and fault diagnosis of transformers are essential for the stable power system operation. This paper presents an RFID-based transformer fault feature extraction and classification algorithm. Experiments show that monitored current signals are stable while the temperature peak is 356°C. Hilbert decomposition reveals regular current and voltage patterns that can be used as fault indicators. Signal strength classification accuracy reached 80% . At rated load, the transformer temperature soared to 186°C, indicating overheating issues. The monitoring during a sample day showed that overload events were concentrated from 16:00-20:00, which required attention. The approach helps accurately identify transformer fault types from real-time RFID data for proactive maintenance. Compared to reactive repairs after failures, this not only improves employee productivity but also reduces costs. Based on customized RFID deployment, the algorithm contributes to the stability and economy of power infrastructure.",
        "link": "http://dx.doi.org/10.3233/rft-230056"
    },
    {
        "id": 22750,
        "title": "Black start in distribution grids through solid-state transformer",
        "authors": "M. Couto, A. Coccia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0555"
    },
    {
        "id": 22751,
        "title": "Field Validated Dynamic Thermal Model for Power Transformer Insulation System Assessment",
        "authors": "Mohamed Ryadi, Alain Tanguy",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481065"
    },
    {
        "id": 22752,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/review1"
    },
    {
        "id": 22753,
        "title": "Transformer-Based Turkish Automatic Speech Recognition",
        "authors": "Davut Taşar, Kutan Koruyan, Cihan Çılgın",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26650/acin.1338604"
    },
    {
        "id": 22754,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/review2"
    },
    {
        "id": 22755,
        "title": "Peer Review #2 of \"A pan-sharpening network using multi-resolution transformer and two-stage feature fusion (v0.1)\"",
        "authors": "DS Venuji Renuka",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1488v0.1/reviews/2"
    },
    {
        "id": 22756,
        "title": "Efficient Seismic Facies Classification Using Transformer-based Masked Autoencoders",
        "authors": "M. Alfarhan, C. Birnie, T. Alkhalifah",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310319"
    },
    {
        "id": 22757,
        "title": "Convtransgan: A Memory-Friendly Combination of Convolution and Transformer for 3d Ct/Mri Translation",
        "authors": "Ji Ma, Yetao Xie, Jinjin Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760354"
    },
    {
        "id": 22758,
        "title": "Classification of Seismic Events Using Density Based Clustering and Transformer Neural Networks",
        "authors": "Luis Delgado, Billy Peralta, Orietta Nicolis, Mailiu Díaz",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760943"
    },
    {
        "id": 22759,
        "title": "OBJECT DETECTION VIA ALTERNATIVE TRANSFORMER",
        "authors": "Neychev Radoslav, Arman Stepanyan",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "Transformer is a great building block for state-of-the-art models for\nevery direction in artificial intelligence (AI). In natural language processing,\nGPT3 is one of the leading language models, and its fine-tuning leads to the\ncreation of automation products in various fields (chatting, analysis of data,\ncontent generation, etc.). In computer vision, it is DALLEE-2 with its fantastic\ncapability to generate realistic art images with the desired style. In\nreinforcement learning (RL) decision transformers [1] are widely used and\nachieve great results in such RL baseline games as ATARI, Key-To-Door tasks,\netc. Even though modern transformer blocks for end-to-end object detection\ntasks converge very slowly, which makes the training process\ncomputationally hard. We introduce alternative transformers improving\narchitecture and training process, reducing convergence and training time\nwith achieving same results in object detection tasks. Training process is\nparallelized, and a loss function is modified to increase model’s capability in\nmultiple tasks. Finally, this architecture can be used as a building block in\nother models, improving their performance",
        "link": "http://dx.doi.org/10.56246/18294480-2023.15-12"
    },
    {
        "id": 22760,
        "title": "Frequency Response Patterns of Transformer Windings with Mechanical Faults",
        "authors": "Szymon BANASZAK",
        "published": "2017-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2017.04.29"
    },
    {
        "id": 22761,
        "title": "D-REX: Static Detection of Relevant Runtime Exceptions with Location Aware Transformer",
        "authors": "Farima Farmahinifarahani, Yadong Lu, Vaibhav Saini, Pierre Baldi, Cristina Lopes",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.613b54401459512fce6a7d03"
    },
    {
        "id": 22762,
        "title": "Turbo Generator Concept Without Winding Overhang Using a Multiphase Transformer",
        "authors": "H. Schroeder, B. Ponick",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icelmach.2018.8506790"
    },
    {
        "id": 22763,
        "title": "Graph-to-Graph Transformer for Transition-based Dependency Parsing",
        "authors": "Alireza Mohammadshahi, James Henderson",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.294"
    },
    {
        "id": 22764,
        "title": "Optical Chromatic Monitoring of High-Voltage Transformer Insulating Oils",
        "authors": "A. T. Sufian, E. Elzagzoug, D. H. Smith",
        "published": "2020-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367815202-7"
    },
    {
        "id": 22765,
        "title": "Transformer tank rupture - A protection engineer's challenge",
        "authors": "Tom Roseberg, Ilija Jankovic, Roger Hedding",
        "published": "2017-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2017.8090054"
    },
    {
        "id": 22766,
        "title": "Emo-Tts:Parallel Transformer-based Text-to-Speech Model with Emotional Awareness",
        "authors": "Mohamed Osman",
        "published": "2022-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icci54321.2022.9756092"
    },
    {
        "id": 22767,
        "title": "Solar Resource for a Compact Absorption Heat Transformer in Mexico",
        "authors": "L.A. López-Pérez, Huicochea Armando",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4176003"
    },
    {
        "id": 22768,
        "title": "EFFECT OF THE ASYMMETRICAL AXIAL DISPLACEMENT OF TRANSFORMER WINDINGS ON FRA CHARACTERISTICS",
        "authors": "",
        "published": "2020-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31838/jcr.07.02.89"
    },
    {
        "id": 22769,
        "title": "The new hermetic power transformer generation “HERMETIC 2.0”",
        "authors": "T. Stirl, M. Saravolac, J. Harthun",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/insucon.2017.8097203"
    },
    {
        "id": 22770,
        "title": "Transformer Networks and Their Real-World Business Applications",
        "authors": "Divam Gupta, Param Gupta, Medha Goyal",
        "published": "2022-7",
        "citations": 0,
        "abstract": "Recent advances in deep learning models have presented many opportunities for businesses. This article focuses on the possible game changing development of transformer networks which have enabled self-supervised learning. These advances provide encouraging opportunities for business applications. The article discusses the different types of learning paradigms and the similarities and dissimilarities between them. The article also discusses how transformer networks enable self-supervised learning. The article finally discusses real-life business applications with data from text, audio and images.",
        "link": "http://dx.doi.org/10.1177/jmrt.221110955"
    },
    {
        "id": 22771,
        "title": "Validation of model for medium voltage distribution transformer under inrush current conditions",
        "authors": "J. Naidoo, A.G. Swanson",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2017.8007696"
    },
    {
        "id": 22772,
        "title": "Session-aware Item-combination Recommendation with Transformer Network",
        "authors": "Tzu-Heng Lin, Chen Gao",
        "published": "2021-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata52589.2021.9671485"
    },
    {
        "id": 22773,
        "title": "ANALYSIS OF HARMONICS REDUCTION METHOD SELECTION FOR TRANSFORMER SUBSTATION",
        "authors": "Szymon Racewicz, Mateusz Rokicki",
        "published": "2019-10-7",
        "citations": 0,
        "abstract": "The article presents an investigation of the harmonic level and its possible reduction methods for one of the Michelin factory transformer substation. The substation electrical network consists of two transformers (1.6 MVA and 2.0 MVA) supplying a production line composed of several electrical equipment based on DC and AC motors. In order to investigate the harmonics level and its influence on the substation operation the measurements of the current and the load factor of the powered machines as well as the coefficients THDu, THDi, Du and Di have been performed using the 3-phase energy quality analyzer Fluke 435II and the network parameter recorder PEL103. Basing on the measurements data the four harmonics reduction methods (passive filters, active filters, 12-pulse rectifier and the Active Front End system) have been proposed and studied. For this purpose the substation electrical network has been modelled using Emerson Harmonics Estymator software. Furthermore, in order to choose the optimal solution the financial analysis of the potential investments has been performed.",
        "link": "http://dx.doi.org/10.31648/ts.5331"
    },
    {
        "id": 22774,
        "title": "Edgeformer: Edge-Enhanced Transformer for High-Quality Image Deblurring",
        "authors": "Yuan Zou, Yinyao Ma",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00093"
    },
    {
        "id": 22775,
        "title": "Transformer Networks for Trajectory Classification",
        "authors": "Keywoong Bae, Suan Lee, Wookey Lee",
        "published": "2022-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigcomp54360.2022.00074"
    },
    {
        "id": 22776,
        "title": "DIELKOMETRICAL TRANSFORMER OIL HUMIDITY SENSOR",
        "authors": "Valerii Hraniak, Ivan Voznitskyi",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "It is known that water is part of the vast majority of organic and inorganic materials. Materials formed in natural conditions or obtained in the production process, as a rule, contain a certain amount of water in their composition, the mass fraction of which depends both on the ability of the material to absorb (sorb) or retain on the surface (adsorb) water, and on the conditions, in which this phenomenon takes place.\nMoisture content significantly affects the physical and electrical properties of non-metallic materials, including transformer oil. In particular, the increase in moisture content in the latter not only significantly reduces its dielectric strength, creating prerequisites for a breakdown between the current-carrying parts of transformers, oil switches and other equipment that involves its use.\nRegular measurements of the humidity of transformer oil are an essential component of the maintenance of a whole group of electrical equipment. And since nowadays the transition from planned maintenance to on-demand maintenance is becoming more and more widespread, it can be concluded that the development of means of controlling the humidity of transformer oil, which would be suitable for high-precision express measurement or work compatible with control systems in the mode real time is an actual scientific and applied problem.\nThe article proposes the construction of a transformer for measuring the dielectric loss tangent angle of transformer oil, which is characterized by increased metrological characteristics and is suitable for real-time operation. The value of the tangent of the dielectric loss angle of the transformer oil measured with the specified measuring transducer, being related to the humidity of the latter by the corresponding functional dependence, can be used for the analytical determination of the latter. A mathematical model of the dielectric angle tangent measuring transducer has been developed, on the basis of which both the sensor conversion equation and other metrological characteristics can be estimated.",
        "link": "http://dx.doi.org/10.37128/2520-6168-2023-2-16"
    },
    {
        "id": 22777,
        "title": "Automatic detection of transformer respirator based on image processing",
        "authors": "Yuling Hu, Shanshan Wei, Peng Gao, Xiaolong Shi",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8242821"
    },
    {
        "id": 22778,
        "title": "Parametric transformer designs with improved technical characteristics",
        "authors": "Andrey V. Styskin, Nelja G. Urazbakhtina",
        "published": "2020-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoecs50468.2020.9278465"
    },
    {
        "id": 22779,
        "title": "Simulation Research on Transformer Short Circuit",
        "authors": "Zitong Li, Ling Zhou",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/appeec48164.2020.9220644"
    },
    {
        "id": 22780,
        "title": "Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "Asmaa ELSAID, AMMAR MOHAMMED, Lamiaa Fattouh, MOHAMMED SAKRE",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2856782/v1"
    },
    {
        "id": 22781,
        "title": "Simulation and Impact Analysis of Remanent Flux on Power Transformer Inrush Current",
        "authors": "Wenqi Ge, Youhua Wang",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2018.8508471"
    },
    {
        "id": 22782,
        "title": "PATN: Polarized Attention based Transformer Network for Multi-focus image fusion",
        "authors": "",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2023.04.011"
    },
    {
        "id": 22783,
        "title": "Analysis on Causes of High Failure Rate of Distribution Transformer and Countermeasures",
        "authors": "",
        "published": "2021-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i8.352"
    },
    {
        "id": 22784,
        "title": "Integrating Transformer and GCN for COVID-19 Forecasting",
        "authors": "Yulan Li, Yang Wang, Kun Ma",
        "published": "2022-8-21",
        "citations": 3,
        "abstract": "The spread of corona virus disease 2019 (COVID-19) has coincided with the rise of Transformer and graph neural networks, leading several studies to propose using them to better predict the evolution of a pandemic. The inconveniences of infectious diseases make it important to predict their spread. However, the single deep learning (DL) model has the problems of unstable prediction effect and poor convergence. When calculating the relationship between different positions within a sequence, Transformer does not consider the local context in which each position is located, which can make the prediction vulnerable to outliers, so the integration of the graph convolutional network (GCN) to capture local information is considered. In this paper, we use Transformer to encode the time sequence information of COVID-19 and GCN to decode the time sequence information with graph structure, so that Transformer and GCN are perfectly combined and spatial information is used to further study the integration of these two methods. In addition, we improve the traditional positional encoding structure and propose a dynamic positional encoding technique to extract dynamic temporal information effectively, which is proved to be the key to capture spatial and temporal patterns in data. To make our predictions more useful, we only focused on three states in the United States, covering one of the most affected states, one of the least affected states, and one intermediate state. We used mean absolute percentage error and mean square error as evaluation indexes. Experimental results show that the proposed time series model has better predictive performance than the current DL models. Moreover, the convergence of our model is also better than the current DL models, providing a more accurate reference for the prevention of epidemics.",
        "link": "http://dx.doi.org/10.3390/su141610393"
    },
    {
        "id": 22785,
        "title": "Frequency Measurement of Transformer Insulating Parameters",
        "authors": "Miroslav Gutten, Daniel Korenciak, Peter Brncal, Milan Sebok, Matej Kucera",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/measurement47340.2019.8780036"
    },
    {
        "id": 22786,
        "title": "Se transformer en faisant",
        "authors": "Marek Lawinski",
        "published": "2021-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/edpe.229.0149"
    },
    {
        "id": 22787,
        "title": "A Hybrid Encoder Transformer Network for Video Inpainting",
        "authors": "Hongshan Gan, Yi Wan",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790116"
    },
    {
        "id": 22788,
        "title": "Bilevel Progressive Homography Estimation Via Correlative Region-Focused Transformer",
        "authors": "Qi Jia, Wei Zhang, Xiaomei Feng, Yu Liu, Nan Pu, Nicu Sebe",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4625861"
    },
    {
        "id": 22789,
        "title": "Multitrans: Multi-Branch Transformer Network for Medical Image Segmentation",
        "authors": "Yanhua Zhang, Gabriella Balestra, Ke Zhang, Jingyu Wang, Samanta Rosati, Valentina Giannini",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693595"
    },
    {
        "id": 22790,
        "title": "A Transformer Based Machine Learning of Molecular Grammar Inherent in Proteins Prone to Liquid Liquid Phase Separation",
        "authors": "Abdul Wasim, Jagannath Mondal",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this study, we delve into the intricate molecular language of Intrinsically Disordered Proteins (IDPs) using specialized transformer neural network-based language models, specifically GPT models, pre-trained on sequences exhibiting varying propensities for liquid-liquid phase separation (LLPS). Our approach encompasses the development of distinct models tailored for proteins highly predisposed to LLPS (LLPS+), those with moderate LLPS potential (LLPS-), and folded proteins resistant to LLPS (PDB*). Through the generation of 18,000 sequences, evenly distributed among the three model types, a majority of which demonstrate minimal similarity to proteins cataloged in the SwissProt database, we derive residue-level transition probability matrices. These matrices offer a probabilistic insight into the amino acid grammar unique to each dataset. Analysis of local sequence properties reveals the potential of sequences from LLPS+ GPT models to undergo phase separation. Subsequent validation through multi-chain simulations further substantiates the phase separating potential of the generated proteins and the generation of phase separating sequences from LLPS+ GPT. Additionally, we introduce and train a classifier capable of discerning whether a given amino acid sequence is prone to LLPS. This comprehensive investigation elucidates the molecular grammar of proteins, facilitating the integration of advanced computational methodologies with practical applications in generating protein sequences with desired phenotype.",
        "link": "http://dx.doi.org/10.1101/2024.03.02.583105"
    },
    {
        "id": 22791,
        "title": "Ucfiltransnet: Cross-Filtering Transformer Network for Ct Image Segmentation",
        "authors": "Li Li, Qiyuan Liu, Xinyi Shi, Yujia Wei, Huanqi Li, Hanguang Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4429463"
    },
    {
        "id": 22792,
        "title": "Life Expectancy of Transformer Insulation System by Reconditioning",
        "authors": "Yuli Rodiah, T. Haryono,  Suharyanto",
        "published": "2018-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstc.2018.8528289"
    },
    {
        "id": 22793,
        "title": "Context-Aware Pedestrian Trajectory Prediction with Multimodal Transformer",
        "authors": "Haleh Damirchi, Michael Greenspan, Ali Etemad",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222754"
    },
    {
        "id": 22794,
        "title": "DAUT: Underwater Image Enhancement Using Depth Aware U-shape Transformer",
        "authors": "Mohamed Badran, Marwan Torki",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222895"
    },
    {
        "id": 22795,
        "title": "Dual Transformer Encoder Model for Medical Image Classification",
        "authors": "Fangyuan Yan, Bin Yan, Mingtao Pei",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222303"
    },
    {
        "id": 22796,
        "title": "Feature Aggregated Queries for Transformer-Based Video Object Detectors",
        "authors": "Yiming Cui",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00616"
    },
    {
        "id": 22797,
        "title": "VaBTFER: An Effective Variant Binary Transformer for Facial Expression Recognition",
        "authors": "Lei Shen, Xing Jin",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "Existing Transformer-based models have achieved impressive success in facial expression recognition (FER) by modeling the long-range relationships among facial muscle movements. However, the size of pure Transformer-based models tends to be in the million-parameter level, which poses a challenge for deploying these models. Moreover, the lack of inductive bias in Transformer usually leads to the difficulty of training from scratch on limited FER datasets. To address these problems, we propose an effective and lightweight variant Transformer for FER called VaTFER. In VaTFER, we firstly construct action unit (AU) tokens by utilizing action unit-based regions and their histogram of oriented gradient (HOG) features. Then, we present a novel spatial-channel feature relevance Transformer (SCFRT) module, which incorporates multilayer channel reduction self-attention (MLCRSA) and a dynamic learnable information extraction (DLIE) mechanism. MLCRSA is utilized to model long-range dependencies among all tokens and decrease the number of parameters. DLIE’s goal is to alleviate the lack of inductive bias and improve the learning ability of the model. Furthermore, we use an excitation module to replace the vanilla multilayer perception (MLP) for accurate prediction. To further reduce computing and memory resources, we introduce a binary quantization mechanism, formulating a novel lightweight Transformer model called variant binary Transformer for FER (VaBTFER). We conduct extensive experiments on several commonly used facial expression datasets, and the results attest to the effectiveness of our methods.",
        "link": "http://dx.doi.org/10.3390/s24010147"
    },
    {
        "id": 22798,
        "title": "EfficientPose: A Lightweight And Efficient Model with Transformer For Human Pose Estimation",
        "authors": "Wei Liang, Zhang Cheng, Yanxia Wang, Junjia Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe current methods for human pose estimation focus on improving the accuracy of prediction results, but they overlook the significant issues of computational cost and large number of parameters in practical deployment.Although some lightweight pose estimation models have successfully in reducing the number of parameters, lightweight models typically employ smaller convolutional kernel to reduce the model size, leading to insufficient capture of contextual information.To address this issue, this paper constructs a lightweight network model EfficientPose.Specifically, to expand the receptive field and acquire richer feature information without increasing computational costs, this paper proposes the Efficient Bottleneck Block (EBB) module.Additionally, to capture global spatial dependencies and enhance the representation capability of low-resolution features, a Transformer encoder is introduced into the model.Meanwhile, to overcome the issue of excessively long training time for lightweight models, a novel iterative training strategy is proposed to fully unleash the potential of EfficientPose.To validate the effectiveness of EfficientPose model, extensive comparative experiments and ablation studies are conducted in this paper.Compared with HRNet-W48, which has the same backbone network, EfficientPose not only reduces the number of parameters by 72\\% when the input image size is the same but also improves the accuracy by 0.8 and 0.9 percentage points in the validation and test sets of COCO, respectively.Experiments show that the EfficientPose model can maintain high accuracy even with a significant reduction in the number of parameters.This provides the potential for further application in real-world scenarios with limited resources.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3534285/v1"
    },
    {
        "id": 22799,
        "title": "Transformer Apparent Age Estimation Based on Probabilistic Health Index",
        "authors": "Shuaibing Li, Haiying Dong",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt-asia.2019.8881798"
    },
    {
        "id": 22800,
        "title": "The simulation of a new high frequency transformer",
        "authors": "Sude HATEM, Erol KURT",
        "published": "2022-6-30",
        "citations": 1,
        "abstract": "Core losses of transformers motivate many engineers and scientists to design and implement different transformers for their specific aims. Since there exists a growing interest on high frequency applications in today’s world, design and optimization studies of a magnetic fluid core transformer (MFCT), having an easy and cheap production approach in high frequency applications, are considered in the present paper. The desired design should operate in a more efficient way within a wide frequency band. The MFCT considered here can be a solution to the eddy currents and core losses encountered in the conventional transformers with its low conductivity, oil-based magnetic fluid and super paramagnetic characteristic. The magnetic fluid in the proposed work consists of a combination of ferromagnetic particles made by iron in an averaged diameter of 70 µm with an adjustable magnetism compared to the traditional magnetic fluids and an engine oil, thereby the magnetic permeability of the overall fluid core can be fully adjustable by the variation of mass proportion between the oil and iron powder as an easy process without any chemical process. A COMSOL Multi-Physics design is performed via a finite element package in three dimensions. It is proven that the iron particles exhibit a complicated pattern inside the engine oil and produce a well-defined high frequency output at the secondary windings in a wide range of frequency.",
        "link": "http://dx.doi.org/10.30521/jes.1123925"
    },
    {
        "id": 22801,
        "title": "Tunable and adjustable broadband RF photonic fractional Hilbert transformer based on a Kerr soliton crystal optical microcomb",
        "authors": "David Moss",
        "published": "No Date",
        "citations": 0,
        "abstract": "We demonstrate an RF photonic fractional Hilbert transformer based on an integrated Kerr micro-comb source featuring a record low free spectral range of 49 GHz. By programming and shaping the comb lines according to calculated tap weights for up to 39 wavelengths across the C-band, we achieve tunable bandwidths ranging from 1.2 to 15.3 GHz as well as variable center frequencies from baseband to 9.5 GHz, for both standard integral and arbitrary fractional orders. We experimentally characterize the RF amplitude and phase response of the tunable bandpass and lowpass Hilbert transformers with 90 and 45-degree phase shifts. The experimental results show good agreement with theory, confirming the effectiveness of our approach as a powerful way to implement standard and fractional order Hilbert transformers with broad and variable bandwidths and center frequencies, with high reconfigurability and greatly reduced size and complexity.",
        "link": "http://dx.doi.org/10.31219/osf.io/wdh7j"
    },
    {
        "id": 22802,
        "title": "Rockfish: A Transformer-based Model for Accurate 5-Methylcytosine Prediction from Nanopore Sequencing",
        "authors": "Dominik Stanojević, Zhe Li, Roger Foo, Mile Šikić",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractDNA methylation plays a crucial role in various biological processes, including cell differentiation, ageing, and cancer development. The most important methylation in mammals is 5-methylcytosine (5mC) which is present in the context of CpG dinucleotides. Sequencing methods such as whole-genome bisulfite sequencing (WGBS) successfully detect 5mC DNA modifications. However, they suffer from the serious drawbacks of short read lengths and might introduce an amplification bias. Here we present Rockfish, a deep learning algorithm that significantly improves read-level 5mC detection by using Nanopore sequencing. Compared to other methods based on Nanopore sequencing, there is an increase in the single-base accuracy and the F1 measure of up to 5% and 12%, respectively. Furthermore, Rockfish shows a high correlation with WGBS and requires lower read depth while being computationally efficient. We deem that Rockfish is broadly applicable to study 5mC methylation in diverse organisms and disease systems to yield biological insights.",
        "link": "http://dx.doi.org/10.1101/2022.11.11.513492"
    },
    {
        "id": 22803,
        "title": "Mwformer: Mesh Understanding with Window-Based Transformer",
        "authors": "Haoyang peng, Meng-Hao Guo, Zheng-Ning Liu, Yong-Liang Yang, Tai-Jiang Mu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4428517"
    },
    {
        "id": 22804,
        "title": "A Multiport Three-stage Power Electronic Transformer",
        "authors": "Dajun Ma, Wu Chen, Liangcai Shu",
        "published": "2020-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec39645.2020.9124369"
    },
    {
        "id": 22805,
        "title": "Transformer Model: A Comprehensive Overview and Comparative Analysis for Code Generation in Coding Interview",
        "authors": "Shobhit Kumar Goel, Shiraz Khurana, Hadya Jahangir",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4659084"
    },
    {
        "id": 22806,
        "title": "Bendductor—Transformer Steel Magnetomechanical Force Sensor",
        "authors": "Przemysław Grenda, Monika Kutyła, Michał Nowicki, Tomasz Charubin",
        "published": "2021-12-10",
        "citations": 4,
        "abstract": "In this paper, the design and investigation of an innovative force sensor, based on the Villari effect, is presented. The sensor was built from electrical steel, in a pressductor pattern, but working in bending load mode. The results of the experimental research allowed for the evaluation of transducer’s performance, mitigation of measurement hysteresis, and optimization of its functional parameters. Several issues have been examined, among them the selection of supply and measured signals, the measured values’ impact on measurement hysteresis, harmonic analysis, and the selection of proper current waveforms and frequencies. The proposed sensor is robust, made from inexpensive materials, and has high sensitivity, as compared to other magnetoelastic sensors. It has much higher stress sensitivity than other magnetoelastic sensors due to deformation mode. Based on the tests, its measuring range can be defined as 0.5–5 N with a near-linear characteristic, SNR of 46 dB, and 0.11 N uncertainty.",
        "link": "http://dx.doi.org/10.3390/s21248250"
    },
    {
        "id": 22807,
        "title": "Design and optimization of modular multilevel DC transformer",
        "authors": "Guangyu Wang, Huiqing Wen",
        "published": "2021-8-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciea51954.2021.9516175"
    },
    {
        "id": 22808,
        "title": "Multi-Head Self-Attention Transformer for Dogecoin Price Prediction",
        "authors": "Sashank Sridhar, Sowmya Sanagavarapu",
        "published": "2021-7-8",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hsi52170.2021.9538640"
    },
    {
        "id": 22809,
        "title": "Efficiently Reconstructing High-Quality Details of 3d Digital Rocks with Super-Resolution Transformer",
        "authors": "Zhihao Xing, Jun Yao, Lei Liu, Hai Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4727828"
    },
    {
        "id": 22810,
        "title": "Efficient image analysis with triple attention vision transformer",
        "authors": "Gehui Li, Tongtong Zhao",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2024.110357"
    },
    {
        "id": 22811,
        "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
        "authors": "Zhipeng Li, Xiaofen Xing, Yuanbo Fang, Weibin Zhang, Hengsheng Fan, Xiangmin Xu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1170"
    },
    {
        "id": 22812,
        "title": "Training vision transformer with gradient centralization optimizer for Alzhemier's disease small dataset increase the diagnostic accuracy",
        "authors": "Uttam Khatri, Goo-Rak Kwon",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170594593.30710633/v1"
    },
    {
        "id": 22813,
        "title": "Re-Perceive Global Vision of Transformer for Remote Sensing Weakly Supervised Object Localization",
        "authors": "Xuran Hu, Mingzhe Zhu, Zhengpeng Feng, Ljubiša Stanković",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4750926"
    },
    {
        "id": 22814,
        "title": "Application of Machine Learning in Transformer Health Index Prediction",
        "authors": "Alhaytham Alqudsi, Ayman El-Hag",
        "published": "2019-7-14",
        "citations": 37,
        "abstract": "The presented paper aims to establish a strong basis for utilizing machine learning (ML) towards the prediction of the overall insulation health condition of medium voltage distribution transformers based on their oil test results. To validate the presented approach, the ML algorithms were tested on two databases of more than 1000 medium voltage transformer oil samples of ratings in the order of tens of MVA. The oil test results were acquired from in-service transformers (during oil sampling time) of two different utility companies in the gulf region. The illustrated procedure aimed to mimic a realistic scenario of how the utility would benefit from the use of different ML tools towards understanding the insulation health index of their transformers. This objective was achieved using two procedural steps. In the first step, three different data training and testing scenarios were used with several pattern recognition tools for classifying the transformer health condition based on the full set of input test features. In the second step, the same pattern recognition tools were used along with the three training/testing scenarios for a reduced number of test features. Also, a previously developed reduced model was the basis to reduce the needed number of tests for transformer health index calculations. It was found that reducing the number of tests did not influence the accuracy of the ML prediction models, which is considered as a significant advantage in terms of transformer asset management (TAM) cost reduction.",
        "link": "http://dx.doi.org/10.3390/en12142694"
    },
    {
        "id": 22815,
        "title": "Star-Transformer",
        "authors": "Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, Zheng Zhang",
        "published": "2019",
        "citations": 82,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-1133"
    },
    {
        "id": 22816,
        "title": "Multi-scale Convolution Transformer for Human Activity Detection",
        "authors": "Dejun Gao, Lei Wang",
        "published": "2022-12-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc56324.2022.10065954"
    },
    {
        "id": 22817,
        "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
        "authors": "Sahar Abdelnabi, Mario Fritz",
        "published": "2021-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sp40001.2021.00083"
    },
    {
        "id": 22818,
        "title": "Fixed-point Quantization for Vision Transformer",
        "authors": "Zhexin Li, Peisong Wang, Zhiyuan Wang, Jian Cheng",
        "published": "2021-10-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728246"
    },
    {
        "id": 22819,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2019-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v2/review2"
    },
    {
        "id": 22820,
        "title": "ECG Synthesis via Diffusion-Based State Space Augmented Transformer",
        "authors": "Md Haider Zama, Friedhelm Schwenker",
        "published": "2023-10-9",
        "citations": 1,
        "abstract": "Cardiovascular diseases (CVDs) are a major global health concern, causing significant morbidity and mortality. AI’s integration with healthcare offers promising solutions, with data-driven techniques, including ECG analysis, emerging as powerful tools. However, privacy concerns pose a major barrier to distributing healthcare data for addressing data-driven CVD classification. To address confidentiality issues related to sensitive health data distribution, we propose leveraging artificially synthesized data generation. Our contribution introduces a novel diffusion-based model coupled with a State Space Augmented Transformer. This synthesizes conditional 12-lead electrocardiograms based on the 12 multilabeled heart rhythm classes of the PTB-XL dataset, with each lead depicting the heart’s electrical activity from different viewpoints. Recent advances establish diffusion models as groundbreaking generative tools, while the State Space Augmented Transformer captures long-term dependencies in time series data. The quality of generated samples was assessed using metrics like Dynamic Time Warping (DTW) and Maximum Mean Discrepancy (MMD). To evaluate authenticity, we assessed the similarity of performance of a pre-trained classifier on both generated and real ECG samples.",
        "link": "http://dx.doi.org/10.3390/s23198328"
    },
    {
        "id": 22821,
        "title": "Captioning Remote Sensing Images Using Transformer Architecture",
        "authors": "Wrucha Nanal, Mohammadreza Hajiarbabi",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067039"
    },
    {
        "id": 22822,
        "title": "WITHDRAWN: A Transformer based approach using LSTM and Paraphrase reference to Translate English Text into Hindi",
        "authors": "Surbhi Sharma, Nisheeth Joshi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs many translation systems and applications, such as textual translation, speech systems, etc. use this approach very well with some constraints of grammatical accuracy and completeness, Machine translation is very old concept and work as an intermediary to perform cross-language communication in this age of the internet. Next, using SMT, these statements are translated into the target language without altering their original meaning Statistical Machine translation (SMT). In this paper, an English to Hindi Machine Translation system model was developed by employing the paraphrasing idea under the NMT tree (Neural Machine Translation). The metric scores produced by paraphrasing are closely like human utterances. In the proposed work, we develop a model that uses paraphrased references to convert plain English text into Hindi text. We will evaluate the translation's quality based on its sufficiency, fluency, and correspondence with human-predicted translation to determine how well this system replaces human expressions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230686/v1"
    },
    {
        "id": 22823,
        "title": "Formal Models",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter introduces formal models of cognition and explains how they are similar to verbal theories but use computer programs and mathematics to avoid the many limitations of human reasoning, thereby adding precision and rigor to their explanations. The chapter discusses Marr’s (1982) levels of analyses and how information-processing systems can be understood and described in terms of the task being performed, the representations and algorithms used to perform the task, and how the latter are implemented by physical systems. This then motivates discussion of three common approaches to modeling human cognition and behavior: process models, production-system models, and connectionist models. Each of these approaches is critiqued, with discussion of its merits and limitations. The three modeling approaches are then further illustrated by showing how each might be used to explain the finding that words can be identified more efficiently if they occur in predictable sentence contexts. The chapter closes with a discussion of how cognitive models are evaluated using their simplicity, theoretical scope, compatibility (e.g., with biology), and their capacity to generate novel predictions for guiding research.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0002"
    },
    {
        "id": 22824,
        "title": "PAFormer: Photoacoustic reconstruction via transformer with mask mechanism",
        "authors": "Juze Zhang, Jingyan Zhang, Peng Ge, Fei Gao",
        "published": "2022-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ius54386.2022.9957348"
    },
    {
        "id": 22825,
        "title": "CAT: Re-Conv Attention in Transformer for Visual Question Answering",
        "authors": "Haotian Zhang, Wei Wu",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956247"
    },
    {
        "id": 22826,
        "title": "La Respirologie",
        "authors": "Olivier Benoit",
        "published": "2022-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/caac.550.0221"
    },
    {
        "id": 22827,
        "title": "\"Transformer en profondeur notre modèle\"",
        "authors": "Benoît Hamon, Philippe Frémeaux",
        "published": "2017-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/dae.hs5.0090"
    },
    {
        "id": 22828,
        "title": "Current Transformer Performance under High Voltage Cable Related Transients",
        "authors": "Dimitar Georgiev, Yulian Rangelov, Georgi Georgiev",
        "published": "2021-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bulef53491.2021.9690825"
    },
    {
        "id": 22829,
        "title": "Design of Transformer Fault Intelligent Diagnosis System",
        "authors": "Ruliang Wu, Cuicui Li",
        "published": "2021-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/insai54028.2021.00062"
    },
    {
        "id": 22830,
        "title": "c-Axis oriented ScAlN/SiO2 multilayer BAW transformer for rectifying antenna applications",
        "authors": "Kota Izumi, Takahiko Yanagitani",
        "published": "2020-9-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ius46767.2020.9251603"
    },
    {
        "id": 22831,
        "title": "Enhanced Multi-scale Transformer Network for Temporal Action Localization",
        "authors": "Hang Yin, Xuezhi Xiang",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icma57826.2023.10215545"
    },
    {
        "id": 22832,
        "title": "Multifactorial Frameworks Modelling Linkages of Power Transformer Failure Modes",
        "authors": "Ricardo M. Rampersad, Sanjay Bahadoorsingh, Chandrabhan Sharma",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481099"
    },
    {
        "id": 22833,
        "title": "Transformers for Wind Turbine Generators and Photovoltaic Applications",
        "authors": "David E. Buckmaster, Hemchandra Shertukde",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-10"
    },
    {
        "id": 22834,
        "title": "A Creditability-based Intrusion Tolerant Method for Protection Equipment in Transformer Substations",
        "authors": "Dongqi Liu",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced.2018.8592254"
    },
    {
        "id": 22835,
        "title": "Dbtcc: Unsupervised Pedestrian Re-Identification Combined Double-Branch Transformer and Clustering Contrastive Learning",
        "authors": "Xuan Wang, Zhaojie Sun, Abdellah Chehri, Yongchao Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4544150"
    },
    {
        "id": 22836,
        "title": "Designing and Simulating Transformer Experiments under Dynamic Experiments",
        "authors": "Ramesh Kumar Patel, Madhu Upadhyay",
        "published": "2021-11-11",
        "citations": 0,
        "abstract": "Enormous power transformers are the main gear for the power lattice. Their dependability not just influences the accessibility of power in the inventory region, yet in addition influences the monetary working of an energy provider. The primary goal of this work of the improvement of the transformer testing simulating model on the MATLAB/SIMULINK climate.The testing which depends on generator and grid of the transformer, as well as the related outcomes in SIMULINK, will be part of the conceptual stage. In addition, the technique will look into the effects of transformer validation in the method relying on grid on devices which are connected to the grids. According to the findings, neither source has an impact on the parameter calculation of Open Circuit and Short Circuit assessments. Realistic situations, on the other hand, would necessitate testing which relies on grid, which would enable a broader variety of transformers of different ratings to be evaluated with increased current capacity. The effect of a mistake on the grid can be quickly assessed by looking at the destination of the mistake, the period of the mistake, and any dips that may have took place.",
        "link": "http://dx.doi.org/10.24113/ijoscience.v7i11.421"
    },
    {
        "id": 22837,
        "title": "Gandalf: Harnessing DistilBERT Transformer and BiLSTM for Precise Website Content Classification and Blocking",
        "authors": "Yajat Malhotra, Krish Chatterjie, Raggav Subramani, Aju Dennisan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573966"
    },
    {
        "id": 22838,
        "title": "A Low-Power Setup Proposal for Power Transformer Loading Tests",
        "authors": "Thiago Galvão, Domingos Simonetti",
        "published": "2019-10-30",
        "citations": 1,
        "abstract": "A setup for testing transformers under load through low power converter is presented in this paper. This setup is used for testing power transformers as it allows to verify their performance under operating conditions regarding aspects such as heating, voltage regulation, and mounting robustness. The main goal of the study is centered on replacing a full power Back-to-Back converter (1 pu) by a fractional power one (less than 0.1 pu). The converter, a Voltage Source Inverter (VSI), is a series connected between two equally sized transformers and controls the current flowing in the system. Load profile configurations set according to power factor, current harmonics, or even power level can be imposed to evaluate the performance of the Transformer Under Test (TUT) and the entire system. Theoretical analysis, and simulation results employing Matlab/Simulink platform, considering a typical transformer of a 75 kVA power distribution grid with 13.8 kV/220 V voltage are presented to corroborate the proposal. The required VSI power achieved in the simulations is a fraction of the total power of transformer under test, and the grid power consumed is also of small order justified by losses.",
        "link": "http://dx.doi.org/10.3390/en12214133"
    },
    {
        "id": 22839,
        "title": "Analysis on the Design of Accident Oil Pool for Oil-immersed Transformer",
        "authors": "",
        "published": "2021-12-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i12.400"
    },
    {
        "id": 22840,
        "title": "Fault diagnosis for electronic transformer with wavelet energy entropy and SVM",
        "authors": "Yongze He, Zhenxing Liu, Yong Zhang",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865129"
    },
    {
        "id": 22841,
        "title": "Regexexplainer: Automatic Description Generation for Regular Expressions Via Transformer",
        "authors": "Guang Yang, Xiang Chen, Chi Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4078491"
    },
    {
        "id": 22842,
        "title": "VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection",
        "authors": "Yu Cui, Moshiur Farazi",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956261"
    },
    {
        "id": 22843,
        "title": "A High-Power RF GaN Amplifier Using Bondwire Transformer for Interstage Match",
        "authors": "Marvin Marbell",
        "published": "2023-4-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wamicon57636.2023.10124894"
    },
    {
        "id": 22844,
        "title": "Tender Process for Transformers",
        "authors": "Khayakazi Dioka, Bert Wouters",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_2"
    },
    {
        "id": 22845,
        "title": "Grid-safe – A Voltage Sensitive EVSE to Mitigate Transformer Overloads",
        "authors": "Deepak Aswani, Jon Mycko",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itec53557.2022.9813770"
    },
    {
        "id": 22846,
        "title": "A Swin-Transformer Based Multi-Scale Context Fusion Network for Coal Gangue Segmentation",
        "authors": "Zhenlin Wang, Zhixin Zhang, Shanglong Xu, Zhufei Leng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552917"
    },
    {
        "id": 22847,
        "title": "Equivariant transformer is all you need",
        "authors": "Akio Tomiya, Yuki Nagai",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22323/1.453.0001"
    },
    {
        "id": 22848,
        "title": "La Re-sensibilisation : transformer la souffrance en douleur",
        "authors": "Marie Paré",
        "published": "2019-7-30",
        "citations": 0,
        "abstract": "À la lumière de deux situations cliniques, l’auteure explore les chemins qui mènent de l’absence à la présence au monde. Le travail thérapeutique, soutenu essentiellement par la présence consciente et modulée du thérapeute, permet le passage de l’une à l’autre. Les mouvements corporels sont considérés comme le support organique des mouvements de contact. Aussi, le travail de re-sensibilisation et de re-mobilisation corporel est appréhendé dans la rencontre thérapeutique comme une des voies qui permet de passer de la souffrance liée à l’absence à la sensation vivante d’être présent, en contact.",
        "link": "http://dx.doi.org/10.3917/cges.041.0018"
    },
    {
        "id": 22849,
        "title": "An Accelerated Magnetotelluric 2D Forward Modeling Network Model: Transformer+Unet",
        "authors": "Chongxin Yuan, Xuben Wang, Fei Deng, Kunpeng Wang, Xiangpeng Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe The accuracy and efficiency of two-dimensional electromagnetic forward simulation calculations are crucial for the success of the inversion process. However, traditional numerical simulation methods used for forward modeling are computationally intensive and slow, especially on personal computer systems. To solve this problem and improve computational efficiency, we propose a scheme based on Transformer-Unet (T-Unet) to accelerate the establishment of a two-dimensional magnetotelluric model. The purpose of constructing a T-Unet neural network is to establish a mapping between the geoelectric model and the apparent resistivity while also creating the corresponding dataset. Through network training and iteration, a neural network weight model is obtained, enabling the prediction of apparent resistivity and phase values for forward modeling results. Experimental results demonstrate that compared to traditional finite element forward modeling, T-Unet not only significantly reduces computational time but also achieves high forward calculation accuracy. Furthermore, T-Unet is applied to NLCG inversion. Similar to the finite element forward modeling results for NLCG, T-Unet inversion results accurately determine the location and structure of anomalous bodies. We firmly believe that deep learning neural networks have the potential to accelerate the improvement of forward computation time efficiency for inversion solutions. In summary, the proposed T-Unet scheme provides an effective solution for enhancing the efficiency of two-dimensional electromagnetic forward modeling. By utilizing deep learning technology, it is expected to advance the inversion process and accelerate forward calculations in the field of geophysics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3045517/v1"
    },
    {
        "id": 22850,
        "title": "Domain Adaptation for Complex Shadow Removal with Shadow Transformer Network",
        "authors": "Woo Jin Ahn, Geon Kang, Hyun Duck Choi, Myo Taeg Lim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374277"
    },
    {
        "id": 22851,
        "title": "Employing Transformer Encoders for Enhanced Functional Connectivity Mapping",
        "authors": "Hasan Atakan Bedel, Tolga Çukur",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223897"
    },
    {
        "id": 22852,
        "title": "Transformer-Based Neural Texture Synthesis and Style Transfer",
        "authors": "Jiahao Lu",
        "published": "2022-1-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3512353.3512366"
    },
    {
        "id": 22853,
        "title": "Convolutional Transformer via Graph Embeddings for Few-shot Toxicity and Side Effect Prediction",
        "authors": "Luis Torres, Bernardete Ribeiro, Joel Arrais",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-66"
    },
    {
        "id": 22854,
        "title": "Partager les savoirs pour transformer nos métiers",
        "authors": "Daisy Moreira Cunha, Simon Mallard",
        "published": "2020-12-1",
        "citations": 0,
        "abstract": "La démarche ergologique contribue à la formalisation de l’expérience professionnelle. En s’appuyant sur un dispositif mis en place au Brésil par des chercheurs avec des syndicalistes mineurs, cet article décrit les effets de cette approche sur les acteurs impliqués en termes de coopération, de posture ou d’un point de vue éthique. Le croisement des savoirs entre les acteurs – ou, dit autrement, « l’expérience de l’expérience » – contribue pleinement au développement conjoint de tous à « disparité de place et parité d’estime ».",
        "link": "http://dx.doi.org/10.3917/edpe.225.0127"
    },
    {
        "id": 22855,
        "title": "Stock Volatility Forecasting with Transformer Network",
        "authors": "Golnaz Sababipour Asl, Ruppa K. Thulasiram, Aerambamoorthy Thavaneswaran",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371830"
    },
    {
        "id": 22856,
        "title": "Vision Transformer and Its Application in Penguin Classification",
        "authors": "Luting Pan",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009747"
    },
    {
        "id": 22857,
        "title": "Transformer les pratiques muséales au Cameroun grâce à la visualisation 3D",
        "authors": "Gérard Dimitri Keumoe, Coline Blot",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/revuehn.3315"
    },
    {
        "id": 22858,
        "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
        "authors": "Sathvik Udupa, Anwesha Roy, Abhayjeet Singh, Aravind Illa, Prasanta Kumar Ghosh",
        "published": "2021-8-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1375"
    },
    {
        "id": 22859,
        "title": "Captioning Transformer With Scene Graph Guiding",
        "authors": "Haishun Chen, Ying Wang, Xin Yang, Jie Li",
        "published": "2021-9-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506193"
    },
    {
        "id": 22860,
        "title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction",
        "authors": "Ziji Zhang, Zhehui Wang, Rajesh Kamma, Sharanya Eswaran, Narayanan Sadagopan",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1135"
    },
    {
        "id": 22861,
        "title": "ViTT: Vision Transformer Tracker",
        "authors": "Xiaoning Zhu, Yannan Jia, Sun Jian, Lize Gu, Zhang Pu",
        "published": "2021-8-20",
        "citations": 8,
        "abstract": "This paper presents a new model for multi-object tracking (MOT) with a transformer. MOT is a spatiotemporal correlation task among interest objects and one of the crucial technologies of multi-unmanned aerial vehicles (Multi-UAV). The transformer is a self-attentional codec architecture that has been successfully used in natural language processing and is emerging in computer vision. This study proposes the Vision Transformer Tracker (ViTT), which uses a transformer encoder as the backbone and takes images directly as input. Compared with convolution networks, it can model global context at every encoder layer from the beginning, which addresses the challenges of occlusion and complex scenarios. The model simultaneously outputs object locations and corresponding appearance embeddings in a shared network through multi-task learning. Our work demonstrates the superiority and effectiveness of transformer-based networks in complex computer vision tasks and paves the way for applying the pure transformer in MOT. We evaluated the proposed model on the MOT16 dataset, achieving 65.7% MOTA, and obtained a competitive result compared with other typical multi-object trackers.",
        "link": "http://dx.doi.org/10.3390/s21165608"
    },
    {
        "id": 22862,
        "title": "Waveletformernet: A Transformer-Based Wavelet Network for Real-World Non-Homogeneous and Dense Fog Removal",
        "authors": "Shengli Zhang, Zhiyong Tao, Sen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4686029"
    },
    {
        "id": 22863,
        "title": "Three-Phase Distribution Transformer Connections Modeling Based on Matrix Operation Method by Phase-coordinates",
        "authors": "Zhigang Zhang, Mingrui Mo, Caizhu Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper proposes a matrix operation method for modeling the three-phase transformer by phase-coordinates. Based on decoupling theory, the 12x12 dimension primitive admittance matrix is obtained at first employing the coupling configuration of the windings. Under the condition of asymmetric magnetic circuits, according to the boundary conditions for transformer connections, the transformers in different connections enable to be modeling by the matrix operation method from the primitive admittance matrix. Another purpose of this paper is to explain the differences of the phase-coordinates and the positive sequence parameters in the impedances of the transformers. The numerical testing results in IEEE-4 system show that the proposed method is valid and efficient.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-125904/v1"
    },
    {
        "id": 22864,
        "title": "MOT: a Multi-Omics Transformer for Multiclass Classification Tumour Types Predictions",
        "authors": "Mazid Abiodoun Osseni, Prudencio Tossou, Francois Laviolette, Jacques Corbeil",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMotivation: Breakthroughs in high-throughput technologies and machine learning methods have enabled the shift towards multi-omics modeling as the preferred mean to understand the mechanisms underlying biological processes, and to improve complex disease prognosis in clinical settings. However, most multi-omic studies only use transcriptomics and epigenomics due to their over-representation in databases and their early technical maturity compared to others omics. For complex phenotypes and mechanisms, not leveraging all the omics despite their varying degree of availability can lead to a failure to understand the underlying biological mechanisms. Results: We proposed MOT (Multi-Omic Transformer), a deep learning based model using the transformer architecture, that discriminates complex phenotypes (herein cancers types) based on five omics data type regardless of their availability: transcriptomics (mRNA and miRNA), epigenomics (DNA methylation), copy number variations (CNVs), and proteomics. At its core, MOT uses a data augmentation scheme that allows it to handle missing omics views and its attention layers give a macro level of interpretability for each phenotypes. Indeed, MOT identifies the required omic type for the best prediction for each phenotype and therefore could guide clinical decision making when acquiring data to confirm a diagnostic. It achieves an accuracy score of 96.04% after 5-fold cross-validation among 33 tumour types. The newly introduced model can integrate and analyse five different omics data while handling the missing omics views and can also identify the essential omics data for the tumour multiclass classification tasks. Availability and implementation: MOT source code is available at https://github.com/dizam92/multiomic_predictions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1348696/v1"
    },
    {
        "id": 22865,
        "title": "Research on Transformer for Natual Language Processing",
        "authors": "Wentao Li",
        "published": "2022-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ahpcai57455.2022.10087672"
    },
    {
        "id": 22866,
        "title": "The Characteristics Analysis of Controllable Reactor of Transformer Type",
        "authors": "GuoSheng Zhao, Pei Li",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt-asia.2019.8881543"
    },
    {
        "id": 22867,
        "title": "Revisiting the Binomial Multisection Transformer [Application Notes]",
        "authors": "Zubair Ahmed",
        "published": "2021-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mmm.2020.3035865"
    },
    {
        "id": 22868,
        "title": "Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition",
        "authors": "Yuhua Wang, Guang Shen, Yuezhu Xu, Jiahang Li, Zhengdao Zhao",
        "published": "2021-8-30",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-2004"
    },
    {
        "id": 22869,
        "title": "Multilingual Transformer-Based Personality Traits Estimation",
        "authors": "Simone Leonardi, Diego Monti, Giuseppe Rizzo, Maurizio Morisio",
        "published": "2020-3-26",
        "citations": 13,
        "abstract": "Intelligent agents have the potential to understand personality traits of human beings because of their every day interaction with us. The assessment of our psychological traits is a useful tool when we require them to simulate empathy. Since the creation of social media platforms, numerous studies dealt with measuring personality traits by gathering users’ information from their social media profiles. Real world applications showed how natural language processing combined with supervised machine learning algorithms are effective in this field. These applications have some limitations such as focusing on English text only and not considering polysemy in text. In this paper, we propose a multilingual model that handles polysemy by analyzing sentences as a semantic ensemble of interconnected words. The proposed approach processes Facebook posts from the myPersonality dataset and it turns them into a high-dimensional array of features, which are then exploited by a deep neural network architecture based on transformer to perform regression. We prove the effectiveness of our work by comparing the mean squared error of our model with existing baselines and the Kullback–Leibler divergence between the relative data distributions. We obtained state-of-the-art results in personality traits estimation from social media posts for all five personality traits.",
        "link": "http://dx.doi.org/10.3390/info11040179"
    },
    {
        "id": 22870,
        "title": "Entangled Transformer for Image Captioning",
        "authors": "Guang Li, Linchao Zhu, Ping Liu, Yi Yang",
        "published": "2019-10",
        "citations": 180,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00902"
    },
    {
        "id": 22871,
        "title": "A Transformer-Based Network for Anisotropic 3D Medical Image Segmentation",
        "authors": "Danfeng Guo, Demetri Terzopoulos",
        "published": "2021-1-10",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9411990"
    },
    {
        "id": 22872,
        "title": "Episodic Transformer for Vision-and-Language Navigation",
        "authors": "Alexander Pashevich, Cordelia Schmid, Chen Sun",
        "published": "2021-10",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01564"
    },
    {
        "id": 22873,
        "title": "Person Re-identification Based on Improved Transformer and Multi-scale, Multi- granularity Feature Learning",
        "authors": "Peng Ji, Hao Chen, Chang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPerson re-identification refers to the use of computer vision techniques to identify specific individuals in images captured by surveillance systems. However, this process is challenged by factors like variations in viewpoint, pose, occlusions, and other environmental conditions. Current pedestrian re-identification methods based on local features often suffer from limited discriminative power in feature extraction, which ultimately results in lower re-identification accuracy. For efficient use of feature information, we propose an improved Transformer and multi-scale, multi-granularity fusion network. Firstly, local multi-granularity branch is introduced to extract semantic information at various granularities, which enhances the richness of features by exploring less significant local characteristics of pedestrians. In addition, the improved Transformer can better focus on key regions of the image and incorporates larger-scale image information. Finally, we use triplet and softmax to calculate loss for different branches. We conducted extensive experiments on three datasets (CUHK03, Market1501, DukeMTMC-reID), and the results demonstrate that our model outperforms many recent state-of-the-art methods in terms of accuracy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3984387/v1"
    },
    {
        "id": 22874,
        "title": "Investigation of the Core's Vibration Characteristics of a Distribution Transformer",
        "authors": "Xiao Chen",
        "published": "2023-2-24",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icopesa56898.2023.10140758"
    },
    {
        "id": 22875,
        "title": "Renewable energy forecasting: A self-supervised learning-based transformer variant",
        "authors": "Jiarui Liu, Yuchen Fu",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.128730"
    },
    {
        "id": 22876,
        "title": "Transformer-less integrated wind turbine-power transmission line systems",
        "authors": "N. Danapour, F. Mohammadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.2259"
    },
    {
        "id": 22877,
        "title": "A Swin Transformer-Based Fault Migration and Diagnosis Approach for Gearboxes",
        "authors": "Yan Zhang, Xifeng Wang, Zhe Wu, Ziwen Wang, Jianming Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4763953"
    },
    {
        "id": 22878,
        "title": "Experimental method to summarize the effect of transformer inrush currents on a digital overcurrent relay",
        "authors": "Elmer Sorrentino, Tatiana Marcano",
        "published": "2020-10-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/andescon50619.2020.9272077"
    },
    {
        "id": 22879,
        "title": "Intelligent Diagnosis of Power Transformer Working Fault",
        "authors": "Guicheng Wang, Chuang Feng, Xiaonan Mu, Haifeng Liu",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2019.8833405"
    },
    {
        "id": 22880,
        "title": "Vibration Monitoring of Converter Transformer by Simplified permutation Entropy",
        "authors": "Xin Zhang, Ronghui Huang, Dongxu Zhou, Fenghua Wang",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623221"
    },
    {
        "id": 22881,
        "title": "An Abnormal Metering Monitoring Method Based on Transformer External Characteristics",
        "authors": "Xia Taofang",
        "published": "2021-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acpee51499.2021.9436978"
    },
    {
        "id": 22882,
        "title": "Simulation and Impact Analysis of Remanent Flux on Power Transformer Inrush Current",
        "authors": "Wenqi Ge, Youhua Wang",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2018.8508352"
    },
    {
        "id": 22883,
        "title": "Configurable optical fiber current transformer",
        "authors": "Kamil Barczak, Mateusz Szablicki",
        "published": "2019-3-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2522345"
    },
    {
        "id": 22884,
        "title": "THE EXPERIMENTS WITH A TRANSFORMER IN HIGH SCHOOL",
        "authors": "Matúš Sitkey",
        "published": "2018-9-27",
        "citations": 0,
        "abstract": "This article describes the education of secondary school pupils in relation to a set of experiments using a school transformer and the Tesla coil.  The set of practical experiments consists of 9 that can be used in the teaching of pupils of physics at secondary school in the thematic unit relating to electricity and magnetism. Each experiment contains a description of the relevant section of the International Standard Classification of Education in which it can be used. These descriptions include a motivational title, pedagogical goals, new keywords, a device list, and a level of difficulty based on the devices used. It also includes an experimental procedure, its physical explanation and experiment note, and supplementary questions for pupils. The research focuses on increasing the interest of pupils and improving their understanding of theory on the subject of electricity and magnetism by way of theoretical lessons connected to practical experiments using the Tesla coil. The research involved a special questionnaire for evaluation using the Thurstone method, which measures the preference of the respondent group. The results are analyzed using a modified Thurstone method of paired comparison with the software, Mathematica.",
        "link": "http://dx.doi.org/10.12955/cbup.v6.1247"
    },
    {
        "id": 22885,
        "title": "A Novel Energy Router Based on Multi-winding Line Frequency Transformer",
        "authors": "Zhitao Guan, Dan Wang, Qing Duan, Chengxiong Mao, Bin Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Energy routers based on the electronic power transformer are suitable\nfor the AC-DC hybrid grid with multiple voltage levels, but their\nstructures are complex. This paper proposes a novel energy router based\non the multi-winding line frequency transformer. By a combination of a\nmulti-winding line frequency transformer and power electronic devices,\nthe proposed energy router can take advantage of the high reliability of\nthe multi-winding line frequency transformer and the high\ncontrollability of power electronic devices. The proposed energy router\nis suitable for the AC-DC hybrid grid with multiple voltage levels and\nhas the characteristic of a simple structure. The simulations and\nexperimental results demonstrate the effectiveness of the proposed\nenergy router.",
        "link": "http://dx.doi.org/10.22541/au.166984734.43118907/v1"
    },
    {
        "id": 22886,
        "title": "Transformer Fleet Monitoring",
        "authors": "Tihomir Jaković, Ivan Murat, Filip Klarić, Samir Keitoue",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.691"
    },
    {
        "id": 22887,
        "title": "Substation testing and commissioning: Power transformer through fault test",
        "authors": "M. Talebi, Y. Unludag",
        "published": "2018-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2018.8349827"
    },
    {
        "id": 22888,
        "title": "Integration Of UPQC With New Converter Transformer For Power Quality Improvement",
        "authors": "K. Naresh Kumar, S. Srinath",
        "published": "2018-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec.2018.8476800"
    },
    {
        "id": 22889,
        "title": "Processing technology, structure and modeling analysis of semiconductor transformer",
        "authors": "Wang Wanyan",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35534/amp.0201003c"
    },
    {
        "id": 22890,
        "title": "Analysis of Transitional Occurrences of Low Voltage Welding Transformer",
        "authors": "",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17559/tv-20221010215018"
    },
    {
        "id": 22891,
        "title": "Single-Photon Cameras Image Reconstruction Using Vision Transformer",
        "authors": "Xingzheng Wang",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccai57533.2023.10201259"
    },
    {
        "id": 22892,
        "title": "Transformer-Based End-to-End Speech Recognition with Residual Gaussian-Based Self-Attention",
        "authors": "Chengdong Liang, Menglong Xu, Xiao-Lei Zhang",
        "published": "2021-8-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-427"
    },
    {
        "id": 22893,
        "title": "An innovative method for detecting transformer air leaks",
        "authors": "T. Mellin, S. Leivo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.0679"
    },
    {
        "id": 22894,
        "title": "Nano-Tech Transformer Cores",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "This chapter describes optimal characterization of composites and nanocomposites materials that handle traditional transformer cores to design transformer core materials. It attempts to offer new designs for new transformer cores to control the magnetization parameters. This chapter draws attention to theories and effective nanoparticle structure that tackle the characteristics of transformer cores. It further sheds light on the effects of nanoparticles on magnetization loss of transformer core by using individual and multiple magnetic nanocomposites. The forecasting and recommendations of the magnetic characterization are also presented for 1-phase and 3-phase transformer cores. ",
        "link": "http://dx.doi.org/10.4018/978-1-7998-8536-8.ch008"
    },
    {
        "id": 22895,
        "title": "Arc-Fault Detection method with Saturated Current Transformer",
        "authors": "Sittichai Wangwiwattana, Koike Yoshikazu",
        "published": "2022-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciprob54042.2022.9798716"
    },
    {
        "id": 22896,
        "title": "Transformer-Less Floating Gate Driver for 3D Power SoC",
        "authors": "Minami Nakayama, Seiya Abe, Satoshi Matsumoto",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/3dic48104.2019.9058891"
    },
    {
        "id": 22897,
        "title": "Design and Thermal Analysis of Three Phase Transformer for Wind Turbine Using Coupled Electromagnetic Field-Thermal Model",
        "authors": "YÜCEL ÇETINCEVIZ, Erdal Sehirli",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4569402"
    },
    {
        "id": 22898,
        "title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation",
        "authors": "Matthew Raffel, Lizhong Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.816"
    },
    {
        "id": 22899,
        "title": "Inrush Blocking Scheme in Transformer Differential Protection",
        "authors": "Balamurugan Saravanan, A. Rathinam",
        "published": "2017-6",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egypro.2017.05.242"
    },
    {
        "id": 22900,
        "title": "Partial discharge detection in transformer winding using FRA analysis",
        "authors": "Nassima AISSAOUI",
        "published": "2022-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2022.08.10"
    },
    {
        "id": 22901,
        "title": "Computer-Assisted Detection Of Intracranial Aneurysms Using A Transformer Deep Neural Network In 3D MR Angiography",
        "authors": "Emily Zhou, Felix Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIntracranial aneurysms, a brain condition caused by weakened blood vessels, result in abnormal bulging or dilation of the intracranial artery walls. Up to 5% of the global population will suffer from this disease, and early detection is critical. Due to their small size, aneurysms are often missed in initial patient image assessments. Further, the process of interpreting 3D medical images can be time-consuming, error-prone, and require trained radiologists. This study implemented a deep learning model using a Transformer Neural Network to enable the automated computer-assisted detection of aneurysms in 3D TOF-MRA images.\nThis novel Transformer model demonstrated overall 91.9% patient-level sensitivity and 95.1% patient specificity in detecting intracranial aneurysms. Additionally, for aneurysms exceeding 5 mm in diameter, it reached patient sensitivity of 95.8% and patient specificity of 96.4%. To our knowledge, this is the first study to implement a Transformer Deep Neural Network for the detection of aneurysms from 3D volumetric TOF-MRA images. The successful results underscore the potential of Transformer models in accelerating diagnoses of cerebrovascular diseases while streamlining radiologists’ workflow. This detection method can be applicable for identifying aneurysms in other organs and diseases such as brain tumors.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1355959/v1"
    },
    {
        "id": 22902,
        "title": "Transformer Networks for Predictive Group Elevator Control",
        "authors": "Jing Zhang, Athanasios Tsiligkaridis, Hiroshi Taguchi, Arvind Raghunathan, Daniel Nikovski",
        "published": "2022-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc55457.2022.9838059"
    },
    {
        "id": 22903,
        "title": "Wireless Channel Estimation Based on Transformer and Super-Resolution",
        "authors": "Jun Li, Rong Wang, Yawei Yuan, Wenjing Zheng, Bo He, Mingming Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4525457"
    },
    {
        "id": 22904,
        "title": "An Analysis of Power Transformer Outages and Reliability Monitoring",
        "authors": "Vezir Rexhepi",
        "published": "2017-12",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egypro.2017.11.053"
    },
    {
        "id": 22905,
        "title": "La résistance, un levier pour transformer l’envie ?",
        "authors": "Sylvie Pons-Nicolas",
        "published": "2021-6-25",
        "citations": 0,
        "abstract": "Réflexion sur les liens entre envie et résistance. À partir d’éléments de l’histoire clinique de la patiente de Bernard Bensidoun, l’auteure questionne l’envie du pénis dans le concept de réaction thérapeutique négative. Elle postule que dans certaines configurations, lorsque les processus d’introjection et d’identification ont été entravés par les conduites d’emprise de mères envieuses, la résistance dans la cure est potentiellement une résistance vitale pour exister. Elle pourrait être un passage par un transfert agi pour se déprendre d’identifications aliénantes. Ce qui permettrait de cheminer d’une altérité dangereuse à une altérité révélatrice de soi dans laquelle une envie appropriative du registre primaire transformée en désir de savoir participerait de la symbolisation de la différence. La capacité de l’analyste à se laisser « utiliser » au sens winnicottien, en jouant de sa part de féminin et de masculin, est un facteur essentiel pour favoriser ces processus de transformation.",
        "link": "http://dx.doi.org/10.3917/rfp.853.0609"
    },
    {
        "id": 22906,
        "title": "De la difficulté de transformer l’essai du changement",
        "authors": "Philippe Jourdy",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/caac.562.0009"
    },
    {
        "id": 22907,
        "title": "A Sequence-to-Sequence Transformer Model for Disconnection Aware Retrosynthesis",
        "authors": "Andrea Byekwaso, Alain C. Vaucher, Philippe Schwaller, Alessandra Toniato, Teodoro Laino",
        "published": "No Date",
        "citations": 1,
        "abstract": "Retrosynthesis is an approach commonly undertaken when considering the manufacture of novel molecules. During this process, a target molecule is broken down and analyzed by considering the bonds to be changed as well as the functional group interconversion. In modern computer-assisted synthesis planning tools, the predictions of these changes are typically carried out automatically. However there may be some benefit to the decision being guided by those executing the process: typically, chemists have a clear idea where the retrosynthetic change should happen, but not how such a transformation is to be realized. Using a data-driven model, the retrosynthesis task can be further explored by giving chemists the option to explore specific disconnections. In this work, we design an approach to provide this option by adapting a transformer-based model for single-step retrosynthesis. The model takes as input a product SMILES string, in which the atoms where the transformation should occur are tagged accordingly. This model predicts precursors corresponding to a disconnection occurring in the correct location in 88.9% of the test set reactions. The assessment with a forward prediction model shows that 76% of the predictions are chemically correct, with 14.1% perfectly matching the ground truth.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-7hp1s"
    },
    {
        "id": 22908,
        "title": "A Transformer Autoencoder with Hidden Layer Extension for Multivariate Time Series Anomaly Detection",
        "authors": "xiaoxia zhang, yi chen, feng hu, Guoyin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4576627"
    },
    {
        "id": 22909,
        "title": "Dynamic rating of the wind farm transformer from the power system’s perspective",
        "authors": "Kateryna Morozovska, Rikard Karlsson, Patrik Hilber",
        "published": "2021-6-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech46648.2021.9494902"
    },
    {
        "id": 22910,
        "title": "Numerical reasoning based on transformer",
        "authors": "Xu Wei",
        "published": "2022-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2661770"
    },
    {
        "id": 22911,
        "title": "Integration Battery Energy Storage Using T- Type Solid State Transformer Based Multilevel Inverter",
        "authors": "Krishna Molli, Ajay D Vimal Raj P",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nStorage technologies are essential for preserving the renewable grid power balance between generation and consumption. In this research, a solid-state transformer (SST) based T-type multilevel inverter (MLI) is proposed for the medium/high voltage integration of large-scale battery energy storage (BES). For any turn ratio, the proposed SST-MLI generates 25 levels with fewer switching devices and SSTs. A novel dynamic phase shift controller (NDPSC) is also being developed for the proposed BES system. The NDPSC controller regulates active and reactive power by maintaining the battery's health. The NDPSC controller uses two gain control factors tuned to produce a fast response without overshooting. The proposed battery storage system's dynamic behavior is simulated in MATLAB/Simulink for various active and reactive power variations and experimentally validated in a low-power laboratory prototype system.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2709187/v1"
    },
    {
        "id": 22912,
        "title": "Transformer Design Principles",
        "authors": "Robert M. Del Vecchio, Bertrand Poulin, Pierre T. Feghali, Dilipkumar M. Shah, Rajendra Ahuja",
        "published": "2017-8-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155920"
    },
    {
        "id": 22913,
        "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
        "authors": "Masakiyo Fujimoto, Hisashi Kawai",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-225"
    },
    {
        "id": 22914,
        "title": "Alkali-Modified Boehmite for Acid Removal in Waste Transformer Oil",
        "authors": "Kai Fei, ZhuoYi Dong, Xuan Meng, Li Shi, NaiWang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer oil plays a critical role in ensuring the insulation and normal operation of power transformers. However, during operation, the transformer oil would undergo oxidation and degradation, leading to the formation of acids and polar compounds that increase its acidity and degrade its performance. Recycling and reclamation of aged transformer oil are necessary to prolong transformer lifespan and mitigate risks. Adsorption separation is a preferred method for regeneration, offering simplicity and high efficiency. This study focuses on impregnated boehmite as an adsorbent for acidity removal in waste transformer oil. The effects of impregnation amount and calcination temperature on acidity removal are investigated, along with a comparative analysis of other adsorbents. The findings contribute to the development of sustainable methods for regenerating transformer oil, ensuring efficient and environmentally friendly operations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3120960/v1"
    },
    {
        "id": 22915,
        "title": "MethylBERT: A Transformer-based model for read-level DNA methylation pattern identification and tumour deconvolution",
        "authors": "Yunhee Jeong, Karl Rohr, Pavlo Lutsik",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractDNA methylation (DNAm) is a key epigenetic mark that shows profound alterations in cancer. Although read-level methylomes enable more in-depth DNAm analysis due to the broad coverage and preservation of rare cell-type signals, the majority of published DNAm analysis methods have targeted array-based data such as EPIC/450K array. Here, we propose MethylBERT, a novel Transformer-based read-level methylation pattern classification model. MethylBERT identifies tumour-derived sequence reads based on their methylation patterns and genomic sequence using a Bidirectional Encoder Representations from Transformers (BERT) model. Based on the calculated classification probability, the method estimates tumour cell fractions within bulk samples and provides an assessment of the model precision. In our evaluation, MethylBERT outperforms existing deconvolution methods and demonstrates high accuracy regardless of methylation pattern complexity, read length and read coverage. Moreover, we show its potential for accurate non-invasive early cancer diagnostics by applying MethylBERT to liquid biopsy samples collected from cancer patients. MethylBERT represents a significant advancement in read-level methylome analysis. It will increase the accuracy of tumour deconvolution and enhance circulating tumour DNA studies.",
        "link": "http://dx.doi.org/10.1101/2023.10.29.564590"
    },
    {
        "id": 22916,
        "title": "Transfer Learning and Transformer Technology",
        "authors": "Raymond S. T. Lee",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-1999-4_8"
    },
    {
        "id": 22917,
        "title": "Exploring the Knowledge of An Outstanding Protein to Protein Interaction Transformer",
        "authors": "Sen Yang, Dawei Feng, Peng Cheng, Yang Liu, Shengqi Wang",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractProtein-to-protein interaction (PPI) prediction aims to predict whether two given proteins interact or not. Compared with traditional experimental methods of high cost and low efficiency, the current deep learning based approach makes it possible to discover massive potential PPIs from large-scale databases. However, deep PPI prediction models perform poorly on unseen species, as their proteins are not in the training set. Targetting on this issue, the paper first proposes PPITrans, a Transformer based PPI prediction model that exploits a language model pre-trained on proteins to conduct binary PPI prediction. To validate the effectiveness on unseen species, PPITrans is trained with Human PPIs and tested on PPIs of other species. Experimental results show that PPITrans significantly outperforms the previous state-of-the-art on various metrics, especially on PPIs of unseen species. For example, the AUPR improves 0.339 absolutely on Fly PPIs. Aiming to explore the knowledge learned by PPITrans from PPI data, this paper also designs a series of probes belonging to three categories. Their results reveal several interesting findings, like that although PPITrans cannot capture the spatial structure of proteins, it can obtain knowledge of PPI type and binding affinity, learning more than binary PPI.",
        "link": "http://dx.doi.org/10.1101/2023.02.09.527848"
    },
    {
        "id": 22918,
        "title": "Leveraging sensory data in estimating transformer lifetime",
        "authors": "Mohsen Mahoor, Alireza Majzoobi, Zohreh S. Hosseini, Amin Khodaei",
        "published": "2017-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps.2017.8107390"
    },
    {
        "id": 22919,
        "title": "Investigations on Service Extensions of Solid State Transformer",
        "authors": "R B Jeyapradha, V Rajini",
        "published": "2019-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icees.2019.8719315"
    },
    {
        "id": 22920,
        "title": "Survey: Transformer based video-language pre-training",
        "authors": "Ludan Ruan, Qin Jin",
        "published": "2022",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aiopen.2022.01.001"
    },
    {
        "id": 22921,
        "title": "Automatic Speech Recognition in Indonesian Using the Transformer Model",
        "authors": "Ilvico Sonata",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icimcis60089.2023.10349042"
    },
    {
        "id": 22922,
        "title": "Meta-Reinforcement Learning with Transformer Networks for Space Guidance Applications",
        "authors": "Lorenzo Federici, Roberto Furfaro",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-2061"
    },
    {
        "id": 22923,
        "title": "Efficient 3D Semantic Segmentation with Superpoint Transformer",
        "authors": "Damien Robert, Hugo Raguet, Loic Landrieu",
        "published": "2023-10-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01577"
    },
    {
        "id": 22924,
        "title": "Augmented Transformer for Speech Detection in Adverse Acoustical Conditions",
        "authors": "Lukasz Smietanka, Tomasz Maka",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/spa59660.2023.10274438"
    },
    {
        "id": 22925,
        "title": "Review of Three Phase Transformer-less PV Converters",
        "authors": "Ibhan Chand Rath, Anshuman Shukla",
        "published": "2019-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsets.2019.8745120"
    },
    {
        "id": 22926,
        "title": "Flexible Protein-Protein Docking with a Multi-Track Iterative Transformer",
        "authors": "Lee-Shin Chu, Jeffrey A. Ruffolo, Ameya Harmalkar, Jeffrey J. Gray",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractConventional protein-protein docking algorithms usually rely on heavy candidate sampling and re-ranking, but these steps are time-consuming and hinder applications that require high-throughput complex structure prediction, e.g., structure-based virtual screening. Existing deep learning methods for protein-protein docking, despite being much faster, suffer from low docking success rates. In addition, they simplify the problem to assume no conformational changes within any protein upon binding (rigid docking). This assumption precludes applications when binding-induced conformational changes play a role, such as allosteric inhibition or docking from uncertain unbound model structures. To address these limitations, we present GeoDock, a multi-track iterative transformer network to predict a docked structure from separate docking partners. Unlike deep learning models for protein structure prediction that input multiple sequence alignments (MSAs), GeoDock inputs just the sequences and structures of the docking partners, which suits the tasks when the individual structures are given. GeoDock is flexible at the protein residue level, allowing the prediction of conformational changes upon binding. For a benchmark set of rigid targets, GeoDock obtains a 41% success rate, outperforming all the other tested methods. For a more challenging benchmark set of flexible targets, GeoDock achieves a similar number of top-model successes as the traditional method ClusPro [1], but fewer than ReplicaDock2 [2]. GeoDock attains an average inference speed of under one second on a single GPU, enabling its application in large-scale structure screening. Although binding-induced conformational changes are still a challenge owing to limited training and evaluation data, our architecture sets up the foundation to capture this backbone flexibility. Code and a demonstration Jupyter notebook are available athttps://github.com/Graylab/GeoDock.",
        "link": "http://dx.doi.org/10.1101/2023.06.29.547134"
    },
    {
        "id": 22927,
        "title": "Bidirectional Transformer Reranker for Grammatical Error Correction",
        "authors": "Ying Zhang, Hidetaka Kamigaito, Manabu Okumura",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.234"
    },
    {
        "id": 22928,
        "title": "A Two-Stage Transformer-Based Point Cloud Upsampling",
        "authors": "Yunhui Shi, Chao Dong, Jin Wang",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326548"
    },
    {
        "id": 22929,
        "title": "MoveFormer: a Transformer-based model for step-selection animal movement modelling",
        "authors": "Ondřej Cífka, Simon Chamaillé-Jammes, Antoine Liutkus",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe movement of animals is a central component of their behavioural strategies. Statistical tools for movement data analysis, however, have long been limited, and in particular, unable to account for past movement information except in a very simplified way. In this work, we propose MoveFormer, a new step-based model of movement capable of learning directly from full animal trajectories. While inspired by the classical step-selection framework and previous work on the quantification of uncertainty in movement predictions, MoveFormer also builds upon recent developments in deep learning, such as the Transformer architecture, allowing it to incorporate long temporal contexts. The model predicts an animal’s next movement step given its past movement history, including not only purely positional and temporal information, but also any available environmental covariates such as land cover or temperature. We apply our model to a diverse dataset made up of over 1550 trajectories from over 100 studies, and show how it can be used to gain insights about the importance of the provided context features, including the extent of past movement history. Our software, along with the trained model weights, is released as open source.",
        "link": "http://dx.doi.org/10.1101/2023.03.05.531080"
    },
    {
        "id": 22930,
        "title": "High quality GAN encoded using Transformer",
        "authors": "Pengsen Zhao, Qiner Wu, Xiaoqiang Jin",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrcv59470.2023.10329066"
    },
    {
        "id": 22931,
        "title": "Facial Expression Recognition Based on Semi-supervised Vision Transformer",
        "authors": "Xiangwei Fu",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isaiee57420.2022.00042"
    },
    {
        "id": 22932,
        "title": "Biomedical Signals Classification with Transformer Based Model",
        "authors": "Sandeep Kumar, Yusuf Uzzaman Khan",
        "published": "2023-2-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/piecon56912.2023.10085908"
    },
    {
        "id": 22933,
        "title": "Conditional Heteroscedasticity:",
        "authors": "",
        "published": "2018-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv9hvt42.9"
    },
    {
        "id": 22934,
        "title": "Insulation optimization of power transformer leads",
        "authors": "Mladen Marković",
        "published": "2022-7-4",
        "citations": 0,
        "abstract": "Power transformers reliability, amongst other things, depends on its insulation system. High voltage leads are a part of the insulation system and should be properly insulated. This includes positioning an insulation barrier between the leads and a tank on a certain distance. This distance affects the safety factors (breakdown probability) for such system. The paper presents optimization process with which both the breakdown probability and leads vs. tank distance could be minimized. It also proposes a method with which this optimization process could be confirmed.",
        "link": "http://dx.doi.org/10.37798/2014631-4182"
    },
    {
        "id": 22935,
        "title": "SEISMIC EVALUATION OF OIL FILLED TRANSFORMER",
        "authors": "",
        "published": "2017-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21090/ijaerd.me063"
    },
    {
        "id": 22936,
        "title": "Lifecycle management of power transformers in a new energy era",
        "authors": "Carlos Gamez",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbpo104e_ch7"
    },
    {
        "id": 22937,
        "title": "Transformer-based Model for Single Documents Neural Summarization",
        "authors": "Elozino Egonmwan, Yllias Chali",
        "published": "2019",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5607"
    },
    {
        "id": 22938,
        "title": "Transformer-Based Direct Speech-To-Speech Translation with Transcoder",
        "authors": "Takatomo Kano, Sakriani Sakti, Satoshi Nakamura",
        "published": "2021-1-19",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383496"
    },
    {
        "id": 22939,
        "title": "Transformer-DARwIn: A miniature humanoid for potential robot companion",
        "authors": "Jean Chagas Vaz, Paul Oh",
        "published": "2018-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce.2018.8326294"
    },
    {
        "id": 22940,
        "title": "A Data-Driven Residential Transformer Overloading Risk Assessment Method",
        "authors": "Ming Dong, Alexandre Nassif",
        "published": "2020-8-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281616"
    },
    {
        "id": 22941,
        "title": "Operation and Maintenance of High and Low Voltage Transformer and Distribution Equipment",
        "authors": "",
        "published": "2021-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i12.171"
    },
    {
        "id": 22942,
        "title": "PCA-Assisted Location of Small Short Circuit in Transformer Winding",
        "authors": "Arash Moradzadeh, Kazem Pourhossein",
        "published": "2020-8-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee50131.2020.9260815"
    },
    {
        "id": 22943,
        "title": "Fully Transformer Detector with Multiscale Encoder and Dynamic Decoder",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2023.06.016"
    },
    {
        "id": 22944,
        "title": "Deep Scaffold Hopping with Multi-modal Transformer Neural Networks",
        "authors": "Shuangjia Zheng, Zengrong Lei, Haitao Ai, Hongming Chen, Daiguo Deng, Yuedong Yang",
        "published": "No Date",
        "citations": 4,
        "abstract": "Scaffold hopping, aiming to identify molecules with novel scaffolds but share a similar target biological activity toward known hit molecules, has always been a topic of interest in rational drug design. Computer-aided scaffold hopping would be a valuable tool but at present it suffers from limited search space and incomplete expert-defined rules and thus provides results of unsatisfactory quality. To addree the issue, we describe a fully data-driven model that learns to perform target-centric scaffold hopping tasks. Our deep multi-modal model, DeepHop, accepts a hit molecule and an interest target protein sequence as inputs and design bioisosteric molecular structures to the target compound. The model was trained on 50K experimental scaffold hopping pairs curated from the public bioactivity database, which spans 40 kinases commonly investigated by medicinal chemists. Extensive experiments demonstrated that DeepHop could design more than 70% molecules with improved bioactivity, high 3D similarity, while low 2D scaffold similarity to the template molecules. Our method achieves 2.2 times larger efficiency than state-of-the-art deep learning methods and 4.7 times than rule-based methods. Case studies have also shown the advantages and usefulness of DeepHop in practical scaffold hopping scenario.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.13011767.v1"
    },
    {
        "id": 22945,
        "title": "Three chanels device for current transformer accuracy testing",
        "authors": "Dragana Naumovic Vukovic, Slobodan Skundric, Aleksandar Zigic",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pee.2017.8171697"
    },
    {
        "id": 22946,
        "title": "Analysis of transformer condition by frequency and time methods",
        "authors": "Matej KUČERA",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2019.09.08"
    },
    {
        "id": 22947,
        "title": "Optimization of Earthing Transformer Sizing in Medium Voltage Networks",
        "authors": "Paolo Marini",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.1782"
    },
    {
        "id": 22948,
        "title": "Pansformers: Transformer-Based Self-Attention Network for Pansharpening",
        "authors": "Nithin G R, Nitish Kumar M, Venkateswaran Narasimhan, Rajanikanth Kakani, Ujjwal Gupta, Ankur Garg",
        "published": "No Date",
        "citations": 0,
        "abstract": "Pansharpening is the task of creating a High-Resolution Multi-Spectral Image (HRMS) by extracting and infusing pixel details from the High-Resolution Panchromatic Image into the Low-Resolution Multi-Spectral (LRMS). With the boom in the amount of satellite image data, researchers have replaced traditional approaches with deep learning models. However, existing deep learning models are not built to capture intricate pixel-level relationships. Motivated by the recent success of self-attention mechanisms in computer vision tasks, we propose Pansformers, a transformer-based self-attention architecture, that computes band-wise attention. A further improvement is proposed in the attention network by introducing a Multi-Patch Attention mechanism, which operates on non-overlapping, local patches of the image. Our model is successful in infusing relevant local details from the Panchromatic image while preserving the spectral integrity of the MS image. We show that our Pansformer model significantly improves the performance metrics and the output image quality on imagery from two satellite distributions IKONOS and LANDSAT-8.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17153228"
    },
    {
        "id": 22949,
        "title": "Constructing an image vision transformer for recognizing hand gestures using surface electromyography signals",
        "authors": "Aly Moslhi, Hesham H. Aly, Medhat ElMessiery",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPurpose\n Interest is growing in developing techniques and methodologies for acquiring and decoding biological signals. Different applications particularly in the area of prosthetic controlling and rehabilitation where an accurate recognition of the hand gesture using surface electromyography (sEMG) signals; are highly needed. sEMG signals are characterized by complex and high-variable information, accordingly extracting useful information from the sEMG signals requires advanced signal processing and data analysis techniques.\nMethods\n The present work uses the NinaPro Database 1, as it is an open-source database and used for benchmarking sEMG classifiers. Hand gesture recognition using sEMG signals is commonly approached using algorithms similar to those employed for image classification, which was the motive to build a Vision Transformer (ViT). Three different techniques have been studied in this work for extracting features from the sEMG signals prior classification, these techniques are: extracting the Fast Fourier Transform (FFT), extracting wavelets, and using a pre-trained convolutional neural network (CNN) for feature extraction.\nResults\n The findings revealed that the performance of the Vision Transformer (ViT) exceeded that of the majority of CNN architectures employed for classifying sEMG signals. The results also show improvements in the training accuracy when using the wavelet than using the other two feature extractors. The Vision Transformer demonstrated its capability to capture fine structures and integrate global information in imaged signals through plausible basis functions. Additionally, the attention mechanism employed by the Vision Transformer shed light on the significance of electrode readings, offering valuable insights for accurate classification.\nConclusion\n While the majority of research in hand gesture prediction using sEMG signals primarily emphasizes convolutional neural networks, it is crucial to shift attention towards exploring the potential of transformer architecture. This study further highlights the importance of feature extraction and reveals the substantial impact it has on classification accuracy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3021836/v1"
    },
    {
        "id": 22950,
        "title": "Structure-based, high transformer ratio collinear two-beam accelerator",
        "authors": "Yong Jiang, Sergey V. Shchelkunov, Jay L. Hirshfield",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.4975883"
    },
    {
        "id": 22951,
        "title": "JDAT: Joint-Dimension-Aware Transformer with Strong Flexibility for EEG Emotion Recognition",
        "authors": "Zeyu Wang, Ziqun Zhou, Haibin Shen, Qi Xu, Kejie Huang",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>Electroencephalography (EEG) emotion recognition, an important task in Human-Computer Interaction (HCI), has made a great breakthrough with the help of deep learning algorithms. Although the application of attention mechanism on conventional models has improved its performance, most previous research rarely focused on multiplex EEG features jointly, lacking a compact model with unified attention modules. This study proposes Joint-Dimension-Aware Transformer (JDAT), a robust model based on squeezed Multi-head Self-Attention (MSA) mechanism for EEG emotion recognition. The adaptive squeezed MSA applied on multidimensional features enables JDAT to focus on diverse EEG information, including space, frequency, and time. Under the joint attention, JDAT is sensitive to the complicated brain activities, such as signal activation, phase-intensity couplings, and resonance. Moreover, its gradually compressed structure contains no recurrent or parallel modules, greatly reducing the memory and complexity, and accelerating the inference process. The proposed JDAT is evaluated on DEAP, DREAMER, and SEED datasets, and experimental results show that it outperforms state-of-the-art methods along with stronger flexibility.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.17056961"
    },
    {
        "id": 22952,
        "title": "Design, Manufacturing and Testing of a Standard Current Transformer",
        "authors": "Fulufhelo Tshisikhawe, Mpho J. Lencwe, SP Daniel Chowdhury",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica.2019.8928879"
    },
    {
        "id": 22953,
        "title": "Technology and Application of Intelligent Sensing and State Sensing for Transformer Equipment",
        "authors": "",
        "published": "2021-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i9.184"
    },
    {
        "id": 22954,
        "title": "Linearizing Transformer with Key-Value Memory",
        "authors": "Yizhe Zhang, Deng Cai",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.24"
    },
    {
        "id": 22955,
        "title": "Research and Design of Three-Stage Power Electronic Transformer",
        "authors": "Zelong Ni",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpeee51686.2021.9383420"
    },
    {
        "id": 22956,
        "title": "Transformer based Refinement Network for Accurate Crack Detection",
        "authors": "Jing-Ming Guo, Herleeyandi Markoni",
        "published": "2021-8-26",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsse52999.2021.9538477"
    },
    {
        "id": 22957,
        "title": "Transformer-Based Bug/Feature Classification",
        "authors": "Ceyhun E. Öztürk, Eyup Halit Yilmaz, Ömer KÖKSAL",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223806"
    },
    {
        "id": 22958,
        "title": "Incorporating Locality into Vision Transformer Via Spectral Graph Convolutional Network",
        "authors": "Longbin Jin, Eun Yi Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4206868"
    },
    {
        "id": 22959,
        "title": "Hierarchical Cross-Distance Attentive Transformer for Group Activity Recognition",
        "authors": "Longteng Kong, Zhaofeng He, Junge Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4292756"
    },
    {
        "id": 22960,
        "title": "Dynamic differential current-based transformer protection using convolutional neural network",
        "authors": "",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2021.02120"
    },
    {
        "id": 22961,
        "title": "Improving Power Transformer Lifetime Prediction using Hyperparameter Optimization",
        "authors": "Ayu Ahadi Ningrum, Ihdalhubbi Maulida, Rudy Ansari, Iwan Syarif",
        "published": "2022-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ies55876.2022.9888428"
    },
    {
        "id": 22962,
        "title": "A Piece-wise Linearized Transformer Winding Model for the Analysis of Internal Voltage Propagation",
        "authors": "Andreas Theocharis, Marjan Popov",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ptc.2019.8810920"
    },
    {
        "id": 22963,
        "title": "Transformer optimization for 3D PLUS Universal RCN Front End converter",
        "authors": "Cedric Colonna, Patrick Dubus, Denis Labrousse",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/espc.2019.8932006"
    },
    {
        "id": 22964,
        "title": "Pansformers: Transformer-Based Self-Attention Network for Pansharpening",
        "authors": "Nithin G R, Nitish Kumar M, Venkateswaran Narasimhan, Rajanikanth Kakani, Ujjwal Gupta, Ankur Garg",
        "published": "No Date",
        "citations": 2,
        "abstract": "Pansharpening is the task of creating a High-Resolution Multi-Spectral Image (HRMS) by extracting and infusing pixel details from the High-Resolution Panchromatic Image into the Low-Resolution Multi-Spectral (LRMS). With the boom in the amount of satellite image data, researchers have replaced traditional approaches with deep learning models. However, existing deep learning models are not built to capture intricate pixel-level relationships. Motivated by the recent success of self-attention mechanisms in computer vision tasks, we propose Pansformers, a transformer-based self-attention architecture, that computes band-wise attention. A further improvement is proposed in the attention network by introducing a Multi-Patch Attention mechanism, which operates on non-overlapping, local patches of the image. Our model is successful in infusing relevant local details from the Panchromatic image while preserving the spectral integrity of the MS image. We show that our Pansformer model significantly improves the performance metrics and the output image quality on imagery from two satellite distributions IKONOS and LANDSAT-8.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17153228.v1"
    },
    {
        "id": 22965,
        "title": "Cyclic Diffeomorphic Transformer Nets For Contour Alignment",
        "authors": "Ilya Kaufman, Ron Shapira Weber, Oren Freifeld",
        "published": "2021-9-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506570"
    },
    {
        "id": 22966,
        "title": "Novel PEV Charging Approaches for Extending Transformer Life",
        "authors": "Theron Smith, Joseph Garcia, Gregory Washington",
        "published": "2022-6-18",
        "citations": 6,
        "abstract": "The study investigates how variable rate charging can affect PEV charging and identifies how this capability can be integrated into residential neighborhoods. The results show that creating PEV chargers that can deliver variable rates will enhance uncontrolled and controlled PEV charging. The integration is summarized into 4 phases. In phase 1, uncontrolled PEV chargers should be enabled to provide any rate to vehicles within 0 to 11.5 kW, which can reduce overloading by up to 28.34%. Phase 2 introduces smart chargers that use forecasted data to determine the optimal time intervals for PEVs to charge using a fixed rate of 4.8 kW, capable of reducing overloading by 42.69%. In Phase 3, a controlled smart charging strategy that can deliver any rate to a vehicle using SRVF’s approach is proposed, which will reduce overloading by up to 42.87%. Lastly, phase 4 recommends a smart charging control that can deliver any rate to a vehicle using RIVF’s approach, reducing overloading by up to 43.37%.",
        "link": "http://dx.doi.org/10.3390/en15124454"
    },
    {
        "id": 22967,
        "title": "Bérurier Noir :  transformer la violence en l’énergie d’une jeunesse solidaire",
        "authors": "François Guillemot",
        "published": "2021-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/rhiz.080.0038"
    },
    {
        "id": 22968,
        "title": "Thermal Properties of TiO2 Nanoparticles Treated Transformer Oil and Coconut Oil",
        "authors": "Kaveenga Rasika Koswattage, Ashan Induranga, Chanaka Galpaya, Vimukthi Vithanage",
        "published": "No Date",
        "citations": 0,
        "abstract": "Heat transfer fluids are used in various industrial systems to maintain them in perfect operating conditions. Extensive research efforts have been dedicated to enhance the thermal properties of these heat transfer fluids to improve their efficiency. Developing nanofluids is a potential candidate for such enhancements. This study investigates the impact of incorporating TiO2 nanoparticles into two types of oils: transformer oil (NYTRO LIBRA) and virgin coconut oil (manufactured by Govi Aruna Pvt. Ltd.) at different temperatures and with varying volume fractions. The nanofluids were prepared using a two-step method by adding CTAB (Cetyltrimethylammonium bromide) surfactant. To minimize nanoparticle agglomeration, this study employed relatively low-volume fractions. Thermal properties by means of thermal conductivity, thermal diffusivity, and volumetric heat capacity were measured in accordance with ASTM (American Society for Testing and Materials) standard methods, using a multifunctional thermal conductivity meter (LAMBDA thermal conductivity meter). The measured thermal conductivity values were compared with theoretical models and previous research findings. It was confirmed that the modification of thermal properties was enhanced by doping TiO2 nanoparticles with different volume fraction.",
        "link": "http://dx.doi.org/10.20944/preprints202309.1682.v1"
    },
    {
        "id": 22969,
        "title": "Thai/English Cross-Language Transliterated Word Retrieval using Transformer",
        "authors": "Apichad Chodkawanich, Boonserm Kijsirikul",
        "published": "2021-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnlp52887.2021.00023"
    },
    {
        "id": 22970,
        "title": "Transformer-Based Hierarchical Dynamic Decoders for Salient Object Detection",
        "authors": "Qingping Zheng, Ling Zheng, Jiankang Deng, Ying Li, Changjing Shang, Qiang Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552874"
    },
    {
        "id": 22971,
        "title": "Visual Transformer for Soil Classification",
        "authors": "Aaryan Jagetia, Umang Goenka, Priyadarshini Kumari, Mary Samuel",
        "published": "2022-7-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sces55490.2022.9887636"
    },
    {
        "id": 22972,
        "title": "Introduction. Transformer la ville : émergence des villes durables dans les Amériques",
        "authors": "Pablo Ligrone, Laurent Roesch",
        "published": "2021-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/orda.6354"
    },
    {
        "id": 22973,
        "title": "Revolutionizing Groundwater Level Prediction in Taiwan: Unleashing the Power of Transformer Neural Networks",
        "authors": "Wei Sun, Jia-Yi Liou, Fi-John Chang",
        "published": "No Date",
        "citations": 0,
        "abstract": "&#160; &#160; In the face of evolving global weather patterns attributed to climate change, precise prediction of groundwater levels is increasingly essential for effective water resource management. This significance is particularly pronounced in regions like Taiwan, where groundwater is a pivotal water source. This study focuses on the Zhuoshui River basin in central Taiwan and explores a Transformer Neural Network (TNN) based on a 20-year hydrometeorological dataset at a 10-day scale to predict groundwater levels. Our investigation reveals that the innovative TNN model outperforms conventional models, such as the Convolutional Neural Network (CNN) and the Long Short-Term Memory neural network (LSTM). The TNN model's superiority is evidenced by its enhanced predictive capabilities, as measured by metrics like R2 and MAE. Notably, the TNN model excels in providing precise forecasts (MAE < 1 m) for the majority of groundwater monitoring stations, notwithstanding challenges in areas facing overexploitation.\n&#160; &#160; This groundbreaking study marks the first attempt of the TNN model to predict groundwater levels, showcasing its robust performance and broad applicability. The TNN model emerges as a valuable tool for groundwater level prediction, contributing to sustainable groundwater management and effective resource utilization amid the backdrop of climate change. With the potential to address climate-related challenges, the TNN model stands as a pivotal asset for optimizing strategies in groundwater resource management.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-2293"
    },
    {
        "id": 22974,
        "title": "A Transformer-Based Data-Driven Model for Real-Time Spatio-Temporal Flood Prediction",
        "authors": "Matteo Pianforini, Susanna Dazzi, Andrea Pilzer, Renato Vacondio",
        "published": "No Date",
        "citations": 0,
        "abstract": "Among the non-structural strategies for mitigating the huge economic losses and casualties caused by floods, the implementation of early-warning systems based on real-time forecasting of flood maps is one of the most effective. The high computational cost associated with two-dimensional (2D) hydrodynamic models, however, prevents their practical application in this context. To overcome this drawback, &#8220;data-driven&#8221; models are gaining considerable popularity due to their high computational efficiency for predictions. In this work, we introduce a novel surrogate model based on the Transformer architecture, named FloodSformer (FS), that efficiently predicts the temporal evolution of inundation maps, with the aim of providing real-time flood forecasts. The FS model combines an encoder-decoder (2D Convolutional Neural Network) with a Transformer block that handles temporal information. This architecture extracts the spatiotemporal information from a sequence of consecutive water depth maps and predicts the water depth map at one subsequent instant. An autoregressive procedure, based on the trained surrogate model, is employed to forecast tens of future maps.As a case study, we investigated the hypothetical inundation due to the collapse of the flood-control dam on the Parma River (Italy). Due to the absence of real inundation maps, the training/testing dataset for the FS model was generated from numerical simulations performed through a 2D shallow&#8208;water code (PARFLOOD). Results show that the FS model is able to recursively forecast the next 90 water depth maps (corresponding to 3 hours for this case study, in which maps are sampled at 2-minute intervals) in less than 1 minute. This is achieved while maintaining an accuracy deemed entirely acceptable for real-time applications: the average Root Mean Square Error (RMSE) is about 10 cm, and the differences between ground-truth and predicted maps are generally lower than 25 cm in the floodable area for the first 60 predicted frames. In conclusion, the short computational time and the good accuracy ensured by the autoregressive procedure make the FS model suitable for early-warning systems.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-566"
    },
    {
        "id": 22975,
        "title": "Grid-Transformer for Few-Shot Hyperspectral Image Classification",
        "authors": "Ying Guo, Mingyi He, Bin Fan",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222023"
    },
    {
        "id": 22976,
        "title": "3D Facial Expression Generator Based on Transformer VAE",
        "authors": "Kaifeng Zou, Boyang Yu, Hyewon Seo",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222674"
    },
    {
        "id": 22977,
        "title": "Guided Transformer for Machine Translation: English to Hindi",
        "authors": "Akhilesh Bisht, Deepa Gupta, Shantipriya Parida",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440876"
    },
    {
        "id": 22978,
        "title": "High-Efficiency, Medium-Voltage-Input, Solid-State-Transformer-Based 400-kW/1000V/400A Extreme Fast Charger for Electric Vehicles",
        "authors": "Charles Zhu",
        "published": "2023-7-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1987553"
    },
    {
        "id": 22979,
        "title": "Graphdeformer: A Spatio-Temporal Model Integrating Graph Neural Network and Transformer for Wind Power Forecasting",
        "authors": "Jing Teng, Yajun Jiang, Ruifeng Shi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4750487"
    },
    {
        "id": 22980,
        "title": "On Designing a SwinIris Transformer Based Iris Recognition System",
        "authors": "Runqing Gao, Thirimachos Bourlai",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3369035"
    },
    {
        "id": 22981,
        "title": "Chapter 7: Working with GPT-3 Introduction",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-008"
    },
    {
        "id": 22982,
        "title": "Japanese mayfly family classification with a vision transformer model",
        "authors": "Yuichi Iwasaki, Hiroko Arai, Akihiro Tamada, Hirokatsu Kataoka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Benthic macroinvertebrates are a frequently used indicator group for biomonitoring and biological assessment of river ecosystems. However, their taxonomic identification is laborious and requires special expertise. In this study, we aimed to assess the capability of a vision transformer (ViT) model for family-level identification of mayflies (order Ephemeroptera). Specifically, we focused on evaluating the model’s capacity to classify three commonly found mayfly families (Baetidae, Ephemerellidae, and Heptageniidae) as well as other families that were grouped together. For the modeling, we originally constructed two different image datasets containing a total of 1,110 images of mayflies, which were split into training and validation datasets, and a test dataset was prepared from two different online photo galleries. The developed ViT model achieved reasonable accuracy, reaching 94.2% and 82.9% for the validation and test datasets, respectively. Given the use of a relatively small number of images in the training process, as well as some variations in the visual styles of the test dataset compared to the training dataset, we consider the level of accuracy to be high. Our results are encouraging toward the use of computer vision for taxonomic identification of macroinvertebrates, although there is still a need to develop specific designs and plans for this purpose, which can vary depending on regional differences in biodiversity as well as sampling and survey methods.",
        "link": "http://dx.doi.org/10.32942/x2p02k"
    },
    {
        "id": 22983,
        "title": "Multi-Horizon Predictions for Parking Availability at Multiple Lots Using Temporal Fusion Transformer",
        "authors": "Sujin Lee, Jungmin Kim, Yoonjin Yoon, Kitae Jang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760777"
    },
    {
        "id": 22984,
        "title": "Transformer-based Convolution-free Visual Place Recognition",
        "authors": "Anna Urban, Bogdan Kwolek",
        "published": "2022-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarcv57592.2022.10004334"
    },
    {
        "id": 22985,
        "title": "Heterogeneous Graph Transformer",
        "authors": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun",
        "published": "2020-4-20",
        "citations": 418,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3366423.3380027"
    },
    {
        "id": 22986,
        "title": "T1/T2 Relaxation Temporal Modelling from Accelerated Acquisitions Using a Latent Transformer",
        "authors": "Michael Tänzer, Fanwen Wang, Mengyun Qiao, Wenjia Bai, Daniel Rueckert, Guang Yang, Sonia Nielles-Vallespin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-52448-6_28"
    },
    {
        "id": 22987,
        "title": "A decoupling control method for multi-ports DC transformer",
        "authors": "Hao Wu",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacre50138.2020.9229966"
    },
    {
        "id": 22988,
        "title": "Multimodal Transformer for Multimodal Machine Translation",
        "authors": "Shaowei Yao, Xiaojun Wan",
        "published": "2020",
        "citations": 48,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.400"
    },
    {
        "id": 22989,
        "title": "Review for \"Investigation on magnetostrictive behaviour of a converter transformer influenced by dominant harmonics: A FEM and ANN based approach\"",
        "authors": "YOGENDRA ARYA",
        "published": "2021-5-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12957/v1/review2"
    },
    {
        "id": 22990,
        "title": "Review for \"Investigation on magnetostrictive behaviour of a converter transformer influenced by dominant harmonics: A FEM and ANN based approach\"",
        "authors": "Elena Helerea",
        "published": "2021-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12957/v1/review1"
    },
    {
        "id": 22991,
        "title": "Perception-Oriented Omnidirectional Image Super-Resolution Based on Transformer Network",
        "authors": "Hongyu An, Xinfeng Zhang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222760"
    },
    {
        "id": 22992,
        "title": "AICT: An Adaptive Image Compression Transformer",
        "authors": "Ahmed Ghorbel, Wassim Hamidouche, Luce Morin",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222799"
    },
    {
        "id": 22993,
        "title": "Contrastive Feature Masking Open-Vocabulary Vision Transformer",
        "authors": "Dahun Kim, Anelia Angelova, Weicheng Kuo",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01430"
    },
    {
        "id": 22994,
        "title": "In-Service Mineral Insulating Oil",
        "authors": "Behrooz Vahidi, Ashkan Teymouri",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-19693-6_2"
    },
    {
        "id": 22995,
        "title": "Time Series Visualization of Natural Disaster Prediction Using Transformer",
        "authors": "",
        "published": "2022-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53469/jrse.2022.04(08).17"
    },
    {
        "id": 22996,
        "title": "In Situ Raman Spectra of a Conical Conductive Shell as an Electro Galvanic Spin Orbit Transformer (SPOT)",
        "authors": "Florian Fontein, Ulrich Fischer",
        "published": "No Date",
        "citations": 0,
        "abstract": "Experimental results are interpreted, which were obtained by tip enhanced Raman Spectroscopy (TERS) in a specific SNOM (Scanning Near-Field Optical Microscope) STM (Scanning Tunneling Microscope) configuration with a tunnel gap. The interpretation is performed in terms of a classical physical model of a photon as a vacuum phonon polariton. The metal tip is considered as a conductive infinitely thin shell of a diamagnetic electron gas. The stationary single electron tunnel current of an electron tunnel passage frequency f \n\ntunnel\n\n  carries not the charge e but is shown to carry an energy hf\n\ne\n and an isotropic orbital angular momentum h/4π which corresponds to an isotropic transversal electron spin density of an angular frequency 2π\n\n2\n\nf\n\ne\n.Raman Spectroscopy, Electron Spin, Photon Spin, relativistic Quantum mechanicsThe electron spin current corresponds to an isotropic azimuthal inertial torque which acts on the photon by an increase of its energy and torque. Neglecting the influence of thermal and radiative losses of the conductive shell, the interpretation reproduces the spectroscopic results within the uncertainty of the experimental results. The photoelectric Raman spectrum of the tunnel gap can be regarded as a complex admittance spectrum which depends in a nonlinear way on the tunnel  voltage, the tunnel current and the wavelength of the scattered light. The conical tunnel gap configuration can be considered as a bipolar Electro-Galvanic Spin -Orbit Transformer (SPOT) and from a thermodynamic viewpoint as a Quantum Energy Converter",
        "link": "http://dx.doi.org/10.3762/bxiv.2019.161.v1"
    },
    {
        "id": 22997,
        "title": "On Defining Smart Cities using Transformer Neural Networks",
        "authors": "Andrei Khurshudov",
        "published": "2024-1-28",
        "citations": 0,
        "abstract": "Cities worldwide are rapidly adopting “smart” technologies, transforming urban life. Despite this trend, a universally accepted definition of “smart city” remains elusive. Past efforts to define it haven’t yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new “compromise” definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing definitions of smart cities, generating a list of potential new composite definitions. Each of these new definitions was then tested against the pre-existing individual definitions we’ve gathered, using cosine similarity as our metric. This process identified smart city definitions with the highest average cosine similarity, semantically positioning them as the closest on average to all the 60 individual definitions selected.",
        "link": "http://dx.doi.org/10.24297/ijct.v24i.9579"
    },
    {
        "id": 22998,
        "title": "An Improved Transformer-based Concrete Crack Classification Method",
        "authors": "Guanting Ye, Wei Dai, Jinsheng Qu, Jintai Tao, Lin Zhu, Qiang Jin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn concrete structures, surface cracks are an important indicator for assessing the durability and serviceability of the structure. Existing convolutional neural networks for concrete crack identification are inefficient and computationally costly. Therefore, a new CSWin transformer-skip (CSW-S) is proposed to classify concrete cracks. The method is optimized by adding residual links to the existing CSWin transformer network and then trained and tested using a dataset with 17,000 images. The experimental results show that the improved CSW-S network has an extended range of extracted image features, which improves the accuracy of crack recognition. A detection accuracy of 96.92% is obtained using the trained CSW-S without pretraining. The improved transformer model has higher recognition efficiency and accuracy than the traditional transformer model and the classical CNN model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2822355/v1"
    },
    {
        "id": 22999,
        "title": "Photovoltaic Integration of a Transformer-based Multi Level Inverter With Lesser Components",
        "authors": "Arya Venugopal",
        "published": "2020-10-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iprecon49514.2020.9315191"
    },
    {
        "id": 23000,
        "title": "Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation",
        "authors": "Piyush Vyas, Anastasia Kuznetsova, Donald S. Williamson",
        "published": "2021-8-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-2007"
    },
    {
        "id": 23001,
        "title": "TBSN: Sparse-Transformer Based Siamese Network for Few-Shot Action Recognition",
        "authors": "Jianglong He, Shuai Gao",
        "published": "2021-5-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc51749.2021.9441568"
    },
    {
        "id": 23002,
        "title": "Voltage Control with Thyristor-Regulated Booster Transformer",
        "authors": "Elena Sosnina, Anatoliy Asabin, Alexey Kralin, Evgeny Kryukov",
        "published": "2018-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgwcp.2018.8634477"
    },
    {
        "id": 23003,
        "title": "Transformer-based Arabic Dialect Identification",
        "authors": "Wanqiu Lin, Maulik Madhavi, Rohan Kumar Das, Haizhou Li",
        "published": "2020-12-4",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp51396.2020.9310504"
    },
    {
        "id": 23004,
        "title": "High Dynamic Range Imaging with Context-aware Transformer",
        "authors": "Fangfang Zhou, Zhengming Fu, Dan Zhang",
        "published": "2023-6-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191491"
    },
    {
        "id": 23005,
        "title": "Small Objects Detection Using Transformer-CNN",
        "authors": "Chun-Liang Lin, Yan-Lin Chen, Yu-Chen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4232850"
    },
    {
        "id": 23006,
        "title": "A transformer model for predicting cognitive impairment from sleep",
        "authors": "Tzu-An Song, Masoud Malekzadeh, Richa Saxena, Shaun M. Purcell, Joyita Dutta",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSleep disturbances are known to be aggravated with normal aging. Additionally, sleep disruptions have a potentially bidirectional causal relationship with dementia due to neurodegenerative diseases like Alzheimer’s disease. Predictive techniques that can automatically detect cognitive impairment from an individual’s sleep data have broad clinical and biological significance. Here, we present a deep learning approach based on a transformer architecture to predict cognitive status from sleep electroencephalography (EEG) data. This work uses data from N = 1, 502 subjects from the Multi-Ethnic Study of Atherosclerosis (MESA) cohort. Our transformer model achieves 70.22% accuracy at the binary classification task for distinguishing cognitively normal and impaired subjects based on their sleep EEG. Our method outperforms traditional feature handcrafting, which has an overall accuracy of 57.61% for the same task. We use a sparse regression model to understand and interpret the information captured by each learned feature from our transformer model. To our knowledge, this is the first effort to use deep learning to predict cognitive impairment from sleep metrics.",
        "link": "http://dx.doi.org/10.1101/2022.07.17.500351"
    },
    {
        "id": 23007,
        "title": "Linear Variable Differential Transformer Temperature Compensation Technique",
        "authors": "Wandee Petchmaneelumka, Pitsini Mano, Vanchai Riewruja",
        "published": "2018-10-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18494/sam.2018.1816"
    },
    {
        "id": 23008,
        "title": "Adversarial Audio Detection Method Based on Transformer",
        "authors": "Yunchen Li, Da Luo",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlise57402.2022.00023"
    },
    {
        "id": 23009,
        "title": "PVTReID: A Quick Person Re-Identification Based Pyramid Vision Transformer",
        "authors": "Ke Han, QianLong Wang, Mingming Zhu, Xiyan Zhang",
        "published": "No Date",
        "citations": 1,
        "abstract": "Due to the influence of background conditions, lighting conditions, occlusion issues and the image resolution, how to extract robust person features is one of the difficulties in ReID research. Vision in Transformers (ViT) has achieved significant results in the field of computer vision. However, the existing problems still limit its application in ReID due to slow extraction of person features and difficulty in utilizing local features of people. To solve the mentioned problems, we utilize Pyramid Vision Transformer (PVT) as the backbone of feature extraction and propose a PVT-based ReID method in conjunction with other studies. Firstly, some improvements suitable for ReID are used on the PVT backbone, and we establish a basic model by using powerful methods verified on CNN-based ReID. Secondly, in an effort to further promote the robustness of the person features extracted by the PVT backbone, two new modules are designed. (1) The local feature clustering (LFC) is recommend to enhance the robustness of person features by calculating the distance between local features and global feature to select the most discrete local features and clustering them. (2) The side information embeddings (SIE) are used to encode non-visual information and send it into the network for training to reduce its impact on person features. Finally, the experiments show that PVTReID has achieved excellent results in ReID datasets and are 20% faster on average than CNN-based ReID methods.",
        "link": "http://dx.doi.org/10.20944/preprints202308.0373.v1"
    },
    {
        "id": 23010,
        "title": "TRANSFORMER OUTPUT CIRCUITS",
        "authors": "",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315214856-2"
    },
    {
        "id": 23011,
        "title": "A Reversible Transformer for LiDAR Point Cloud Semantic Segmentation",
        "authors": "Perpertual Hope Akwensi, Ruisheng Wang",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/crv60082.2023.00011"
    },
    {
        "id": 23012,
        "title": "Dyformer: A Dynamic Transformer-Based Architecture for Multivariate Time Series Classification",
        "authors": "Chao Yang, Xianzhi Wang, Lina Yao, Guodong Long, Guandong Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4362666"
    },
    {
        "id": 23013,
        "title": "Fault minimization by phaser correction of transformer",
        "authors": "Vidhatri Gujela, Om Prakash Gujela",
        "published": "2019-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i-pact44901.2019.8960001"
    },
    {
        "id": 23014,
        "title": "Simulation study on the lightning overvoltage invasion control transformer intelligent substation",
        "authors": "Chuyan Xi, Jie Hao, Ying Zhang",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5033839"
    },
    {
        "id": 23015,
        "title": "YOLOv5 UAV Feature Detection with Swin-Transformer",
        "authors": "Lizhong Xiao, Shuai Li",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciibms60103.2023.10347720"
    },
    {
        "id": 23016,
        "title": "Insulating Media",
        "authors": "Thomas A. Prevost, David L. Hanson, Leo J. Savio, Ted Haupert",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-12"
    },
    {
        "id": 23017,
        "title": "Evaluation of DFIG Wind Turbine Generator and Transformer Conditions with Electrical Signature Analysis",
        "authors": "Howard W Penrose",
        "published": "2022-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic51169.2022.9833161"
    },
    {
        "id": 23018,
        "title": "A Weighting Factor-Fuzzy logic based Transformer Residual Life Estimation Model",
        "authors": "Edwell. T. Mharakurwa, Shaibu. A. Juma",
        "published": "2021-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica52236.2021.9543460"
    },
    {
        "id": 23019,
        "title": "Transformer for Ultrafast Ultrasound Localization Microscopy",
        "authors": "Gaobo Zhang, Yaoting Yue, Fei Dai, Xin Liu, Dean Ta",
        "published": "2022-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ius54386.2022.9958335"
    },
    {
        "id": 23020,
        "title": "Transformer-based Algorithm for Commodity Detection in Fisheye Images",
        "authors": "Chen Zhang, Tangwen Yang",
        "published": "2022-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp56322.2022.9965271"
    },
    {
        "id": 23021,
        "title": "Transformer based Ensemble Learning to Hate Speech Detection Leveraging Sentiment and Emotion Knowledge Sharing",
        "authors": "Prashant Kapil, Asif Ekbal",
        "published": "2022-6-18",
        "citations": 0,
        "abstract": "In recent years, the increasing propagation of hate speech on social media has encouraged researchers to address the problem of hateful content identification. To build an efficient hate speech detection model, a large number of annotated data is needed to train the model. To solve this approach we utilized eleven datasets from the hate speech domain and compared different transformer encoder-based approaches such as BERT, and ALBERT in single-task learning and multi-task learning (MTL) framework. We also leveraged the eight sentiment and emotion analysis datasets in the training to enrich the features in the MTL setting. The stacking based ensemble of BERT-MTL and ALBERT-MTL is utilized to combine the features from best two models. The experiments demonstrate the efficacy of the approach by attaining state-of-the-art results in all the datasets. The qualitative and quantitative error analysis was done to figure out the misclassified tweets and the effect of models on the different data sets.",
        "link": "http://dx.doi.org/10.5121/csit.2022.121014"
    },
    {
        "id": 23022,
        "title": "Constant-Voltage Transformers",
        "authors": "Arindam Maitra, Anish Gaikwad, Arshad Mansoor, Douglas Dorr, Ralph Ferraro",
        "published": "2017-12-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b12110-9"
    },
    {
        "id": 23023,
        "title": "JDAT: Joint-Dimension-Aware Transformer with Strong Flexibility for EEG Emotion Recognition",
        "authors": "Zeyu Wang, Ziqun Zhou, Haibin Shen, Qi Xu, Kejie Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Electroencephalography (EEG) emotion recognition, an important task in Human-Computer Interaction (HCI), has made a great breakthrough with the help of deep learning algorithms. Although the application of attention mechanism on conventional models has improved its performance, most previous research rarely focused on multiplex EEG features jointly, lacking a compact model with unified attention modules. This study proposes Joint-Dimension-Aware Transformer (JDAT), a robust model based on squeezed Multi-head Self-Attention (MSA) mechanism for EEG emotion recognition. The adaptive squeezed MSA applied on multidimensional features enables JDAT to focus on diverse EEG information, including space, frequency, and time. Under the joint attention, JDAT is sensitive to the complicated brain activities, such as signal activation, phase-intensity couplings, and resonance. Moreover, its gradually compressed structure contains no recurrent or parallel modules, greatly reducing the memory and complexity, and accelerating the inference process. The proposed JDAT is evaluated on DEAP, DREAMER, and SEED datasets, and experimental results show that it outperforms state-of-the-art methods along with stronger flexibility.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.17056961.v1"
    },
    {
        "id": 23024,
        "title": "Fuzzy System and Dq0 Transformation for Protection of Power Transformer",
        "authors": "V. K. Sahu, Y. Pahariya",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4159622"
    },
    {
        "id": 23025,
        "title": "Introduction. Transformer la nature, anthropologie du pharmakôn",
        "authors": "Andrea-Luz Gutierrez-Choquevilca",
        "published": "2017-4-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/cas.014.0009"
    },
    {
        "id": 23026,
        "title": "Time series prediction of transformer oil chromatography based on Attention-PSO-GRU model",
        "authors": "Yuanxin Teng, Guan Wang, Xiaomeng Su, Meiying Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The content of dissolved gas in transformer oil is an important\ncharacteristic to reflect the operating condition of transformers. To\novercome the problems that the traditional transformer oil\nchromatography gas prediction model cannot effectively use the older\ntransformer oil chromatography data and it is difficult to track the\ntransformer oil chromatography data under abnormal conditions and it can\nonly rely on experience to adjust the model parameters, we introduce\nGated Recurrent Unit (GRU), Attention Mechanism (APM) and Particle Swarm\nOptimization (PSO) to construct the Attention-PSO-GRU prediction model.\nThe experimental results show that the Attention-PSO-GRU model can reach\n97.9% accuracy in predicting the normal transformer oil chromatography\ndata and has a better tracking ability for the abnormal transformer oil\nchromatography data. Therefore, the Attention-PSO-GRU model proposed in\nthis paper can effectively improve the accuracy of transformer oil\nchromatography gas prediction, which has practical significance for\npreventing transformer faults and ensuring the safe and stable operation\nof power systems.",
        "link": "http://dx.doi.org/10.22541/au.166976016.64632334/v1"
    },
    {
        "id": 23027,
        "title": "Efficient Transformer with Locally Shared Attention for Video Quality Assessment",
        "authors": "Junyong You, Yuan Lin",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9898025"
    },
    {
        "id": 23028,
        "title": "Withdrawal: Transformer-based comparative multi-view illegal transaction detection",
        "authors": " ",
        "published": "2023-4-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1371/journal.pone.0283785"
    },
    {
        "id": 23029,
        "title": "A Transformer-based Method for Recommending the Continuing Care Retirement Communities",
        "authors": "Zhenggui Xiang",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385926"
    },
    {
        "id": 23030,
        "title": "Pose-aware Disentangled Multiscale Transformer for Pose Guided Person Image Generation",
        "authors": "Kei Shibasaki, Masaaki Ikehara",
        "published": "2023-9-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10290101"
    },
    {
        "id": 23031,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2019-10-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v1/review1"
    },
    {
        "id": 23032,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2020-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v3/review2"
    },
    {
        "id": 23033,
        "title": "(In)sécurité numérique et PME : transformer les défis en atouts",
        "authors": "Eric Hazane",
        "published": "2017-2-10",
        "citations": 0,
        "abstract": "Si les cas de cyberattaques frappant les grands groupes industriels focalisent l’attention des médias, les PME sont quotidiennement visées par des cyberattaquants appâtés par la vulnérabilité de petites structures peu ou pas protégées, faute de moyens ou de sensibilisation à la nouvelle économie numérique. Dans cet article Eric Hazane, expert en cybersécurité et cofondateur d’EchoRadar 1 , s’attarde sur l’enjeu pour les PME de cette insécurité numérique. Creuset de l’emploi de demain, les petites et moyennes entreprises sont exposées au défi de la transition numérique qui nécessite d’en saisir les opportunités autant que d’en apprécier les risques, qui peuvent être létaux. L’auteur plaide notamment pour que les TPE/PME qui n’ont pas l’assise financière pour consacrer un budget à la cybersécurité se regroupent, à l’échelle d’une région, pour investir en commun – donc à moindre coût – afin de disposer d’une palette d’outils de sécurisation en amont et d’un soutien technique, juridique et financier en aval.",
        "link": "http://dx.doi.org/10.3917/sestr.022.0014"
    },
    {
        "id": 23034,
        "title": "Scene Text Image Super-Resolution Via Content Perceptual Loss and Criss-Cross Transformer Blocks",
        "authors": "Rui Qin, Yu-wing Tai, Bin Wang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4464486"
    },
    {
        "id": 23035,
        "title": "End-to-End Heart Failure Detection Based on Multipath Transformer Encoder",
        "authors": "Nan Jiang, Siyu Yang, Shudong Xia, Yanyun Dai, Jijun Tong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4639512"
    },
    {
        "id": 23036,
        "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
        "authors": "Guan-Lin Chao, Ian Lane",
        "published": "2019-9-15",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1355"
    },
    {
        "id": 23037,
        "title": "Fast Point Transformer",
        "authors": "Chunghyun Park, Yoonwoo Jeong, Minsu Cho, Jaesik Park",
        "published": "2022-6",
        "citations": 46,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.01644"
    },
    {
        "id": 23038,
        "title": "A Generative Model for Raw Audio Using Transformer Architectures",
        "authors": "Prateek Verma, Chris Chafe",
        "published": "2021-9-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/dafx51585.2021.9768298"
    },
    {
        "id": 23039,
        "title": "Primer: Fast Private Transformer Inference on Encrypted Data",
        "authors": "Mengxin Zheng, Qian Lou, Lei Jiang",
        "published": "2023-7-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247719"
    },
    {
        "id": 23040,
        "title": "MCformer: A Transformer Based Deep Neural Network for Automatic Modulation Classification",
        "authors": "Shahab Hamidi-Rad, Swayambhoo Jain",
        "published": "2021-12",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685815"
    },
    {
        "id": 23041,
        "title": "Training Tips for the Transformer Model",
        "authors": "Martin Popel, Ondřej Bojar",
        "published": "2018-4-1",
        "citations": 75,
        "abstract": "Abstract\nThis article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra “more data and larger models”, we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.",
        "link": "http://dx.doi.org/10.2478/pralin-2018-0002"
    },
    {
        "id": 23042,
        "title": "Code Summarization with Structure-induced Transformer",
        "authors": "Hongqiu Wu, Hai Zhao, Min Zhang",
        "published": "2021",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.93"
    },
    {
        "id": 23043,
        "title": "WITHDRAWN: A Transformer based approach using LSTM and Paraphrase reference to Translate English Text into Hindi",
        "authors": "Surbhi Sharma, Nisheeth Joshi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn, as it was submitted in error. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230686/v2"
    },
    {
        "id": 23044,
        "title": "Equilibrium models vs. evolutionary economic models",
        "authors": "Lawrence A. Boland",
        "published": "2017-5-18",
        "citations": 0,
        "abstract": "This chapter examines the extent to which models based on evolutionary economics can provide a worthy alternative to neoclassical equilibrium models. The chapter discusses the difference between Darwinian and non-Darwinian evolutionary economic models. The chapter includes a discussion of Armen Alchian’s 1950 article that he says is erroneously identified as evolutionary economics. The works of evolutionary economists Ulrich Witt, Jack Vromen, Richard Nelson and Sidney are explained and critically examined. The chapter also considers the question of whether evolutionary economics can ever displace equilibrium economics. Some current evolutionary economic model builders have serious doubts. The chapter concludes with some speculations about going beyond the consideration of the individual firm to include sociological aspects of consumer preference.",
        "link": "http://dx.doi.org/10.1093/acprof:oso/9780190274320.003.0012"
    },
    {
        "id": 23045,
        "title": "Neil Young—“Transformer Man” (1982)",
        "authors": "George Plasketes",
        "published": "2022-8-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003093206-10"
    },
    {
        "id": 23046,
        "title": "A Complementary Dual-Backbone Transformer Aggregating Weak Cues for Object Detection in Extremely Dark Videos",
        "authors": "Bo Zhang, Jinli Suo, Qionghai Dai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4343182"
    },
    {
        "id": 23047,
        "title": "Transformer and Influencer: Giovanni Battista Ramusio’s Impact on Western European Geography",
        "authors": "Margaret Small",
        "published": "2023-3-19",
        "citations": 0,
        "abstract": "\n\n\nIn the mid-sixteenth century, the study of cosmography was in a state of upheaval in Western Europe, for the European voyages of exploration had disrupted the old ideas of the nature and structure of the world. As a consequence, cosmographers and geographers struggled to accommodate the ever-expanding influx of new empirical knowledge into their works. In the 1550s the Venetian Giovanni Battista Ramusio compiled the Navigationi et viaggi, initiating a new form of geography which endeavoured to present a world cosmography through the eyes of travellers, ideally transmitting the knowledge gained from an age of exploration. In framing his work, Ramusio used both his knowledge of the classics and his humanist editorial skills, while his tests for inclusion derived from attested observation. Over seventy narratives, originally written in a variety of languages, were presented by Ramusio in vernacular Italian and skilfully woven together with intervening Discorsi, written by Ramusio by way of commentary. Ramusio’s forensic editorial skills, mastery in acquiring texts which had hitherto seen little or no printed circulation, diligence in translating, editing and presenting them in an accessible format made his work invaluable. Proposals to republish it in French or English, however, never came to fruition; therefore, scholars had to turn to the vernacular Italian for the information. The article examines how theNavigationi et viaggi became a bedrock of European geographical knowledge examining, in particular, its use by the English geographer John Dee and the French cosmographer royal, André Thevet. It shows how the travellers’ tales, mediated through the hands of a sedentary Venetian, crisscrossed Europe and became fundamental in creating a new geographical understanding dependent on the words of the eyewitness.\n\n\n",
        "link": "http://dx.doi.org/10.36253/jems-2279-7149-14383"
    },
    {
        "id": 23048,
        "title": "Multimodal Neural Machine Translation Using CNN and Transformer Encoder",
        "authors": "Hiroki Takushima, Akihiro Tamura, Takashi Ninomiya, Hideki Nakayama",
        "published": "2019-4-2",
        "citations": 1,
        "abstract": "Multimodal machine translation uses images related to source language sentences as inputs to improve translation quality. Previous multimodal Neural Machine Translation (NMT) models, which incorporate visual features of each image region into an encoder for source language sentences or an attention mechanism between an encoder and a decoder, cannot catch the relation between visual features from each image region. This paper proposes a new multimodal NMT model, which encodes an input image using a Convolutional Neural Network (CNN) and a Transformer encoder. In particular, the proposed image encoder first extracts visual features from each image region using a CNN, and then encodes an input image on the basis of the extracted visual features using a Transformer encoder, where the relation between visual features from each image region are captured by a self-attention mechanism of the Transformer encoder. The experiments on the English-German translation task using the Multi30k data set show that the proposed model achieves 0.96 BLEU points improvement against a baseline Transformer NMT model without image inputs and 0.47 BLEU points improvement against a baseline multimodal Transformer NMT model without a Transformer encoder for images.",
        "link": "http://dx.doi.org/10.29007/hxhn"
    },
    {
        "id": 23049,
        "title": "Study on the Additional Error Testing of the Electronic Voltage Transformer",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/icmit.2017.55"
    },
    {
        "id": 23050,
        "title": "Study on Transient Saturation Characteristic of Current Transformer",
        "authors": "Gao Qi, Huang Shaofeng",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ifeea51475.2020.00033"
    },
    {
        "id": 23051,
        "title": "Text Detection of Transformer Based on Deep Learning Algorithm",
        "authors": "",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17559/tv-20211027110610"
    },
    {
        "id": 23052,
        "title": "Transformer Health Monitoring System Using IOT",
        "authors": "IJSREM Journal",
        "published": "2022-6-16",
        "citations": 0,
        "abstract": "Electrical power plays a significant role in people's daily activities. Due to the advancement of technology IoT technique is used in most real-time applications. Various sensors are used to collect real data from the environment . With the help of the IoT concept, people can create a machine to machine connection. This proposed system is designed to monitor and detect the faults of the transformer and immediately sent the message to the authenticated person. The important indicators used to measure the condition of the transformer are temperature, oil level, and vibration. This system can reduce manpower and increases the stability, accuracy, and efficiency of the transformer. The sensor data transfer to the controller and  check the indicators limit values. If the indicator's value crosses the threshold values the message transfer to the concerned people. All the sensor values are sent to the android phone through IOT. This system will be an advanced step to the automation by diminishing human dependency. Thus Transformer health monitoring system offers a more improved transformer monitoring. KEY WORDS: Distributed Transformer, IOT, Transformer health monitoring, Sensors, Wi-Fi module, PC / Laptop",
        "link": "http://dx.doi.org/10.55041/ijsrem14683"
    },
    {
        "id": 23053,
        "title": "P2.2 - Differential Inductive Sensing System using a Differential Transformer with Shielded and Split Primary Coil",
        "authors": "M. Berger, A. Zygmanowski, S. Zimmermann",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5162/15dss2021/p2.2"
    },
    {
        "id": 23054,
        "title": "AEformer: Asymmetric Embedding Transformer for Stock Market Prediction based on Investor Sentiment",
        "authors": "Linling Jiang, Songtao Yue, Mingli Zhang, Fan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nStock market prediction is an essential topic in economics. However, owing to the noise and volatility of the stock market, timely market prediction is generally considered one of the most challenging problems. Several researchers have introduced investor sentiment into stock prediction models and have achieved good results. Applying investor sentiment to high-frequency stock price forecasts can lead to risk aversion and improved returns. We have designed a model for high-frequency stock price prediction known as asymmetric embedding transformer (AEformer) that uses investor sentiment. We filtered stock comments using category information and enhanced the utilization of investor sentiment for stock prediction by incorporating an asymmetric embedding layer combined with a channel-wise independent self-attention mechanism. The experimental results show that AEformer outperforms the other models in high-frequency stock predictions using investor sentiment. Moreover, the asymmetric embedding layer is effective in improving the forecasting performance of transformer-based models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3344960/v1"
    },
    {
        "id": 23055,
        "title": "Investigation of EHV Current Transformer Failure by Dielectric Frequency Response Technique",
        "authors": "Diego M. Robalino, Ismail Guner",
        "published": "2020-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic47619.2020.9158762"
    },
    {
        "id": 23056,
        "title": "Transformer less inverter for grid-connected photovoltaic systems using fuzzy logic controller",
        "authors": "Mr.P. Suresh, Mr.K. Gopi, Dr.Shaik Rafi Kiran",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22161/ijaers/nctet.2017.eee.51"
    },
    {
        "id": 23057,
        "title": "A review of transformer aging and control strategies",
        "authors": "Sri Nikhil Gupta Gourisetti, Harold Kirkham, Deepak Sivaraman",
        "published": "2017-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps.2017.8107234"
    },
    {
        "id": 23058,
        "title": "Pengukuran dan Analisis High Frequency Current Transformer Pendeteksi Partial Discharge",
        "authors": "Andi Junaidi",
        "published": "2021-4-13",
        "citations": 0,
        "abstract": "The lifetime of high voltage equipment is very dependent on the condition of insulation, poor insulation means the life span of high voltage equipment is getting shorter. Partial Discharge is one of the causes of deterioration of high voltage equipment insulation. The ability to know the phenomenon of partial discharge in equipment becomes very important and adds value to the maintenance of high voltage equipment. Many explanations regarding partial discharge, partial discharge is the lack of uniformity of the electric field in the isolation or dielectric media which will result in the phenomenon of Partial Discharge where this will lead to the failure of the isolation media. Partial Discharge that occurs continuously can cause damage (breakdown) on a high voltage equipment. Therefore, before a high voltage device is used, Partial Discharge detection is necessary. Partial Discharge Detection can use the Partial Discharge measurement circuit. In testing partial discharges can be tested using a sensor with various circuits including High Frequency Current Transformers (HFCT) so that this research will discuss one part of the Partial Discharge measurement circuit, the HFCT detector. Hopefully, the HFCT detector can be further developed and can be used for better partial discharge detection.",
        "link": "http://dx.doi.org/10.33322/kilat.v10i1.983"
    },
    {
        "id": 23059,
        "title": "Electromagnetic Phenomena in Metals with Constant Permeability",
        "authors": "Janusz Turowski, Marek Turowski",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b16373-6"
    },
    {
        "id": 23060,
        "title": "Perceptual Vibrotactile Signal Code Based on Transformer",
        "authors": "Yingzhao Zhu",
        "published": "2022-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imcec55388.2022.10019881"
    },
    {
        "id": 23061,
        "title": "c-Axiszig-zag polarization inverted ScAlN multilayer for FBAR transformer rectifying antenna",
        "authors": "Rei Karasawa, Takahiko Yanagitani",
        "published": "2017-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2017.8092551"
    },
    {
        "id": 23062,
        "title": "Chapitre 2. Satisfaction client : la métamorphose des données",
        "authors": "Christophe Benavent",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0033"
    },
    {
        "id": 23063,
        "title": "Transformer EM-FL-PSO Design Optimization",
        "authors": "Rakan A. Almazmomi, Michael A. Gutierrez-McCoy, A. A. Arkadan",
        "published": "2021-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/greentech48523.2021.00073"
    },
    {
        "id": 23064,
        "title": "Small Objects Detection Using Transformer-Cnn",
        "authors": "Chun-Liang Lin, Yan-Lin Chen, Yu-Chen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4273562"
    },
    {
        "id": 23065,
        "title": "Method for Calculating Increased Frequency High-Voltage Power Transformer",
        "authors": "Alexander Sokolov, Timofey Shadrikov",
        "published": "2021-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uralcon52005.2021.9559589"
    },
    {
        "id": 23066,
        "title": "Vision Transformer Modules for Compositional Reasoning in Visual Question Answering Tasks",
        "authors": "Joe McCooey, Hossein Malekmohamadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4074601"
    },
    {
        "id": 23067,
        "title": "Random Swin Transformer",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2022-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas55662.2022.10003789"
    },
    {
        "id": 23068,
        "title": "Robust and Efficient Modulation Recognition with Pyramid Signal Transformer",
        "authors": "He Su, Xinyi Fan, Huajun Liu",
        "published": "2022-12-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001593"
    },
    {
        "id": 23069,
        "title": "Smart Transformer-based Network Reconfiguration for Improved Resilience",
        "authors": "Hrishikesan V M, Clara Najera-Aleson, Marius Langwasser, Marco Liserre",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202806"
    },
    {
        "id": 23070,
        "title": "Efficient Transformer-Based Compressed Video Modeling via Informative Patch Selection",
        "authors": "Tomoyuki Suzuki, Yoshimitsu Aoki",
        "published": "2022-12-26",
        "citations": 0,
        "abstract": "Recently, Transformer-based video recognition models have achieved state-of-the-art results on major video recognition benchmarks. However, their high inference cost significantly limits research speed and practical use. In video compression, methods considering small motions and residuals that are less informative and assigning short code lengths to them (e.g., MPEG4) have successfully reduced the redundancy of videos. Inspired by this idea, we propose Informative Patch Selection (IPS), which efficiently reduces the inference cost by excluding redundant patches from the input of the Transformer-based video model. The redundancy of each patch is calculated from motions and residuals obtained while decoding a compressed video. The proposed method is simple and effective in that it can dynamically reduce the inference cost depending on the input without any policy model or additional loss term. Extensive experiments on action recognition demonstrated that our method could significantly improve the trade-off between the accuracy and inference cost of the Transformer-based video model. Although the method does not require any policy model or additional loss term, its performance approaches that of existing methods that do require them.",
        "link": "http://dx.doi.org/10.3390/s23010244"
    },
    {
        "id": 23071,
        "title": "Prior Knowledge Guided Three-Branch Transformer for Hoi Detection",
        "authors": "Dongji Chen, Huicheng Lai, Guxue Gao, Jun Ma, Junkai Li, Hutuo Quan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4608308"
    },
    {
        "id": 23072,
        "title": "Trial Operation for Transformers and Reactors",
        "authors": "John Lapworth",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-80469-5_19"
    },
    {
        "id": 23073,
        "title": "CSPFormer: A cross-spatial pyramid transformer for visual place recognition",
        "authors": "Zhenyu Li, Pengjie Xu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127472"
    },
    {
        "id": 23074,
        "title": "Real-time traffic sign detection network based on Swin Transformer",
        "authors": "Wei Zhu, Yue Ying, Yayu zheng, Yikai Chen, Shucheng Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the field of autonomous driving, the detection of traffic signs remains a significant challenge, especially when it comes to the real-time detection of medium and small targets. The difficulty of detecting small objects decreases accuracy. To address these challenges, we propose a real-time traffic sign detection algorithm based on the Swin Transformer (RTSDST) that improves computation performance and accuracy for multi-scale target detection on SoCs installed onboard autonomous driving vehicles. Our approach includes a head specifically designed for detecting tiny objects, followed by the adoption of Swin Transformer blocks to effectively capture the spatial and channel dependencies of the feature maps, which improves the accuracy of detecting targets of varying sizes. To efficiently identify regions of interest in large coverage images, we employ a Residual Convolutional Attention Module to generate sequential feature maps between the channel and spatial dimensions and weigh them against the original map. A realistic traffic sign detection dataset, Tsinghua-Tencent 100K (TT100K), which includes medium and small traffic sign targets, was adopted in this article to evaluate the effectiveness of our proposed RTSDST. The evaluation results show that RTSDST has excellent performance on multi-scale scenes. Additionally, we also evaluated our network on the VisDrone dataset for small target detection. Our method has state-of-art performance on small targets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3299732/v1"
    },
    {
        "id": 23075,
        "title": "Adaptive Algorithm for Distribution Transformer Protection to Improve Smart Grid Stability",
        "authors": "D. D. Patel, Nilesh Chothani, K. D. Mistry, Dhaval Tailor",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00560"
    },
    {
        "id": 23076,
        "title": "MOT: a Multi-Omics Transformer for multiclass classification tumour types predictions",
        "authors": "Mazid Abiodoun Osseni, Prudencio Tossou, Francois Laviolette, Jacques Corbeil",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMotivation: Breakthroughs in high-throughput technologies and machine learning methods have enabled the shift towards multi-omics modeling as the preferred mean to understand the mechanisms underlying biological processes, and to improve complex disease prognosis in clinical settings. However, most multi-omic studies only use transcriptomics and epigenomics due to their over-representation in databases and their early technical maturity compared to others omics. For complex phenotypes and mechanisms, not leveraging all the omics despite their varying degree of availability can lead to a failure to understand the underlying biological mechanisms. \nResults: We proposed MOT (Multi-Omic Transformer), a deep learning based model using the transformer architecture, that discriminates complex phenotypes (herein cancers types) based on five omics data type regardless of their availability: transcriptomics (mRNA and miRNA), epigenomics (DNA methylation), copy number variations (CNVs), and proteomics. At its core, MOT uses a data augmentation scheme that allows it to handle missing omics views and its attention layers give a macro level of interpretability for each phenotypes. Indeed, MOT identifies the required omic type for the best prediction for each phenotype and therefore could guide clinical decision making when acquiring data to confirm a diagnostic. It achieves an accuracy score of 96.04% after 5-fold cross-validation among 33 tumour types. The newly introduced model can integrate and analyse five different omics data while handling the missing omics views and can also identify the essential omics data for the tumour multiclass classification tasks. \nAvailability and implementation: MOT source code is available at https://github.com/dizam92/multiomic_predictions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1348696/v2"
    },
    {
        "id": 23077,
        "title": "A Financial Market Trading Strategy: Improved Deep Reinforcement Learning System Via Multi-Transformer",
        "authors": "ZR Wang, Li Ping, Wu Zhao, GJY Nian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4516320"
    },
    {
        "id": 23078,
        "title": "Electronic Circuit for Reduction of Power-On Transient Current in Transformer",
        "authors": "Jiri Hammerbauer, Milan Stork",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ae58099.2023.10274395"
    },
    {
        "id": 23079,
        "title": "DDETR-SLAM: A Transformer-Based Approach to Pose Optimization in Dynamic Environments",
        "authors": "Feng Li, Yuanyuan Liu, Guodong Wang, Chunguang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSimultaneous Localization and Mapping (SLAM) is a critical technology for accurate robot localization and path planning. It has been an important area of research to improve localization accuracy. In this paper, we propose a Transformer-based visual semantic SLAM algorithm (DDETR-SLAM) to address the shortcomings of traditional visual SLAM frameworks, such as large localization errors in dynamic scenes and “ghosting” in 3D mapping. First, by incorporating the Deformable DETR (DEtection TRansformer) network as an object detection thread, the pose estimation accuracy of the system is improved compared to ORB-SLAM2. Furthermore, a dynamic feature point culling algorithm that combines the semantic information is designed to eliminate outlier points generated by dynamic objects, thereby improving the accuracy and robustness of SLAM localization and mapping. Experiments are conducted on the public TUM datasets to verify the localization accuracy, computational efficiency, and readability of the point cloud map of DDETR-SLAM. The results show that in highly dynamic environments, the ATE (Absolute Trajectory Error), translation error, and rotation error are reduced by 98.45%, 95.34%, and 92.67%, respectively, when compared to ORB-SLAM2. In most cases, our proposed system outperforms DS-SLAM, DynaSLAM, Detect-SLAM, RGB-D SLAM, and YOLOv5 + ORB-SLAM2, and our methodology improves location accuracy. The dense mapping also has better readability. The RPE (Relative Trajectory Error) is only 0.0076 m, and the ATE is only 0.0063 m.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2965479/v1"
    },
    {
        "id": 23080,
        "title": "A Transformer-based Patient Clustering Method in Continuing Care Retirement Communities",
        "authors": "Zhenggui Xiang",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385837"
    },
    {
        "id": 23081,
        "title": "Assessing the Performance of Chat Generative Pretrained Transformer (ChatGPT) in Answering Andrology-Related Questions",
        "authors": "",
        "published": "2023-11-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5152/tud.2023.23171"
    },
    {
        "id": 23082,
        "title": "Cooperation of the Plasma Reactor with a Converter Power Supply Equipped with a Transformer of Special Design",
        "authors": "Grzegorz Komarzyniec, Michał Aftyka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Plasma generation by means of electrical discharge requires specialized power supply systems. The applicability of plasma for various plasma processes depends on its parameters, and these in turn depend on the parameters of the power supply systems. Arc plasma can be unstable, generating a lot of electromagnetic interference and overvoltage and overcurrent. The power system of a plasma reactor must guarantee good plasma control characteristics, be immune to disturbances and ensure good cooperation with the power grid. The article analyzes the cooperation of a three-phase plasma reactor, with gliding arc discharge, with a power supply system of a new type. This system integrates an AC/DC/AC converter with a five-column transformer of special design in a single device. Using the properties of magnetic circuits, it was possible to integrate the functions of ignition and sustaining the burning of the discharge in the reactor in a single transformer. Proper design of the transformer is crucial to achieve good cooperation of the AC/DC/AC converter with both the plasma reactor and the power supply network. The presented power supply design shows a number of positive features predisposing it to powering arc plasma reactors.",
        "link": "http://dx.doi.org/10.20944/preprints202309.0129.v1"
    },
    {
        "id": 23083,
        "title": "A Swin-Transformer Based Multi-Scale Context Fusion Network for Coal Gangue Segmentation",
        "authors": "Zhenlin Wang, Zhixin Zhang, Shanglong Xu, Zhufei Leng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4591254"
    },
    {
        "id": 23084,
        "title": "TopoFormer: Multiscale Topology-enabled Structure-to-Sequence Transformer for Protein-Ligand Interaction Predictions",
        "authors": "Guo-Wei Wei, Dong Chen, Jian Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPre-trained deep Transformers have had tremendous success in a wide variety of disciplines. However, in computational biology, essentially all Transformers  are built upon the biological sequences, which ignores vital stereochemical information  and may result in crucial errors in downstream predictions. On the other hand, three-dimensional (3D) molecular structures are incompatible with the sequential architecture of Transformer and natural language processing (NLP) models in general. This work addresses this foundational challenge by a topological Transformer (TopoFormer). TopoFormer is built by integrating NLP and a multiscale topology techniques, the persistent topological hyperdigraph Laplacian (PTHL), which systematically converts intricate 3D protein-ligand complexes at various spatial scales into a NLP-admissible sequence of topological invariants and homotopic shapes. Element-specific PTHLs are further developed to embed crucial physical, chemical, and biological interactions into topological sequences. TopoFormer surges ahead of conventional algorithms and recent deep learning variants and gives rise to exemplary scoring accuracy and superior performance in ranking, docking, and screening tasks in a number of benchmark datasets. The proposed topological sequences can be extracted from all kinds of structural data in data science to facilitate various NLP models,  heralding a new era in AI-driven discovery.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3640878/v1"
    },
    {
        "id": 23085,
        "title": "Meta-Model Based Scaling Laws of a Two-Winding Transformer",
        "authors": "Ahmed Tahir",
        "published": "2017-3-1",
        "citations": 0,
        "abstract": "Recently, there has been growing interest in system-based global optimization techniques for the design of small systems such as micro-grids. To reduce the search space of the global optimization technique, a meta-model based scaling law was introduced. In this paper, a scaling technique was derived in which a transformer mass and power loss were expressed in terms of rated power, specified current density, and frequency. Curve-fitting techniques were used to derive a meta-model for the scaled mass and power loss. To achieve more generality, the meta-model was also defined as a function of frequency.",
        "link": "http://dx.doi.org/10.37376/lyjer.v1i1.283"
    },
    {
        "id": 23086,
        "title": "Influence of the converter transformer valve-side bushing fault on commutation reactance protection and improvement scheme",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2023.146046"
    },
    {
        "id": 23087,
        "title": "Smart Transformer Based Ship Microgrid System",
        "authors": "Dwijasish Das, Chandan Kumar",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec57805.2023.10384947"
    },
    {
        "id": 23088,
        "title": "Postural Sway Classification using Modified Vision Transformer",
        "authors": "Ebrahim A. Nehary, Sreeraman Rajan, Bruno Ando",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/biocas58349.2023.10389098"
    },
    {
        "id": 23089,
        "title": "Compatibility of mineral insulating oil with transformer construction materials",
        "authors": "Natasa Bernard, Biljana Cucek",
        "published": "2017-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124727"
    },
    {
        "id": 23090,
        "title": "VTSeg: Video Transformer for Semantic Segmentation",
        "authors": "Vasile Lup, Ion Giosan",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccp60212.2023.10398651"
    },
    {
        "id": 23091,
        "title": "Residual Swin Transformer Channel Attention Network for Image Demosaicing",
        "authors": "Wenzhu Xing, Karen Egiazarian",
        "published": "2022-9-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/euvip53989.2022.9922679"
    },
    {
        "id": 23092,
        "title": "Privacy-Preserving Video Understanding via Transformer-based Federated Learning",
        "authors": "Keval Doshi, Yasin Yilmaz",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsc61021.2023.10354099"
    },
    {
        "id": 23093,
        "title": "Segmenter: Transformer for Semantic Segmentation",
        "authors": "Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid",
        "published": "2021-10",
        "citations": 666,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00717"
    },
    {
        "id": 23094,
        "title": "Author response for \"Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure\"",
        "authors": " Zhuo Chen,  Rigui Zhou,  Pengju Ren",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3ra07708j/v2/response1"
    },
    {
        "id": 23095,
        "title": "Analysis of Transformer No-Load Loss Using Finite Element Method Validated by Experimental Tests",
        "authors": "Behnam Hashemi, Ali Asghar Taheri, Fatemeh Jozi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760505"
    },
    {
        "id": 23096,
        "title": "Smart Transformer Health Monitoring System",
        "authors": "Ali Ahmed",
        "published": "2023-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3568294.3580196"
    },
    {
        "id": 23097,
        "title": "Transformer Encoder Based Self-Supervised Learning for Hvac Fault Detection with Unlabeled Data",
        "authors": "Mohammad Abdollah Fadel Abdollah, Rossano Scoccia, Marcello Aprile",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4715186"
    },
    {
        "id": 23098,
        "title": "Voice Activity Detection Optimized by Adaptive Attention Span Transformer",
        "authors": "Wenpeng Mu, Bingshan Liu",
        "published": "2023",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3262518"
    },
    {
        "id": 23099,
        "title": "Image Fusion Transformer",
        "authors": "Vibashan Vs, Jeya Maria Jose Valanarasu, Poojan Oza, Vishal M. Patel",
        "published": "2022-10-16",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897280"
    },
    {
        "id": 23100,
        "title": "Improving Structural MRI Preprocessing with Hybrid Transformer GANs",
        "authors": "Ovidijus Grigas, Rytis Maskeliūnas, Robertas Damaševičius",
        "published": "2023-9-11",
        "citations": 3,
        "abstract": "Magnetic resonance imaging (MRI) is a technique that is widely used in practice to evaluate any pathologies in the human body. One of the areas of interest is the human brain. Naturally, MR images are low-resolution and contain noise due to signal interference, the patient’s body’s radio-frequency emissions and smaller Tesla coil counts in the machinery. There is a need to solve this problem, as MR tomographs that have the capability of capturing high-resolution images are extremely expensive and the length of the procedure to capture such images increases by the order of magnitude. Vision transformers have lately shown state-of-the-art results in super-resolution tasks; therefore, we decided to evaluate whether we can employ them for structural MRI super-resolution tasks. A literature review showed that similar methods do not focus on perceptual image quality because upscaled images are often blurry and are subjectively of poor quality. Knowing this, we propose a methodology called HR-MRI-GAN, which is a hybrid transformer generative adversarial network capable of increasing resolution and removing noise from 2D T1w MRI slice images. Experiments show that our method quantitatively outperforms other SOTA methods in terms of perceptual image quality and is capable of subjectively generalizing to unseen data. During the experiments, we additionally identified that the visual saliency-induced index metric is not applicable to MRI perceptual quality assessment and that general-purpose denoising networks are effective when removing noise from MR images.",
        "link": "http://dx.doi.org/10.3390/life13091893"
    },
    {
        "id": 23101,
        "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
        "authors": "Guan-Lin Chao, Ian Lane",
        "published": "2019-9-15",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1355"
    },
    {
        "id": 23102,
        "title": "WITHDRAWN: A Transformer based approach using LSTM and Paraphrase reference to Translate English Text into Hindi",
        "authors": "Surbhi Sharma, Nisheeth Joshi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn, as it was submitted in error. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230686/v2"
    },
    {
        "id": 23103,
        "title": "Multi-level Temporal Information Sharing Transformer-Based Feature Reuse Network for Cardiac MRI Reconstruction",
        "authors": "Guangming Wang, Jun Lyu, Fanwen Wang, Chengyan Wang, Jing Qin",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-52448-6_39"
    },
    {
        "id": 23104,
        "title": "Models of Models of Models of Models of Things",
        "authors": "",
        "published": "2017-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/11180.003.0005"
    },
    {
        "id": 23105,
        "title": "Models mixed and models new",
        "authors": "Ken Richardson",
        "published": "2019-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429297571-5"
    },
    {
        "id": 23106,
        "title": "METHOD OF CONTROL OF TECHNICAL CONDITION OF INSULATION OF ELECTRICAL EQUIPMENT OF THE GENERATOR-TRANSFORMER UNIT",
        "authors": "Vasil KUTIN, Marina KUTINA, Oleksandr SHPACHUK,  ,  ",
        "published": "2022-5-26",
        "citations": 0,
        "abstract": "Significant wear of the main electrical equipment of the power supply scheme (synchronous generators, complete shielded power lines, transformers) of thermal and nuclear power plants, implementation of the policy of extending the service life of existing power units, and gradual implementation of equipment repair strategies according to technical condition determines the relevance of development of new and improvement of existing systems for monitoring the technical condition of electrical equipment in general and its insulation characteristics in particular. Insulation diagnostics, relay protection devices and information-measuring systems currently operated on power units do not allow tracking changes in electrical insulation parameters of electrical equipment such as insulation resistance and capacitance relative to ground and tangent angle of dielectric loss detection that makes detection of defects in isolation at an early stage of their development impossible. Relay protection devices can be insensitive to single-phase earth faults of the stator winding near the neutral and with a symmetrical decrease in the insulation parameters of the stator winding, as well as malfunction when starting units due to voltage asymmetry in generator phases. The method of control of technical condition of electrical equipment insulation in the generator-transformer unit circuit is proposed, which is based on combined superimposition of direct current and alternating current with frequency less than industrial, control of discharge of precharged capacitor and control of alternating current flow with Rogovsky coils. The proposed method will detect both abrupt and gradual changes in the technical condition of the insulation of electrical equipment of the “generator-transformer” unit, improve the quality of planning of repair work and speed up the search for faulty elements in case of damage.",
        "link": "http://dx.doi.org/10.31891/2307-5732-2022-309-3-138-142"
    },
    {
        "id": 23107,
        "title": "Investigation on Common-Mode Voltage Suppression in Smart Transformer-Fed Distributed Hybrid Grids",
        "authors": "Rongwu Zhu, Giampaolo Buticchi, Marco Liserre",
        "published": "2018-10",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2017.2779803"
    },
    {
        "id": 23108,
        "title": "Impact of multilevel inverter supply on transformer losses",
        "authors": "Abhay P. Saraf, S. S. Dhamse",
        "published": "2017-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccmc.2017.8282584"
    },
    {
        "id": 23109,
        "title": "Pancreatitis Classification: Vision Transformer and Interpreters for Diagnostic Support in CT Imaging",
        "authors": "Linwanghao Fang, Jin Peng, Chi Zhang, Wei Zhang, Hua Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4555925"
    },
    {
        "id": 23110,
        "title": "Modelling and Simulation of Transformer Tests under Dynamic Testing",
        "authors": "Ramesh Kumar Patel, Madhu Upadhyay",
        "published": "2021-10-25",
        "citations": 0,
        "abstract": "Large power transformers are the most important equipment for the power grid. Their reliability not only affects the availability of electricity in the supply area, but also affects the economic functioning of an energy supplier. The main objective of this work of the development of the transformer testing simulation model on the MATLAB/SIMULINK environment. The designing aspect shall comprise the generator-based and grid-based testing of the transformer and associated results on the same in the SIMULINK. And the methodology shall study the impacts on the grid-connected devices due to testing of the transformers in the grid-based method. The study concluded that there is effectively no effect on the parameter evaluation of OC and SC tests by either source. However, the practical conditions would require grid-based testing that allows a greater range of transformers of various ratings to be tested with high current capability. The impact of fault on the grid can be easily evaluated through location of fault, duration of fault, and dip if occurred. The testing should follow proper timing to avoid such disturbances on the grid. This leads to the rescheduling of the other approved outages or Short Circuit tests.",
        "link": "http://dx.doi.org/10.24113/ijoscience.v7i9.412"
    },
    {
        "id": 23111,
        "title": "Transformer Oil Quality Assessment Using Machine Learning Techniques",
        "authors": "M. Sinduja, R.V. Maheswari, B. Vigneshwaran",
        "published": "2022-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci54379.2022.9741071"
    },
    {
        "id": 23112,
        "title": "Analysis of Core Losses in Transformer Working at Static Var Compensator",
        "authors": "Piotr Osinski, Pawel Witczak",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "This article presents the comparison of 3D and 2D finite element models of a power transformer designed for reactive power compensation stations. There is a lack of studies in the literature on internal electromagnetic phenomena in the active part of a transformer operated in these conditions. The results of numerical 2D and 3D calculations of no-load current and losses in the transformer core were obtained by using various methods and models. The impact of considering the hysteresis loop phenomenon on the calculation of core losses was investigated by using the Jiles–Atherton core losses model. The results obtained in the paper show that the model of the core must contain the areas representing the influence of overlappings on the no-load current and also on the flux density field in the core. The capacitive load of the transformer increases the flux density in the core limbs by several percent, so the power losses there must also increase accordingly. As a summary of the research, differences in the values of losses in each core element between the capacitive load and no-load conditions are presented. The results presented in this paper indicate that considering nonlinearity related to the magnetic hysteresis loop has a significant impact on the calculation of the core losses of power transformers.",
        "link": "http://dx.doi.org/10.3390/en16124584"
    },
    {
        "id": 23113,
        "title": "A Multimodal Vision Transformer for Interpretable Fusion of Functional and Structural Neuroimaging Data",
        "authors": "Yuda Bi, Anees Abrol, Zening Fu, Vince D. Calhoun",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractDeep learning models, despite their potential for increasing our understanding of intricate neuroimaging data, can be hampered by challenges related to interpretability. Multimodal neuroimaging appears to be a promising approach that allows us to extract supplementary information from various imaging modalities. It’s noteworthy that functional brain changes are often more pronounced in schizophrenia, albeit potentially less reproducible, while structural MRI effects are more replicable but usually manifest smaller effects. Instead of conducting isolated analyses for each modality, the joint analysis of these data can bolster the effects and further refine our neurobiological understanding of schizophrenia. This paper introduces a novel deep learning model, the multimodal vision transformer (MultiViT), specifically engineered to enhance the accuracy of classifying schizophrenia by using structural MRI (sMRI) and functional MRI (fMRI) data independently and simultaneously leveraging the combined information from both modalities. This study uses functional network connectivity data derived from a fully automated independent component analysis method as the fMRI features and segmented gray matter volume (GMV) as the sMRI features. These offer sensitive, high-dimensional features for learning from structural and functional MRI data. The resulting MultiViT model is lightweight and robust, outperforming unimodal analyses. Our approach has been applied to data collected from control subjects and patients with schizophrenia, with the MultiViT model achieving an AUC of 0.833, which is significantly higher than the average 0.766 AUC for unimodal baselines and 0.78 AUC for multimodal baselines. Advanced algorithmic approaches for predicting and characterizing these disorders have consistently evolved, though subject and diagnostic heterogeneity pose significant challenges. Given that each modality provides only a partial representation of the brain, we can gather more comprehensive information by harnessing both modalities than by relying on either one independently. Furthermore, we conducted a saliency analysis to gain insights into the co-alterations in structural gray matter and functional network connectivity disrupted in schizophrenia. While it’s clear that the MultiViT model demonstrates differences compared to previous multimodal methods, the specifics of how it compares to methods such as MCCA and JICA are still under investigation, and more research is needed in this area. The findings underscore the potential of interpretable multimodal data fusion models like the MultiViT, highlighting their robustness and potential in the classification and understanding of schizophrenia.",
        "link": "http://dx.doi.org/10.1101/2023.07.14.549002"
    },
    {
        "id": 23114,
        "title": "Transformer and Tapchanger",
        "authors": "T. V. Sridhar",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-3955-8_1"
    },
    {
        "id": 23115,
        "title": "Reliable Estimation of Dissipation Factor of In-service Power Transformer",
        "authors": "Debmalya Pramanik, Arijit Baral",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon55714.2022.9972517"
    },
    {
        "id": 23116,
        "title": "Transformer Design",
        "authors": "Warsame Hassan Ali, Samir Ibrahim Abood, Matthew N. O. Sadiku",
        "published": "2019-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429290619-5"
    },
    {
        "id": 23117,
        "title": "TR-Former: Token Based Residual Transformer For Single Image Denoising",
        "authors": "Yihan Li",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135292"
    },
    {
        "id": 23118,
        "title": "Smart Transformer-based Hybrid LVAC and LVDC Interconnected Microgrid",
        "authors": "Dwijasish Das, Hrishikesan V.M., Chandan Kumar",
        "published": "2018-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec.2018.8635923"
    },
    {
        "id": 23119,
        "title": "Research on transformer and attention in applied algorithms",
        "authors": "Junyan Zhang",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The transformer is an encoder-decoder-based structure and model for deep learning that completely utilizes the self-attention mechanism. It has gained remarkable success in natural language processing and computer vision and is becoming the predominant research direction. This study first analyzes the transformer and attention mechanism, summarizes their advantages, and explores how they help the recommendation algorithm dynamically focus on specific parts of the input that are helpful to perform the current recommendation task. After analyzing the framework of the attention mechanism network and its weight computation for data received. To further enhance the practicality of objects in natural situations and the precision of object recognition, a transformer detection approach based on deformable convolution is presented. And analyzed how the transformer works in the generative pre-trained transformer. These algorithms illustrate the efficacy and robustness of the transformer, indicating that the transformer that incorporates the attention mechanism may satisfy the requirements of the majority of deep learning tasks. However, the unpredictability of demands, the exponential growth of information, and other issues will continue to make it challenging to deal with global interaction mechanisms and a unified framework for multimodal data.",
        "link": "http://dx.doi.org/10.54254/2755-2721/13/20230737"
    },
    {
        "id": 23120,
        "title": "TUnA: An uncertainty aware transformer model for sequence-based protein-protein interaction prediction",
        "authors": "Young Su Ko, Jonathan Parkinson, Cong Liu, Wei Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProtein-protein interactions (PPIs) are important for many biological processes, but predicting them from sequence data remains challenging. Existing deep learning models often cannot generalize to proteins not present in the training set, and do not provide uncertainty estimates for their predictions. To address these limitations, we present TUnA, a Transformer-based uncertainty aware model for PPI prediction. TUnA uses ESM-2 embeddings with Transformer encoders and incorporates a Spectral-normalized Neural Gaussian Process. TUnA achieves state-of-the-art performance and, importantly, evaluates uncertainty for unseen sequences. We demonstrate that TUnA’s uncertainty estimates can effectively identify the most reliable predictions, significantly reducing false positives. This capability is crucial in bridging the gap between computational predictions and experimental validation.",
        "link": "http://dx.doi.org/10.1101/2024.02.19.581072"
    },
    {
        "id": 23121,
        "title": "Lightweight Multimodal Cycle-Attention Transformer Towards Cancer Diagnosis",
        "authors": "Shicong Liu, Xin Ma, Shenyang Deng, Yuanchi Suo, Jianjun Zhang, Wing  W. Y. Ng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4725371"
    },
    {
        "id": 23122,
        "title": "Feasibility of Amorphous Symmetric Core Transformer Under Distribution Network Planning",
        "authors": "Mehran Hajiaghapour-Moghimi, Ahmad Moradnouri, Mehdi Vakilian",
        "published": "2018-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epdc.2018.8536288"
    },
    {
        "id": 23123,
        "title": "Impact of Demand Response on transformer loss of life",
        "authors": "Nonika Loitongbam, T Ghose",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece48148.2020.9223103"
    },
    {
        "id": 23124,
        "title": "Chapitre 5. L’insatisfaction client face aux pannes des produits",
        "authors": "Dominique Roux",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0093"
    },
    {
        "id": 23125,
        "title": "Shear Mode Polarity Inverted ScAIN Multilayer for Application to Transformer in Rectifying Antenna",
        "authors": "Rei Karasawa, Takahiko Yanagitani",
        "published": "2018-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2018.8580083"
    },
    {
        "id": 23126,
        "title": "Current Transformer Accuracy Improvement by Digital Compensation Technique",
        "authors": "Makarand Sudhakar Ballal, Manish Ganesh Wath, Banavath Venkatesh",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc.2018.8771706"
    },
    {
        "id": 23127,
        "title": "Fault Diagnosis of Transformer for Auxiliary Power Supply of Photovoltaic Micro Inverter",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/icsemc.2017.39"
    },
    {
        "id": 23128,
        "title": "Analyzing Short Circuit Forces in Transformer for Double Layer Helical LV Winding using FEM",
        "authors": "Deepika Bhalla",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23940/ijpe.18.03.p3.425433"
    },
    {
        "id": 23129,
        "title": "Solid State Transformer (SST) interfaced Doubly Fed Induction Generator (DFIG) wind turbine",
        "authors": "Nasroddin Parseh, Mohammad Mohammadi",
        "published": "2017-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iraniancee.2017.7985202"
    },
    {
        "id": 23130,
        "title": "Havt: Hierarchical Attention Vision Transformer for Fine-Grained Visual Classification",
        "authors": "Xiaobin Hu, Shining Zhu, Taile Peng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4132949"
    },
    {
        "id": 23131,
        "title": "Extraction of Partial Discharge Signal in Predominantly VHF Frequency Range in the Presence of Strong Noise in Power Transformer",
        "authors": "Djordje Dukanac",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper provides a detailed analysis of the online detected partial discharge (PD) in the predominantly VHF range in the power transformer during normal operation in the thermal power plant. A standard UHF drain valve sensor is used with the ability to also capture VHF frequencies of received signals. If PD detection is performed during off-line testing, there are excessive costs proportional to the time during which the power transformers are disconnected from the network. For this economic reason, online PD detection techniques are more convenient. The UHF technique has a higher signal-to-noise ratio compared to IEC 60270 and the acoustic method. To accurately determine the strength and waveform of the PD signal, especially if the source position is far from the UHF sensor or if the signal is weak, it is necessary to properly separate the useful part of the recorded signal from the background noise. The criterion for this is that there are no time shifts of the first peaks of the most prominent PD. For that reason, the beginning and the approximate end of PD signal has to be determined. The results show some obvious similarities of PDs in the recorded signals, such as frequency range, duration, repetition rate and the same dominant frequency, which sufficiently indicates that it is the same type of PD.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1980740/v1"
    },
    {
        "id": 23132,
        "title": "A Novel Multi Stage Transformer for Compensating Unbalanced Loads",
        "authors": "T. Ahmadi",
        "published": "2020-7-15",
        "citations": 2,
        "abstract": "In the proposed Load balancing transformer, two additional coupling winding are placed in the secondary of each phase. One of these coupling windings is in series with the winding of other phase and this combination of windings is reversely paralleled with the secondary winding of the third phase. Under unbalanced load conditions, the unbalanced part of the load current, flows through the proposed combination of windings. In this way, the unbalanced part of the load current is distributed between all phases. The proposed topology for multi stage load balancing transformer is based on the ordinary load-balancing transformer. Finally, these topologies are simulated with MATLAB and the results are discussed.",
        "link": "http://dx.doi.org/10.37394/23203.2020.15.29"
    },
    {
        "id": 23133,
        "title": "Research on vibration law of converter transformer oil tank surface",
        "authors": "N. Liang, J. Sun",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.0282"
    },
    {
        "id": 23134,
        "title": "TransRSS: Transformer-based Radar Semantic Segmentation",
        "authors": "Hao Zou, Zhen Xie, Jiarong Ou, Yutao Gao",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161200"
    },
    {
        "id": 23135,
        "title": "Signature Verification by Multi-Size Assembled-Attention with the Backbone of Swin-Transformer",
        "authors": "Jian Chu, Wenkang Zhang, Yufan Zheng, Rafiq Ahmad",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nHandwritten signature verification is an indispensable means of identification in biometric information recognition and has broad application prospects and research significance in financial, judicial, and educational systems. With the advancement of signature forgery technology, people have higher requirements for the accuracy and efficiency of signature verification, and sophisticated convolutional networks capable of automatic feature extraction are gradually being applied to the field of handwriting recognition. However, these convolution methods still have the potential for improvement in recognition capability, generalization capability, and accuracy rate. This paper proposes a novel network model, Multi-Size Assembled-Attention Swin-Transformer network, to perform signature handwriting authenticity identification. The inputs to the network are signature images that are resized to multiple sizes, including (224, 224), (112, 112), (56, 56).  Then, features within the same image are extracted using the self-attention mechanism in Swin-Transformer, and features between different images are also extracted with the cross-attention mechanism in Assembled-Attention Block, enabling signature feature information to interact within the same image and between different images. Also,  Regularized Dropout strategy and adversarial method are implemented in the training stage. Therefore, our method considerably prompts the identification ability of the signature handwriting and obtains state-of-the-art performance, especially 57.1% and 50.4% improvement, in the situation of training in CEDAR and evaluation in Bengali and Hindi. Meantime, we evaluated the impact of the input images passing through the model's times on performance and found that the network achieves the optimal performance at times of four.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2533077/v1"
    },
    {
        "id": 23136,
        "title": "Semi-Supervised COVID-19 CT Segmentation Via Contrastive Learning Guided by CNN and Transformer",
        "authors": "Zhiyong Xiao, Hao Sun, Zhaohong Deng, Weidong Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4396473"
    },
    {
        "id": 23137,
        "title": "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR",
        "authors": "Takashi Maekaku, Yuya Fujita, Yifan Peng, Shinji Watanabe",
        "published": "2022-9-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11441"
    },
    {
        "id": 23138,
        "title": "Identifying Transformer Oil Criticality Using Fuzzy Logic Approach",
        "authors": "Leena Gautam, Rajesh Kumar, Y.R. Sood",
        "published": "2020-7-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sces50439.2020.9236724"
    },
    {
        "id": 23139,
        "title": "TS2Anet: Ship detection network based on transformer",
        "authors": "Dingye Liu",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.seares.2023.102415"
    },
    {
        "id": 23140,
        "title": "Switch-Transformer Sentiment Analysis Model for Arabic Dialects that Utilizes Mixture of Experts Mechanism",
        "authors": "Laith H. Baniata, Sangwoo Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent times, models like the Transformer have showcased remarkable prowess in tasks related to natural language processing. However, these models tend to be excessively intricate and demand extensive training. Additionally, while the multi-head self-attention mechanism in the Transformer model aims to capture semantic connections between words in a sequence, it encounters limitations when handling short sequences, thereby limiting its effectiveness in 5-polarity Arabic sentiment analysis tasks. The switch-transformer model has recently emerged as a high-performing alternative. Nevertheless, when these models are trained using single-task learning, they often fall short of achieving exceptional performance and struggle to generate robust latent feature representations, especially when working with compact datasets. This challenge is particularly pronounced in the case of the Arabic dialect, which is considered a low-resource language. Given these constraints, this research introduces a novel approach to sentiment analysis in Arabic text. This method leverages multitask learning in tandem with the switch-transformer shared encoder to enhance model adaptability and refine sentence representation. By introducing a mixture of expert (MoE) mechanism that break down the problem into smaller, more manageable sub-problems, the model becomes adept at handling lengthy sequences and intricate input-output relationships, benefiting both five-point and three-polarity Arabic sentiment analysis tasks. This proposed model effectively discerns sentiment in Arabic dialect sentences. The empirical results highlight the outstanding performance of the suggested model, as evidenced in evaluations on the Hotel Arabic-Reviews Dataset, the Book Reviews Arabic Dataset, and the LARB dataset.",
        "link": "http://dx.doi.org/10.20944/preprints202311.0187.v1"
    },
    {
        "id": 23141,
        "title": "A Light Transformer For Speech-To-Intent Applications",
        "authors": "Pu Wang, Hugo Van hamme",
        "published": "2021-1-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383559"
    },
    {
        "id": 23142,
        "title": "Transformer zero-sample fault diagnosis based on multimodal data fusion",
        "authors": "Qianchen Huang, Xiangsen Wei, Ziyi Wang, Huiya Wang",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10054816"
    },
    {
        "id": 23143,
        "title": "MV Propulsion Drive using Solid State Transformer (SST) Technology",
        "authors": "Himanshu Patel, Shekhar Bhawal, Kamalesh Hatua",
        "published": "2023-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpec56611.2023.10078474"
    },
    {
        "id": 23144,
        "title": "Building hierarchical use classification based on multiple data sources with a multi-label multimodal transformer network",
        "authors": "Wen Zhou, Claudio Persello, Alfred Stein",
        "published": "No Date",
        "citations": 0,
        "abstract": "Effective urban planning, city digital twins, and informed policy formulation rely heavily on precise building use information. While existing research often focuses on broad categories of building use, there is a noticeable gap in the classification of buildings&#8217; detailed use. This study addresses this gap by concurrently extracting both broad and detailed hierarchical information regarding building use. Our approach involves leveraging multiple data sources, including high spatial resolution remote sensing images (RS), digital surface models (DSM), street view images (SVI), and textual information from point of interest (POI) data. Given the complexity of mixed-use buildings, where different functions coexist, we treat building hierarchical use classification as a multi-label task, determining the presence of specific categories within a building. To maximize the utility of features across diverse modalities and their interrelationships, we introduce a novel multi-label multimodal Transformer-based feature fusion network. This network can simultaneously predict four broad categories and thirteen detailed categories, representing the first instance of utilizing these four modalities for building use classification. Experimental results demonstrate the effectiveness of our model, achieving a weighted average F1 score (WAF) of 91% for broad categories, 77% for detailed categories, and 84% for hierarchical categories. The macro average F1 scores (MAF) are 81%, 48%, and 56%, respectively. Ablation experiments highlight RS data as the cornerstone for hierarchical building use classification. DSM and POI provide slight supplementary information, while SVI data may introduce more noise than effective information. Our analysis of hierarchy consistency, supplementary, and exclusiveness between broad and detailed categories shows our model can effectively learn these relations. We compared two ways to obtain broad categories: classifying them directly and scaling up detailed categories, associating them with their broad counterparts. Experiments show that the WAF and MAF of the former are 3.8% and 6% higher than the latter. Notably, our research visualizes attention models for different modalities, revealing the synergy among them. Despite the model&#8217;s emphasis on SVI and POI data, the critical role of RS and DSM in building hierarchical use classification is underscored. By considering hierarchical use categories and accommodating mixed-use scenarios, our method provides more accurate and comprehensive insights into land use patterns.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-12472"
    },
    {
        "id": 23145,
        "title": "Pour transformer l’art, nous devons changer de culture",
        "authors": "Fred Turner, Emmanuel Vergès, Valentine Leÿs",
        "published": "2021-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/lobs.058.0060"
    },
    {
        "id": 23146,
        "title": "Display-Semantic Transformer for Scene Text Recognition",
        "authors": "Xinqi Yang, Wushour Silamu, Miaomiao Xu, Yanbing Li",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "Linguistic knowledge helps a lot in scene text recognition by providing semantic information to refine the character sequence. The visual model only focuses on the visual texture of characters without actively learning linguistic information, which leads to poor model recognition rates in some noisy (distorted and blurry, etc.) images. In order to address the aforementioned issues, this study builds upon the most recent findings of the Vision Transformer, and our approach (called Display-Semantic Transformer, or DST for short) constructs a masked language model and a semantic visual interaction module. The model can mine deep semantic information from images to assist scene text recognition and improve the robustness of the model. The semantic visual interaction module can better realize the interaction between semantic information and visual features. In this way, the visual features can be enhanced by the semantic information so that the model can achieve a better recognition effect. The experimental results show that our model improves the average recognition accuracy on six benchmark test sets by nearly 2% compared to the baseline. Our model retains the benefits of having a small number of parameters and allows for fast inference speed. Additionally, it attains a more optimal balance between accuracy and speed.",
        "link": "http://dx.doi.org/10.3390/s23198159"
    },
    {
        "id": 23147,
        "title": "An Efficient Skeleton-Based Fall Detection Algorithm Using Temporal Convolutional Networks with Transformer Encoder",
        "authors": "Xiaoqun Yu, Chenfeng Wang, Wenyu Wu, Shuping Xiong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4750350"
    },
    {
        "id": 23148,
        "title": "Human–Object Interaction Detection with Ratio-Transformer",
        "authors": "Tianlang Wang, Tao Lu, Wenhua Fang, Yanduo Zhang",
        "published": "2022-8-11",
        "citations": 1,
        "abstract": "Human–object interaction (HOI) is a human-centered object detection task that aims to identify the interactions between persons and objects in an image. Previous end-to-end methods have used the attention mechanism of a transformer to spontaneously identify the associations between persons and objects in an image, which effectively improved detection accuracy; however, a transformer can increase computational demands and slow down detection processes. In addition, the end-to-end method can result in asymmetry between foreground and background information. The foreground data may be significantly less than the background data, while the latter consumes more computational resources without significantly improving detection accuracy. Therefore, we proposed an input-controlled transformer, “ratio-transformer” to solve an HOI task, which could not only limit the amount of information in the input transformer by setting a sampling ratio, but also significantly reduced the computational demands while ensuring detection accuracy. The ratio-transformer consisted of a sampling module and a transformer network. The sampling module divided the input feature map into foreground versus background features. The irrelevant background features were a pooling sampler, which were then fused with the foreground features as input data for the transformer. As a result, the valid data input into the Transformer network remained constant, while irrelevant information was significantly reduced, which maintained the foreground and background information symmetry. The proposed network was able to learn the feature information of the target itself and the association features between persons and objects so it could query to obtain the complete HOI interaction triplet. The experiments on the VCOCO dataset showed that the proposed method reduced the computational demand of the transformer by 57% without any loss of accuracy, as compared to other current HOI methods.",
        "link": "http://dx.doi.org/10.3390/sym14081666"
    },
    {
        "id": 23149,
        "title": "Overloading Scenario in Solid State Transformer",
        "authors": "Nasiru B. Kadandani, Mohamed Dahidah, Salaheddine Ethni, Mohammed A. Alharbi",
        "published": "2020-10-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irec48820.2020.9310446"
    },
    {
        "id": 23150,
        "title": "A Two-Stage Attention Based Hierarchical Transformer for Remaining Useful Life Prediction",
        "authors": "Zhengyang Fan, Wanru Li, Kuo-Chu Chang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Accurate estimation of Remaining Useful Life (RUL) for aircraft engines is essential for ensuring safety and uninterrupted operations in the aviation industry. Numerous investigations have leveraged the success of attention-based Transformer architecture in sequence modeling tasks, particularly in its application to RUL prediction. These studies primarily focus on utilizing onboard sensor readings as input predictors. While various Transformer-based approaches have demonstrated improvement in RUL predictions, their exclusive focus on temporal attention within multivariate time series sensor readings, without considering sensor-wise attention, raises concerns about potential inaccuracies in RUL predictions. To address this concern, our paper proposes a novel solution in the form of a two-stage attention based hierarchical transformer (STAR) frame-work. This approach incorporates a two-stage attention mechanism, systematically addressing both temporal and sensor-wise attentions. Furthermore, we enhance the STAR RUL prediction framework by integrate hierarchical encoder-decoder structures to capture valuable information across different time scales. By conducting extensive numerical experiments with the CMAPSS datasets, we demonstrate that our proposed STAR framework significantly outperforms current state-of-the-art models for RUL prediction.",
        "link": "http://dx.doi.org/10.20944/preprints202312.2236.v1"
    },
    {
        "id": 23151,
        "title": "Operation and Supervision Control in Smart Transformer-based Meshed and Hybrid Grids",
        "authors": "Rongwu Zhu, Marco Liserre",
        "published": "2020-9-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/energycon48941.2020.9236572"
    },
    {
        "id": 23152,
        "title": "A Future of Smarter Digital Health Empowered by Generative Pretrained Transformer (Preprint)",
        "authors": "Hongyu Miao, Chengdong Li, Jing Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nUNSTRUCTURED\nGenerative pretrained transformer (GPT) tools have been thriving, as ignited by the remarkable success of OpenAI’s recent chatbot product. GPT technology offers countless opportunities to significantly improve or renovate current health care research and practice paradigms, especially digital health interventions and digital health–enabled clinical care, and a future of smarter digital health can thus be expected. In particular, GPT technology can be incorporated through various digital health platforms in homes and hospitals embedded with numerous sensors, wearables, and remote monitoring devices. In this viewpoint paper, we highlight recent research progress that depicts the future picture of a smarter digital health ecosystem through GPT-facilitated centralized communications, automated analytics, personalized health care, and instant decision-making.\n",
        "link": "http://dx.doi.org/10.2196/preprints.49963"
    },
    {
        "id": 23153,
        "title": "Deep Transformer-Based Asset Price and Direction Prediction",
        "authors": "Abdul Haluk Batur Gezici, Emre Sefer",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3358452"
    },
    {
        "id": 23154,
        "title": "Unveiling the Potential of Vision Transformer Architecture for Person Re-identification",
        "authors": "N. Perwaiz, M. Shahzad, M.M. Fraz",
        "published": "2022-10-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inmic56986.2022.9972908"
    },
    {
        "id": 23155,
        "title": "Transformer Based Multi-Source Domain Adaptation",
        "authors": "Dustin Wright, Isabelle Augenstein",
        "published": "2020",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.639"
    },
    {
        "id": 23156,
        "title": "Distinguishing Malicious Drones Using Vision Transformer",
        "authors": "Sonain Jamil, Muhammad Sohail Abbas, Arunabha M. Roy",
        "published": "2022-3-31",
        "citations": 20,
        "abstract": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
        "link": "http://dx.doi.org/10.3390/ai3020016"
    },
    {
        "id": 23157,
        "title": "Molecular Transformer for Chemical Reaction Prediction and Uncertainty Estimation",
        "authors": "Philippe Schwaller, Teodoro Laino, Theophile Gaudin, Peter Bolgar, Costas Bekas, Alpha A. Lee",
        "published": "No Date",
        "citations": 11,
        "abstract": "Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other works, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7297379.v1"
    },
    {
        "id": 23158,
        "title": "InterTrack: Interaction Transformer for 3D Multi-Object Tracking",
        "authors": "John Willes, Cody Reading, Steven L. Waslander",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/crv60082.2023.00017"
    },
    {
        "id": 23159,
        "title": "Pedestrian Trajectory Prediction for Real-Time Autonomous Systems via Context-Augmented Transformer Networks",
        "authors": "Khaled Saleh",
        "published": "2022-10-2",
        "citations": 4,
        "abstract": "Forecasting the trajectory of pedestrians in shared urban traffic environments from non-invasive sensor modalities is still considered one of the challenging problems facing the development of autonomous vehicles (AVs). In the literature, this problem is often tackled using recurrent neural networks (RNNs). Despite the powerful capabilities of RNNs in capturing the temporal dependency in the pedestrians’ motion trajectories, they were argued to be challenged when dealing with longer sequential data. Additionally, whilst the accommodation for contextual information (such as scene semantics and agents interactions) was shown to be effective for robust trajectory prediction, they can also impact the overall real-time performance of prediction system. Thus, in this work, we are introducing a framework based on the transformer networks that were demonstrated recently to be more efficient and outperformed RNNs in many sequential-based tasks. We relied on a fusion of sensor modalities, namely the past positional information, agent interactions information and scene physical semantics information as an input to our framework in order to not only provide a robust trajectory prediction of pedestrians, but also achieve real-time performance for multi-pedestrians’ trajectory prediction. We have evaluated our framework on three real-life datasets of pedestrians in shared urban traffic environments and it has outperformed the compared baseline approaches in both short-term and long-term prediction horizons. For the short-term prediction horizon, our approach has achieved lower scores according to the average displacement error and the root-mean squared error (ADE/RMSE) of predictions over the state-of-the art (SOTA) approach by more than 11 cm and 23 cm, respectively. While for the long-term prediction horizon, our approach has achieved lower ADE and FDE over the SOTA approach by more than 62 cm and 165 cm, respectively. Additionally, our approach has achieved superior real time performance by scoring only 0.025 s (i.e., it can provide 40 individual trajectory predictions per second).",
        "link": "http://dx.doi.org/10.3390/s22197495"
    },
    {
        "id": 23160,
        "title": "Towards an Automatic Transformer to Fhir Structured Radiology Report Via Gpt-4",
        "authors": "YuLin Pan, JinXia Fang, ChunTing Zhu, Minda Li, HuiQun Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4717860"
    },
    {
        "id": 23161,
        "title": "Experiment 12 Open-Circuit Test and Short-Circuit Test on a Single-Phase Transformer",
        "authors": "",
        "published": "2017-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683922797-012"
    },
    {
        "id": 23162,
        "title": "Application of Transformer in Medical Image Segmentation",
        "authors": "Yong Wu",
        "published": "2021-10-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.34297/ajbsr.2021.14.002014"
    },
    {
        "id": 23163,
        "title": "Hot Spot Temperature Analysis of Transformer using FEM on COMSOL",
        "authors": "Swapnil Solanki, Rohit Jangid, Gaurav Srivastava | Prateek Sharma | Ritvik Chaturvedi,  ",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd11280"
    },
    {
        "id": 23164,
        "title": "Two Legs Voltage Source Converter with DSTATCOM, T-Connected Transformer for Power Quality Improvement",
        "authors": "Bibhuti Bhusan Kumar,  ",
        "published": "2020-12-30",
        "citations": 0,
        "abstract": "The  numerous  loads  can  impact  the  working  environment  and  performance  of  the  source  apparatus.  Hence,  the  reimbursement  of this current will help in enhancing the presentation of the power system apparatus. This paper presents the strategy and employment of a Distribution Static Compensator (DSTATCOM) with the T-Connected transformer for compensation of load neutral current in the presence of three-phase unbalanced linear load. There are different types of control  strategy  in  that  one  of  the  best  methods  is  the  unit  vector  template method-based control algorithm has been implemented for the control of the proposed DSTATCOM. The proposed model has been simulated in a SIMULINK/ MATLAB environment. The simulation results show the effectiveness of the proposed algorithm.",
        "link": "http://dx.doi.org/10.24321/2456.1401.202007"
    },
    {
        "id": 23165,
        "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
        "authors": "Bofan Song, Dharma Raj KC, Rubin Yuchan Yang, Shaobai Li, Chicheng Zhang, Rongguang Liang",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "link": "http://dx.doi.org/10.3390/cancers16050987"
    },
    {
        "id": 23166,
        "title": "Performance and Emission Charecterstics of Diesel Engine Blended with Used Transformer Oil: A Review",
        "authors": "Samyak Jain, Yogesh Yadav,  ",
        "published": "2018-12-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd19185"
    },
    {
        "id": 23167,
        "title": "VulD-Transformer: Source Code Vulnerability Detection via Transformer",
        "authors": "Xuejun Zhang, Fenghe Zhang, Bo Zhao, Bo Zhou, Boyang Xiao",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3609437.3609451"
    },
    {
        "id": 23168,
        "title": "MR-Transformer: Multiresolution Transformer for Multivariate Time Series Prediction",
        "authors": "Siying Zhu, Jiawei Zheng, Qianli Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3327416"
    },
    {
        "id": 23169,
        "title": "基于transformer和自适应嵌入策略的可逆信息隐藏",
        "authors": "Linna Zhou, Zhigao Lu, Weike You, Xiaofei Fang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300041"
    },
    {
        "id": 23170,
        "title": "Faster Transformer-DS: Multiscale Vehicle Detection of Remote-Sensing Images Based on Transformer and Distance-Scale Loss",
        "authors": "Jiahuan Zhang, Hengzhen Liu, Yi Zhang, Menghan Li, Zongqian Zhan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jstars.2023.3335283"
    },
    {
        "id": 23171,
        "title": "On the use of Real-Time transformer temperature estimation for improving transformer operational tripping Schemes",
        "authors": "Yuhang Cong, Peter Wall, Qi Li, Yigui Li, Vladimir Terzija",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2022.108420"
    },
    {
        "id": 23172,
        "title": "Aware-Transformer: A Novel Pure Transformer-Based Model for Remote Sensing Image Captioning",
        "authors": "Yukun Cao, Jialuo Yan, Yijia Tang, Zhenyi He, Kangle Xu, Yu Cheng",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-50069-5_10"
    },
    {
        "id": 23173,
        "title": "Effect of DC Bias on the Operation Characteristics and Magnetic Circuit Difference of Single-Phase Transformer and Three-Phase Transformer",
        "authors": "Beibei Liang, Zhiwei Chen, Landong Liang, Fei Xia, Qipei Zhou, Yingying Zhang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-1528-4_139"
    },
    {
        "id": 23174,
        "title": "基于轻量化方向Transformer模型的肺炎X光片辅助诊断",
        "authors": "周涛 Zhou Tao, 叶鑫宇 Ye Xinyu, 刘凤珍 Liu Fengzhen, 陆惠玲 Lu Huiling",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/aos230447"
    },
    {
        "id": 23175,
        "title": "High-Frequency Transformer Design With Medium-Voltage Insulation for Resonant Converter in Solid-State Transformer",
        "authors": "Zheqing Li, Eric Hsieh, Qiang Li, Fred C. Lee",
        "published": "2023-8",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpel.2023.3279030"
    },
    {
        "id": 23176,
        "title": "Enhancing the Transformer Decoder with Transition-based Syntax",
        "authors": "Leshem Choshen, Omri Abend",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.conll-1.27"
    },
    {
        "id": 23177,
        "title": "Im-to-Lim: A Transformer-Based Framework for Limerick Generation Associated with an Image",
        "authors": "Divyanshi Thapa, Praveen Joe I R, Shajina Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4551886"
    },
    {
        "id": 23178,
        "title": "Graph Transformer Attention Networks for Traffic Flow Prediction",
        "authors": "Haochun Ruan, Xinxin Feng, Haifeng Zheng",
        "published": "2021-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc54389.2021.9674238"
    },
    {
        "id": 23179,
        "title": "Acoustic Multi-Parameter Early Warning Method for Transformer DC Bias State",
        "authors": "Yuhao Zhou, Bowen Wang",
        "published": "2022-4-10",
        "citations": 2,
        "abstract": "The acoustic signal in the operation of a power transformer contains a lot of transformer operation state information, which is of great significance to the detection of DC bias state. In this paper, three typical parameters used for DC bias state detection are selected by comparing the acoustic variation of a 500 kV Jingting transformer substation No. 2 transformer with that of the core model built in the laboratory; then, acoustic samples of the 162 EHV normal state transformers are collected, and the distribution regularity of three typical parameters in normal state is given. Finally, according to the distribution regularity, clear warning threshold of typical parameters are given, and the DC bias cases from the 500 kV Jingting transformer substation are used to verify the effectiveness of the threshold.",
        "link": "http://dx.doi.org/10.3390/s22082906"
    },
    {
        "id": 23180,
        "title": "Vision Transformer and Parallel Convolutional Neural Network for Speech Emotion Recognition",
        "authors": "Saber Hashemi, Mohammad Asgari",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334797"
    },
    {
        "id": 23181,
        "title": "A CNN-Transformer Deep Learning Model for Real-time Sleep Stage Classification in an Energy-Constrained Wireless Device",
        "authors": "Zongyan Yao, Xilin Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThis paper proposes a deep learning (DL) model for automatic sleep stage classification based on single-channel EEG data. The DL model features a convolutional neural network (CNN) and transformers. The model was designed to run on energy and memory-constrained devices for real-time operation with local processing. The Fpz-Cz EEG signals from a publicly available Sleep-EDF dataset are used to train and test the model. Four convolutional filter layers were used to extract features and reduce the data dimension. Then, transformers were utilized to learn the time-variant features of the data. To improve performance, we also implemented a subject specific training before the inference (i.e., prediction) stage. With the subject specific training, the F1 score was 0.91, 0.37, 0.84, 0.877, and 0.73 for wake, N1-N3, and rapid eye movement (REM) stages, respectively. The performance of the model was comparable to the state-of-the-art works with significantly greater computational costs. We tested a reduced-sized version of the proposed model on a low-cost Arduino Nano 33 BLE board and it was fully functional and accurate. In the future, a fully integrated wireless EEG sensor with edge DL will be developed for sleep research in pre-clinical and clinical experiments, such as real-time sleep modulation.",
        "link": "http://dx.doi.org/10.1101/2022.11.21.22282544"
    },
    {
        "id": 23182,
        "title": "A new inrush current identification algorithm based on transformer core saturation",
        "authors": "Ming Jin, Yuanlong Liu",
        "published": "2017-7",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274695"
    },
    {
        "id": 23183,
        "title": "Laboratory Investigations of Parallel Connected Inverters Feeding Medium Voltage Transformer",
        "authors": "Maciej Kozak",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2018.8591837"
    },
    {
        "id": 23184,
        "title": "Summarized Diagnostic Parameter for Condition Assessment of Power Transformer Windings Insulation",
        "authors": "I.A. Khudonogov, E.Yu. Puzina, A.G. Tuigunova",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867610"
    },
    {
        "id": 23185,
        "title": "Detect Turn-takings in Subtitle Streams with Semantic Recall Transformer Encoder",
        "authors": "Yuhai Liang, Qiang Zhou",
        "published": "2020-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp51396.2020.9310512"
    },
    {
        "id": 23186,
        "title": "A  Foveated Vision-Transformer Model for Scene Classification",
        "authors": "Aditya Jonnalagadda, Miguel Eckstein",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1167/jov.22.14.4440"
    },
    {
        "id": 23187,
        "title": "Transformer Winding Movement differentiated using Frequency Response Analysis",
        "authors": "Rongeet Talukdar",
        "published": "2021-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaect49130.2021.9392474"
    },
    {
        "id": 23188,
        "title": "Machinery Prognostics and High-Dimensional Data Feature Extraction Based on A Transformer Self-Attention Transfer Network",
        "authors": "Shilong Sun, Tengyi Peng, Haodong Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Machinery degradation assessment can offer meaningful prognosis and health management in-formation. Although numerous machine prediction models based on artificial intelligence have emerged in recent years, they still face a series of challenges: (1) Many models continue to rely on manual feature extraction. (2) Deep learning models still struggle with long sequence prediction tasks. (3) Health indicators are inefficient for remaining useful life (RUL) prediction with cross-operational environments when dealing with high-dimensional datasets as inputs. This research proposes a health indicator construction methodology based on a transformer self-attention transfer network (TSTN). This methodology can directly deal with the high-dimensional raw dataset and keep all the information without missing when the signals are taken as the input of the diagnosis and prognosis model.  First, we design an encoder with a long-term and short-term self-attention mechanism to capture crucial time-varying information from a high-dimensional dataset. Second, we propose an estimator that can map the embedding from the encoder output to the estimated degradation trends. Then, we present a domain dis-criminator to extract invariant features from different machine operating conditions. The case studies with the FEMTO-ST bearing dataset and the Monte Carlo method for RUL prediction during the degradation process are conducted. The experiment results fully exhibited the signif-icant advantages of the proposed method compared to other state-of-the-art techniques.",
        "link": "http://dx.doi.org/10.20944/preprints202309.2035.v1"
    },
    {
        "id": 23189,
        "title": "La bataille pour la gouvernance mondiale de l’alimentation et de l’agriculture",
        "authors": "Frédéric Mousseau, Maurice Hérion",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0061"
    },
    {
        "id": 23190,
        "title": "Design of three-stage power electronic transformer for smart grid",
        "authors": "Jiaqi Li",
        "published": "2021-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceemt52412.2021.9602125"
    },
    {
        "id": 23191,
        "title": "Deep Natural Language Processing",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904"
    },
    {
        "id": 23192,
        "title": "A Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "asmaa Elsaid, ammar mohamed, lamiaa Fattouh, mohamed sakre",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2672691/v1"
    },
    {
        "id": 23193,
        "title": "Longitudinal Propagation in a Magnetized Time-Varying Plasma ∗",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-8"
    },
    {
        "id": 23194,
        "title": "Syntactic aperture radar imaging",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00003-3"
    },
    {
        "id": 23195,
        "title": "A Simple Resonant Frequency Tracking Technique for DC Transformer Operation",
        "authors": "Yuqi Wei, Alan Mantooth",
        "published": "2020-11-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid48559.2020.9330634"
    },
    {
        "id": 23196,
        "title": "Microwave-assisted Synthesis of TiO2-based Transformer Nanofluid: Investigation on the perspective of Electrical and Thermal  Properties",
        "authors": "G. Koperundevi, S. Raja",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe study on dispersing nanoparticles in transformer oil to elevate their dielectric properties has evolved significantly in the recent decade. Alongside the conventional dispersion techniques (mechanical stirring & ultrasonic homogenization) in practice, this work experiments with the microwave energy of 2.45 GHz frequency radiated onto the TiO2 nanoparticle-based nanofluid (TNF). The electrical and thermal properties of TNF (TiO2 nanoparticle & surfactant dispersed in transformer mineral oil) are experimentally investigated & presented in this article. Effective synthesis procedure to enhance the dielectric properties with good dispersivity has been recognized from the superlative combination of dispersion techniques. Its effectiveness in enhancing the electrical and thermal properties is investigated by verifying its dielectric breakdown strength, dielectric constant (εr), dielectric dissipation factor (tan δ), and flash & fire point. Results justified that the TNF synthesized by combining the processes of stirring, Ultrasonic homogenization, and microwave irradiation in a rational sequence exhibited better electrical & unaltered thermal properties when compared with samples prepared through stirring and sonication. TNF prepared through microwave synthesis improved the AC breakdown voltage (BDV) by 16.92% more than TNF prepared without microwave synthesis. Hence this could be an effective route to prepare the TNF with improved electrical properties.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1860339/v1"
    },
    {
        "id": 23197,
        "title": "Transformer-Based Object Detection in Drone Images Using Split Attention Module: Pvsamnet",
        "authors": "Sirisha Museboyina, Sudha S.V",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4250755"
    },
    {
        "id": 23198,
        "title": "A fault ride through strategy of multi-ports DC transformer",
        "authors": "Hao Wu",
        "published": "2020-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacre50138.2020.9230334"
    },
    {
        "id": 23199,
        "title": "The Thomson Jumping Ring Experiment and Ideal Transformer",
        "authors": "Chiu-king Ng",
        "published": "2022-5-1",
        "citations": 0,
        "abstract": "In this paper, we utilize the readily known theory of the ideal transformer to furnish a self-contained qualitative explanation on the AC-powered Thomson jumping ring (TJR) experiment.",
        "link": "http://dx.doi.org/10.1119/5.0036490"
    },
    {
        "id": 23200,
        "title": "Parameter Agnostic Stacked Wavelet Transformer for Detecting Singularities",
        "authors": "Akshay Agarwal, Mayank Vatsa, Richa Singh, Nalini Ratha",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4292759"
    },
    {
        "id": 23201,
        "title": "Facial Emotion Recognition using Video Visual Transformer and Attention Dropping",
        "authors": "Bogdan Mocanu, Ruxandra Tapu",
        "published": "2023-7-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isscs58449.2023.10190868"
    },
    {
        "id": 23202,
        "title": "Analysis Of Offshore Platform Current Transformer Failures",
        "authors": "Alok Gupta, Mustafa Demiroglu, Nick Isaacs",
        "published": "2022-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pcic42668.2022.10181253"
    },
    {
        "id": 23203,
        "title": "HARMONIC ANALYSIS – DISTRIBUTION TRANSFORMER MONITORING",
        "authors": "J. F. Tissier, J. Cornet, A. BenRahmoun",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.2129"
    },
    {
        "id": 23204,
        "title": "Generating Natural Pedestrian Crowds by Learning Real Crowd Trajectories Through a Transformer-Based Gan",
        "authors": "Dapeng Yan, Gangyi Ding, Kexiang Huang, Tianyu Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4510964"
    },
    {
        "id": 23205,
        "title": "Bidirectional Coupling of Electromagnetic and Temperature Fields for Power Electronic Transformer",
        "authors": "Shihu Zhang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icopesa56898.2023.10141194"
    },
    {
        "id": 23206,
        "title": "An Enhanced Method on Transformer-Based Model for ONE2SEQ Keyphrase Generation",
        "authors": "Lingyun Shen, Xiaoqiu Le",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "Keyphrase generation is a long-standing task in scientific literature retrieval. The Transformer-based model outperforms other baseline models in this challenge dramatically. In cross-domain keyphrase generation research, topic information plays a guiding role during generation, while in keyphrase generation of individual text, titles can replace topic roles and convey more semantic information. As a result, we proposed an enhanced model architecture named TAtrans. In this research, we investigate the advantages of title attention and sequence code representing phrase order in keyphrase sequence in improving Transformer-based keyphrase generation. We conduct experiments on five widely-used English datasets specifically designed for keyphrase generation. Our method achieves an F1 score in the top five, surpassing the Transformer-based model by 3.2% in KP20k. The results demonstrate that the proposed method outperforms all the previous models on prediction present keyphrases. To evaluate the performance of the proposed model in the Chinese dataset, we construct a new Chinese abstract dataset called CNKIL, which contains a total of 54,546 records. The F1 score of the top five for predicting present keyphrases on the CNKIL dataset exceeds 2.2% compared to the Transformer-based model. However, there is no significant improvement in the model’s performance in predicting absent keyphrases.",
        "link": "http://dx.doi.org/10.3390/electronics12132968"
    },
    {
        "id": 23207,
        "title": "Goal-Oriented Transformer to Predict Context-Aware Trajectories in Urban Scenarios",
        "authors": "Álvaro Quintanar, Rubén Izquierdo, Ignacio Parra, David Fernández-Llorca",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023039057"
    },
    {
        "id": 23208,
        "title": "CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment",
        "authors": "Yuchen Liu, Li-Chia Yang, Alexander Pawlicki, Marko Stamenovic",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10857"
    },
    {
        "id": 23209,
        "title": "FiLM Conditioning with Enhanced Feature to the Transformer-based End-to-End Noisy Speech Recognition",
        "authors": "Da-Hee Yang, Joon-Hyuk Chang",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-161"
    },
    {
        "id": 23210,
        "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
        "authors": "Chenhao Shuai, Chaohua Shi, Lu Gan, Hongqing Liu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-113"
    },
    {
        "id": 23211,
        "title": "Study of transformer and motor winding under pulsed power application",
        "authors": "Arijit Basuray, Saibal Chatterjee",
        "published": "2017-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppc.2017.8291298"
    },
    {
        "id": 23212,
        "title": "Comparison of Modular Multilevel Converter based Solid State Transformer for ACDC application",
        "authors": "Zhengzhao Li, Zian Qin, Reza Mirzadarani, Mohamad Ghaffarian Niasar, Pavol Bauer",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> For electrolyzer applications, traditional solutions using line frequency transformers plus rectifiers are bulky, heavy and have low controllability. The Solid State Transformer (SST) could be a promising solution to solve the mentioned issues. This paper compares the semiconductor ratings and capacitance of five different Modular Multilevel Converter (MMC) based Solid State Transformer (SST) topologies. The results show that the DRU-MMC based topologies have the lowest semiconductor ratings and capacitance. Because of the unidirectional power flow requirement, the source side MMC of Back-to-Back (BtB) MMC based SST could be replaced by DRU, thus the cost is drastically saved. Another interesting finding is that the DRU-MMC energy ripple is much lower than half of the energy ripple in BtB MMC, which is different from HVDC MMC. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22548775.v2"
    },
    {
        "id": 23213,
        "title": "Impact of Moisture and Remaining Life Estimation",
        "authors": "",
        "published": "2017-8-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch7"
    },
    {
        "id": 23214,
        "title": "ThaiTC:Thai Transformer-based Image Captioning",
        "authors": "Teetouch Jaknamon, Sanparith Marukatat",
        "published": "2022-11-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isai-nlp56921.2022.9960246"
    },
    {
        "id": 23215,
        "title": "Frequency Characteristics of Power Transformer for Isolated DC-DC Converter",
        "authors": "Takayuki Nakamura, Toshiyuki Murakami",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3145200"
    },
    {
        "id": 23216,
        "title": "Improving Text Classification with Transformer",
        "authors": "Gokhan Soyalp, Artun Alar, Kaan Ozkanli, Beytullah Yildiz",
        "published": "2021-9-15",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk52708.2021.9558906"
    },
    {
        "id": 23217,
        "title": "Operation and control of smart transformer based distribution grid in a microgrid system",
        "authors": "Dwijasish Das, Chandan Kumar",
        "published": "2017-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec.2017.8310448"
    },
    {
        "id": 23218,
        "title": "Novel Approach Based on Transformer with Attention Mechanism for Text Summarization",
        "authors": "Mostafa Gamal, Hesham F. A. Hamed, Mustafa Abdul Salam, Sara Sweidan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4609431"
    },
    {
        "id": 23219,
        "title": "Heat map of a simple indoor transformer model",
        "authors": "Sergii Shevchenko, Hanus Roman, Andrii Potryvai",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/khpiweek61412.2023.10313001"
    },
    {
        "id": 23220,
        "title": "Face Mask Detection using Vision Transformer",
        "authors": "Bhavik Pandya, Darshana Patel, Kin-Choong Yow",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccece58730.2023.10288856"
    },
    {
        "id": 23221,
        "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation",
        "authors": "Jingjing Chen, Qirong Mao, Dong Liu",
        "published": "2020-10-25",
        "citations": 150,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2205"
    },
    {
        "id": 23222,
        "title": "Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese",
        "authors": "Shiyu Zhou, Linhao Dong, Shuang Xu, Bo Xu",
        "published": "2018-9-2",
        "citations": 54,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-1107"
    },
    {
        "id": 23223,
        "title": "Audio Pyramid Transformer with Domain Adaption for Weakly Supervised Sound Event Detection and Audio Classification",
        "authors": "Yifei Xin, Dongchao Yang, Yuexian Zou",
        "published": "2022-9-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10057"
    },
    {
        "id": 23224,
        "title": "An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition",
        "authors": "Ruchao Fan, Wei Chu, Peng Chang, Jing Xiao, Abeer Alwan",
        "published": "2021-8-30",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1955"
    },
    {
        "id": 23225,
        "title": "Solar Irradiance Anticipative Transformer",
        "authors": "Thomas M. Mercier, Tasmiat Rahman, Amin Sabet",
        "published": "2023-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00200"
    },
    {
        "id": 23226,
        "title": "Kernel Transformer Networks for Compact Spherical Convolution",
        "authors": "Yu-Chuan Su, Kristen Grauman",
        "published": "2019-6",
        "citations": 59,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00967"
    },
    {
        "id": 23227,
        "title": "Transformer Enables Reference Free And Unsupervised Analysis of Spatial Transcriptomics",
        "authors": "Chongyue Zhao, Zhongli Xu, Xinjun Wang, Kong Chen, Heng Huang, Wei Chen",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractThe development of spatial transcriptomics technologies makes it possible to study tissue heterogeneity at the scale of spatial expressed microenvironment. However, most of the previous methods collapse the spatial patterns in the low spatial resolution. Existing reference based deconvolution methods integrate single-cell reference and spatial transcriptomics data to predict the proportion of cell-types, but the availability of suitable single-cell reference is often limited. In this paper, we propose a novel Transformer based model (TransfromerST) to integrate the spatial gene expression measurements and their spatial patterns in the histology image (if available) without single cell reference. TransfromerST enables the learning of the locally realistic and globally consistent constituents at nearly single cell resolution. TransfromerST firstly uses a transformer based variational autoencoder to explore the latent representation of gene expression, which is further embedded with the spatial relationship learned from adaptive graph Transformer model. The super-resolved cross-scale graph network improves the model-fit to enhanced structure-functional interactions. The public and in-house experimental results with multimodal spatial transcriptomics data demonstrate TransfromerST could highlight the tissue structures at nearly single cell resolution and detect the spatial variable genes and meta gene for each spatial domain. In summary, TransfromerST provides an effective and efficient alternative for spatial transcriptomics tissue clustering, super-resolution and gene expression prediction from histology image.",
        "link": "http://dx.doi.org/10.1101/2022.08.11.503261"
    },
    {
        "id": 23228,
        "title": "Transformer-Based Fire Detection in Videos",
        "authors": "Konstantina Mardani, Nicholas Vretos, Petros Daras",
        "published": "2023-3-11",
        "citations": 2,
        "abstract": "Fire detection in videos forms a valuable feature in surveillance systems, as its utilization can prevent hazardous situations. The combination of an accurate and fast model is necessary for the effective confrontation of this significant task. In this work, a transformer-based network for the detection of fire in videos is proposed. It is an encoder–decoder architecture that consumes the current frame that is under examination, in order to compute attention scores. These scores denote which parts of the input frame are more relevant for the expected fire detection output. The model is capable of recognizing fire in video frames and specifying its exact location in the image plane in real-time, as can be seen in the experimental results, in the form of segmentation mask. The proposed methodology has been trained and evaluated for two computer vision tasks, the full-frame classification task (fire/no fire in frames) and the fire localization task. In comparison with the state-of-the-art models, the proposed method achieves outstanding results in both tasks, with 97% accuracy, 20.4 fps processing time, 0.02 false positive rate for fire localization, and 97% for f-score and recall metrics in the full-frame classification task.",
        "link": "http://dx.doi.org/10.3390/s23063035"
    },
    {
        "id": 23229,
        "title": "Federated Learning with Dynamic Transformer for Text to Speech",
        "authors": "Zhenhou Hong, Jianzong Wang, Xiaoyang Qu, Jie Liu, Chendong Zhao, Jing Xiao",
        "published": "2021-8-30",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-2039"
    },
    {
        "id": 23230,
        "title": "Paraphrase Bidirectional Transformer with Multi-task Learning",
        "authors": "Bowon Ko, Ho-Jin Choi",
        "published": "2020-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigcomp48618.2020.00-72"
    },
    {
        "id": 23231,
        "title": "COVID-19 Fake News and Misinformation Detection using Transformer Learning",
        "authors": "Zepeng Cui",
        "published": "2022-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icekim55072.2022.00210"
    },
    {
        "id": 23232,
        "title": "Facial Expression Recognition with Enhanced Relation-Aware Attention and Cross-Feature Fusion transformer",
        "authors": "YAN DONG, Ting Wang, Yanfeng Pu, Jian Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nFace expression recognition(FER) is an important research branch in the field of the computer vision neighborhood. Three prevalent problems in FER tasks that severely impact recognition rates are inter-class similarity, intra-class differences, and facial occlusion issues. Although there have been studies that address some of these issues, none of them can adequately address all three issues in a unified framework. In this paper, we propose a novel dual-branch structure of enhanced relation-aware attention and cross-feature fusion transformer network to comprehensively solve all three issues. Specifically, we design the Enhanced Relation-Aware Attention module to maximize the exploration of more local expression features. At the same time, the Transformer Perceptual Encoder module is adopted to establishing the contextual relationship between individual patches under global information. This greatly alleviates the inter-class similarity problem and the facial occlusion and facial pose transformation problems. On the basis of a dual branch structure, we extract facial image features using facial landmarks features to guide them and design Cross-Feature Fusion Transformer module to deeply cross-fuse two different semantic features. Experiments are performed and results show that our method can greatly alleviated intra-class difference problem with comparison of several traditional methods on three commonly used datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3948258/v1"
    },
    {
        "id": 23233,
        "title": "Research on Capacitance Voltage Transformer Harmonic Measurement",
        "authors": "Xin Shen, Hong-Chun Shu, Min Cao",
        "published": "2019-5-16",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18494/sam.2019.2270"
    },
    {
        "id": 23234,
        "title": "Plant disease recognition based on parallel Vision Transformer",
        "authors": "Haoxin Liu",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "Ensuring food security and increasing crop yields are significant challenges facing the world today as the global population grows. In this context, the seasonable and accurate perception of plant diseases is essential. This paper proposes to combine Vision Transformer (ViT) and Gpipe to identify disease pictures of plant leaves. Among them, ViT is used to ensure the identification accuracy of the model, and Gpipe is used to improve the running speed of the model. This experiment uses the PlantVillage dataset, which includes diseased pictures of various plant leaves, to evaluate model performance. It uses the method of controlled experiments to Identify the models performance and efficiency of parallelism in the pipeline. After many experiments, the recognition accuracy of the color picture part of ViT on this data set has reached 93%. In addition, the memory requirements for a single GPU are significantly decreased using pipeline parallelism. This study provides a low-cost and efficient plant disease identification tool for the agricultural field, which can detect plant diseases correctly and in time. It is beneficial to increase food production and ensure food safety.",
        "link": "http://dx.doi.org/10.54254/2755-2721/41/20230739"
    },
    {
        "id": 23235,
        "title": "Review for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": " mohamad abedini",
        "published": "2021-6-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/review3"
    },
    {
        "id": 23236,
        "title": "Decision letter for \"Investigation on magnetostrictive behaviour of a converter transformer influenced by dominant harmonics: A FEM and ANN based approach\"",
        "authors": "",
        "published": "2021-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12957/v1/decision1"
    },
    {
        "id": 23237,
        "title": "Adaptation of Multilingual T5 Transformer for Indonesian Language",
        "authors": "Mukhlish Fuadi, Adhi Dharma Wibawa, Surya Sumpeno",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itis59651.2023.10420049"
    },
    {
        "id": 23238,
        "title": "DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising",
        "authors": "Junhui Li, Pu Wang, Youshan Zhang",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3294476"
    },
    {
        "id": 23239,
        "title": "ACCELERATION OF TRANSFORMER ARCHITECTURES ON JETSON XAVIER USING TENSORRT",
        "authors": "K.H. Nikoghosyan, T.B. Khachatryan, E.A. Harutyunyan, D.M. Galstyan",
        "published": "2023",
        "citations": 0,
        "abstract": "Transformer models have become a key component in many natural language processing and computer vision tasks. However, these models are often computationally intensive and require a lot of resources to run efficiently. To address this challenge, this study studies the use of TensorRT, an optimization library provided by NVIDIA, to accel-erate the inference speed of transformer models on Jetson Xavier NX, a low-power and high-performance embedded platform. \nThis research demonstrates the significant impact of TensorRT optimization on transformer models. Specifically, we present two case studies: one involving a Transformer model for text-to-speech synthesis and another featuring a Vision Transformer model for image classification. In both cases, TensorRT optimization leads to substantial improve-ments in inference speed, making these models highly efficient for edge device deploy-ment. For the text-to-speech task, TensorRT optimization results in a remarkable 60% re-duction in inference time while decreasing memory usage by 17%. Similarly, for image classification, the Vision Transformer model experiences over a 60% increase in inference speed with a negligible 0.1% decrease in accuracy. This study not only showcases the prac-tical benefits of TensorRT but also highlights the potential for further optimization and deployment of transformer models on edge platforms. This demonstrates the potential of TensorRT to optimize transformer models, both in terms of performance and memory usage. This could have far-reaching implications for edge computing, allowing more appli-cations to be deployed on low-power devices.",
        "link": "http://dx.doi.org/10.53297/18293336-2023.2-30"
    },
    {
        "id": 23240,
        "title": "Hot Spot Temperature Analysis of Transformer using FEM on COMSOL",
        "authors": "Swapnil Solanki, Rohit Jangid, Gaurav Srivastava | Prateek Sharma | Ritvik Chaturvedi,  ",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd11280"
    },
    {
        "id": 23241,
        "title": "Two Legs Voltage Source Converter with DSTATCOM, T-Connected Transformer for Power Quality Improvement",
        "authors": "Bibhuti Bhusan Kumar,  ",
        "published": "2020-12-30",
        "citations": 0,
        "abstract": "The  numerous  loads  can  impact  the  working  environment  and  performance  of  the  source  apparatus.  Hence,  the  reimbursement  of this current will help in enhancing the presentation of the power system apparatus. This paper presents the strategy and employment of a Distribution Static Compensator (DSTATCOM) with the T-Connected transformer for compensation of load neutral current in the presence of three-phase unbalanced linear load. There are different types of control  strategy  in  that  one  of  the  best  methods  is  the  unit  vector  template method-based control algorithm has been implemented for the control of the proposed DSTATCOM. The proposed model has been simulated in a SIMULINK/ MATLAB environment. The simulation results show the effectiveness of the proposed algorithm.",
        "link": "http://dx.doi.org/10.24321/2456.1401.202007"
    },
    {
        "id": 23242,
        "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
        "authors": "Bofan Song, Dharma Raj KC, Rubin Yuchan Yang, Shaobai Li, Chicheng Zhang, Rongguang Liang",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "link": "http://dx.doi.org/10.3390/cancers16050987"
    },
    {
        "id": 23243,
        "title": "Performance and Emission Charecterstics of Diesel Engine Blended with Used Transformer Oil: A Review",
        "authors": "Samyak Jain, Yogesh Yadav,  ",
        "published": "2018-12-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd19185"
    },
    {
        "id": 23244,
        "title": "VulD-Transformer: Source Code Vulnerability Detection via Transformer",
        "authors": "Xuejun Zhang, Fenghe Zhang, Bo Zhao, Bo Zhou, Boyang Xiao",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3609437.3609451"
    },
    {
        "id": 23245,
        "title": "MR-Transformer: Multiresolution Transformer for Multivariate Time Series Prediction",
        "authors": "Siying Zhu, Jiawei Zheng, Qianli Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3327416"
    },
    {
        "id": 23246,
        "title": "基于transformer和自适应嵌入策略的可逆信息隐藏",
        "authors": "Linna Zhou, Zhigao Lu, Weike You, Xiaofei Fang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300041"
    },
    {
        "id": 23247,
        "title": "Faster Transformer-DS: Multiscale Vehicle Detection of Remote-Sensing Images Based on Transformer and Distance-Scale Loss",
        "authors": "Jiahuan Zhang, Hengzhen Liu, Yi Zhang, Menghan Li, Zongqian Zhan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jstars.2023.3335283"
    },
    {
        "id": 23248,
        "title": "On the use of Real-Time transformer temperature estimation for improving transformer operational tripping Schemes",
        "authors": "Yuhang Cong, Peter Wall, Qi Li, Yigui Li, Vladimir Terzija",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijepes.2022.108420"
    },
    {
        "id": 23249,
        "title": "Aware-Transformer: A Novel Pure Transformer-Based Model for Remote Sensing Image Captioning",
        "authors": "Yukun Cao, Jialuo Yan, Yijia Tang, Zhenyi He, Kangle Xu, Yu Cheng",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-50069-5_10"
    },
    {
        "id": 23250,
        "title": "Enhancing the Transformer Decoder with Transition-based Syntax",
        "authors": "Leshem Choshen, Omri Abend",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.conll-1.27"
    },
    {
        "id": 23251,
        "title": "Im-to-Lim: A Transformer-Based Framework for Limerick Generation Associated with an Image",
        "authors": "Divyanshi Thapa, Praveen Joe I R, Shajina Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4551886"
    },
    {
        "id": 23252,
        "title": "Graph Transformer Attention Networks for Traffic Flow Prediction",
        "authors": "Haochun Ruan, Xinxin Feng, Haifeng Zheng",
        "published": "2021-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc54389.2021.9674238"
    },
    {
        "id": 23253,
        "title": "Acoustic Multi-Parameter Early Warning Method for Transformer DC Bias State",
        "authors": "Yuhao Zhou, Bowen Wang",
        "published": "2022-4-10",
        "citations": 2,
        "abstract": "The acoustic signal in the operation of a power transformer contains a lot of transformer operation state information, which is of great significance to the detection of DC bias state. In this paper, three typical parameters used for DC bias state detection are selected by comparing the acoustic variation of a 500 kV Jingting transformer substation No. 2 transformer with that of the core model built in the laboratory; then, acoustic samples of the 162 EHV normal state transformers are collected, and the distribution regularity of three typical parameters in normal state is given. Finally, according to the distribution regularity, clear warning threshold of typical parameters are given, and the DC bias cases from the 500 kV Jingting transformer substation are used to verify the effectiveness of the threshold.",
        "link": "http://dx.doi.org/10.3390/s22082906"
    },
    {
        "id": 23254,
        "title": "Vision Transformer and Parallel Convolutional Neural Network for Speech Emotion Recognition",
        "authors": "Saber Hashemi, Mohammad Asgari",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334797"
    },
    {
        "id": 23255,
        "title": "A CNN-Transformer Deep Learning Model for Real-time Sleep Stage Classification in an Energy-Constrained Wireless Device",
        "authors": "Zongyan Yao, Xilin Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThis paper proposes a deep learning (DL) model for automatic sleep stage classification based on single-channel EEG data. The DL model features a convolutional neural network (CNN) and transformers. The model was designed to run on energy and memory-constrained devices for real-time operation with local processing. The Fpz-Cz EEG signals from a publicly available Sleep-EDF dataset are used to train and test the model. Four convolutional filter layers were used to extract features and reduce the data dimension. Then, transformers were utilized to learn the time-variant features of the data. To improve performance, we also implemented a subject specific training before the inference (i.e., prediction) stage. With the subject specific training, the F1 score was 0.91, 0.37, 0.84, 0.877, and 0.73 for wake, N1-N3, and rapid eye movement (REM) stages, respectively. The performance of the model was comparable to the state-of-the-art works with significantly greater computational costs. We tested a reduced-sized version of the proposed model on a low-cost Arduino Nano 33 BLE board and it was fully functional and accurate. In the future, a fully integrated wireless EEG sensor with edge DL will be developed for sleep research in pre-clinical and clinical experiments, such as real-time sleep modulation.",
        "link": "http://dx.doi.org/10.1101/2022.11.21.22282544"
    },
    {
        "id": 23256,
        "title": "A new inrush current identification algorithm based on transformer core saturation",
        "authors": "Ming Jin, Yuanlong Liu",
        "published": "2017-7",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274695"
    },
    {
        "id": 23257,
        "title": "Laboratory Investigations of Parallel Connected Inverters Feeding Medium Voltage Transformer",
        "authors": "Maciej Kozak",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2018.8591837"
    },
    {
        "id": 23258,
        "title": "Summarized Diagnostic Parameter for Condition Assessment of Power Transformer Windings Insulation",
        "authors": "I.A. Khudonogov, E.Yu. Puzina, A.G. Tuigunova",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867610"
    },
    {
        "id": 23259,
        "title": "Detect Turn-takings in Subtitle Streams with Semantic Recall Transformer Encoder",
        "authors": "Yuhai Liang, Qiang Zhou",
        "published": "2020-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp51396.2020.9310512"
    },
    {
        "id": 23260,
        "title": "A  Foveated Vision-Transformer Model for Scene Classification",
        "authors": "Aditya Jonnalagadda, Miguel Eckstein",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1167/jov.22.14.4440"
    },
    {
        "id": 23261,
        "title": "Transformer Winding Movement differentiated using Frequency Response Analysis",
        "authors": "Rongeet Talukdar",
        "published": "2021-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaect49130.2021.9392474"
    },
    {
        "id": 23262,
        "title": "Machinery Prognostics and High-Dimensional Data Feature Extraction Based on A Transformer Self-Attention Transfer Network",
        "authors": "Shilong Sun, Tengyi Peng, Haodong Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Machinery degradation assessment can offer meaningful prognosis and health management in-formation. Although numerous machine prediction models based on artificial intelligence have emerged in recent years, they still face a series of challenges: (1) Many models continue to rely on manual feature extraction. (2) Deep learning models still struggle with long sequence prediction tasks. (3) Health indicators are inefficient for remaining useful life (RUL) prediction with cross-operational environments when dealing with high-dimensional datasets as inputs. This research proposes a health indicator construction methodology based on a transformer self-attention transfer network (TSTN). This methodology can directly deal with the high-dimensional raw dataset and keep all the information without missing when the signals are taken as the input of the diagnosis and prognosis model.  First, we design an encoder with a long-term and short-term self-attention mechanism to capture crucial time-varying information from a high-dimensional dataset. Second, we propose an estimator that can map the embedding from the encoder output to the estimated degradation trends. Then, we present a domain dis-criminator to extract invariant features from different machine operating conditions. The case studies with the FEMTO-ST bearing dataset and the Monte Carlo method for RUL prediction during the degradation process are conducted. The experiment results fully exhibited the signif-icant advantages of the proposed method compared to other state-of-the-art techniques.",
        "link": "http://dx.doi.org/10.20944/preprints202309.2035.v1"
    },
    {
        "id": 23263,
        "title": "La bataille pour la gouvernance mondiale de l’alimentation et de l’agriculture",
        "authors": "Frédéric Mousseau, Maurice Hérion",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0061"
    },
    {
        "id": 23264,
        "title": "Design of three-stage power electronic transformer for smart grid",
        "authors": "Jiaqi Li",
        "published": "2021-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceemt52412.2021.9602125"
    },
    {
        "id": 23265,
        "title": "Deep Natural Language Processing",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904"
    },
    {
        "id": 23266,
        "title": "A Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "asmaa Elsaid, ammar mohamed, lamiaa Fattouh, mohamed sakre",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2672691/v1"
    },
    {
        "id": 23267,
        "title": "Longitudinal Propagation in a Magnetized Time-Varying Plasma ∗",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-8"
    },
    {
        "id": 23268,
        "title": "Syntactic aperture radar imaging",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00003-3"
    },
    {
        "id": 23269,
        "title": "A Simple Resonant Frequency Tracking Technique for DC Transformer Operation",
        "authors": "Yuqi Wei, Alan Mantooth",
        "published": "2020-11-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid48559.2020.9330634"
    },
    {
        "id": 23270,
        "title": "Microwave-assisted Synthesis of TiO2-based Transformer Nanofluid: Investigation on the perspective of Electrical and Thermal  Properties",
        "authors": "G. Koperundevi, S. Raja",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe study on dispersing nanoparticles in transformer oil to elevate their dielectric properties has evolved significantly in the recent decade. Alongside the conventional dispersion techniques (mechanical stirring & ultrasonic homogenization) in practice, this work experiments with the microwave energy of 2.45 GHz frequency radiated onto the TiO2 nanoparticle-based nanofluid (TNF). The electrical and thermal properties of TNF (TiO2 nanoparticle & surfactant dispersed in transformer mineral oil) are experimentally investigated & presented in this article. Effective synthesis procedure to enhance the dielectric properties with good dispersivity has been recognized from the superlative combination of dispersion techniques. Its effectiveness in enhancing the electrical and thermal properties is investigated by verifying its dielectric breakdown strength, dielectric constant (εr), dielectric dissipation factor (tan δ), and flash & fire point. Results justified that the TNF synthesized by combining the processes of stirring, Ultrasonic homogenization, and microwave irradiation in a rational sequence exhibited better electrical & unaltered thermal properties when compared with samples prepared through stirring and sonication. TNF prepared through microwave synthesis improved the AC breakdown voltage (BDV) by 16.92% more than TNF prepared without microwave synthesis. Hence this could be an effective route to prepare the TNF with improved electrical properties.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1860339/v1"
    },
    {
        "id": 23271,
        "title": "Transformer-Based Object Detection in Drone Images Using Split Attention Module: Pvsamnet",
        "authors": "Sirisha Museboyina, Sudha S.V",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4250755"
    },
    {
        "id": 23272,
        "title": "A fault ride through strategy of multi-ports DC transformer",
        "authors": "Hao Wu",
        "published": "2020-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacre50138.2020.9230334"
    },
    {
        "id": 23273,
        "title": "The Thomson Jumping Ring Experiment and Ideal Transformer",
        "authors": "Chiu-king Ng",
        "published": "2022-5-1",
        "citations": 0,
        "abstract": "In this paper, we utilize the readily known theory of the ideal transformer to furnish a self-contained qualitative explanation on the AC-powered Thomson jumping ring (TJR) experiment.",
        "link": "http://dx.doi.org/10.1119/5.0036490"
    },
    {
        "id": 23274,
        "title": "Parameter Agnostic Stacked Wavelet Transformer for Detecting Singularities",
        "authors": "Akshay Agarwal, Mayank Vatsa, Richa Singh, Nalini Ratha",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4292759"
    },
    {
        "id": 23275,
        "title": "Multimodal Transformer for Bearing Fault Diagnosis: A New Method Based on Frequency-Time Feature Decomposition",
        "authors": "Kai Li, Chen Wang, Haiyan Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBearing fault diagnosis technology enables early detection of faults and prevention of potential equipment failures. To fully utilize signal data, this paper introduces an innovative diagnostic model named Frequency-Time Multimodal Transformer (FTM-Transformer). It constructs two-dimensional frequency feature data and one-dimensional time-domain feature data, and incorporates a multimodal information fusion module comprehensively exploit the distinctive features of different modes of signals, thereby enhancing fault diagnosis accuracy. The FTM-Transformer model utilizes Multivariate Decomposition for Time and Frequency Features (MDTF) as the feature extraction module to analyze vibration signals from multiple sensors. MDTF employs Multivariate Variational Mode Decomposition (MVMD) to decompose the vibration signals into Intrinsic Mode Functions (IMFs) and applies the Discrete Wavelet Transform (DWT) to extract feature maps of these IMFs, while also analyzing statistical time-domain features of the IMFs to form feature vectors. Building upon this foundation, the FTMTransformer module is designed to effectively integrates the frequency-domain feature maps and time-domain feature vectors using a deep Transformer model. Experimental results on the bearing fault dataset from Case Western Reserve University demonstrate that the FTM-Transformer model achieves remarkable diagnostic accuracy, significantly outperforming traditional models such as Vision Transformer (ViT) and ResNet. Furthermore, transfer testing on the gear dataset from Southeast University validates the adaptability of the diagnostic model to other application scenarios, showcasing its superior performance. The proposed MDTF and FTM-Transformer methods offer a novel and effective solution to bearing fault diagnosis, contributing to improved equipment reliability and maintenance efficiency in various industrial applications.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3258385/v1"
    },
    {
        "id": 23276,
        "title": "Aerial Image Object Detection with Vision Transformer Detector (ViTDet)",
        "authors": "Liya Wang, Alex Tien",
        "published": "2023-7-16",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282836"
    },
    {
        "id": 23277,
        "title": "Experience with Current Transformer Calibration System Based on Rogowski Coil",
        "authors": "Esa-Pekka Suomalainen, Jari Hallstrom",
        "published": "2018-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpem.2018.8501048"
    },
    {
        "id": 23278,
        "title": "Transformer-Based Weakly Supervised 3d Human Pose Estimation",
        "authors": "Xiaoguang Wu, Hujie Xie, Xiaochen Niu, Chen Wang, Zelei Wang, Shiwen Zhang, Yuze Shan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4611139"
    },
    {
        "id": 23279,
        "title": "TA-UNet3+: a transformer-based method for kidney tumor segmentation",
        "authors": "Xiqing Hu",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2678615"
    },
    {
        "id": 23280,
        "title": "Empowering MBTI Personality Classification through Transformer-Based Summarization Model",
        "authors": "Seif Elmoushy, Mostafa Saeed, Wael H. Gomaa",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217442"
    },
    {
        "id": 23281,
        "title": "IAFormer: A Transformer Network for Image Aesthetic Evaluation and Cropping",
        "authors": "Lei Wang, Yue Jin",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acait56212.2022.10137804"
    },
    {
        "id": 23282,
        "title": "Instance Segmentation Combined CMT and Swin Transformer in Driving Scenes",
        "authors": "Zhengyi Zha",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135453"
    },
    {
        "id": 23283,
        "title": "Deforestation Detection in the Brazilian Amazon Using Transformer-based Networks",
        "authors": "Mariam Alshehri, Anes Ouadou, Grant Scott",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00130"
    },
    {
        "id": 23284,
        "title": "Front Matter",
        "authors": "",
        "published": "2021-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119800194.fmatter"
    },
    {
        "id": 23285,
        "title": "Synchronous Reference Frame Control of Transformer Based DC mA Current Sensor",
        "authors": "Muhammed Calar, Korhan Kayisli",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsmartgrid58556.2023.10170943"
    },
    {
        "id": 23286,
        "title": "Piano automatic transcription based on transformer",
        "authors": "Yuan Wang",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "Recent years, research on automatic music transcription has made significant progress as deep learning techniques have been validated to demonstrate strong performance in complex data applications. Although the existing work is exciting, they all rely on specific domain knowledge to enable the design of model architectures and training modes for different tasks. At the same time, the noise generated in the process of automatic music transcription data collection cannot be ignored, which makes the existing work unsatisfactory. To address the issues highlighted above, we propose an end-to-end framework based on Transformer. Through the encoder-decoder structure, we realize the direct conversion of the spectrogram of the collected piano audio to MIDI output. Further, to remove the impression of environmental noise on transcription quality, we design a training mechanism mixed with white noise to improve the robustness of our proposed model. Our experiments on the classic piano transcription datasets show that the proposed method can greatly improve the quality of automatic music transcription.",
        "link": "http://dx.doi.org/10.3233/jifs-233653"
    },
    {
        "id": 23287,
        "title": "Mdtnet: Partial Transformer with Degradation-Aware Module for Restoring Old Photos with Multiple Degradations",
        "authors": "Yuan Zhao, Cao Liqin, Fan Zhang, Yanfei Zhong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670346"
    },
    {
        "id": 23288,
        "title": "High-Dimensional Population Flow Time Series Forecasting Via an Interpretable Hierarchical Transformer",
        "authors": "SONGHUA HU, Chenfeng Xiong",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4049754"
    },
    {
        "id": 23289,
        "title": "Towards an Effective and Efficient Transformer for Rain-by-Snow Weather Removal",
        "authors": "Tao Gao, Yuanbo Wen, Kaihao Zhang, Peng Cheng, Ting Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458244"
    },
    {
        "id": 23290,
        "title": "HLT: A Hierarchical Vulnerability Detection Model Based on Transformer",
        "authors": "Yupan Chen, Zhihong Liu",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdis55630.2022.00015"
    },
    {
        "id": 23291,
        "title": "Broadband Frequency Domain Dielectric Spectroscopy of Cellulose Insulation Material in Transformer",
        "authors": "Shengkang Wang, Fuchang Lin, Hua Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177292"
    },
    {
        "id": 23292,
        "title": "Transformer Scale Gate for Semantic Segmentation",
        "authors": "Hengcan Shi, Munawar Hayat, Jianfei Cai",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00298"
    },
    {
        "id": 23293,
        "title": "Enhancing Remote Sensing Semantic Segmentation Through Hybrid Convolutional Neural Network and Transformer",
        "authors": "Zheng Kang, Yu Chen, Jingrong Wang, Jiao Zhan, Nan Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4696209"
    },
    {
        "id": 23294,
        "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
        "authors": "Xiyuan Wang, Fangyuan Wang, Bo Xu, Liang Xu, Jing Xiao",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-697"
    },
    {
        "id": 23295,
        "title": "Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation",
        "authors": "Chang Han, Weiping Tu, Yuhong Yang, Jingyi Li, Xinhong Li",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10077"
    },
    {
        "id": 23296,
        "title": "Operational Performance Evaluation of Phase Shifting Transformer and Unified Power Flow Controller in Power System",
        "authors": "",
        "published": "2020-7-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37896/sr7.7/055"
    },
    {
        "id": 23297,
        "title": "TSNet: Token Sparsification for Efficient Video Transformer",
        "authors": "Hao Wang, Wenjia Zhang, Guohua Liu",
        "published": "2023-9-24",
        "citations": 1,
        "abstract": "In the domain of video recognition, video transformers have demonstrated remarkable performance, albeit at significant computational cost. This paper introduces TSNet, an innovative approach for dynamically selecting informative tokens from given video samples. The proposed method involves a lightweight prediction module that assigns importance scores to each token in the video. Tokens with top scores are then utilized for self-attention computation. We apply the Gumbel-softmax technique to sample from the output of the prediction module, enabling end-to-end optimization of the prediction module. We aim to extend our method on hierarchical vision transformers rather than single-scale vision transformers. We use a simple linear module to project the pruned tokens, and the projected result is then concatenated with the output of the self-attention network to maintain the same number of tokens while capturing interactions with the selected tokens. Since feedforward networks (FFNs) contribute significant computation, we also propose linear projection for the pruned tokens to accelerate the model, and the existing FFN layer progresses the selected tokens. Finally, in order to ensure that the structure of the output remains unchanged, the two groups of tokens are reassembled based on their spatial positions in the original feature map. The experiments conducted primarily focus on the Kinetics-400 dataset using UniFormer, a hierarchical video transformer backbone that incorporates convolution in its self-attention block. Our model demonstrates comparable results to the original model while reducing computation by over 13%. Notably, by hierarchically pruning 70% of input tokens, our approach significantly decreases 55.5% of the FLOPs, while the decline in accuracy is confined to 2%. Additional testing of wide applicability and adaptability with other transformers such as the Video Swin Transformer was also performed and indicated its progressive potentials in video recognition benchmarks. By implementing our token sparsification framework, video vision transformers can achieve a remarkable balance between enhanced computational speed and a slight reduction in accuracy.",
        "link": "http://dx.doi.org/10.3390/app131910633"
    },
    {
        "id": 23298,
        "title": "Underwater Target Detection Algorithm Based on YOLO and Swin Transformer for Sonar Images",
        "authors": "Ruoyu Chen, Shuyue Zhan, Ying Chen",
        "published": "2022-10-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9976986"
    },
    {
        "id": 23299,
        "title": "INTRUSION DETECTION MODEL BASED ON IMPROVED TRANSFORMER",
        "authors": "Svitlana Gavrylenko, Vadym Poltoratskyi, Alina Nechyporenko",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "The object of the study is the process of identifying the state of a computer network. The subject of the study are the methods of identifying the state of computer networks. The purpose of the paper is to improve the efficacy of intrusion detection in computer networks by developing a method based on transformer models. The results obtained. The work analyzes traditional machine learning algorithms, deep learning methods and considers the advantages of using transformer models. A method for detecting intrusions in computer networks is proposed. This method differs from known approaches by utilizing the Vision Transformer for Small-size Datasets (ViTSD) deep learning algorithm. The method incorporates procedures to reduce the correlation of input data and transform data into a specific format required for model operations. The developed methods are implemented using Python and the GOOGLE COLAB cloud service with Jupyter Notebook. Conclusions. Experiments confirmed the efficiency of the proposed method. The use of the developed method based on the ViTSD algorithm and the data preprocessing procedure increases the model's accuracy to 98.7%. This makes it possible to recommend it for practical use, in order to improve the accuracy of identifying the state of a computer system.",
        "link": "http://dx.doi.org/10.20998/2522-9052.2024.1.12"
    },
    {
        "id": 23300,
        "title": "Multistep retrosynthesis combining a disconnection aware triple transformer loop with a route penalty score guided tree search",
        "authors": "David Kreutter, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "Computer-aided synthesis planning (CASP) aims to automatically learn organic reactivity from literature and perform retrosynthesis of unseen molecules. CASP systems must learn reactions sufficiently precisely to propose realistic disconnections while avoiding overfitting to leave room for diverse options, and explore possible routes such as to allow short synthetic sequences to emerge. Herein we report an open-source CASP tool proposing original solutions to both challenges. First, we use a triple transformer loop (TTL) predicting starting materials (T1), reagents (T2), and products (T3) to explore various disconnections sites defined by combining exhaustive, template-based and transformer-based tagging procedures. Second, we integrate TTL into a multistep tree search algorithm (TTLA) prioritizing sequences using a route penalty score (RPScore) considering the number of steps, their confidence score, and the simplicity of all intermediates along the route. Our approach favours short synthetic routes to commercial starting materials, as exemplified by retrosynthetic analyses of recently approved drugs.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-8khth-v2"
    },
    {
        "id": 23301,
        "title": "Transformer Differential Protection",
        "authors": "Carlos P. Pacheco Cárdenas, Edison A. González González, Flavio A. Quizhpi Palomeque",
        "published": "2023-1-24",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iconat57137.2023.10079990"
    },
    {
        "id": 23302,
        "title": "Video Scene Searching using Modified Bi-Modal Transformer",
        "authors": "Siddhant Raje, Aparna Sarawadekar, Prajna Nayak, Amiya Kumar Tripathy",
        "published": "2022-7-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp54529.2022.9864519"
    },
    {
        "id": 23303,
        "title": "GaitTransformer: Multiple-Temporal-Scale Transformer for Cross-View Gait Recognition",
        "authors": "Yufeng Cui, Yimei Kang",
        "published": "2022-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859928"
    },
    {
        "id": 23304,
        "title": "Machine learning applications in estimating transformer loss of life",
        "authors": "Alireza Majzoobi, Mohsen Mahoor, Amin Khodaei",
        "published": "2017-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274564"
    },
    {
        "id": 23305,
        "title": "Transformer Protection Algorithm Based on S-Transform",
        "authors": "Kubra Nur Akpinar, Okan Ozgonenel, Unal Kurt",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302473"
    },
    {
        "id": 23306,
        "title": "The Method to Determine the Turns Ratio Correction of the Inductive Current Transformer",
        "authors": "Ernest Stano",
        "published": "2021-12-20",
        "citations": 11,
        "abstract": "This paper presents the method for evaluation of the turns ratio correction of the inductive current transformer using the magnetization curves determined at the non-load state and in the load conditions. The presented method may be applied to determine even a fractional winding correction factor. The standard IEC 61869-2 provides the method to determine the turns ratio correction of the tested CT from the measured rms values of voltages on its primary and secondary winding in the non-load state. However, this approach is limited in determining the significant changes in the number of turns of the secondary winding. Moreover, the paper presents the influence of the applied turns ratio correction on the frequency characteristics of the current error and phase displacement of the inductive current transformers evaluated for the transformation of the distorted current.",
        "link": "http://dx.doi.org/10.3390/en14248602"
    },
    {
        "id": 23307,
        "title": "Vision Transformer-Based Multi-Phase Accident Detection and False Positive Mitigation",
        "authors": "Uday Jartarghar, Devaj Sanghvi, Mehwish Nidgundi, Kanhaiya Kumar, Sneha Varur",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe use of computer vision algorithms for real-time accident detection is a highly researched method to minimize delays in post-crash care. Various methods are adopted in the literature; However, based on our understanding, these methods are not sufficiently reliable in accurately detecting accidents, and there is a high occurrence of false accident detections. This paper aims to achieve precise accident detection while minimizing false positive detections. Our approach involves a four-phase framework that integrates vehicle detection and continuous tracking techniques, specifically utilizing the You Only Look Once (YOLO) and ByteTrack algorithms. Subsequently, we have developed a criterion for identifying abrupt change involving checking for overlap between vehicles and angle of collision, a potential accident indicator. During our third phase, we precisely determine the location of the accident within the sudden alteration frames. The fourth pivotal phase stands out, leveraging Vision Transformer (ViT), an encoder only model, to carefully eliminate fake accidents. Our methodology surpasses the typical use of Convolutional Neural Network (CNN)-based approaches by demonstrating a comprehensive integration of several deep learning techniques. The framework was evaluated on Real-World Surveillance videos with diverse conditions; The performance of proposed framework was found effective, outperforming the existing works.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3903862/v1"
    },
    {
        "id": 23308,
        "title": "On-chip transformer design and application to RF and mm-wave front-ends",
        "authors": "John R. Long",
        "published": "2017-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cicc.2017.7993713"
    },
    {
        "id": 23309,
        "title": "Enhancing Building Change Detection with UVT-BCD: A UNet-Vision Transformer Fusion Approach",
        "authors": "T S Geetha, C Chellaswamy, T Kali Raja",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBuilding change detection (BCD) is particularly important for comprehending ground changes and activities carried out by humans. Since its introduction, deep learning has emerged as the dominant method for BCD. Despite this, the detection accuracy continues to be inadequate because of the constraints imposed by feature extraction requirements. Consequently, the purpose of this study is to present a feature enhancement network that combines a UNet encoder and a vision transformer (UVT) structure in order to identify BCD (UVT-BCD). A deep convolutional network and a section of the vision transformer structure are combined in this model. The result is a strong feature extraction capability that can be used for a wide variety of building types. To improve the ability of small-scale structures to be detected, you should design an attention mechanism that takes into consideration both the spatial and channel dimensions. A cross-channel context semantic aggregation module is used to carry out information aggregation in the channel dimension. Experiments have been conducted in numerous cases using two different BCD datasets to evaluate the performance of the previously suggested model. The findings reveal that UVT-BCD outperforms existing approaches, achieving improvements of 5.95% in overall accuracy, 5.33% in per-class accuracy, and 8.28% in the Cohen's Kappa statistic for the LEVIR-CD dataset. Furthermore, it demonstrates enhancements of 6.05% and 6.4% in overall accuracy, 6.56% and 5.89% in per-class accuracy, and 6.71% and 6.23% in the Cohen's Kappa statistic for the WHU-CD dataset.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4004190/v1"
    },
    {
        "id": 23310,
        "title": "Empathetic Robot With Transformer-Based Dialogue Agent",
        "authors": "Baijun Xie, Chung Hyuk Park",
        "published": "2021-7-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ur52253.2021.9494669"
    },
    {
        "id": 23311,
        "title": "Chapter 6: The BERT Family in Greater Depth",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-007"
    },
    {
        "id": 23312,
        "title": "De Novo Atomic Protein Structure Modeling for Cryo-EM Density Maps Using 3D Transformer and Hidden Markov Model",
        "authors": "Nabin Giri, Jianlin Cheng",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractAccurately building three-dimensional (3D) atomic structures from 3D cryo-electron microscopy (cryo-EM) density maps is a crucial step in the cryo-EM-based determination of the structures of protein complexes. Despite improvements in the resolution of 3D cryo-EM density maps, the de novo conversion of density maps into 3D atomic structures for protein complexes that do not have accurate homologous or predicted structures to be used as templates remains a significant challenge. Here, we introduce Cryo2Struct, a fully automated ab initio cryo-EM structure modeling method that utilizes a 3D transformer to identify atoms and amino acid types in cryo-EM density maps first, and then employs a novel Hidden Markov Model (HMM) to connect predicted atoms to build backbone structures of proteins. Tested on a standard test dataset of 128 cryo-EM density maps with varying resolutions (2.1 - 5.6 Å) and different numbers of residues (730 - 8,416), Cryo2Struct built substantially more accurate and complete protein structural models than the widely used ab initio method - Phenix in terms of multiple evaluation metrics. Moreover, on a new test dataset of 500 recently released density maps with varying resolutions (1.9 - 4.0 Å) and different numbers of residues (234 - 8,828), it built more accurate models than on the standard dataset. And its performance is rather robust against the change of the resolution of density maps and the size of protein structures.",
        "link": "http://dx.doi.org/10.1101/2024.01.02.573943"
    },
    {
        "id": 23313,
        "title": "DT-F Transformer: Dual transpose fusion transformer for polarization image fusion",
        "authors": "Jinyang Liu, Shutao Li, Renwei Dian, Ze Song",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.inffus.2024.102274"
    },
    {
        "id": 23314,
        "title": "Determination of the Active Electrical Energy Losses  in the Transformer on the Basis of Temperature Monitoring of the Transformer Surfaces",
        "authors": "Sergey Kostinsky,  , Christina Vasileva, Vladimir Mihaylov,  ,  ",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17213/0136-3360-2017-4-69-75"
    },
    {
        "id": 23315,
        "title": "融合CNN与Transformer结构的遥感图像分类方法",
        "authors": "金传 Jin Chuan, 童常青 Tong Changqing",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/lop223154"
    },
    {
        "id": 23316,
        "title": "Transformer Fuse Sizing - The NEC is not the Last Word",
        "authors": "Del John Ventruella",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppic.2018.8502224"
    },
    {
        "id": 23317,
        "title": "Review of power transformer fault diagnosis",
        "authors": "Enyuan Shi, Yifa Sheng",
        "published": "2022-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2660323"
    },
    {
        "id": 23318,
        "title": "Fault Diagnosis of Oil-Immersed Power Transformer: A Case Study",
        "authors": "Naris Chattranont, Nattachote Rugthaicharoencheep",
        "published": "2021-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpei52436.2021.9690666"
    },
    {
        "id": 23319,
        "title": "Vision Transformer and Attention-Based Melanoma Disease Classification",
        "authors": "Prakhar Shobhit, Niraj Kumar",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/c2i659362.2023.10430697"
    },
    {
        "id": 23320,
        "title": "Deep Features Fusion with Mutual Attention Transformer for Skin Lesion Diagnosis",
        "authors": "Li Zhou, Yan Luo",
        "published": "2021-9-19",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506211"
    },
    {
        "id": 23321,
        "title": "MCTE: Marrying Convolution and Transformer Efficiently for End-to-End Medical Image Segmentation",
        "authors": "Jiuqiang Li",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222041"
    },
    {
        "id": 23322,
        "title": "Continuously Masked Transformer for Image Inpainting",
        "authors": "Keunsoo Ko, Chang-Su Kim",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01211"
    },
    {
        "id": 23323,
        "title": "Mitigating Data Imbalance Problem in Transformer-Based Intent Detection",
        "authors": "Osman BÜYÜK, Mustafa ERDEN, Levent ARSLAN",
        "published": "2022-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31590/ejosat.1044812"
    },
    {
        "id": 23324,
        "title": "A Hybrid Voltage Regulation Transformer Based on Interline Power Converters",
        "authors": "Yafeng Wang, Tiefu Zhao",
        "published": "2022-3-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43599.2022.9773686"
    },
    {
        "id": 23325,
        "title": "A transformer-based deep learning framework to predict employee attrition",
        "authors": "Wenhui Li",
        "published": "2023-9-27",
        "citations": 0,
        "abstract": "In all areas of business, employee attrition has a detrimental impact on the accuracy of profit management. With modern advanced computing technology, it is possible to construct a model for predicting employee attrition to minimize business owners’ costs. Despite the reality that these types of models have never been evaluated under real-world conditions, several implementations were developed and applied to the IBM HR Employee Attrition dataset to evaluate how these models may be incorporated into a decision support system and their effect on strategic decisions. In this study, a Transformer-based neural network was implemented and was characterized by contextual embeddings adapting to tubular data as a computational technique for determining employee turnover. Experimental outcomes showed that this model had significantly improved prediction efficiency compared to other state-of-the-art models. In addition, this study pointed out that deep learning, in general, and Transformer-based networks, in particular, are promising for dealing with tabular and unbalanced data.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1570"
    },
    {
        "id": 23326,
        "title": "Investigation of text data augmentation for transformer training via translation technique",
        "authors": "Dominykas Šeputis",
        "published": "2021-5-14",
        "citations": 0,
        "abstract": "Data augmentation can improve model’s final accuracy by introducing new data samples to the dataset. In this paper, text data augmentation using translation technique is investigated. Synthetic translations, generated by Opus-MT model are compared to the unique foreign data samples in terms of an impact to the trans- former network-based models’ performance. The experimental results showed that multilingual models like DistilBERT in some cases benefit from the introduction of the addition artificially created data samples presented in a foreign language.",
        "link": "http://dx.doi.org/10.15388/lmitt.2021.11"
    },
    {
        "id": 23327,
        "title": "Underwater image enhancement using lightweight vision transformer",
        "authors": "Muneeba Daud, Hammad Afzal, Khawir Mahmood",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18550-z"
    },
    {
        "id": 23328,
        "title": "Chinese text sentiment analysis based on transformer model",
        "authors": "Huang Jing, Cai Yang",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwecai55315.2022.00043"
    },
    {
        "id": 23329,
        "title": "Precipitation Forecasting Using Transformer: A Comparative Study with Unet",
        "authors": "Sining Ang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "Accurate precipitation prediction has a huge socio-economic impact. With the development of algorithms, architectures, and hardware technologies in the field of machine learning, the disadvantages of traditional numerical weather prediction methods are becoming more and more obvious. Noting that related studies have attempted to use the latest machine learning architectures for precipitation prediction, this work uses Unet and Transformer for precipitation prediction based on cloud layer information and compares them. The results indicate that Transformer can achieve 94.13% accuracy, while Unet has 96.89%. Finally, in conducting the data comparison, a conjecture related to the lagging of Transformer data is proposed based on the results of this experiment.",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6613"
    },
    {
        "id": 23330,
        "title": "Design of an equipotential shielding 1000 kV capacitor voltage transformer",
        "authors": "",
        "published": "2019-8-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2018.01190"
    },
    {
        "id": 23331,
        "title": "Transformer electric field calculation using BEM and FEM",
        "authors": "Ana Drandić, Bojan Trkulja",
        "published": "2017",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.719"
    },
    {
        "id": 23332,
        "title": "Joint Microseismic Event Detection and Location Based on a Detection Transformer",
        "authors": "Y. Yang, C. Birnie, T. Alkhalifah",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310096"
    },
    {
        "id": 23333,
        "title": "Partially Trained Music Generation based on Transformer",
        "authors": "Jiandong Tang, Lanqing Yin, Jinming Yu",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscss57650.2023.10169184"
    },
    {
        "id": 23334,
        "title": "ABTrans: A Transformer-based model for predicting interaction between anti-Aß antibodies and peptides",
        "authors": "Yuhong Su, Lingfeng Zhang, Yangjing Wang, Buyong Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractUnderstanding the recognition of antibodies and Aβ peptide is crucial for the development of more effective therapeutic agents. Here we studied the interaction between anti-Aβ antibodies and different peptides by building a deep learning model, using the dodecapeptide sequences elucidated from phage display and known anti-Aβ antibody sequences collected from public sources. Our multi-classification model, ABTrans was trained to determine the four levels of binding ability between anti-Aβ antibody and dodecapeptide: not binding, weak binding, medium binding, and strong binding. The accuracy of our model reached 0.8278. Using the ABTrans, we examined the cross-reaction of anti-Aβ antibodies with other human amyloidogenic proteins, and we found that Aducanumab and Donanemab have the least cross-reactions with other human amyloidogenic proteins. We also systematically screened all human proteins interaction with eleven selected anti-Aβ antibodies to identify possible peptide fragments that could be an off-target candidate.Key PointsABTrans is a Transformer-based model that was developed for the first time to predict the interaction between anti-Aß antibodies and peptides.ABTrans was trained using a dataset with 1.5 million peptides and 110 anti-Aβ antibodies.ABTrans achieved an accuracy of 0.8278 and is capable of determining the four levels of binding ability between antibody and Aß: not binding, weak binding, medium binding, and strong binding.ABTrans has potential applications in predicting off-target and cross-reactivity effects of antibodies and in designing new anti-Aß antibodies.",
        "link": "http://dx.doi.org/10.1101/2023.10.09.561503"
    },
    {
        "id": 23335,
        "title": "Research on Fault Diagnosis for Power Transformer Based on HHGA-WNN",
        "authors": "Hailiang Yang",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec.2018.8740608"
    },
    {
        "id": 23336,
        "title": "Multi-scale filter-enhanced transformer for sequential recommendation",
        "authors": "Xiyu Cui",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Recent research has shown that self-attention mechanisms improve Sequential Recommender Systems (SRS) by capturing sequential associations with the interactions. Nevertheless, existing work still needs to address two critical limitations. Firstly, the behavior of users in the original sequences contains various preference signals that are implicit and noisy and hard to reflect the users intentions fully. As a result, it would deteriorate the representation of their true intentions to model all interactions. Secondly, most models only model single-scale interaction sequences and ignore the multi-scale feature relationships of the sequences. In order to address these limitations, the paper proposes MFTSRec (Multi-scale Filter Enhanced Transformer Sequential Recommender), which can weaken those interactions irrelevant to the users intentions from their implicit feedback and adaptively focus on the users multi-scale intentions. Besides, this paper also does extensive experiments on four benchmark datasets and further demonstrates the effectiveness and robustness of MFTSRec compared to the state-of-the-art model.",
        "link": "http://dx.doi.org/10.54254/2755-2721/13/20230723"
    },
    {
        "id": 23337,
        "title": "Correlation analysis of hotspot temperatures in power transformer – a case study",
        "authors": "Michal Kunicki",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2019.8778179"
    },
    {
        "id": 23338,
        "title": "An Automatic Transformer from Sequential to Parallel Java Code",
        "authors": "Alessandro Midolo, Emiliano Tramontana",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "Sequential programs can benefit from parallel execution to improve their performance. When developing a parallel application, several techniques are employed to achieve the desired behavior: identifying parts that can run in parallel, synchronizing access to shared data, tuning performance, etc. Admittedly, manually transforming a sequential application to make it parallel can be tedious due to the large number of lines of code to inspect, the possibility of errors arising from inaccurate data dependence analysis leading to unpredictable behavior, and inefficiencies when the workload between parallel threads is unbalanced. This paper proposes an automatic approach that analyzes Java source code to identify method calls that are suitable for parallel execution and transforms them so that they run in another thread. The approach is based on data dependence and control dependence analyses to determine the execution flow and data accessed. Based on the proposed method, a tool has been developed to enhance applications by incorporating parallelism, i.e., transforming suitable method calls to execute on parallel threads, and synchronizing data access where needed. The developed tool has been extensively tested to verify the accuracy of its analysis in finding parallel execution opportunities, the correctness of the source code alterations, and the resultant performance gain.",
        "link": "http://dx.doi.org/10.3390/fi15090306"
    },
    {
        "id": 23339,
        "title": "Traditional Village Classification Model Based on Transformer Network",
        "authors": "Qi Zhong",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "The study of traditional villages holds significant implications in cultural, historical, and societal contexts. Despite the considerable research focus on the architectural styles of Qiang, Tibetan, Han, and Hui ethnic villages due to their distinctiveness, rapidly and accurately identifying the types of traditional villages in practical surveys remains a challenge. To address this issue, this paper establishes an aerial image dataset for Qiang, Tibetan, Han, and Hui ethnic villages and introduces a specialized feature extraction network, Transformer-Village, designed for the classification and detection of traditional villages using deep learning algorithms. The overall structure of the network is lightweight, incorporating condconv dynamic convolution as the core layer structure; furthermore, a spatial self-attention-related feature extraction network is designed based on Transformer. In conclusion, through simulated experiments, Transformer-Village coupled with the YOLO detector achieves a 97.2% mAP on the test set, demonstrating superior detection accuracy compared to other baseline models. Overall, the experimental results suggest that this work is feasible and practical.",
        "link": "http://dx.doi.org/10.22158/asir.v7n4p126"
    },
    {
        "id": 23340,
        "title": "AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese",
        "authors": "Abhijnan Nath, Sheikh Mannan, Nikhil Krishnaswamy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.739"
    },
    {
        "id": 23341,
        "title": "Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation",
        "authors": "Haoyi Wu, Kewei Tu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.482"
    },
    {
        "id": 23342,
        "title": "Wheat Full-Width Harvesting Navigation Line Extraction Method Using Improved Swin-Transformer",
        "authors": "anon anon, Chengqian Jin, Man Chen, Zeyu Cai, Zheng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4576704"
    },
    {
        "id": 23343,
        "title": "Integrated Modeling for In-Depth EEG Based Emotional State Analysis via Convolutional-Transformer Fusion",
        "authors": "Noman Ali, A. S Kang, Saurabh Himral",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study introduces a hybrid model designed for the predic- tion of emotional states derived from electroencephalogram (EEG) data, employing an amalgamation of convolutional and trans- former layers. The architectural framework of the model is metic- ulously structured to facilitate concurrent assimilation of local pat- terns and long-range dependencies inherent in EEG data, thereby augmenting its discernment of nuanced emotional experiences. The investigation presented herein undertakes a comprehensive explo- ration of the fusion technique, with a primary focus on dis- cerning three elemental emotional dimensions: Arousal, Valence, and Dominance, in addition to their concurrent combinations. The research methodology encompasses an in-depth evaluation of the model’s performance across these diverse emotional states, encompassing the intricate task of simultaneous Valence-Arousal (VA) prediction. Furthermore, the study extends its purview to encompass the intricate Valence-Arousal-Dominance (VAD) space, thereby providing a thorough analysis of the model’s efficacy. To articulate the model’s discriminative efficacy, this study meticu- lously presents the detailed F1 scores corresponding to each emotional state classification: Arousal (96.8), Valence (97.3), Valence-Arousal (VA) simultaneously (95.6), and Valence-Arousal-Dominance simultaneously (94.9). These scores serve as a testament to the model’s robust per- formance across diverse emotional dimensions. Importantly, to fortify the credibility of our findings, rigorous experimentation has been con- ducted on the DEAP dataset, unveiling noteworthy results even in scenarios involving simultaneous recognition of multiple emotional states.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3739841/v1"
    },
    {
        "id": 23344,
        "title": "Simulasi Monitoring Pergeseran Tanah Menggunakan Sensor LVDT (Linear Variable Differential Transformer)",
        "authors": "Wilson Jefriyanto",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "Pergeseran tanah merupakan salah satu faktor terjadinya tanah longsor (landslide). Untuk itu, diperlukan suatu prototype alat yang dapat  membantu memberikan informasi dini dan peringatan akan terjadinya longsor. Telah dilakukan penelitian tentang simulasi pergeseran tanah dengan menggunakan sensor LVDT (Linear Variable Differential Transformer). Sensor LVDT dihubungkan dengan Arduino uno yang kemudian ditampilkan pada LCD dan juga monitoring pada komputer. Pengamatan pergeseran tanah  dilakukan selama 5 jam dengan melakukan variasi terhadap sudut kemiringan tanah. Tujuan dari penelitian ini yaitu untuk menguji kerja dari sensor LVDT yang merupakan hasil dari penelitian sebelumnya. Dari hasil pengamatan, terlihat bahwa sensor LVDT ini mampu membaca pergeseran tanah dengan ketlitian 0.01 mm. Adapun jangkauan pergeseran alat ini yaitu hany sampai 14 cm, untuk penelitian selanjutnya akan dikembankan sensor LVDT dengan jangkaun pengukuran yang lebih panjang.",
        "link": "http://dx.doi.org/10.31605/saintifik.v7i1.281"
    },
    {
        "id": 23345,
        "title": "Triplet Vision Transformer for Word Image Retrieval in Historical Arabic Documents",
        "authors": "Abir Fathallah, Mounim A.  A. El Yacoubi, Najoua  Essoukri Ben Amara",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4577874"
    },
    {
        "id": 23346,
        "title": "Application of Blind-Source-Separation algorithm for investigating transformer vibration patterns",
        "authors": "Muhammad Mueer, Lakshitha Naranpanawe, Chandima Ekanayake",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535622"
    },
    {
        "id": 23347,
        "title": "Transformer Based Memory Network for Video Anomaly Detection",
        "authors": "zhiqiang wang, xingsheng gu, Xiaojing Gu, Jingyu Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4219648"
    },
    {
        "id": 23348,
        "title": "Partial Discharge Detection of Transformer Winding",
        "authors": "Jiahao Zou",
        "published": "2021-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3495018.3495461"
    },
    {
        "id": 23349,
        "title": "Weakly-Supervised Self-Ensembling Vision Transformer for MRI Cardiac Segmentation",
        "authors": "Ziyang Wang, Haodong Zhang, Yang Liu",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00051"
    },
    {
        "id": 23350,
        "title": "SIMULINK MODEL OF POWER TRANSFORMER PROTECTION USING DIFFERENTIAL PROTECTION SCHEME",
        "authors": "",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets33157"
    },
    {
        "id": 23351,
        "title": "Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold",
        "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191125"
    },
    {
        "id": 23352,
        "title": "A Resonant Approach to Transformer Loss Characterization",
        "authors": "Michael Solomentsev, Odinaka Okeke, Alex J. Hanson",
        "published": "2022-3-20",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43599.2022.9773516"
    },
    {
        "id": 23353,
        "title": "Dual-Source Transformer Model for Neural Machine Translation with Linguistic Knowledge",
        "authors": "Yirong Pan, Xiao Li, Yating Yang, Rui Dong",
        "published": "No Date",
        "citations": 3,
        "abstract": "Incorporating source-side linguistic knowledge into the neural machine translation (NMT) model has recently achieved impressive performance on machine translation tasks. One popular method is to generalize the word embedding layer of the encoder to encode each word and its linguistic features. The other method is to change the architecture of the encoder to encode syntactic information. However, the former cannot explicitly balance the contribution from the word and its linguistic features. The latter cannot flexibly utilize various types of linguistic information. Focusing on the above issues, this paper proposes a novel NMT approach that models the words in parallel to the linguistic knowledge by using two separate encoders. Compared with the single encoder based NMT model, the proposed approach additionally employs the knowledge-based encoder to specially encode linguistic features. Moreover, it shares parameters across encoders to enhance the model representation ability of the source-side language. Extensive experiments show that the approach achieves significant improvements of up to 2.4 and 1.1 BLEU points on Turkish→English and English→Turkish machine translation tasks, respectively, which indicates that it is capable of better utilizing the external linguistic knowledge and effective improving the machine translation quality.",
        "link": "http://dx.doi.org/10.20944/preprints202002.0273.v1"
    },
    {
        "id": 23354,
        "title": "Optimization of vision transformer-based detection of COVID-19 from chest X-ray images",
        "authors": "Jinsol Ko, Soyeon Park, Hyun Goo Woo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: For diagnosing coronavirus disease 2019 (COVID-19), chest X-rays have emerged as a preferred modality because of their accessibility, affordability, and capability to identify various pathologies. Recent advances in deep learning algorithms have shown promise in distinguishing COVID-19 from other lung diseases. However, the use of different optimization methods can affect the performance of the deep learning models. We aimed to compare the effects of the different optimization methods, identifying the best-performing algorithms for the detection of COVID-19 using chest X-rays.\nMethods: Chest X-ray images, including the seven classes of Normal, COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis, were obtained. We trained the Vision Transformer (ViT) model using different optimizers such as Adaptive Moment Estimation (Adam), AdamW, Nesterov accelerated Adam (NAdam), Rectified Adam (RAdam), Stochastic Gradient Descent with weight decay (SGDW), and Momentum, and compared their performances.\nResults: We found that the RAdam optimizer at a learning rate of 10-5 achieved the highest accuracy, highest weighted average of F1-score, and lowest false negative rate of COVID-19 for both 4 Class and 7 Class Dataset. On the other hand, AdamW showed better performance for the samples with small sample sizes. The optimizers derived from Adam (i.e. Adam, AdamW, NAdam, and RAdam), showed robust results against different learning rates, while SGDW and Momentum showed less significant robustness.\nConclusions: We suggest that Adam-derived optimizers, particularly RAdam, showed best performance in training the ViT model for detecting COVID-19 using chest X-ray images. Our results may help in the efforts to improve the performance of the model and to make it clinically useful.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3284664/v1"
    },
    {
        "id": 23355,
        "title": "Collaborative Transformer-CNN Learning for Semi-supervised Medical Image Segmentation",
        "authors": "Wei Li, Huihua Yang",
        "published": "2022-12-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995501"
    },
    {
        "id": 23356,
        "title": "Research on Knowledge Reasoning in Transformer Partial Discharge Fault Diagnosis",
        "authors": "Yu Wang, Yuxin Wang, Jinsha Yuan, Yin Liu",
        "published": "2020-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327364"
    },
    {
        "id": 23357,
        "title": "Transformer Compressed Sensing Via Global Image Tokens",
        "authors": "Marlon Bran Lorenzana, Craig Engstrom, Shekhar S. Chandra",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897630"
    },
    {
        "id": 23358,
        "title": "On-Line Monitoring Technology of Transformer Oil Level Based on Ultrasonic Sensor",
        "authors": "Yongcan Zhu, Dongyang Liu, Ruiwen Zhou, Yi Tian, Xinbo Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374266"
    },
    {
        "id": 23359,
        "title": "Knowledge-guided Transformer for Joint Topic and Sentiment Analysis of Chinese Classical Poetry",
        "authors": "Bin Wu, Yuting Wei, Yangfu Zhu, Linmei Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe analyses of the topic and sentiment are essential for understanding Chinese classical poetry and historical culture. Existing works fail to consider the lexical knowledge mined from poem annotations, which partly contains some information about the topic and sentiment. In addition, most works ignore the interdependence and diversity of the topic and sentiment in one poem. Hence, in this paper, we propose a Knowledge-guided Transformer Model (KTM) for joint multiple topic and sentiment analysis of Chinese classical poetry. Specifically, we first respectively construct two lexical dictionaries for the topic and sentiment based on the poem annotations. Then we take full advantage of the lexical dictionaries with a knowledge-based mask-transformer to represent poems. Furthermore, considering the correlations between the topic and sentiment, our model jointly classifies the multiple topics and sentiments in Chinese classical poetry by stacking the two subtasks. Considering there is no public dataset, we release a new Chinese classical poetry dataset CCPD for joint multiple topic and sentiment analysis. Extensive experiments demonstrate that our model achieves state-of-the-art performance on both topic and sentiment analyses, especially on tail labels.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2725320/v1"
    },
    {
        "id": 23360,
        "title": "L’invasion de l’Ukraine va-t-elle transformer l’Europe en vraie puissance ?",
        "authors": " Open Diplomacy",
        "published": "2022-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/poec.040.0014"
    },
    {
        "id": 23361,
        "title": "Multi-Modal Enhancement Transformer Network for Skeleton-Based Human Interaction Recognition",
        "authors": "Qianshuo Hu, Haijun Liu",
        "published": "2024-2-20",
        "citations": 0,
        "abstract": "Skeleton-based human interaction recognition is a challenging task in the field of vision and image processing. Graph Convolutional Networks (GCNs) achieved remarkable performance by modeling the human skeleton as a topology. However, existing GCN-based methods have two problems: (1) Existing frameworks cannot effectively take advantage of the complementary features of different skeletal modalities. There is no information transfer channel between various specific modalities. (2) Limited by the structure of the skeleton topology, it is hard to capture and learn the information about two-person interactions. To solve these problems, inspired by the human visual neural network, we propose a multi-modal enhancement transformer (ME-Former) network for skeleton-based human interaction recognition. ME-Former includes a multi-modal enhancement module (ME) and a context progressive fusion block (CPF). More specifically, each ME module consists of a multi-head cross-modal attention block (MH-CA) and a two-person hypergraph self-attention block (TH-SA), which are responsible for enhancing the skeleton features of a specific modality from other skeletal modalities and modeling spatial dependencies between joints using the specific modality, respectively. In addition, we propose a two-person skeleton topology and a two-person hypergraph representation. The TH-SA block can embed their structural information into the self-attention to better learn two-person interaction. The CPF block is capable of progressively transforming the features of different skeletal modalities from low-level features to higher-order global contexts, making the enhancement process more efficient. Extensive experiments on benchmark NTU-RGB+D 60 and NTU-RGB+D 120 datasets consistently verify the effectiveness of our proposed ME-Former by outperforming state-of-the-art methods.",
        "link": "http://dx.doi.org/10.3390/biomimetics9030123"
    },
    {
        "id": 23362,
        "title": "A Transformer Design for High-Voltage Application Using LLC Resonant Converter",
        "authors": "Umut Ondin, Abdulkadir Balikci",
        "published": "2023-1-30",
        "citations": 1,
        "abstract": "The inductor–inductor–capacitor (LLC) resonant converter is a suitable topology for wide output voltage and load range applications with limited circuit parameters. One of the most significant design boundaries of an LLC resonant converter in high-voltage applications is the parasitic capacitance effect of the main circuit components, particularly the transformer and junction capacitances of the secondary rectifier network. Parasitic capacitance effects are much higher in high-voltage applications than in low-voltage applications. Therefore, the use of an LLC resonant converter is limited to high-voltage applications. This study proposes to reduce the capacitive effects of high-voltage transformers and rectification networks with a multi-winding transformer with an integrated rectifier design and to use it in high-voltage applications with the advantages of the LLC resonant converter. For the proposed prototype, comparative experimental measurements were conducted using a conventional scheme. The measurements validate the reliability of the LLC converter for high-voltage applications, improving the output regulation performance while significantly reducing parasitic capacitances.",
        "link": "http://dx.doi.org/10.3390/en16031377"
    },
    {
        "id": 23363,
        "title": "TATune: A RocksDB Knob Tuning System Based on Transformer",
        "authors": "Yun-Zhang Hu, Hui Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3343455"
    },
    {
        "id": 23364,
        "title": "Automatic speech recognition with efficient transformer",
        "authors": "Shuhan Luo",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3012507"
    },
    {
        "id": 23365,
        "title": "EINet: camouflaged object detection with pyramid vision transformer",
        "authors": "Chen Li, Ge Jiao",
        "published": "2022-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.jei.31.5.053002"
    },
    {
        "id": 23366,
        "title": "Stability analysis of an Ideal-Transformer-Model interface algorithm",
        "authors": "Janja Dolenc, Ambroz Bozicek, Bostjan Blazic",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iyce45807.2019.8991575"
    },
    {
        "id": 23367,
        "title": "The Animation Transformer: Visual Correspondence via Segment Matching",
        "authors": "Evan Casey, Victor Perez, Zhuoru Li",
        "published": "2021-10",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01113"
    },
    {
        "id": 23368,
        "title": "TEST-Net: Transformer-Enhanced Spatio-Temporal Network for infectious disease prediction",
        "authors": "Kai Chen, Yao Liu, Tianjiao Ji, Yi Zhang, Yang Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOutbreaks of infectious diseases have caused tremendous human suffering and incalculable economic losses, and infectious diseases are a global public health problem that threatens human society. \nTherefore, it is necessary to model the spatial and temporal distribution characteristics of infectious diseases, explore the transmission trend of infectious diseases, establish an infection early warning model and take corresponding preventive and control measures, which can make the prevention and control work more targeted and forward-looking. \nGiven the complex spatial correlation and temporal variation of infectious diseases, deep learning-based Spatio-temporal sequence prediction is widely employed because of its superior performance in capturing Spatio-temporal features.\nHowever, current deep learning-based infectious disease prediction methods utilize an encoder-decoder structure that provides barely satisfactory accuracy due to a lack of understanding of infectious disease prevalence factors or deficiencies in capturing representative Spatio-temporal patterns.\nIn this paper, we develop the Transformer-Enhanced Spatio-Temporal Network (TEST-Net) which consists of a temporal location coding module and a Spatio-temporal feature fusion module for Infectious disease prediction.  \nTemporal information is input in TEST-Net by Temporal Location Encoding (TLE), and temporal and spatial correlation of sequences is extracted by a transformer-based attention network, and temporal features are fused with spatial features by a Spatio-temporal feature fusion network. \nCompared with other state-of-the-art methods, qualitative and quantitative results show that TSET-Net has an excellent performance in modeling the spatial and temporal distribution characteristics of data and performs well in the accuracy of long-term prediction of infectious disease.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3784607/v1"
    },
    {
        "id": 23369,
        "title": "Transformer breather thermal image decomposition for fault diagnosis",
        "authors": "R Vidhya, P.Vanaja Ranjan, N.R. Shanker",
        "published": "2021-2-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icees51510.2021.9383639"
    },
    {
        "id": 23370,
        "title": "Addressing Class Imbalance for Transformer Based Knee MRI Classification",
        "authors": "Gokay Sezen, Ilkay Oksuz",
        "published": "2022-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk55850.2022.9919578"
    },
    {
        "id": 23371,
        "title": "Material Texture Recognition using Ultrasonic Images with Transformer Neural Networks",
        "authors": "Xin Zhang, Jafar Saniie",
        "published": "2021-5-14",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eit51626.2021.9491908"
    },
    {
        "id": 23372,
        "title": "Power Auto-transformer Mechanical Faults Diagnosis based on Frequency Response Analysis",
        "authors": "Mojtaba Mahvi, Vahid Behjat, Hossein Mohseni",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081531"
    },
    {
        "id": 23373,
        "title": "English to Hindi Translation using Transformer",
        "authors": "Abhinav Y. Watve, Madhuri A. Bhalekar",
        "published": "2023-3-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icidca56705.2023.10100193"
    },
    {
        "id": 23374,
        "title": "A Modern Approach to Differential Partial Discharge Diagnosis on Instrument Transformer",
        "authors": "S.M. Hoek, B. Batlle",
        "published": "2020-7-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd46958.2020.9341993"
    },
    {
        "id": 23375,
        "title": "A COVID-19 Search Engine (CO-SE) with Transformer-based architecture",
        "authors": "Shaina Raza",
        "published": "2022-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.health.2022.100068"
    },
    {
        "id": 23376,
        "title": "Refinement of the Steinmetz Coefficient for the Transformer Core Steel",
        "authors": "Sergey PLOTNIKOV, Tat'yana SHCHEGOLEVA",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24160/0013-5380-2023-4-73-78"
    },
    {
        "id": 23377,
        "title": "Transformer Networks for Non-Intrusive Speech Quality Prediction",
        "authors": "M K Jayesh, Mukesh Sharma, Praneeth Vonteddu, Mahaboob Ali Basha Shaik, Sriram Ganapathy",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10020"
    },
    {
        "id": 23378,
        "title": "Semantic Video Transformer for Robust Action Recognition",
        "authors": "Keval Doshi, Yasin Yilmaz",
        "published": "2023-11-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsc61021.2023.10354225"
    },
    {
        "id": 23379,
        "title": "Multistep retrosynthesis combining a disconnection aware triple transformer loop with a route penalty score guided tree search",
        "authors": "David Kreutter, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 0,
        "abstract": "Computer-aided synthesis planning (CASP) aims to automatically learn organic reactivity from literature and perform retrosynthesis of unseen molecules. CASP systems must learn reactions sufficiently precisely to propose realistic disconnections while avoiding overfitting to leave room for diverse options, and explore possible routes such as to allow short synthetic sequences to emerge. Herein we report a CASP tool proposing original solutions to both challenges. First, we use a triple transformer loop (TTL) predicting starting materials (T1), reagents (T2), and products (T3) to explore diverse disconnections obtained by tagging potentially reacting atoms both systematically and using templates. Second, we integrate TTL into a multistep tree search algorithm (TTLA) prioritizing sequences using a route penalty score (RPScore) considering the number of steps, their confidence score, and the simplicity of all intermediates along the route. Our approach favours short synthetic routes to commercial starting materials, as exemplified by retrosynthetic analyses of recently approved drugs.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-8khth"
    },
    {
        "id": 23380,
        "title": "Multiscale Representations Learning Transformer Framework for Point Cloud Classification",
        "authors": "Yajie Sun, Ali Zia, Jun Zhou",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10223135"
    },
    {
        "id": 23381,
        "title": "Text Graph Transformer for Document Classification",
        "authors": "Haopeng Zhang, Jiawei Zhang",
        "published": "2020",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.668"
    },
    {
        "id": 23382,
        "title": "A study on transformer-based Object Detection",
        "authors": "Hritik Vaidwan, Nikhil Seth, Anil Singh Parihar, Kavinder Singh",
        "published": "2021-6-25",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit51480.2021.9498550"
    },
    {
        "id": 23383,
        "title": "Speech Transformer with Speaker Aware Persistent Memory",
        "authors": "Yingzhu Zhao, Chongjia Ni, Cheung-Chi Leung, Shafiq Joty, Eng Siong Chng, Bin Ma",
        "published": "2020-10-25",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1281"
    },
    {
        "id": 23384,
        "title": "Transformer Based Language Identification for Malayalam-English Code-Mixed Text",
        "authors": "S. Thara, Prabaharan Poornachandran",
        "published": "2021",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3104106"
    },
    {
        "id": 23385,
        "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
        "authors": "Xuanyu Zhang, Zhepeng Lv, Qing Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.546"
    },
    {
        "id": 23386,
        "title": "A Validated Practice for Transformer Retrofilling",
        "authors": "Fabio Scatiggio, Giirgio Camp, Riccardo Actis, Vander Tumiatti",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl59152.2023.10209334"
    },
    {
        "id": 23387,
        "title": "Development and Application of Cleaning Tool for Overhaul of Transformer Oil-loaded Chamber",
        "authors": "",
        "published": "2022-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i8(02).07"
    },
    {
        "id": 23388,
        "title": "Condition Monitoring of Interconnecting Transformer Through ANN Approach",
        "authors": "Monika Patel, Arun Pachori",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wocn45266.2019.8995086"
    },
    {
        "id": 23389,
        "title": "Parametric sensitivity analysis of power transformer differential protection",
        "authors": "L. M. Peres, K. M. Silva",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnps.2017.8252932"
    },
    {
        "id": 23390,
        "title": "A Modular and Flexible High-Frequency-Link Transformer with a Reduced Device Count and Zero High-Side Devices",
        "authors": "Sudip K. Mazumder,  ",
        "published": "2018-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1476384"
    },
    {
        "id": 23391,
        "title": "Vision Transformer in Stenosis Detection of Coronary Arteries",
        "authors": "Karol Przystalski, Michał Jungiewicz, Piotr Wawryka, Karol Sabatowski",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4175204"
    },
    {
        "id": 23392,
        "title": "Enhanced transformer model for video caption generation",
        "authors": "Soumya Varma, J. Dinesh Peter",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "AbstractAutomatic Video captioning system is a method of describing the content in a video by analysing its visual aspects with regard to space and time and producing a meaningful caption that explains the video. A decade of research in this area has resulted in a steep growth in the quality and appropriateness of the generated caption compared with the expected result. The research has been driven from the very basic method to most advanced transformer method. Machine generated caption of a video must be adhering to many expected standards. For humans, this task may be a trivial one, however its not the same for a machine to analyse the content and generate a semantically coherent description for it. The caption which is generated in a natural language must also adhere to its lexical and syntactical structure. The video captioning process is a culmination of computer vision and natural language processing tasks. Commencing with template based conventional approach, it has surpassed statistical method, traditional deep learning approaches and is now in the trend of using transformers. This work made an extensive study of the literature and has proposed an improved transformer‐based architecture for video captioning process. The transformer architecture made use of an encoder and decoder model that has two and three sublayers respectively. Multi‐head self attention and cross attention are part of the model which bring about very beneficial results. The decoder is auto‐regressive and uses a masked layer to prevent the model from foreseeing future words in the caption. An enhanced encoder‐decoder Transformer model with CNN for feature extraction has been used in our work. This model captures the long‐range dependencies and temporal relationships more effectively. The model has been evaluated with benchmark datasets and compared with state‐of‐the‐art methods and found to be slightly better in the performance. The performance scores are slightly varying for BLEU, METEOR, ROUGE and CIDEr. Furthermore, we propose the idea of curriculum learning if incorporated can improve the results again.",
        "link": "http://dx.doi.org/10.1111/exsy.13392"
    },
    {
        "id": 23393,
        "title": "Prospect of Zrse2 Monolayer as Detector Material for Transformer Fault Detection: A First-Principles Study",
        "authors": "Tiasa Mondal, Md.  Kawsar Alam, Orchi Hassan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4537050"
    },
    {
        "id": 23394,
        "title": "SS design, operation and control of smart transformer in power distribution system",
        "authors": "",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2017.8216728"
    },
    {
        "id": 23395,
        "title": "Supraharmonics Transfer Characteristics of Transformer",
        "authors": "Ruimin Duan, Jie He, Cheng Guo, Fulin Zhou",
        "published": "2021-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciea51954.2021.9516082"
    },
    {
        "id": 23396,
        "title": "The Application of Volt-Booster Transformer as a Part of Asynchronous Motors Control Devices",
        "authors": "Yaroslava K. Starostina",
        "published": "2021-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon52004.2021.9537425"
    },
    {
        "id": 23397,
        "title": "Performance Analysis of Transformer-Less Inverter Topologies for PV Applications",
        "authors": "R Monisha, H.S Sridhar,  Manojna",
        "published": "2021-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon52639.2021.9641702"
    },
    {
        "id": 23398,
        "title": "SAFMem: Accelerating Transformer Self-Attention Functionality via Memristor-Based Hardware",
        "authors": "Meriem Bettayeb, Yasmin Halawani, Muhammad Umair Khan, Hani Saleh, Baker Mohammad",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe adoption of transformer networks has experienced a notable surge in various AI applications. However, the increasedcomputational complexity, stemming primarily from the self-attention mechanism, parallels the manner in which convolutionoperations constrain the capabilities and speed of Convolutional Neural Networks (CNNs). The self-attention algorithm,specifically the Matrix-matrix Multiplication (MatMul) operations, demands a substantial amount of memory and computationalcomplexity, thereby restricting the overall performance of the transformer. This paper introduces an efficient hardwareaccelerator for the transformer network, leveraging memristor-based in-memory computing. The design targets the memorybottleneck associated with MatMul operations in the self-attention process, utilizing approximate analog computation andthe highly parallel computations facilitated by the memristor crossbar architecture. Remarkably, this approach resulted in areduction of approximately 10 times in the number of Multiply-Accumulate (MAC) operations in transformer networks, whilemaintaining 93.37% accuracy for the MNIST dataset, as validated by a comprehensive circuit simulator employing NeuroSim3.0. Simulation outcomes indicate an area utilization of 6895.7 μm2, a latency of 15.52 seconds, an energy consumption of3 mJ, and a leakage power of 59.55 μW . The methodology outlined in this paper represents a substantial stride towards ahardware-friendly transformer architecture for edge device, poised to achieve real-time performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3546552/v1"
    },
    {
        "id": 23399,
        "title": "Transformer parameter monitoring system using PROTEUS software",
        "authors": "Rashmi Ashok Panherkar, Prajakta Vaidya",
        "published": "2017-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecct.2017.8118021"
    },
    {
        "id": 23400,
        "title": "Voltage Sag Propagation Model Considering Transformer Operation Parameters",
        "authors": "Song Yifan, Li Hongtao, Sun He",
        "published": "2022-3-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeees54426.2022.9759558"
    },
    {
        "id": 23401,
        "title": "Joint Microseismic Event Detection and Location Based on a Detection Transformer",
        "authors": "Y. Yang, C. Birnie, T. Alkhalifah",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202310096"
    },
    {
        "id": 23402,
        "title": "Partially Trained Music Generation based on Transformer",
        "authors": "Jiandong Tang, Lanqing Yin, Jinming Yu",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscss57650.2023.10169184"
    },
    {
        "id": 23403,
        "title": "ABTrans: A Transformer-based model for predicting interaction between anti-Aß antibodies and peptides",
        "authors": "Yuhong Su, Lingfeng Zhang, Yangjing Wang, Buyong Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractUnderstanding the recognition of antibodies and Aβ peptide is crucial for the development of more effective therapeutic agents. Here we studied the interaction between anti-Aβ antibodies and different peptides by building a deep learning model, using the dodecapeptide sequences elucidated from phage display and known anti-Aβ antibody sequences collected from public sources. Our multi-classification model, ABTrans was trained to determine the four levels of binding ability between anti-Aβ antibody and dodecapeptide: not binding, weak binding, medium binding, and strong binding. The accuracy of our model reached 0.8278. Using the ABTrans, we examined the cross-reaction of anti-Aβ antibodies with other human amyloidogenic proteins, and we found that Aducanumab and Donanemab have the least cross-reactions with other human amyloidogenic proteins. We also systematically screened all human proteins interaction with eleven selected anti-Aβ antibodies to identify possible peptide fragments that could be an off-target candidate.Key PointsABTrans is a Transformer-based model that was developed for the first time to predict the interaction between anti-Aß antibodies and peptides.ABTrans was trained using a dataset with 1.5 million peptides and 110 anti-Aβ antibodies.ABTrans achieved an accuracy of 0.8278 and is capable of determining the four levels of binding ability between antibody and Aß: not binding, weak binding, medium binding, and strong binding.ABTrans has potential applications in predicting off-target and cross-reactivity effects of antibodies and in designing new anti-Aß antibodies.",
        "link": "http://dx.doi.org/10.1101/2023.10.09.561503"
    },
    {
        "id": 23404,
        "title": "Research on Fault Diagnosis for Power Transformer Based on HHGA-WNN",
        "authors": "Hailiang Yang",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec.2018.8740608"
    },
    {
        "id": 23405,
        "title": "Multi-scale filter-enhanced transformer for sequential recommendation",
        "authors": "Xiyu Cui",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Recent research has shown that self-attention mechanisms improve Sequential Recommender Systems (SRS) by capturing sequential associations with the interactions. Nevertheless, existing work still needs to address two critical limitations. Firstly, the behavior of users in the original sequences contains various preference signals that are implicit and noisy and hard to reflect the users intentions fully. As a result, it would deteriorate the representation of their true intentions to model all interactions. Secondly, most models only model single-scale interaction sequences and ignore the multi-scale feature relationships of the sequences. In order to address these limitations, the paper proposes MFTSRec (Multi-scale Filter Enhanced Transformer Sequential Recommender), which can weaken those interactions irrelevant to the users intentions from their implicit feedback and adaptively focus on the users multi-scale intentions. Besides, this paper also does extensive experiments on four benchmark datasets and further demonstrates the effectiveness and robustness of MFTSRec compared to the state-of-the-art model.",
        "link": "http://dx.doi.org/10.54254/2755-2721/13/20230723"
    },
    {
        "id": 23406,
        "title": "Correlation analysis of hotspot temperatures in power transformer – a case study",
        "authors": "Michal Kunicki",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2019.8778179"
    },
    {
        "id": 23407,
        "title": "An Automatic Transformer from Sequential to Parallel Java Code",
        "authors": "Alessandro Midolo, Emiliano Tramontana",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "Sequential programs can benefit from parallel execution to improve their performance. When developing a parallel application, several techniques are employed to achieve the desired behavior: identifying parts that can run in parallel, synchronizing access to shared data, tuning performance, etc. Admittedly, manually transforming a sequential application to make it parallel can be tedious due to the large number of lines of code to inspect, the possibility of errors arising from inaccurate data dependence analysis leading to unpredictable behavior, and inefficiencies when the workload between parallel threads is unbalanced. This paper proposes an automatic approach that analyzes Java source code to identify method calls that are suitable for parallel execution and transforms them so that they run in another thread. The approach is based on data dependence and control dependence analyses to determine the execution flow and data accessed. Based on the proposed method, a tool has been developed to enhance applications by incorporating parallelism, i.e., transforming suitable method calls to execute on parallel threads, and synchronizing data access where needed. The developed tool has been extensively tested to verify the accuracy of its analysis in finding parallel execution opportunities, the correctness of the source code alterations, and the resultant performance gain.",
        "link": "http://dx.doi.org/10.3390/fi15090306"
    },
    {
        "id": 23408,
        "title": "Traditional Village Classification Model Based on Transformer Network",
        "authors": "Qi Zhong",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "The study of traditional villages holds significant implications in cultural, historical, and societal contexts. Despite the considerable research focus on the architectural styles of Qiang, Tibetan, Han, and Hui ethnic villages due to their distinctiveness, rapidly and accurately identifying the types of traditional villages in practical surveys remains a challenge. To address this issue, this paper establishes an aerial image dataset for Qiang, Tibetan, Han, and Hui ethnic villages and introduces a specialized feature extraction network, Transformer-Village, designed for the classification and detection of traditional villages using deep learning algorithms. The overall structure of the network is lightweight, incorporating condconv dynamic convolution as the core layer structure; furthermore, a spatial self-attention-related feature extraction network is designed based on Transformer. In conclusion, through simulated experiments, Transformer-Village coupled with the YOLO detector achieves a 97.2% mAP on the test set, demonstrating superior detection accuracy compared to other baseline models. Overall, the experimental results suggest that this work is feasible and practical.",
        "link": "http://dx.doi.org/10.22158/asir.v7n4p126"
    },
    {
        "id": 23409,
        "title": "AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese",
        "authors": "Abhijnan Nath, Sheikh Mannan, Nikhil Krishnaswamy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.739"
    },
    {
        "id": 23410,
        "title": "Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation",
        "authors": "Haoyi Wu, Kewei Tu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.482"
    },
    {
        "id": 23411,
        "title": "Wheat Full-Width Harvesting Navigation Line Extraction Method Using Improved Swin-Transformer",
        "authors": "anon anon, Chengqian Jin, Man Chen, Zeyu Cai, Zheng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4576704"
    },
    {
        "id": 23412,
        "title": "Integrated Modeling for In-Depth EEG Based Emotional State Analysis via Convolutional-Transformer Fusion",
        "authors": "Noman Ali, A. S Kang, Saurabh Himral",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study introduces a hybrid model designed for the predic- tion of emotional states derived from electroencephalogram (EEG) data, employing an amalgamation of convolutional and trans- former layers. The architectural framework of the model is metic- ulously structured to facilitate concurrent assimilation of local pat- terns and long-range dependencies inherent in EEG data, thereby augmenting its discernment of nuanced emotional experiences. The investigation presented herein undertakes a comprehensive explo- ration of the fusion technique, with a primary focus on dis- cerning three elemental emotional dimensions: Arousal, Valence, and Dominance, in addition to their concurrent combinations. The research methodology encompasses an in-depth evaluation of the model’s performance across these diverse emotional states, encompassing the intricate task of simultaneous Valence-Arousal (VA) prediction. Furthermore, the study extends its purview to encompass the intricate Valence-Arousal-Dominance (VAD) space, thereby providing a thorough analysis of the model’s efficacy. To articulate the model’s discriminative efficacy, this study meticu- lously presents the detailed F1 scores corresponding to each emotional state classification: Arousal (96.8), Valence (97.3), Valence-Arousal (VA) simultaneously (95.6), and Valence-Arousal-Dominance simultaneously (94.9). These scores serve as a testament to the model’s robust per- formance across diverse emotional dimensions. Importantly, to fortify the credibility of our findings, rigorous experimentation has been con- ducted on the DEAP dataset, unveiling noteworthy results even in scenarios involving simultaneous recognition of multiple emotional states.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3739841/v1"
    },
    {
        "id": 23413,
        "title": "Simulasi Monitoring Pergeseran Tanah Menggunakan Sensor LVDT (Linear Variable Differential Transformer)",
        "authors": "Wilson Jefriyanto",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "Pergeseran tanah merupakan salah satu faktor terjadinya tanah longsor (landslide). Untuk itu, diperlukan suatu prototype alat yang dapat  membantu memberikan informasi dini dan peringatan akan terjadinya longsor. Telah dilakukan penelitian tentang simulasi pergeseran tanah dengan menggunakan sensor LVDT (Linear Variable Differential Transformer). Sensor LVDT dihubungkan dengan Arduino uno yang kemudian ditampilkan pada LCD dan juga monitoring pada komputer. Pengamatan pergeseran tanah  dilakukan selama 5 jam dengan melakukan variasi terhadap sudut kemiringan tanah. Tujuan dari penelitian ini yaitu untuk menguji kerja dari sensor LVDT yang merupakan hasil dari penelitian sebelumnya. Dari hasil pengamatan, terlihat bahwa sensor LVDT ini mampu membaca pergeseran tanah dengan ketlitian 0.01 mm. Adapun jangkauan pergeseran alat ini yaitu hany sampai 14 cm, untuk penelitian selanjutnya akan dikembankan sensor LVDT dengan jangkaun pengukuran yang lebih panjang.",
        "link": "http://dx.doi.org/10.31605/saintifik.v7i1.281"
    },
    {
        "id": 23414,
        "title": "Triplet Vision Transformer for Word Image Retrieval in Historical Arabic Documents",
        "authors": "Abir Fathallah, Mounim A.  A. El Yacoubi, Najoua  Essoukri Ben Amara",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4577874"
    },
    {
        "id": 23415,
        "title": "Application of Blind-Source-Separation algorithm for investigating transformer vibration patterns",
        "authors": "Muhammad Mueer, Lakshitha Naranpanawe, Chandima Ekanayake",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535622"
    },
    {
        "id": 23416,
        "title": "Transformer Based Memory Network for Video Anomaly Detection",
        "authors": "zhiqiang wang, xingsheng gu, Xiaojing Gu, Jingyu Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4219648"
    },
    {
        "id": 23417,
        "title": "Partial Discharge Detection of Transformer Winding",
        "authors": "Jiahao Zou",
        "published": "2021-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3495018.3495461"
    },
    {
        "id": 23418,
        "title": "Weakly-Supervised Self-Ensembling Vision Transformer for MRI Cardiac Segmentation",
        "authors": "Ziyang Wang, Haodong Zhang, Yang Liu",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00051"
    },
    {
        "id": 23419,
        "title": "SIMULINK MODEL OF POWER TRANSFORMER PROTECTION USING DIFFERENTIAL PROTECTION SCHEME",
        "authors": "",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets33157"
    },
    {
        "id": 23420,
        "title": "Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold",
        "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191125"
    },
    {
        "id": 23421,
        "title": "A Resonant Approach to Transformer Loss Characterization",
        "authors": "Michael Solomentsev, Odinaka Okeke, Alex J. Hanson",
        "published": "2022-3-20",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec43599.2022.9773516"
    },
    {
        "id": 23422,
        "title": "Dual-Source Transformer Model for Neural Machine Translation with Linguistic Knowledge",
        "authors": "Yirong Pan, Xiao Li, Yating Yang, Rui Dong",
        "published": "No Date",
        "citations": 3,
        "abstract": "Incorporating source-side linguistic knowledge into the neural machine translation (NMT) model has recently achieved impressive performance on machine translation tasks. One popular method is to generalize the word embedding layer of the encoder to encode each word and its linguistic features. The other method is to change the architecture of the encoder to encode syntactic information. However, the former cannot explicitly balance the contribution from the word and its linguistic features. The latter cannot flexibly utilize various types of linguistic information. Focusing on the above issues, this paper proposes a novel NMT approach that models the words in parallel to the linguistic knowledge by using two separate encoders. Compared with the single encoder based NMT model, the proposed approach additionally employs the knowledge-based encoder to specially encode linguistic features. Moreover, it shares parameters across encoders to enhance the model representation ability of the source-side language. Extensive experiments show that the approach achieves significant improvements of up to 2.4 and 1.1 BLEU points on Turkish→English and English→Turkish machine translation tasks, respectively, which indicates that it is capable of better utilizing the external linguistic knowledge and effective improving the machine translation quality.",
        "link": "http://dx.doi.org/10.20944/preprints202002.0273.v1"
    },
    {
        "id": 23423,
        "title": "Optimization of vision transformer-based detection of COVID-19 from chest X-ray images",
        "authors": "Jinsol Ko, Soyeon Park, Hyun Goo Woo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: For diagnosing coronavirus disease 2019 (COVID-19), chest X-rays have emerged as a preferred modality because of their accessibility, affordability, and capability to identify various pathologies. Recent advances in deep learning algorithms have shown promise in distinguishing COVID-19 from other lung diseases. However, the use of different optimization methods can affect the performance of the deep learning models. We aimed to compare the effects of the different optimization methods, identifying the best-performing algorithms for the detection of COVID-19 using chest X-rays.\nMethods: Chest X-ray images, including the seven classes of Normal, COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis, were obtained. We trained the Vision Transformer (ViT) model using different optimizers such as Adaptive Moment Estimation (Adam), AdamW, Nesterov accelerated Adam (NAdam), Rectified Adam (RAdam), Stochastic Gradient Descent with weight decay (SGDW), and Momentum, and compared their performances.\nResults: We found that the RAdam optimizer at a learning rate of 10-5 achieved the highest accuracy, highest weighted average of F1-score, and lowest false negative rate of COVID-19 for both 4 Class and 7 Class Dataset. On the other hand, AdamW showed better performance for the samples with small sample sizes. The optimizers derived from Adam (i.e. Adam, AdamW, NAdam, and RAdam), showed robust results against different learning rates, while SGDW and Momentum showed less significant robustness.\nConclusions: We suggest that Adam-derived optimizers, particularly RAdam, showed best performance in training the ViT model for detecting COVID-19 using chest X-ray images. Our results may help in the efforts to improve the performance of the model and to make it clinically useful.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3284664/v1"
    },
    {
        "id": 23424,
        "title": "Collaborative Transformer-CNN Learning for Semi-supervised Medical Image Segmentation",
        "authors": "Wei Li, Huihua Yang",
        "published": "2022-12-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995501"
    },
    {
        "id": 23425,
        "title": "Research on Knowledge Reasoning in Transformer Partial Discharge Fault Diagnosis",
        "authors": "Yu Wang, Yuxin Wang, Jinsha Yuan, Yin Liu",
        "published": "2020-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327364"
    },
    {
        "id": 23426,
        "title": "Transformer Compressed Sensing Via Global Image Tokens",
        "authors": "Marlon Bran Lorenzana, Craig Engstrom, Shekhar S. Chandra",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897630"
    },
    {
        "id": 23427,
        "title": "On-Line Monitoring Technology of Transformer Oil Level Based on Ultrasonic Sensor",
        "authors": "Yongcan Zhu, Dongyang Liu, Ruiwen Zhou, Yi Tian, Xinbo Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374266"
    },
    {
        "id": 23428,
        "title": "Knowledge-guided Transformer for Joint Topic and Sentiment Analysis of Chinese Classical Poetry",
        "authors": "Bin Wu, Yuting Wei, Yangfu Zhu, Linmei Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe analyses of the topic and sentiment are essential for understanding Chinese classical poetry and historical culture. Existing works fail to consider the lexical knowledge mined from poem annotations, which partly contains some information about the topic and sentiment. In addition, most works ignore the interdependence and diversity of the topic and sentiment in one poem. Hence, in this paper, we propose a Knowledge-guided Transformer Model (KTM) for joint multiple topic and sentiment analysis of Chinese classical poetry. Specifically, we first respectively construct two lexical dictionaries for the topic and sentiment based on the poem annotations. Then we take full advantage of the lexical dictionaries with a knowledge-based mask-transformer to represent poems. Furthermore, considering the correlations between the topic and sentiment, our model jointly classifies the multiple topics and sentiments in Chinese classical poetry by stacking the two subtasks. Considering there is no public dataset, we release a new Chinese classical poetry dataset CCPD for joint multiple topic and sentiment analysis. Extensive experiments demonstrate that our model achieves state-of-the-art performance on both topic and sentiment analyses, especially on tail labels.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2725320/v1"
    },
    {
        "id": 23429,
        "title": "L’invasion de l’Ukraine va-t-elle transformer l’Europe en vraie puissance ?",
        "authors": " Open Diplomacy",
        "published": "2022-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/poec.040.0014"
    },
    {
        "id": 23430,
        "title": "Multi-Modal Enhancement Transformer Network for Skeleton-Based Human Interaction Recognition",
        "authors": "Qianshuo Hu, Haijun Liu",
        "published": "2024-2-20",
        "citations": 0,
        "abstract": "Skeleton-based human interaction recognition is a challenging task in the field of vision and image processing. Graph Convolutional Networks (GCNs) achieved remarkable performance by modeling the human skeleton as a topology. However, existing GCN-based methods have two problems: (1) Existing frameworks cannot effectively take advantage of the complementary features of different skeletal modalities. There is no information transfer channel between various specific modalities. (2) Limited by the structure of the skeleton topology, it is hard to capture and learn the information about two-person interactions. To solve these problems, inspired by the human visual neural network, we propose a multi-modal enhancement transformer (ME-Former) network for skeleton-based human interaction recognition. ME-Former includes a multi-modal enhancement module (ME) and a context progressive fusion block (CPF). More specifically, each ME module consists of a multi-head cross-modal attention block (MH-CA) and a two-person hypergraph self-attention block (TH-SA), which are responsible for enhancing the skeleton features of a specific modality from other skeletal modalities and modeling spatial dependencies between joints using the specific modality, respectively. In addition, we propose a two-person skeleton topology and a two-person hypergraph representation. The TH-SA block can embed their structural information into the self-attention to better learn two-person interaction. The CPF block is capable of progressively transforming the features of different skeletal modalities from low-level features to higher-order global contexts, making the enhancement process more efficient. Extensive experiments on benchmark NTU-RGB+D 60 and NTU-RGB+D 120 datasets consistently verify the effectiveness of our proposed ME-Former by outperforming state-of-the-art methods.",
        "link": "http://dx.doi.org/10.3390/biomimetics9030123"
    },
    {
        "id": 23431,
        "title": "A Transformer Design for High-Voltage Application Using LLC Resonant Converter",
        "authors": "Umut Ondin, Abdulkadir Balikci",
        "published": "2023-1-30",
        "citations": 1,
        "abstract": "The inductor–inductor–capacitor (LLC) resonant converter is a suitable topology for wide output voltage and load range applications with limited circuit parameters. One of the most significant design boundaries of an LLC resonant converter in high-voltage applications is the parasitic capacitance effect of the main circuit components, particularly the transformer and junction capacitances of the secondary rectifier network. Parasitic capacitance effects are much higher in high-voltage applications than in low-voltage applications. Therefore, the use of an LLC resonant converter is limited to high-voltage applications. This study proposes to reduce the capacitive effects of high-voltage transformers and rectification networks with a multi-winding transformer with an integrated rectifier design and to use it in high-voltage applications with the advantages of the LLC resonant converter. For the proposed prototype, comparative experimental measurements were conducted using a conventional scheme. The measurements validate the reliability of the LLC converter for high-voltage applications, improving the output regulation performance while significantly reducing parasitic capacitances.",
        "link": "http://dx.doi.org/10.3390/en16031377"
    },
    {
        "id": 23432,
        "title": "TATune: A RocksDB Knob Tuning System Based on Transformer",
        "authors": "Yun-Zhang Hu, Hui Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3343455"
    },
    {
        "id": 23433,
        "title": "Automatic speech recognition with efficient transformer",
        "authors": "Shuhan Luo",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3012507"
    },
    {
        "id": 23434,
        "title": "EINet: camouflaged object detection with pyramid vision transformer",
        "authors": "Chen Li, Ge Jiao",
        "published": "2022-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.jei.31.5.053002"
    },
    {
        "id": 23435,
        "title": "Stability analysis of an Ideal-Transformer-Model interface algorithm",
        "authors": "Janja Dolenc, Ambroz Bozicek, Bostjan Blazic",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iyce45807.2019.8991575"
    },
    {
        "id": 23436,
        "title": "The Animation Transformer: Visual Correspondence via Segment Matching",
        "authors": "Evan Casey, Victor Perez, Zhuoru Li",
        "published": "2021-10",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01113"
    },
    {
        "id": 23437,
        "title": "TEST-Net: Transformer-Enhanced Spatio-Temporal Network for infectious disease prediction",
        "authors": "Kai Chen, Yao Liu, Tianjiao Ji, Yi Zhang, Yang Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOutbreaks of infectious diseases have caused tremendous human suffering and incalculable economic losses, and infectious diseases are a global public health problem that threatens human society. \nTherefore, it is necessary to model the spatial and temporal distribution characteristics of infectious diseases, explore the transmission trend of infectious diseases, establish an infection early warning model and take corresponding preventive and control measures, which can make the prevention and control work more targeted and forward-looking. \nGiven the complex spatial correlation and temporal variation of infectious diseases, deep learning-based Spatio-temporal sequence prediction is widely employed because of its superior performance in capturing Spatio-temporal features.\nHowever, current deep learning-based infectious disease prediction methods utilize an encoder-decoder structure that provides barely satisfactory accuracy due to a lack of understanding of infectious disease prevalence factors or deficiencies in capturing representative Spatio-temporal patterns.\nIn this paper, we develop the Transformer-Enhanced Spatio-Temporal Network (TEST-Net) which consists of a temporal location coding module and a Spatio-temporal feature fusion module for Infectious disease prediction.  \nTemporal information is input in TEST-Net by Temporal Location Encoding (TLE), and temporal and spatial correlation of sequences is extracted by a transformer-based attention network, and temporal features are fused with spatial features by a Spatio-temporal feature fusion network. \nCompared with other state-of-the-art methods, qualitative and quantitative results show that TSET-Net has an excellent performance in modeling the spatial and temporal distribution characteristics of data and performs well in the accuracy of long-term prediction of infectious disease.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3784607/v1"
    },
    {
        "id": 23438,
        "title": "Transformer breather thermal image decomposition for fault diagnosis",
        "authors": "R Vidhya, P.Vanaja Ranjan, N.R. Shanker",
        "published": "2021-2-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icees51510.2021.9383639"
    },
    {
        "id": 23439,
        "title": "Addressing Class Imbalance for Transformer Based Knee MRI Classification",
        "authors": "Gokay Sezen, Ilkay Oksuz",
        "published": "2022-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk55850.2022.9919578"
    },
    {
        "id": 23440,
        "title": "Material Texture Recognition using Ultrasonic Images with Transformer Neural Networks",
        "authors": "Xin Zhang, Jafar Saniie",
        "published": "2021-5-14",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eit51626.2021.9491908"
    },
    {
        "id": 23441,
        "title": "Power Auto-transformer Mechanical Faults Diagnosis based on Frequency Response Analysis",
        "authors": "Mojtaba Mahvi, Vahid Behjat, Hossein Mohseni",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081531"
    },
    {
        "id": 23442,
        "title": "English to Hindi Translation using Transformer",
        "authors": "Abhinav Y. Watve, Madhuri A. Bhalekar",
        "published": "2023-3-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icidca56705.2023.10100193"
    },
    {
        "id": 23443,
        "title": "A Modern Approach to Differential Partial Discharge Diagnosis on Instrument Transformer",
        "authors": "S.M. Hoek, B. Batlle",
        "published": "2020-7-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd46958.2020.9341993"
    },
    {
        "id": 23444,
        "title": "A COVID-19 Search Engine (CO-SE) with Transformer-based architecture",
        "authors": "Shaina Raza",
        "published": "2022-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.health.2022.100068"
    },
    {
        "id": 23445,
        "title": "Refinement of the Steinmetz Coefficient for the Transformer Core Steel",
        "authors": "Sergey PLOTNIKOV, Tat'yana SHCHEGOLEVA",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24160/0013-5380-2023-4-73-78"
    },
    {
        "id": 23446,
        "title": "Transformer Networks for Non-Intrusive Speech Quality Prediction",
        "authors": "M K Jayesh, Mukesh Sharma, Praneeth Vonteddu, Mahaboob Ali Basha Shaik, Sriram Ganapathy",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10020"
    },
    {
        "id": 23447,
        "title": "Semantic Video Transformer for Robust Action Recognition",
        "authors": "Keval Doshi, Yasin Yilmaz",
        "published": "2023-11-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsc61021.2023.10354225"
    },
    {
        "id": 23448,
        "title": "Multistep retrosynthesis combining a disconnection aware triple transformer loop with a route penalty score guided tree search",
        "authors": "David Kreutter, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 0,
        "abstract": "Computer-aided synthesis planning (CASP) aims to automatically learn organic reactivity from literature and perform retrosynthesis of unseen molecules. CASP systems must learn reactions sufficiently precisely to propose realistic disconnections while avoiding overfitting to leave room for diverse options, and explore possible routes such as to allow short synthetic sequences to emerge. Herein we report a CASP tool proposing original solutions to both challenges. First, we use a triple transformer loop (TTL) predicting starting materials (T1), reagents (T2), and products (T3) to explore diverse disconnections obtained by tagging potentially reacting atoms both systematically and using templates. Second, we integrate TTL into a multistep tree search algorithm (TTLA) prioritizing sequences using a route penalty score (RPScore) considering the number of steps, their confidence score, and the simplicity of all intermediates along the route. Our approach favours short synthetic routes to commercial starting materials, as exemplified by retrosynthetic analyses of recently approved drugs.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-8khth"
    },
    {
        "id": 23449,
        "title": "Multiscale Representations Learning Transformer Framework for Point Cloud Classification",
        "authors": "Yajie Sun, Ali Zia, Jun Zhou",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10223135"
    },
    {
        "id": 23450,
        "title": "Text Graph Transformer for Document Classification",
        "authors": "Haopeng Zhang, Jiawei Zhang",
        "published": "2020",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.668"
    },
    {
        "id": 23451,
        "title": "A study on transformer-based Object Detection",
        "authors": "Hritik Vaidwan, Nikhil Seth, Anil Singh Parihar, Kavinder Singh",
        "published": "2021-6-25",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit51480.2021.9498550"
    },
    {
        "id": 23452,
        "title": "Speech Transformer with Speaker Aware Persistent Memory",
        "authors": "Yingzhu Zhao, Chongjia Ni, Cheung-Chi Leung, Shafiq Joty, Eng Siong Chng, Bin Ma",
        "published": "2020-10-25",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1281"
    },
    {
        "id": 23453,
        "title": "Transformer Based Language Identification for Malayalam-English Code-Mixed Text",
        "authors": "S. Thara, Prabaharan Poornachandran",
        "published": "2021",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3104106"
    },
    {
        "id": 23454,
        "title": "Hot Spot Temperature Analysis of Transformer using FEM on COMSOL",
        "authors": "Swapnil Solanki, Rohit Jangid, Gaurav Srivastava | Prateek Sharma | Ritvik Chaturvedi,  ",
        "published": "2018-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd11280"
    },
    {
        "id": 23455,
        "title": "Two Legs Voltage Source Converter with DSTATCOM, T-Connected Transformer for Power Quality Improvement",
        "authors": "Bibhuti Bhusan Kumar,  ",
        "published": "2020-12-30",
        "citations": 0,
        "abstract": "The  numerous  loads  can  impact  the  working  environment  and  performance  of  the  source  apparatus.  Hence,  the  reimbursement  of this current will help in enhancing the presentation of the power system apparatus. This paper presents the strategy and employment of a Distribution Static Compensator (DSTATCOM) with the T-Connected transformer for compensation of load neutral current in the presence of three-phase unbalanced linear load. There are different types of control  strategy  in  that  one  of  the  best  methods  is  the  unit  vector  template method-based control algorithm has been implemented for the control of the proposed DSTATCOM. The proposed model has been simulated in a SIMULINK/ MATLAB environment. The simulation results show the effectiveness of the proposed algorithm.",
        "link": "http://dx.doi.org/10.24321/2456.1401.202007"
    },
    {
        "id": 23456,
        "title": "Performance and Emission Charecterstics of Diesel Engine Blended with Used Transformer Oil: A Review",
        "authors": "Samyak Jain, Yogesh Yadav,  ",
        "published": "2018-12-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd19185"
    },
    {
        "id": 23457,
        "title": "VulD-Transformer: Source Code Vulnerability Detection via Transformer",
        "authors": "Xuejun Zhang, Fenghe Zhang, Bo Zhao, Bo Zhou, Boyang Xiao",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3609437.3609451"
    },
    {
        "id": 23458,
        "title": "MR-Transformer: Multiresolution Transformer for Multivariate Time Series Prediction",
        "authors": "Siying Zhu, Jiawei Zheng, Qianli Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3327416"
    },
    {
        "id": 23459,
        "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
        "authors": "Bofan Song, Dharma Raj KC, Rubin Yuchan Yang, Shaobai Li, Chicheng Zhang, Rongguang Liang",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "link": "http://dx.doi.org/10.3390/cancers16050987"
    },
    {
        "id": 23460,
        "title": "基于transformer和自适应嵌入策略的可逆信息隐藏",
        "authors": "Linna Zhou, Zhigao Lu, Weike You, Xiaofei Fang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1631/fitee.2300041"
    },
    {
        "id": 23461,
        "title": "Synchronous Reference Frame Control of Transformer Based DC mA Current Sensor",
        "authors": "Muhammed Calar, Korhan Kayisli",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsmartgrid58556.2023.10170943"
    },
    {
        "id": 23462,
        "title": "S’approprier des savoirs issus des sciences sociales et transformer ses dispositions genrées",
        "authors": "Laurence Bachmann, Anne Perriard",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/contextes.11390"
    },
    {
        "id": 23463,
        "title": "Transformer fault diagnosis based on TWOA-SVM",
        "authors": "Yongmiao Yin, Hongwei Ma, Feichao Yan",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ifeea60725.2023.10429459"
    },
    {
        "id": 23464,
        "title": "Laboratory Investigations of Parallel Connected Inverters Feeding Medium Voltage Transformer",
        "authors": "Maciej Kozak",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2018.8591837"
    },
    {
        "id": 23465,
        "title": "Summarized Diagnostic Parameter for Condition Assessment of Power Transformer Windings Insulation",
        "authors": "I.A. Khudonogov, E.Yu. Puzina, A.G. Tuigunova",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867610"
    },
    {
        "id": 23466,
        "title": "Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation",
        "authors": "Chang Han, Weiping Tu, Yuhong Yang, Jingyi Li, Xinhong Li",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10077"
    },
    {
        "id": 23467,
        "title": "A CNN-Transformer Deep Learning Model for Real-time Sleep Stage Classification in an Energy-Constrained Wireless Device",
        "authors": "Zongyan Yao, Xilin Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThis paper proposes a deep learning (DL) model for automatic sleep stage classification based on single-channel EEG data. The DL model features a convolutional neural network (CNN) and transformers. The model was designed to run on energy and memory-constrained devices for real-time operation with local processing. The Fpz-Cz EEG signals from a publicly available Sleep-EDF dataset are used to train and test the model. Four convolutional filter layers were used to extract features and reduce the data dimension. Then, transformers were utilized to learn the time-variant features of the data. To improve performance, we also implemented a subject specific training before the inference (i.e., prediction) stage. With the subject specific training, the F1 score was 0.91, 0.37, 0.84, 0.877, and 0.73 for wake, N1-N3, and rapid eye movement (REM) stages, respectively. The performance of the model was comparable to the state-of-the-art works with significantly greater computational costs. We tested a reduced-sized version of the proposed model on a low-cost Arduino Nano 33 BLE board and it was fully functional and accurate. In the future, a fully integrated wireless EEG sensor with edge DL will be developed for sleep research in pre-clinical and clinical experiments, such as real-time sleep modulation.",
        "link": "http://dx.doi.org/10.1101/2022.11.21.22282544"
    },
    {
        "id": 23468,
        "title": "Transformer Winding Movement differentiated using Frequency Response Analysis",
        "authors": "Rongeet Talukdar",
        "published": "2021-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaect49130.2021.9392474"
    },
    {
        "id": 23469,
        "title": "Towards an Effective and Efficient Transformer for Rain-by-Snow Weather Removal",
        "authors": "Tao Gao, Yuanbo Wen, Kaihao Zhang, Peng Cheng, Ting Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458244"
    },
    {
        "id": 23470,
        "title": "Longitudinal Propagation in a Magnetized Time-Varying Plasma ∗",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-8"
    },
    {
        "id": 23471,
        "title": "Syntactic aperture radar imaging",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00003-3"
    },
    {
        "id": 23472,
        "title": "A fault ride through strategy of multi-ports DC transformer",
        "authors": "Hao Wu",
        "published": "2020-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacre50138.2020.9230334"
    },
    {
        "id": 23473,
        "title": "The Thomson Jumping Ring Experiment and Ideal Transformer",
        "authors": "Chiu-king Ng",
        "published": "2022-5-1",
        "citations": 0,
        "abstract": "In this paper, we utilize the readily known theory of the ideal transformer to furnish a self-contained qualitative explanation on the AC-powered Thomson jumping ring (TJR) experiment.",
        "link": "http://dx.doi.org/10.1119/5.0036490"
    },
    {
        "id": 23474,
        "title": "Parameter Agnostic Stacked Wavelet Transformer for Detecting Singularities",
        "authors": "Akshay Agarwal, Mayank Vatsa, Richa Singh, Nalini Ratha",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4292759"
    },
    {
        "id": 23475,
        "title": "TSNet: Token Sparsification for Efficient Video Transformer",
        "authors": "Hao Wang, Wenjia Zhang, Guohua Liu",
        "published": "2023-9-24",
        "citations": 1,
        "abstract": "In the domain of video recognition, video transformers have demonstrated remarkable performance, albeit at significant computational cost. This paper introduces TSNet, an innovative approach for dynamically selecting informative tokens from given video samples. The proposed method involves a lightweight prediction module that assigns importance scores to each token in the video. Tokens with top scores are then utilized for self-attention computation. We apply the Gumbel-softmax technique to sample from the output of the prediction module, enabling end-to-end optimization of the prediction module. We aim to extend our method on hierarchical vision transformers rather than single-scale vision transformers. We use a simple linear module to project the pruned tokens, and the projected result is then concatenated with the output of the self-attention network to maintain the same number of tokens while capturing interactions with the selected tokens. Since feedforward networks (FFNs) contribute significant computation, we also propose linear projection for the pruned tokens to accelerate the model, and the existing FFN layer progresses the selected tokens. Finally, in order to ensure that the structure of the output remains unchanged, the two groups of tokens are reassembled based on their spatial positions in the original feature map. The experiments conducted primarily focus on the Kinetics-400 dataset using UniFormer, a hierarchical video transformer backbone that incorporates convolution in its self-attention block. Our model demonstrates comparable results to the original model while reducing computation by over 13%. Notably, by hierarchically pruning 70% of input tokens, our approach significantly decreases 55.5% of the FLOPs, while the decline in accuracy is confined to 2%. Additional testing of wide applicability and adaptability with other transformers such as the Video Swin Transformer was also performed and indicated its progressive potentials in video recognition benchmarks. By implementing our token sparsification framework, video vision transformers can achieve a remarkable balance between enhanced computational speed and a slight reduction in accuracy.",
        "link": "http://dx.doi.org/10.3390/app131910633"
    },
    {
        "id": 23476,
        "title": "Enhancing the Transformer Decoder with Transition-based Syntax",
        "authors": "Leshem Choshen, Omri Abend",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.conll-1.27"
    },
    {
        "id": 23477,
        "title": "Machinery Prognostics and High-Dimensional Data Feature Extraction Based on A Transformer Self-Attention Transfer Network",
        "authors": "Shilong Sun, Tengyi Peng, Haodong Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Machinery degradation assessment can offer meaningful prognosis and health management in-formation. Although numerous machine prediction models based on artificial intelligence have emerged in recent years, they still face a series of challenges: (1) Many models continue to rely on manual feature extraction. (2) Deep learning models still struggle with long sequence prediction tasks. (3) Health indicators are inefficient for remaining useful life (RUL) prediction with cross-operational environments when dealing with high-dimensional datasets as inputs. This research proposes a health indicator construction methodology based on a transformer self-attention transfer network (TSTN). This methodology can directly deal with the high-dimensional raw dataset and keep all the information without missing when the signals are taken as the input of the diagnosis and prognosis model.  First, we design an encoder with a long-term and short-term self-attention mechanism to capture crucial time-varying information from a high-dimensional dataset. Second, we propose an estimator that can map the embedding from the encoder output to the estimated degradation trends. Then, we present a domain dis-criminator to extract invariant features from different machine operating conditions. The case studies with the FEMTO-ST bearing dataset and the Monte Carlo method for RUL prediction during the degradation process are conducted. The experiment results fully exhibited the signif-icant advantages of the proposed method compared to other state-of-the-art techniques.",
        "link": "http://dx.doi.org/10.20944/preprints202309.2035.v1"
    },
    {
        "id": 23478,
        "title": "Graph Transformer Attention Networks for Traffic Flow Prediction",
        "authors": "Haochun Ruan, Xinxin Feng, Haifeng Zheng",
        "published": "2021-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc54389.2021.9674238"
    },
    {
        "id": 23479,
        "title": "La bataille pour la gouvernance mondiale de l’alimentation et de l’agriculture",
        "authors": "Frédéric Mousseau, Maurice Hérion",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0061"
    },
    {
        "id": 23480,
        "title": "Design of three-stage power electronic transformer for smart grid",
        "authors": "Jiaqi Li",
        "published": "2021-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceemt52412.2021.9602125"
    },
    {
        "id": 23481,
        "title": "Deep Natural Language Processing",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904"
    },
    {
        "id": 23482,
        "title": "A Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "asmaa Elsaid, ammar mohamed, lamiaa Fattouh, mohamed sakre",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2672691/v1"
    },
    {
        "id": 23483,
        "title": "Detect Turn-takings in Subtitle Streams with Semantic Recall Transformer Encoder",
        "authors": "Yuhai Liang, Qiang Zhou",
        "published": "2020-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp51396.2020.9310512"
    },
    {
        "id": 23484,
        "title": "A  Foveated Vision-Transformer Model for Scene Classification",
        "authors": "Aditya Jonnalagadda, Miguel Eckstein",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1167/jov.22.14.4440"
    },
    {
        "id": 23485,
        "title": "A Simple Resonant Frequency Tracking Technique for DC Transformer Operation",
        "authors": "Yuqi Wei, Alan Mantooth",
        "published": "2020-11-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid48559.2020.9330634"
    },
    {
        "id": 23486,
        "title": "Microwave-assisted Synthesis of TiO2-based Transformer Nanofluid: Investigation on the perspective of Electrical and Thermal  Properties",
        "authors": "G. Koperundevi, S. Raja",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe study on dispersing nanoparticles in transformer oil to elevate their dielectric properties has evolved significantly in the recent decade. Alongside the conventional dispersion techniques (mechanical stirring & ultrasonic homogenization) in practice, this work experiments with the microwave energy of 2.45 GHz frequency radiated onto the TiO2 nanoparticle-based nanofluid (TNF). The electrical and thermal properties of TNF (TiO2 nanoparticle & surfactant dispersed in transformer mineral oil) are experimentally investigated & presented in this article. Effective synthesis procedure to enhance the dielectric properties with good dispersivity has been recognized from the superlative combination of dispersion techniques. Its effectiveness in enhancing the electrical and thermal properties is investigated by verifying its dielectric breakdown strength, dielectric constant (εr), dielectric dissipation factor (tan δ), and flash & fire point. Results justified that the TNF synthesized by combining the processes of stirring, Ultrasonic homogenization, and microwave irradiation in a rational sequence exhibited better electrical & unaltered thermal properties when compared with samples prepared through stirring and sonication. TNF prepared through microwave synthesis improved the AC breakdown voltage (BDV) by 16.92% more than TNF prepared without microwave synthesis. Hence this could be an effective route to prepare the TNF with improved electrical properties.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1860339/v1"
    },
    {
        "id": 23487,
        "title": "Transformer-Based Object Detection in Drone Images Using Split Attention Module: Pvsamnet",
        "authors": "Sirisha Museboyina, Sudha S.V",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4250755"
    },
    {
        "id": 23488,
        "title": "Mdtnet: Partial Transformer with Degradation-Aware Module for Restoring Old Photos with Multiple Degradations",
        "authors": "Yuan Zhao, Cao Liqin, Fan Zhang, Yanfei Zhong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670346"
    },
    {
        "id": 23489,
        "title": "Vision Transformer and Parallel Convolutional Neural Network for Speech Emotion Recognition",
        "authors": "Saber Hashemi, Mohammad Asgari",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334797"
    },
    {
        "id": 23490,
        "title": "Automated Anomalous Child Repetitive Head Movement Identification through Transformer Networks",
        "authors": "Nushara Deshith Wedasingha, Pradeepa Samarasinghe, Lasantha Seneviratne, Michela Papndrea, Alessandro Puiatti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe exponential growth of behavioral disorders among human beings is an increasing concern in the medical community due to a lack of medical resources for early identification of atypical behaviors. Many psychological journals which address behavioral disorders indicate that the most prominent way of puzzling out this problem is to identify behavioral disorder characteristics such as repetitive behaviors in early childhood, and recover through therapy. Even though there are many standards presently used in diagnosing behavioral disorder characteristics, due to a lack of facilities and limited number of professionals in the relevant fields, these traditional standards have failed to cater the increasing number of cases with behavioral disorders. Hence, the need for developing automated approaches to overcome the problems in current systems of diagnosing children with behavioral disorders has arisen from the research community. Therefore, the purpose of this study is to develop an automated model that analyzes a video to distinguish typical, and atypical repetitive head movements of children while using different learning methods to mitigate issues that affect the performance of the model due to the scarcity of child datasets. In this work, we present a fusion of transformer networks, and Non-deterministic Finite Automata (NFA) techniques, which classify repetitive head movements of a child as typical or atypical based on the analysis of gender, age, the type of the repetitive head movement along with count, duration, and frequency of each repetitive head movement. Different transfer learning methods were experimented to enhance the performance of the model. The experimental results on five datasets: NIR face dataset, Bosphorus 3D face dataset, ASD Dataset, SSBD dataset, and head movements in the wild dataset, corroborate our proposed model has outperformed many state-of-the-art frameworks when distinguishing typical and atypical repetitive head movements of children.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2534541/v1"
    },
    {
        "id": 23491,
        "title": "Adoption of Response Surface Methodology for Optimization of Benzotriazole Additive in Ester Fluids as Transformer Insulant",
        "authors": "Amalanathan A J, Sarathi R",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper reports the critical investigation on the streaming current of ester fluids towards power transformer. The optimization of benzotriazole (BTA) additive is tested based on the streaming current analysis considering its influence with and without the temperature. From the experimental results, the BTA concentration of 130 ppm on ester fluids observed a negative streaming current. The influence of temperature increases the streaming current of ester fluids. Only a minimal change is noticed in the viscosity of ester fluids with the addition of BTA but to the contradict, an increased dissipation factor is observed. The Response surface methodology (RSM) algorithm is being utilized as an optimization algorithm to identify the permissible level of BTA under transformer standstill and normal operating temperature conditions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2633994/v1"
    },
    {
        "id": 23492,
        "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
        "authors": "Timo Lohrenz, Zhengyang Li, Tim Fingscheidt",
        "published": "2021-8-30",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-555"
    },
    {
        "id": 23493,
        "title": "Dual-Transformer Inductor-Less LLC Resonant Converter Topology",
        "authors": "Haitham Kanakri, Euzeli Cipriano Dos Santos",
        "published": "2022-3-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/peci54197.2022.9744050"
    },
    {
        "id": 23494,
        "title": "IAFormer: A Transformer Network for Image Aesthetic Evaluation and Cropping",
        "authors": "Lei Wang, Yue Jin",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acait56212.2022.10137804"
    },
    {
        "id": 23495,
        "title": "Instance Segmentation Combined CMT and Swin Transformer in Driving Scenes",
        "authors": "Zhengyi Zha",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135453"
    },
    {
        "id": 23496,
        "title": "Deforestation Detection in the Brazilian Amazon Using Transformer-based Networks",
        "authors": "Mariam Alshehri, Anes Ouadou, Grant Scott",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00130"
    },
    {
        "id": 23497,
        "title": "Front Matter",
        "authors": "",
        "published": "2021-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119800194.fmatter"
    },
    {
        "id": 23498,
        "title": "A new inrush current identification algorithm based on transformer core saturation",
        "authors": "Ming Jin, Yuanlong Liu",
        "published": "2017-7",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274695"
    },
    {
        "id": 23499,
        "title": "TA-UNet3+: a transformer-based method for kidney tumor segmentation",
        "authors": "Xiqing Hu",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2678615"
    },
    {
        "id": 23500,
        "title": "Transformer-Based Weakly Supervised 3d Human Pose Estimation",
        "authors": "Xiaoguang Wu, Hujie Xie, Xiaochen Niu, Chen Wang, Zelei Wang, Shiwen Zhang, Yuze Shan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4611139"
    },
    {
        "id": 23501,
        "title": "Experience with Current Transformer Calibration System Based on Rogowski Coil",
        "authors": "Esa-Pekka Suomalainen, Jari Hallstrom",
        "published": "2018-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpem.2018.8501048"
    },
    {
        "id": 23502,
        "title": "Acoustic Multi-Parameter Early Warning Method for Transformer DC Bias State",
        "authors": "Yuhao Zhou, Bowen Wang",
        "published": "2022-4-10",
        "citations": 2,
        "abstract": "The acoustic signal in the operation of a power transformer contains a lot of transformer operation state information, which is of great significance to the detection of DC bias state. In this paper, three typical parameters used for DC bias state detection are selected by comparing the acoustic variation of a 500 kV Jingting transformer substation No. 2 transformer with that of the core model built in the laboratory; then, acoustic samples of the 162 EHV normal state transformers are collected, and the distribution regularity of three typical parameters in normal state is given. Finally, according to the distribution regularity, clear warning threshold of typical parameters are given, and the DC bias cases from the 500 kV Jingting transformer substation are used to verify the effectiveness of the threshold.",
        "link": "http://dx.doi.org/10.3390/s22082906"
    },
    {
        "id": 23503,
        "title": "Piano automatic transcription based on transformer",
        "authors": "Yuan Wang",
        "published": "2023-11-4",
        "citations": 0,
        "abstract": "Recent years, research on automatic music transcription has made significant progress as deep learning techniques have been validated to demonstrate strong performance in complex data applications. Although the existing work is exciting, they all rely on specific domain knowledge to enable the design of model architectures and training modes for different tasks. At the same time, the noise generated in the process of automatic music transcription data collection cannot be ignored, which makes the existing work unsatisfactory. To address the issues highlighted above, we propose an end-to-end framework based on Transformer. Through the encoder-decoder structure, we realize the direct conversion of the spectrogram of the collected piano audio to MIDI output. Further, to remove the impression of environmental noise on transcription quality, we design a training mechanism mixed with white noise to improve the robustness of our proposed model. Our experiments on the classic piano transcription datasets show that the proposed method can greatly improve the quality of automatic music transcription.",
        "link": "http://dx.doi.org/10.3233/jifs-233653"
    },
    {
        "id": 23504,
        "title": "Empowering MBTI Personality Classification through Transformer-Based Summarization Model",
        "authors": "Seif Elmoushy, Mostafa Saeed, Wael H. Gomaa",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217442"
    },
    {
        "id": 23505,
        "title": "Im-to-Lim: A Transformer-Based Framework for Limerick Generation Associated with an Image",
        "authors": "Divyanshi Thapa, Praveen Joe I R, Shajina Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4551886"
    },
    {
        "id": 23506,
        "title": "Broadband Frequency Domain Dielectric Spectroscopy of Cellulose Insulation Material in Transformer",
        "authors": "Shengkang Wang, Fuchang Lin, Hua Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177292"
    },
    {
        "id": 23507,
        "title": "Multimodal Transformer for Bearing Fault Diagnosis: A New Method Based on Frequency-Time Feature Decomposition",
        "authors": "Kai Li, Chen Wang, Haiyan Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBearing fault diagnosis technology enables early detection of faults and prevention of potential equipment failures. To fully utilize signal data, this paper introduces an innovative diagnostic model named Frequency-Time Multimodal Transformer (FTM-Transformer). It constructs two-dimensional frequency feature data and one-dimensional time-domain feature data, and incorporates a multimodal information fusion module comprehensively exploit the distinctive features of different modes of signals, thereby enhancing fault diagnosis accuracy. The FTM-Transformer model utilizes Multivariate Decomposition for Time and Frequency Features (MDTF) as the feature extraction module to analyze vibration signals from multiple sensors. MDTF employs Multivariate Variational Mode Decomposition (MVMD) to decompose the vibration signals into Intrinsic Mode Functions (IMFs) and applies the Discrete Wavelet Transform (DWT) to extract feature maps of these IMFs, while also analyzing statistical time-domain features of the IMFs to form feature vectors. Building upon this foundation, the FTMTransformer module is designed to effectively integrates the frequency-domain feature maps and time-domain feature vectors using a deep Transformer model. Experimental results on the bearing fault dataset from Case Western Reserve University demonstrate that the FTM-Transformer model achieves remarkable diagnostic accuracy, significantly outperforming traditional models such as Vision Transformer (ViT) and ResNet. Furthermore, transfer testing on the gear dataset from Southeast University validates the adaptability of the diagnostic model to other application scenarios, showcasing its superior performance. The proposed MDTF and FTM-Transformer methods offer a novel and effective solution to bearing fault diagnosis, contributing to improved equipment reliability and maintenance efficiency in various industrial applications.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3258385/v1"
    },
    {
        "id": 23508,
        "title": "High-Dimensional Population Flow Time Series Forecasting Via an Interpretable Hierarchical Transformer",
        "authors": "SONGHUA HU, Chenfeng Xiong",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4049754"
    },
    {
        "id": 23509,
        "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
        "authors": "Xiyuan Wang, Fangyuan Wang, Bo Xu, Liang Xu, Jing Xiao",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-697"
    },
    {
        "id": 23510,
        "title": "Enhancing Remote Sensing Semantic Segmentation Through Hybrid Convolutional Neural Network and Transformer",
        "authors": "Zheng Kang, Yu Chen, Jingrong Wang, Jiao Zhan, Nan Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4696209"
    },
    {
        "id": 23511,
        "title": "Load control using sensitivity identification by means of smart transformer",
        "authors": "Giovanni De Carne, Giampaolo Buticchi, Marco Liserre, Costas Vournas",
        "published": "2017-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ptc.2017.7980992"
    },
    {
        "id": 23512,
        "title": "IoT Based Transformer Oil Temperature Monitoring System",
        "authors": "Deba Kumar Mahanta, Iftikar Rahman",
        "published": "2022-4-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscds53736.2022.9760876"
    },
    {
        "id": 23513,
        "title": "End-to-End Speech Translation with the Transformer",
        "authors": "Laura Cross Vila, Carlos Escolano, José A. R. Fonollosa, Marta R. Costa-Jussà",
        "published": "2018-11-21",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/iberspeech.2018-13"
    },
    {
        "id": 23514,
        "title": "Experience Simple Transformer library in solving Mojaz Multi-Topic Labelling Task",
        "authors": "Moataz Ajlouni",
        "published": "2021-5-24",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icics52457.2021.9464602"
    },
    {
        "id": 23515,
        "title": "HLT: A Hierarchical Vulnerability Detection Model Based on Transformer",
        "authors": "Yupan Chen, Zhihong Liu",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdis55630.2022.00015"
    },
    {
        "id": 23516,
        "title": "BERTMHC: Improves MHC-peptide class II interaction prediction with transformer and multiple instance learning",
        "authors": "Jun Cheng, Kaïdre Bendjama, Karola Rittner, Brandon Malone",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractMotivationIncreasingly comprehensive characterisation of cancer associated genetic alteration has paved the way for the development of highly specific therapeutic vaccines. Predicting precisely binding and presentation of peptides by MHC alleles is an important step towards such therapies. Recent data suggest that presentation of both class I and II epitopes is critical for the induction of a sustained effective immune response. However, the prediction performance for MHC class II has been limited compared to class I.ResultsWe present a transformer neural network model which leverages on self-supervised pretraining from a large corpus of protein sequences. We also propose a multiple instance learning (MIL) framework to deconvolve mass spectrometry data where multiple potential MHC alleles may have presented each peptide. We show that pretraining boosted the performance for these tasks. Combining pretraining and the novel MIL approach, our model outperforms state-of-the-art models for both binding and mass spectrometry presentation predictions.AvailabilityOur model is available athttps://github.com/s6juncheng/BERTMHCContactjun.cheng@neclab.eu,brandon.malone@neclab.eu",
        "link": "http://dx.doi.org/10.1101/2020.11.24.396101"
    },
    {
        "id": 23517,
        "title": "Overload Operation of LV-Side Inverter in Smart Transformer",
        "authors": "Rongwu Zhu, Vivek Raveendran, Marco Liserre",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8912607"
    },
    {
        "id": 23518,
        "title": "Operational Performance Evaluation of Phase Shifting Transformer and Unified Power Flow Controller in Power System",
        "authors": "",
        "published": "2020-7-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.37896/sr7.7/055"
    },
    {
        "id": 23519,
        "title": "Transformer-Based Hate Speech Detection in Assamese",
        "authors": "Koyel Ghosh, Debarshi Sonowal, Abhilash Basumatary, Bidisha Gogoi, Apurbalal Senapati",
        "published": "2023-6-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183497"
    },
    {
        "id": 23520,
        "title": "INTRUSION DETECTION MODEL BASED ON IMPROVED TRANSFORMER",
        "authors": "Svitlana Gavrylenko, Vadym Poltoratskyi, Alina Nechyporenko",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "The object of the study is the process of identifying the state of a computer network. The subject of the study are the methods of identifying the state of computer networks. The purpose of the paper is to improve the efficacy of intrusion detection in computer networks by developing a method based on transformer models. The results obtained. The work analyzes traditional machine learning algorithms, deep learning methods and considers the advantages of using transformer models. A method for detecting intrusions in computer networks is proposed. This method differs from known approaches by utilizing the Vision Transformer for Small-size Datasets (ViTSD) deep learning algorithm. The method incorporates procedures to reduce the correlation of input data and transform data into a specific format required for model operations. The developed methods are implemented using Python and the GOOGLE COLAB cloud service with Jupyter Notebook. Conclusions. Experiments confirmed the efficiency of the proposed method. The use of the developed method based on the ViTSD algorithm and the data preprocessing procedure increases the model's accuracy to 98.7%. This makes it possible to recommend it for practical use, in order to improve the accuracy of identifying the state of a computer system.",
        "link": "http://dx.doi.org/10.20998/2522-9052.2024.1.12"
    },
    {
        "id": 23521,
        "title": "Aerial Image Object Detection with Vision Transformer Detector (ViTDet)",
        "authors": "Liya Wang, Alex Tien",
        "published": "2023-7-16",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282836"
    },
    {
        "id": 23522,
        "title": "Long-Term Prediction Model for NOx Emission Based on LSTM–Transformer",
        "authors": "Youlin Guo, Zhizhong Mao",
        "published": "2023-9-18",
        "citations": 1,
        "abstract": "Excessive nitrogen oxide (NOx) emissions result in growing environmental problems and increasingly stringent emission standards. This requires a precise control for NOx emissions. A prerequisite for precise control is accurate NOx emission detection. However, the NOx measurement sensors currently in use have serious lag problems in measurement due to the harsh operating environment and other problems. To address this issue, we need to make long-term prediction for NOx emissions. In this paper, we propose a long-term prediction model based on LSTM–Transformer. First, the model uses self-attention to capture long-term trend. Second, long short-term memory network (LSTM) is used to capture short-term trends and as secondary position encoding to provide positional information. We construct them using a parallel structure. In long-term prediction, experimental results on two real datasets with different sampling intervals show that the proposed prediction model performs better than the currently popular methods, with 28.2% and 19.1% relative average improvements on the two datasets, respectively.",
        "link": "http://dx.doi.org/10.3390/electronics12183929"
    },
    {
        "id": 23523,
        "title": "Underwater Target Detection Algorithm Based on YOLO and Swin Transformer for Sonar Images",
        "authors": "Ruoyu Chen, Shuyue Zhan, Ying Chen",
        "published": "2022-10-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9976986"
    },
    {
        "id": 23524,
        "title": "Transformer maze for the evaluation of the learning and memory in rodents",
        "authors": "Elena Filatova",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2022.e11211"
    },
    {
        "id": 23525,
        "title": "Transformer Scale Gate for Semantic Segmentation",
        "authors": "Hengcan Shi, Munawar Hayat, Jianfei Cai",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00298"
    },
    {
        "id": 23526,
        "title": "Multistep retrosynthesis combining a disconnection aware triple transformer loop with a route penalty score guided tree search",
        "authors": "David Kreutter, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "Computer-aided synthesis planning (CASP) aims to automatically learn organic reactivity from literature and perform retrosynthesis of unseen molecules. CASP systems must learn reactions sufficiently precisely to propose realistic disconnections while avoiding overfitting to leave room for diverse options, and explore possible routes such as to allow short synthetic sequences to emerge. Herein we report an open-source CASP tool proposing original solutions to both challenges. First, we use a triple transformer loop (TTL) predicting starting materials (T1), reagents (T2), and products (T3) to explore various disconnections sites defined by combining exhaustive, template-based and transformer-based tagging procedures. Second, we integrate TTL into a multistep tree search algorithm (TTLA) prioritizing sequences using a route penalty score (RPScore) considering the number of steps, their confidence score, and the simplicity of all intermediates along the route. Our approach favours short synthetic routes to commercial starting materials, as exemplified by retrosynthetic analyses of recently approved drugs.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-8khth-v2"
    },
    {
        "id": 23527,
        "title": "Performance of generative pre-trained Transformer-4 (GPT-4) in RCOG diploma-style questions",
        "authors": "Richard Armitage",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/postmj/qgae038"
    },
    {
        "id": 23528,
        "title": "Stock Price Prediction Based on Temporal Fusion Transformer",
        "authors": "Xiaokang Hu",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlbdbi54094.2021.00019"
    },
    {
        "id": 23529,
        "title": "CAT: Cross Attention in Vision Transformer",
        "authors": "Hezheng Lin, Xing Cheng, Xiangyu Wu, Dong Shen",
        "published": "2022-7-18",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859720"
    },
    {
        "id": 23530,
        "title": "Models of Discourse Representation",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter first describes what has been learned about how readers represent the meaning of discourse by integrating the meanings to individual sentences to construct the representations needed to understand larger segments of text. The chapter reviews the key findings related to text processing and how this sparked an ongoing debate about the extent to which the making of inferences during reading is obligatory. The chapter reviews precursor theories and models of discourse representation that attempt to explain how discourse representations are generated via the interaction of language processing and memory. The chapter then reviews a large, representative sample of the models that have been used to simulate and understand aspects of discourse processing. They are reviewed in their order of development to show how the models have evolved to accommodate new empirical findings. The chapter concludes with an explicit comparative analysis of the discourse-processing models and discusses the empirical findings that each model can and cannot explain.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0005"
    },
    {
        "id": 23531,
        "title": "Models of Word Identification",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter first describes the tasks that are used to study how readers identify printed words (e.g., the lexical-decision task) and then reviews the key empirical findings related to skilled and impaired word identification (i.e., dyslexia). As explained, these findings have both motivated the development of computer models of word identification and been used to evaluate the explanatory adequacy of those models. The chapter then reviews several precursor theories and models of word identification that provide recurring metaphors (e.g., generating word pronunciations via analogy vs. the application of rules) in the development of later, more formally implemented word-identification models. The chapter reviews a large representative sample of these models in the order of their development, to show how the models have evolved in response to empirical research and the need to accommodate new findings (e.g., how the letters in words are perceived in their correct order). The chapter concludes with an explicit comparative analysis of the word-identification models and discussion of the findings that each model can and cannot explain.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0003"
    },
    {
        "id": 23532,
        "title": "Models of Sentence Processing",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter first describes what has been learned about how readers process sentences, using information from individual words in combination with linguistic knowledge to generate larger units of meaning corresponding to phrases and sentences. The chapter then reviews what has been learned about sentence processing using various methods, but most notably, the measurement of readers’ eye movements. The chapter then reviews precursor theories and models of sentence processing—models that provide early attempts to explain how readers construct the meanings of phrases and sentences, and that motivate much of the subsequent research to understand the relative contributions of syntactic versus semantic information in sentence processing. The chapter then reviews a large, representative sample of the models that have been used to simulate and understand various facets of sentence processing. These are presented in their order of development to show how the models have evolved to accommodate new empirical findings. The chapter concludes with an explicit comparative analysis of the sentence-processing models and discussion of the empirical findings that each model can and cannot explain.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0004"
    },
    {
        "id": 23533,
        "title": "Weed Recognition Method based on Hybrid CNN-Transformer Model",
        "authors": "Jun Zhang",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "As an important task in precision agriculture, weed recognition plays a crucial role in crop management and yield increase. However, achieving high accuracy and efficiency at the same time remains a challenge. To address the balance between accuracy and timeliness in weed recognition, this paper proposes a hybrid CNN-Transformer model for weed recognition. The model uses a combination of convolutional neural network (CNN) and Transformer structures for feature extraction and classification, taking into account both global and local information. In addition, the proposed Transformer Block incorporates the SDTA (Segmentation Depth Transpose Attention) mechanism to improve timeliness. Furthermore, this paper improves the original ViT model to enhance its accuracy. Experimental results on the Deep Weeds dataset by Olsen et al. show that the proposed hybrid model outperforms the original Vision Transformer model in weed recognition accuracy (89.43% vs. 96.08%). This research provides an effective solution for weed recognition using a hybrid model, with high practical value and application prospects.",
        "link": "http://dx.doi.org/10.54097/fcis.v4i2.10209"
    },
    {
        "id": 23534,
        "title": "Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning",
        "authors": "Kanagasabai Rajaraman, Saravanan Rajamanickam, Wei Shi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.588"
    },
    {
        "id": 23535,
        "title": "Surprises of the Transformer as a Coupled Oscillator System",
        "authors": "J. P. Silva, A. J. Silvestre",
        "published": "2022-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003187103-3"
    },
    {
        "id": 23536,
        "title": "Persian Sentiment Analysis via a Transformer Model concerning Banking Sector",
        "authors": "Seyed Jamal Haddadi, Elham Khoeini, Pezhman Salmani, Mehdi Beygi, Mehrdad Haddad Khoshkar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe competitive landscape of a country's banking sector necessitates an in-depth understanding of customer satisfaction levels concerning the services provided. Presently, customers predominantly express their feedback via social media platforms in the form of posts and comments. This study endeavors to create a highly accurate sentiment detection algorithm for the Iranian banking system, utilizing a transformer model. In the initial stages, we collected data by crawling comments from Twitter, which are subsequently labeled and filtered according to the names of Iranian banks, dating from 2019. Following this, an optimized Deep Neural Network (DNN)-based pre-trained ParsBERT model, a monolingual Persian model, is fine-tuned using this data. Finally, our model is evaluated on a test dataset, and the results are validated by comparing them with the original multilingual BERT, Bidirectional Long Short-Term Memory (Bi-LSTM) network, and four other classification methods. To address the Out-Of-Vocabulary (OOV) issue, a character-level embedding is incorporated in conjunction with the word-level embedding. This approach aids in tackling the multitude of variations observed in non-native words, extracting character-level features using a character-level Bi-LSTM. The proposed model highlights the statistical superiority of our method when compared to the other methods evaluated.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3211293/v1"
    },
    {
        "id": 23537,
        "title": "Simulation of nanofluid as a two-phase flow in a distribution transformer",
        "authors": "Leyla Raeisian, Peter Werle, Hamid Niazmand",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic43217.2019.9046609"
    },
    {
        "id": 23538,
        "title": "Conductivity of transformer oil under high-frequency voltage",
        "authors": "Yuli Rodiah, T. Haryono,  Suharyanto",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichveps.2017.8225938"
    },
    {
        "id": 23539,
        "title": "Supervisory Control for a Switched Mode Hydraulic Transformer",
        "authors": "Sangyoon Lee, Perry Y. Li",
        "published": "2018-9-12",
        "citations": 1,
        "abstract": "A supervisory control for a hydraulic transformer is developed. The hydraulic transformer being controlled is configured in a traditional manner where a pair of hydraulic pump/motors are mechanically coupled together. This transformer can be configured in three distinct modes depending on how each port is connected. A supervisory control determines, for the desired output pressure and output flow, the mode and shaft speed that the transformer should operate in order to minimize the power loss. The resulting controller structure ensures that the transformer provides the desired flow while following the desired mode and shaft speed. The supervisory control is further modified to avoid high frequency switching and to achieve bumpless transfer between modes. Experimental results demonstrate the efficacy of the supervisory controller to increase the efficiency of the hydraulic transformer driven system.",
        "link": "http://dx.doi.org/10.1115/fpmc2018-8935"
    },
    {
        "id": 23540,
        "title": "Short-Term Load Forecasting Based on Ceemdan and Transformer",
        "authors": "Peng Ran, Kun Dong, Xu Liu, Jing Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4174825"
    },
    {
        "id": 23541,
        "title": "Frequency Response Modelling of Transformer Windings Connected in Parallel",
        "authors": "Szymon Banaszak, Konstanty Marek Gawrylczyk, Katarzyna Trela",
        "published": "2020-3-17",
        "citations": 8,
        "abstract": "This paper describes the approach to the frequency response modelling of transformer windings consisting of coils connected in parallel. At present, computer models are intensively developed with the aim of simulating the influence of faults on the frequency response of the active part of power transformers. Frequency response analysis (FRA) is one of the standard methods used for the assessment of the mechanical condition of a transformer’s windings and core. The interpretation of the FRA results is crucial in the diagnostics of the active part of the transformer. Proper simulations of the FRA results allow the improvement and simplification of the interpretation process of the windings’ faults. Usually only serial winding wires are simulated in computer modelling and parallel wires are simplified, leading to simulation inaccuracies. In this work, a combined electromagnetic field/network method, which includes parallel connections of the coils, is proposed. The method is based on lumped RLC elements. The results of the analysis conducted by the computer model are referred to as the real transformer measurement. The modelling was also performed for the case of a winding with a fault. The results of modelling were assessed with four numerical indices used for FRA interpretation.",
        "link": "http://dx.doi.org/10.3390/en13061395"
    },
    {
        "id": 23542,
        "title": "High-Frequency Oscillation of the Active-Bridge-Transformer-Based DC/DC Converter",
        "authors": "Shusheng Wei, Wusong Wen",
        "published": "2022-5-2",
        "citations": 2,
        "abstract": "The dual-active-bridge converter (DAB) has attracted tremendous attention in recent years. However, its EMI issues, especially the high-frequency oscillation (HFO) induced by the dv/dt and parasitic elements of the transformer, are significant challenges. The multi-active-bridge converter (MAB) based on the multi-winding transformer also faces similar problems, which are even more complicated. This article investigates the HFO of active-bridge-transformer-based DC/DC converters including DAB and MAB. Firstly, the general HFO model is studied using the analysis of the AC equivalent circuit considering the asymmetrical parameters. Ignoring the AC resistance in the circuit, the high-order model of the voltage oscillation could be reduced to a second-order system. Based on the simplified model, the oscillation voltage generated by an active bridge is analyzed in the time domain. Then, a universal active voltage-oscillation-suppression method-selected harmonic-elimination phase-shift (SHE PS) modulation method is proposed. The impacts of the system parameters on the method are also revealed. The experimental results show the excellent performance of the proposed active suppression method, with voltage spike amplitude (VSA) reductions of 92.1% and 77.8% for the DAB and MAB prototypes, respectively.",
        "link": "http://dx.doi.org/10.3390/en15093311"
    },
    {
        "id": 23543,
        "title": "Single Stage Transformer Less Reconfigurable Inverter for PV Applications",
        "authors": "E Amina, K Muhammedali Shafeeque",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icirca.2018.8597301"
    },
    {
        "id": 23544,
        "title": "Re_Trans: Combined Retrieval and transformer model for Source Code Summarization",
        "authors": "Chunyan Zhang, Qinglei Zhou, Meng Qiao, Ke Tang, Lianqiu Xu, Fudong Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nSource code summarization (SCS) refers to the natural language description on what a source code is performing. It can help developers understand programs and maintain software efficiently. Retrieval methods (RM) generate SCS reorganize terms selected from source code or use SCS of similar code snippets. Generative methods (GM) generate SCS via attentional encoder-decoder architecture. However, a GM can generate SCS for any code, but sometime the accuracy is still far from expectation (due to the lack of numerous high-quality training set). A RM is considered with a higher accuracy, but usually fail to generate SCS for a source code in the absence of a similar candidate in the database. In order to effectively combine the advantages of RM and GM, we propose a new method Re_Trans. For a given code, we first utilize RM to get the most similar code w.r.t sematic and its SCS (S_RM). Then, we input the given code and similar code into the trained discriminator. If the discriminator outputs 1, we take S_RM as the result, otherwise, we utilize the GM, transformer, to generate the given code’ SCS. Particularly, we use AST-augmented and code sequence-augmented information to make the source code semantic extraction more fully. Furthermore, we build a new SCS retrieval library through the public dataset. We evaluate our method on a data set of 2.1 million Java code-comment pairs, and experimental results show improvement over the state-of-the-art (SOTA) benchmarks, which demonstrates the effectiveness and efficiency of our method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1742575/v1"
    },
    {
        "id": 23545,
        "title": "Shear Mode Polarity Inverted ScAlN Multilayer for Application to BAW Transformer in Rectifying Antenna",
        "authors": "Sarina Kinoshita, Takahiko Yanagitani",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2019.8925674"
    },
    {
        "id": 23546,
        "title": "Shear Mode Polarity Inverted ScAlN Multilayer for Application to BAW Transformer in Rectifying Antenna",
        "authors": "Sarina Kinoshita, Takahiko Yanagitani",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2019.8926197"
    },
    {
        "id": 23547,
        "title": "Disturbance Property of High-frequency Transformer Model for Photovoltaic Applications",
        "authors": "Duc-Thanh Do, Holger Hirsch",
        "published": "2021-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie45552.2021.9576195"
    },
    {
        "id": 23548,
        "title": "Transformer Parameter Monitoring and Protection System Based on Arduino",
        "authors": "Najuka Jawale, Prasad Kumbhar, Ganesh Kurle, A.A. Shinde",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3418443"
    },
    {
        "id": 23549,
        "title": "Thermal evaluation of railway transformer used in autotransformer feeding systems",
        "authors": "Mingyu Han, James Hill, Zhongdong Wang, Peter Crossley",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/energycon.2018.8398739"
    },
    {
        "id": 23550,
        "title": "Index",
        "authors": "",
        "published": "2021-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119800194.index"
    },
    {
        "id": 23551,
        "title": "MMGT: Multimodal Graph-Based Transformer for Pain Detection",
        "authors": "Kevin Feghoul, Deise Santana Maia, Mohamed Daoudi, Ali Amad",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10290098"
    },
    {
        "id": 23552,
        "title": "Oil Movement in Closed Environment of Distribution Transformer Tank Problem Simulation",
        "authors": "Yurii Baidak, Valentin Matukhno, Belikova Liudmila",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3201122"
    },
    {
        "id": 23553,
        "title": "Multi-image transformer for multi-focus image fusion",
        "authors": "Levent Karacan",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.image.2023.117058"
    },
    {
        "id": 23554,
        "title": "Un nouveau financement pour transformer la recherche et l'innovation en matière de science des données",
        "authors": "Makoni Munyaradzi",
        "published": "2021-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/d44148-021-00112-2"
    },
    {
        "id": 23555,
        "title": "EMI Characteristic Analysis of Plane Transformer Based on LLC Circuit",
        "authors": "Xingwang Yu, Subin Lin",
        "published": "2021-11-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/peas53589.2021.9628727"
    },
    {
        "id": 23556,
        "title": "Improving Transformer with Dynamic Convolution and Shortcut for Video-Text Retrieval",
        "authors": "",
        "published": "2022-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.07.016"
    },
    {
        "id": 23557,
        "title": "Predicting Conversation Outcomes Using Multimodal Transformer",
        "authors": "Can Li, Wenbo Wang, Bitty Balducci, Detelina Marinova, Yi Shang",
        "published": "2021-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533935"
    },
    {
        "id": 23558,
        "title": "An Efficient Vision Transformer Model for PCB Component Classification",
        "authors": "Cagri Can Surmeli, Hazim Kemal Ekenel",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10224039"
    },
    {
        "id": 23559,
        "title": "MeViT: Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand",
        "authors": "Teerapong Panboonyuen, Chaiyut Charoenphon, Chalermchon Satirapod",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSemantic segmentation is a fundamental task in remote sensing image analysis that aims to classify each pixel in an image into different land use and land cover (LULC) segmentation task. In this paper, We propose MeViT (Medium-Resolution Vision Transformer) on Landsat satellite imagery for the main economic crops in Thailand as follows: (i) para rubber, (ii) corn, and (iii) pineapple. Therefore, our proposed MeViT enhances Vision Transformers (ViTs), one of the modern deep learning on computer vision tasks, to learn semantically rich and spatially-precise multi-scale representations by integrating medium-resolution multi-branch architectures with ViTs. We revised mixed-scale convolutional feedforward networks (MixCFN) by incorporating multiple depth-wise convolution paths to extract multi-scale local information to balance the model’s performance and efficiency. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on the publicly available dataset of Thailand scenes and compare the results with several state-of-the-art deep learning methods such as AutoDeeplab1, SwinTransformer2, Twins3, CSWinTransformer4, SegFormer5, and HRViT6. The experimental results demonstrate that our proposed MeViT outperforms existing methods and performs better in the semantic segmentation of Thailand scenes The evaluation metrics used are precision, recall, F1 score, and mean Intersection over Union (IoU). Among the models compared, MeViT, our proposed model, achieves the best performance in all evaluation metrics. MeViT achieves a precision of 0.9222, a recall of 0.9469, an F1 score of 0.9344, and a mean IoU of 0.8363. These results demonstrate the effectiveness of our proposed approach in accurately segmenting Thai Landsat-8 data. The achieved F1 score of overall using our proposed MeViT is 93.44% that is a major significance of this work.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2945208/v1"
    },
    {
        "id": 23560,
        "title": "Accelerating COVID-19 research with graph mining and transformer-based learning",
        "authors": "Ilya Tyagin, Ankit Kulshrestha, Justin Sybrandt, Krish Matta, Michael Shtutman, Ilya Safro",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTIn 2020, the White House released the, “Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset,” wherein artificial intelligence experts are asked to collect data and develop text mining techniques that can help the science community answer high-priority scientific questions related to COVID-19. The Allen Institute for AI and collaborators announced the availability of a rapidly growing open dataset of publications, the COVID-19 Open Research Dataset (CORD-19). As the pace of research accelerates, biomedical scientists struggle to stay current. To expedite their investigations, scientists leverage hypothesis generation systems, which can automatically inspect published papers to discover novel implicit connections. We present an automated general purpose hypothesis generation systems AGATHA-C and AGATHA-GP for COVID-19 research. The systems are based on graph-mining and the transformer model. The systems are massively validated using retrospective information rediscovery and proactive analysis involving human-in-the-loop expert analysis. Both systems achieve high-quality predictions across domains (in some domains up to 0.97% ROC AUC) in fast computational time and are released to the broad scientific community to accelerate biomedical research. In addition, by performing the domain expert curated study, we show that the systems are able to discover on-going research findings such as the relationship between COVID-19 and oxytocin hormone.ReproducibilityAll code, details, and pre-trained models are available at https://github.com/IlyaTyagin/AGATHA-C-GPCCS CONCEPTS• Applied computing → Bioinformatics; Document management and text processing; • Computing methodologies → Learning latent representations; Neural networks; Information extraction; Semantic networks.",
        "link": "http://dx.doi.org/10.1101/2021.02.11.430789"
    },
    {
        "id": 23561,
        "title": "P02 - Simulation, Manufacturing and Evaluation of a Transformer Eddy-Current Sensor for Deep-Drawing Processes",
        "authors": "S. Kamrani, F. Dencker, R. Ottermann, M. Wurz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5162/smsi2023/p02"
    },
    {
        "id": 23562,
        "title": "Hybrid Smart Converter Transformer for HVDC with Advanced Grid Support",
        "authors": "Moazzam Nazir, Johan H. Enslin",
        "published": "2020-11-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid48559.2020.9330631"
    },
    {
        "id": 23563,
        "title": "Copula-Based Transformer in Stereoscopic EEG Data to Assess Visual Discomfort",
        "authors": "Yawen Zheng, Xiaojie Zhao, Li Yao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3986909"
    },
    {
        "id": 23564,
        "title": "Named entity recognition of power communication planning based on transformer",
        "authors": "Tong Weiyue",
        "published": "2022-6-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic54216.2022.9836600"
    },
    {
        "id": 23565,
        "title": "A Study on Design of Microstrip Linear Tapered Line Impedance Transformer Using FFT",
        "authors": "Taisei Urakami, Yusuke Kusama",
        "published": "2020-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc47863.2020.9331565"
    },
    {
        "id": 23566,
        "title": "Emotion-aware and Intent-controlled Empathetic Response Generation using Hierarchical Transformer Network",
        "authors": "Tulika Saha, Sophia Ananiadou",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892592"
    },
    {
        "id": 23567,
        "title": "Land Use Classification Efficient Vision Transformer",
        "authors": "Arthur C. Depoian, Colleen P. Bailey, Parthasarathy Guturu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282974"
    },
    {
        "id": 23568,
        "title": "Learn Dynamic Facial Motion Representations Using Transformer Encoder",
        "authors": "Zheng Sun, Andrew Sumsion, Shad Torrie, Dah-Jye Lee",
        "published": "2022-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ietc54973.2022.9796917"
    },
    {
        "id": 23569,
        "title": "HIGHLY SECURED TRANSFORMER LOAD SHARING SYSTEM USING MICROCONTROLLER",
        "authors": "Mr.S.SARATH KUMAR",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "Abstract-The transformer is a static device, which converts power from one level to another level. The aim of the paper is to protect the transformer under overload condition by load sharing. Due to overload on transformer, the efficiency drops and windings get overheated and may get burnt. Thus by sharing load on transformer, the transformer is protected. This will be done by connecting another transformer in parallel through a micro-controller. The microcontroller compares the load on the first transformer with a reference value. When the load exceeds the reference value, the second transformer will share the extra load. Therefore, the two transformer work efficiently and damage is prevented. In this project three modules are used to control the load currents. The first module is a sensing unit, which is used to sense the current of the load and the second module is a control unit. The last module is microcontroller unit and it will read the analogue signal and perform some calculation and finally gives control signal to a relay. A GSM modem is also used to inform the control station about switching. The advantages of the paper are transformer protection, uninterrupted power supply, and short circuit protection. When designing low-voltage power system to the supply large load currents, paralleled lower-current modules are often preferred over a single, large power converter for several reasons. These include the efficiency of designing and manufacturing standard modular converters which can be combined in any number necessary to meet a given load requirement and the enhanced reliability gained through redundancy. Key Word: Transformer, Load Sharing, Microcontroller, Relay Driver",
        "link": "http://dx.doi.org/10.55041/ijsrem25576"
    },
    {
        "id": 23570,
        "title": "Proposing a New Formulation for Determining Economic Utilization Factor of Distribution Transformer",
        "authors": "Sina Kowsari Movahed, Reza Abedini",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epdc.2019.8903657"
    },
    {
        "id": 23571,
        "title": "\"TRANSFORMER LANGUAGE MODEL-BASED MOBILE LEARNING SOLUTION FOR HIGHER EDUCATION\"",
        "authors": "",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33965/es_ml2023_202302l048"
    },
    {
        "id": 23572,
        "title": "CMOS-Integrated High-Voltage Transformer for a Galvanically Isolated DC-DC Converter",
        "authors": "Patrick Meehan, Karl Rinne",
        "published": "2022-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/issc55427.2022.9826217"
    },
    {
        "id": 23573,
        "title": "Improvement and Design of 66KV GIS Substation Voltage Transformer Based on Electric Thermal Storage",
        "authors": "",
        "published": "2022-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i2.354"
    },
    {
        "id": 23574,
        "title": "A Structural Engineer’s Perspective on Transformer Installations",
        "authors": "Daniel S. Cuffman, Benjamin A. Roberts",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784484463.005"
    },
    {
        "id": 23575,
        "title": "Image KeyPoints Detector Based On Transformer Of Wavelet Transform",
        "authors": "Zhenyu Wu, Yue Lu",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248774"
    },
    {
        "id": 23576,
        "title": "Effectively Managing Today’s Transformer Challenges for Increased Asset Reliability &amp; Sustainability",
        "authors": "Traci Hopkins",
        "published": "2023-4-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gridedge54130.2023.10102732"
    },
    {
        "id": 23577,
        "title": "Reagent Prediction with a Molecular Transformer Improves Reaction Data Quality",
        "authors": "Mikhail Andronov, Varvara Voinarovska, Natalia Andronova, Michael Wand, Djork-Arné Clevert, Jürgen Schmidhuber",
        "published": "No Date",
        "citations": 0,
        "abstract": "Automated synthesis planning is key for efficient generative chemistry. Since reactions of given reactants may yield different products depending on conditions such as the chemical context imposed by specific reagents, computer-aided synthesis planning should benefit from recommendations of reaction conditions. Traditional synthesis planning software, however, typically proposes reactions without specifying such conditions, relying on human organic chemists who know the conditions to carry out suggested reactions. In particular, reagent prediction for arbitrary reactions, a crucial aspect of condition recommendation, has been largely overlooked in cheminformatics until lately. Here we employ the Molecular Transformer, a state-of-the-art model for reaction prediction and single-step retrosynthesis, to tackle this problem. We train the model on the US patents dataset (USPTO) and test it on Reaxys to demonstrate its out-of-distribution generalization capabilities. Our reagent prediction model also improves the quality of product prediction: the Molecular Transformer is able to substitute the reagents in the noisy USPTO data with reagents that enable product prediction models to outperform those trained on plain USPTO. This allows to improve upon the state-of-the-art in reaction product prediction on the USPTO MIT benchmark.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-sn2kr"
    },
    {
        "id": 23578,
        "title": "Bag of Tricks for Optimizing Transformer Efficiency",
        "authors": "Ye Lin, Yanyang Li, Tong Xiao, Jingbo Zhu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.357"
    },
    {
        "id": 23579,
        "title": "Convolutional neural network and vision transformer for image classification",
        "authors": "Jiaqi Lu",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Visual Transformer (ViT) has been a hot topic for research for the past few years after it first emerged in the field. On image recognitions, due to the amount of information ViT could retrieve from the source image, in cases it can rival the traditionally prevailing Convolutional Neural Network (CNN). Then there emerged different models based on ViT, all being built having a specific field or a flaw not addressed by original ViT in mind. In this paper these models are being tested on the same dataset along with a standard CNN to see how they perform compare to each other, and the best performing ViT model was then changed to see how there would be some possible improvements.",
        "link": "http://dx.doi.org/10.54254/2755-2721/5/20230542"
    },
    {
        "id": 23580,
        "title": "Generating Face Images Using VQGAN and Sparse Transformer",
        "authors": "Dong-Hyuck Im, Yong-Seok Seo",
        "published": "2021-10-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc52510.2021.9621202"
    },
    {
        "id": 23581,
        "title": "Vision Grid Transformer for Document Layout Analysis",
        "authors": "Cheng Da, Chuwei Luo, Qi Zheng, Cong Yao",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01783"
    },
    {
        "id": 23582,
        "title": "Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation",
        "authors": "Leyla Khaleghi, Joshua Marshall, Ali Etemad",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9955633"
    },
    {
        "id": 23583,
        "title": "An Improved Mobile Calibration Standard for Transformer Loss Measurement Systems",
        "authors": "H. Cayer, B. Ayhan, T. Kefeli",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpem.2018.8500822"
    },
    {
        "id": 23584,
        "title": "Drug-target interaction prediction using a multi-modal transformer network demonstrates high generalizability to unseen proteins",
        "authors": "Alexander Kroll, Sahasra Ranjan, Martin J. Lercher",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTMost drugs are small molecules, with their activities typically arising from interactions with protein targets. Accurate predictions of these interactions could greatly accelerate pharmaceutical research. Current machine learning models designed for this task have a limited ability to generalize beyond the proteins used for training. This limitation is likely due to a lack of information exchange between the protein and the small molecule during the generation of the required numerical representations. Here, we introduce ProSmith, a machine learning framework that employs a multimodal Transformer Network to simultaneously process protein amino acid sequences and small molecule strings in the same input. This approach facilitates the exchange of all relevant information between the two types of molecules during the computation of their numerical representations, allowing the model to account for their structural and functional interactions. Our final model combines gradient boosting predictions based on the resulting multimodal Transformer Network with independent predictions based on separate deep learning representations of the proteins and small molecules. The resulting predictions outperform all previous models for predicting drug-target interactions, and the model demonstrates unprecedented generalization capabilities to unseen proteins. We further show that the superior performance of ProSmith is not limited to drug-target interaction predictions, but also leads to improvements in other protein-small molecule interaction prediction tasks, the prediction of Michaelis constantsKMof enzyme-substrate pairs and the identification of potential substrates for enzymes. The Python code provided can be used to easily implement and improve machine learning predictions of interactions between proteins and arbitrary drug candidates or other small molecules.",
        "link": "http://dx.doi.org/10.1101/2023.08.21.554147"
    },
    {
        "id": 23585,
        "title": "Design of Power Transformer Fault Diagnosis System Based on Wavelet Transform",
        "authors": "Wei Lu, Bo Ma",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnetic59568.2023.00087"
    },
    {
        "id": 23586,
        "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
        "authors": "Zhifu Gao, ShiLiang Zhang, Ian McLoughlin, Zhijie Yan",
        "published": "2022-9-18",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-9996"
    },
    {
        "id": 23587,
        "title": "TransInpaint: Transformer-based Image Inpainting with Context Adaptation",
        "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00092"
    },
    {
        "id": 23588,
        "title": "Human motion detection based on Transformer spatiotemporal feature fusion",
        "authors": "Cai Wei Wei, Gong Xun",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cis58238.2022.00036"
    },
    {
        "id": 23589,
        "title": "Enhancing Missing Values Imputation through Transformer-Based Predictive Modeling",
        "authors": "Ayub Hina, Jamil Harun",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "This paper tackles the vital issue of missing value imputation in data preprocessing, where traditional techniques like zero, mean, and KNN imputation fall short in capturing intricate data relationships. This often results in suboptimal outcomes, and discarding records with missing values leads to significant information loss. Our innovative approach leverages advanced transformer models renowned for handling sequential data. The proposed predictive framework trains a transformer model to predict missing values, yielding a marked improvement in imputation accuracy. Comparative analysis against traditional methods—zero, mean, and KNN imputation—consistently favors our transformer model. Importantly, LSTM validation further underscores the superior performance of our approach. In hourly data, our model achieves a remarkable R2 score of 0.96, surpassing KNN imputation by 0.195. For daily data, the R2 score of 0.806 outperforms KNN imputation by 0.015 and exhibits a notable superiority of 0.25 over mean imputation. Additionally, in monthly data, the proposed model’s R2 score of 0.796 excels, showcasing a significant improvement of 0.1 over mean imputation. These compelling results highlight the proposed model’s ability to capture underlying patterns, offering valuable insights for enhancing missing values imputation in data analyses.",
        "link": "http://dx.doi.org/10.61927/igmin140"
    },
    {
        "id": 23590,
        "title": "Transformer-based Dynamic Fusion Clustering Network",
        "authors": "Chunchun Zhang, Yaliang Zhao, Jinke Wang",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.109984"
    },
    {
        "id": 23591,
        "title": "Locational marginal price forecasting using Transformer-based deep learning network",
        "authors": "Shengyi Liao, Zhuo Wang, Yao Luo, Haiyan Liang",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549619"
    },
    {
        "id": 23592,
        "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
        "authors": "Jindřich Libovický, Jindřich Helcl, David Mareček",
        "published": "2018",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6326"
    },
    {
        "id": 23593,
        "title": "Scale-Aware Modulation Meet Transformer",
        "authors": "Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00553"
    },
    {
        "id": 23594,
        "title": "S2 Transformer for Image Captioning",
        "authors": "Pengpeng Zeng, Haonan Zhang, Jingkuan Song, Lianli Gao",
        "published": "2022-7",
        "citations": 20,
        "abstract": "Transformer-based architectures with grid features represent the state-of-the-art in visual and language reasoning tasks, such as visual question answering and image-text matching. However, directly applying them to image captioning may result in spatial and fine-grained semantic information loss. Their applicability to image captioning is still largely under-explored. Towards this goal, we propose a simple yet effective method, Spatial- and Scale-aware Transformer (S2 Transformer) for image captioning. Specifically, we firstly propose a Spatial-aware Pseudo-supervised (SP) module, which resorts to feature clustering to help preserve spatial information for grid features. Next, to maintain the model size and produce superior results, we build a simple weighted residual connection, named Scale-wise Reinforcement (SR) module, to simultaneously explore both low- and high-level encoded features with rich semantics. Extensive experiments on the MSCOCO benchmark demonstrate that our method achieves new state-of-art performance without bringing excessive parameters compared with the vanilla transformer. The source code is available at https://github.com/zchoi/S2-Transformer",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/224"
    },
    {
        "id": 23595,
        "title": "ASR Error Correction with Augmented Transformer for Entity Retrieval",
        "authors": "Haoyu Wang, Shuyan Dong, Yue Liu, James Logan, Ashish Kumar Agrawal, Yang Liu",
        "published": "2020-10-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1753"
    },
    {
        "id": 23596,
        "title": "MOFTransformer: A Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-Organic Frameworks",
        "authors": "Yeonghun Kang, Hyunsoo Park, Berend Smit, Jihan Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this work, we introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million hypothetical MOFs. The multi-modal model uses an integrated atom-based graph and energy-grid embeddings to capture both the local and global features of the MOFs, respectively. By fine-tuning the pre-trained model with small datasets (from 5,000 to 20,000), our model outperforms all other machine learning models across various properties that include gas adsorption, diffusion, electronic properties, and even text mined data. Beyond its universal transfer learning capabilities, MOFTransformer generates chemical insight by analyzing feature importance from attention scores within the self-attention layers. As such, this model can serve as a bedrock platform for other MOF researchers that seek to develop new machine learning models for their work.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-hcjzc"
    },
    {
        "id": 23597,
        "title": "Peer Review #1 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.1)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.1/reviews/1"
    },
    {
        "id": 23598,
        "title": "Peer Review #2 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.3)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.3/reviews/2"
    },
    {
        "id": 23599,
        "title": "Distributed Spatial Transformer for Object Tracking in Multi-Camera",
        "authors": "Sio-Kei Im, Ka-Hou Chan",
        "published": "2023-2-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icact56868.2023.10079540"
    },
    {
        "id": 23600,
        "title": "Semi-Autoregressive Transformer for Image Captioning",
        "authors": "Yuanen Zhou, Yong Zhang, Zhenzhen Hu, Meng Wang",
        "published": "2021-10",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00350"
    },
    {
        "id": 23601,
        "title": "Mat-Transformer-Based State Prediction Method for Information Equipment",
        "authors": "Min Wang, chen zheng, Kun Wang, zhigang chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4055751"
    },
    {
        "id": 23602,
        "title": "Experimental Validation of a Transformer-less Inverter with Improved Gain for Grid-PV Interface",
        "authors": "Omkar Patkar, Mini Rajeev",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec47332.2019.9034810"
    },
    {
        "id": 23603,
        "title": "Matrix Converter-Based Current Predictive Control of Novel Power Electronic Transformer",
        "authors": "Yougui Guo, Bowen Yang, Liyang Duan, Wenlang Deng",
        "published": "2021-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54647/energy48103"
    },
    {
        "id": 23604,
        "title": "Line Loss Management for Transformer Courts Based on Data Visualization",
        "authors": "Huo Na, Wang Sheng, Wang Wei",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced.2018.8592352"
    },
    {
        "id": 23605,
        "title": "On-Chip Spiral Transformer",
        "authors": "",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811255366_0003"
    },
    {
        "id": 23606,
        "title": "T3srs: Tensor Train Transformer for Compressing Sequential Recommender Systems",
        "authors": "Hao Li, Jianli Zhao, Huan Huo, Sheng Fang, Jianjian Chen, Lutong Yao, Yiran Hua",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4408354"
    },
    {
        "id": 23607,
        "title": "Diagnostics of Transformer Oils Using the Multiple Linear Regression Model",
        "authors": "Oleg Shutenko, Serhii Ponomarenko",
        "published": "2020-9-21",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paep49887.2020.9240875"
    },
    {
        "id": 23608,
        "title": "Design and software implementation of solid state transformer",
        "authors": "Dr Raaed Faleh Hassan",
        "published": "2018-8-21",
        "citations": 1,
        "abstract": "The work presented in this paper concerned with the analysis, design and software implementation of the Solid State Transformer as an alternative to the conventional power transformer. The proposed transformer aims to perform the same task as the conventional one with additional facilities and advantages. Three stages are considered to configure the Solid State Transformer. The first stage which is known as input stage and implemented using Vienna rectifier which converts the AC voltage of the main supply to a DC voltage. The second stage (isolation stage) step down the DC voltage to a lower level DC voltage. This stage consists of a single – phase five-level diode clamped inverter, 1 KHz step – down transformer and fully controlled bridge rectifier. The output stage (third stage) is a three-phase three-level diode clamped inverter which converts the low level DC voltage to a three-phase, 50 Hz AC voltage. Model Predictive Current Control has been employed for driving transformer’s stages. The gating signal is produced directly when the given cost function is minimized, therefore there is no need of any modulator. Behavior of the proposed structure is achieved by simulation which shows high quality power conversion with low Total Harmonic Distortion.    ",
        "link": "http://dx.doi.org/10.14419/ijet.v7i3.16423"
    },
    {
        "id": 23609,
        "title": "Optimization of Cooling Ducts in Nanofluid-Filled Power Transformer Windings",
        "authors": "Yunpeng Zhang, Siu-Lau Ho, Weinong Fu",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmrc.2018.8601080"
    },
    {
        "id": 23610,
        "title": "An Efficient Deep Bidirectional Transformer Model for Energy Disaggregation",
        "authors": "Stavros Sykiotis, Maria Kaselimi, Anastasios Doulamis, Nikolaos Doulamis",
        "published": "2022-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco55093.2022.9909768"
    },
    {
        "id": 23611,
        "title": "Research on the Influence of Vehicle Surge on Electronic Potential Transformer of EMUs",
        "authors": "",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53469/jrse.2023.05(10).03"
    },
    {
        "id": 23612,
        "title": "Hybrid Condition Monitoring System for Power Transformer Fault Diagnosis",
        "authors": "Engin Baker, Secil Varbak Nese, Erkan Dursun",
        "published": "2023-1-20",
        "citations": 5,
        "abstract": "The important parts of a transformer, such as the core, windings, and insulation materials, are in the oil-filled tank. It is difficult to detect faults in these materials in a closed area. Dissolved Gas Analysis (DGA)-based fault diagnosis methods predict a fault that may occur in the transformer and take the necessary precautions before the fault grows. Although these fault diagnosis methods have an accuracy of over 95%, their validity is controversial since limited data are used in the studies. The success rates and reliability of fault diagnosis methods in transformers, one of the most important pieces of power systems equipment, should be increased. In this study, a hybrid fault diagnosis system is designed using DGA-based methods and Fuzzy Logic. A mathematical approach and support vector machines (SVMs) were used as decision-making methods in the hybrid fault diagnosis systems. The results of tests performed with 317 real fault data sets relating to transformers showed accuracy of 95.58% using a mathematical approach and 96.23% using SVMs.",
        "link": "http://dx.doi.org/10.3390/en16031151"
    },
    {
        "id": 23613,
        "title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video",
        "authors": "Dmitriy Serdyuk, Otavio Braga, Olivier Siohan",
        "published": "2022-9-18",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10920"
    },
    {
        "id": 23614,
        "title": "Characterisation of Amorphous Metal Materials for High-Frequency High-Power-Density Transformer",
        "authors": "Anas Bashir-U-Din",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icseng.2018.8638251"
    },
    {
        "id": 23615,
        "title": "Modeling and design of solid state smart transformer for microgrid",
        "authors": "Sreedhar Madichetty, Bhanu Duggal, Aumkar Borgaonkar, Sukumar Mishra",
        "published": "2018-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etechnxt.2018.8385293"
    },
    {
        "id": 23616,
        "title": "Contactless excitation system with rotary transformer for hydro-generators",
        "authors": "Andrei Muresan, Ioan Vadan, Madalin Ardelean",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mps.2019.8759691"
    },
    {
        "id": 23617,
        "title": "Inter Disc Fault Simulation in Transformer Windings",
        "authors": "Abhishek Saini, Manisha Sharma, Arun Chantola",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicct.2018.8473165"
    },
    {
        "id": 23618,
        "title": "Transformer Modelling Considering Power Losses Using an Inverse Jiles-Atherton Approach",
        "authors": "José  Antonio Badri, Jordi-Roger Riba, Antoni Garcia, Santi Trujillo, Albert Marzàbal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442124"
    },
    {
        "id": 23619,
        "title": "Performance analysis of Transformer — Less Unified Power Flow Controller",
        "authors": "Shaikh Mohammed Tauseef, S. M. Kulkarni",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icces45898.2019.9002535"
    },
    {
        "id": 23620,
        "title": "Transformer Equipment Temperature Monitoring Based on the Network Framework of Django",
        "authors": "Chunsheng Song, Ruiping Huo, Shuqing Wang, Changzhi Lv",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8996768"
    },
    {
        "id": 23621,
        "title": "Planar transformer design of LLC DC-DC converters with electromagnetics simulation",
        "authors": "Kumpei Yoshikawa, Tetsuya Oshikata",
        "published": "2019-5-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcm45535.2019.9232900"
    },
    {
        "id": 23622,
        "title": "Research on pathologic myopia recognition based on vision transformer",
        "authors": "Chen Yang",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Currently, the diagnosis of pathological myopia is mostly done through manual diagnosis, which not only requires experienced ophthalmologists but is also time-consuming and labour-intensive. In order to improve the diagnostic efficiency and accuracy, and to prevent irreversible visual impairment caused by missed diagnosis, misdiagnosis, and delayed treatment, this paper presents a fine-grained image analysis task of classifying fundus images of patients with pathological myopia and non-pathological myopia. To accurately identify subtle differences in features among similar fundus images, a pathological myopia recognition model based on Vision Transformer (ViT) is proposed. The model incorporates a feature selection module using self-attention mechanism that can effectively select important features in the fundus images, thereby eliminating the influence of irrelevant regions on recognition. Experimental results demonstrate that this method outperforms traditional ViT models, achieving high accuracy in pathological myopia recognition.",
        "link": "http://dx.doi.org/10.54254/2755-2721/15/20230823"
    },
    {
        "id": 23623,
        "title": "Topic and Style-aware Transformer for Multimodal Emotion Recognition",
        "authors": "Shuwen Qiu, Nitesh Sekhar, Prateek Singhal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.130"
    },
    {
        "id": 23624,
        "title": "OPTIMAL DESIGN OF POWER TRANSFORMER TANK USING ANT/FIREFLY HYBRID HEURISTIC ALGORITHM",
        "authors": "Mehmet ZİLE",
        "published": "2020-1-1",
        "citations": 2,
        "abstract": "Power transformers are one of the most important and expensive components of power transmission lines. Reducing costs in the manufacture of power transformers has always been the subject of science. In this study, an ant/firefly hybrid heuristic algorithm has been developed to optimize the materials used in power transformer tanks and therefore to reduce tank costs. Based on this algorithm, an interface program has been created in Visual Studio Program. Using this power transformer tank design computer program, the materials used in the tanks have been optimized. As a result of the optimization, the materials used in the transformer tank, which is designed as a result, are saved between 5% and 15%. By this study, it has become possible to reduce the manufacturing costs of power transformers.",
        "link": "http://dx.doi.org/10.31127/tuje.583975"
    },
    {
        "id": 23625,
        "title": "Research on harmonic transmission characteristics of capacitor voltage transformer",
        "authors": "Chaofeng Wei, Qionglin Li, Jiandong Jiang",
        "published": "2017-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/piers-fall.2017.8293568"
    },
    {
        "id": 23626,
        "title": "CNN- Transformer-Based Modeling and Visual Measurement of Compound Eye Vision System",
        "authors": "Shangwu Feng, Li Yang, Yuan Li",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240669"
    },
    {
        "id": 23627,
        "title": "Dynamic demand minimization using a smart transformer",
        "authors": "Junru Chen, Cathal O'Loughlin, Terence O'Donnell",
        "published": "2017-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2017.8216730"
    },
    {
        "id": 23628,
        "title": "Distribution transformer monitoring and reactive power compensation",
        "authors": "Mingfeng LUO, Dongyin LAI",
        "published": "2018-6-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3166/ejee.20.309-324"
    },
    {
        "id": 23629,
        "title": "Insulation optimization of traction transformer for Lightweight Application",
        "authors": "Xiong Bin, Ding YiWei, Huang Kangjie",
        "published": "2022-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems56177.2022.9983308"
    },
    {
        "id": 23630,
        "title": "Unit Gain Characteristic based Resonant frequency Tracking for DC Transformer Operation",
        "authors": "Yuqi Wei, Alan Mantooth",
        "published": "2021-6-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec42165.2021.9487067"
    },
    {
        "id": 23631,
        "title": "Breast Cancer Detection System from Thermal Images using SWIN Transformer",
        "authors": "Ahatsham Hayat",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17492/computology.v3i1.2301"
    },
    {
        "id": 23632,
        "title": "Multitask Swin Transformer for Classification and Characterization of Pulmonary Nodules in CT Images",
        "authors": "Haizhe Jin, Cheng Yu, Renjie Zheng, Yongyan Fu, Yinan Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4597429"
    },
    {
        "id": 23633,
        "title": "Transformer-stage based segmentation of empty sella and peripheral arteries",
        "authors": "Maoyi Zhang, Changqing Ding",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135429"
    },
    {
        "id": 23634,
        "title": "Best choice of insulation gas medium for MMGS in fast linear transformer driver",
        "authors": "Xuandong Liu, Xianfei Liu",
        "published": "2020-3-1",
        "citations": 2,
        "abstract": "Gas spark closing switch is one of the most important components for a fast linear transformer driver (FLTD). In this paper, we describe the investigation of insulation gas media for a multi-gap multi-channel gas spark closing switch. A six-gap gas switch with corona needles was tested in a typical FLTD brick with two capacitors of 40 nF and a load resistor of about 10 Ω. Corona discharge current, self-breakdown voltage distribution, and triggered breakdown performance were tested when the gas switch was filled with air, N2, CO2, SF6/N2, and C4F7N/N2. When C4F7N/N2 was applied, there was no abnormal breakdown with low voltage found in the whole test process; the trigger breakdown delay time and switch jitter were very stable, and no pre-fire was found during about 2000 triggered shots. Therefore, we think the C4F7N/N2 gas mixture with a very little amount of C4F7N can dramatically improve the switch performance. It is valuable and easy to realize the application of C4F7N/N2 mixture in large scale pulsed power facilities.",
        "link": "http://dx.doi.org/10.1063/1.5136275"
    },
    {
        "id": 23635,
        "title": "An Open-End Winding Hybrid Transformer for Low Voltage Distribution Grids",
        "authors": "SALVATORE FOTI, Haseeb Hassan Khan, Giacomo Scelba, Antonio Testa, Luigi Danilo Tornello",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4540979"
    },
    {
        "id": 23636,
        "title": "Ensemble Learning with Residual Transformer for Brain Tumor Segmentation",
        "authors": "Lanhong Yao, Zheyuan Zhang, Ulas Bagci",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230404"
    },
    {
        "id": 23637,
        "title": "Research on Wind Speed Prediction Modeling Based on Single Variable Transformer",
        "authors": "Yue Liu",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257715"
    },
    {
        "id": 23638,
        "title": "Note: Investigation of a Marx generator imitating a Tesla transformer",
        "authors": "B. H. McGuyer",
        "published": "2018-8-1",
        "citations": 0,
        "abstract": "A compact Marx generator was built to mimic a spark-gap Tesla transformer. The generator produced radio-frequency pulses of up to ±200 kV and ±15 A with a frequency between 110 and 280 kHz at a repetition rate of 120 Hz. The generator tolerated larger circuit-parameter perturbations than is expected for conventional Tesla transformers. Possible applications include research on the control and laser guiding of spark discharges.",
        "link": "http://dx.doi.org/10.1063/1.5035286"
    },
    {
        "id": 23639,
        "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
        "authors": "Yu Suzuki, Tsuneo Kato, Akihiro Tamura",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10344"
    },
    {
        "id": 23640,
        "title": "Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition",
        "authors": "Hai-tao Xu, Jie Zhang, Li-rong Dai",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-18"
    },
    {
        "id": 23641,
        "title": "Fire Detection using Transformer Network",
        "authors": "Mohammad Shahid, Kai-lung Hua",
        "published": "2021-8-24",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3460426.3463665"
    },
    {
        "id": 23642,
        "title": "Multi-encoder Transformer Network for Automatic Post-Editing",
        "authors": "Jaehun Shin, Jong-Hyeok Lee",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6470"
    },
    {
        "id": 23643,
        "title": "MR Image Harmonization with Transformer",
        "authors": "Dong Han, Rui Yu, Shipeng Li, Jing Wang, Yuzun Yang, Zhixun Zhao, Yiming Wei, Shan Cong",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMany clinical applications require medical image harmonization to combine and normalize images from different scanners or protocols. This paper introduces a Transformer-based MR image harmonization method. Our proposed method leverages the self-attention mechanism of the Transformer to learn the complex relationships between image patches and effectively transfer the imaging characteristics from a source image domain to a target image domain. We evaluate our approach to state-of-the-art methods using a publicly available dataset of brain MRI scans and show that it provides superior quantitative metrics and visual quality. Furthermore, we demonstrate that the proposed approach is highly resistant to fluctuations in image modality, resolution, and noise. Overall, the experiment results indicate that our approach is a promising method for medical image harmonization that can improve the accuracy and reliability of automated analysis and diagnosis in clinical settings.",
        "link": "http://dx.doi.org/10.1101/2023.08.16.23294184"
    },
    {
        "id": 23644,
        "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
        "authors": "Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.489"
    },
    {
        "id": 23645,
        "title": "Transformer la crise en occasion : le quotidien d’un centre d’accueil et de crise",
        "authors": "Nicolas Gougoulis",
        "published": "2018-11-14",
        "citations": 1,
        "abstract": "En appui sur une étude clinique et sur la littérature actuelle sur le sujet, l’auteur explore les particularités d’un fonctionnement en crise des patients état limite. La prise en charge en institution devient un terrain favorable d’accueil pouvant cependant reproduire, voire amplifier les clivages et les projections des patients. L’équipe d’un centre de crise est dans l’obligation d’inventer des réponses cliniques adaptées pour transformer la crise en occasion de réflexion et par là même ouvrir la possibilité d’une dimension psychothérapeutique du travail psychique.",
        "link": "http://dx.doi.org/10.3917/clini.016.0052"
    },
    {
        "id": 23646,
        "title": "A Transformer-based Approach for Dynamic Referee Assistance",
        "authors": "Trong-Thuan Nguyen, Minh-Triet Tran",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mapr59823.2023.10289042"
    },
    {
        "id": 23647,
        "title": "The current status and prospects of transformer in  multimodality",
        "authors": "Yingjie Zhang",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "At present, the attention mechanism represented by transformer has greatly promoted the development of natural language processing (NLP) and image processing (CV). However, in the multimodal field, the application of attention mechanism still mainly focuses on extracting the features of different types of data, and then fusing these features (such as text and image). With the increasing scale of the model and the instability of the Internet data, feature fusion has been difficult to solve the growing variety of multimodal problems for us, and the multimodal field has always lacked a model that can uniformly handle all types of data. In this paper, we first take the CV and NLP fields as examples to review various derived models of transformer. Then, based on the mechanism of word embedding and image embedding, we discuss how embedding with different granularity is handled uniformly under the attention mechanism in multimodal scenes. Further, we reveal that this mechanism will not only be limited to CV and NLP, but the real unified model will be able to handle tasks across data types through pre-training and fine tuning. Finally, on the specific implementation of the unified model, this paper lists several cases, and analyzes the valuable research directions in related fields.",
        "link": "http://dx.doi.org/10.54254/2755-2721/11/20230240"
    },
    {
        "id": 23648,
        "title": "Transformer Graph Network for Coronary Plaque Localization in CCTA",
        "authors": "Mario Viti, Hugues Talbot, Nicolas Gogin",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi52829.2022.9761646"
    },
    {
        "id": 23649,
        "title": "A Current Transformer (CT) Saturation Detection Method for Bus Differential Protection",
        "authors": "Monir Hossain, Ittiphong Leevongwat, Parviz Rastgoufard",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc.2018.8664064"
    },
    {
        "id": 23650,
        "title": "A Gated Graph Transformer for Protein Complex Structure Quality Assessment and its Performance in CASP15",
        "authors": "Xiao Chen, Alex Morehead, Jian Liu, Jianlin Cheng",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractMotivationProteins interact to form complexes to carry out essential biological functions. Computational methods such as AlphaFold-multimer have been developed to predict the quaternary structures of protein complexes. An important yet largely unsolved challenge in protein complex structure prediction is to accurately estimate the quality of predicted protein complex structures without any knowledge of the corresponding native structures. Such estimations can then be used to select high-quality predicted complex structures to facilitate biomedical research such as protein function analysis and drug discovery.ResultsIn this work, we introduce a new gated neighborhood-modulating graph transformer to predict the quality of 3D protein complex structures. It incorporates node and edge gates within a graph transformer framework to control information flow during graph message passing. We trained, evaluated and tested the method (called DProQA) on newly-curated protein complex datasets before the 15th Critical Assessment of Techniques for Protein Structure Prediction (CASP15) and then blindly tested it in the 2022 CASP15 experiment. The method was ranked 3rd among the single-model quality assessment methods in CASP15 in terms of the ranking loss of TM-score on 36 complex targets. The rigorous internal and external experiments demonstrate that DProQA is effective in ranking protein complex structures.AvailabilityThe source code, data, and pre-trained models are available athttps://github.com/jianlin-cheng/DProQAContactchengji@missouri.eduSupplementary informationSupplementary data are available atBioinformaticsonline.",
        "link": "http://dx.doi.org/10.1101/2022.05.19.492741"
    },
    {
        "id": 23651,
        "title": "Implementation of Transformer-Based Model for Acute Lymphoblastic Leukemia Segmentation",
        "authors": "Phumiphat Charoentananuwat, Suree Pumrin",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon58879.2023.10322461"
    },
    {
        "id": 23652,
        "title": "A Hybrid Transformer Topology for Distribution Network Voltage Regulation",
        "authors": "Rupert Power, Alykhan Mithani, Udaya Madawala, Craig Baguley",
        "published": "2021-12-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec52827.2021.9709487"
    },
    {
        "id": 23653,
        "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
        "authors": "Shiliang Zhang, Ming Lei, Zhijie Yan",
        "published": "2019-9-15",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1290"
    },
    {
        "id": 23654,
        "title": "An isolated DC-DC converter with fully integrated magnetic core transformer",
        "authors": "Zhao Tianting, Zhuo Yue, Chen Baoxing",
        "published": "2017-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cicc.2017.7993620"
    },
    {
        "id": 23655,
        "title": "A D-band wide tuning range VCO using switching transformer",
        "authors": "Yu-Teng Chang, Hsin-Chia Lu",
        "published": "2017-6",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mwsym.2017.8058864"
    },
    {
        "id": 23656,
        "title": "Agglomerative Transformer for Human-Object Interaction Detection",
        "authors": "Danyang Tu, Wei Sun, Guangtao Zhai, Wei Shen",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01976"
    },
    {
        "id": 23657,
        "title": "Aggregating Global Features into Local Vision Transformer",
        "authors": "Krushi Patel, Andres M. Bur, Fengjun Li, Guanghui Wang",
        "published": "2022-8-21",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956379"
    },
    {
        "id": 23658,
        "title": "Transformer-Based Online Speech Recognition with Decoder-end Adaptive Computation Steps",
        "authors": "Mohan Li, Catalin Zorila, Rama Doddipatla",
        "published": "2021-1-19",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383613"
    },
    {
        "id": 23659,
        "title": "Transformer-Based Model for Predicting Customers’ Next Purchase Day in e-Commerce",
        "authors": "Alexandru Grigoraș, Florin Leon",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "The paper focuses on predicting the next purchase day (NPD) for customers in e-commerce, a task with applications in marketing, inventory management, and customer retention. A novel transformer-based model for NPD prediction is introduced and compared to traditional methods such as ARIMA, XGBoost, and LSTM. Transformers offer advantages in capturing long-term dependencies within time series data through self-attention mechanisms. This adaptability to various time series patterns, including trends, seasonality, and irregularities, makes them a promising choice for NPD prediction. The transformer model demonstrates improvements in prediction accuracy compared to the baselines. Additionally, a clustered transformer model is proposed, which further enhances accuracy, emphasizing the potential of this architecture for NPD prediction.",
        "link": "http://dx.doi.org/10.3390/computation11110210"
    },
    {
        "id": 23660,
        "title": "Role of nanofiller porosity in monitoring streamer and phonon dynamics in Transformer oil",
        "authors": "Mississippi M Bhunia, Dipanwita Mitra, Kalyan Kumar Chattopadhyay, Paramita Chattopadhyay",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper presents a maiden attempt to witness several nanofluidic properties of porous nanofillers dispersed in Transformer oil (TO). For this, Porous Boron Nitride (PBN)-PBN 950 and PBN 1050 at two different porosities with low (high) and high (low) specific surface area (SSA) and aspect ratio (AR) respectively, both composed of 2D porous flakes of quasi-crystalline BN of hexagonal phase, arranged in rod and flower pattern were obtained via gas-solid interaction at 950 and 1050oC respectively. Nanofluids of both PBNs in TO displayed a linear relationship between porosity and insulation characteristics with ~26-39 % rise in AC Breakdown Voltage along with Resistivity. It was explained by the role of nanofiller porosity in providing large interfacial zones and deep traps for quantitively scavenging and holding streamer charges at the oil-nanofiller interface. Further, despite of being quasi-crystalline and porous, both PBNs in TO upgraded the thermal properties by quasi-ballistic transfer of acoustic phonons via 2D flakes of hexagonal phase. However, the thermal conductivity varied inversely with porosity with ~ 32.5 - 28 % surge with PBN 950 and PBN 1050, respectively at 50oC. This trend was theoretically supported as well as explained due to the increase in phonon-defect scattering owing to the increase in porosity and pore asymmetry together with reduced AR in PBN 1050. Finally, the unchanged flash and pour points with negligible increase in acidity over months plus the aforementioned results at low filler fractions establishes the superiority of PBN over other BN nanofillers for applications in liquid insulation in future",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-3t3kl"
    },
    {
        "id": 23661,
        "title": "Peer Review #2 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.2)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.2/reviews/2"
    },
    {
        "id": 23662,
        "title": "WOMEN IN THE MINISTRY OF JESUS: JESUS THE LIBERATOR AND TRANSFORMER",
        "authors": "Stella Bogi",
        "published": "2022",
        "citations": 1,
        "abstract": "Centuries have gone by since Jesus came to liberate and transform men and women. The very truth that women served in the team of Jesus while he was on the earth shows that in an androcentric patriarchal world, women were welcomed. This was made possible because Jesus went against the tide of patriarchy to reach out to the downtrodden, marginalised, and oppressed women. These women were victims of socio-cultural and oral traditions set up by the Jews. Jesus allowed women to be a part of his life. Even before his birth, God prepared women of low estate to be a part of his genealogy. We see the role of women in his birth, ministry, crucifixion, death, and resurrection. Jesus liberated and transformed a woman caught in adultery, the Samaritan woman whose life was in a mess, and women in need of healing to name a few. Women were able to serve on the team of Jesus because of their transformed lives. Many churches have come forward to allow women in the church ministries and to serve God.",
        "link": "http://dx.doi.org/10.54513/bsj.2022.4401"
    },
    {
        "id": 23663,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "Rubén Romero",
        "published": "2019-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v2/review1"
    },
    {
        "id": 23664,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "Rubén Romero",
        "published": "2019-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v3/review1"
    },
    {
        "id": 23665,
        "title": "Effect of Magnetic Field on Streamer Dynamics in Transformer Oil",
        "authors": "Mihir Bhatt, Praghnesh Bhatt",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icemce57940.2023.10434177"
    },
    {
        "id": 23666,
        "title": "Multi-Stage Transformer 3D Object Detection Method",
        "authors": "Yanfei Liu, Kanglin Ning",
        "published": "2022-9-19",
        "citations": 1,
        "abstract": "With the development of autonomous driving, 3D object detection has experience great expectations. As the light detection and ranging (LiDAR) sensor can precisely measure the distance between environments and themselves, it has become the key component of current 3D object detection methods. However, the varing density and unstructure storage of LiDAR points cloud make it hard for feature learning. To tackle this problem, this paper proposes a multi-task transformer 3D object detection method.This method include a fast transformer based 3D encoder and a multi-stage transformer decoder. Extensive experiments demonstrate that our method can supress current other 3D object detection methods with a clear margin.",
        "link": "http://dx.doi.org/10.54097/fcis.v1i2.1629"
    },
    {
        "id": 23667,
        "title": "Building Usage Classification Using a Transformer-based Multimodal Deep Learning Method",
        "authors": "Wen Zhou, Claudio Persello, Alfred Stein",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jurse57346.2023.10144168"
    },
    {
        "id": 23668,
        "title": "Relational-Convergent Transformer for image captioning",
        "authors": "Lizhi Chen, You Yang, Juntao Hu, Longyue Pan, Hao Zhai",
        "published": "2023-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.displa.2023.102377"
    },
    {
        "id": 23669,
        "title": "Circularly Deformable Medical Image Registration Based on Transformer-CNN with Prompt",
        "authors": "longhao li, Li Li, yunfeng zhang, fangxun bao, Xunxiang Yao, Caiming Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4700960"
    },
    {
        "id": 23670,
        "title": "Deepfake audio detection with vision transformer based method",
        "authors": "Guzin Ulutas, Gul Tahaoglu, Beste Ustubioglu",
        "published": "2023-7-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsp59544.2023.10197715"
    },
    {
        "id": 23671,
        "title": "Distribution transformer failure in India root causes and remedies",
        "authors": "Narasimha Pandit, R. L. Chakrasali",
        "published": "2017-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icimia.2017.7975582"
    },
    {
        "id": 23672,
        "title": "Fast-Slow Transformer for Visually Grounding Speech",
        "authors": "Puyuan Peng, David Harwath",
        "published": "2022-5-23",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp43922.2022.9747103"
    },
    {
        "id": 23673,
        "title": "Breast Cancer Detection Using Transformer and BiLSTM Based Ensemble Learning",
        "authors": "Rabia Eda Yılmaz, Görkem Serbes",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223993"
    },
    {
        "id": 23674,
        "title": "Target Detection in Sea Clutter with Transformer Neural Network",
        "authors": "Shanshan Tian, Wuqi Wang, Guangxin Ding, Zhiwei Zhang",
        "published": "2021-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar53847.2021.10028317"
    },
    {
        "id": 23675,
        "title": "MSTFDN: Multi-scale transformer fusion dehazing network",
        "authors": "Yan Yang, Haowen Zhang, Xudong Wu, Xiaozhen Liang",
        "published": "2022-7-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-03674-2"
    },
    {
        "id": 23676,
        "title": "Enhanced Transformer Model for Data-to-Text Generation",
        "authors": "Li GONG, Josep Crego, Jean Senellart",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5615"
    },
    {
        "id": 23677,
        "title": "Attention Based Mapping for Plants Leaf to Classify Diseases using Vision Transformer",
        "authors": "Kumar Rethik, Dhanpratap Singh",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170081"
    },
    {
        "id": 23678,
        "title": "On-line medium voltage cell &amp; transformer maintenance",
        "authors": "A. Primadianto, C. Ekana, Y. Devalesy, W. Prabowo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0258"
    },
    {
        "id": 23679,
        "title": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer",
        "authors": "Shuaicheng Zhang, Qiang Ning, Lifu Huang",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-naacl.29"
    },
    {
        "id": 23680,
        "title": "TRANSQLATION: TRANsformer-based SQL RecommendATION",
        "authors": "Shirin Tahmasebi, Amir H. Payberah, Ahmet Soylu, Dumitru Roman, Mihhail Matskin",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386277"
    },
    {
        "id": 23681,
        "title": "Insulation Residual Life Estimation Based on Transformer Condition Assessment Data",
        "authors": "Alan Sbravati, Luiz V. Cheim, George K. Frimpong",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177293"
    },
    {
        "id": 23682,
        "title": "A task-unified network with transformer and spatial-temporal convolution for left ventricular quantification",
        "authors": "Dapeng Li, Yanjun Peng, Jindong Sun, Yanfei Guo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nQuantification of the cardiac function is vital for diagnosing and curing the cardiovascular diseases.\nLeft ventricular function measurement is the most commonly used measure to evaluate the function\nof cardiac in clinical practice, how to improve the accuracy of left ventricular quantitative assessment\nresults has always been the subject of research by medical researchers. Although considerable efforts\nhave been put forward to measure the left ventricle (LV) automatically using deep learning methods,\nthe accurate quantification is yet a challenge work as a result of the changeable anatomy structure\nof heart in the systolic diastolic cycle. Besides, most methods used direct regression method which\nlacks of visual based analysis. In this work, a deep learning segmentation and regression task-unified\nnetwork with transformer and spatial-temporal convolution was proposed to segment and quantify\nthe LV simultaneously. The segmentation module leveraged a U-Net like 3D Transformer model to\npredict the contour of three anatomy structures, while the regression module learned spatial-temporal\nrepresentations from the original images and the reconstruct feature maps from segmentation path to\nestimate the fifinally desired quantification metrics. Furthermore, we employed a joint task loss function\nto train the two module networks. Our framework is evaluated on the MICCAI 2017 Left Ventricle\nFull Quantification Challenge dataset. The results of experiments demonstrate the effectiveness of\nour framework, which achieves competitive cardiac quantification metric results and at the same time\nproduces visualized segmentation results that are conducive to later analysis.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2590069/v1"
    },
    {
        "id": 23683,
        "title": "AMST2: Aggregated Multi-Level Spatial and Temporal Context-Based Transformer for Robust Aerial Tracking",
        "authors": "Hasil Park, Injae Lee, Dasol Jeong, Joonki Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecently, many existing visual trackers have made significant progress by incorporating either spatial information from multi-level convolution layers or temporal information for tracking. However, the complementary advantages of both spatial and temporal information cannot be leveraged when these two types of information are used separately. In this paper, we present a new approach for robust visual tracking using a transformer-based model that incorporates both spatial and temporal context information at multiple levels. To integrate the refined similarity maps through multi-level spatial and temporal encoders, we propose an aggregation encoder. Consequently, the output of the proposed aggregation encoder contains useful features that integrate the global contexts of multi-level spatial and the temporal contexts. The contrasting yet complementary representation of multi-level spatial and temporal contexts provided by this feature can help prevent tracking failures in a range of complex aerial scenarios, including occlusion, motion blur, and scale variations. Additionally, the proposed architecture can achieve more robust object tracking against significant variations by updating the features of the latest object while retaining the initial template information. Extensive experiments on five challenging short-term and long-term aerial tracking benchmarks have demonstrated that the proposed tracker outperforms state-of-the-art tracking methods in terms of both real-time processing speed and performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2770662/v1"
    },
    {
        "id": 23684,
        "title": "Multi-tailed vision transformer for efficient inference",
        "authors": "Yunke Wang, Bo Du, Wenyuan Wang, Chang Xu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106235"
    },
    {
        "id": 23685,
        "title": "Image Inpainting by Mscswin Transformer Adversarial Autoencoder",
        "authors": "Bo-Wei Chen, Tsung-Jung Liu, Kuan-Hsien Liu",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222778"
    },
    {
        "id": 23686,
        "title": "Hierarchical vision transformer model for polyp segmentation",
        "authors": "Geetha S., Gopakumar C., Vishnu Vinod",
        "published": "2023-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicaps57044.2023.10074447"
    },
    {
        "id": 23687,
        "title": "Frequency Response Quality Index for Assessing the Mechanical Condition of Transformer Windings",
        "authors": "Eugeniusz Kornatowski, Szymon Banaszak",
        "published": "2019-12-19",
        "citations": 9,
        "abstract": "Frequency response analysis (FRA) is a popular method for assessing a transformer’s mechanical condition. The paper proposes a new method for interpreting the frequency response measurement results. The currently used numerical indices only give one value, which may be misleading in the analysis, while the proposed frequency response quality index (FRQI) tool analyses three separate features in the whole frequency range. The applied numerical calculations technique allows for estimations of not only the values of the average quality indices, but also locally for given frequency ranges of the analysed spectrum. It allows for determination of the problems that can be found in the active part of a transformer. The presented results come from three transformers, representing cases of typical faults. Two of them are from industry, while one was used for deformational tests in laboratory conditions. The proposed FRQI method showed its usefulness in FRA test results analysis and may be introduced into the automated assessment of such data. Each of the component parameters is sensitive to other types of differences observed between the compared frequency response curves, and may be used as a good quality detection tool.",
        "link": "http://dx.doi.org/10.3390/en13010029"
    },
    {
        "id": 23688,
        "title": "Variational Transformer Networks for Layout Generation",
        "authors": "Diego Martin Arroyo, Janis Postels, Federico Tombari",
        "published": "2021-6",
        "citations": 43,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.01343"
    },
    {
        "id": 23689,
        "title": "Models of the Reading Architecture",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter describes what has been learned about reading architecture, or how the mental processes that support word identification, sentence processing, and discourse representation during reading are coordinated with the systems that support vision, attention, and eye-movement control. The chapter reviews key findings that shed light on the nature of reading architecture, mainly using the results of eye-movement experiments. The chapter then reviews precursor theories and models of the reading architecture—early attempts to explain and simulate reading in its entirety. The chapter goes on to review a large, representative sample of the models that have been used to simulate and understand natural reading. Models are reviewed in their order of development to show how they have evolved to accommodate new empirical findings. The chapter concludes with an explicit comparative analysis of the models and a discussion of the empirical findings that each model can and cannot explain.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0006"
    },
    {
        "id": 23690,
        "title": "Models of Models: The Symbiotic Relationship between Models and Wargames",
        "authors": "Vikram Mittal, Jeffrey B. Demarest, Kennon S. Gilliam, Robert L. Page",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006401502150223"
    },
    {
        "id": 23691,
        "title": "Models of Models: The Symbiotic Relationship between Models and Wargames",
        "authors": "Vikram Mittal, Jeffrey B. Demarest, Kennon S. Gilliam, Robert L. Page",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006401500001626"
    },
    {
        "id": 23692,
        "title": "Auto Operation and Monitoring of Transformer using IoT Applications",
        "authors": "Sathish R., Vinod Kumar D., Muthusami C.",
        "published": "2022-7-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icict54344.2022.9850919"
    },
    {
        "id": 23693,
        "title": "Research and calculation of a commutator transformer-capacitor pulsegenerator Nosov G.V., Nosova M.G",
        "authors": "G.V. NOSOV, M.G. NOSOVA",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53891/00135860_2022_10_40"
    },
    {
        "id": 23694,
        "title": "TRC-Unet: Transformer Connections for Near-infrared Blurred Image Segmentation",
        "authors": "Jiazhe Wang, Yoshie Osamu, Koichi Shimizu",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956727"
    },
    {
        "id": 23695,
        "title": "Internal Fault Analysis in Disk-Type Transformer Winding Using Network Reduction",
        "authors": "Yazid Alkraimeen, Pablo Gomez",
        "published": "2018-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eit.2018.8500127"
    },
    {
        "id": 23696,
        "title": "18-Pulse transformer device for flexible connection of asynchronous power systems",
        "authors": "Valeriu Bosneaga, Victor Suslov",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sielmen.2017.8123304"
    },
    {
        "id": 23697,
        "title": "Study of output characters of pulse transformer with closed magnetic circuit",
        "authors": "Arijit Basuray, Saibal Chatterjee",
        "published": "2017-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppc.2017.8291308"
    },
    {
        "id": 23698,
        "title": "Population-Based Optimization Algorithms Used in Improving Performance of Rectifier Transformer",
        "authors": "Barbara Kulesz, Andrzej Sikora, Adam Zielonka",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem.2018.8442625"
    },
    {
        "id": 23699,
        "title": "Vision Transformer Hashing for Image Retrieval",
        "authors": "Shiv Ram Dubey, Satish Kumar Singh, Wei-Ta Chu",
        "published": "2022-7-18",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859900"
    },
    {
        "id": 23700,
        "title": "DIPT: Diversified Personalized Transformer for QAC systems",
        "authors": "Mahdi Dehghani, Samira Vaez Barenji, Saeed Farzi",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke60553.2023.10326229"
    },
    {
        "id": 23701,
        "title": "Experimental Validation of a Transformer-less Inverter with Improved Gain for Grid-PV Interface",
        "authors": "Omkar Patkar, Mini Rajeev",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npec47332.2019.9034810"
    },
    {
        "id": 23702,
        "title": "Matrix Converter-Based Current Predictive Control of Novel Power Electronic Transformer",
        "authors": "Yougui Guo, Bowen Yang, Liyang Duan, Wenlang Deng",
        "published": "2021-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54647/energy48103"
    },
    {
        "id": 23703,
        "title": "Line Loss Management for Transformer Courts Based on Data Visualization",
        "authors": "Huo Na, Wang Sheng, Wang Wei",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced.2018.8592352"
    },
    {
        "id": 23704,
        "title": "On-Chip Spiral Transformer",
        "authors": "",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811255366_0003"
    },
    {
        "id": 23705,
        "title": "T3srs: Tensor Train Transformer for Compressing Sequential Recommender Systems",
        "authors": "Hao Li, Jianli Zhao, Huan Huo, Sheng Fang, Jianjian Chen, Lutong Yao, Yiran Hua",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4408354"
    },
    {
        "id": 23706,
        "title": "Diagnostics of Transformer Oils Using the Multiple Linear Regression Model",
        "authors": "Oleg Shutenko, Serhii Ponomarenko",
        "published": "2020-9-21",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paep49887.2020.9240875"
    },
    {
        "id": 23707,
        "title": "Design and software implementation of solid state transformer",
        "authors": "Dr Raaed Faleh Hassan",
        "published": "2018-8-21",
        "citations": 1,
        "abstract": "The work presented in this paper concerned with the analysis, design and software implementation of the Solid State Transformer as an alternative to the conventional power transformer. The proposed transformer aims to perform the same task as the conventional one with additional facilities and advantages. Three stages are considered to configure the Solid State Transformer. The first stage which is known as input stage and implemented using Vienna rectifier which converts the AC voltage of the main supply to a DC voltage. The second stage (isolation stage) step down the DC voltage to a lower level DC voltage. This stage consists of a single – phase five-level diode clamped inverter, 1 KHz step – down transformer and fully controlled bridge rectifier. The output stage (third stage) is a three-phase three-level diode clamped inverter which converts the low level DC voltage to a three-phase, 50 Hz AC voltage. Model Predictive Current Control has been employed for driving transformer’s stages. The gating signal is produced directly when the given cost function is minimized, therefore there is no need of any modulator. Behavior of the proposed structure is achieved by simulation which shows high quality power conversion with low Total Harmonic Distortion.    ",
        "link": "http://dx.doi.org/10.14419/ijet.v7i3.16423"
    },
    {
        "id": 23708,
        "title": "Optimization of Cooling Ducts in Nanofluid-Filled Power Transformer Windings",
        "authors": "Yunpeng Zhang, Siu-Lau Ho, Weinong Fu",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmrc.2018.8601080"
    },
    {
        "id": 23709,
        "title": "An Efficient Deep Bidirectional Transformer Model for Energy Disaggregation",
        "authors": "Stavros Sykiotis, Maria Kaselimi, Anastasios Doulamis, Nikolaos Doulamis",
        "published": "2022-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco55093.2022.9909768"
    },
    {
        "id": 23710,
        "title": "Research on the Influence of Vehicle Surge on Electronic Potential Transformer of EMUs",
        "authors": "",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53469/jrse.2023.05(10).03"
    },
    {
        "id": 23711,
        "title": "Enhanced Transformer Model for Data-to-Text Generation",
        "authors": "Li GONG, Josep Crego, Jean Senellart",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5615"
    },
    {
        "id": 23712,
        "title": "MSTFDN: Multi-scale transformer fusion dehazing network",
        "authors": "Yan Yang, Haowen Zhang, Xudong Wu, Xiaozhen Liang",
        "published": "2022-7-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-022-03674-2"
    },
    {
        "id": 23713,
        "title": "Agglomerative Transformer for Human-Object Interaction Detection",
        "authors": "Danyang Tu, Wei Sun, Guangtao Zhai, Wei Shen",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01976"
    },
    {
        "id": 23714,
        "title": "Aggregating Global Features into Local Vision Transformer",
        "authors": "Krushi Patel, Andres M. Bur, Fengjun Li, Guanghui Wang",
        "published": "2022-8-21",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956379"
    },
    {
        "id": 23715,
        "title": "Transformer-Based Online Speech Recognition with Decoder-end Adaptive Computation Steps",
        "authors": "Mohan Li, Catalin Zorila, Rama Doddipatla",
        "published": "2021-1-19",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383613"
    },
    {
        "id": 23716,
        "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
        "authors": "Shiliang Zhang, Ming Lei, Zhijie Yan",
        "published": "2019-9-15",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1290"
    },
    {
        "id": 23717,
        "title": "Image Inpainting by Mscswin Transformer Adversarial Autoencoder",
        "authors": "Bo-Wei Chen, Tsung-Jung Liu, Kuan-Hsien Liu",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222778"
    },
    {
        "id": 23718,
        "title": "An isolated DC-DC converter with fully integrated magnetic core transformer",
        "authors": "Zhao Tianting, Zhuo Yue, Chen Baoxing",
        "published": "2017-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cicc.2017.7993620"
    },
    {
        "id": 23719,
        "title": "Unit Gain Characteristic based Resonant frequency Tracking for DC Transformer Operation",
        "authors": "Yuqi Wei, Alan Mantooth",
        "published": "2021-6-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec42165.2021.9487067"
    },
    {
        "id": 23720,
        "title": "Breast Cancer Detection System from Thermal Images using SWIN Transformer",
        "authors": "Ahatsham Hayat",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17492/computology.v3i1.2301"
    },
    {
        "id": 23721,
        "title": "Multitask Swin Transformer for Classification and Characterization of Pulmonary Nodules in CT Images",
        "authors": "Haizhe Jin, Cheng Yu, Renjie Zheng, Yongyan Fu, Yinan Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4597429"
    },
    {
        "id": 23722,
        "title": "Modeling and design of solid state smart transformer for microgrid",
        "authors": "Sreedhar Madichetty, Bhanu Duggal, Aumkar Borgaonkar, Sukumar Mishra",
        "published": "2018-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etechnxt.2018.8385293"
    },
    {
        "id": 23723,
        "title": "Contactless excitation system with rotary transformer for hydro-generators",
        "authors": "Andrei Muresan, Ioan Vadan, Madalin Ardelean",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mps.2019.8759691"
    },
    {
        "id": 23724,
        "title": "Inter Disc Fault Simulation in Transformer Windings",
        "authors": "Abhishek Saini, Manisha Sharma, Arun Chantola",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicct.2018.8473165"
    },
    {
        "id": 23725,
        "title": "Transformer Modelling Considering Power Losses Using an Inverse Jiles-Atherton Approach",
        "authors": "José  Antonio Badri, Jordi-Roger Riba, Antoni Garcia, Santi Trujillo, Albert Marzàbal",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442124"
    },
    {
        "id": 23726,
        "title": "Performance analysis of Transformer — Less Unified Power Flow Controller",
        "authors": "Shaikh Mohammed Tauseef, S. M. Kulkarni",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icces45898.2019.9002535"
    },
    {
        "id": 23727,
        "title": "Transformer Equipment Temperature Monitoring Based on the Network Framework of Django",
        "authors": "Chunsheng Song, Ruiping Huo, Shuqing Wang, Changzhi Lv",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8996768"
    },
    {
        "id": 23728,
        "title": "Multi-Stage Transformer 3D Object Detection Method",
        "authors": "Yanfei Liu, Kanglin Ning",
        "published": "2022-9-19",
        "citations": 1,
        "abstract": "With the development of autonomous driving, 3D object detection has experience great expectations. As the light detection and ranging (LiDAR) sensor can precisely measure the distance between environments and themselves, it has become the key component of current 3D object detection methods. However, the varing density and unstructure storage of LiDAR points cloud make it hard for feature learning. To tackle this problem, this paper proposes a multi-task transformer 3D object detection method.This method include a fast transformer based 3D encoder and a multi-stage transformer decoder. Extensive experiments demonstrate that our method can supress current other 3D object detection methods with a clear margin.",
        "link": "http://dx.doi.org/10.54097/fcis.v1i2.1629"
    },
    {
        "id": 23729,
        "title": "Planar transformer design of LLC DC-DC converters with electromagnetics simulation",
        "authors": "Kumpei Yoshikawa, Tetsuya Oshikata",
        "published": "2019-5-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcm45535.2019.9232900"
    },
    {
        "id": 23730,
        "title": "Research on pathologic myopia recognition based on vision transformer",
        "authors": "Chen Yang",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Currently, the diagnosis of pathological myopia is mostly done through manual diagnosis, which not only requires experienced ophthalmologists but is also time-consuming and labour-intensive. In order to improve the diagnostic efficiency and accuracy, and to prevent irreversible visual impairment caused by missed diagnosis, misdiagnosis, and delayed treatment, this paper presents a fine-grained image analysis task of classifying fundus images of patients with pathological myopia and non-pathological myopia. To accurately identify subtle differences in features among similar fundus images, a pathological myopia recognition model based on Vision Transformer (ViT) is proposed. The model incorporates a feature selection module using self-attention mechanism that can effectively select important features in the fundus images, thereby eliminating the influence of irrelevant regions on recognition. Experimental results demonstrate that this method outperforms traditional ViT models, achieving high accuracy in pathological myopia recognition.",
        "link": "http://dx.doi.org/10.54254/2755-2721/15/20230823"
    },
    {
        "id": 23731,
        "title": "Topic and Style-aware Transformer for Multimodal Emotion Recognition",
        "authors": "Shuwen Qiu, Nitesh Sekhar, Prateek Singhal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.130"
    },
    {
        "id": 23732,
        "title": "OPTIMAL DESIGN OF POWER TRANSFORMER TANK USING ANT/FIREFLY HYBRID HEURISTIC ALGORITHM",
        "authors": "Mehmet ZİLE",
        "published": "2020-1-1",
        "citations": 2,
        "abstract": "Power transformers are one of the most important and expensive components of power transmission lines. Reducing costs in the manufacture of power transformers has always been the subject of science. In this study, an ant/firefly hybrid heuristic algorithm has been developed to optimize the materials used in power transformer tanks and therefore to reduce tank costs. Based on this algorithm, an interface program has been created in Visual Studio Program. Using this power transformer tank design computer program, the materials used in the tanks have been optimized. As a result of the optimization, the materials used in the transformer tank, which is designed as a result, are saved between 5% and 15%. By this study, it has become possible to reduce the manufacturing costs of power transformers.",
        "link": "http://dx.doi.org/10.31127/tuje.583975"
    },
    {
        "id": 23733,
        "title": "CNN- Transformer-Based Modeling and Visual Measurement of Compound Eye Vision System",
        "authors": "Shangwu Feng, Li Yang, Yuan Li",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240669"
    },
    {
        "id": 23734,
        "title": "A Gated Graph Transformer for Protein Complex Structure Quality Assessment and its Performance in CASP15",
        "authors": "Xiao Chen, Alex Morehead, Jian Liu, Jianlin Cheng",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractMotivationProteins interact to form complexes to carry out essential biological functions. Computational methods such as AlphaFold-multimer have been developed to predict the quaternary structures of protein complexes. An important yet largely unsolved challenge in protein complex structure prediction is to accurately estimate the quality of predicted protein complex structures without any knowledge of the corresponding native structures. Such estimations can then be used to select high-quality predicted complex structures to facilitate biomedical research such as protein function analysis and drug discovery.ResultsIn this work, we introduce a new gated neighborhood-modulating graph transformer to predict the quality of 3D protein complex structures. It incorporates node and edge gates within a graph transformer framework to control information flow during graph message passing. We trained, evaluated and tested the method (called DProQA) on newly-curated protein complex datasets before the 15th Critical Assessment of Techniques for Protein Structure Prediction (CASP15) and then blindly tested it in the 2022 CASP15 experiment. The method was ranked 3rd among the single-model quality assessment methods in CASP15 in terms of the ranking loss of TM-score on 36 complex targets. The rigorous internal and external experiments demonstrate that DProQA is effective in ranking protein complex structures.AvailabilityThe source code, data, and pre-trained models are available athttps://github.com/jianlin-cheng/DProQAContactchengji@missouri.eduSupplementary informationSupplementary data are available atBioinformaticsonline.",
        "link": "http://dx.doi.org/10.1101/2022.05.19.492741"
    },
    {
        "id": 23735,
        "title": "Building Usage Classification Using a Transformer-based Multimodal Deep Learning Method",
        "authors": "Wen Zhou, Claudio Persello, Alfred Stein",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jurse57346.2023.10144168"
    },
    {
        "id": 23736,
        "title": "Transformer-stage based segmentation of empty sella and peripheral arteries",
        "authors": "Maoyi Zhang, Changqing Ding",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135429"
    },
    {
        "id": 23737,
        "title": "Best choice of insulation gas medium for MMGS in fast linear transformer driver",
        "authors": "Xuandong Liu, Xianfei Liu",
        "published": "2020-3-1",
        "citations": 2,
        "abstract": "Gas spark closing switch is one of the most important components for a fast linear transformer driver (FLTD). In this paper, we describe the investigation of insulation gas media for a multi-gap multi-channel gas spark closing switch. A six-gap gas switch with corona needles was tested in a typical FLTD brick with two capacitors of 40 nF and a load resistor of about 10 Ω. Corona discharge current, self-breakdown voltage distribution, and triggered breakdown performance were tested when the gas switch was filled with air, N2, CO2, SF6/N2, and C4F7N/N2. When C4F7N/N2 was applied, there was no abnormal breakdown with low voltage found in the whole test process; the trigger breakdown delay time and switch jitter were very stable, and no pre-fire was found during about 2000 triggered shots. Therefore, we think the C4F7N/N2 gas mixture with a very little amount of C4F7N can dramatically improve the switch performance. It is valuable and easy to realize the application of C4F7N/N2 mixture in large scale pulsed power facilities.",
        "link": "http://dx.doi.org/10.1063/1.5136275"
    },
    {
        "id": 23738,
        "title": "An Open-End Winding Hybrid Transformer for Low Voltage Distribution Grids",
        "authors": "SALVATORE FOTI, Haseeb Hassan Khan, Giacomo Scelba, Antonio Testa, Luigi Danilo Tornello",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4540979"
    },
    {
        "id": 23739,
        "title": "Ensemble Learning with Residual Transformer for Brain Tumor Segmentation",
        "authors": "Lanhong Yao, Zheyuan Zhang, Ulas Bagci",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230404"
    },
    {
        "id": 23740,
        "title": "Research on Wind Speed Prediction Modeling Based on Single Variable Transformer",
        "authors": "Yue Liu",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257715"
    },
    {
        "id": 23741,
        "title": "Breast Cancer Detection Using Transformer and BiLSTM Based Ensemble Learning",
        "authors": "Rabia Eda Yılmaz, Görkem Serbes",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223993"
    },
    {
        "id": 23742,
        "title": "Target Detection in Sea Clutter with Transformer Neural Network",
        "authors": "Shanshan Tian, Wuqi Wang, Guangxin Ding, Zhiwei Zhang",
        "published": "2021-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar53847.2021.10028317"
    },
    {
        "id": 23743,
        "title": "Distribution transformer failure in India root causes and remedies",
        "authors": "Narasimha Pandit, R. L. Chakrasali",
        "published": "2017-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icimia.2017.7975582"
    },
    {
        "id": 23744,
        "title": "Note: Investigation of a Marx generator imitating a Tesla transformer",
        "authors": "B. H. McGuyer",
        "published": "2018-8-1",
        "citations": 0,
        "abstract": "A compact Marx generator was built to mimic a spark-gap Tesla transformer. The generator produced radio-frequency pulses of up to ±200 kV and ±15 A with a frequency between 110 and 280 kHz at a repetition rate of 120 Hz. The generator tolerated larger circuit-parameter perturbations than is expected for conventional Tesla transformers. Possible applications include research on the control and laser guiding of spark discharges.",
        "link": "http://dx.doi.org/10.1063/1.5035286"
    },
    {
        "id": 23745,
        "title": "Advanced Signal Processing Techniques for Partial Discharge Measurement",
        "authors": "",
        "published": "2017-8-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch5"
    },
    {
        "id": 23746,
        "title": "Multi-encoder Transformer Network for Automatic Post-Editing",
        "authors": "Jaehun Shin, Jong-Hyeok Lee",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6470"
    },
    {
        "id": 23747,
        "title": "MR Image Harmonization with Transformer",
        "authors": "Dong Han, Rui Yu, Shipeng Li, Jing Wang, Yuzun Yang, Zhixun Zhao, Yiming Wei, Shan Cong",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMany clinical applications require medical image harmonization to combine and normalize images from different scanners or protocols. This paper introduces a Transformer-based MR image harmonization method. Our proposed method leverages the self-attention mechanism of the Transformer to learn the complex relationships between image patches and effectively transfer the imaging characteristics from a source image domain to a target image domain. We evaluate our approach to state-of-the-art methods using a publicly available dataset of brain MRI scans and show that it provides superior quantitative metrics and visual quality. Furthermore, we demonstrate that the proposed approach is highly resistant to fluctuations in image modality, resolution, and noise. Overall, the experiment results indicate that our approach is a promising method for medical image harmonization that can improve the accuracy and reliability of automated analysis and diagnosis in clinical settings.",
        "link": "http://dx.doi.org/10.1101/2023.08.16.23294184"
    },
    {
        "id": 23748,
        "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
        "authors": "Guo Yang, Daniel Lo, Robert Mullins, Yiren Zhao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.489"
    },
    {
        "id": 23749,
        "title": "Transformer la crise en occasion : le quotidien d’un centre d’accueil et de crise",
        "authors": "Nicolas Gougoulis",
        "published": "2018-11-14",
        "citations": 1,
        "abstract": "En appui sur une étude clinique et sur la littérature actuelle sur le sujet, l’auteur explore les particularités d’un fonctionnement en crise des patients état limite. La prise en charge en institution devient un terrain favorable d’accueil pouvant cependant reproduire, voire amplifier les clivages et les projections des patients. L’équipe d’un centre de crise est dans l’obligation d’inventer des réponses cliniques adaptées pour transformer la crise en occasion de réflexion et par là même ouvrir la possibilité d’une dimension psychothérapeutique du travail psychique.",
        "link": "http://dx.doi.org/10.3917/clini.016.0052"
    },
    {
        "id": 23750,
        "title": "A Transformer-based Approach for Dynamic Referee Assistance",
        "authors": "Trong-Thuan Nguyen, Minh-Triet Tran",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mapr59823.2023.10289042"
    },
    {
        "id": 23751,
        "title": "Transformer Graph Network for Coronary Plaque Localization in CCTA",
        "authors": "Mario Viti, Hugues Talbot, Nicolas Gogin",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi52829.2022.9761646"
    },
    {
        "id": 23752,
        "title": "The current status and prospects of transformer in  multimodality",
        "authors": "Yingjie Zhang",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "At present, the attention mechanism represented by transformer has greatly promoted the development of natural language processing (NLP) and image processing (CV). However, in the multimodal field, the application of attention mechanism still mainly focuses on extracting the features of different types of data, and then fusing these features (such as text and image). With the increasing scale of the model and the instability of the Internet data, feature fusion has been difficult to solve the growing variety of multimodal problems for us, and the multimodal field has always lacked a model that can uniformly handle all types of data. In this paper, we first take the CV and NLP fields as examples to review various derived models of transformer. Then, based on the mechanism of word embedding and image embedding, we discuss how embedding with different granularity is handled uniformly under the attention mechanism in multimodal scenes. Further, we reveal that this mechanism will not only be limited to CV and NLP, but the real unified model will be able to handle tasks across data types through pre-training and fine tuning. Finally, on the specific implementation of the unified model, this paper lists several cases, and analyzes the valuable research directions in related fields.",
        "link": "http://dx.doi.org/10.54254/2755-2721/11/20230240"
    },
    {
        "id": 23753,
        "title": "Transformer-Based Model for Predicting Customers’ Next Purchase Day in e-Commerce",
        "authors": "Alexandru Grigoraș, Florin Leon",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "The paper focuses on predicting the next purchase day (NPD) for customers in e-commerce, a task with applications in marketing, inventory management, and customer retention. A novel transformer-based model for NPD prediction is introduced and compared to traditional methods such as ARIMA, XGBoost, and LSTM. Transformers offer advantages in capturing long-term dependencies within time series data through self-attention mechanisms. This adaptability to various time series patterns, including trends, seasonality, and irregularities, makes them a promising choice for NPD prediction. The transformer model demonstrates improvements in prediction accuracy compared to the baselines. Additionally, a clustered transformer model is proposed, which further enhances accuracy, emphasizing the potential of this architecture for NPD prediction.",
        "link": "http://dx.doi.org/10.3390/computation11110210"
    },
    {
        "id": 23754,
        "title": "Role of nanofiller porosity in monitoring streamer and phonon dynamics in Transformer oil",
        "authors": "Mississippi M Bhunia, Dipanwita Mitra, Kalyan Kumar Chattopadhyay, Paramita Chattopadhyay",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper presents a maiden attempt to witness several nanofluidic properties of porous nanofillers dispersed in Transformer oil (TO). For this, Porous Boron Nitride (PBN)-PBN 950 and PBN 1050 at two different porosities with low (high) and high (low) specific surface area (SSA) and aspect ratio (AR) respectively, both composed of 2D porous flakes of quasi-crystalline BN of hexagonal phase, arranged in rod and flower pattern were obtained via gas-solid interaction at 950 and 1050oC respectively. Nanofluids of both PBNs in TO displayed a linear relationship between porosity and insulation characteristics with ~26-39 % rise in AC Breakdown Voltage along with Resistivity. It was explained by the role of nanofiller porosity in providing large interfacial zones and deep traps for quantitively scavenging and holding streamer charges at the oil-nanofiller interface. Further, despite of being quasi-crystalline and porous, both PBNs in TO upgraded the thermal properties by quasi-ballistic transfer of acoustic phonons via 2D flakes of hexagonal phase. However, the thermal conductivity varied inversely with porosity with ~ 32.5 - 28 % surge with PBN 950 and PBN 1050, respectively at 50oC. This trend was theoretically supported as well as explained due to the increase in phonon-defect scattering owing to the increase in porosity and pore asymmetry together with reduced AR in PBN 1050. Finally, the unchanged flash and pour points with negligible increase in acidity over months plus the aforementioned results at low filler fractions establishes the superiority of PBN over other BN nanofillers for applications in liquid insulation in future",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-3t3kl"
    },
    {
        "id": 23755,
        "title": "Circularly Deformable Medical Image Registration Based on Transformer-CNN with Prompt",
        "authors": "longhao li, Li Li, yunfeng zhang, fangxun bao, Xunxiang Yao, Caiming Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4700960"
    },
    {
        "id": 23756,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "Rubén Romero",
        "published": "2019-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v2/review1"
    },
    {
        "id": 23757,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "Rubén Romero",
        "published": "2019-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v3/review1"
    },
    {
        "id": 23758,
        "title": "A Current Transformer (CT) Saturation Detection Method for Bus Differential Protection",
        "authors": "Monir Hossain, Ittiphong Leevongwat, Parviz Rastgoufard",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc.2018.8664064"
    },
    {
        "id": 23759,
        "title": "Fast-Slow Transformer for Visually Grounding Speech",
        "authors": "Puyuan Peng, David Harwath",
        "published": "2022-5-23",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp43922.2022.9747103"
    },
    {
        "id": 23760,
        "title": "Characterisation of Amorphous Metal Materials for High-Frequency High-Power-Density Transformer",
        "authors": "Anas Bashir-U-Din",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icseng.2018.8638251"
    },
    {
        "id": 23761,
        "title": "Fire Detection using Transformer Network",
        "authors": "Mohammad Shahid, Kai-lung Hua",
        "published": "2021-8-24",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3460426.3463665"
    },
    {
        "id": 23762,
        "title": "Distribution transformer monitoring and reactive power compensation",
        "authors": "Mingfeng LUO, Dongyin LAI",
        "published": "2018-6-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3166/ejee.20.309-324"
    },
    {
        "id": 23763,
        "title": "Insulation optimization of traction transformer for Lightweight Application",
        "authors": "Xiong Bin, Ding YiWei, Huang Kangjie",
        "published": "2022-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems56177.2022.9983308"
    },
    {
        "id": 23764,
        "title": "Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition",
        "authors": "Hai-tao Xu, Jie Zhang, Li-rong Dai",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-18"
    },
    {
        "id": 23765,
        "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
        "authors": "Yu Suzuki, Tsuneo Kato, Akihiro Tamura",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10344"
    },
    {
        "id": 23766,
        "title": "Dynamic demand minimization using a smart transformer",
        "authors": "Junru Chen, Cathal O'Loughlin, Terence O'Donnell",
        "published": "2017-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2017.8216730"
    },
    {
        "id": 23767,
        "title": "WOMEN IN THE MINISTRY OF JESUS: JESUS THE LIBERATOR AND TRANSFORMER",
        "authors": "Stella Bogi",
        "published": "2022",
        "citations": 1,
        "abstract": "Centuries have gone by since Jesus came to liberate and transform men and women. The very truth that women served in the team of Jesus while he was on the earth shows that in an androcentric patriarchal world, women were welcomed. This was made possible because Jesus went against the tide of patriarchy to reach out to the downtrodden, marginalised, and oppressed women. These women were victims of socio-cultural and oral traditions set up by the Jews. Jesus allowed women to be a part of his life. Even before his birth, God prepared women of low estate to be a part of his genealogy. We see the role of women in his birth, ministry, crucifixion, death, and resurrection. Jesus liberated and transformed a woman caught in adultery, the Samaritan woman whose life was in a mess, and women in need of healing to name a few. Women were able to serve on the team of Jesus because of their transformed lives. Many churches have come forward to allow women in the church ministries and to serve God.",
        "link": "http://dx.doi.org/10.54513/bsj.2022.4401"
    },
    {
        "id": 23768,
        "title": "TRANSQLATION: TRANsformer-based SQL RecommendATION",
        "authors": "Shirin Tahmasebi, Amir H. Payberah, Ahmet Soylu, Dumitru Roman, Mihhail Matskin",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386277"
    },
    {
        "id": 23769,
        "title": "On-line medium voltage cell &amp; transformer maintenance",
        "authors": "A. Primadianto, C. Ekana, Y. Devalesy, W. Prabowo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0258"
    },
    {
        "id": 23770,
        "title": "Insulation Residual Life Estimation Based on Transformer Condition Assessment Data",
        "authors": "Alan Sbravati, Luiz V. Cheim, George K. Frimpong",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177293"
    },
    {
        "id": 23771,
        "title": "Relational-Convergent Transformer for image captioning",
        "authors": "Lizhi Chen, You Yang, Juntao Hu, Longyue Pan, Hao Zhai",
        "published": "2023-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.displa.2023.102377"
    },
    {
        "id": 23772,
        "title": "Deepfake audio detection with vision transformer based method",
        "authors": "Guzin Ulutas, Gul Tahaoglu, Beste Ustubioglu",
        "published": "2023-7-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsp59544.2023.10197715"
    },
    {
        "id": 23773,
        "title": "Effect of Magnetic Field on Streamer Dynamics in Transformer Oil",
        "authors": "Mihir Bhatt, Praghnesh Bhatt",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icemce57940.2023.10434177"
    },
    {
        "id": 23774,
        "title": "Attention Based Mapping for Plants Leaf to Classify Diseases using Vision Transformer",
        "authors": "Kumar Rethik, Dhanpratap Singh",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170081"
    },
    {
        "id": 23775,
        "title": "A D-band wide tuning range VCO using switching transformer",
        "authors": "Yu-Teng Chang, Hsin-Chia Lu",
        "published": "2017-6",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mwsym.2017.8058864"
    },
    {
        "id": 23776,
        "title": "Research on harmonic transmission characteristics of capacitor voltage transformer",
        "authors": "Chaofeng Wei, Qionglin Li, Jiandong Jiang",
        "published": "2017-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/piers-fall.2017.8293568"
    },
    {
        "id": 23777,
        "title": "Implementation of Transformer-Based Model for Acute Lymphoblastic Leukemia Segmentation",
        "authors": "Phumiphat Charoentananuwat, Suree Pumrin",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon58879.2023.10322461"
    },
    {
        "id": 23778,
        "title": "A task-unified network with transformer and spatial-temporal convolution for left ventricular quantification",
        "authors": "Dapeng Li, Yanjun Peng, Jindong Sun, Yanfei Guo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nQuantification of the cardiac function is vital for diagnosing and curing the cardiovascular diseases.\nLeft ventricular function measurement is the most commonly used measure to evaluate the function\nof cardiac in clinical practice, how to improve the accuracy of left ventricular quantitative assessment\nresults has always been the subject of research by medical researchers. Although considerable efforts\nhave been put forward to measure the left ventricle (LV) automatically using deep learning methods,\nthe accurate quantification is yet a challenge work as a result of the changeable anatomy structure\nof heart in the systolic diastolic cycle. Besides, most methods used direct regression method which\nlacks of visual based analysis. In this work, a deep learning segmentation and regression task-unified\nnetwork with transformer and spatial-temporal convolution was proposed to segment and quantify\nthe LV simultaneously. The segmentation module leveraged a U-Net like 3D Transformer model to\npredict the contour of three anatomy structures, while the regression module learned spatial-temporal\nrepresentations from the original images and the reconstruct feature maps from segmentation path to\nestimate the fifinally desired quantification metrics. Furthermore, we employed a joint task loss function\nto train the two module networks. Our framework is evaluated on the MICCAI 2017 Left Ventricle\nFull Quantification Challenge dataset. The results of experiments demonstrate the effectiveness of\nour framework, which achieves competitive cardiac quantification metric results and at the same time\nproduces visualized segmentation results that are conducive to later analysis.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2590069/v1"
    },
    {
        "id": 23779,
        "title": "AMST2: Aggregated Multi-Level Spatial and Temporal Context-Based Transformer for Robust Aerial Tracking",
        "authors": "Hasil Park, Injae Lee, Dasol Jeong, Joonki Paik",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecently, many existing visual trackers have made significant progress by incorporating either spatial information from multi-level convolution layers or temporal information for tracking. However, the complementary advantages of both spatial and temporal information cannot be leveraged when these two types of information are used separately. In this paper, we present a new approach for robust visual tracking using a transformer-based model that incorporates both spatial and temporal context information at multiple levels. To integrate the refined similarity maps through multi-level spatial and temporal encoders, we propose an aggregation encoder. Consequently, the output of the proposed aggregation encoder contains useful features that integrate the global contexts of multi-level spatial and the temporal contexts. The contrasting yet complementary representation of multi-level spatial and temporal contexts provided by this feature can help prevent tracking failures in a range of complex aerial scenarios, including occlusion, motion blur, and scale variations. Additionally, the proposed architecture can achieve more robust object tracking against significant variations by updating the features of the latest object while retaining the initial template information. Extensive experiments on five challenging short-term and long-term aerial tracking benchmarks have demonstrated that the proposed tracker outperforms state-of-the-art tracking methods in terms of both real-time processing speed and performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2770662/v1"
    },
    {
        "id": 23780,
        "title": "A Hybrid Transformer Topology for Distribution Network Voltage Regulation",
        "authors": "Rupert Power, Alykhan Mithani, Udaya Madawala, Craig Baguley",
        "published": "2021-12-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec52827.2021.9709487"
    },
    {
        "id": 23781,
        "title": "Peer Review #2 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.2)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.2/reviews/2"
    },
    {
        "id": 23782,
        "title": "Hybrid Condition Monitoring System for Power Transformer Fault Diagnosis",
        "authors": "Engin Baker, Secil Varbak Nese, Erkan Dursun",
        "published": "2023-1-20",
        "citations": 5,
        "abstract": "The important parts of a transformer, such as the core, windings, and insulation materials, are in the oil-filled tank. It is difficult to detect faults in these materials in a closed area. Dissolved Gas Analysis (DGA)-based fault diagnosis methods predict a fault that may occur in the transformer and take the necessary precautions before the fault grows. Although these fault diagnosis methods have an accuracy of over 95%, their validity is controversial since limited data are used in the studies. The success rates and reliability of fault diagnosis methods in transformers, one of the most important pieces of power systems equipment, should be increased. In this study, a hybrid fault diagnosis system is designed using DGA-based methods and Fuzzy Logic. A mathematical approach and support vector machines (SVMs) were used as decision-making methods in the hybrid fault diagnosis systems. The results of tests performed with 317 real fault data sets relating to transformers showed accuracy of 95.58% using a mathematical approach and 96.23% using SVMs.",
        "link": "http://dx.doi.org/10.3390/en16031151"
    },
    {
        "id": 23783,
        "title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video",
        "authors": "Dmitriy Serdyuk, Otavio Braga, Olivier Siohan",
        "published": "2022-9-18",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10920"
    },
    {
        "id": 23784,
        "title": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer",
        "authors": "Shuaicheng Zhang, Qiang Ning, Lifu Huang",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-naacl.29"
    },
    {
        "id": 23785,
        "title": "Hierarchical vision transformer model for polyp segmentation",
        "authors": "Geetha S., Gopakumar C., Vishnu Vinod",
        "published": "2023-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicaps57044.2023.10074447"
    },
    {
        "id": 23786,
        "title": "Multi-tailed vision transformer for efficient inference",
        "authors": "Yunke Wang, Bo Du, Wenyuan Wang, Chang Xu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106235"
    },
    {
        "id": 23787,
        "title": "Frequency Response Quality Index for Assessing the Mechanical Condition of Transformer Windings",
        "authors": "Eugeniusz Kornatowski, Szymon Banaszak",
        "published": "2019-12-19",
        "citations": 9,
        "abstract": "Frequency response analysis (FRA) is a popular method for assessing a transformer’s mechanical condition. The paper proposes a new method for interpreting the frequency response measurement results. The currently used numerical indices only give one value, which may be misleading in the analysis, while the proposed frequency response quality index (FRQI) tool analyses three separate features in the whole frequency range. The applied numerical calculations technique allows for estimations of not only the values of the average quality indices, but also locally for given frequency ranges of the analysed spectrum. It allows for determination of the problems that can be found in the active part of a transformer. The presented results come from three transformers, representing cases of typical faults. Two of them are from industry, while one was used for deformational tests in laboratory conditions. The proposed FRQI method showed its usefulness in FRA test results analysis and may be introduced into the automated assessment of such data. Each of the component parameters is sensitive to other types of differences observed between the compared frequency response curves, and may be used as a good quality detection tool.",
        "link": "http://dx.doi.org/10.3390/en13010029"
    },
    {
        "id": 23788,
        "title": "Variational Transformer Networks for Layout Generation",
        "authors": "Diego Martin Arroyo, Janis Postels, Federico Tombari",
        "published": "2021-6",
        "citations": 43,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr46437.2021.01343"
    },
    {
        "id": 23789,
        "title": "Models of the Reading Architecture",
        "authors": "Erik D. Reichle",
        "published": "2021-9-3",
        "citations": 0,
        "abstract": "This chapter describes what has been learned about reading architecture, or how the mental processes that support word identification, sentence processing, and discourse representation during reading are coordinated with the systems that support vision, attention, and eye-movement control. The chapter reviews key findings that shed light on the nature of reading architecture, mainly using the results of eye-movement experiments. The chapter then reviews precursor theories and models of the reading architecture—early attempts to explain and simulate reading in its entirety. The chapter goes on to review a large, representative sample of the models that have been used to simulate and understand natural reading. Models are reviewed in their order of development to show how they have evolved to accommodate new empirical findings. The chapter concludes with an explicit comparative analysis of the models and a discussion of the empirical findings that each model can and cannot explain.",
        "link": "http://dx.doi.org/10.1093/oso/9780195370669.003.0006"
    },
    {
        "id": 23790,
        "title": "Maximum Likelihood Classification for Transformer Fault Diagnosis Using Dissolved Gas Analysis",
        "authors": "S M Sreelakshmi, Lakshmi Tharamal, P Preetha",
        "published": "2021-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic49891.2021.9612303"
    },
    {
        "id": 23791,
        "title": "Review on Solid State Transformer Based on Microgrids Architectures",
        "authors": "Marwa Chalbi, Ghada Boukettaya",
        "published": "2021-3-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssd52085.2021.9429444"
    },
    {
        "id": 23792,
        "title": "Agroécologie et reconstruction d’une agriculture post-covid-19",
        "authors": "Miguel A. Altieri, Clara Ines Nicholls, Maurice Hérion",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0109"
    },
    {
        "id": 23793,
        "title": "Determination of Per-Phase Equivalent Circuit Parameters of Three-Phase Transformer Using MATLAB/Simulink",
        "authors": "Francis Katende, James Katende",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ist-africa56635.2022.9845552"
    },
    {
        "id": 23794,
        "title": "Improving HS-DACS Based Streaming Transformer ASR with Deep Reinforcement Learning",
        "authors": "Mohan Li, Rama Doddipatla",
        "published": "2021-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru51503.2021.9688268"
    },
    {
        "id": 23795,
        "title": "Computationally efficient leakage inductance calculation for a high-frequency core-type transformer",
        "authors": "Veda Samhitha Duppalli, Scott Sudhoff",
        "published": "2017-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ests.2017.8069348"
    },
    {
        "id": 23796,
        "title": "Transformer-Based Unsupervised Cross-Sensor Domain Adaptation for Electromechanical Actuator Fault Diagnosis",
        "authors": "Zihan Chen, Chao He",
        "published": "2023-1-11",
        "citations": 1,
        "abstract": "There have been some successful attempts to develop data-driven fault diagnostic methods in recent years. A common assumption in most studies is that the data of the source and target domains are obtained from the same sensor. Nevertheless, because electromechanical actuators may have complex motion trajectories and mechanical structures, it may not always be possible to acquire the data from a particular sensor position. When the sensor locations of electromechanical actuators are changed, the fault diagnosis problem becomes further complicated because the feature space is significantly distorted. The literature on this subject is relatively underdeveloped despite its critical importance. This paper introduces a Transformer-based end-to-end cross-sensor domain fault diagnosis method for electromechanical actuators to overcome these obstacles. An enhanced Transformer model is developed to obtain domain-stable features at various sensor locations. A convolutional embedding method is also proposed to improve the model’s ability to integrate local contextual information. Further, the joint distribution discrepancy between two sensor domains is minimized by using Joint Maximum Mean Discrepancy. Finally, the proposed method is validated using an electromechanical actuator dataset. Twenty-four transfer tasks are designed to validate cross-sensor domain adaptation fault diagnosis problems, covering all combinations of three sensor locations under different operating conditions. According to the results, the proposed method significantly outperforms the comparative method in terms of varying sensor locations.",
        "link": "http://dx.doi.org/10.3390/machines11010102"
    },
    {
        "id": 23797,
        "title": "RLFAT: A Transformer-Based Relay Link Forged Attack Detection Mechanism in SDN",
        "authors": "Tianyi Zhang, Yong Wang",
        "published": "2023-5-15",
        "citations": 1,
        "abstract": "SDN is a modern internet architecture that has transformed the traditional internet structure in recent years. By segregating the control and data planes of the network, SDN facilitates centralized management, scalability, dynamism, and programmability. However, this very feature makes SDN controllers vulnerable to cyber attacks, which can cause network-wide crashes, unlike conventional networks. One of the most stealthy attacks that SDN controllers face is the relay link forgery attack in topology deception attacks. Such an attack can result in erroneous overall views for SDN controllers, leading to network functionality breakdowns and even crashes. In this article, we introduce the Relay Link Forgery Attack detection model based on the Transformer deep learning model for the first time. The model (RLFAT) detects relay link forgery attacks by extracting features from network flows received by SDN controllers. A dataset of network flows received by SDN controllers from a large number of SDN networks with different topologies was collected. Finally, the Relay-based Link Forgery Attack detection model was trained on this dataset, and its performance was evaluated using accuracy, recall, F1 score, and AUC metrics. For better validation, comparative experiments were conducted with some common deep learning models. The experimental results show that our proposed model (RLFAT) has good performance in detecting RLFA and outperforms other models.",
        "link": "http://dx.doi.org/10.3390/electronics12102247"
    },
    {
        "id": 23798,
        "title": "A Single-Phase Electromagnetic Transformer with an Adjustable Output Voltage",
        "authors": "Junwei Cui, Liyan Qu, Wei Qiao",
        "published": "2019-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8912933"
    },
    {
        "id": 23799,
        "title": "Impact of distribution of impurity particles on electric strength of transformer oil",
        "authors": "O.S. Melnikova",
        "published": "2019",
        "citations": 1,
        "abstract": "To extend the service life and ensure the operability of oil-filled transformer equipment, the attention is paid to the development of methods for monitoring the state of oil-barrier insulation. When monitoring the technical condition of transformer oil, the class of liquid purity is determined depending on the rated voltage of the equipment. However, the influence of the parameters of mechanical impurities on the breakdown voltage is not taken into account, thereby lowering the requirements for the quality of oil barrier insulation. This makes it relevant to study the influence of the size distribution of impurity particles on the electric strength of the internal insulation of power transformers and determine the parameters of particles of mechanical impurities to justify the underestimation of the quality indicators of transformer oil in operation. Methods of mathematical statistics were employed using the Gnedenko-Weibull distribution based on the standard values of liquid purity classes. To determine the maximum and minimum voltages, the standard values of the average breakdown voltages and the results of operational tests of transformer oil in a standard spark gap were used. The relation between the particle size of mechanical impurities and the breakdown voltage of transformer oil has been established. The particle size distribution of impurities has been obtained for 12 and 13 classes of liquid purity for power transformers with a voltage of 110–750 kV. The particle size range that defines the maximum and minimum breakdown voltages has been determined, and the values of limit concentrations of mechanical particles have been established. The obtained parameters of impurity particles which determine the maximum and minimum breakdown voltages of the operating oils can be used to evaluate the technical condition when diagnosing the internal insulation of power transformers in order to increase their operational reliability, as well as to adjust the regulatory requirements for the quality of operational transformer oil according to the content of mechanical impurities.",
        "link": "http://dx.doi.org/10.17588/2072-2672.2019.6.041-049"
    },
    {
        "id": 23800,
        "title": "GUILGET: GUI Layout GEneration with Transformer",
        "authors": "Andrey Sobolevsky, Guillaume-Alexandre Bilodeau, Jinghui Cheng, Jin L.C. Guo",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.08fe0a25"
    },
    {
        "id": 23801,
        "title": "A New Method to Identify and Blocking Inrush Current in Transformer",
        "authors": "Cyrus Ghodrati",
        "published": "2023-2-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpeee56777.2023.10217729"
    },
    {
        "id": 23802,
        "title": "Multimodal Transformer for Nursing Activity Recognition",
        "authors": "Momal Ijaz, Renato Diaz, Chen Chen",
        "published": "2022-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw56347.2022.00224"
    },
    {
        "id": 23803,
        "title": "Power Loss Reduction and Voltage Regulation in Loop Distribution Lines Using Sen Transformer",
        "authors": "Dhrupa Patel, Anandita Chawdhury",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3552471"
    },
    {
        "id": 23804,
        "title": "Using the FDS Technique in Transformer Factory Drying",
        "authors": "D. F. Garcia, D. Cordoba, J. Vasquez",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/arwtr.2019.8930177"
    },
    {
        "id": 23805,
        "title": "County augmented transformer for COVID-19 state hospitalizations prediction",
        "authors": "Siawpeng Er, Shihao Yang, Tuo Zhao",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "AbstractThe prolonged COVID-19 pandemic has tied up significant medical resources, and its management poses a challenge for the public health care decision making. Accurate predictions of the hospitalizations are crucial for the decision makers to make informed decision for the medical resource allocation. This paper proposes a method named County Augmented Transformer (CAT). To generate accurate predictions of four-week-ahead COVID-19 related hospitalizations for every states in the United States. Inspired by the modern deep learning techniques, our method is based on a self-attention model (known as the transformer model) that is actively used in Natural Language Processing. Our transformer based model can capture both short-term and long-term dependencies within the time series while enjoying computational efficiency. Our model is a data based approach that utilizes the publicly available information including the COVID-19 related number of confirmed cases, deaths, hospitalizations data, and the household median income data. Our numerical experiments demonstrate the strength and the usability of our model as a potential tool for assisting the medical resources allocation.",
        "link": "http://dx.doi.org/10.1038/s41598-023-36378-9"
    },
    {
        "id": 23806,
        "title": "Oil Movement In Closed Environment Of Distribution Transformer Tank Problem Simulation",
        "authors": "",
        "published": "2017-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15192/pscp.asr.2017.18.2.6166"
    },
    {
        "id": 23807,
        "title": "Paying attention to astronomical transients: Introducing the time-series transformer for photometric classification",
        "authors": "Tarek Allam, Jason D McEwen",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "Abstract\nFuture surveys such as the Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will observe an order of magnitude more astrophysical transient events than any previous survey before. With this deluge of photometric data, it will be impossible for all such events to be classified by humans alone. Recent efforts have sought to leverage machine learning methods to tackle the challenge of astronomical transient classification, with ever improving success. Transformers are a recently developed deep learning architecture, first proposed for natural language processing, that have shown a great deal of recent success. In this work we develop a new transformer architecture, which uses multi-head self attention at its core, for general multi-variate time-series data. Furthermore, the proposed time-series transformer architecture supports the inclusion of an arbitrary number of additional features, while also offering interpretability. We apply the time-series transformer to the task of photometric classification, minimising the reliance of expert domain knowledge for feature selection, while achieving results comparable to state-of-the-art photometric classification methods. We achieve a logarithmic-loss of 0.507 on imbalanced data in a representative setting using data from the Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC). Moreover, we achieve a micro-averaged receiver operating characteristic area under curve of 0.98 and micro-averaged precision-recall area under curve of 0.87.",
        "link": "http://dx.doi.org/10.1093/rasti/rzad046"
    },
    {
        "id": 23808,
        "title": "An Improved Multi-reference Frame Loop Filter Algorithm Based on Transformer for VVC",
        "authors": "Zhi Liu, Yunpeng Duan, Mengmeng Zhang",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dcc52660.2022.00078"
    },
    {
        "id": 23809,
        "title": "Study on Discrimination between Inrush and Fault in Transformer: ANN Approach",
        "authors": "S. R. Paraskar",
        "published": "2022-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9734/bpi/rtcams/v8/2606c"
    },
    {
        "id": 23810,
        "title": "Design of a Low-Voltage Distribution Transformer Based on Inductive Filtering",
        "authors": "",
        "published": "2022-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17559/tv-20210812043603"
    },
    {
        "id": 23811,
        "title": "Solid state tesla transformer for flashover test on suspension insulators",
        "authors": "Watchara Pongsathit, Peerawut Yutthagowith, Worrakan Limcharoen",
        "published": "2017-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iseim.2017.8088731"
    },
    {
        "id": 23812,
        "title": "Dielectric spectroscopy of magnetic fluid based on transformer oil",
        "authors": "Jozef Kudelcik, Stefan Hardon, Miroslav Gutten",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/diagnostika49114.2020.9214648"
    },
    {
        "id": 23813,
        "title": "Assessment of Best Practices for Mitigation of Rapid Voltage Change due to Transformer Inrush",
        "authors": "Gaurav Singh, Carl Miller, William Howe",
        "published": "2019-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ptc.2019.8810934"
    },
    {
        "id": 23814,
        "title": "Estimating time-delayed variables using transformer-based soft sensors",
        "authors": "Jelke Wibbeke, Darian Alves, Sebastian Rohjans",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "AbstractIn the course of digitization, there is an increased interest in sensor data, including data from old systems with a service life of several decades. Since the installation of sensor technology can be quite expensive, soft sensors are often used to enhance the monitoring capabilities. Soft sensors use easy-to-measure variables to predict hard-to-measure variables, employing arbitrary models. This is particularly challenging if the observed system is complex and exhibits dynamic behavior, e.g., transient responses after changes in the system. Data-driven models are, therefore, often used. As recent studies suggest using Transformer-based models for regression tasks, this paper investigates the use of Transformer-based soft sensors for modelling the dynamic behavior of systems. To this extent, the performance of Multilayer Perceptron (MLP) and Long Short-term Memory (LSTM) models are compared to Transformers, based on two data sets featuring dynamic behavior in terms of time-delayed variables. The outcomes of this paper demonstrate that while the Transformer can map time delays, it is outperformed by MLP and LSTM. This deviation from previous Transformer evaluations is noteworthy as it may be influenced by the dynamic characteristics of the input data set, and its attention-based mechanism may not be optimized for sequential data. It is important to mention that the previous studies in this area did not focus on time-delayed dynamic variables.",
        "link": "http://dx.doi.org/10.1186/s42162-023-00274-3"
    },
    {
        "id": 23815,
        "title": "Fixed-Point Optimization of Transformer Neural Network",
        "authors": "Yoonho Boo, Wonyong Sung",
        "published": "2020-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9054724"
    },
    {
        "id": 23816,
        "title": "Patch-wise Mixed-Precision Quantization of Vision Transformer",
        "authors": "Junrui Xiao, Zhikai Li, Lianwei Yang, Qingyi Gu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191205"
    },
    {
        "id": 23817,
        "title": "Practical Evaluation of a Brush-less DFIG Employing a Rotary Transformer",
        "authors": "Stefan Botha, Nkosinathi Gule",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec60914.2024.10445081"
    },
    {
        "id": 23818,
        "title": "Integrating Self-Attention Transformer with Triplet Neural Networks for Protein Gene Ontology Prediction",
        "authors": "Yi-Heng Zhu, Chengxin Zhang, Dong-Jun Yu, Yang Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAccurate identification of protein function is critical to elucidate life mechanism and design new drugs. We proposed a novel deep-learning method, ATGO, to predict Gene Ontology (GO) attributes of proteins through a triplet neural-network architecture embedded with pre-trained self-attention transformer models. The method was systematically tested on 1068 non-redundant benchmarking proteins and 3328 targets from the third Critical Assessment of Protein Function Annotation (CAFA) challenge. Experimental results showed that ATGO achieved a significant increase of the GO prediction accuracy compared to the state-of-the-art approaches in all aspects of molecular function, biological process, and cellular component. Detailed data analyses showed that the major advantage of ATGO lies in the utilization of attention transformer models which can extract discriminative functional pattern from the feature embeddings. Meanwhile, the proposed triplet network helps enhance the association of functional similarity with feature similarity in the sequence embedding space. In addition, it was found that the combination of the network scores with the complementary homology-based inferences could further improve the accuracy and coverage of the predicted models. These results demonstrated a new avenue for high-accuracy deep-learning function prediction that is applicable to large-scale protein function annotations from sequence alone.AvailabilityThe benchmark dataset, standalone package, and online server for ATGO are available at https://zhanggroup.org/ATGO/.Author SummaryIn the post-genome sequencing era, a major challenge in computational molecular biology is to annotate the biological functions of all gene and gene products, which have been classified, in the context of the widely used Gene Ontology (GO), into three aspects of molecular function, biological process, and cellular component. In this work, we proposed a new open-source deep-learning architecture, ATGO, to deduce GO terms of proteins from the primary amino acid sequence, through the integration of the triplet neural-network with attention transformer models. Large benchmark tests showed that, when powered with a pre-trained self-attention transformer model, ATGO achieved a significantly improved performance than other state-of-the-art approaches in all the GO aspect predictions. Following the rapid progress of self-attention neural network techniques, which have demonstrated remarkable impacts on language processing and multi-sensory data process, and most recently on protein structure prediction, this study showed the significant potential of attention transformer models on protein function annotations.",
        "link": "http://dx.doi.org/10.1101/2022.07.07.499156"
    },
    {
        "id": 23819,
        "title": "HFFT: Hierarchical Global-local Feature Fusion Transformer Network for Facial Expression Recognition in the Wild",
        "authors": "Rui Xu, Aibin Huang, Yuanjing Hu, Xibo Feng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4421668"
    },
    {
        "id": 23820,
        "title": "Transformer based Natural Language Generation for Question-Answering",
        "authors": "Imen Akermi, Johannes Heinecke, Frédéric Herledan",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.41"
    },
    {
        "id": 23821,
        "title": "S3T: Self-Supervised Pre-training with Swin Transformer for Music Classification",
        "authors": "Hang Zhao, Chen Zhang, Bilei Zhu, Zejun Ma, Kejun Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we propose S3T, a self-supervised pre-training method with Swin Transformer for music classification, aiming to learn meaningful music representations from massive easily accessible unlabeled music data. S3T introduces a momentum-based paradigm, MoCo, with Swin Transformer as its feature extractor to music time-frequency domain. For better music representations learning, S3T contributes a music data augmentation pipeline and two specially designed pre-processors. To our knowledge, S3T is the first method combining the Swin Transformer with a self-supervised learning method for music classification. We evaluate S3T on music genre classification and music tagging tasks with linear classifiers trained on learned representations. Experimental results show that S3T outperforms the previous self-supervised method (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two tasks respectively, and also surpasses the task-specific state-of-the-art supervised methods. Besides, S3T shows advances in label efficiency using only 10\\% labeled data exceeding CLMR on both tasks with 100\\% labeled data.",
        "link": "http://dx.doi.org/10.31219/osf.io/buwcv"
    },
    {
        "id": 23822,
        "title": "Influence of Core Temperature on the Efficiency of a Planar Transformer",
        "authors": "Constantin Ropoteanu, Paul Svasta",
        "published": "2020-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isse49702.2020.9121037"
    },
    {
        "id": 23823,
        "title": "Effective Document Image Enhancement Using tokens-to-token Transformer Network",
        "authors": "Risab Biswas, Swalpa  Kumar Roy, Umapada Pal",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4354038"
    },
    {
        "id": 23824,
        "title": "Vehicle Re-Identification Method Based on Swin-Transformer Network",
        "authors": "Jianrong Li, Chang Yu, Chuanlei Zhang, Ting Ke",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4207513"
    },
    {
        "id": 23825,
        "title": "Span-adaptive Transformer for the Cascade Relation Triple Extraction",
        "authors": "Kai Liu, Hongjun Zhang",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaice54393.2021.00074"
    },
    {
        "id": 23826,
        "title": "Partial discharge monitoring using EMWs",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00008-2"
    },
    {
        "id": 23827,
        "title": "New soft sensors for distribution transformer monitoring",
        "authors": "Jean François Tissier, Jérôme Cornet",
        "published": "2017-10-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/oap-cired.2017.0840"
    },
    {
        "id": 23828,
        "title": "Multi-Step Day-Ahead Photovoltaic Power Forecasting Using Transformer and Pretrained Recurrent Neural Networks",
        "authors": "Jimin Kim, Josue Obregon, Hoonseok Park, Jae-Yoon Jung",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4672582"
    },
    {
        "id": 23829,
        "title": "Segmentation of Liver CT Images Based on Weighted Medical Transformer Model",
        "authors": "Qun Gu, Hai Zhang, Rui Cai, Si Yi Sui, Rui Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep Convolutional Neural Networks have been developed in the field of medical image segmentation, although the existing convolutional structure in which the local information of the image is utilized improves the performance, but the dependency information between the contexts is lost. Therefore highly expresses the relationship between contexts using the multi-attention mechanism in the Transformer structureand applies the Transformer network architecture to medical CT liver image segmentation. Most of the models based on this Transformer structure require a large number of datasets for training, but for the medical field, the sample size of the dataset is so small that it is difficult to train the model of the Transformer structure, for this reason, a Weighted Medical Transformer (WMT) model is proposed. This weighting mechanism can improve the problem of inaccurate relative position encoding when the amount of data is insufficient. At the same time, the feature fusion of coarse-grained segmentation and fine-grained segmentation has the advantage of focusing on the inter-block dependency information in coarse-grained segmentation and the intra-block detail information in fine-grained segmentation. The experimental results of this model on the CT dataset show that the F1 score and Iou score are as high as 88.48% and 79.41%, and the accuracy is higher than that of uet++ and Axial Attention U-Net and so on. The method does not require a large dataset and has high execution efficiency compared to other methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3742998/v1"
    },
    {
        "id": 23830,
        "title": "Data Processing with Uncertain Transformer Taps.(Dept.E)",
        "authors": "Mofreh Mohamed",
        "published": "2021-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21608/bfemu.2021.172666"
    },
    {
        "id": 23831,
        "title": "Characterizations of air bubble movement in different transformer oil channel structures",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ijfet.2023.050707"
    },
    {
        "id": 23832,
        "title": "Retrieval-Based Transformer Pseudocode Generation",
        "authors": "Anas Alokla, Walaa Gad, Waleed Nazih, Mustafa Aref, Abdel-Badeeh Salem",
        "published": "2022-2-16",
        "citations": 7,
        "abstract": "The comprehension of source code is very difficult, especially if the programmer is not familiar with the programming language. Pseudocode explains and describes code contents that are based on the semantic analysis and understanding of the source code. In this paper, a novel retrieval-based transformer pseudocode generation model is proposed. The proposed model adopts different retrieval similarity methods and neural machine translation to generate pseudocode. The proposed model handles words of low frequency and words that do not exist in the training dataset. It consists of three steps. First, we retrieve the sentences that are similar to the input sentence using different similarity methods. Second, pass the source code retrieved (input retrieved) to the deep learning model based on the transformer to generate the pseudocode retrieved. Third, the replacement process is performed to obtain the target pseudo code. The proposed model is evaluated using Django and SPoC datasets. The experiments show promising performance results compared to other language models of machine translation. It reaches 61.96 and 50.28 in terms of BLEU performance measures for Django and SPoC, respectively.",
        "link": "http://dx.doi.org/10.3390/math10040604"
    },
    {
        "id": 23833,
        "title": "Unexpected Transformer Tripping Provides Insight on Circulating Inrush Current",
        "authors": "P.E. Neeraj Karnik, P.E. Jeremy Kupcho",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10252725"
    },
    {
        "id": 23834,
        "title": "Deep Magnetic Resonance Fingerprinting Based on Local and Global Vision Transformer",
        "authors": "Peng Li, Yue Hu",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230741"
    },
    {
        "id": 23835,
        "title": "Transformer for Automated Feedback System Design",
        "authors": "Isaac Hughes, John F. O’Brien",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aim46323.2023.10196217"
    },
    {
        "id": 23836,
        "title": "TABASCO: A transformer based contextualization toolkit",
        "authors": "Ambarish Moharil, Arpit Sharma",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.scico.2023.102994"
    },
    {
        "id": 23837,
        "title": "Continuous Operation of Smart Transformer-fed Distribution Grid with Single-phase Faults",
        "authors": "Rongwu Zhu, Marco Liserre",
        "published": "2018-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2018.8558226"
    },
    {
        "id": 23838,
        "title": "Simulation analysis of a control system for a Solid-State Transformer",
        "authors": "Mario Jorge Marques, Rui Esteves Araujo",
        "published": "2021-7-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yef-ece52297.2021.9505099"
    },
    {
        "id": 23839,
        "title": "Enhanced Security with Encrypted Vision Transformer in Federated Learning",
        "authors": "Aso Rei, Shiota Sayaka, Kiya Hitoshi",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315352"
    },
    {
        "id": 23840,
        "title": "Transformer Fault Diagnosis based on Deep Brief Sparse Autoencoder",
        "authors": "Zhong Xu, Wenxiong Mo, Yong Wang, Simin Luo, Tian Liu",
        "published": "2019-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866347"
    },
    {
        "id": 23841,
        "title": "Voice Conversion with Transformer Network",
        "authors": "Ruolan Liu, Xiao Chen, Xue Wen",
        "published": "2020-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9054523"
    },
    {
        "id": 23842,
        "title": "FITrans: Skin Lesion Segmentation Based on Feature Integration and Transformer",
        "authors": "Likun Zhang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105777"
    },
    {
        "id": 23843,
        "title": "Hybrid Vision Transformer Model for Hyperspectral Image Classification",
        "authors": "Jiaqi Yang, Bo Du, Chen Wu",
        "published": "2022-7-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9884262"
    },
    {
        "id": 23844,
        "title": "Geometric Transformer for End-to-End Molecule Properties Prediction",
        "authors": "Yoni Choukroun, Lior Wolf",
        "published": "2022-7",
        "citations": 4,
        "abstract": "Transformers have become methods of choice in many applications thanks to their ability to represent complex interactions between elements. \n\nHowever, extending the Transformer architecture to non-sequential data such as molecules and enabling its training on small datasets remains a challenge. \n\nIn this work, we introduce a Transformer-based architecture for molecule property prediction, which is able to capture the geometry of the molecule. \n\nWe modify the classical positional encoder by an initial encoding of the molecule geometry, as well as a learned gated self-attention mechanism. \n\nWe further suggest an augmentation scheme for molecular data capable of avoiding the overfitting induced by the overparameterized architecture. \n\nThe proposed framework outperforms the state-of-the-art methods while being based on pure machine learning solely, i.e. the method does not incorporate domain knowledge from quantum chemistry and does not use extended geometric inputs besides the pairwise atomic distances.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/401"
    },
    {
        "id": 23845,
        "title": "Style Augmented Transformer Architecture for Automatic Essay Assessment",
        "authors": "Tirthankar Dasgupta, Gaurav K. Singh, Lipika Dey",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt58122.2023.00105"
    },
    {
        "id": 23846,
        "title": "Person Re-Identification Based on Improved Transformer and CNN",
        "authors": "Yuyun Dai",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccasit58768.2023.10351748"
    },
    {
        "id": 23847,
        "title": "Adaptive Hybrid Vision Transformer for Small Datasets",
        "authors": "Mingjun Yin, Zhiyong Chang, Yan Wang",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00132"
    },
    {
        "id": 23848,
        "title": "Cgt: A Clause Graph Transformer Structure for Aspect-Based Sentiment Analysis",
        "authors": "Zelong Su, Bin Gao, Xiaoou Pan, Yu Ji, Shutian Liu, Zhengjun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697938"
    },
    {
        "id": 23849,
        "title": "Review for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "Rubén Romero",
        "published": "2019-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v1/review2"
    },
    {
        "id": 23850,
        "title": "Peer Review #1 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.3)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.3/reviews/1"
    },
    {
        "id": 23851,
        "title": "Voltage Balance Control of Five-Level Cascaded H-Bridge Rectifier-Based Smart Transformer",
        "authors": "Hiba Helali, Adel Khedher",
        "published": "2022-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irec56325.2022.10002082"
    },
    {
        "id": 23852,
        "title": "Transformer based multimodal similarity search method for E-Commerce platforms",
        "authors": "Chandan Charchit Sahoo, Deepak Singh Tomar, Jyoti Bharti",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183514"
    },
    {
        "id": 23853,
        "title": "SPRNet: Sitting Posture Recognition Using improved Vision Transformer",
        "authors": "Yi Fang, Shoudong Shi, Jingsen Fang, Wenting Yin",
        "published": "2022-7-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892021"
    },
    {
        "id": 23854,
        "title": "Analysis and Design of a Special Type Power Transformer Used in Solar Power Plants",
        "authors": "",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20508/ijrer.v12i2.12857.g8454"
    },
    {
        "id": 23855,
        "title": "Remote voltage estimation in LV feeders with local monitoring at transformer level",
        "authors": "Valentin Rigoni, Andrew Keane",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8273891"
    },
    {
        "id": 23856,
        "title": "TRANSFORMER AND INDUCTOR DESIGN EXAMPLES",
        "authors": "",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315214856-3"
    },
    {
        "id": 23857,
        "title": "Stacked Graph Transformer for HIV Molecular Prediction",
        "authors": "Yogesh Raisinghani, Aagam Shah, Mrugendrasinh Rahevar",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/c2i454156.2021.9689399"
    },
    {
        "id": 23858,
        "title": "A 20MHz-Transformer-Based Isolated Gate Driver for 3.3kV SiC MOSFETs",
        "authors": "Zhehui Guo, Hui Li",
        "published": "2021-6-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec42165.2021.9487227"
    },
    {
        "id": 23859,
        "title": "AST-Net: Lightweight Hybrid Transformer for Multimodal Brain Tumor Segmentation",
        "authors": "Peixu Wang, Shikun Liu, Jialin Peng",
        "published": "2022-8-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956705"
    },
    {
        "id": 23860,
        "title": "Data Augmentation for Transformer-based G2P",
        "authors": "Zach Ryan, Mans Hulden",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.sigmorphon-1.21"
    },
    {
        "id": 23861,
        "title": "Model-Based Control of a Digital Hydraulic Transformer-Based Hybrid Actuator",
        "authors": "Matti Linjama",
        "published": "2018-9-12",
        "citations": 0,
        "abstract": "Energy-efficient motion control of hydraulic actuators is a challenging task. Throttle-free solutions have the potential for high efficiency. The main throttle-free approaches are pump-controlled systems, transformer-based solutions, and digital hydraulic solutions, such as switching transformers, multi-chamber cylinder and multi-pressure systems. This paper presents a novel solution based on a so-called digital hydraulic power management system (DHPMS). The DHPMS is freely rotating and a hydraulic accumulator is used for energy storage. In contrast to existing approaches, each actuator has its own DHPMS and a small accumulator to locally handle the power peaks. Only an average amount of power is needed from the hydraulic grid, radically reducing the size of the supply pump and the hydraulic piping and hosing. Pump flow is only 12.5% of the peak flow of the actuator in the case studied. Control of this type of system is challenging, and the model-based approach is used. The controller uses a simplified model and functionality is verified by using a detailed simulation model of the system. The results show that the approach is feasible but is demanding on the control valves. The system delay is also relatively long, which reduces the control performance in high-end systems. Nevertheless, this approach has potential in mobile machines, for example.",
        "link": "http://dx.doi.org/10.1115/fpmc2018-8866"
    },
    {
        "id": 23862,
        "title": "Efficient Transformer Inference with Statically Structured Sparse Attention",
        "authors": "Steve Dai, Hasan Genc, Rangharajan Venkatesan, Brucek Khailany",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247993"
    },
    {
        "id": 23863,
        "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures",
        "authors": "Sushant Singh, Ausif Mahmood",
        "published": "2021",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3077350"
    },
    {
        "id": 23864,
        "title": "GS9.3 - Pd-coated SnO2 Nanorod Arrays for Detection of Dissolved H2 in Transformer Oil",
        "authors": "M. H. Kim, B. Jang, W. Lee",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5162/imcs2018/gs9.3"
    },
    {
        "id": 23865,
        "title": "Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations",
        "authors": "Kaden Griffith, Jugal Kalita",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci49370.2019.00101"
    },
    {
        "id": 23866,
        "title": "Transformer balun design in Gallium Arsenide and Silicon Germanium processes",
        "authors": "Sudipta Chakraborty, Leigh E. Milner, Anthony Parker, Michael Heimlich",
        "published": "2018-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ausms.2018.8346963"
    },
    {
        "id": 23867,
        "title": "Frequency domain transformer model for electromagnetic transient analysis of networks",
        "authors": "Ghassan Bilal, Juan M. Villanueva-Ramirez, Pablo Gomez",
        "published": "2017-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps.2017.8107183"
    },
    {
        "id": 23868,
        "title": "A Transformer-based Attention Flow Model for Intelligent Question and Answering Chatbot",
        "authors": "Yunze Xiao",
        "published": "2022-1-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccrd54409.2022.9730454"
    },
    {
        "id": 23869,
        "title": "Research on Power Dispatching Speech Recognition Based on Improved Transformer",
        "authors": "HanJun Wang, HaoYang Lin",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictech58362.2023.00103"
    },
    {
        "id": 23870,
        "title": "Multi-Spatial Scale Spatio-Temporal Transformer: A Refined Traffic Data Forecasting Method",
        "authors": "悦 张, Lei Zhang, Bailong Liu, Zhizhen Liang, Xuefei Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAccurate traffic data forecasting is essential to improve the efficiency of intelligent transportation systems. Existing traffic prediction models only model spatial dependency based on the connectivity of roads, which overlooks the characteristic information of hidden spatial dependency and leads to a loss of prediction accuracy. In addition, there exists a strict relative positional relationship in the temporal dependency between traffic data, which is often overlooked by existing models, making it difficult to accurately model the temporal dependency. To solve these problems, this paper proposes a traffic data prediction method (MSS-STT) based on Multi-Spatial Scale Spatio-Temporal Transformer. MSS-STT first employs multiple specialized spatial Transformer networks to model different spatial scales in order to capture spatial dependencies and patterns at various levels. It also utilizes graph convolutional neural networks to extract static spatial structural features. Then, a gating mechanism is used to fuse the spatial dependencies from different spatial scales and the static spatial features. Finally, MSS-STT extracts different temporal dependencies by considering the order of time points and the varying contributions to the prediction from different relative positions between time points in historical traffic data. Experiments on three real-world datasets from the Caltrans Performance Measurement System (PeMS) show that the proposed MSS-STT model outperforms the state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3190420/v1"
    },
    {
        "id": 23871,
        "title": "An Improved Template Representation-based Transformer for Abstractive Text Summarization",
        "authors": "Jiaming Sun, Yunli Wang, Zhoujun Li",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207609"
    },
    {
        "id": 23872,
        "title": "Unsymmetrical Fault Analysis of PV for Different Transformer Configurations",
        "authors": "Sunil Kumar Maurya, Surayanarayana Gangolu, Saumendra Sarangi",
        "published": "2020-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/piicon49524.2020.9112957"
    },
    {
        "id": 23873,
        "title": "Power Transformer Characterization and Design Optimization Environment",
        "authors": "Michael A. Gutierrez-McCoy, A. A. Arkadan",
        "published": "2022-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cefc55061.2022.9940879"
    },
    {
        "id": 23874,
        "title": "Mwlt: Transformer-Based Missing Well Log Prediction",
        "authors": "Lei Lin, Hao Wei, Tiantian Wu, Pengyun Zhang, Zhi Zhong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4236228"
    },
    {
        "id": 23875,
        "title": "Transformer-based Hierarchical Topic-to-Essay Generation",
        "authors": "Wangbo He, Yuan Rao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.04.058"
    },
    {
        "id": 23876,
        "title": "Oxford miRNA Gardener",
        "authors": "Yoichi Robertus Fujii",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3165-1_2"
    },
    {
        "id": 23877,
        "title": "Online teaching emotion analysis based on GRU and nonlinear transformer algorithm",
        "authors": "Lan Ding",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "Nonlinear models of neural networks demonstrate the ability to autonomously extract significant attributes from a given target, thus facilitating automatic analysis of classroom emotions. This article introduces an online auxiliary tool for analyzing emotional states in virtual classrooms using the nonlinear vision algorithm Transformer. This research uses multimodal fusion, students’ auditory input, facial expression and text data as the foundational elements of sentiment analysis. In addition, a modal feature extractor has been developed to extract multimodal emotions using convolutional and gated cycle unit (GRU) architectures. In addition, inspired by the Transformer algorithm, a cross-modal Transformer algorithm is proposed to enhance the processing of multimodal information. The experiments demonstrate that the training performance of the proposed model surpasses that of similar methods, with its recall, precision, accuracy, and F1 values achieving 0.8587, 0.8365, 0.8890, and 0.8754, respectively, which is superior accuracy in capturing students’ emotional states, thus having important implications in assessing students’ engagement in educational courses.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1696"
    },
    {
        "id": 23878,
        "title": "Flow and Temperature Visualization Over Radiators of Transformer Using Experimental and Advanced CFD Simulations",
        "authors": "Sachin B. Paramane",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3102052"
    },
    {
        "id": 23879,
        "title": "Automatic license plate recognition using transformer",
        "authors": "Zhang Teng, Jia Wei",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2680529"
    },
    {
        "id": 23880,
        "title": "Explainable Anomaly Detection Using Vision Transformer Based SVDD",
        "authors": "Ji-Won Baek, Kyungyong Chung",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.035246"
    },
    {
        "id": 23881,
        "title": "Frequency response modeling of power transformer windings considering the attributes of ferromagnetic core",
        "authors": "Katarzyna Trela, Konstanty Marek Gawrylczyk",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiphdw.2018.8388328"
    },
    {
        "id": 23882,
        "title": "Dynamic Short Circuit Withstand Competency of Power Transformer",
        "authors": "Divyesh U Vasava, Sanjay N Patel",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccs45141.2019.9065906"
    },
    {
        "id": 23883,
        "title": "A Review of Transformer Fault Diagnosis Based on Information System Theory and Machine Learning",
        "authors": "Xiaoli Huang, Yumiao Yuan, Jingyu Li",
        "published": "No Date",
        "citations": 1,
        "abstract": "Safe, high-quality and economical electric energy transportation is the basic requirements of modern power system operation. Transformer, as one of the core equipment of power system and national grid system, has the characteristics of diverse types, variable models and wide deployment. It is the basic equipment for power system to realize voltage change and electric energy distribution. Because the power system transformer needs to operate with load for a long time, the probability of failure is usually higher than that of other power equipment in general. This paper starts from the transformer fault and fault diagnosis. Firstly, the transformer fault types, traditional transformer fault diagnosis techniques and the advantages of machine learning in transformer fault diagnosis are reviewed. Secondly, the application of information system theory and information entropy in transformer fault diagnosis is introduced. Then it introduces machine learning technology, feature selection and extraction technology in transformer fault diagnosis, and machine learning algorithms such as support vector machine and extreme learning machine for transformer fault diagnosis. Finally, the potential development trend of information system theory, information entropy and machine learning in transformer fault diagnosis is forecasted. The combination of transformer fault prediction and machine learning algorithm is helpful for power system maintenance personnel to accurately predict the running state of power equipment, and also provides a new method and technology for the safe and reliable operation and regular maintenance of power system.",
        "link": "http://dx.doi.org/10.20944/preprints202305.0036.v1"
    },
    {
        "id": 23884,
        "title": "Transformer for contactless electric vehicle charging with bidirectional power flow",
        "authors": "Oluwafemi J. Aworo, Jonathan K.H. Shek",
        "published": "2017-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8274555"
    },
    {
        "id": 23885,
        "title": "Cmot: A Cross-Modality Transformer for Rgb-D Fusion in Person Re-Identification with Online Learning Capabilities",
        "authors": "Hamza Mukhtar, Muhammad  Usman Ghani Khan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4560895"
    },
    {
        "id": 23886,
        "title": "Transformer for Multiple Object Tracking: Exploring Locality to Vision",
        "authors": "Shan Wu, Amnir Hadachi, Chaoru Lu, Damien Vivet",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4143319"
    },
    {
        "id": 23887,
        "title": "Smart Transformer Enabled Triple-Active Bridge Converter for Modern Data Centres",
        "authors": "Anandh N, Dwijasish Das, Chandan Kumar",
        "published": "2022-12-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc57038.2022.10069928"
    },
    {
        "id": 23888,
        "title": "Design of Broadband Power Combiner Based on Transmission Line Transformer",
        "authors": "Yuxuan Su, Sui Qiang",
        "published": "2022-8-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmmt55580.2022.10023169"
    },
    {
        "id": 23889,
        "title": "Two-Stage Spatio- Temporal Vision Transformer for the Detection of Violent Scenes",
        "authors": "Mihai Gabriel Constantin, Bogdan Ionescu",
        "published": "2022-6-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comm54429.2022.9817200"
    },
    {
        "id": 23890,
        "title": "An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization",
        "authors": "Oguzhan Katar, Ozal Yildirim",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the model’s examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT model’s, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
        "link": "http://dx.doi.org/10.3390/diagnostics13142459"
    },
    {
        "id": 23891,
        "title": "Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model",
        "authors": "Yinghan Long, Sayeed Chowdhury, Kaushik Roy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.558"
    },
    {
        "id": 23892,
        "title": "A Practical Scheme for Vibration Signal Measurement-Based Power Transformer on-Load Tap Changer Condition Monitoring",
        "authors": "Junhyuck Seo",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535923"
    },
    {
        "id": 23893,
        "title": "Video Sparse Transformer With Attention-Guided Memory for Video Object Detection",
        "authors": "Masato Fujitake, Akihiro Sugimoto",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3184031"
    },
    {
        "id": 23894,
        "title": "ESTIMATION OF PARAMETERS OF TWO-WINDING TRANSFORMER EQUIVALENT CIRCUIT BASED ON PMU",
        "authors": "T.G. Klimova, V.S. Smirnov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46960/2658-6754_2023_1_102"
    },
    {
        "id": 23895,
        "title": "SST-MIS: Semi-STDP Optimized Medical Image Segmentation Using Modified Swin-Transformer and Pyramid Model",
        "authors": "First Zidong Chen, Second Fadratul Hafinaz Hassan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis article presents a medical image segmentation model based on a semi-STDP (Spike-Timing-Dependent Plasticity) optimization strategy. The study utilizes a modified Swin-Transformer as the feature extraction module and combines it with a pyramid model. The STDP is employed to optimize the semi-segmentation results generated by the pyramid model, achieving precise segmentation of medical images. The architecture and features of the modified Swin-Transformer are described in detail, highlighting its excellent performance in feature extraction. Next, the design concept of the pyramid model is introduced, which can extract and fuse features at different scales to enhance the accuracy and robustness of the segmentation results. Furthermore, an innovative neural network module called the STDP-based segmentation module is proposed. The proposed model is experimentally validated using two medical image datasets and compared with other classical medical image segmentation methods. The experimental results demonstrate significant improvements in accuracy and robustness, validating the effectiveness of the semi-STDP optimization strategy and the superiority of the proposed model. Finally, in-depth analysis and discussion of the experimental results are provided, along with prospects for future improvements. This research has important theoretical and practical implications for improving the accuracy and automation level of medical image segmentation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3167498/v1"
    },
    {
        "id": 23896,
        "title": "Numerical Evaluation of the Directed Oil Cooling System of a Mobile Power Transformer",
        "authors": "Luciene Martins Moura, RUDOLF Huebner Huebner, Paulo Vinicius Trevizoli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPower transformers represent an important part of the capital investment in transmission and distribution substations. The cooling of the windings (electrical coil) depends on the convection of heat, enhanced by the forced circulation of oil through the windings and heat exchangers. The forced circulation of oil combined with the forced circulation of air in the heat exchangers is usually found in mobile transformers, whose compact structure is a challenge in terms of heat transfer rates. An improper design or fabrication problem associated with the assembling of the cooling system may result in an inefficient exchange of heat, which may lead to transformer failure from overheating or a reduction of the life span. In this context, the present work proposes a 2D mathematical model to simulate the winding cooling system of a 138x69-34x13.8 kV 25 MVA Mobile Power Transformer with the objective of investigating the causes of overheating. The model is implemented in CFD Ansys-Fluent® version 17.0 and validated with experimental data. It is evaluated the velocity and temperature distribution, and the identification of the hot spots on the transformer operating considering the nominal conditions for oil flow rate, inlet temperature, and power dissipated. The hot spot temperatures are compared with the current Brazilian Association of Technical Standards - NBR 5356-2: 2007 Power Transformers Part 2: Heating, 2007. After, some geometric and/or operational constraints are artificially imposed to the transformer. Their impact on the velocity and temperature fields, as well as the hot spot temperatures, are mapped in order to verify if they still respect the standard´s temperature limits and if imposed constraints may lead to the transformer failure. The numerical results demonstrated that the geometric imperfections of the disks, guides, or axial cooling ducts dimensions directly affect the oil flow and the temperature distributions, as well modify the position and greatly increase the temperature of the hottest spot in the winding, even extrapolating the ABNT NBR 5356:2 standard in some situations. This way, the proposed mathematical model can be used as a consistent tool for understanding the thermal behavior and, to investigate and map the occurrence of failures in mobile transformers.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3194857/v1"
    },
    {
        "id": 23897,
        "title": "Weakly supervised text classification method based on transformer",
        "authors": "ling gan, aijun yi",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2672391"
    },
    {
        "id": 23898,
        "title": "ChatGPT-A Generative Pre-Trained Transformer",
        "authors": " Manisha Rajesh Gupta",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "ChatGPT is an advanced conversational AI model developed by OpenAI. It is designed to engage in natural and coherent conversations with users, providing human-like responses to a wide range of topics and questions. It leverages deep learning technique. ChatGPT uses a combination of machine learning techniques, including deep learning and natural language processing, to understand and generate human-like text. The model has been trained on a vast amount of internet text data to ensure its ability to generate relevant and contextually accurate responses. ChatGPT has applications in customer service, virtual assistants, and other conversational interfaces, offering a powerful tool for natural language understanding and generation. It is most likely used to generate human like responses and makes the communication interactive.",
        "link": "http://dx.doi.org/10.48175/ijarsct-15087"
    },
    {
        "id": 23899,
        "title": "Frequency-Supporting Control of a Solid-State Transformer",
        "authors": "Frederik Stallmann, Axel Mertens",
        "published": "2022-6-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compel53829.2022.9829966"
    },
    {
        "id": 23900,
        "title": "GeMI: interactive interface for transformer-based Genomic Metadata Integration",
        "authors": "Giuseppe Serna Garcia, Michele Leone, Anna Bernasconi, Mark J Carman",
        "published": "2022-6-3",
        "citations": 5,
        "abstract": "Abstract\nThe Gene Expression Omnibus (GEO) is a public archive containing &gt;4 million digital samples from functional genomics experiments collected over almost two decades. The accompanying metadata describing the experiments suffer from redundancy, inconsistency and incompleteness due to the prevalence of free text and the lack of well-defined data formats and their validation. To remedy this situation, we created Genomic Metadata Integration (GeMI; http://gmql.eu/gemi/), a web application that learns to automatically extract structured metadata (in the form of key-value pairs) from the plain text descriptions of GEO experiments. The extracted information can then be indexed for structured search and used for various downstream data mining activities. GeMI works in continuous interaction with its users. The natural language processing transformer-based model at the core of our system is a fine-tuned version of the Generative Pre-trained Transformer 2 (GPT2) model that is able to learn continuously from the feedback of the users thanks to an active learning framework designed for the purpose. As a part of such a framework, a machine learning interpretation mechanism (that exploits saliency maps) allows the users to understand easily and quickly whether the predictions of the model are correct and improves the overall usability. GeMI’s ability to extract attributes not explicitly mentioned (such as sex, tissue type, cell type, ethnicity and disease) allows researchers to perform specific queries and classification of experiments, which was previously possible only after spending time and resources with tedious manual annotation. The usefulness of GeMI is demonstrated on practical research use cases.\nDatabase URL\nhttp://gmql.eu/gemi/",
        "link": "http://dx.doi.org/10.1093/database/baac036"
    },
    {
        "id": 23901,
        "title": "Drug-target interaction prediction using a multi-modal transformer network demonstrates high generalizability to unseen proteins",
        "authors": "Alexander Kroll, Sahasra Ranjan, Martin J. Lercher",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTMost drugs are small molecules, with their activities typically arising from interactions with protein targets. Accurate predictions of these interactions could greatly accelerate pharmaceutical research. Current machine learning models designed for this task have a limited ability to generalize beyond the proteins used for training. This limitation is likely due to a lack of information exchange between the protein and the small molecule during the generation of the required numerical representations. Here, we introduce ProSmith, a machine learning framework that employs a multimodal Transformer Network to simultaneously process protein amino acid sequences and small molecule strings in the same input. This approach facilitates the exchange of all relevant information between the two types of molecules during the computation of their numerical representations, allowing the model to account for their structural and functional interactions. Our final model combines gradient boosting predictions based on the resulting multimodal Transformer Network with independent predictions based on separate deep learning representations of the proteins and small molecules. The resulting predictions outperform all previous models for predicting drug-target interactions, and the model demonstrates unprecedented generalization capabilities to unseen proteins. We further show that the superior performance of ProSmith is not limited to drug-target interaction predictions, but also leads to improvements in other protein-small molecule interaction prediction tasks, the prediction of Michaelis constantsKMof enzyme-substrate pairs and the identification of potential substrates for enzymes. The Python code provided can be used to easily implement and improve machine learning predictions of interactions between proteins and arbitrary drug candidates or other small molecules.",
        "link": "http://dx.doi.org/10.1101/2023.08.21.554147"
    },
    {
        "id": 23902,
        "title": "MOFTransformer: A Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-Organic Frameworks",
        "authors": "Yeonghun Kang, Hyunsoo Park, Berend Smit, Jihan Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this work, we introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million hypothetical MOFs. The multi-modal model uses an integrated atom-based graph and energy-grid embeddings to capture both the local and global features of the MOFs, respectively. By fine-tuning the pre-trained model with small datasets (from 5,000 to 20,000), our model outperforms all other machine learning models across various properties that include gas adsorption, diffusion, electronic properties, and even text mined data. Beyond its universal transfer learning capabilities, MOFTransformer generates chemical insight by analyzing feature importance from attention scores within the self-attention layers. As such, this model can serve as a bedrock platform for other MOF researchers that seek to develop new machine learning models for their work.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-hcjzc"
    },
    {
        "id": 23903,
        "title": "Development of UHF Sensors for Partial Discharge Detection in Power Transformer",
        "authors": "Hua Chai, B. T. Phung, Daming Zhang",
        "published": "2018-9",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmd.2018.8535779"
    },
    {
        "id": 23904,
        "title": "Distributed Spatial Transformer for Object Tracking in Multi-Camera",
        "authors": "Sio-Kei Im, Ka-Hou Chan",
        "published": "2023-2-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icact56868.2023.10079540"
    },
    {
        "id": 23905,
        "title": "An Improved Mobile Calibration Standard for Transformer Loss Measurement Systems",
        "authors": "H. Cayer, B. Ayhan, T. Kefeli",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpem.2018.8500822"
    },
    {
        "id": 23906,
        "title": "Human motion detection based on Transformer spatiotemporal feature fusion",
        "authors": "Cai Wei Wei, Gong Xun",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cis58238.2022.00036"
    },
    {
        "id": 23907,
        "title": "Land Use Classification Efficient Vision Transformer",
        "authors": "Arthur C. Depoian, Colleen P. Bailey, Parthasarathy Guturu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282974"
    },
    {
        "id": 23908,
        "title": "\"TRANSFORMER LANGUAGE MODEL-BASED MOBILE LEARNING SOLUTION FOR HIGHER EDUCATION\"",
        "authors": "",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33965/es_ml2023_202302l048"
    },
    {
        "id": 23909,
        "title": "TransInpaint: Transformer-based Image Inpainting with Context Adaptation",
        "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00092"
    },
    {
        "id": 23910,
        "title": "Transformer-based Dynamic Fusion Clustering Network",
        "authors": "Chunchun Zhang, Yaliang Zhao, Jinke Wang",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.109984"
    },
    {
        "id": 23911,
        "title": "Syntax-aware Transformer Encoder for Neural Machine Translation",
        "authors": "Sufeng Duan, Hai Zhao, Junru Zhou, Rui Wang",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037672"
    },
    {
        "id": 23912,
        "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
        "authors": "Zhifu Gao, ShiLiang Zhang, Ian McLoughlin, Zhijie Yan",
        "published": "2022-9-18",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-9996"
    },
    {
        "id": 23913,
        "title": "Design of Low-Frequency Tightly Coupled Dipole Array With Transformer Balun",
        "authors": "YiRong Liu, ShunLian Chai, ChengYun Cao",
        "published": "2021-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar53847.2021.10028175"
    },
    {
        "id": 23914,
        "title": "Locational marginal price forecasting using Transformer-based deep learning network",
        "authors": "Shengyi Liao, Zhuo Wang, Yao Luo, Haiyan Liang",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549619"
    },
    {
        "id": 23915,
        "title": "Semi-Autoregressive Transformer for Image Captioning",
        "authors": "Yuanen Zhou, Yong Zhang, Zhenzhen Hu, Meng Wang",
        "published": "2021-10",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00350"
    },
    {
        "id": 23916,
        "title": "A Simplified Solid-State Transformer Model for Teaching Basic Concepts and Ideas",
        "authors": "Coleman W. Smith",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm46819.2021.9637878"
    },
    {
        "id": 23917,
        "title": "Pyramid Transformer for Traffic Sign Detection",
        "authors": "Omid Nejati Manzari, Amin Boudesh, Shahriar B. Shokouhi",
        "published": "2022-11-17",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke57176.2022.9960090"
    },
    {
        "id": 23918,
        "title": "Image Captioning using CNN and Attention Based Transformer",
        "authors": "Deepa Mulimani, Prakashgoud Patil, Nagaraj Chaklabbi",
        "published": "2023",
        "citations": 1,
        "abstract": "Image captioning is a technique for generating sentences that describe a scenario captured in photos. It can identify objects in a picture and carries out a few processes with the goal of locating the image’s most crucial parts. Algorithms now have the ability to generate text in the context of natural phrases that accurately describe an image. To extract image visual features, this work employs a pre-trained Convolution Neural Network (CNN) viz. EfficientNetB0, and then uses Transformer Encoder and Decoder to construct an appropriate caption. The model is trained using the Flickr8k dataset. The findings back up the model’s capacity to understand and produce text from pictures. The evaluation metric is the BLEU (bilingual evaluation understudy) score. The model obtains the image description, converts into text, and then into a voice. For visually impaired people who are unable to grasp visuals, image description is the ideal approach.",
        "link": "http://dx.doi.org/10.56155/978-81-955020-2-8-14"
    },
    {
        "id": 23919,
        "title": "End-to-End Speaker-Attributed ASR with Transformer",
        "authors": "Naoyuki Kanda, Guoli Ye, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, Takuya Yoshioka",
        "published": "2021-8-30",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-101"
    },
    {
        "id": 23920,
        "title": "ASR Error Correction with Augmented Transformer for Entity Retrieval",
        "authors": "Haoyu Wang, Shuyan Dong, Yue Liu, James Logan, Ashish Kumar Agrawal, Yang Liu",
        "published": "2020-10-25",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1753"
    },
    {
        "id": 23921,
        "title": "Enhancing Missing Values Imputation through Transformer-Based Predictive Modeling",
        "authors": "Ayub Hina, Jamil Harun",
        "published": "2024-1-23",
        "citations": 0,
        "abstract": "This paper tackles the vital issue of missing value imputation in data preprocessing, where traditional techniques like zero, mean, and KNN imputation fall short in capturing intricate data relationships. This often results in suboptimal outcomes, and discarding records with missing values leads to significant information loss. Our innovative approach leverages advanced transformer models renowned for handling sequential data. The proposed predictive framework trains a transformer model to predict missing values, yielding a marked improvement in imputation accuracy. Comparative analysis against traditional methods—zero, mean, and KNN imputation—consistently favors our transformer model. Importantly, LSTM validation further underscores the superior performance of our approach. In hourly data, our model achieves a remarkable R2 score of 0.96, surpassing KNN imputation by 0.195. For daily data, the R2 score of 0.806 outperforms KNN imputation by 0.015 and exhibits a notable superiority of 0.25 over mean imputation. Additionally, in monthly data, the proposed model’s R2 score of 0.796 excels, showcasing a significant improvement of 0.1 over mean imputation. These compelling results highlight the proposed model’s ability to capture underlying patterns, offering valuable insights for enhancing missing values imputation in data analyses.",
        "link": "http://dx.doi.org/10.61927/igmin140"
    },
    {
        "id": 23922,
        "title": "Peer Review #1 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.1)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.1/reviews/1"
    },
    {
        "id": 23923,
        "title": "Peer Review #2 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.3)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.3/reviews/2"
    },
    {
        "id": 23924,
        "title": "Transformer protection using multifunctional relay",
        "authors": "L. Anand, K J Anoop, K. Kanchana",
        "published": "2017-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpcsi.2017.8392268"
    },
    {
        "id": 23925,
        "title": "A Generalized Modeling and Analysis for Transformer Parameterization",
        "authors": "Faiza Mobeen, Sadiq Ahmad, Abdullah Shoukat, Samia Batool",
        "published": "2021-12-22",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icet54505.2021.9689869"
    },
    {
        "id": 23926,
        "title": "S2 Transformer for Image Captioning",
        "authors": "Pengpeng Zeng, Haonan Zhang, Jingkuan Song, Lianli Gao",
        "published": "2022-7",
        "citations": 20,
        "abstract": "Transformer-based architectures with grid features represent the state-of-the-art in visual and language reasoning tasks, such as visual question answering and image-text matching. However, directly applying them to image captioning may result in spatial and fine-grained semantic information loss. Their applicability to image captioning is still largely under-explored. Towards this goal, we propose a simple yet effective method, Spatial- and Scale-aware Transformer (S2 Transformer) for image captioning. Specifically, we firstly propose a Spatial-aware Pseudo-supervised (SP) module, which resorts to feature clustering to help preserve spatial information for grid features. Next, to maintain the model size and produce superior results, we build a simple weighted residual connection, named Scale-wise Reinforcement (SR) module, to simultaneously explore both low- and high-level encoded features with rich semantics. Extensive experiments on the MSCOCO benchmark demonstrate that our method achieves new state-of-art performance without bringing excessive parameters compared with the vanilla transformer. The source code is available at https://github.com/zchoi/S2-Transformer",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/224"
    },
    {
        "id": 23927,
        "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
        "authors": "Jindřich Libovický, Jindřich Helcl, David Mareček",
        "published": "2018",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6326"
    },
    {
        "id": 23928,
        "title": "Scale-Aware Modulation Meet Transformer",
        "authors": "Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00553"
    },
    {
        "id": 23929,
        "title": "Adiabatic Analysis of the MSW in a Transient Magnetoplasma",
        "authors": "Dikshitulu K. Kalluri",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218113-9"
    },
    {
        "id": 23930,
        "title": "TX$^2$: Transformer eXplainability and eXploration",
        "authors": "Nathan Martindale, Scott Stewart",
        "published": "2021-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21105/joss.03652"
    },
    {
        "id": 23931,
        "title": "Molecular Transformer – A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
        "authors": "Philippe Schwaller, Teodoro Laino, Theophile Gaudin, Peter Bolgar, Costas Bekas, Alpha A. Lee",
        "published": "No Date",
        "citations": 1,
        "abstract": "Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7297379.v2"
    },
    {
        "id": 23932,
        "title": "Leveraging Ensemble Method with Transformer for Robust Drug Use Detection on Twitter",
        "authors": "Reem Ghannam AlGhannam, Mourad Ykhlef, Hmood Al-Dossari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Social media platforms are increasingly enabling the propagation of content from groups related to drug use, thus posing risks for the wider population and, in particular, individuals who are amenable to drug use and drug addiction. The detection of drug use content on social media platforms is a priority for governments, technology companies, and drug law enforcement organizations. To counter this issue, various techniques have been developed to identify and promptly remove drug use content, while also blocking its creators from network access. In this paper, we introduce a manually annotated Twitter dataset, comprising 156,521 tweets published between 2008 and 2022, specifically compiled for the purpose of drug use detection. The dataset underwent annotation by several group of expert annotators who classified the tweets as either drug use or non-drug use. Exploratory data analysis was conducted to comprehend the dataset&#039;s characteristics. Various classification algorithms, including SVM, XGBoost, RF, NB, LSTM, and BERT were employed using the dataset. Among the traditional machine learning models, SVM utilizing term frequency-inverse document frequency features achieved the highest F1-Score (0.9017). However, BERT with textual features concatenated with numerical and categorical features in ensemble method surpassed the performance of traditional models, attaining F1-Score of 0.9112. To facilitate future research and enhance English online drug use classification accuracy, the dataset will be made publicly available.",
        "link": "http://dx.doi.org/10.20944/preprints202307.1577.v1"
    },
    {
        "id": 23933,
        "title": "Piezoelectric transformer for high-side MOSFET driver supplying",
        "authors": "Pavel Valenta, Vaclav Koucky, Jiri Hammerbauer",
        "published": "2017-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2017.7967265"
    },
    {
        "id": 23934,
        "title": "Online Transformer Monitoring System",
        "authors": " Mohan.S, S. Vinothkumar, K. Saravanakumar, M. Sudarsan",
        "published": "2021-12-1",
        "citations": 0,
        "abstract": "Power transformers, are basically used for stepping up& down the voltage levels. They are the primary equipment used in power transmission system. So it is primary to maintain all the transformers located geographically, but due to lack of man power it is impossible to monitor regularly. Due to these reasons, if a failure occurs in transformer may cause the network power shutdown. Though there are lot of protection measures that accompany a transformer, but by providing a online monitoring system will increase the reliability and reporting instantaneous fault confidently. This paper gives out the details in design and construction of an automatic monitoring system for power transformer parameters. A node-mcu module [esp8266] was enabled to monitoring of voltage, oil level and temperature (oil & winding) on a typical power transformer. With the internet of things (IoT), a self-defense system is designed and implemented for the transformer. In this system Transformer parameter are continuously marked and a graph is plotted. If the level of the parameter increases than the actual value, it gives buzzer alarm, if no action taken then the whole system will be tripped safely in power transformers.",
        "link": "http://dx.doi.org/10.3233/apc210287"
    },
    {
        "id": 23935,
        "title": "Transition of E-Core Transformer to Hybrid Electromagnetic System with Magnetic Flux Modulation",
        "authors": "Ivan Yatchev, Iosko Balabozov",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bulef48056.2019.9030783"
    },
    {
        "id": 23936,
        "title": "Hybrid Measurement-Based Parameter Estimator for Series Transmission Line and Power Transformer",
        "authors": "Seyed Sina Mousavi-Seyedi",
        "published": "2019-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iraniancee.2019.8786618"
    },
    {
        "id": 23937,
        "title": "A methodology for integrated transformer compact modeling",
        "authors": "Yiannis Moisiadis, Konstantinos Nikelis, Padelis Papadopoulos",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mocast.2017.7937611"
    },
    {
        "id": 23938,
        "title": "Statistical Analysis of Data for Dissolved Gases in Transformer",
        "authors": "Navneet Bhargava, Aparna Gupta, Litesh Bopche",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacat.2018.8933700"
    },
    {
        "id": 23939,
        "title": "Adapting Pretrained Transformer to Lattices for Spoken Language Understanding",
        "authors": "Chao-Wei Huang, Yun-Nung Chen",
        "published": "2019-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9003825"
    },
    {
        "id": 23940,
        "title": "IDTransformer: Transformer for Intrinsic Image Decomposition",
        "authors": "Partha Das, Maxime Gevers, Sezer Karaoglu, Theo Gevers",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00089"
    },
    {
        "id": 23941,
        "title": "Diabetic Retinopathy Classification using Vision Transformer",
        "authors": "A. M. Mutawa, Sai Sruthi",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elecs55825.2022.00012"
    },
    {
        "id": 23942,
        "title": "Optimal Power Flow for AC and DC Grids Based on Power Electronic Transformer",
        "authors": " Qi Geng,  Yan Hu",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/cp.2019.0508"
    },
    {
        "id": 23943,
        "title": "SCPA Transformer Based Matching Network Design Flow and SiP Implementation",
        "authors": "Abdelhamed Eldeeb, Graciele Batistell, Wolfgang Bosch, Johannes Sturm",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/austrochip.2018.8520704"
    },
    {
        "id": 23944,
        "title": "Analysis of the Distributed Bifilar Isolation Transformer and Current Balun",
        "authors": "James McLean",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/emcsi38923.2020.9191598"
    },
    {
        "id": 23945,
        "title": "A Novel Image Caption Model Based on Transformer Structure",
        "authors": "Shuang Wang, Yaping Zhu",
        "published": "2021-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicse52190.2021.9404124"
    },
    {
        "id": 23946,
        "title": "Pressure Characteristics of a Variable Hydraulic Transformer",
        "authors": "Guanzhong Yang, Jihai Jiang",
        "published": "2017-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2174/2212797609666161125104506"
    },
    {
        "id": 23947,
        "title": "Research on Remaining Useful Lifetime Prediction Methods of Main Transformer in Nuclear Power Station",
        "authors": "Zikang Li, Minjun Peng",
        "published": "2021-8-4",
        "citations": 0,
        "abstract": "Abstract\nThe main transformer of the NPP is the key equipment connects with public power grid and it is also the important electrical equipment to provide power for the power plant electrical equipment during the shutdown of the generator set. In consequence, it’s failure will cause huge consequences and economic losses. Therefore, the research of reaming life prediction technology for nuclear power plants(NPP) is of great significance to the economic operation and safety of NPP.\nIn this paper, according to DL/T984-2005 and other related literature, it is determined that the aging condition of solid insulating fiber material is the main factor to determine the life prediction of transformer. The main aging mechanism of that is summarized, and the effect of temperature is emphatic introduced. In general, the ambient temperature and load curve are the important factors affecting the aging of transformer insulation, furthermore affecting the life of transformer. Therefore, based on GB/T15164-94 recommended ambient temperature and transformer load, this paper calculates the transformer life loss model.\nOn this basis, this paper introduces the basic particle filter (PF) and Kalman filter (KF) algorithm flow. Based on the physical formula, a data model is developed to estimate the transformer life loss. Besides, the results are compared and analyzed. The study finally found that there were no significant differences in the accuracy of the predictions. However, considering the fact that the aging of transformer is a monotonic decreasing process, the particle filter based on degradation rate has a slight advantage in life prediction.",
        "link": "http://dx.doi.org/10.1115/icone28-64425"
    },
    {
        "id": 23948,
        "title": "Understanding corona activity in nanoparticles dispersed transformer oil under harmonic AC voltages",
        "authors": "Ramanujam Sarathi, Kumari Swati",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iseim.2017.8088694"
    },
    {
        "id": 23949,
        "title": "Transformer Based End-to-End Speech Recognition with Linear Attention",
        "authors": "ShuHang Yu, Ping Li",
        "published": "2022-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccar55106.2022.9782628"
    },
    {
        "id": 23950,
        "title": "Transformer Winding Deformation Diagnosis Based on Signal Distance",
        "authors": "Shengmin Li, Dan Zhao, Yuanyuan He",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8921588"
    },
    {
        "id": 23951,
        "title": "Isolation high frequency transformer for auxiliary converter of locomotives with autonomous power plants",
        "authors": "I.A. PYANZINA, D.V. PYANZIN",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53891/00135860_2022_2_51"
    },
    {
        "id": 23952,
        "title": "Winding Geometry Impact on High Power Medium Frequency Transformer Design",
        "authors": "Nikolina Djekanovic, Drazen Dujic",
        "published": "2022-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indel55690.2022.9965461"
    },
    {
        "id": 23953,
        "title": "Transformer-based Scene Graph Generation Network With Relational Attention Module",
        "authors": "Takuma Yamamoto, Yuya Obinata, Osafumi Nakayama",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956599"
    },
    {
        "id": 23954,
        "title": "Experimental Study on Damage Evolution of Soft-Hard Interactive Rock Based on Improved Transformer Algorithm",
        "authors": "Qi xianyin, Mingzhe Xu, Feng mengyao, Geng diandong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4603424"
    },
    {
        "id": 23955,
        "title": "Multimodal Transformer for Risk Classification: Analyzing the Impact of Different Data Modalities",
        "authors": "Niklas Holtz, Jorge Marx Gomez",
        "published": "2023-5-20",
        "citations": 1,
        "abstract": "Risk classification plays a critical role in domains such as finance, insurance, and healthcare. However, identifying risks can be a challenging task when dealing with different types of data. In this paper, we present a novel approach using the Multimodal Transformer for risk classification, and we investigate the use of data augmentation for risk data through automated retrieval of news articles. We achieved this through keyword extraction based on the title and descriptions of risks and using various selection metrics. We evaluate our approach using a real-world dataset containing numerical, categorical, and textual data. Our results demonstrate that the use of the Multimodal Transformer for risk classification outperforms other models that only utilize textual data. We show that the inclusion of numerical and categorical data improves the performance of the model, particularly for risks that are difficult to classify based on textual data alone. Additionally, our research indicates that the utilization of data augmentation techniques yields enhanced performance outcomes in models. This methodology presents a promising avenue for enterprises to effectively mitigate risks and make well-informed decisions.",
        "link": "http://dx.doi.org/10.5121/csit.2023.130803"
    },
    {
        "id": 23956,
        "title": "Discussion on Measurement of Dielectric Loss Factor and Capacitance of Transformer Winding and Bushing",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v3i8(05).31"
    },
    {
        "id": 23957,
        "title": "Intelligent protection scheme for power transformer",
        "authors": "Yogesh M. Makwana, Bhavesh R. Bhalja, Ashesh M. Shah, Jignesh M. Solanki",
        "published": "2017-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps.2017.8107404"
    },
    {
        "id": 23958,
        "title": "Research on evaluation of transformer online monitoring data and experimental data",
        "authors": "Demeng Bai, Chao Gu, Jian Wang, Guocheng Wang",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5075693"
    },
    {
        "id": 23959,
        "title": "Study of Loss and Temperature Considering Different Shielding Structure in Power Transformer",
        "authors": "Cui Xiao, Chen Dezhi, Bai Baodong",
        "published": "2021-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intermag42984.2021.9579810"
    },
    {
        "id": 23960,
        "title": "MVCformer: A transformer-based multi-view clustering method",
        "authors": "Mingyu Zhao, Weidong Yang, Feiping Nie",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119622"
    },
    {
        "id": 23961,
        "title": "Research on Transformer Inductance Parameter Calculation Model for High Frequency Characteristic Analysis",
        "authors": "X.M. Zi, G. Jian, Z. Ying",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2018.8508491"
    },
    {
        "id": 23962,
        "title": "Chapitre 1. Le « ROI » de la réclamation client",
        "authors": "Daniel Ray, William Sabadie",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0013"
    },
    {
        "id": 23963,
        "title": "Insulation power factor increase of transformer oil due to urban dust contamination",
        "authors": "Greg Parsons, Waldemar Ziomek, Krishnamurthy Vijayan",
        "published": "2021-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic49891.2021.9612329"
    },
    {
        "id": 23964,
        "title": "Tranvit: An Integrated Vision Transformer Framework for Real-Time Transit Travel Time Prediction",
        "authors": "Awad Abdelhalim, Jinhua Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4199936"
    },
    {
        "id": 23965,
        "title": "GViT: Locality enhanced Vision Transformer using Spectral Graph Convolutional Network",
        "authors": "Longbin Jin, Eun Yi Kim",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892822"
    },
    {
        "id": 23966,
        "title": "Transformer internal electromagnetic field distribution and its internal sensor arrangement scheme",
        "authors": "X. Tian, J. Li",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.0472"
    },
    {
        "id": 23967,
        "title": "SigFox based Voltage Monitoring System for Pole Mount Distribution Transformer",
        "authors": "Edwell. T. Mharakurwa, Ayub. M. Aron, Edison. G. Ngunjiri",
        "published": "2021-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica52236.2021.9543444"
    },
    {
        "id": 23968,
        "title": "A digitally-tuned triple-band transformer power combiner for CMOS power amplifiers",
        "authors": "Rahul Singh, Jeyanandh Paramesh",
        "published": "2017-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rfic.2017.7969085"
    },
    {
        "id": 23969,
        "title": "Vision Transformer Model for Efficient Stroke Detection in Neuroimaging",
        "authors": "Oguzhan Katar, Ozal Yildirim, Yesim Eroglu",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391051"
    },
    {
        "id": 23970,
        "title": "Converter-based Intelligent Transformer for Enhanced Grid Monitoring and Control",
        "authors": "Moazzam Nazir, Johan H. Enslin",
        "published": "2020-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid48559.2020.9330675"
    },
    {
        "id": 23971,
        "title": "Dielectric stress distribution in interleaved transformer windings under PWM-type waveforms",
        "authors": "Rodrigo Nuricumbo-Guillen, Fermin Pascual Espino-Cortes",
        "published": "2017-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2017.8004620"
    },
    {
        "id": 23972,
        "title": "Chest X-ray Image Analysis using Convolutional Vision Transformer",
        "authors": "Anzhelika Mezina, Radim Burget",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13164/eeict.2023.161"
    },
    {
        "id": 23973,
        "title": "Accelerated Thermal Aging Effect on The New and Reclamation Transformer Oil Behavior",
        "authors": "F. Guerbas, L. Adjaout, A. Abada",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic55835.2023.10177372"
    },
    {
        "id": 23974,
        "title": "A Novel 6dof Pose Estimation Method Using Transformer Fusion",
        "authors": "huafeng wang, Haodu Zhang, Wanquan Liu, Zhimin Hu, Haoqi Gao, Weifeng Lv, Xianfeng Gu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4512539"
    },
    {
        "id": 23975,
        "title": "Multi-View Aggregation Transformer for No-Reference Point Cloud Quality Assessment",
        "authors": "Baoyang Mu, Feng Shao, Xiongli Chai, Qiang Liu, Hangwei Chen, Qiuping Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4372765"
    },
    {
        "id": 23976,
        "title": "Transformer based Twitter Trending Topics Sentiment Drift Analysis in Real Time",
        "authors": "E Susi, A P Shanthi",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoac59537.2023.10249424"
    },
    {
        "id": 23977,
        "title": "TIRec: Transformer-based Invoice Text Recognition",
        "authors": "Yanlan Chen",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590034"
    },
    {
        "id": 23978,
        "title": "Light-Trans YOLO: A Lightweight Network Based on Transformer for Defect Detection of Industrial Lace Surface",
        "authors": "Yafei Wang, Junfeng Jing, Siyu Sheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLace surface Defect detection has always been a crucial step in the industrial production of lace products. However, due to the complex texture and deforma-bility of lace, as well as the difficulty of distinguishing minor defects from normal images. Therefore, the detection of defects on lace surfaces is a challenging but rarely studied task. In this paper, we propose a new lightweight detection framework , Light-Trans YOLO, to detect lace surface defects. First, our backbone network uses the lightweight network C3 GhostNet. In addition, to obtain more complete global information, we add the lightweight Mobile Transformer Block (MTB) to the backbone network. Then we use the proposed standard deep-wise separable convolution (SDSConv) and SDSBottleneck to design a new neck and add Coordinate Attention (CA) at the end, which overcomes the problem of information loss of deep separable convolution and extracts more effective information. In the training, we propose the loss function ϵSIoU, which can improve the separability of defective and normal samples. We conduct experiments on the industrial lace surface defect dataset collected in lace production sites, and the experiments prove that the mAP of our model is 96.6%, which is 7.7% higher than YOLOV5s, and the FPS and F1-score of the model reaches 50.3 and 0.93, which indicates that our model has a great trade-off between detection accuracy and speed.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3312918/v1"
    },
    {
        "id": 23979,
        "title": "Latest Trends In Use Of Transformer Oils",
        "authors": "Koti Reddy",
        "published": "2019-7-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14445/22315381/ijett-v67i7p207"
    },
    {
        "id": 23980,
        "title": "Exploring Vision Transformer model for detecting Lithography Hotspots",
        "authors": " Sumedha, Rajesh Rohilla",
        "published": "2022-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/centcon56610.2022.10051370"
    },
    {
        "id": 23981,
        "title": "Modeling Recurrence for Transformer",
        "authors": "Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, Zhaopeng Tu",
        "published": "2019",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-1122"
    },
    {
        "id": 23982,
        "title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study",
        "authors": "Peyman Passban, Puneeth Saladi, Qun Liu",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.323"
    },
    {
        "id": 23983,
        "title": "Evolutionary neural architecture search combining multi-branch ConvNet and improved transformer",
        "authors": "Yang Xu, Yongjie Ma",
        "published": "2023-9-22",
        "citations": 3,
        "abstract": "AbstractDeep convolutional neural networks (CNNs) have achieved promising performance in the field of deep learning, but the manual design turns out to be very difficult due to the increasingly complex topologies of CNNs. Recently, neural architecture search (NAS) methods have been proposed to automatically design network architectures, which are superior to handcrafted counterparts. Unfortunately, most current NAS methods suffer from either highly computational complexity of generated architectures or limitations in the flexibility of architecture design. To address above issues, this article proposes an evolutionary neural architecture search (ENAS) method based on improved Transformer and multi-branch ConvNet. The multi-branch block enriches the feature space and enhances the representational capacity of a network by combining paths with different complexities. Since convolution is inherently a local operation, a simple yet powerful “batch-free normalization Transformer Block” (BFNTBlock) is proposed to leverage both local information and long-range feature dependencies. In particular, the design of batch-free normalization (BFN) and batch normalization (BN) mixed in the BFNTBlock blocks the accumulation of estimation shift ascribe to the stack of BN, which has favorable effects for performance improvement. The proposed method achieves remarkable accuracies, 97.24 $$\\%$$\n%\n and 80.06 $$\\%$$\n%\n on CIFAR10 and CIFAR100, respectively, with high computational efficiency, i.e. only 1.46 and 1.53 GPU days. To validate the universality of our method in application scenarios, the proposed algorithm is verified on two real-world applications, including the GTSRB and NEU-CLS dataset, and achieves a better performance than common methods.",
        "link": "http://dx.doi.org/10.1038/s41598-023-42931-3"
    },
    {
        "id": 23984,
        "title": "Low-light image enhancement based on Transformer and CNN architecture",
        "authors": "Keyuan Chen, Bin Chen, Shiqian Wu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326484"
    },
    {
        "id": 23985,
        "title": "An Operating Condition Monitoring and Fault Diagnosis for Transformer Based on Partial Discharge and Artificial Neural Networks",
        "authors": "Fu-Hsien Chen, Horng-Lin Shieh",
        "published": "No Date",
        "citations": 0,
        "abstract": "The operation of transformers suffers from natural or man-made disasters, subjecting the equipment to prolonged exposure to temperature, lightning strikes, transient over-voltages, and over-current, gradually producing insulation aging phenomenon, which results in partial discharges within the transformer. Without dealing with it beforehand properly, it may lead to insulation breakdown. Nowadays, with the mature development of microchip control technology and wireless communication technology, which is helpful for realizing the research for online real-time and long-term monitoring capabilities for power equipment. A transformer operating condition monitoring and diagnostic system that can rapidly and effectively prevent the breakdown of transformer insulation, which can impact the overall power supply system is developed in this paper. The system is divided into two parts based on its function. The first part is the operating condition monitoring and real-time alert unit for the transformer, primarily composed of an embedded computing module, a sensor module, and a transmission module. Its work content is to capture real-time online partial discharge ultrasonic signals and high frequency current comparator pulse signals, and analyze and determine the condition of the internal insulation and operational status whether it is fault or not. The second part is the central control unit, consisting mainly of an industrial computer and a transmission module. Its work content is to store the ultrasonic and high frequency current comparator pulse signals from the transformer operation monitoring and real-time alert unit. It uses the software to eliminate signal noise and interference, and then utilizes artificial neural networks to identify the type of fault. In this paper, the research for the phenomenon of insulation degradation in transformer equipment aims to establish aging trend profiles through the dual malfunctioning alert and aging process analysis, as well as the long-term measurement of ultrasonic signals from partial discharges and high-frequency current comparator pulse signals. This approach provides crucial reference data and a foundation for disposable on-site measurement, assessing the phenomenon of insulation aging stages, defects and damage of the transformers effectively. It allows for timely replacement or maintenance in advance, achieving the prevention of malfunctioning and predictive diagnosis of aging.",
        "link": "http://dx.doi.org/10.20944/preprints202401.1240.v1"
    },
    {
        "id": 23986,
        "title": "Three-phase transformer banks",
        "authors": "Thomas Howard Ortmeyer",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/978-0-7503-6084-5ch4"
    },
    {
        "id": 23987,
        "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
        "authors": "László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Tamás Gábor Csapó",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1607"
    },
    {
        "id": 23988,
        "title": "Privacy-Aware Human Activity Classification using a Transformer-based Model",
        "authors": "Khirakorn Thipprachak, Poj Tangamchit, Sarawut Lerspalungsanti",
        "published": "2022-12-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci51031.2022.10022115"
    },
    {
        "id": 23989,
        "title": "Remote Sensing Vehicle Object Detection Based on Transformer",
        "authors": "Jiawen Xu, Yu Shen",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic58329.2023.10408823"
    },
    {
        "id": 23990,
        "title": "Driver Vigilance Detection from EEG Signals using Transformer Networks",
        "authors": "Dingcheng Gao, Bing Du, Xiaoming Tao, Jianhua Lu",
        "published": "2022-12-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10000618"
    },
    {
        "id": 23991,
        "title": "GQFormer: A Multi-Quantile Generative Transformer for Time Series Forecasting",
        "authors": "Shayan Jawed, Lars Schmidt-Thieme",
        "published": "2022-12-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020927"
    },
    {
        "id": 23992,
        "title": "Novel Transformer with Variable Leakage and Magnetizing Inductances",
        "authors": "Angshuman Sharma, Jonathan W. Kimball",
        "published": "2021-10-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce47101.2021.9595797"
    },
    {
        "id": 23993,
        "title": "Analysis of the methods for determining losses in the transformer magnetic core",
        "authors": "Sergey M. Plotnikov",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32446/0368-1025it.2022-1-52-57"
    },
    {
        "id": 23994,
        "title": "Automatic bat call classification using transformer networks",
        "authors": "Frank Fundel, Daniel A. Braun, Sebastian Gottwald",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ecoinf.2023.102288"
    },
    {
        "id": 23995,
        "title": "Shifted-Window Hierarchical Vision Transformer for Distracted Driver Detection",
        "authors": "Hong Vin Koay, Joon Huang Chuah, Chee-Onn Chow",
        "published": "2021-8-23",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp52854.2021.9550995"
    },
    {
        "id": 23996,
        "title": "Log Anomaly Detection Using Adaptive Universal Transformer",
        "authors": "Sergio Ryan Wibisono, Achmad Imam Kistijantoro",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta.2019.8904299"
    },
    {
        "id": 23997,
        "title": "Decision letter for \"Adaptive integral sliding mode controller for solid state transformer based on generalized averaged model and T‐S fuzzy method\"",
        "authors": "",
        "published": "2021-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.13256/v1/decision1"
    },
    {
        "id": 23998,
        "title": "Peer Review #2 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.4)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.4/reviews/2"
    },
    {
        "id": 23999,
        "title": "Peer Review #3 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.4)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.4/reviews/3"
    },
    {
        "id": 24000,
        "title": "Peer Review #1 of \"Molecular characterization and expression profiling of transformer 2 and fruitless-like homologs in the black tiger shrimp, Penaeus monodon (v0.4)\"",
        "authors": "",
        "published": "2022-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12980v0.4/reviews/1"
    },
    {
        "id": 24001,
        "title": "Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus",
        "authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nDue to its importance in studying people’s thoughts on various Web 2.0 services, emotion classification is a critical undertaking. Most existing research is focused on the English language , with little work on low-resource languages, e.g., Bangla. In recent years, sentiment analysis, particularly emotion classification in English, has received increasing attention, but little study has been done in the context of Bangla (one of the world’s most widely spoken languages). In this research, we propose a complete set of approaches for identifying and extracting emotions from Bangla texts. We provide a Bangla emotion classification for six classes, i.e., anger, disgust, fear, joy, sadness, and surprise, from Bangla words using transformer-based models, which exhibit phenomenal results in recent days, especially for high-resource languages. The Unified Bangla Multi-class Emotion Corpus (UBMEC) is used to assess the performance of our models. UBMEC is created by combining two previously released manually labelled datasets of Bangla comments on six emotion classes with fresh manually labelled Bangla comments created by us. The corpus dataset and code we used in this work are publicly available.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3051650/v1"
    },
    {
        "id": 24002,
        "title": "Multi-resolution transformer-based point cloud completion network for intelligent manufacture",
        "authors": "Yongcong Zhang, Fang Fang, Yahui Gan, Bo Zhou",
        "published": "2022-11-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055332"
    },
    {
        "id": 24003,
        "title": "Transformer-based highlights extraction from scientific papers",
        "authors": "Moreno La Quatra, Luca Cagliero",
        "published": "2022-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.109382"
    },
    {
        "id": 24004,
        "title": "TABS: Transformer Based Seizure Detection",
        "authors": "J. Pedoeem, S. Abittan, G. Bar Yosef, S. Keene",
        "published": "2020-12-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spmb50085.2020.9353612"
    },
    {
        "id": 24005,
        "title": "Coupling Spatial and Channel Transformer for Single Image Deraining",
        "authors": "Yuto Namba, Jiande Sun, Xian-Hua Han",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222823"
    },
    {
        "id": 24006,
        "title": "Patient-Specific Seizure prediction from Scalp EEG Using Vision Transformer",
        "authors": "Xiaoling Zhang, Huiyan Li",
        "published": "2022-3-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec53115.2022.9734546"
    },
    {
        "id": 24007,
        "title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer",
        "authors": "Mingjian Chen, Xu Tan, Yi Ren, Jin Xu, Hao Sun, Sheng Zhao, Tao Qin",
        "published": "2020-10-25",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-3139"
    },
    {
        "id": 24008,
        "title": "Why Models of Innovation Are Models, or What Work Is Being Done in Calling Them Models",
        "authors": "",
        "published": "2017-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/10782.003.0015"
    },
    {
        "id": 24009,
        "title": "From CNN to Transformer: A Review of Medical Image Segmentation Models",
        "authors": "Wenjian Yao, Jiajun Bai, Wei Liao, Yuheng Chen, Mengjuan Liu, Yao Xie",
        "published": "2024-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10278-024-00981-7"
    },
    {
        "id": 24010,
        "title": "A Brief Survey of Vortex Models",
        "authors": "",
        "published": "2020-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-1328.vid"
    },
    {
        "id": 24011,
        "title": "Evaluation and Verification of Series Resonant Converter with Transformer Operating Regimes",
        "authors": "Nikolay Hinov, Tsveti Hranov",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hitech.2018.8566490"
    },
    {
        "id": 24012,
        "title": "Determination of Mathematical Model Parameters of a Medium Frequency Transformer",
        "authors": "Michal Michna, Andrzej Wilk, Piotr Dworakowski, Bruno Lefebvre",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem.2018.8442840"
    },
    {
        "id": 24013,
        "title": "Zero-Shot Sketch Based Image Retrieval Using Graph Transformer",
        "authors": "Sumrit Gupta, Ushasi Chaudhuri, Biplab Banerjee, Saurabh Kumar",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956095"
    },
    {
        "id": 24014,
        "title": "Development and Application of Dry-type Tridimensional Wound-core Transformer",
        "authors": "Chengping Zhang, Zubing Zou, Fei Gai",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cieec47146.2019.cieec-2019726"
    },
    {
        "id": 24015,
        "title": "Modelling Transformer Core with Appropriate Boundary Conditions for Partial Discharge Studies",
        "authors": "Santosh Janaki Raman, Pritam Mukherjee, Sanjib Kumar Panda",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic43217.2019.9046571"
    },
    {
        "id": 24016,
        "title": "Analysis and Simulation of a Typical Two Winding Transformer",
        "authors": "Shreya Adhikary, Bhaskar Roy, Bikas Mondal",
        "published": "2022-2-11",
        "citations": 0,
        "abstract": "In this study, we show how input power, terminal voltage, and efficiency changes as a function of load current. In this situation, we've chosen three scenarios they are-\n\nCoil connection in Parallel\nCoil connection in Series\nTwo Winding Transformer\nHere, we have connected the coils of a single-phase transformer in series and parallel, then observed the transformer's loading as an autotransformer and as a two-winding transformer. It is essential to comprehend the instant polarities of the secondary terminals in relation to the primary in order to properly connect the winding.",
        "link": "http://dx.doi.org/10.46610/joped.2022.v08i01.001"
    },
    {
        "id": 24017,
        "title": "Measurement on Transformer Windings by Impact Test",
        "authors": "Daniel Korenciak, Miroslav Gutten, Matej Kucera, Milan Sebok, Tomasz Koltunowicz, Pawel Zukowski",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/measurement47340.2019.8780084"
    },
    {
        "id": 24018,
        "title": "Internet of Things-Based Arduino Controlled On-Load Tap Changer Distribution Transformer",
        "authors": "Krishan Arora",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003102267-12"
    },
    {
        "id": 24019,
        "title": "Cross-Component Transferable Transformer Pipeline Obeying Dynamic Seesaw for Rotating Machinery with Imbalanced Data",
        "authors": "Binbin Xu, Boquan Ma, Zheng Yang, Fei Chen, Xiaobing Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4425233"
    },
    {
        "id": 24020,
        "title": "ECONOMICAL IMPLEMENTATION OF ATTENTION IN THE TRANSFORMER ENCODER FOR SPEECH RECOGNITION",
        "authors": "V. Y. Chuchupal",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58633/2305-8129_2022_1_119"
    },
    {
        "id": 24021,
        "title": "A transformer method that predicts human lives from sequences of life events",
        "authors": "",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s43588-023-00586-0"
    },
    {
        "id": 24022,
        "title": "Optimization of high frequency transformer based on advanced genetic algorithm",
        "authors": "G. Xiaowei, Y. Zhiting, J. Danchen",
        "published": "2017-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppc.2017.8291261"
    },
    {
        "id": 24023,
        "title": "DUSFormer: Dual-Swin Transformer V2 Aggregate Network for Polyp Segmentation",
        "authors": "Zhangrun Xia, Jingliang Chen, Chengzhun Lu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3352428"
    },
    {
        "id": 24024,
        "title": "Unified power quality conditioner with shared legs and high-frequency transformer",
        "authors": "Alan S. Felinto, Cursino B. Jacobina",
        "published": "2020-10-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce44975.2020.9235624"
    },
    {
        "id": 24025,
        "title": "Substation-oriented PMU placement considering transformer tap settings",
        "authors": "Nikolaos M. Manousakis, George N. Korres",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichve.2018.8641992"
    },
    {
        "id": 24026,
        "title": "Three-Phase Current Reconstruction Methodology for Permanent Magnet Synchronous Motor Driver based on Current Transformer Sensors",
        "authors": "Jianan Cao, Gefei Meng, Jia Peng, Yunjia Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "The position sensorless Permanent Magnet Synchronous Motor (PMSM) is\nwidely applied in the field of high reliability, and its driver, the\ninverter is controlled by closed loop feedback with the three-phase\ncurrent. For medium and high power PMSM, both safety isolation and\ndetection accuracy of measuring current are extremely significant. This\npaper presents a method of using Current Transformer (CT) with center\ntaps to measure the high-frequency switch current and of an algorithm to\nreconstruct the low-frequency three-phase current. The measurement unit\nand reconfiguration algorithm are then applied to the motor starting\nstage controlled by the DSP controller TMS320F28335. Lastly, by\ncomparing the waveform of the reconstructed current and one of the\ninverter phase current, the experimental results show that the proposed\nreconstruction method is effective.",
        "link": "http://dx.doi.org/10.22541/au.165940742.28535663/v1"
    },
    {
        "id": 24027,
        "title": "Transformer-Based BiLSTM for Aspect-Level Sentiment Classification",
        "authors": "Tao Cai, Baocheng Yu, Wenxia Xu",
        "published": "2021-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rcae53607.2021.9638807"
    },
    {
        "id": 24028,
        "title": "Impact of the Winding Arrangement on Efficiency of the Resistance Spot Welding Transformer",
        "authors": "Gašper Habjan, Martin Petrun",
        "published": "2019-9-30",
        "citations": 1,
        "abstract": "In this paper, the impact of the winding arrangement on the efficiency of the resistance spot welding (RSW) transformer is presented. First, the design and operation of the transformer inside a high power RSW system are analyzed. Based on the presented analysis, the generation of imbalanced excitation of the magnetic core is presented, which leads to unfavorable leakage magnetic fluxes inside the transformer. Such fluxes are linked to the dynamic power loss components that significantly decrease the efficiency of the transformer. Based on the presented analysis, design guidelines to reduce the unwanted leakage fluxes are pointed out. The presented theoretical analysis is confirmed by measurements using a laboratory experimental system. The presented experimental results confirm that the proposed improved winding arrangement increased the efficiency of the transformer in average for 6.27%.",
        "link": "http://dx.doi.org/10.3390/en12193735"
    },
    {
        "id": 24029,
        "title": "Power transformer winding model for lightning impulse testing",
        "authors": "Tomislav Župan, Bojan Trkulja, Željko Štih",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.717"
    },
    {
        "id": 24030,
        "title": "Transformer Decoder Based Reinforcement Learning Approach for Conversational Response Generation",
        "authors": "Farshid Faal, Jia Yuan Yu, Ketra Schmitt",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207289"
    },
    {
        "id": 24031,
        "title": "CFD Based Sensitivity Study of Cooling Performance of Transformer Radiators",
        "authors": "Bernardo Galletti, Andreas Blaszczyk, Wei Wu",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/arwtr.2019.8930189"
    },
    {
        "id": 24032,
        "title": "Protection Transformer and Transmission Line in Power System Based on MATLAB Simulink",
        "authors": "Mohammed IBRAHIM",
        "published": "2021-9-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2021.10.04"
    },
    {
        "id": 24033,
        "title": "Transformer Neural Network-Based Molecular Optimization Using General Transformations",
        "authors": "Jiazhen He, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Atanas Patronov, Esben Jannik Bjerrum, Ola Engkvist",
        "published": "No Date",
        "citations": 0,
        "abstract": "Molecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug discovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the large chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimicking the chemist's intuition in terms of matched molecular pairs (MMPs). Although MMPs is a typical and widely used strategy by medicinal chemists, it offers limited capability in terms of exploring the space of solutions. There are more options to modify a starting molecule to achieve desirable properties, e.g. one can simultaneously modify the molecule at different places including changing the scaffold. This study trains the same Transformer architecture on different datasets. These datasets consist of a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets reflecting general transformations are constructed from ChEMBL based on two approaches: Tanimoto similarity (allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold constant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while keeping the same model architecture. Our results show that the models trained on differently prepared datasets transform a given starting molecule in a way that it reflects the nature of the dataset used for training the model. These models could complement each other and unlock the capability for the chemists to pursue different options for improving a starting molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-z8rk6"
    },
    {
        "id": 24034,
        "title": "Modeling based on frequency to oversee the condition of transformer",
        "authors": "Swathy Sasikumar, V. A. Kulkarni",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipact.2017.8245192"
    },
    {
        "id": 24035,
        "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
        "authors": "jiawei wang, Weihong Lin, Chixiang Ma, Mingze Li, Zheng Sun, Lei Sun, Qiang Huo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4399248"
    },
    {
        "id": 24036,
        "title": "Sen Transformer",
        "authors": "",
        "published": "2021-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119824398.ch6"
    },
    {
        "id": 24037,
        "title": "Exploration of fault current limiting transformer using variable reactance",
        "authors": "M. KendreSomnath, C. R. Lakade",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iconstem.2017.8261389"
    },
    {
        "id": 24038,
        "title": "Power Transformer Protection using ANN and Wavelet Transforms",
        "authors": "R.Naveena Bhargavi, M.Lakshmi Swarupa, M. Rajitha",
        "published": "2021-3-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaccs51430.2021.9441828"
    },
    {
        "id": 24039,
        "title": "A Transformer-Based Prior Legal Case Retrieval Method",
        "authors": "Ceyhun E. Öztürk, Ş. Bariş ÖzçelıK, Aykut Koç",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223938"
    },
    {
        "id": 24040,
        "title": "API Misuse Detection Method Based on Transformer",
        "authors": "Jingbo Yang, Jian Ren, Wenjun Wu",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qrs57517.2022.00100"
    },
    {
        "id": 24041,
        "title": "Cross-Site Scripting Attack Detection Method Based on Transformer",
        "authors": "Bitao Peng, Xiyi Xiao, Juan Wang",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc56324.2022.10065892"
    },
    {
        "id": 24042,
        "title": "TEDformer: Temporal Feature Enhanced Decomposed Transformer for Long-term Series Forecasting",
        "authors": "Jiayi Fan, Bingyao Wang, Dong Bian",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3287893"
    },
    {
        "id": 24043,
        "title": "Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer",
        "authors": "Dan Zhang, Fangfang Zhou",
        "published": "2023",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3243829"
    },
    {
        "id": 24044,
        "title": "ENGLISH TRANSFORMER-BUILDING TERMS AND THE WAYS OF THEIR FORMATION",
        "authors": "I. M. Fesenko, O. M. Syvachuk",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26661/2414-1135-2020-80-2-44"
    },
    {
        "id": 24045,
        "title": "Comparing the Effectiveness of Classic Mask Rcnn and Vision Transformer in Early Weed Detection",
        "authors": "Shahnawaz Qureshi, Asif Ameer, Ali Zia, Ahsan Latif, Seppo Karrila",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4436132"
    },
    {
        "id": 24046,
        "title": "Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle",
        "authors": "Akash Sonth, Abhijit Sarkar, Hirva Bhagat, Lynn Abbott",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186584"
    },
    {
        "id": 24047,
        "title": "Word Syllabification for Indonesian Language using Transformer",
        "authors": "Muhammad Haykal Kamil, Suyanto Suyanto, Mochammad Arif Bijaksana",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isitia59021.2023.10221089"
    },
    {
        "id": 24048,
        "title": "Hmt: Hybrid Mechanism Transformer for Bio-Fabrication Prediction Under Complex Environmental Conditions",
        "authors": "Yichen Song, Hu Xu, Changdi Li, Qunshan He, Zijian Tian, Xinggao Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4594980"
    },
    {
        "id": 24049,
        "title": "TRANSFORMER PRODUCTION IMPROVEMENT  BY LEAN AND MTM-2 TECHNIQUE",
        "authors": "Somkeit Noamna, Theerapong Thongphun, Chalermpon Kongjit",
        "published": "2022-6-1",
        "citations": 1,
        "abstract": "The situation of the covid-19 epidemic is a driving force of the global market’s demand increase of electronic devices and parts. Entire electronic component manufacturers, especially the transformer manufacturing industry, which is a device that supplies power to many electronic devices, encounters problems in producing products that are unable to keep up with the quickly increasing demand. This research aims to increase the productivity of small transformers by lean approach. The paper depicts processes relevant to improving production processes, reducing waste, and finding unnecessary processes. The method begins with two actions. First, study the current situation in transformer manufacturing of a case study. Second, study the customer order to delivery process using the Value Stream Mapping (VSM) and analyze entire processes of transformer manufacturing to identify standard time by unit work. The main technique is for measuring working time by timing the forward motion with the time measurement method version 2 (MTM-2). The Cause and Effect diagram was displayed with improving guidelines on two operations. First the concept of lean manufacturing was used in principal role, second the ECRS technique (Eliminate, Combine, Rearrange and Simplify) was applied to reduce \"waste\" as well as to optimize and reduce the manufacturing process of the transformer. The results lead to an increase in the final product per hour from 45 pieces per hour to 75 pieces per hour which increases up to 30% per hour. In addition, the productivity improvements increased the productivity of 3.46 workers per hour to 6.82 per hour (increase of 97.11%) and production time was reduced from 1,109 seconds to 229 seconds (73.04% of productivity).",
        "link": "http://dx.doi.org/10.11113/aej.v12.16712"
    },
    {
        "id": 24050,
        "title": "Mitotic Cell Detection in Histopathological Images of Neuroendocrine Tumors Using YOLOv5-Transformer",
        "authors": "Zehra Karhan, Fuat Akal, pembe oltulu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327641"
    },
    {
        "id": 24051,
        "title": "Estimation of Parameters and Error Prediction for Instrument Transformer Using Machine Learning",
        "authors": "",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets43956"
    },
    {
        "id": 24052,
        "title": "Automatic Synthesis for On-chip Transformer",
        "authors": "Yinfei Weng, Zachary Su",
        "published": "2020-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nemo49486.2020.9343434"
    },
    {
        "id": 24053,
        "title": "Residential Load Clustering Contribution to Accurate Distribution Transformer Sizing",
        "authors": "Mehran Hajiaghapour-Moghimi, Kamyar Azimi-Hosseini, Ehsan Hajipour, Mehdi Vakilian",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081518"
    },
    {
        "id": 24054,
        "title": "Phase-Shifting Method with Dy11 Transformer to Reduce Harmonics",
        "authors": "Rudy Setiabudy, Immanuel Surya, Herlina Herlina",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecos47637.2019.8984589"
    },
    {
        "id": 24055,
        "title": "Enhancing ECG Signal Data through Denoising Features with Transformer Generative Adversarial Networks for Model Classification 1D-CNN",
        "authors": "Hendrico Yehezky, Alhadi Bustamam, Hermawan Hermawan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn important component of telemedicine's remote cardiac health monitoring of patients is the use of artificial intelligence (AI) technology to detect electrocardiograph (ECG) signals. Failure to properly diagnose and treat abnormal ECG patterns caused by arrhythmia symptoms can result in a fatal outcome. Given that arrhythmia symptoms contribute significantly to noncommunicable cardiovascular disease (CVD), which is responsible for approximately 32% of global mortality, this concern becomes even more significant. The high sensitivity of ECG signals to both external and internal electrical disturbances makes accurate interpretation of these signals for arrhythmia detection challenging. An effective denoising technique is presented in this method as a substitute approach to reduce noise disturbances in ECG signal data and enhance the quality of the training data for AI detection models. This pre-processing technique combines a synthesis approach with Gaussian filtering, an auto-encoder-decoder (transformer), and generative adversarial networks (GANs). The MIT-BIH dataset is the subject of research for this study, which has been categorized into Normal, Atrial Premature, Premature Ventricular Contraction, Fusion of Ventricular and Normal, and Fusion of Paced and Normal. The research findings show that the quality of the synthesized data is almost identical to that of the original data. It is advised to use a deep neural network (DNN) model instead of the previous prediction model for this enhanced dataset, specifically a one-dimensional convolutional neural network (1D-CNN), which is well suited for training this reconstruction data through this experiment.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3432401/v1"
    },
    {
        "id": 24056,
        "title": "Reconfigurable photonic fractional Fourier transformer",
        "authors": "Shaowen Peng, Shangyuan Li, Guanyu Han, Xiaoxiao Xue, Xiaoping Zheng",
        "published": "2021-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2605312"
    },
    {
        "id": 24057,
        "title": "Transformer winding temperature rise measurement system",
        "authors": "Weili Wu, Hualong Zhang, Wentong Wu",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2640478"
    },
    {
        "id": 24058,
        "title": "Encoder/Decoder Transformer-Based Framework to Detect Hate Speech from Tweets",
        "authors": " Usman, S. M. K. Quadri",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003371380-19"
    },
    {
        "id": 24059,
        "title": "Fire and Explosion Risks and Consequences in Electrical Substations—A Transformer Case Study",
        "authors": "Mohanad El-Harbawi",
        "published": "2022-1-1",
        "citations": 4,
        "abstract": "Abstract\nThis study aims to find how fires and explosions can occur in enclosed spaces where electrical transformers are installed and to investigate the consequences of the damages to the surrounding areas caused by these accidents. This study began with the collection of a mineral oil waste sample from an indoor substation transformer in Riyadh, Saudi Arabia. This sample was analyzed to determine its composition. Results revealed that 30 components ranging from C6 to C30 were detected in the sample. The mixture flammability limits, calculated using Le Chatelier rules and found to be 0.97 and 6.56, indicated that the vapor mixture for the waste oil sample was not flammable at 25 °C and 1 atm. Consequence analysis was used to predict the outcome of fire and explosion events based on a transformer with a capacity of 1100 liters. The peak overpressure generated by an explosion was estimated to be 80.97 kPa. Moreover, the thermal radiation produced by various types of fires was estimated as a function of the distance from the accident center. The thermal flux from a boiling liquid expanding vapor explosion (BLEVE) was 99.8 kW/m2, which is greater than that from jet and pool fires. The probability of an individual suffering injury or dying as a result of exposure to fire and/or an explosion was estimated using dose-response models. The results showed that the peak overpressure produced by an explosion can cause severe damage within 20 m of the explosion center. However, the results also showed that there is a 100% probability of the thermal radiation from a BLEVE causing fatalities up to a distance of 140 m. The risk due to the fragmentation of the transformer tanks was also assessed, and a majority of fragments would land within a range of 111.2 m.",
        "link": "http://dx.doi.org/10.1115/1.4054143"
    },
    {
        "id": 24060,
        "title": "Accurate prediction of gestational diabetes mellitus via a novel transformer method",
        "authors": "Hui Wang, Ye Yao, Jieying Zheng, Danhong Peng, Jiansheng Wu, Jun Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDiabetes is a common complication that happened in pregnant women, and it often leads to many serious consequences for fetuses and gravidas. Accurate diagnosis of gestational diabetes mellitus (GDM) is the key to providing prompt and precise treatment and disease management. The artificial intelligence-based method is currently the most commonly used auxiliary way for clinical medical diagnosis. However, as all we know, there is no report on the assistance of GDM diagnosis based on artificial intelligence till now. In this work, we collected the clinical samples of 1000 pregnant women from ZhongDa Hospital of Southeast University in Nanjing city, which involves 221 cases of GDM. Then, a matrix factorization method was used to fill up all missing values in the original data.  Next, a random forest model was adopted to evaluate the importance of each feature dimension to aid in finding potential clinical markers for the GDM diagnosis. Finally, a novel transformer-based method called TF-GDM was proposed for predicting gestational diabetes mellitus accurately. The results show that our TF-GDM method achieves excellent performance, with the accuracy, precision, and recall of 0.93, 0.88, and 0.92, respectively, and also with the F1 score and AUC value of 0.90 and 0.94, respectively. The results demonstrate that our TF-GDM method is significantly better than classic machine learning-based and deep learning-based methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2461259/v1"
    },
    {
        "id": 24061,
        "title": "Cybersecurity Enhancement of Transformer Differential Protection",
        "authors": "Sampath R",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "Abstract: The increasing use of information and communication technologies (ICT) in the operational environments of power grids has been essential for operators to improve the monitoring, maintenance, and control of power generation, transmission, and distribution; however, this has come at the expense of increasing the grid's exposure to cyber threats. This paper looks at cyberattack scenarios that target protective relays in substations, which can be the most important part of protecting power systems from abnormal conditions. The overall performance of the power grid could suffer significantly if the relays' operations are disrupted, possibly resulting in widespread blackouts. Utilizing the potential of machine learning to detect anomalous behavior in transformer differential protective relays, we investigate methods for improving substation cybersecurity. In order to find cyberattacks, the proposed method looks at operational technology (OT) data from the substation current transformers (CTs). Power frameworks recreation utilizing OPAL-RTHYPERSIM is utilized to create preparing informational collections, to simulate the cyberattacks and to evaluate the network safety enhancement capability of the proposed AI calculations. Terms in the index include differential protective relays, transformers, operational technology, cyber physical systems, and machine learning.",
        "link": "http://dx.doi.org/10.22214/ijraset.2023.51963"
    },
    {
        "id": 24062,
        "title": "Residual life estimation of power transformer based on Karl Fischer and Adaptive neuro-fuzzy interference system",
        "authors": "Permit Mathuhu Sekatane, Thomas Otieno Olwal",
        "published": "2021-9-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/africon51333.2021.9570981"
    },
    {
        "id": 24063,
        "title": "Medical Reports Summarization Using Text-To-Text Transformer",
        "authors": "Abdulkader Helwan, Danielle Azar, Dilber Uzun Ozsahin",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aset56582.2023.10180671"
    },
    {
        "id": 24064,
        "title": "Research on excitation current and vibration characteristics of DC biased transformer",
        "authors": "",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2019.00410"
    },
    {
        "id": 24065,
        "title": "Task Context Transformer and Gcn for Few-Shot Learning of Cross-Domain",
        "authors": "Pengfang Li, Fang Liu, Licheng Jiao, Lingling Li, Puhua Chen, Shuo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342068"
    },
    {
        "id": 24066,
        "title": "Aircraft inspection based on improved Swin Transformer",
        "authors": "Li feng, Jia jun",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3025486"
    },
    {
        "id": 24067,
        "title": "Performance analysis of SOTA Transformer Network in Numerical Expression Calculation",
        "authors": "Isha Ganguli, Rajat Subhra Bhowmick, Jaya Sil",
        "published": "2020-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon49873.2020.9342053"
    },
    {
        "id": 24068,
        "title": "Transformer Bushing Insulation Defect Detection Method Based on 3D Surface Map",
        "authors": "Longxiang Yuan, Yang Ding",
        "published": "2021-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwcsg53609.2021.00042"
    },
    {
        "id": 24069,
        "title": "Automatic Power Transformer Tester",
        "authors": "Khaja Moideen S. -, Akash S. -, Gokulraj N. -, Loganathan S. -, D. Sivaraj -",
        "published": "2023-4-2",
        "citations": 0,
        "abstract": "An essential step in the production of transformers is the quality testing of the devices. Transformers with normal operation have a lower failure rate and longer lifespan. When done manually, checking the transformer in its whole is a time-consuming and laborious task. Using the software platform LabVIEW (Laboratory Virtual Instrumentation Engineering Workbench), the testing method is automated and can save the time .The test platform is made to conduct tests with high voltage and zero load voltage test. A comprehensive design, simulation, and visualisation environment is provided by the LABVIEW software platform. The programme is built on cutting-edge. computing techniques making it possible to calculate the ideal transformer active and mechanical part configuration.",
        "link": "http://dx.doi.org/10.36948/ijfmr.2023.v05i02.2085"
    },
    {
        "id": 24070,
        "title": "Modeling and Detection of Inter-turn Faults in Distribution Transformer",
        "authors": "U. Rajendra Prasad, C. Vyjayanthi, Jaison K.",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icps48983.2019.9067533"
    },
    {
        "id": 24071,
        "title": "Large Power Transformer Magnetic Core Vibration Model by Using Dynamic Genetic Algorithm",
        "authors": "Janis Marks",
        "published": "2021-4-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pemc48073.2021.9432624"
    },
    {
        "id": 24072,
        "title": "Continuous Sign Language Recognition and Translation Using Hybrid Transformer-Based Neural Network",
        "authors": "Rajalakshmi E, Elakkiya R, Subramaniyaswamy V, Ketan Kotecha, Mayuri Mehta, Vasile Palade",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4424708"
    },
    {
        "id": 24073,
        "title": "A Compound Data Poisoning Technique with Significant Adversarial Effects on Transformer-based Text Classification Tasks",
        "authors": "Edmon Begoli, Maria Mahbub, Sudarshan Sriniva, Linsey Passarella",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer-based models have demonstrated much success in various natural language processing (NLP) tasks. However, they are often vulnerable to adversarial attacks, such as data poisoning, that can intentionally fool the model into generating incorrect results. In this paper, we present a novel, compound variant of a data poisoning attack on a transformer-based model that maximizes the poisoning effect while minimizing the scope of poisoning. We do so by combining the established data poisoning technique (label flipping) with a novel adversarial artifact selection and insertion technique aimed at minimizing detectability and the scope of the poisoning footprint. We find that using a combination of these two techniques, we achieve a state-of-the-art attack success rate (ASR) of ~90% while poisoning only 0.5% of the original training set, thus minimizing the scope and detectability of the poisoning action.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2443929/v1"
    },
    {
        "id": 24074,
        "title": "Joint Multi-Dimensional Dynamic Attention and Transformer for Efficient Image Restoration",
        "authors": "Huan Zhang, Xu Zhang, Nian Cai, Jiang-lei Di, Weisi LIN, Yun Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4437069"
    },
    {
        "id": 24075,
        "title": "Hardening Soft Information: A Transformer-Based Approach to Forecasting Stock Return Volatility",
        "authors": "Matthew Caron, Oliver Muller",
        "published": "2020-12-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378134"
    },
    {
        "id": 24076,
        "title": "Alternative Liquid Dielectrics for High Voltage Transformer Insulation Systems",
        "authors": "",
        "published": "2021-12-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119800194"
    },
    {
        "id": 24077,
        "title": "Regional Transformer for Image Super-Resolution",
        "authors": "Sen Yang, Jiahong Yang, Dahong Xu, Xi Li",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmvit57620.2023.00011"
    },
    {
        "id": 24078,
        "title": "Multi-Modal Transformer with Multi-Head Attention for Emotion Recognition",
        "authors": "Chi Xu, Yifei Gao",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsece58870.2023.10263303"
    },
    {
        "id": 24079,
        "title": "Enhancing Vegetable Sales Forecasting with A CNN-LSTM-Transformer Hybrid Model",
        "authors": "Aoxiang Tian",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": " Due to the short shelf life of vegetable products, a significant portion of the inventory cannot be resold the following day. To facilitate more informed procurement decisions in superstores and minimize vegetable wastage, this study proposes a hybrid prediction model based on CNN-LSTM-Transformer for enhancing the accuracy of forecasting vegetable sales volumes. Firstly, an LSTM model is incorporated to account for the recurring and seasonal variations in vegetable sales. Secondly, a CNN model is introduced to address the limitations of LSTM in capturing spatial data components. The convolutional and pooling layers of CNN help establish spatial relationships among different feature values in the dataset. Finally, a Transformer model is integrated to tackle the issue of long-term dependencies, which LSTM alone struggles to resolve. The Transformer model employs a parallel attention mechanism, eliminating temporal dependencies and effectively addressing LSTM's long-term dependency challenge. This integration also accelerates model training. The evaluation metric employed in this paper is RMSE (Root Mean Square Error). The three-year sales volume of 11 vegetable types is predicted using seasonal ARIMA, XGBoost, LSTM, CNN-LSTM, and CNN-LSTM-Transformer models. The results indicate that the CNN-LSTM-Transformer model achieves the lowest RMSE at 0.0758, followed by ARIMA at 0.0792, CNN-LSTM at 0.0830, XGBoost at 0.0858, and LSTM at 0.0913. These findings demonstrate that the CNN-LSTM-Transformer model exhibits superior accuracy in forecasting vegetable sales volumes, yielding more precise predictions. This accurate forecasting not only aids superstores in reducing waste and enhancing profitability but also assists government authorities in rationalizing vegetable subsidy policies and optimizing the vegetable production and marketing system.",
        "link": "http://dx.doi.org/10.54097/rr7t4d15"
    },
    {
        "id": 24080,
        "title": "TFE: A Transformer Architecture for Occlusion Aware Facial Expression Recognition",
        "authors": "Jixun Gao, Yuanyuan Zhao",
        "published": "2021-10-25",
        "citations": 6,
        "abstract": "Facial expression recognition (FER) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based FER approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based FER method (TFE) that is capable of adaptatively focusing on the most important and unoccluded facial regions. TFE is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for FER. Compared with traditional transformer, the novelty of TFE is two-fold: (i) To effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (ii) Given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed TFE on the two prevalent in-the-wild facial expression datasets (AffectNet and RAF-DB) and the their modifications with artificial occlusions. Experimental results show that TFE improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art FE methods, TFE obtains consistent improvements. Visualization results show TFE is capable of automatically focusing on the discriminative and non-occluded facial regions for robust FER.",
        "link": "http://dx.doi.org/10.3389/fnbot.2021.763100"
    },
    {
        "id": 24081,
        "title": "CNN-Mixer Hierarchical Spectral Transformer for Hyperspectral Image Classification",
        "authors": "Wei Liu, Saurabh Prasad, Melba Crawford",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281792"
    },
    {
        "id": 24082,
        "title": "Infrared target tracking based on transformer",
        "authors": "Zhou Xi, Li XiaoHong",
        "published": "2023-6-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2682473"
    },
    {
        "id": 24083,
        "title": "A Method for Measuring Liquid Weight Using a Hilbert Transformer",
        "authors": "Jun Obara, Naoyuki Aikawa",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mwscas57524.2023.10406133"
    },
    {
        "id": 24084,
        "title": "Multimodal Item Categorization Fully Based on Transformer",
        "authors": "Lei Chen, Houwei Chou, Yandi Xia, Hirokazu Miyake",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.ecnlp-1.13"
    },
    {
        "id": 24085,
        "title": "Transformer-based quality assessment model for generalized user-generated multimedia audio content",
        "authors": "Deebha Mumtaz, Ajit Jena, Vinit Jakhetiya, Karan Nathwani, Sharath Chandra Guntuku",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10386"
    },
    {
        "id": 24086,
        "title": "<i>De novo</i> mass spectrometry peptide sequencing with a transformer model",
        "authors": "Melih Yilmaz, William E. Fondrie, Wout Bittremieux, Sewoong Oh, William Stafford Noble",
        "published": "No Date",
        "citations": 15,
        "abstract": "AbstractTandem mass spectrometry is the only high-throughput method for analyzing the protein content of complex biological samples and is thus the primary technology driving the growth of the field of proteomics. A key outstanding challenge in this field involves identifying the sequence of amino acids—the peptide—responsible for generating each observed spectrum, without making use of prior knowledge in the form of a peptide sequence database. Although various machine learning methods have been developed to address this de novo sequencing problem, challenges that arise when modeling tandem mass spectra have led to complex models that combine multiple neural networks and post-processing steps. We propose a simple yet powerful method for de novo peptide sequencing, Casanovo, that uses a transformer framework to map directly from a sequence of observed peaks (a mass spectrum) to a sequence of amino acids (a peptide). Our experiments show that Casanovo achieves state-of-the-art performance on a benchmark dataset using a standard cross-species evaluation framework which involves testing with spectra with never-before-seen peptide labels. Casanovo not only achieves superior performance but does so at a fraction of the model complexity and inference time required by other methods.",
        "link": "http://dx.doi.org/10.1101/2022.02.07.479481"
    },
    {
        "id": 24087,
        "title": "The Impedance-Based Stability Analysis of the Single-Phase Solid State Transformer",
        "authors": "Rui Wang, Qiuye Sun, Xuemeng Zhang, Yuanpeng Du",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623288"
    },
    {
        "id": 24088,
        "title": "Adaptive Non-Local Regression Prior based on Transformer for Image Deblurring",
        "authors": "Yixing Ji",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3633637.3633713"
    },
    {
        "id": 24089,
        "title": "PiTE: TCR-epitope Binding Affinity Prediction Pipeline using Transformer-based Sequence Encoder",
        "authors": "Pengfei Zhang, Seojin Bang, Heewook Lee",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811270611_0032"
    },
    {
        "id": 24090,
        "title": "Chat Generative Pretrained Transformer: Extinction of the Designer or Rise of an Augmented Designer",
        "authors": "Amaninder Singh Gill",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "Abstract\nSystematic design process is used in engineering systems design to develop solutions to problems of varying complexity. There have been efforts within the community to develop tools to perform automated generative design at varying stages of the systematic design process. However, a Large Language Model (LLM) has never been used. To this end, this paper presents an initial investigation into the use of OpenAI’s ChatGPT to automatically generate solutions to an engineering problem. It is demonstrated that for the most part ChatGPT is quite capable of generating conceptual design for an engineering problem. In light of this technology, this paper floats questions on the future direction of research and education for the engineering systems design community.",
        "link": "http://dx.doi.org/10.1115/detc2023-116971"
    },
    {
        "id": 24091,
        "title": "Sparse Universal Transformer",
        "authors": "Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.12"
    },
    {
        "id": 24092,
        "title": "CD-Net: Histopathology Representation Learning Using Context-Detail Transformer Network",
        "authors": "Saarthak Kapse, Srijan Das, Prateek Prasanna",
        "published": "2023-4-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230626"
    },
    {
        "id": 24093,
        "title": "Condition Evaluation and Fault Diagnosis of Power Transformer Based on GAN-CNN",
        "authors": "",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23977/jeeem.2023.060302"
    },
    {
        "id": 24094,
        "title": "Evaluation of Reinforcement Learning in Transformer-based Molecular Design",
        "authors": "Jiazhen He, Alessandro Tibo, Jon Paul Janet, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Ola Engkvist",
        "published": "No Date",
        "citations": 0,
        "abstract": "Designing compounds with a range of desirable properties is a fundamental challenge in drug discovery. In pre-clinical early drug discovery, novel compounds are often designed based on an already existing promising starting compound thorough structural modifications for further property optimization. Recently, transformer-based deep learning models have been explored for the task of molecular optimization by training on pairs of similar molecules. This provides a starting point for generating similar molecules to a given input molecule, but has limited flexibility regarding user-defined property profiles. Here, we evaluate the effect of reinforcement learning on transformer-based molecular generative models. The generative model can be considered as a pre-trained model with knowledge of the chemical space close to an input compound, while reinforcement learning can be viewed as a tuning phase, steering the model towards chemical space with user-specific desirable properties. The evaluation of two distinct tasks - molecular optimization and scaffold hopping - suggest that reinforcement learning could guide the transformer-based generative model towards the generation of more compounds of interest. Additionally, the impact of pre-trained models, learning steps and learning rates are investigated.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2024-r9ljm"
    },
    {
        "id": 24095,
        "title": "Learning path recommendation based on Transformer reordering",
        "authors": "Yunxiang Liu, Yuanyuan Zhang, Guoqing Zhang",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isctt51595.2020.00025"
    },
    {
        "id": 24096,
        "title": "Text-conditioned Transformer for automatic pronunciation error detection",
        "authors": "Zhan Zhang, Yuehai Wang, Jianyi Yang",
        "published": "2021-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.specom.2021.04.004"
    },
    {
        "id": 24097,
        "title": "Research method of identifying transformer inrush current and fault current based on VMD-HHT",
        "authors": "Shangbin Jiao, Yuan Chang, Qing Zhang",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866504"
    },
    {
        "id": 24098,
        "title": "Multi-Modal Transformer for RGB-D Salient Object Detection",
        "authors": "Peipei Song, Jing Zhang, Piotr Koniusz, Nick Barnes",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9898069"
    },
    {
        "id": 24099,
        "title": "OSVConTramer: A Hybrid CNN and Transformer based Online Signature Verification",
        "authors": "Chandra Sekhar Vorugunti, Avinash Gautam, Viswanath Pulabaigari",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcb57857.2023.10449120"
    },
    {
        "id": 24100,
        "title": "Application of the Analysis of Variance (ANOVA) in the Interpretation of Power Transformer Faults",
        "authors": "Bonginkosi A. Thango",
        "published": "2022-10-1",
        "citations": 8,
        "abstract": "Electrical power transformers are the most exorbitant and tactically prominent components of the South African electrical power grid. In contrast, they are burdened by internal winding faults predominantly on account of insulation system failure. It is essential that these faults must be swiftly and precisely uncovered and suitable measures should be adopted to separate the faulty unit from the entire system. The frequency response analysis (FRA) is a technique for tracking a transformer’s mechanical integrity. Nevertheless, classifying the category of the fault and its gravity by benchmarking measured FRA responses is still backbreaking and for the most part, anchored in personnel proficiency. This work presents a quantum leap to normalize the FRA interpretation procedure by suggesting an interpretation code criteria based on an empirical survey of transformers ranging from 315 kVA to 40 MVA. The study then proposes an analysis of variance (ANOVA) based interpretation tool for diagnosing the statistical significance of FRA fingerprint and measured profiles. The latter cannot be relied upon by an expert or by the naked eye. Additionally, descriptive FRA frequency sub-region data statistics are proposed to evaluate the shift in both the magnitude and measuring frequency characteristics to formulate the recommended interpretation code criteria. To corroborate the code criteria by incorporating ANOVA and descriptive statistics, the study presents various case studies with unknown FRA profiles for fault diagnosis. The results constitute proof of the reliability of the proposed code criteria and a proposed hybrid of ANOVA and descriptive statistics.",
        "link": "http://dx.doi.org/10.3390/en15197224"
    },
    {
        "id": 24101,
        "title": "Image Classification Based on Convolution and Lite Transformer",
        "authors": "Xiaoliang Han, Kaihe Wang, Shengxia Tu, Weisong Zhou",
        "published": "2022-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaml57167.2022.00009"
    },
    {
        "id": 24102,
        "title": "A Transformer-Based Front-End Circuit for Grounded Capacitive Sensors with Square-Wave Excitation",
        "authors": "Marcelo A. Haberman, Enrique M. Spinelli, Ferran Reverter",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/proceedings2024097014"
    },
    {
        "id": 24103,
        "title": "Lattice Transformer for Speech Translation",
        "authors": "Pei Zhang, Niyu Ge, Boxing Chen, Kai Fan",
        "published": "2019",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1649"
    },
    {
        "id": 24104,
        "title": "A Multiscale Visualization of Attention in the Transformer Model",
        "authors": "Jesse Vig",
        "published": "2019",
        "citations": 177,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-3007"
    },
    {
        "id": 24105,
        "title": "Meshed-Memory Transformer for Image Captioning",
        "authors": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara",
        "published": "2020-6",
        "citations": 475,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr42600.2020.01059"
    },
    {
        "id": 24106,
        "title": "Spam email classification based on SVM, Transformer and Naive Bayes",
        "authors": "Yijun Qiao",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "As a matter of fact, with the booming of information and big data, there are too many unwanted e-mails called spam sent to peoples e-mail account in recent years. On this basis, it could lead to a lot of problems including occupying the public resources, causing financial loss and so on. With this in mind, spam filtering technique is in need to solve the problem and address the issues. In reality, based on previous analysis, machine learning methods are very effective in spam filtering. On this basis, this study carried out background research of machine learning algorithms in spam filtering, and find the spam e-mail dataset of Kaggle.com, and implement 3 algorithms on the dataset. According to the analysis, Transformer and SVM work better on the dataset, and SVM is the best. At the same time, the current limitations are discussed as well. In addition, the prospects are demonstrated in the meantime.",
        "link": "http://dx.doi.org/10.54254/2755-2721/48/20241337"
    },
    {
        "id": 24107,
        "title": "Surface tracking along the interphase barrier of a large transformer",
        "authors": "W. Thansiphraserth, P. L. Lewin",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceidp.2017.8257563"
    },
    {
        "id": 24108,
        "title": "Limitations of Partial Discharge De-noising of Power Transformer Using Adaptive Singular Value Decomposition",
        "authors": "Hossein Karami, Gevork B. Gharehpetian",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081523"
    },
    {
        "id": 24109,
        "title": "Comparison of Transformer and Convolutional Neural Network For   Multi-Sensor   Rolling Bearing Fault Diagnosis",
        "authors": "Xiang Gao, Xiangjun Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4044243"
    },
    {
        "id": 24110,
        "title": "Analysis of No-load Conditions of an Overexcited Transformer",
        "authors": "Anatolii G. Lavrov, Sergey V. Baklanov",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cts48763.2019.8973346"
    },
    {
        "id": 24111,
        "title": "Analysis of Deep Ensemble Transformer Model for Fake News Detection",
        "authors": "Rashmi Panda, Satya Kumari",
        "published": "2022-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsis55133.2022.9915941"
    },
    {
        "id": 24112,
        "title": "Overcoming Transformer Fine-Tuning process to improve Twitter Sentiment Analysis for Spanish Dialects",
        "authors": "Daniel Palomino",
        "published": "2020-12-12",
        "citations": 0,
        "abstract": "Is there an effective Spanish Sentiment Analysis algorithm? The aim of this paper is to answer this question. The task is challenging because there are several dialects for the Spanish Language. Thus, identically written words could have several meanings and polarities regarding Spanish speaking countries. To tackle this multidialect issue we rely on a transfer learning approach. To do so, we train a BERT language model to “transfer” general features of the Spanish language. Then, we fine-tune the language model to specific dialects. BERT is also used to generate contextual data augmentation aimed to prevent overfitting. Finally, we build the polarity classifier and propose a fine-tuning step using groups of layers. Our design choices allow us to achieve state-of-the-art results regarding multidialect benchmark datasets.",
        "link": "http://dx.doi.org/10.52591/lxai202012124"
    },
    {
        "id": 24113,
        "title": "Design of Capacitive Coupled Transformer for Power Converter Applications",
        "authors": "Necdet Kaan Onur, Gokturk Poyrazoglu",
        "published": "2021-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elma52514.2021.9502968"
    },
    {
        "id": 24114,
        "title": "Software Defect Prediction Method Based on Transformer Model",
        "authors": "Wei Zheng, Lijuan Tan, Chengbin Liu",
        "published": "2021-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica52286.2021.9498179"
    },
    {
        "id": 24115,
        "title": "Doubly-fed Solid State Auto-Transformer (SSAT) concept for Multi-pulse Rectifiers",
        "authors": "Farhana Islam, Harish S. Krishnamoorthy",
        "published": "2021-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce47101.2021.9595409"
    },
    {
        "id": 24116,
        "title": "Analysis of Fundamental Differences in Transformer 87T Differential Protection",
        "authors": "Mike Kockott, Zoran Gajic, Galina Antonova, Sergiu Paduraru",
        "published": "2021-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre48231.2021.9429724"
    },
    {
        "id": 24117,
        "title": "Correlation of Tangible Quality Parameters of Vegetable-Based Transformer Fluids",
        "authors": "Adango Miadonye, Mumuni Amadu, James Stephens, Thomas Okeefe",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4246485"
    },
    {
        "id": 24118,
        "title": "Energy Harvesting Using Piezo Transformer from Residual Fault Current 30 Ma",
        "authors": "Gurivi Reddy, Yogesh Rajwade",
        "published": "2022-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismsit56059.2022.9932816"
    },
    {
        "id": 24119,
        "title": "Advantages of interwinding capacitive test setup in FRA diagnostics of transformer windings",
        "authors": "Wojciech SZOKA",
        "published": "2018-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2018.07.10"
    },
    {
        "id": 24120,
        "title": "Improved Research on the Transformer-Inductor Simulation Model of Magnetics",
        "authors": "Jiang Liyuan",
        "published": "2017-11-30",
        "citations": 0,
        "abstract": "Transformer-inductor simulation model notÂ only reflects the characteristics of magnetic path andÂ circuit, but also brings in magnetic components thatÂ reflected the parasitic capacitance. There are furtherÂ research, strict derivation and magnetic circuit equivalentÂ for the model in this article. Under the condition ofÂ considering hysteresis, saturation effect we can conclude aÂ new modeling and its equivalent, which can make theÂ magnetic curve and characteristic get better fitting. ItÂ shows that the transformer-inductance simulation model isÂ easy to spread and use.",
        "link": "http://dx.doi.org/10.26689/jera.v1i3.163"
    },
    {
        "id": 24121,
        "title": "Research on Network Intrusion Detection Based on Transformer",
        "authors": "Gang Gan, Weiju Kong",
        "published": "2023-5-4",
        "citations": 0,
        "abstract": "With the advancement of technology, the development of various industries has become inseparable from informatization. People's lives have become closely related to the network. While using the network to facilitate our lives, massive data is also generated. Traditional firewall technologies are no longer sufficient to meet current needs. Deep learning algorithms can establish complex mapping relationships between network data, and can extract hidden correlation features between data features to achieve data recognition and prediction. Therefore, this paper introduces Transformer and Bidirectional Long Short-Term Memory (BiLSTM) into the field of intrusion detection, and proposes an intrusion detection method based on the combination of Transformer-Encoder and BiLSTM (TBL). Deep Neural Networks (DNN) are used to further extract data features, and the softmax function is used to output classification results. In order to verify the effectiveness of this method, this paper trains and tests the TBL method on the NSL-KDD dataset, and verifies its feasibility and superiority.",
        "link": "http://dx.doi.org/10.54097/fcis.v3i3.7987"
    },
    {
        "id": 24122,
        "title": "Design of Magnetic Integrated LLC Transformer",
        "authors": "Liang Xu, Dong Qing Miao, Jing Qiu Zhang",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "The direct current/direct current (DC/DC) converter of the LLC (inductor-inductor-capacitor) converter is an important part of affecting the work efficiency, volume, and weight of the device. It not only has the functions of traditional transformers but is also able to solve the problems of traditional power transformers’ high price, huge volume, prodigious no-load loss, and inflexible control. This paper studies the DC/DC converter mainly, according to the given indexes, the magnetic integrated LLC resonant transformer is designed in detail. The magnetic integrated transformer greatly reduces the converter volume, and the selection of devices is completed based on parameters design. In addition, according to design parameters, losses and the efficiency of the LLC resonant transformer are calculated. The results meet the efficiency requirements. A test platform of a full-bridge LLC resonant converter is built according to theoretical research. The correctness and effectiveness of theoretical research and design methods of the DC/DC converter are verified by analyzing the experimental waveforms.",
        "link": "http://dx.doi.org/10.26689/ssr.v5i6.5079"
    },
    {
        "id": 24123,
        "title": "Recurrent Attention for the Transformer",
        "authors": "Jan Rosendahl, Christian Herold, Frithjof Petrick, Hermann Ney",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.insights-1.10"
    },
    {
        "id": 24124,
        "title": "Detection of Fake News Using Transformer Model",
        "authors": "Momina Qazi, Muhammad U.S. Khan, Mazhar Ali",
        "published": "2020-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icomet48670.2020.9074071"
    },
    {
        "id": 24125,
        "title": "Super-resolution for satellite imagery: uncovering details using a new Cross Band Transformer architecture",
        "authors": "Jasper S. Wijnands, Nikolaos Ntantis, Jan Fokke Meirink, Domenica Dibenedetto",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recent advances in artificial intelligence (AI) techniques have enabled the processing and analysis of vast datasets, such as archives of satellite observations. In the geosciences, remote sensing has transformed the way in which the atmosphere and surface are observed. Traditionally, substantial funding is directed towards the development of new satellites to improve observation accuracy. Nowadays, novel methods based on AI could become a complementary approach to further enhance the resolution of observations. Therefore, we developed a new, state-of-the-art super-resolution methodology.\nSatellites commonly measure electromagnetic radiation, reflected or emitted by the earth's surface and atmosphere, in different parts of the spectrum. Many instruments capture both panchromatic (PAN) and low-resolution multi-spectral (LRMS) images. While PAN typically covers a broad spectral range, LRMS focuses on details in narrow bands within that range. Pansharpening is the task of fusing the spatial details of PAN with the spectral richness of LRMS, to obtain high-resolution multi-spectral (HRMS) images. This has proven to be valuable in many areas of the geosciences, leading to new capabilities such as detecting small-sized marine plastic litter and identifying buried archaeological remains. Although HRMS images are not directly captured by the satellite, they can provide enhanced visual clarity, uncover intricate patterns and allow for more accurate and detailed analyses.\nTechnically, pansharpening is closely related to the single image super-resolution task, where attention-based models have achieved excellent results. In our study a new Cross Band Transformer (CBT) for pansharpening was developed, incorporating and adapting successful features of vision transformer architectures. Information sharing between the panchromatic and multi-spectral input streams was enabled through two novel components: the Shifted Cross-Band Attention Block and the Overlapping Cross-Band Attention Block, implementing mechanisms for shifted and overlapping cross-attention. Each block led to a more accurate fusion of panchromatic and multi-spectral data. For evaluation, CBT was also compared to seven competitive benchmark methods, including MDCUN, PanFormer and ArbRPN. Our model produced state-of-the-art results on the widely used GaoFen-2 and WorldView-3 pansharpening datasets. Based on peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) scores of the generated images, CBT outperformed all benchmark methods. Our AI method can be integrated in existing remote sensing pipelines, as CBT converts actual observations into a high-resolution equivalent for use in downstream tasks. A PyTorch implementation of CBT is available at https://github.com/VisionVoyagerX/CBT.\nFurthermore, we developed the Sev2Mod dataset, available at https://zenodo.org/record/8360458. Unlike conventional benchmark datasets, Sev2Mod acquired input and target pairs from two different satellite instruments: (i) SEVIRI onboard the Meteosat Second Generation (MSG) satellite in geostationary orbit and (ii) MODIS onboard the Terra satellite in polar, sun-synchronous orbit. SEVIRI measures a fixed field of view quasi-continuously, while MODIS passes only twice a day but observes at a much higher spatial resolution. Our study investigated image generation at the spatial resolution of MODIS, while preserving SEVIRI's high temporal resolution. Since Sev2Mod is better aligned with actual situations one may encounter in applications of pansharpening methods (e.g., noise, bias, approximate temporal matching), it provides a solid foundation to design robust pansharpening models for real-world applications.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-340"
    },
    {
        "id": 24126,
        "title": "DESIGN AND TESTS OF TRANSFORMER RESISTANCE SPOT WELDER",
        "authors": "Aleksander Smolarski",
        "published": "2021-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/13.2021.12.5"
    },
    {
        "id": 24127,
        "title": "Transformer Component Recognition in Pictures Taken by Submersible Robots",
        "authors": "Yingjie Yan, Yadong Liu, Zhicheng Xie, Jun Deng",
        "published": "2021-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec53008.2021.9735767"
    },
    {
        "id": 24128,
        "title": "DLGNet: A Transformer-based Model for Dialogue Response Generation",
        "authors": "Olabiyi Oluwatobi, Erik Mueller",
        "published": "2020",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.nlp4convai-1.7"
    },
    {
        "id": 24129,
        "title": "Multi Criteria Decision Making Approach for Interpretation of Transformer Dissolved Gas Analysis",
        "authors": "Madavan R, Saroja S, Haseena S",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4235043"
    },
    {
        "id": 24130,
        "title": "A transformer-based framework for automatic COVID19 diagnosis in chest CTs",
        "authors": "Lei Zhang, Yan Wen",
        "published": "2021-10",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw54120.2021.00063"
    },
    {
        "id": 24131,
        "title": "Power Inter Cell Transformer Modelling for ASV Application",
        "authors": "Guillaume Pellecuer, Thierry Martire, Loic Daridon",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon49645.2022.9968513"
    },
    {
        "id": 24132,
        "title": "Transformer-based Automatic Mapping of Clinical Notes to Specific Clinical Concepts",
        "authors": "Jay Ganesh, Ajay Bansal",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00080"
    },
    {
        "id": 24133,
        "title": "IC-CViT: Inverse-Consistent Convolutional Vision Transformer for Diffeomorphic Image Registration",
        "authors": "Tao Xu, Ting Jiang, Xiaoning Li",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191209"
    },
    {
        "id": 24134,
        "title": "Development of retrofit MV/LV transformer designs to accommodate increased electrification",
        "authors": "C. Power, A. Walsh",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.0656"
    },
    {
        "id": 24135,
        "title": "Comparison of Fault Identification Methods for Analyzing Transformer Dissolved Gas*",
        "authors": "Neha Hirkaney, Sweta Lall",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epee59859.2023.10351906"
    },
    {
        "id": 24136,
        "title": "The Actual Operating Lifespan of Power Transformer Based on Traditional Techniques and Adaptive Neuro-Fuzzy Interference System",
        "authors": "Permit Mathuhu Sekatane",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica57932.2023.10363304"
    },
    {
        "id": 24137,
        "title": "Transformer Neural Network for Structure Constrained Molecular Optimization",
        "authors": "Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, eva nittinger, Christian Tyrchan, Werngard Czechtizky",
        "published": "No Date",
        "citations": 1,
        "abstract": "Finding molecules with a desirable balance of multiple properties is a main challenge in drug discovery. Here, we focus on the task of molecular optimization, where a starting molecule with promising properties needs to be further optimized towards the desirable properties. Typically, chemists would apply chemical transformations to the starting molecule based on their intuition. A widely used strategy is the concept of matched molecular pairs where two molecules differ by a single transformation. In particular, a chemist would be interested in keeping one part of the starting molecule (core) constant, while substituting the other part (R-group), to optimize the starting molecule towards desirable properties. Motivated by this, we train a Transformer model, Transformer-R, to generate R-groups given the starting molecule (with its core and R-group specified) and the specified desirable properties. The generated R-groups will be attached to the core to form the final molecules, which are guaranteed to keep the core of interest and are expected to satisfy the desirable properties in the input. Our model could accelerate the process of optimizing antiviral drug candidates in terms of various properties of interest, e.g. pharmacokinetics.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14416133"
    },
    {
        "id": 24138,
        "title": "DEVTrV2: Enhanced Data-Efficient Video Transformer For Violence Detection",
        "authors": "Almamon Rasool Abdali, Ammar Abdullah Aggar",
        "published": "2022-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icivc55077.2022.9886172"
    },
    {
        "id": 24139,
        "title": "Help Transformer Improve Performance in Automatic Mathematics Word Problem-Solving",
        "authors": "Dong Liu, Guanfang Wang, Jialiang Yang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3220777"
    },
    {
        "id": 24140,
        "title": "A Transformer-based Cascade Network with Boundary Enhancement Loss for Retinal Vessel Segmentation",
        "authors": "Binke Cai, Liyan Ma",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956483"
    },
    {
        "id": 24141,
        "title": "Sliceformer: Deep Dense Depth Estimation from a Single Indoor Omnidirectional Image Using a Slice-Based Transformer",
        "authors": "Yihong Wu, Yuwen Heng, Mahesan Niranjan, Hansung Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760939"
    },
    {
        "id": 24142,
        "title": "Traffic Prediction Based on Spatiotemporal-Guided Multi Graph Sandwich-Transformer",
        "authors": "Yanjie Wen, zhihong li, Xiaoyu Wang, Wangtu Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4206935"
    },
    {
        "id": 24143,
        "title": "The Evaluation of Distribution Transformer in PEA using CBRM",
        "authors": "P. Rodkumnerd, K. Hongesombut",
        "published": "2019-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gtdasia.2019.8715997"
    },
    {
        "id": 24144,
        "title": "Biodegradability Testing of Transformer Oils: Review",
        "authors": "Ahmad Abualasal, István Kiss",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cando-epe60507.2023.10418015"
    },
    {
        "id": 24145,
        "title": "Evaluation of Harmonic Distortion in Distribution Networks under Transformer N-1 Security Criterion",
        "authors": "Pablo Rodríguez-Pajarón, Araceli Hernández, Jovica V. Milanović",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202763"
    },
    {
        "id": 24146,
        "title": "Analysis and mitigation of interaction between transformer inrush current and HVDC operation",
        "authors": "Y.Z. Zhang, C. M. Bush",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8273789"
    },
    {
        "id": 24147,
        "title": "Image Aesthetic Description Based on Semantic Addition Transformer Model",
        "authors": "",
        "published": "2021-10",
        "citations": 0,
        "abstract": "Image aesthetic quality assessment has been a hot research topic in the field of image analysis during the last decade. Most recently, people have proposed comment type assessment to describe the aesthetics of an image using text automatically. However, existing works have rarely considered the quality of the aesthetic description. In this work, we propose a novel neural image aesthetic description network framework, named Deep Image Aesthetic Reviewer (DIAReviewer), based on Semantic Addition Transformer Model, the learning of Residual Network, and the Attention Mechanism in a single framework. Beyond that, we design a Semantic Addition module to compromise the image feature and semantic information to focus on the comment quality, such as fluency and complexity. We introduce a new image dataset named Aesthetic Review Dataset (ARD), which contains one or more aesthetic comments for each image. Finally, the experimental results on ARD show that our model outperforms other methods in content complexity and sentence fluency of aesthetic descriptions.",
        "link": "http://dx.doi.org/10.4018/ijcini.20211001oa01"
    },
    {
        "id": 24148,
        "title": "4.4 The Physics of the Hilbert Transformer",
        "authors": "",
        "published": "2024-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780691235325-023"
    },
    {
        "id": 24149,
        "title": "CALCULATION OF MECHANICAL RELIABILITY OF TRANSFORMER HOUSING",
        "authors": "Vladimir Dmitrievich Lebedev, Stanislav Fedorovich Smirnov, Vladimir Viktorovich Terentiev",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35523/2307-5872-2021-36-3-63-69"
    },
    {
        "id": 24150,
        "title": "Improving Medical Transformer Design",
        "authors": "Yoganand Velayutham",
        "published": "2021-11",
        "citations": 0,
        "abstract": " Design engineer, YOGANAND VELAYUTHAM, looks at how to improve the design of medical grade transformers ",
        "link": "http://dx.doi.org/10.12968/s0047-9624(22)60565-0"
    },
    {
        "id": 24151,
        "title": "Digital Differential Protection of the «Generator-Transformer» Block",
        "authors": "N.S. Buryanina, R.O. Gogolev, Y.F. Korolyuk, E.V. Lesnykh, K.V. Suslov",
        "published": "2019-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eastconf.2019.8725341"
    },
    {
        "id": 24152,
        "title": "fBERT: A Neural Transformer for Identifying Offensive Content",
        "authors": "Diptanu Sarkar, Marcos Zampieri, Tharindu Ranasinghe, Alexander Ororbia",
        "published": "2021",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.154"
    },
    {
        "id": 24153,
        "title": "ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective",
        "authors": "Sultan Alrowili, Vijay Shanker",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.108"
    },
    {
        "id": 24154,
        "title": "TransKP: Transformer based Key-Phrase Extraction",
        "authors": "Mukund Rungta, Rishabh Kumar, Mehak Preet Dhaliwal, Hemant Tiwari, Vanraj Vala",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206812"
    },
    {
        "id": 24155,
        "title": "Experimental investigation on stability and dielectric break down strength of transformer oil based nanofluids",
        "authors": "Ravi Babu S., Sambasiva Rao G.",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5032037"
    },
    {
        "id": 24156,
        "title": "UCViT: Hardware-Friendly Vision Transformer via Unified Compression",
        "authors": "HongRui Song, Ya Wang, Meiqi Wang, Zhongfeng Wang",
        "published": "2022-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas48785.2022.9937660"
    },
    {
        "id": 24157,
        "title": "TransDBC: Transformer for Multivariate Time-Series based Driver Behavior Classification",
        "authors": "Jayant Vyas, Nishit Bhardwaj,  Bhumika, Debasis Das",
        "published": "2022-7-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892146"
    },
    {
        "id": 24158,
        "title": "Coreless Transformer based High Voltage Generator for Intense Magnetic Field Applications",
        "authors": "Saijun Mao, Jan Abraham Ferreira",
        "published": "2020-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec39645.2020.9124165"
    },
    {
        "id": 24159,
        "title": "Increasing the Efficiency of Power Electronics Application at Transformer Substations",
        "authors": "V.S. Klimash, A.M. Konstantinov",
        "published": "2020-10-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271413"
    },
    {
        "id": 24160,
        "title": "A Single-Phase Transformer-less Grid Connected Photovoltaic Inverter",
        "authors": "Asmita M. Gaikwad, Shailendra K. Mittal",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icisc47916.2020.9171090"
    },
    {
        "id": 24161,
        "title": "Bev-Cfkt: A Lidar-Camera Cross-Modality-Interaction Fusion and Knowledge Transfer Framework with Transformer for Bev 3d Object Detection",
        "authors": "Ming Wei, Junguo Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4559725"
    },
    {
        "id": 24162,
        "title": "The Design of Wireless Power Transformer for Electronic Flower Pot",
        "authors": "Chia-Yang Liu, Chih-Lung Hsiao",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce46687.2019.9015476"
    },
    {
        "id": 24163,
        "title": "An Interpretable and Transferrable Vision Transformer Model for Rapid Materials Spectra Classification",
        "authors": "Zhenru Chen, Yunchao Xie, Yuchao Wu, Yuyi Lin, Shigetaka Tomiya, Jian Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Rapid analysis of materials characterization spectra is pivotal for preventing accumulation of unwieldy datasets, thus accelerating subsequent decision-making. However, current methods heavily rely on experience and domain knowledge, which not only proves tedious but also is hard to keep up with the pace of data acquisition. In this context, we introduce a transferable Vision Transformer (ViT) model for identification of materials from their spectra, including XRD and FTIR. First, an optimal ViT model was trained to predict metal organic frameworks (MOFs) from their XRD spectra. It attains prediction accuracies of 70%, 93%, and 94.9% for Top-1, Top-3, and Top-5, respectively, and a shorter training time of 269 seconds in comparison to a convolutional neural network model. The dimension reduction and attention weight map underline its adeptness at capturing relevant features in the XRD spectra for determining the prediction outcome. Moreover, the model can be transferred to a new one for prediction of organic molecules from their FTIR spectra, attaining remarkable Top-1, Top-3, and Top-5 prediction accuracies of 84%, 94.1%, and 96.7%, respectively. The introduced ViT based model would set a new revenue to handling diverse types of spectroscopic data, thus expediting the materials characterization processes.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-fhhr4"
    },
    {
        "id": 24164,
        "title": "A Transformer-based Trajectory Prediction Method",
        "authors": "Zhenggui Xiang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3570361.3615733"
    },
    {
        "id": 24165,
        "title": "Enhancing Cervical Pre-Cancerous Classification Using Advanced Vision Transformer",
        "authors": "Manal Darwish, Mohamad Ziad Altabel, Rahib H. Abiyev",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "One of the most common types of cancer among in women is cervical cancer. Incidence and fatality rates are steadily rising, particularly in developing nations, due to a lack of screening facilities, experienced specialists, and public awareness. Visual inspection is used to screen for cervical cancer after the application of acetic acid (VIA), histopathology test, Papanicolaou (Pap) test, and human papillomavirus (HPV) test. The goal of this research is to employ a vision transformer (ViT) enhanced with shifted patch tokenization (SPT) techniques to create an integrated and robust system for automatic cervix-type identification. A vision transformer enhanced with shifted patch tokenization is used in this work to learn the distinct features between the three different cervical pre-cancerous types. The model was trained and tested on 8215 colposcopy images of the three types, obtained from the publicly available mobile-ODT dataset. The model was tested on 30% of the whole dataset and it showed a good generalization capability of 91% accuracy. The state-of-the art comparison indicated the outperformance of our model. The experimental results show that the suggested system can be employed as a decision support tool in the detection of the cervical pre-cancer transformation zone, particularly in low-resource settings with limited experience and resources.",
        "link": "http://dx.doi.org/10.3390/diagnostics13182884"
    },
    {
        "id": 24166,
        "title": "Determining Power Transformer Maintenance Plan Using Three-Dimensional Risk Matrix",
        "authors": "Phoumsavath Souvannalath, Suttichai Premrudeepreechacharn, Kanchit Ngamsanroaj",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powercon53406.2022.9929632"
    },
    {
        "id": 24167,
        "title": "Features of Modeling of Microprocessor Protection of a Transformer",
        "authors": "O. N. Kuzyakov, E. P. Vlasova",
        "published": "2023-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icieam57311.2023.10138957"
    },
    {
        "id": 24168,
        "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation",
        "authors": "Xipin Wei, Junhui Chen, Zirui Zheng, Li Guo, Lantian Li, Dong Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1626"
    },
    {
        "id": 24169,
        "title": "J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences",
        "authors": "Yi-Fen Liu, Xiang-Li Lu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-695"
    },
    {
        "id": 24170,
        "title": "Design of simplifying modeling of stacked iron core transformer",
        "authors": "Yi Feng",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3010828"
    },
    {
        "id": 24171,
        "title": "Transformer la formation des aides-soignants, un enjeu majeur pour l’avenir",
        "authors": "Alexandre Niggel",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aidsoi.2023.10.004"
    },
    {
        "id": 24172,
        "title": "Retracted: 3D Human Pose Estimation Based on Transformer Algorithm",
        "authors": "",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1155/2023/9813246"
    },
    {
        "id": 24173,
        "title": "A novel llc resonant dc-dc converter with integrated transformer",
        "authors": "Shota Kimura, Kimihiro Nanamori, Mostafa Noah, Masayoshi Yamamoto",
        "published": "2017-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intlec.2017.8214186"
    },
    {
        "id": 24174,
        "title": "Fashion Compatibility Learning Via Triplet-Swin Transformer",
        "authors": "Hosna Darvishi, Reza Azmi, Fatemeh Moradian, Maral Zarvani",
        "published": "2023-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csicc58665.2023.10105392"
    },
    {
        "id": 24175,
        "title": "Dissolved Gas Analysis Interpretation and Intelligent Machine Learning Techniques",
        "authors": "",
        "published": "2017-8-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch4"
    },
    {
        "id": 24176,
        "title": "Transformer-Based Monocular Depth Estimation Using Token Attention",
        "authors": "Zhiyong Huo, Yihang Chen, Junyu Wei, Quan Guo, Yuankai Huo",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4248658"
    },
    {
        "id": 24177,
        "title": "Building Blocks for a Complex-Valued Transformer Architecture",
        "authors": "Florian Eilers, Xiaoyi Jiang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095349"
    },
    {
        "id": 24178,
        "title": "A Link Prediction Model of Dynamic Heterogeneous Network Based on Transformer",
        "authors": "Beibei Ruan, Cui Zhu, Wenjun Zhu",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892546"
    },
    {
        "id": 24179,
        "title": "On Block g-Circulant Matrices with Discrete Cosine and Sine Transforms for Transformer-Based Translation Machine",
        "authors": "Euis Asriani, Intan Muchtadi-Alamsyah, Ayu Purwarianti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer has emerged as one of the modern neural networks that applied in numerous applications. However, the large and deep architecture of transformers makes themselves both computation and memory-intensive. In this paper, we propose the block g-circulant matrices to replace the dense weight matrices in feed forward layers of the transformer and leverage the DCT-DST algorithm to multiply these matrices with the input vector. Our experiment on Portuguese-English datasets show that the proposed approach achieves a significant gain in model memory efficiency compared to dense transformer, with a slight accuracy degradation. We found that the highest model memory efficiency is achieved by the model Dense-block 1-circulant DCT-DST of 128 dimension at 22,14%. We further show that the same model achieved a BLEU score of 26.47%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3853113/v1"
    },
    {
        "id": 24180,
        "title": "EEG Source Imaging based on a Transformer Encoder Network",
        "authors": "Tongtong Zheng, Zijing Guan",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105793"
    },
    {
        "id": 24181,
        "title": "Topological Overview on Solid-state Transformer Traction Technology in High-speed Trains",
        "authors": "Deepak Ronanki, Sheldon S. Williamson",
        "published": "2018-6",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itec.2018.8450135"
    },
    {
        "id": 24182,
        "title": "Spatio-Temporal Swin Transformer-based 4-D EEG Emotion Recognition",
        "authors": "Zongnan Chen, Jiarui Jin, Jiahui Pan",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385526"
    },
    {
        "id": 24183,
        "title": "Reluctance circuit analysis and structure optimization of contactless transformer",
        "authors": "Luona Xu, Yumei Du, Liming Shi",
        "published": "2017-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2017.8055953"
    },
    {
        "id": 24184,
        "title": "Détourner, transformer, s’approprier : les pratiques créatives des joueurs et joueuses de jeu vidéo",
        "authors": "Fanny Barnabé",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.pulg.26299"
    },
    {
        "id": 24185,
        "title": "Arabic News Summarization based on T5 Transformer Approach",
        "authors": "Qusai Ismail, Kefah Alissa, Rehab M. Duwairi",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icics60529.2023.10330509"
    },
    {
        "id": 24186,
        "title": "System of Predicting Dementia Using Transformer Based Ensemble Learning",
        "authors": "Kazu Nishikawa, Rin Hirakawa, Hideaki Kawano, Yoshihisa Nakatoh",
        "published": "2022-1-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce53296.2022.9730395"
    },
    {
        "id": 24187,
        "title": "A Block Cipher Recognition Scheme Based on Deep Learning Transformer Algorithm",
        "authors": "Ke Yuan, Bowen Zhang, Yuwei Zhou, Hanlin Sun, Wei Yang, Chunfu Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCryptographic algorithms are the core of data encryption, and their identification is a prerequisite for in-depth analysis of cryptography. Cryptographic algorithm identification is the process of distinguishing or identifying encryption methods by analyzing potentially distinctive information in the ciphertext when the ciphertext is known. It serves as the foundation for cryptanalysis work. The machine learning-based approach for identifying cryptographic algorithms extracts ciphertext features and trains the machine learning algorithm to build a cryptographic system identification classifier. It has the characteristics of high accuracy, concise operation process, and strong practicability. The ciphertext's complexity and the inter-data interference both rise with the number of cryptographic algorithms. This will lead to a reduction in the recognition rate of traditional machine learning cryptographic algorithm recognition solutions, poor recognition stability, and huge challenges to recognition capabilities. Deep learning algorithms have the characteristics of strong learning ability, a large amount of data processing, wide-coverage, good adaptability, and portability, etc., and have become a hot spot in the realm of cryptographic algorithm identification. In this work, a cryptographic algorithm identification methodology based on the Transformer algorithm is proposed, and the ciphertext feature extraction approach in the NIST randomness test is improved. In the research on cryptographic algorithm identification, we selected five block cipher algorithms: AES, 3DES, Blowfish, CAST, and RC2 as the research objects, and conducted two-class and five-class recognition experiments. According to experimental data, the approach suggested in this article has superior accuracy and stability than the classic machine learning model when the ciphertext size and other experimental variables are the same. Among them, on ciphertext files ranging from 1kb to 512kb, compared to the accuracy of the conventional classic machine learning algorithms SVM, GNB, KNN, RF, and LR, the average recognition accuracy of the two categories is 0.855, which is 29% higher. 33.9%, 28.5%, 34.2%, and 29.9%. Compared with the MLP accuracy of the same deep learning algorithm, the accuracy is also improved by 9%. When conducting the five-category experiment, the recognition accuracy of the scheme put forward in this piece is not less than 0.4. When the ciphertext file is 512KB, the recognition accuracy is as high as 0.42, and the average recognition accuracy of the five categories is 0.412. It is far greater than the other five classic machine learning algorithms and significantly better than the 20% random classification accuracy. Other algorithms' recognition rates fluctuate as ciphertext file sizes change. Among them, the Transformer algorithm has the smallest fluctuation and the best overall effect.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3920602/v1"
    },
    {
        "id": 24188,
        "title": "Transformer Design Principles",
        "authors": "Robert M. Del Vecchio, Bertrand Poulin, Pierre T. Feghali, Dilipkumar M. Shah, Rajendra Ahuja",
        "published": "2017-12-19",
        "citations": 50,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/ebk1439805824"
    },
    {
        "id": 24189,
        "title": "Convolutional Transformer with Sentiment-aware Attention for Sentiment Analysis",
        "authors": "Pengfei Li, Peixiang Zhong, Jiaheng Zhang, Kezhi Mao",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206796"
    },
    {
        "id": 24190,
        "title": "Magnetic Flux Density Determination in 3D Wound Core Transformer Using H-balance Equation",
        "authors": "Pedram Elhaminia, Ehsan Hajipour, Mehdi Vakilian",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081444"
    },
    {
        "id": 24191,
        "title": "Dielectric Response Model for Transformer Insulation Using Frequency Domain Spectroscopy and Vector Fitting",
        "authors": "Giovanni Hernandez, Abner Ramirez",
        "published": "2022-4-5",
        "citations": 6,
        "abstract": "This paper proposes a rational approximation-based approach to find positive real parameters for the extended Debye model (EDM), aimed at condition assessment of insulation systems of power transformers. The EDM can model the slow and fast polarization phenomenon, including relaxation mechanisms with different relaxation times within a composite dielectric material. In the proposed approach, the complex permittivity of the transformer’s composite insulation is approximated via rational functions, as given by the vector fitting (VF) software tool, and the EDM parameters are identified from the obtained poles/residues. To guarantee positive real parameters, i.e., a physically realizable circuit, VF is internally modified to calculate the final residues of the rational approximation via a constrained linear least-squares problem without resorting to further post-processing algorithms, as in existing methods, hence without affecting fitting accuracy. The effectiveness of the parametrized EDM is demonstrated in two ways: (a) by reconstructing frequency domain spectroscopy (FDS) curves provided via measurements in new oil-immersed power transformers and (b) by the comparison of the calculated polarization current given by EDM versus real measurements in time domain. The achieved fitting accuracy in most of the cases is above 99 percent for the reconstructed FDS curves, while the polarization current waveform is reproduced with good agreement.",
        "link": "http://dx.doi.org/10.3390/en15072655"
    },
    {
        "id": 24192,
        "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Yves Scherrer, Jörg Tiedemann",
        "published": "2020",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.49"
    },
    {
        "id": 24193,
        "title": "Standardized survey of transformer reliability: On behalf of CIGRE WG A2.37",
        "authors": "S. Tenbohlen, J. Jagers, F. Vahidi",
        "published": "2017-9",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iseim.2017.8166559"
    },
    {
        "id": 24194,
        "title": "A Series-connected 18-pulse Rectifier Using Isolated Transformer",
        "authors": "Quanhui Li, Fangang Meng, Huaqiang Zhang",
        "published": "2019-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8922339"
    },
    {
        "id": 24195,
        "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition",
        "authors": "Wenyong Huang, Wenchao Hu, Yu Ting Yeung, Xiao Chen",
        "published": "2020-10-25",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2361"
    },
    {
        "id": 24196,
        "title": "High-Frequency Transformer Design for Smart Microgrids",
        "authors": "M N Nachappa, Ajay Kumar, Deeksha Choudhary",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/peeic59336.2023.10450384"
    },
    {
        "id": 24197,
        "title": "SiC-Based Power Electronic Traction Transformer (PETT) for 3 kV DC Rail Traction",
        "authors": "Marek Adamowicz, Janusz Szewczyk",
        "published": "2020-10-24",
        "citations": 13,
        "abstract": "The design of rolling stock plays a key role in the attractiveness of the rail transport. Train design must strictly meet the requirements of rail operators to ensure high quality and cost-effective services. Semiconductor power devices made from silicon carbide (SiC) have reached a level of technology enabling their widespread use in traction power converters. SiC transistors offering energy savings, quieter operation, improved reliability and reduced maintenance costs have become the choice for the next-generation railway power converters and are quickly replacing the IGBT technology which has been used for decades. The paper describes the design and development of a novel SiC-based DC power electronic traction transformer (PETT) intended for electric multiple units (EMUs) operated in 3 kV DC rail traction. The details related to the 0.5 MVA peak power medium voltage prototype, including the electrical design of the main building blocks are presented in the first part of the paper. The second part deals with the implementation of the developed SiC-based DC PETT into a regional train operating on a 3 kV DC traction system. The experimental results obtained during the testing are presented to demonstrate the performance of the developed 3 kV DC PETT prototype.",
        "link": "http://dx.doi.org/10.3390/en13215573"
    },
    {
        "id": 24198,
        "title": "Decision letter for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2020-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v3/decision1"
    },
    {
        "id": 24199,
        "title": "Multi-scale spatial-temporal transformer for 3D human pose estimation",
        "authors": "Yongpeng Wu, Junna Gao",
        "published": "2021-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icvisp54630.2021.00051"
    },
    {
        "id": 24200,
        "title": "Visual Saliency Transformer",
        "authors": "Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, Junwei Han",
        "published": "2021-10",
        "citations": 178,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00468"
    },
    {
        "id": 24201,
        "title": "Moftransformer: a Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-organic Frameworks",
        "authors": "Yeonghun Kang, Hyunsoo Park, Berend Smit, Jihan Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million hypothetical MOFs. The multi-modal model uses an integrated atom-based graph and energy-grid embeddings to capture both the local and global features of the MOFs, respectively. By fine-tuning the pre-trained model with small datasets (from 5,000 to 20,000), our model outperforms all other machine learning models across various properties that include gas adsorption, diffusion, electronic properties, and even text mined data. Beyond its universal transfer learning capabilities, MOFTransformer generates chemical insight by analyzing feature importance from attention scores within the self-attention layers. As such, this model can serve as a bedrock platform for other MOF researchers that seek to develop new machine learning models for their work.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2201064/v1"
    },
    {
        "id": 24202,
        "title": "PCB Defect Detection Method Based on Transformer-YOLO",
        "authors": "Wei Chen, Zhongtian Huang, Qian Mu, Yi Sun",
        "published": "2022",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3228206"
    },
    {
        "id": 24203,
        "title": "ST-ASDNET: A BLSTM-FCN-Transformer basedASD Classification Model for Time-series fMRI",
        "authors": "Xin Deng, Lianhua Zhang, Rui Liu, Zhuofeng Chen, Bin Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutism spectrum disorder (ASD) is a neurodevelopmental disorder that causesrepetitive and stereotyped behavior and social dysfunction. Early diagnosis andintervention can improve the effectiveness of treatment. However, the currentdiagnostic methods of ASD still use the subjective symptom criteria of clinicalobservation, which is time-consuming and labor-intensive. In recent years, theemergence of functional magnetic resonance imaging (fMRI) neuroimaging techniques has facilitated the identification of potential biomarkers for the diagnosisof ASD. In this study, we developed a deep learning framework called Spatio-Temporal Autism Spectrum Disorder Network (ST-ASDNet) to distinguish ASDsfrom normal controls (NCs) as well as to distinguish ASD subtypes based onthe fMRI data. Specifically, two modules: 1) bidirectional long and short-termmemory Transformer (BLSTM-Transformer) and 2) full convolutional networkTransformer (FCN-Transformer) are proposed to obtain the spatial and the temporal characteristics of the fMRI data, respectively. Additionally, we explored theinfluence of different brain atlas for preprocessing the fMRI data to recognize theASDs. The study verifies that a more detailed division of brain regions is helpfulfor the recognition of ASDs. The proposed model is evaluated on the Craddock200 (CC200) atlas and the Automated Anatomical Labeling (AAL) atlas with the robust accuracy of 70.26% and 67.80%, respectively. The effect of the proposed model is competitive in diagnosing ASD compared with other methods,which could provide a useful and effective way to assist the clinical diagnosis.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3579465/v1"
    },
    {
        "id": 24204,
        "title": "Intention-aware Transformer with Adaptive Social and Temporal Learning for Vehicle Trajectory Prediction",
        "authors": "Yu Hu, Xiaobo Chen",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956216"
    },
    {
        "id": 24205,
        "title": "Improving Transformer-Based Speech Recognition with Unsupervised Pre-Training and Multi-Task Semantic Knowledge Learning",
        "authors": "Song Li, Lin Li, Qingyang Hong, Lingling Liu",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2007"
    },
    {
        "id": 24206,
        "title": "Study of Different Transformer based Networks For Glaucoma Detection",
        "authors": "Siddhartha Mallick, Jayanta Paul, Nandita Sengupta, Jaya Sil",
        "published": "2022-11-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon55691.2022.9977730"
    },
    {
        "id": 24207,
        "title": "Smart Transformer Based Meshed Hybrid Microgrid with MVDC Interconnection",
        "authors": "V.M. Hrishikesan, Chandan Kumar",
        "published": "2020-10-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon43393.2020.9255284"
    },
    {
        "id": 24208,
        "title": "Depthformer: Multiscale Vision Transformer for Monocular Depth Estimation with Global Local Information Fusion",
        "authors": "Ashutosh Agarwal, Chetan Arora",
        "published": "2022-10-16",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897187"
    },
    {
        "id": 24209,
        "title": "La Voie est Libre, une utopie citoyenne pour transformer l’espace public",
        "authors": "Cathy Lamri, Clément Girard",
        "published": "2019-11-27",
        "citations": 0,
        "abstract": "À l’heure du réchauffement climatique et de la crise de la démocratie, les citoyens peuvent avoir plusieurs temps d’avance sur leurs politiques. Cet article est le récit de La Voie est Libre, une utopie devenue réalité entre 2009 à 2015 sur une autoroute à Montreuil (93), exercice atypique de réappropriation de l’espace public, raconté par deux de ses initiateurs.",
        "link": "http://dx.doi.org/10.3917/turb.135.0022"
    },
    {
        "id": 24210,
        "title": "PointTrans: Rethinking 3D Object Detection from a Translation Perspective with Transformer",
        "authors": "Jingyang Liu, Yucheng Xu, Wanbiao Lin, Lei Sun",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240559"
    },
    {
        "id": 24211,
        "title": "Dynamic Multi-Scale Network for Dual-Pixel Images Defocus Deblurring with Transformer",
        "authors": "Dafeng Zhang, Xiaobing Wang",
        "published": "2022-7-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859631"
    },
    {
        "id": 24212,
        "title": "A Hybrid Visual Transformer for Efficient Deep Human Activity Recognition",
        "authors": "Youcef Djenouri, Ahmed Nabil Belbachir",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00080"
    },
    {
        "id": 24213,
        "title": "Vegetable Oil based Bio-lubricants and Transformer Fluids",
        "authors": "Dhorali Gnanasekaran, Venkata Prasad Chavidi",
        "published": "2018",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-4870-8"
    },
    {
        "id": 24214,
        "title": "Temperature Characteristics of Oil-Immersed Transformer Based on Fiber Bragg Grating Temperature Sensing Technology",
        "authors": "Chuan Luo, Zizhou Li, Bo Yang, Zhengang Zhao, Chuan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740637"
    },
    {
        "id": 24215,
        "title": "Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography",
        "authors": "Stephane Cuenat, Raphael Couturier",
        "published": "2022-3-18",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790134"
    },
    {
        "id": 24216,
        "title": "Evaluation of as-installed properties of transformer bushings",
        "authors": "Nicholas D. Oliveto, Andrei M. Reinhorn",
        "published": "2018-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engstruct.2018.01.064"
    },
    {
        "id": 24217,
        "title": "Local Consensus Transformer for Correspondence Learning",
        "authors": "Gang Wang, Yufei Chen",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00201"
    },
    {
        "id": 24218,
        "title": "HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud",
        "authors": "Zhixing Hou, Yan Yan, Chengzhong Xu, Hui Kong",
        "published": "2022-5-23",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811737"
    },
    {
        "id": 24219,
        "title": "Transformer Incipient Fault Detection Technique Based on Neural Network",
        "authors": "Gideon Dadzie, Emmanuel Asuming Frimpong, Caleb Myers Allotey, Eunice Lois Boateng",
        "published": "2020-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica49420.2020.9219948"
    },
    {
        "id": 24220,
        "title": "MLFT-Net: Point Cloud Completion Using Multi-Level Feature Transformer",
        "authors": "Yueling Du, Jin Xie",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscsic57216.2022.00042"
    },
    {
        "id": 24221,
        "title": "Improved Transformer Net for Hyperspectral Image Classification",
        "authors": "Yuhao Qing, Wenyi Liu, Liuyan Feng, Wanjia Gao",
        "published": "2021-6-5",
        "citations": 97,
        "abstract": "In recent years, deep learning has been successfully applied to hyperspectral image classification (HSI) problems, with several convolutional neural network (CNN) based models achieving an appealing classification performance. However, due to the multi-band nature and the data redundancy of the hyperspectral data, the CNN model underperforms in such a continuous data domain. Thus, in this article, we propose an end-to-end transformer model entitled SAT Net that is appropriate for HSI classification and relies on the self-attention mechanism. The proposed model uses the spectral attention mechanism and the self-attention mechanism to extract the spectral–spatial features of the HSI image, respectively. Initially, the original HSI data are remapped into multiple vectors containing a series of planar 2D patches after passing through the spectral attention module. On each vector, we perform linear transformation compression to obtain the sequence vector length. During this process, we add the position–coding vector and the learnable–embedding vector to manage capturing the continuous spectrum relationship in the HSI at a long distance. Then, we employ several multiple multi-head self-attention modules to extract the image features and complete the proposed network with a residual network structure to solve the gradient dispersion and over-fitting problems. Finally, we employ a multilayer perceptron for the HSI classification. We evaluate SAT Net on three publicly available hyperspectral datasets and challenge our classification performance against five current classification methods employing several metrics, i.e., overall and average classification accuracy and Kappa coefficient. Our trials demonstrate that SAT Net attains a competitive classification highlighting that a Self-Attention Transformer network and is appealing for HSI classification.",
        "link": "http://dx.doi.org/10.3390/rs13112216"
    },
    {
        "id": 24222,
        "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
        "authors": "Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, Zhengqi Wen",
        "published": "2020-10-25",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2086"
    },
    {
        "id": 24223,
        "title": "Multimodal Fusion for Human Action Recognition via Spatial Transformer",
        "authors": "Yaohui Sun, Weiyao Xu, Ju Gao, Xiaoyi Yu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327638"
    },
    {
        "id": 24224,
        "title": "Image transformer for explainable autonomous driving system",
        "authors": "Jiqian Dong, Sikai Chen, Shuya Zong, Tiantian Chen, Samuel Labi",
        "published": "2021-9-19",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9565103"
    },
    {
        "id": 24225,
        "title": "Channel-Spatial Transformer for Efficient Image Super-Resolution",
        "authors": "Jiuqiang Li, Shilei Zhu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446047"
    },
    {
        "id": 24226,
        "title": "Enhancing Parameter Efficiency in Model Inference using an Ultralight Inter-Transformer Linear Structure",
        "authors": "Haoxiang Shi, Tetsuya Sakai",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3378518"
    },
    {
        "id": 24227,
        "title": "Éloge de l’Extra-Terrestre",
        "authors": "Dominiq Jenvrey",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "Rejetée par la figure du terrestre, celle de l’extraterrestre est Moderne : elle est née d’une Fiction se voulant séparée du Réel. L’extraterrestre, pourtant, entraîne les humains à la rencontre avec des entités qu’ils méconnaissent ou ne connaissent pas encore. D’où la nécessité de relier la fiction au réel, en assumant sa puissance effective sur nos réalités par le biais de ce qu’on pourrait appeler une effiction . Cela nous permettrait de passer d’un régime d’imagination moderne à un régime d’imagination terrestre.",
        "link": "http://dx.doi.org/10.3917/mult.094.0238"
    },
    {
        "id": 24228,
        "title": "Surface tracking along the interphase barrier of a large transformer",
        "authors": "W. Thansiphraserth, P. L. Lewin",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceidp.2017.8257563"
    },
    {
        "id": 24229,
        "title": "Limitations of Partial Discharge De-noising of Power Transformer Using Adaptive Singular Value Decomposition",
        "authors": "Hossein Karami, Gevork B. Gharehpetian",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081523"
    },
    {
        "id": 24230,
        "title": "Comparison of Transformer and Convolutional Neural Network For   Multi-Sensor   Rolling Bearing Fault Diagnosis",
        "authors": "Xiang Gao, Xiangjun Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4044243"
    },
    {
        "id": 24231,
        "title": "Analysis of No-load Conditions of an Overexcited Transformer",
        "authors": "Anatolii G. Lavrov, Sergey V. Baklanov",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cts48763.2019.8973346"
    },
    {
        "id": 24232,
        "title": "Analysis of Deep Ensemble Transformer Model for Fake News Detection",
        "authors": "Rashmi Panda, Satya Kumari",
        "published": "2022-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsis55133.2022.9915941"
    },
    {
        "id": 24233,
        "title": "Overcoming Transformer Fine-Tuning process to improve Twitter Sentiment Analysis for Spanish Dialects",
        "authors": "Daniel Palomino",
        "published": "2020-12-12",
        "citations": 0,
        "abstract": "Is there an effective Spanish Sentiment Analysis algorithm? The aim of this paper is to answer this question. The task is challenging because there are several dialects for the Spanish Language. Thus, identically written words could have several meanings and polarities regarding Spanish speaking countries. To tackle this multidialect issue we rely on a transfer learning approach. To do so, we train a BERT language model to “transfer” general features of the Spanish language. Then, we fine-tune the language model to specific dialects. BERT is also used to generate contextual data augmentation aimed to prevent overfitting. Finally, we build the polarity classifier and propose a fine-tuning step using groups of layers. Our design choices allow us to achieve state-of-the-art results regarding multidialect benchmark datasets.",
        "link": "http://dx.doi.org/10.52591/lxai202012124"
    },
    {
        "id": 24234,
        "title": "Design of Capacitive Coupled Transformer for Power Converter Applications",
        "authors": "Necdet Kaan Onur, Gokturk Poyrazoglu",
        "published": "2021-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elma52514.2021.9502968"
    },
    {
        "id": 24235,
        "title": "Software Defect Prediction Method Based on Transformer Model",
        "authors": "Wei Zheng, Lijuan Tan, Chengbin Liu",
        "published": "2021-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica52286.2021.9498179"
    },
    {
        "id": 24236,
        "title": "Doubly-fed Solid State Auto-Transformer (SSAT) concept for Multi-pulse Rectifiers",
        "authors": "Farhana Islam, Harish S. Krishnamoorthy",
        "published": "2021-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce47101.2021.9595409"
    },
    {
        "id": 24237,
        "title": "Analysis of Fundamental Differences in Transformer 87T Differential Protection",
        "authors": "Mike Kockott, Zoran Gajic, Galina Antonova, Sergiu Paduraru",
        "published": "2021-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre48231.2021.9429724"
    },
    {
        "id": 24238,
        "title": "Correlation of Tangible Quality Parameters of Vegetable-Based Transformer Fluids",
        "authors": "Adango Miadonye, Mumuni Amadu, James Stephens, Thomas Okeefe",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4246485"
    },
    {
        "id": 24239,
        "title": "Energy Harvesting Using Piezo Transformer from Residual Fault Current 30 Ma",
        "authors": "Gurivi Reddy, Yogesh Rajwade",
        "published": "2022-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismsit56059.2022.9932816"
    },
    {
        "id": 24240,
        "title": "Advantages of interwinding capacitive test setup in FRA diagnostics of transformer windings",
        "authors": "Wojciech SZOKA",
        "published": "2018-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2018.07.10"
    },
    {
        "id": 24241,
        "title": "System of Predicting Dementia Using Transformer Based Ensemble Learning",
        "authors": "Kazu Nishikawa, Rin Hirakawa, Hideaki Kawano, Yoshihisa Nakatoh",
        "published": "2022-1-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce53296.2022.9730395"
    },
    {
        "id": 24242,
        "title": "Improved Research on the Transformer-Inductor Simulation Model of Magnetics",
        "authors": "Jiang Liyuan",
        "published": "2017-11-30",
        "citations": 0,
        "abstract": "Transformer-inductor simulation model notÂ only reflects the characteristics of magnetic path andÂ circuit, but also brings in magnetic components thatÂ reflected the parasitic capacitance. There are furtherÂ research, strict derivation and magnetic circuit equivalentÂ for the model in this article. Under the condition ofÂ considering hysteresis, saturation effect we can conclude aÂ new modeling and its equivalent, which can make theÂ magnetic curve and characteristic get better fitting. ItÂ shows that the transformer-inductance simulation model isÂ easy to spread and use.",
        "link": "http://dx.doi.org/10.26689/jera.v1i3.163"
    },
    {
        "id": 24243,
        "title": "Research on Network Intrusion Detection Based on Transformer",
        "authors": "Gang Gan, Weiju Kong",
        "published": "2023-5-4",
        "citations": 0,
        "abstract": "With the advancement of technology, the development of various industries has become inseparable from informatization. People's lives have become closely related to the network. While using the network to facilitate our lives, massive data is also generated. Traditional firewall technologies are no longer sufficient to meet current needs. Deep learning algorithms can establish complex mapping relationships between network data, and can extract hidden correlation features between data features to achieve data recognition and prediction. Therefore, this paper introduces Transformer and Bidirectional Long Short-Term Memory (BiLSTM) into the field of intrusion detection, and proposes an intrusion detection method based on the combination of Transformer-Encoder and BiLSTM (TBL). Deep Neural Networks (DNN) are used to further extract data features, and the softmax function is used to output classification results. In order to verify the effectiveness of this method, this paper trains and tests the TBL method on the NSL-KDD dataset, and verifies its feasibility and superiority.",
        "link": "http://dx.doi.org/10.54097/fcis.v3i3.7987"
    },
    {
        "id": 24244,
        "title": "Design of Magnetic Integrated LLC Transformer",
        "authors": "Liang Xu, Dong Qing Miao, Jing Qiu Zhang",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "The direct current/direct current (DC/DC) converter of the LLC (inductor-inductor-capacitor) converter is an important part of affecting the work efficiency, volume, and weight of the device. It not only has the functions of traditional transformers but is also able to solve the problems of traditional power transformers’ high price, huge volume, prodigious no-load loss, and inflexible control. This paper studies the DC/DC converter mainly, according to the given indexes, the magnetic integrated LLC resonant transformer is designed in detail. The magnetic integrated transformer greatly reduces the converter volume, and the selection of devices is completed based on parameters design. In addition, according to design parameters, losses and the efficiency of the LLC resonant transformer are calculated. The results meet the efficiency requirements. A test platform of a full-bridge LLC resonant converter is built according to theoretical research. The correctness and effectiveness of theoretical research and design methods of the DC/DC converter are verified by analyzing the experimental waveforms.",
        "link": "http://dx.doi.org/10.26689/ssr.v5i6.5079"
    },
    {
        "id": 24245,
        "title": "Recurrent Attention for the Transformer",
        "authors": "Jan Rosendahl, Christian Herold, Frithjof Petrick, Hermann Ney",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.insights-1.10"
    },
    {
        "id": 24246,
        "title": "DESIGN AND TESTS OF TRANSFORMER RESISTANCE SPOT WELDER",
        "authors": "Aleksander Smolarski",
        "published": "2021-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/13.2021.12.5"
    },
    {
        "id": 24247,
        "title": "Transformer Component Recognition in Pictures Taken by Submersible Robots",
        "authors": "Yingjie Yan, Yadong Liu, Zhicheng Xie, Jun Deng",
        "published": "2021-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec53008.2021.9735767"
    },
    {
        "id": 24248,
        "title": "Advanced exergy analysis applied to a single-stage heat transformer",
        "authors": "D. Colorado",
        "published": "2017-4",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.applthermaleng.2017.01.109"
    },
    {
        "id": 24249,
        "title": "Multi Criteria Decision Making Approach for Interpretation of Transformer Dissolved Gas Analysis",
        "authors": "Madavan R, Saroja S, Haseena S",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4235043"
    },
    {
        "id": 24250,
        "title": "Dielectric Response Model for Transformer Insulation Using Frequency Domain Spectroscopy and Vector Fitting",
        "authors": "Giovanni Hernandez, Abner Ramirez",
        "published": "2022-4-5",
        "citations": 6,
        "abstract": "This paper proposes a rational approximation-based approach to find positive real parameters for the extended Debye model (EDM), aimed at condition assessment of insulation systems of power transformers. The EDM can model the slow and fast polarization phenomenon, including relaxation mechanisms with different relaxation times within a composite dielectric material. In the proposed approach, the complex permittivity of the transformer’s composite insulation is approximated via rational functions, as given by the vector fitting (VF) software tool, and the EDM parameters are identified from the obtained poles/residues. To guarantee positive real parameters, i.e., a physically realizable circuit, VF is internally modified to calculate the final residues of the rational approximation via a constrained linear least-squares problem without resorting to further post-processing algorithms, as in existing methods, hence without affecting fitting accuracy. The effectiveness of the parametrized EDM is demonstrated in two ways: (a) by reconstructing frequency domain spectroscopy (FDS) curves provided via measurements in new oil-immersed power transformers and (b) by the comparison of the calculated polarization current given by EDM versus real measurements in time domain. The achieved fitting accuracy in most of the cases is above 99 percent for the reconstructed FDS curves, while the polarization current waveform is reproduced with good agreement.",
        "link": "http://dx.doi.org/10.3390/en15072655"
    },
    {
        "id": 24251,
        "title": "A Series-connected 18-pulse Rectifier Using Isolated Transformer",
        "authors": "Quanhui Li, Fangang Meng, Huaqiang Zhang",
        "published": "2019-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8922339"
    },
    {
        "id": 24252,
        "title": "Standardized survey of transformer reliability: On behalf of CIGRE WG A2.37",
        "authors": "S. Tenbohlen, J. Jagers, F. Vahidi",
        "published": "2017-9",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iseim.2017.8166559"
    },
    {
        "id": 24253,
        "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Yves Scherrer, Jörg Tiedemann",
        "published": "2020",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.49"
    },
    {
        "id": 24254,
        "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition",
        "authors": "Wenyong Huang, Wenchao Hu, Yu Ting Yeung, Xiao Chen",
        "published": "2020-10-25",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2361"
    },
    {
        "id": 24255,
        "title": "High-Frequency Transformer Design for Smart Microgrids",
        "authors": "M N Nachappa, Ajay Kumar, Deeksha Choudhary",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/peeic59336.2023.10450384"
    },
    {
        "id": 24256,
        "title": "PointCMT: An MLP-Transformer Network for Contrastive Learning of Point Representation",
        "authors": "Chuyu Wang, Xianfeng Han, Guoqiang Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191174"
    },
    {
        "id": 24257,
        "title": "Coreless Transformer based High Voltage Generator for Intense Magnetic Field Applications",
        "authors": "Saijun Mao, Jan Abraham Ferreira",
        "published": "2020-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec39645.2020.9124165"
    },
    {
        "id": 24258,
        "title": "Increasing the Efficiency of Power Electronics Application at Transformer Substations",
        "authors": "V.S. Klimash, A.M. Konstantinov",
        "published": "2020-10-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271413"
    },
    {
        "id": 24259,
        "title": "A Single-Phase Transformer-less Grid Connected Photovoltaic Inverter",
        "authors": "Asmita M. Gaikwad, Shailendra K. Mittal",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icisc47916.2020.9171090"
    },
    {
        "id": 24260,
        "title": "Power Inter Cell Transformer Modelling for ASV Application",
        "authors": "Guillaume Pellecuer, Thierry Martire, Loic Daridon",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon49645.2022.9968513"
    },
    {
        "id": 24261,
        "title": "Development of retrofit MV/LV transformer designs to accommodate increased electrification",
        "authors": "C. Power, A. Walsh",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.0656"
    },
    {
        "id": 24262,
        "title": "Transformer Neural Network for Structure Constrained Molecular Optimization",
        "authors": "Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, eva nittinger, Christian Tyrchan, Werngard Czechtizky",
        "published": "No Date",
        "citations": 1,
        "abstract": "Finding molecules with a desirable balance of multiple properties is a main challenge in drug discovery. Here, we focus on the task of molecular optimization, where a starting molecule with promising properties needs to be further optimized towards the desirable properties. Typically, chemists would apply chemical transformations to the starting molecule based on their intuition. A widely used strategy is the concept of matched molecular pairs where two molecules differ by a single transformation. In particular, a chemist would be interested in keeping one part of the starting molecule (core) constant, while substituting the other part (R-group), to optimize the starting molecule towards desirable properties. Motivated by this, we train a Transformer model, Transformer-R, to generate R-groups given the starting molecule (with its core and R-group specified) and the specified desirable properties. The generated R-groups will be attached to the core to form the final molecules, which are guaranteed to keep the core of interest and are expected to satisfy the desirable properties in the input. Our model could accelerate the process of optimizing antiviral drug candidates in terms of various properties of interest, e.g. pharmacokinetics.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14416133"
    },
    {
        "id": 24263,
        "title": "DEVTrV2: Enhanced Data-Efficient Video Transformer For Violence Detection",
        "authors": "Almamon Rasool Abdali, Ammar Abdullah Aggar",
        "published": "2022-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icivc55077.2022.9886172"
    },
    {
        "id": 24264,
        "title": "Help Transformer Improve Performance in Automatic Mathematics Word Problem-Solving",
        "authors": "Dong Liu, Guanfang Wang, Jialiang Yang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3220777"
    },
    {
        "id": 24265,
        "title": "A Transformer-based Cascade Network with Boundary Enhancement Loss for Retinal Vessel Segmentation",
        "authors": "Binke Cai, Liyan Ma",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956483"
    },
    {
        "id": 24266,
        "title": "Traffic Prediction Based on Spatiotemporal-Guided Multi Graph Sandwich-Transformer",
        "authors": "Yanjie Wen, zhihong li, Xiaoyu Wang, Wangtu Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4206935"
    },
    {
        "id": 24267,
        "title": "The Evaluation of Distribution Transformer in PEA using CBRM",
        "authors": "P. Rodkumnerd, K. Hongesombut",
        "published": "2019-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gtdasia.2019.8715997"
    },
    {
        "id": 24268,
        "title": "Biodegradability Testing of Transformer Oils: Review",
        "authors": "Ahmad Abualasal, István Kiss",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cando-epe60507.2023.10418015"
    },
    {
        "id": 24269,
        "title": "Analysis and mitigation of interaction between transformer inrush current and HVDC operation",
        "authors": "Y.Z. Zhang, C. M. Bush",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8273789"
    },
    {
        "id": 24270,
        "title": "Image Aesthetic Description Based on Semantic Addition Transformer Model",
        "authors": "",
        "published": "2021-10",
        "citations": 0,
        "abstract": "Image aesthetic quality assessment has been a hot research topic in the field of image analysis during the last decade. Most recently, people have proposed comment type assessment to describe the aesthetics of an image using text automatically. However, existing works have rarely considered the quality of the aesthetic description. In this work, we propose a novel neural image aesthetic description network framework, named Deep Image Aesthetic Reviewer (DIAReviewer), based on Semantic Addition Transformer Model, the learning of Residual Network, and the Attention Mechanism in a single framework. Beyond that, we design a Semantic Addition module to compromise the image feature and semantic information to focus on the comment quality, such as fluency and complexity. We introduce a new image dataset named Aesthetic Review Dataset (ARD), which contains one or more aesthetic comments for each image. Finally, the experimental results on ARD show that our model outperforms other methods in content complexity and sentence fluency of aesthetic descriptions.",
        "link": "http://dx.doi.org/10.4018/ijcini.20211001oa01"
    },
    {
        "id": 24271,
        "title": "4.4 The Physics of the Hilbert Transformer",
        "authors": "",
        "published": "2024-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780691235325-023"
    },
    {
        "id": 24272,
        "title": "CALCULATION OF MECHANICAL RELIABILITY OF TRANSFORMER HOUSING",
        "authors": "Vladimir Dmitrievich Lebedev, Stanislav Fedorovich Smirnov, Vladimir Viktorovich Terentiev",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35523/2307-5872-2021-36-3-63-69"
    },
    {
        "id": 24273,
        "title": "Transformer-based Automatic Mapping of Clinical Notes to Specific Clinical Concepts",
        "authors": "Jay Ganesh, Ajay Bansal",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00080"
    },
    {
        "id": 24274,
        "title": "IC-CViT: Inverse-Consistent Convolutional Vision Transformer for Diffeomorphic Image Registration",
        "authors": "Tao Xu, Ting Jiang, Xiaoning Li",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191209"
    },
    {
        "id": 24275,
        "title": "Improving Medical Transformer Design",
        "authors": "Yoganand Velayutham",
        "published": "2021-11",
        "citations": 0,
        "abstract": " Design engineer, YOGANAND VELAYUTHAM, looks at how to improve the design of medical grade transformers ",
        "link": "http://dx.doi.org/10.12968/s0047-9624(22)60565-0"
    },
    {
        "id": 24276,
        "title": "Digital Differential Protection of the «Generator-Transformer» Block",
        "authors": "N.S. Buryanina, R.O. Gogolev, Y.F. Korolyuk, E.V. Lesnykh, K.V. Suslov",
        "published": "2019-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eastconf.2019.8725341"
    },
    {
        "id": 24277,
        "title": "ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective",
        "authors": "Sultan Alrowili, Vijay Shanker",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.108"
    },
    {
        "id": 24278,
        "title": "TransKP: Transformer based Key-Phrase Extraction",
        "authors": "Mukund Rungta, Rishabh Kumar, Mehak Preet Dhaliwal, Hemant Tiwari, Vanraj Vala",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206812"
    },
    {
        "id": 24279,
        "title": "Experimental investigation on stability and dielectric break down strength of transformer oil based nanofluids",
        "authors": "Ravi Babu S., Sambasiva Rao G.",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5032037"
    },
    {
        "id": 24280,
        "title": "UCViT: Hardware-Friendly Vision Transformer via Unified Compression",
        "authors": "HongRui Song, Ya Wang, Meiqi Wang, Zhongfeng Wang",
        "published": "2022-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas48785.2022.9937660"
    },
    {
        "id": 24281,
        "title": "Bev-Cfkt: A Lidar-Camera Cross-Modality-Interaction Fusion and Knowledge Transfer Framework with Transformer for Bev 3d Object Detection",
        "authors": "Ming Wei, Junguo Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4559725"
    },
    {
        "id": 24282,
        "title": "The Design of Wireless Power Transformer for Electronic Flower Pot",
        "authors": "Chia-Yang Liu, Chih-Lung Hsiao",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce46687.2019.9015476"
    },
    {
        "id": 24283,
        "title": "An Interpretable and Transferrable Vision Transformer Model for Rapid Materials Spectra Classification",
        "authors": "Zhenru Chen, Yunchao Xie, Yuchao Wu, Yuyi Lin, Shigetaka Tomiya, Jian Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Rapid analysis of materials characterization spectra is pivotal for preventing accumulation of unwieldy datasets, thus accelerating subsequent decision-making. However, current methods heavily rely on experience and domain knowledge, which not only proves tedious but also is hard to keep up with the pace of data acquisition. In this context, we introduce a transferable Vision Transformer (ViT) model for identification of materials from their spectra, including XRD and FTIR. First, an optimal ViT model was trained to predict metal organic frameworks (MOFs) from their XRD spectra. It attains prediction accuracies of 70%, 93%, and 94.9% for Top-1, Top-3, and Top-5, respectively, and a shorter training time of 269 seconds in comparison to a convolutional neural network model. The dimension reduction and attention weight map underline its adeptness at capturing relevant features in the XRD spectra for determining the prediction outcome. Moreover, the model can be transferred to a new one for prediction of organic molecules from their FTIR spectra, attaining remarkable Top-1, Top-3, and Top-5 prediction accuracies of 84%, 94.1%, and 96.7%, respectively. The introduced ViT based model would set a new revenue to handling diverse types of spectroscopic data, thus expediting the materials characterization processes.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-fhhr4"
    },
    {
        "id": 24284,
        "title": "A Transformer-based Trajectory Prediction Method",
        "authors": "Zhenggui Xiang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3570361.3615733"
    },
    {
        "id": 24285,
        "title": "Enhancing Cervical Pre-Cancerous Classification Using Advanced Vision Transformer",
        "authors": "Manal Darwish, Mohamad Ziad Altabel, Rahib H. Abiyev",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "One of the most common types of cancer among in women is cervical cancer. Incidence and fatality rates are steadily rising, particularly in developing nations, due to a lack of screening facilities, experienced specialists, and public awareness. Visual inspection is used to screen for cervical cancer after the application of acetic acid (VIA), histopathology test, Papanicolaou (Pap) test, and human papillomavirus (HPV) test. The goal of this research is to employ a vision transformer (ViT) enhanced with shifted patch tokenization (SPT) techniques to create an integrated and robust system for automatic cervix-type identification. A vision transformer enhanced with shifted patch tokenization is used in this work to learn the distinct features between the three different cervical pre-cancerous types. The model was trained and tested on 8215 colposcopy images of the three types, obtained from the publicly available mobile-ODT dataset. The model was tested on 30% of the whole dataset and it showed a good generalization capability of 91% accuracy. The state-of-the art comparison indicated the outperformance of our model. The experimental results show that the suggested system can be employed as a decision support tool in the detection of the cervical pre-cancer transformation zone, particularly in low-resource settings with limited experience and resources.",
        "link": "http://dx.doi.org/10.3390/diagnostics13182884"
    },
    {
        "id": 24286,
        "title": "Arabic News Summarization based on T5 Transformer Approach",
        "authors": "Qusai Ismail, Kefah Alissa, Rehab M. Duwairi",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icics60529.2023.10330509"
    },
    {
        "id": 24287,
        "title": "A Block Cipher Recognition Scheme Based on Deep Learning Transformer Algorithm",
        "authors": "Ke Yuan, Bowen Zhang, Yuwei Zhou, Hanlin Sun, Wei Yang, Chunfu Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCryptographic algorithms are the core of data encryption, and their identification is a prerequisite for in-depth analysis of cryptography. Cryptographic algorithm identification is the process of distinguishing or identifying encryption methods by analyzing potentially distinctive information in the ciphertext when the ciphertext is known. It serves as the foundation for cryptanalysis work. The machine learning-based approach for identifying cryptographic algorithms extracts ciphertext features and trains the machine learning algorithm to build a cryptographic system identification classifier. It has the characteristics of high accuracy, concise operation process, and strong practicability. The ciphertext's complexity and the inter-data interference both rise with the number of cryptographic algorithms. This will lead to a reduction in the recognition rate of traditional machine learning cryptographic algorithm recognition solutions, poor recognition stability, and huge challenges to recognition capabilities. Deep learning algorithms have the characteristics of strong learning ability, a large amount of data processing, wide-coverage, good adaptability, and portability, etc., and have become a hot spot in the realm of cryptographic algorithm identification. In this work, a cryptographic algorithm identification methodology based on the Transformer algorithm is proposed, and the ciphertext feature extraction approach in the NIST randomness test is improved. In the research on cryptographic algorithm identification, we selected five block cipher algorithms: AES, 3DES, Blowfish, CAST, and RC2 as the research objects, and conducted two-class and five-class recognition experiments. According to experimental data, the approach suggested in this article has superior accuracy and stability than the classic machine learning model when the ciphertext size and other experimental variables are the same. Among them, on ciphertext files ranging from 1kb to 512kb, compared to the accuracy of the conventional classic machine learning algorithms SVM, GNB, KNN, RF, and LR, the average recognition accuracy of the two categories is 0.855, which is 29% higher. 33.9%, 28.5%, 34.2%, and 29.9%. Compared with the MLP accuracy of the same deep learning algorithm, the accuracy is also improved by 9%. When conducting the five-category experiment, the recognition accuracy of the scheme put forward in this piece is not less than 0.4. When the ciphertext file is 512KB, the recognition accuracy is as high as 0.42, and the average recognition accuracy of the five categories is 0.412. It is far greater than the other five classic machine learning algorithms and significantly better than the 20% random classification accuracy. Other algorithms' recognition rates fluctuate as ciphertext file sizes change. Among them, the Transformer algorithm has the smallest fluctuation and the best overall effect.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3920602/v1"
    },
    {
        "id": 24288,
        "title": "Style-Guided Inference of Transformer for High-resolution Image Synthesis",
        "authors": "Jonghwa Yim, Minjae Kim",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00179"
    },
    {
        "id": 24289,
        "title": "A study of transformer connections to improve power quality in traction application",
        "authors": "Sandeep Kumar Chowdhury, Peeyoosh Gupta",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/piicon56320.2022.10045155"
    },
    {
        "id": 24290,
        "title": "Determining Power Transformer Maintenance Plan Using Three-Dimensional Risk Matrix",
        "authors": "Phoumsavath Souvannalath, Suttichai Premrudeepreechacharn, Kanchit Ngamsanroaj",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powercon53406.2022.9929632"
    },
    {
        "id": 24291,
        "title": "Features of Modeling of Microprocessor Protection of a Transformer",
        "authors": "O. N. Kuzyakov, E. P. Vlasova",
        "published": "2023-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icieam57311.2023.10138957"
    },
    {
        "id": 24292,
        "title": "Design of simplifying modeling of stacked iron core transformer",
        "authors": "Yi Feng",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3010828"
    },
    {
        "id": 24293,
        "title": "Transformer la formation des aides-soignants, un enjeu majeur pour l’avenir",
        "authors": "Alexandre Niggel",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aidsoi.2023.10.004"
    },
    {
        "id": 24294,
        "title": "Retracted: 3D Human Pose Estimation Based on Transformer Algorithm",
        "authors": "",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1155/2023/9813246"
    },
    {
        "id": 24295,
        "title": "A novel llc resonant dc-dc converter with integrated transformer",
        "authors": "Shota Kimura, Kimihiro Nanamori, Mostafa Noah, Masayoshi Yamamoto",
        "published": "2017-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intlec.2017.8214186"
    },
    {
        "id": 24296,
        "title": "Fashion Compatibility Learning Via Triplet-Swin Transformer",
        "authors": "Hosna Darvishi, Reza Azmi, Fatemeh Moradian, Maral Zarvani",
        "published": "2023-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csicc58665.2023.10105392"
    },
    {
        "id": 24297,
        "title": "Dissolved Gas Analysis Interpretation and Intelligent Machine Learning Techniques",
        "authors": "",
        "published": "2017-8-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119239970.ch4"
    },
    {
        "id": 24298,
        "title": "Transformer-Based Monocular Depth Estimation Using Token Attention",
        "authors": "Zhiyong Huo, Yihang Chen, Junyu Wei, Quan Guo, Yuankai Huo",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4248658"
    },
    {
        "id": 24299,
        "title": "Building Blocks for a Complex-Valued Transformer Architecture",
        "authors": "Florian Eilers, Xiaoyi Jiang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095349"
    },
    {
        "id": 24300,
        "title": "Topological Overview on Solid-state Transformer Traction Technology in High-speed Trains",
        "authors": "Deepak Ronanki, Sheldon S. Williamson",
        "published": "2018-6",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itec.2018.8450135"
    },
    {
        "id": 24301,
        "title": "ANovel Single Stage AC-AC Converter For Hybrid Solid State Transformer (HSST)",
        "authors": "Sanjay Rajendran, Alex Q. Huang",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce50734.2022.9947583"
    },
    {
        "id": 24302,
        "title": "Transformer inrush current mitigation concept for hybrid transformers",
        "authors": "J. Burkard, J. Biela",
        "published": "2017-9",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/epe17ecceeurope.2017.8099283"
    },
    {
        "id": 24303,
        "title": "Research on Switching Overvoltage of No-load Transformer with Long Cable",
        "authors": "Ren Hongtao",
        "published": "2020-9-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spies48661.2020.9243034"
    },
    {
        "id": 24304,
        "title": "Root Cause Analysis in Power Transformer Failure with Improved Intelligent Methods",
        "authors": "Sreelakshmi S Baiju, Adarsh S",
        "published": "2022-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iprecon55716.2022.10059538"
    },
    {
        "id": 24305,
        "title": "Improvement in Transformer Differential Protection Using Singular Value Decomposition",
        "authors": "Het Bhalja, Bhavesh R. Bhalja, Pramod Agarwal",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gtd49768.2023.00068"
    },
    {
        "id": 24306,
        "title": "Incorporer des objets",
        "authors": "Déborah Nourrit, Céline Rosselin-Bareille",
        "published": "2017-6-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/socio-anthropologie.2572"
    },
    {
        "id": 24307,
        "title": "Modeling method of transformer sensor communication model based on intelligent algorithm",
        "authors": "Chenying Yi, Yangjun Zhou",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ifeea60725.2023.10429634"
    },
    {
        "id": 24308,
        "title": "Moftransformer: a Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-organic Frameworks",
        "authors": "Yeonghun Kang, Hyunsoo Park, Berend Smit, Jihan Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million hypothetical MOFs. The multi-modal model uses an integrated atom-based graph and energy-grid embeddings to capture both the local and global features of the MOFs, respectively. By fine-tuning the pre-trained model with small datasets (from 5,000 to 20,000), our model outperforms all other machine learning models across various properties that include gas adsorption, diffusion, electronic properties, and even text mined data. Beyond its universal transfer learning capabilities, MOFTransformer generates chemical insight by analyzing feature importance from attention scores within the self-attention layers. As such, this model can serve as a bedrock platform for other MOF researchers that seek to develop new machine learning models for their work.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2201064/v1"
    },
    {
        "id": 24309,
        "title": "PCB Defect Detection Method Based on Transformer-YOLO",
        "authors": "Wei Chen, Zhongtian Huang, Qian Mu, Yi Sun",
        "published": "2022",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3228206"
    },
    {
        "id": 24310,
        "title": "ST-ASDNET: A BLSTM-FCN-Transformer basedASD Classification Model for Time-series fMRI",
        "authors": "Xin Deng, Lianhua Zhang, Rui Liu, Zhuofeng Chen, Bin Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutism spectrum disorder (ASD) is a neurodevelopmental disorder that causesrepetitive and stereotyped behavior and social dysfunction. Early diagnosis andintervention can improve the effectiveness of treatment. However, the currentdiagnostic methods of ASD still use the subjective symptom criteria of clinicalobservation, which is time-consuming and labor-intensive. In recent years, theemergence of functional magnetic resonance imaging (fMRI) neuroimaging techniques has facilitated the identification of potential biomarkers for the diagnosisof ASD. In this study, we developed a deep learning framework called Spatio-Temporal Autism Spectrum Disorder Network (ST-ASDNet) to distinguish ASDsfrom normal controls (NCs) as well as to distinguish ASD subtypes based onthe fMRI data. Specifically, two modules: 1) bidirectional long and short-termmemory Transformer (BLSTM-Transformer) and 2) full convolutional networkTransformer (FCN-Transformer) are proposed to obtain the spatial and the temporal characteristics of the fMRI data, respectively. Additionally, we explored theinfluence of different brain atlas for preprocessing the fMRI data to recognize theASDs. The study verifies that a more detailed division of brain regions is helpfulfor the recognition of ASDs. The proposed model is evaluated on the Craddock200 (CC200) atlas and the Automated Anatomical Labeling (AAL) atlas with the robust accuracy of 70.26% and 67.80%, respectively. The effect of the proposed model is competitive in diagnosing ASD compared with other methods,which could provide a useful and effective way to assist the clinical diagnosis.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3579465/v1"
    },
    {
        "id": 24311,
        "title": "Intention-aware Transformer with Adaptive Social and Temporal Learning for Vehicle Trajectory Prediction",
        "authors": "Yu Hu, Xiaobo Chen",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956216"
    },
    {
        "id": 24312,
        "title": "Improving Transformer-Based Speech Recognition with Unsupervised Pre-Training and Multi-Task Semantic Knowledge Learning",
        "authors": "Song Li, Lin Li, Qingyang Hong, Lingling Liu",
        "published": "2020-10-25",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2007"
    },
    {
        "id": 24313,
        "title": "Study of Different Transformer based Networks For Glaucoma Detection",
        "authors": "Siddhartha Mallick, Jayanta Paul, Nandita Sengupta, Jaya Sil",
        "published": "2022-11-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon55691.2022.9977730"
    },
    {
        "id": 24314,
        "title": "Smart Transformer Based Meshed Hybrid Microgrid with MVDC Interconnection",
        "authors": "V.M. Hrishikesan, Chandan Kumar",
        "published": "2020-10-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon43393.2020.9255284"
    },
    {
        "id": 24315,
        "title": "Depthformer: Multiscale Vision Transformer for Monocular Depth Estimation with Global Local Information Fusion",
        "authors": "Ashutosh Agarwal, Chetan Arora",
        "published": "2022-10-16",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897187"
    },
    {
        "id": 24316,
        "title": "La Voie est Libre, une utopie citoyenne pour transformer l’espace public",
        "authors": "Cathy Lamri, Clément Girard",
        "published": "2019-11-27",
        "citations": 0,
        "abstract": "À l’heure du réchauffement climatique et de la crise de la démocratie, les citoyens peuvent avoir plusieurs temps d’avance sur leurs politiques. Cet article est le récit de La Voie est Libre, une utopie devenue réalité entre 2009 à 2015 sur une autoroute à Montreuil (93), exercice atypique de réappropriation de l’espace public, raconté par deux de ses initiateurs.",
        "link": "http://dx.doi.org/10.3917/turb.135.0022"
    },
    {
        "id": 24317,
        "title": "PointTrans: Rethinking 3D Object Detection from a Translation Perspective with Transformer",
        "authors": "Jingyang Liu, Yucheng Xu, Wanbiao Lin, Lei Sun",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240559"
    },
    {
        "id": 24318,
        "title": "Dynamic Multi-Scale Network for Dual-Pixel Images Defocus Deblurring with Transformer",
        "authors": "Dafeng Zhang, Xiaobing Wang",
        "published": "2022-7-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859631"
    },
    {
        "id": 24319,
        "title": "A Hybrid Visual Transformer for Efficient Deep Human Activity Recognition",
        "authors": "Youcef Djenouri, Ahmed Nabil Belbachir",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00080"
    },
    {
        "id": 24320,
        "title": "Vegetable Oil based Bio-lubricants and Transformer Fluids",
        "authors": "Dhorali Gnanasekaran, Venkata Prasad Chavidi",
        "published": "2018",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-4870-8"
    },
    {
        "id": 24321,
        "title": "Temperature Characteristics of Oil-Immersed Transformer Based on Fiber Bragg Grating Temperature Sensing Technology",
        "authors": "Chuan Luo, Zizhou Li, Bo Yang, Zhengang Zhao, Chuan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740637"
    },
    {
        "id": 24322,
        "title": "Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography",
        "authors": "Stephane Cuenat, Raphael Couturier",
        "published": "2022-3-18",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790134"
    },
    {
        "id": 24323,
        "title": "Evaluation of as-installed properties of transformer bushings",
        "authors": "Nicholas D. Oliveto, Andrei M. Reinhorn",
        "published": "2018-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engstruct.2018.01.064"
    },
    {
        "id": 24324,
        "title": "Local Consensus Transformer for Correspondence Learning",
        "authors": "Gang Wang, Yufei Chen",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00201"
    },
    {
        "id": 24325,
        "title": "HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud",
        "authors": "Zhixing Hou, Yan Yan, Chengzhong Xu, Hui Kong",
        "published": "2022-5-23",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811737"
    },
    {
        "id": 24326,
        "title": "Transformer Incipient Fault Detection Technique Based on Neural Network",
        "authors": "Gideon Dadzie, Emmanuel Asuming Frimpong, Caleb Myers Allotey, Eunice Lois Boateng",
        "published": "2020-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica49420.2020.9219948"
    },
    {
        "id": 24327,
        "title": "MLFT-Net: Point Cloud Completion Using Multi-Level Feature Transformer",
        "authors": "Yueling Du, Jin Xie",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscsic57216.2022.00042"
    },
    {
        "id": 24328,
        "title": "Improved Transformer Net for Hyperspectral Image Classification",
        "authors": "Yuhao Qing, Wenyi Liu, Liuyan Feng, Wanjia Gao",
        "published": "2021-6-5",
        "citations": 97,
        "abstract": "In recent years, deep learning has been successfully applied to hyperspectral image classification (HSI) problems, with several convolutional neural network (CNN) based models achieving an appealing classification performance. However, due to the multi-band nature and the data redundancy of the hyperspectral data, the CNN model underperforms in such a continuous data domain. Thus, in this article, we propose an end-to-end transformer model entitled SAT Net that is appropriate for HSI classification and relies on the self-attention mechanism. The proposed model uses the spectral attention mechanism and the self-attention mechanism to extract the spectral–spatial features of the HSI image, respectively. Initially, the original HSI data are remapped into multiple vectors containing a series of planar 2D patches after passing through the spectral attention module. On each vector, we perform linear transformation compression to obtain the sequence vector length. During this process, we add the position–coding vector and the learnable–embedding vector to manage capturing the continuous spectrum relationship in the HSI at a long distance. Then, we employ several multiple multi-head self-attention modules to extract the image features and complete the proposed network with a residual network structure to solve the gradient dispersion and over-fitting problems. Finally, we employ a multilayer perceptron for the HSI classification. We evaluate SAT Net on three publicly available hyperspectral datasets and challenge our classification performance against five current classification methods employing several metrics, i.e., overall and average classification accuracy and Kappa coefficient. Our trials demonstrate that SAT Net attains a competitive classification highlighting that a Self-Attention Transformer network and is appealing for HSI classification.",
        "link": "http://dx.doi.org/10.3390/rs13112216"
    },
    {
        "id": 24329,
        "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech Recognition",
        "authors": "Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, Zhengqi Wen",
        "published": "2020-10-25",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2086"
    },
    {
        "id": 24330,
        "title": "Multimodal Fusion for Human Action Recognition via Spatial Transformer",
        "authors": "Yaohui Sun, Weiyao Xu, Ju Gao, Xiaoyi Yu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327638"
    },
    {
        "id": 24331,
        "title": "Image transformer for explainable autonomous driving system",
        "authors": "Jiqian Dong, Sikai Chen, Shuya Zong, Tiantian Chen, Samuel Labi",
        "published": "2021-9-19",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9565103"
    },
    {
        "id": 24332,
        "title": "Channel-Spatial Transformer for Efficient Image Super-Resolution",
        "authors": "Jiuqiang Li, Shilei Zhu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446047"
    },
    {
        "id": 24333,
        "title": "Enhancing Parameter Efficiency in Model Inference using an Ultralight Inter-Transformer Linear Structure",
        "authors": "Haoxiang Shi, Tetsuya Sakai",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3378518"
    },
    {
        "id": 24334,
        "title": "Éloge de l’Extra-Terrestre",
        "authors": "Dominiq Jenvrey",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "Rejetée par la figure du terrestre, celle de l’extraterrestre est Moderne : elle est née d’une Fiction se voulant séparée du Réel. L’extraterrestre, pourtant, entraîne les humains à la rencontre avec des entités qu’ils méconnaissent ou ne connaissent pas encore. D’où la nécessité de relier la fiction au réel, en assumant sa puissance effective sur nos réalités par le biais de ce qu’on pourrait appeler une effiction . Cela nous permettrait de passer d’un régime d’imagination moderne à un régime d’imagination terrestre.",
        "link": "http://dx.doi.org/10.3917/mult.094.0238"
    },
    {
        "id": 24335,
        "title": "基于Transformer的跨年龄人脸识别方法",
        "authors": "刘成 Liu Cheng, 曹良才 Cao Liangcai, 靳业 Jin Ye, 王浩威 Wang Haowei, 殷松峰 Yin Songfeng",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/lop220785"
    },
    {
        "id": 24336,
        "title": "基于Vision Transformer的小儿肺炎辅助诊断",
        "authors": "赵爽 Zhao Shuang, 魏国辉 Wei Guohui, 赵文华 Zhao Wenhua, 马志庆 Ma Zhiqing",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/lop213019"
    },
    {
        "id": 24337,
        "title": "Histogram-Based Method to Avoid Maloperation of Transformer Differential Protection Due to Current-Transformer Saturation Under External Faults",
        "authors": "Tao Zheng, Ting Huang, Yulong Ma, Zihang Zhang, Lianguang Liu",
        "published": "2018-4",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpwrd.2017.2712806"
    },
    {
        "id": 24338,
        "title": "A CNN-Transformer Hybrid Model Based on CSWin Transformer for UAV Image Object Detection",
        "authors": "Wanjie Lu, Chaozhen Lan, Chaoyang Niu, Wei Liu, Liang Lyu, Qunshan Shi, Shiju Wang",
        "published": "2023",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jstars.2023.3234161"
    },
    {
        "id": 24339,
        "title": "Parameters of the Hysteresis Model of Transformer Steel Sheets",
        "authors": "Michal Sierzega, Zbigniew Szular, Witold Mazgaj",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wzee.2018.8749055"
    },
    {
        "id": 24340,
        "title": "Transformer-DARwIn: A Hybrid Locomotion Humanoid Designed to Walk or Roll",
        "authors": "Jean Chagas Vaz, Paul Y. Oh",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/urai.2018.8441840"
    },
    {
        "id": 24341,
        "title": "Design and Simulation of Electromagnetic Metamaterial Unit for High-frequency Transformer",
        "authors": "Wang Yingying, Wang Yuyang, Wu Shuai, Fu Weinong",
        "published": "2021-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intermag42984.2021.9579942"
    },
    {
        "id": 24342,
        "title": "Performance of Representative Transformer-less Topologies for Photovoltaic Applications",
        "authors": "Duc-Thanh Do, Holger Hirsch",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/emc/si/pi/emceurope52599.2021.9559286"
    },
    {
        "id": 24343,
        "title": "B-Cell Linear Epitope Prediction Using Transformer Encoder",
        "authors": "Youjin Kim, Jinhee Park, Junseok Kwon",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55196.2022.9952973"
    },
    {
        "id": 24344,
        "title": "Proportional Current Sampling and Integrated CT Design with Transformer",
        "authors": "Haijun Yang, Alpha J. Zhang",
        "published": "2022-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/peac56338.2022.9959099"
    },
    {
        "id": 24345,
        "title": "Double Linear Transformer for Background Music Generation from Videos",
        "authors": "Xueting Yang, Ying Yu, Xiaoyu Wu",
        "published": "2022-5-17",
        "citations": 0,
        "abstract": "Many music generation research works have achieved effective performance, while rarely combining music with given videos. We propose a model with two linear Transformers to generate background music according to a given video. To enhance the melodic quality of the generated music, we firstly input note-related and rhythm-related music features separately into each Transformer network. In particular, we pay attention to the connection and the independence of music features. Then, in order to generate the music that matches the given video, the current state-of-the-art cross-modal inference method is set up to establish the relationship between visual mode and sound mode. Subjective and objective experiment indicate that the generated background music matches the video well and is also melodious.",
        "link": "http://dx.doi.org/10.3390/app12105050"
    },
    {
        "id": 24346,
        "title": "Learning a spatial-temporal texture transformer network for video inpainting",
        "authors": "Pengsen Ma, Tao Xue",
        "published": "2022-10-13",
        "citations": 0,
        "abstract": "We study video inpainting, which aims to recover realistic textures from damaged frames. Recent progress has been made by taking other frames as references so that relevant textures can be transferred to damaged frames. However, existing video inpainting approaches neglect the ability of the model to extract information and reconstruct the content, resulting in the inability to reconstruct the textures that should be transferred accurately. In this paper, we propose a novel and effective spatial-temporal texture transformer network (STTTN) for video inpainting. STTTN consists of six closely related modules optimized for video inpainting tasks: feature similarity measure for more accurate frame pre-repair, an encoder with strong information extraction ability, embedding module for finding a correlation, coarse low-frequency feature transfer, refinement high-frequency feature transfer, and decoder with accurate content reconstruction ability. Such a design encourages joint feature learning across the input and reference frames. To demonstrate the advancedness and effectiveness of the proposed model, we conduct comprehensive ablation learning and qualitative and quantitative experiments on multiple datasets by using standard stationary masks and more realistic moving object masks. The excellent experimental results demonstrate the authenticity and reliability of the STTTN.",
        "link": "http://dx.doi.org/10.3389/fnbot.2022.1002453"
    },
    {
        "id": 24347,
        "title": "Abridged Methodology Development for Analyzing Transformer Winding During Lightning Surges",
        "authors": "Gaurav Dhiman, Grupesh Tapiawala, Ram Krishna Mishra",
        "published": "2018-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/poweri.2018.8704346"
    },
    {
        "id": 24348,
        "title": "BiVaSE: A bilingual variational sentence encoder with randomly initialized Transformer layers",
        "authors": "Bence Nyéki",
        "published": "2022-12-12",
        "citations": 0,
        "abstract": "AbstractTransformer-based NLP models have achieved state-of-the-art results in many NLP tasks including text classification and text generation. However, the layers of these models do not output any explicit representations for texts units larger than tokens (e.g. sentences), although such representations are required to perform text classification. Sentence encodings are usually obtained by applying a pooling technique during fine-tuning on a specific task. In this paper, a new sentence encoder is introduced. Relying on an autoencoder architecture, it was trained to learn sentence representations from the very beginning of its training. The model was trained on bilingual data with variational Bayesian inference. Sentence representations were evaluated in downstream and linguistic probing tasks. Although the newly introduced encoder generally performs worse than well-known Transformer-based encoders, the experiments show that it was able to learn to incorporate linguistic information in the sentence representations.",
        "link": "http://dx.doi.org/10.1556/2062.2022.00584"
    },
    {
        "id": 24349,
        "title": "Joint learning of images and videos with a single Vision Transformer",
        "authors": "Shuki Shimizu, Toru Tamaki",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10215661"
    },
    {
        "id": 24350,
        "title": "Enhancing Persian Text Summarization Using the mT5 Transformer Model:A Three-Phased Fine-Tuning Approach and Reinforcement Learning",
        "authors": "Vahid Nejad Mahmood Abadi, Fahimeh Ghasemian",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the contemporary era, grappling with the vast expanse of big data presents a formidable obstacle, particularly when it comes to extracting vital information from extensive textual sources. The constant influx of news articles from various agencies necessitates an enormous amount of time to digest comprehensively. A viable solution to address this challenge lies in the realm of automatic text summarization, which is a pivotal and intricate endeavor within the field of natural language processing. Text summarization involves transforming pertinent textual content into a concise format that reduces its word count without compromising its underlying meaning. In recent years, transformers have emerged as a prominent force in the landscape of natural language processing, particularly in the realm of text summarization. This research endeavors to harness the power of transformers by training the mT5-base model on a three-step fine-tuning phase on Persian news articles. Subsequently, reinforcement learning via the PPO algorithm is integrated with the fine-tuned model. Finally, we evaluate the model's performance in summarizing Persian texts, shedding light on its efficacy in addressing the formidable task of distilling meaningful insights from a sea of textual data. Our model has set a new benchmark in the field of Persian text summarization, achieving outstanding ROUGE scores of 53.17 for ROUGE-1, 37.12 for ROUGE-2, and 44.13 for ROUGE-L. These remarkable results reflect a significant advancement in the quality of Persian text summarization, signaling a promising era of more refined and context-aware summaries.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3682780/v1"
    },
    {
        "id": 24351,
        "title": "DESIGN FEATURES OF VOLTAGE TRANSFORMER FOR AIR CLEANING FILTERS",
        "authors": "L.E. Roginskaya, Z.I. Yalalova, Yu.V. Rakhmanova, A.S. Gorbunov",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46960/44170389_2021_26"
    },
    {
        "id": 24352,
        "title": "Research on Loss Reduction and Energy Saving Operation of Distribution Transformer and Line below 10kV",
        "authors": "",
        "published": "2021-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47939/et.v2i8.252"
    },
    {
        "id": 24353,
        "title": "A Helmholtz Resonator-Based Acoustic Metamaterial for Power Transformer Noise Control",
        "authors": "Naser Sharafkhani",
        "published": "2022-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s40857-021-00256-z"
    },
    {
        "id": 24354,
        "title": "Enhancing Ligand-Based Virtual Screening with 3D Shape Similarity via a Distance-Aware Transformer Model",
        "authors": "Manuel S. Sellner, Amr H. Mahmoud, Markus A. Lill",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractFollowing the assumption that chemically similar molecules exhibit similar biologcial properties, ligand-based virtual screening can be a valuable starting point in drug discovery projects. While 2D-based similarity metrics generally focus on similar scaffolds or substructures, 3D-based methods can capture the shape of a molecule, allowing for the identification of compounds with different scaffolds. We recently published a proof-of-concept study which demonstrated how a Transformer model can be adapted to preserve 2D similarities in latent space in the form of Euclidean distances. In this work, we extend this research and prove that the approach can be adapted to 3D similarities. We use pharmacophore-based shape similarity as 3D similarity measure. We show that the model is able to enrich the predicted most similar hits with compounds with different scaffolds that are indeed similar in 3D space. Whereas classical pharmacophore- or shape-based 3D similarity methods rely on expensive alignment processes, in our approach, we identify similar compounds directly by the Euclidean distances in latent space. This enables for the first time the 3D screening of ultra-large databases with high efficiency.",
        "link": "http://dx.doi.org/10.1101/2023.11.17.567506"
    },
    {
        "id": 24355,
        "title": "Detection of Online Sexism Using Lexical Features and Transformer",
        "authors": "Elizabeth Martinez, Juan Cuadrado, Juan Carlos Martinez-Santos, Edwin Puertas",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/c358072.2023.10436298"
    },
    {
        "id": 24356,
        "title": "A Study on Mixed Corn Oil with Mineral Oil as Dielectric Component for Power Transformer",
        "authors": "",
        "published": "2018-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pecon.2018.8684033"
    },
    {
        "id": 24357,
        "title": "SELF-EdiT: Structure-Constrained Molecular Optimization Using SELFIES Editing Transformer",
        "authors": "Shengmin ‍Piao, Jonghwan Choi, Sangmin Seo, Sanghyun Park",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4267816"
    },
    {
        "id": 24358,
        "title": "Transformer Based Multi-model Fusion for Medical Image Segmentation",
        "authors": "Bo Dong, Wenhai Wang, Jinpeng Li",
        "published": "2021-11-1",
        "citations": 0,
        "abstract": "We present our solutions to the MedAI for all three tasks: polyp segmentation task, instrument segmentation task, and transparency task. We use the same framework to process the two segmentation tasks of polyps and instruments. The key improvement over last year is new state-of-the-art vision architectures, especially transformers which significantly outperform ConvNets for the medical image segmentation tasks. Our solution consists of multiple segmentation models, and each model uses a transformer as the backbone network. we get the best IoU score of 0.915 on the instrument segmentation task and 0.836 on polyp segmentation task after submitting. Meanwhile, we provide complete solutions in https://github.com/dongbo811/MedAI-2021.",
        "link": "http://dx.doi.org/10.5617/nmi.9171"
    },
    {
        "id": 24359,
        "title": "Analysis of the Relationship between the Parameters of IPT Transformer and Power Electronic System",
        "authors": "Sampath Jayalath, Azeem Khan",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wpt.2018.8639141"
    },
    {
        "id": 24360,
        "title": "Attention-Based Transformer Thermal Infrared Tracker",
        "authors": "Changan Wei, Qiqi Li, Baoqin Zhang, Shouda Jiang",
        "published": "2023-8-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icemi59194.2023.10270671"
    },
    {
        "id": 24361,
        "title": "A Hybrid Transformer Approach for Chinese Ner with Features Augmentation",
        "authors": "Zhigang Jin, Xiaoyong He, Xiaodong Wu, Xiaofang Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4087645"
    },
    {
        "id": 24362,
        "title": "A Transformer-based Joint Prediction in Hybrid Mechanism with Kalman Filter for 3D-MOT",
        "authors": "Rahmad Sadli, soheyb Ribouh, Atika Rivenq, Abdenour Hadid, Abdelmalik Taleb Ahmed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a novel 3D MOT system exploiting a hybrid mechanism combining Kalman Filters and Transformers for high-performance MOT. We investigate the use of Transformers in 3D Multi-Object Tracking. We propose a new Transformer- based Joint Prediction approach in the Hybrid Mechanism with Kalman Filter for 3D-MOT. To assess the validity of our proposed approach, we compare our experimental results against those of state-of-the-art methods in 3D MOT. Our approach achieves interesting performance and clearly outperforms the PC3T and other methods in terms of tracking precision.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24148149"
    },
    {
        "id": 24363,
        "title": "Long Short-Term Memory Spatial Transformer Network",
        "authors": "Shiyang Feng, Tianyue Chen, Hao Sun",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic.2019.8785574"
    },
    {
        "id": 24364,
        "title": "Improving Multihead Finite State Machine With Transformer Neural Network",
        "authors": "Ulyana Pavlova, Valerii Boldakov",
        "published": "2022-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sibircon56155.2022.10017086"
    },
    {
        "id": 24365,
        "title": "Design of Rotary Transformer Based on Transient Field-Circuit Coupled Model",
        "authors": "H. Zhong, Y. Wang, B. Peng, Y. Yang",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2018.8508689"
    },
    {
        "id": 24366,
        "title": "Frequency Diagnostics of Transformer Insulating Parameters",
        "authors": "Peter Brncal, Miroslav Gutten, Viktor Cefer, Daniel Korenciak, Leszek Jarzebowicz",
        "published": "2020-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/diagnostika49114.2020.9214676"
    },
    {
        "id": 24367,
        "title": "High Performance Triggering Transformer for Stack of Series Connected Thyristors",
        "authors": "V. Senaj, D. Cabrerizo Pastor, T. Kramer",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppps34859.2019.9009655"
    },
    {
        "id": 24368,
        "title": "The Go Transformer: Natural Language Modeling for Game Play",
        "authors": "Matthew Ciolino, Josh Kalin, David Noever",
        "published": "2020-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ai4i49448.2020.00012"
    },
    {
        "id": 24369,
        "title": "Convolutional Feature based Vision Transformer Model for Speech Command Recognition",
        "authors": "Sayantan Das, Sandipan Dhar, Nanda Dulal Jana",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440809"
    },
    {
        "id": 24370,
        "title": "A Dual Transformer Model for Intelligent Decision Support for Maintenance of Wind Turbines",
        "authors": "Joyjit Chatterjee, Nina Dethlefs",
        "published": "2020-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206839"
    },
    {
        "id": 24371,
        "title": "Transformer-Based Lip-Reading with Regularized Dropout and Relaxed Attention",
        "authors": "Zhengyang Li, Timo Lohrenz, Matthias Dunkelberg, Tim Fingscheidt",
        "published": "2023-1-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10023442"
    },
    {
        "id": 24372,
        "title": "Language Modeling with Transformer",
        "authors": "JIAN GUO ZHANG, JIAN PING LI, HUANG LI",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip47768.2019.9067534"
    },
    {
        "id": 24373,
        "title": "Revised Spatial Transformer Network towards Improved Image Super-resolutions",
        "authors": "Hossam M. Kasem, Kwok-Wai Hung, Jianmin Jiang",
        "published": "2018-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr.2018.8546080"
    },
    {
        "id": 24374,
        "title": "A Transformer Protection Scheme Based on The Deep Forest Algorithm",
        "authors": "Anyang He, Zaibin Jiao, Zongbo Li",
        "published": "2020-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281888"
    },
    {
        "id": 24375,
        "title": "Semantic Parameter Matching in Web APIs with Transformer-based Question Answering",
        "authors": "Sebastian Kotstein, Christian Decker",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sose58276.2023.00020"
    },
    {
        "id": 24376,
        "title": "Calculation of Transformer Capacity Suitable for Induction Generator Operation",
        "authors": "Jong Gyeum Kim, Young Jeen Park",
        "published": "2021-10-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icems52562.2021.9634296"
    },
    {
        "id": 24377,
        "title": "Fault Diagnosis for Transformer Rectifier Unit on More-Electric Aircraft",
        "authors": "Yi Xiang, Fei Liu, Zhaodi Li",
        "published": "2022-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cieec54735.2022.9846545"
    },
    {
        "id": 24378,
        "title": "An Inter and Intra Transformer for Hate Speech Detection",
        "authors": "Xi Huang, Minxuan Xu",
        "published": "2021-12-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaecst54258.2021.9695652"
    },
    {
        "id": 24379,
        "title": "Investigation into the Impacts of Unbalanced GIC in Transformer Saturation Analysis",
        "authors": "Robert Arritt, Andres Ovalle, Roger Dugan",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kpec58008.2023.10215468"
    },
    {
        "id": 24380,
        "title": "PSpice Modeling of the Inrush Current in a 10 kVA Superconducting Transformer",
        "authors": "Pawel Surdacki, Leszek Jaroszynski, Lukasz Wozniak",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paee.2018.8441033"
    },
    {
        "id": 24381,
        "title": "Lattice Transformer for Speech Translation",
        "authors": "Pei Zhang, Niyu Ge, Boxing Chen, Kai Fan",
        "published": "2019",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1649"
    },
    {
        "id": 24382,
        "title": "A Multiscale Visualization of Attention in the Transformer Model",
        "authors": "Jesse Vig",
        "published": "2019",
        "citations": 177,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-3007"
    },
    {
        "id": 24383,
        "title": "Learning to Cluster Faces via Hypergraph Convolution with Transformer on Large Graph",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2023.06.018"
    },
    {
        "id": 24384,
        "title": "Intelligent fault diagnosis technology of power transformer based on Artificial Intelligence",
        "authors": "Li Feng, Ye Bo",
        "published": "2022-3-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec53115.2022.9734331"
    },
    {
        "id": 24385,
        "title": "St-Keys: Self-Supervised Transformer for Keyword Spotting in Historical Handwritten Documents",
        "authors": "Sana Khamekhem Jemni, Sourour Ammar, Mohamed  Ali Souibgui, Yousri Kessentini, Abbas Cheddad",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4384165"
    },
    {
        "id": 24386,
        "title": "Analysis and Improvement Measures of Acetylene Gas in Converter Transformer",
        "authors": "Qu Wentao, Zhao Hang, Muhammad Ubaid ur Rehman",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4449555"
    },
    {
        "id": 24387,
        "title": "Oh-Former: Omni-Relational High-Order Transformer for Occluded Person Re-Identification",
        "authors": "Xianing Chen, Chunlin Yu, Jialang Xu, Qiong Cao, Jingya Wang, Shenghua Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4455951"
    },
    {
        "id": 24388,
        "title": "Integrated Transformer and Autotransformer Design in Advanced RFSOI Process",
        "authors": "Xiaoqing Pei, Xu Zhu, Xudong Wang",
        "published": "2023-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmmt58241.2023.10276683"
    },
    {
        "id": 24389,
        "title": "SkinViT: A transformer based method for Melanoma and Nonmelanoma classification",
        "authors": "Somaiya Khan, Ali Khan",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "Over the past few decades, skin cancer has emerged as a major global health concern. The efficacy of skin cancer treatment greatly depends upon early diagnosis and effective treatment. The automated classification of Melanoma and Nonmelanoma is quite challenging task due to presence of high visual similarities across different classes and variabilities within each class. According to the best of our knowledge, this study represents the classification of Melanoma and Nonmelanoma utilising Basal Cell Carcinoma (BCC) and Squamous Cell Carcinoma (SCC) under the Nonmelanoma class for the first time. Therefore, this research focuses on automated detection of different skin cancer types to provide assistance to the dermatologists in timely diagnosis and treatment of Melanoma and Nonmelanoma patients. Recently, artificial intelligence (AI) methods have gained popularity where Convolutional Neural Networks (CNNs) are employed to accurately classify various skin diseases. However, CNN has limitation in its ability to capture global contextual information which may lead to missing important information. In order to address this issue, this research explores the outlook attention mechanism inspired by vision outlooker, which improves important features while suppressing noisy features. The proposed SkinViT architecture integrates an outlooker block, transformer block and MLP head block to efficiently capture both fine level and global features in order to enhance the accuracy of Melanoma and Nonmelanoma classification. The proposed SkinViT method is assessed by different performance metrics such as recall, precision, classification accuracy, and F1 score. We performed extensive experiments on three datasets, Dataset1 which is extracted from ISIC2019, Dataset2 collected from various online dermatological database and Dataset3 combines both datasets. The proposed SkinViT achieved 0.9109 accuracy on Dataset1, 0.8911 accuracy on Dataset3 and 0.8611 accuracy on Dataset2. Moreover, the proposed SkinViT method outperformed other SOTA models and displayed higher accuracy compared to the previous work in the literature. The proposed method demonstrated higher performance efficiency in classification of Melanoma and Nonmelanoma dermoscopic images. This work is expected to inspire further research in implementing a system for detecting skin cancer that can assist dermatologists in timely diagnosing Melanoma and Nonmelanoma patients.",
        "link": "http://dx.doi.org/10.1371/journal.pone.0295151"
    },
    {
        "id": 24390,
        "title": "Indoor Violence Detection using Lightweight Transformer Model",
        "authors": "Arushi Kumar, Arpan Shetty, Archit Sagar, Charushree A, Preet Kanwal",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170251"
    },
    {
        "id": 24391,
        "title": "Pedestrian Trajectory Prediction via Spatial Interaction Transformer Network",
        "authors": "Tong Su, Yu Meng, Yan Xu",
        "published": "2021-7-11",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivworkshops54471.2021.9669249"
    },
    {
        "id": 24392,
        "title": "Improved Transformer-Based SSD Detector for Airborne Object Detection",
        "authors": "Yang Zhou, Shuang Cong",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icftic57696.2022.10075226"
    },
    {
        "id": 24393,
        "title": "A 80 GHz VCO using Transformer Based Frequency Doubler",
        "authors": "Ioannis Dimitrios Psycharis, Grigorios Kalivas",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mocast57943.2023.10176596"
    },
    {
        "id": 24394,
        "title": "Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation",
        "authors": "Felix Gaschi, Francois Plesse, Parisa Rastin, Yannick Toussaint",
        "published": "2022-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892197"
    },
    {
        "id": 24395,
        "title": "Sparse Transformer-Based Algorithm for Long-Short Temporal Association Action Recognition",
        "authors": "Yue Lu, Yingyun Yang",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cost60524.2023.00027"
    },
    {
        "id": 24396,
        "title": "A Hierarchical Vision Transformer Using Overlapping Patch and Self-Supervised Learning",
        "authors": "Yaxin Ma, Ming Li, Jun Chang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191916"
    },
    {
        "id": 24397,
        "title": "4.3 The Mathematics of the Hilbert Transformer",
        "authors": "",
        "published": "2024-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780691235325-022"
    },
    {
        "id": 24398,
        "title": "M2-Conformer: Multi-modal CNN- Transformer for Driving Behavior Detection",
        "authors": "Jun Gao, Jiangang Yi, Yi Lu Murphey",
        "published": "2022-4-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isas55863.2022.9757336"
    },
    {
        "id": 24399,
        "title": "A “protein structure transformer” for integrative structural biology and molecular design",
        "authors": "Matteo Dal Peraro",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bpj.2023.11.1755"
    },
    {
        "id": 24400,
        "title": "Transformer fault diagnosis based on IWOA optimized XGBoost",
        "authors": "Sidan Lu, Xianwen Zeng",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3008720"
    },
    {
        "id": 24401,
        "title": "Evaluation and Verification of Series Resonant Converter with Transformer Operating Regimes",
        "authors": "Nikolay Hinov, Tsveti Hranov",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hitech.2018.8566490"
    },
    {
        "id": 24402,
        "title": "Determination of Mathematical Model Parameters of a Medium Frequency Transformer",
        "authors": "Michal Michna, Andrzej Wilk, Piotr Dworakowski, Bruno Lefebvre",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem.2018.8442840"
    },
    {
        "id": 24403,
        "title": "Zero-Shot Sketch Based Image Retrieval Using Graph Transformer",
        "authors": "Sumrit Gupta, Ushasi Chaudhuri, Biplab Banerjee, Saurabh Kumar",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956095"
    },
    {
        "id": 24404,
        "title": "Development and Application of Dry-type Tridimensional Wound-core Transformer",
        "authors": "Chengping Zhang, Zubing Zou, Fei Gai",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cieec47146.2019.cieec-2019726"
    },
    {
        "id": 24405,
        "title": "Modelling Transformer Core with Appropriate Boundary Conditions for Partial Discharge Studies",
        "authors": "Santosh Janaki Raman, Pritam Mukherjee, Sanjib Kumar Panda",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic43217.2019.9046571"
    },
    {
        "id": 24406,
        "title": "Analysis and Simulation of a Typical Two Winding Transformer",
        "authors": "Shreya Adhikary, Bhaskar Roy, Bikas Mondal",
        "published": "2022-2-11",
        "citations": 0,
        "abstract": "In this study, we show how input power, terminal voltage, and efficiency changes as a function of load current. In this situation, we've chosen three scenarios they are-\n\nCoil connection in Parallel\nCoil connection in Series\nTwo Winding Transformer\nHere, we have connected the coils of a single-phase transformer in series and parallel, then observed the transformer's loading as an autotransformer and as a two-winding transformer. It is essential to comprehend the instant polarities of the secondary terminals in relation to the primary in order to properly connect the winding.",
        "link": "http://dx.doi.org/10.46610/joped.2022.v08i01.001"
    },
    {
        "id": 24407,
        "title": "Measurement on Transformer Windings by Impact Test",
        "authors": "Daniel Korenciak, Miroslav Gutten, Matej Kucera, Milan Sebok, Tomasz Koltunowicz, Pawel Zukowski",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/measurement47340.2019.8780084"
    },
    {
        "id": 24408,
        "title": "Internet of Things-Based Arduino Controlled On-Load Tap Changer Distribution Transformer",
        "authors": "Krishan Arora",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003102267-12"
    },
    {
        "id": 24409,
        "title": "Cross-Component Transferable Transformer Pipeline Obeying Dynamic Seesaw for Rotating Machinery with Imbalanced Data",
        "authors": "Binbin Xu, Boquan Ma, Zheng Yang, Fei Chen, Xiaobing Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4425233"
    },
    {
        "id": 24410,
        "title": "ECONOMICAL IMPLEMENTATION OF ATTENTION IN THE TRANSFORMER ENCODER FOR SPEECH RECOGNITION",
        "authors": "V. Y. Chuchupal",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58633/2305-8129_2022_1_119"
    },
    {
        "id": 24411,
        "title": "Multi-Modal Transformer for RGB-D Salient Object Detection",
        "authors": "Peipei Song, Jing Zhang, Piotr Koniusz, Nick Barnes",
        "published": "2022-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9898069"
    },
    {
        "id": 24412,
        "title": "Research method of identifying transformer inrush current and fault current based on VMD-HHT",
        "authors": "Shangbin Jiao, Yuan Chang, Qing Zhang",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866504"
    },
    {
        "id": 24413,
        "title": "Substation-oriented PMU placement considering transformer tap settings",
        "authors": "Nikolaos M. Manousakis, George N. Korres",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichve.2018.8641992"
    },
    {
        "id": 24414,
        "title": "Three-Phase Current Reconstruction Methodology for Permanent Magnet Synchronous Motor Driver based on Current Transformer Sensors",
        "authors": "Jianan Cao, Gefei Meng, Jia Peng, Yunjia Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "The position sensorless Permanent Magnet Synchronous Motor (PMSM) is\nwidely applied in the field of high reliability, and its driver, the\ninverter is controlled by closed loop feedback with the three-phase\ncurrent. For medium and high power PMSM, both safety isolation and\ndetection accuracy of measuring current are extremely significant. This\npaper presents a method of using Current Transformer (CT) with center\ntaps to measure the high-frequency switch current and of an algorithm to\nreconstruct the low-frequency three-phase current. The measurement unit\nand reconfiguration algorithm are then applied to the motor starting\nstage controlled by the DSP controller TMS320F28335. Lastly, by\ncomparing the waveform of the reconstructed current and one of the\ninverter phase current, the experimental results show that the proposed\nreconstruction method is effective.",
        "link": "http://dx.doi.org/10.22541/au.165940742.28535663/v1"
    },
    {
        "id": 24415,
        "title": "Transformer-Based BiLSTM for Aspect-Level Sentiment Classification",
        "authors": "Tao Cai, Baocheng Yu, Wenxia Xu",
        "published": "2021-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rcae53607.2021.9638807"
    },
    {
        "id": 24416,
        "title": "Impact of the Winding Arrangement on Efficiency of the Resistance Spot Welding Transformer",
        "authors": "Gašper Habjan, Martin Petrun",
        "published": "2019-9-30",
        "citations": 1,
        "abstract": "In this paper, the impact of the winding arrangement on the efficiency of the resistance spot welding (RSW) transformer is presented. First, the design and operation of the transformer inside a high power RSW system are analyzed. Based on the presented analysis, the generation of imbalanced excitation of the magnetic core is presented, which leads to unfavorable leakage magnetic fluxes inside the transformer. Such fluxes are linked to the dynamic power loss components that significantly decrease the efficiency of the transformer. Based on the presented analysis, design guidelines to reduce the unwanted leakage fluxes are pointed out. The presented theoretical analysis is confirmed by measurements using a laboratory experimental system. The presented experimental results confirm that the proposed improved winding arrangement increased the efficiency of the transformer in average for 6.27%.",
        "link": "http://dx.doi.org/10.3390/en12193735"
    },
    {
        "id": 24417,
        "title": "Power transformer winding model for lightning impulse testing",
        "authors": "Tomislav Župan, Bojan Trkulja, Željko Štih",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.717"
    },
    {
        "id": 24418,
        "title": "Transformer Decoder Based Reinforcement Learning Approach for Conversational Response Generation",
        "authors": "Farshid Faal, Jia Yuan Yu, Ketra Schmitt",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207289"
    },
    {
        "id": 24419,
        "title": "PiTE: TCR-epitope Binding Affinity Prediction Pipeline using Transformer-based Sequence Encoder",
        "authors": "Pengfei Zhang, Seojin Bang, Heewook Lee",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811270611_0032"
    },
    {
        "id": 24420,
        "title": "Transformer Neural Network-Based Molecular Optimization Using General Transformations",
        "authors": "Jiazhen He, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Atanas Patronov, Esben Jannik Bjerrum, Ola Engkvist",
        "published": "No Date",
        "citations": 0,
        "abstract": "Molecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug discovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the large chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimicking the chemist's intuition in terms of matched molecular pairs (MMPs). Although MMPs is a typical and widely used strategy by medicinal chemists, it offers limited capability in terms of exploring the space of solutions. There are more options to modify a starting molecule to achieve desirable properties, e.g. one can simultaneously modify the molecule at different places including changing the scaffold. This study trains the same Transformer architecture on different datasets. These datasets consist of a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets reflecting general transformations are constructed from ChEMBL based on two approaches: Tanimoto similarity (allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold constant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while keeping the same model architecture. Our results show that the models trained on differently prepared datasets transform a given starting molecule in a way that it reflects the nature of the dataset used for training the model. These models could complement each other and unlock the capability for the chemists to pursue different options for improving a starting molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-z8rk6"
    },
    {
        "id": 24421,
        "title": "Modeling based on frequency to oversee the condition of transformer",
        "authors": "Swathy Sasikumar, V. A. Kulkarni",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipact.2017.8245192"
    },
    {
        "id": 24422,
        "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
        "authors": "jiawei wang, Weihong Lin, Chixiang Ma, Mingze Li, Zheng Sun, Lei Sun, Qiang Huo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4399248"
    },
    {
        "id": 24423,
        "title": "Sen Transformer",
        "authors": "",
        "published": "2021-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119824398.ch6"
    },
    {
        "id": 24424,
        "title": "Exploration of fault current limiting transformer using variable reactance",
        "authors": "M. KendreSomnath, C. R. Lakade",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iconstem.2017.8261389"
    },
    {
        "id": 24425,
        "title": "ENGLISH TRANSFORMER-BUILDING TERMS AND THE WAYS OF THEIR FORMATION",
        "authors": "I. M. Fesenko, O. M. Syvachuk",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26661/2414-1135-2020-80-2-44"
    },
    {
        "id": 24426,
        "title": "Comparing the Effectiveness of Classic Mask Rcnn and Vision Transformer in Early Weed Detection",
        "authors": "Shahnawaz Qureshi, Asif Ameer, Ali Zia, Ahsan Latif, Seppo Karrila",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4436132"
    },
    {
        "id": 24427,
        "title": "TransEM-Net: Transformer Based Efficient Multi-Magnification Network for Histopathology",
        "authors": "Geetank Raipuria, Aman Srivastava, Nitin Singhal",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230730"
    },
    {
        "id": 24428,
        "title": "Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle",
        "authors": "Akash Sonth, Abhijit Sarkar, Hirva Bhagat, Lynn Abbott",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186584"
    },
    {
        "id": 24429,
        "title": "Word Syllabification for Indonesian Language using Transformer",
        "authors": "Muhammad Haykal Kamil, Suyanto Suyanto, Mochammad Arif Bijaksana",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isitia59021.2023.10221089"
    },
    {
        "id": 24430,
        "title": "Hmt: Hybrid Mechanism Transformer for Bio-Fabrication Prediction Under Complex Environmental Conditions",
        "authors": "Yichen Song, Hu Xu, Changdi Li, Qunshan He, Zijian Tian, Xinggao Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4594980"
    },
    {
        "id": 24431,
        "title": "TRANSFORMER PRODUCTION IMPROVEMENT  BY LEAN AND MTM-2 TECHNIQUE",
        "authors": "Somkeit Noamna, Theerapong Thongphun, Chalermpon Kongjit",
        "published": "2022-6-1",
        "citations": 1,
        "abstract": "The situation of the covid-19 epidemic is a driving force of the global market’s demand increase of electronic devices and parts. Entire electronic component manufacturers, especially the transformer manufacturing industry, which is a device that supplies power to many electronic devices, encounters problems in producing products that are unable to keep up with the quickly increasing demand. This research aims to increase the productivity of small transformers by lean approach. The paper depicts processes relevant to improving production processes, reducing waste, and finding unnecessary processes. The method begins with two actions. First, study the current situation in transformer manufacturing of a case study. Second, study the customer order to delivery process using the Value Stream Mapping (VSM) and analyze entire processes of transformer manufacturing to identify standard time by unit work. The main technique is for measuring working time by timing the forward motion with the time measurement method version 2 (MTM-2). The Cause and Effect diagram was displayed with improving guidelines on two operations. First the concept of lean manufacturing was used in principal role, second the ECRS technique (Eliminate, Combine, Rearrange and Simplify) was applied to reduce \"waste\" as well as to optimize and reduce the manufacturing process of the transformer. The results lead to an increase in the final product per hour from 45 pieces per hour to 75 pieces per hour which increases up to 30% per hour. In addition, the productivity improvements increased the productivity of 3.46 workers per hour to 6.82 per hour (increase of 97.11%) and production time was reduced from 1,109 seconds to 229 seconds (73.04% of productivity).",
        "link": "http://dx.doi.org/10.11113/aej.v12.16712"
    },
    {
        "id": 24432,
        "title": "Multi-Modal Transformer with Multi-Head Attention for Emotion Recognition",
        "authors": "Chi Xu, Yifei Gao",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsece58870.2023.10263303"
    },
    {
        "id": 24433,
        "title": "Mitotic Cell Detection in Histopathological Images of Neuroendocrine Tumors Using YOLOv5-Transformer",
        "authors": "Zehra Karhan, Fuat Akal, pembe oltulu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327641"
    },
    {
        "id": 24434,
        "title": "Estimation of Parameters and Error Prediction for Instrument Transformer Using Machine Learning",
        "authors": "",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets43956"
    },
    {
        "id": 24435,
        "title": "Automatic Synthesis for On-chip Transformer",
        "authors": "Yinfei Weng, Zachary Su",
        "published": "2020-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nemo49486.2020.9343434"
    },
    {
        "id": 24436,
        "title": "Residential Load Clustering Contribution to Accurate Distribution Transformer Sizing",
        "authors": "Mehran Hajiaghapour-Moghimi, Kamyar Azimi-Hosseini, Ehsan Hajipour, Mehdi Vakilian",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc49016.2019.9081518"
    },
    {
        "id": 24437,
        "title": "Phase-Shifting Method with Dy11 Transformer to Reduce Harmonics",
        "authors": "Rudy Setiabudy, Immanuel Surya, Herlina Herlina",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecos47637.2019.8984589"
    },
    {
        "id": 24438,
        "title": "Reconfigurable photonic fractional Fourier transformer",
        "authors": "Shaowen Peng, Shangyuan Li, Guanyu Han, Xiaoxiao Xue, Xiaoping Zheng",
        "published": "2021-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2605312"
    },
    {
        "id": 24439,
        "title": "Transformer winding temperature rise measurement system",
        "authors": "Weili Wu, Hualong Zhang, Wentong Wu",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2640478"
    },
    {
        "id": 24440,
        "title": "Encoder/Decoder Transformer-Based Framework to Detect Hate Speech from Tweets",
        "authors": " Usman, S. M. K. Quadri",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003371380-19"
    },
    {
        "id": 24441,
        "title": "Fire and Explosion Risks and Consequences in Electrical Substations—A Transformer Case Study",
        "authors": "Mohanad El-Harbawi",
        "published": "2022-1-1",
        "citations": 4,
        "abstract": "Abstract\nThis study aims to find how fires and explosions can occur in enclosed spaces where electrical transformers are installed and to investigate the consequences of the damages to the surrounding areas caused by these accidents. This study began with the collection of a mineral oil waste sample from an indoor substation transformer in Riyadh, Saudi Arabia. This sample was analyzed to determine its composition. Results revealed that 30 components ranging from C6 to C30 were detected in the sample. The mixture flammability limits, calculated using Le Chatelier rules and found to be 0.97 and 6.56, indicated that the vapor mixture for the waste oil sample was not flammable at 25 °C and 1 atm. Consequence analysis was used to predict the outcome of fire and explosion events based on a transformer with a capacity of 1100 liters. The peak overpressure generated by an explosion was estimated to be 80.97 kPa. Moreover, the thermal radiation produced by various types of fires was estimated as a function of the distance from the accident center. The thermal flux from a boiling liquid expanding vapor explosion (BLEVE) was 99.8 kW/m2, which is greater than that from jet and pool fires. The probability of an individual suffering injury or dying as a result of exposure to fire and/or an explosion was estimated using dose-response models. The results showed that the peak overpressure produced by an explosion can cause severe damage within 20 m of the explosion center. However, the results also showed that there is a 100% probability of the thermal radiation from a BLEVE causing fatalities up to a distance of 140 m. The risk due to the fragmentation of the transformer tanks was also assessed, and a majority of fragments would land within a range of 111.2 m.",
        "link": "http://dx.doi.org/10.1115/1.4054143"
    },
    {
        "id": 24442,
        "title": "Accurate prediction of gestational diabetes mellitus via a novel transformer method",
        "authors": "Hui Wang, Ye Yao, Jieying Zheng, Danhong Peng, Jiansheng Wu, Jun Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDiabetes is a common complication that happened in pregnant women, and it often leads to many serious consequences for fetuses and gravidas. Accurate diagnosis of gestational diabetes mellitus (GDM) is the key to providing prompt and precise treatment and disease management. The artificial intelligence-based method is currently the most commonly used auxiliary way for clinical medical diagnosis. However, as all we know, there is no report on the assistance of GDM diagnosis based on artificial intelligence till now. In this work, we collected the clinical samples of 1000 pregnant women from ZhongDa Hospital of Southeast University in Nanjing city, which involves 221 cases of GDM. Then, a matrix factorization method was used to fill up all missing values in the original data.  Next, a random forest model was adopted to evaluate the importance of each feature dimension to aid in finding potential clinical markers for the GDM diagnosis. Finally, a novel transformer-based method called TF-GDM was proposed for predicting gestational diabetes mellitus accurately. The results show that our TF-GDM method achieves excellent performance, with the accuracy, precision, and recall of 0.93, 0.88, and 0.92, respectively, and also with the F1 score and AUC value of 0.90 and 0.94, respectively. The results demonstrate that our TF-GDM method is significantly better than classic machine learning-based and deep learning-based methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2461259/v1"
    },
    {
        "id": 24443,
        "title": "Cybersecurity Enhancement of Transformer Differential Protection",
        "authors": "Sampath R",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "Abstract: The increasing use of information and communication technologies (ICT) in the operational environments of power grids has been essential for operators to improve the monitoring, maintenance, and control of power generation, transmission, and distribution; however, this has come at the expense of increasing the grid's exposure to cyber threats. This paper looks at cyberattack scenarios that target protective relays in substations, which can be the most important part of protecting power systems from abnormal conditions. The overall performance of the power grid could suffer significantly if the relays' operations are disrupted, possibly resulting in widespread blackouts. Utilizing the potential of machine learning to detect anomalous behavior in transformer differential protective relays, we investigate methods for improving substation cybersecurity. In order to find cyberattacks, the proposed method looks at operational technology (OT) data from the substation current transformers (CTs). Power frameworks recreation utilizing OPAL-RTHYPERSIM is utilized to create preparing informational collections, to simulate the cyberattacks and to evaluate the network safety enhancement capability of the proposed AI calculations. Terms in the index include differential protective relays, transformers, operational technology, cyber physical systems, and machine learning.",
        "link": "http://dx.doi.org/10.22214/ijraset.2023.51963"
    },
    {
        "id": 24444,
        "title": "Medical Reports Summarization Using Text-To-Text Transformer",
        "authors": "Abdulkader Helwan, Danielle Azar, Dilber Uzun Ozsahin",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aset56582.2023.10180671"
    },
    {
        "id": 24445,
        "title": "Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer",
        "authors": "Dan Zhang, Fangfang Zhou",
        "published": "2023",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3243829"
    },
    {
        "id": 24446,
        "title": "TEDformer: Temporal Feature Enhanced Decomposed Transformer for Long-term Series Forecasting",
        "authors": "Jiayi Fan, Bingyao Wang, Dong Bian",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3287893"
    },
    {
        "id": 24447,
        "title": "Cross-Site Scripting Attack Detection Method Based on Transformer",
        "authors": "Bitao Peng, Xiyi Xiao, Juan Wang",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc56324.2022.10065892"
    },
    {
        "id": 24448,
        "title": "API Misuse Detection Method Based on Transformer",
        "authors": "Jingbo Yang, Jian Ren, Wenjun Wu",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qrs57517.2022.00100"
    },
    {
        "id": 24449,
        "title": "A Transformer-Based Prior Legal Case Retrieval Method",
        "authors": "Ceyhun E. Öztürk, Ş. Bariş ÖzçelıK, Aykut Koç",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223938"
    },
    {
        "id": 24450,
        "title": "Research on excitation current and vibration characteristics of DC biased transformer",
        "authors": "",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2019.00410"
    },
    {
        "id": 24451,
        "title": "Task Context Transformer and Gcn for Few-Shot Learning of Cross-Domain",
        "authors": "Pengfang Li, Fang Liu, Licheng Jiao, Lingling Li, Puhua Chen, Shuo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342068"
    },
    {
        "id": 24452,
        "title": "Alternative Liquid Dielectrics for High Voltage Transformer Insulation Systems",
        "authors": "",
        "published": "2021-12-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119800194"
    },
    {
        "id": 24453,
        "title": "Protection Transformer and Transmission Line in Power System Based on MATLAB Simulink",
        "authors": "Mohammed IBRAHIM",
        "published": "2021-9-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2021.10.04"
    },
    {
        "id": 24454,
        "title": "Unified power quality conditioner with shared legs and high-frequency transformer",
        "authors": "Alan S. Felinto, Cursino B. Jacobina",
        "published": "2020-10-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce44975.2020.9235624"
    },
    {
        "id": 24455,
        "title": "Performance analysis of SOTA Transformer Network in Numerical Expression Calculation",
        "authors": "Isha Ganguli, Rajat Subhra Bhowmick, Jaya Sil",
        "published": "2020-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon49873.2020.9342053"
    },
    {
        "id": 24456,
        "title": "Transformer Bushing Insulation Defect Detection Method Based on 3D Surface Map",
        "authors": "Longxiang Yuan, Yang Ding",
        "published": "2021-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwcsg53609.2021.00042"
    },
    {
        "id": 24457,
        "title": "Automatic Power Transformer Tester",
        "authors": "Khaja Moideen S. -, Akash S. -, Gokulraj N. -, Loganathan S. -, D. Sivaraj -",
        "published": "2023-4-2",
        "citations": 0,
        "abstract": "An essential step in the production of transformers is the quality testing of the devices. Transformers with normal operation have a lower failure rate and longer lifespan. When done manually, checking the transformer in its whole is a time-consuming and laborious task. Using the software platform LabVIEW (Laboratory Virtual Instrumentation Engineering Workbench), the testing method is automated and can save the time .The test platform is made to conduct tests with high voltage and zero load voltage test. A comprehensive design, simulation, and visualisation environment is provided by the LABVIEW software platform. The programme is built on cutting-edge. computing techniques making it possible to calculate the ideal transformer active and mechanical part configuration.",
        "link": "http://dx.doi.org/10.36948/ijfmr.2023.v05i02.2085"
    },
    {
        "id": 24458,
        "title": "Modeling and Detection of Inter-turn Faults in Distribution Transformer",
        "authors": "U. Rajendra Prasad, C. Vyjayanthi, Jaison K.",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icps48983.2019.9067533"
    },
    {
        "id": 24459,
        "title": "Large Power Transformer Magnetic Core Vibration Model by Using Dynamic Genetic Algorithm",
        "authors": "Janis Marks",
        "published": "2021-4-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pemc48073.2021.9432624"
    },
    {
        "id": 24460,
        "title": "Continuous Sign Language Recognition and Translation Using Hybrid Transformer-Based Neural Network",
        "authors": "Rajalakshmi E, Elakkiya R, Subramaniyaswamy V, Ketan Kotecha, Mayuri Mehta, Vasile Palade",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4424708"
    },
    {
        "id": 24461,
        "title": "A Compound Data Poisoning Technique with Significant Adversarial Effects on Transformer-based Text Classification Tasks",
        "authors": "Edmon Begoli, Maria Mahbub, Sudarshan Sriniva, Linsey Passarella",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer-based models have demonstrated much success in various natural language processing (NLP) tasks. However, they are often vulnerable to adversarial attacks, such as data poisoning, that can intentionally fool the model into generating incorrect results. In this paper, we present a novel, compound variant of a data poisoning attack on a transformer-based model that maximizes the poisoning effect while minimizing the scope of poisoning. We do so by combining the established data poisoning technique (label flipping) with a novel adversarial artifact selection and insertion technique aimed at minimizing detectability and the scope of the poisoning footprint. We find that using a combination of these two techniques, we achieve a state-of-the-art attack success rate (ASR) of ~90% while poisoning only 0.5% of the original training set, thus minimizing the scope and detectability of the poisoning action.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2443929/v1"
    },
    {
        "id": 24462,
        "title": "Multimodal Item Categorization Fully Based on Transformer",
        "authors": "Lei Chen, Houwei Chou, Yandi Xia, Hirokazu Miyake",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.ecnlp-1.13"
    },
    {
        "id": 24463,
        "title": "A Method for Measuring Liquid Weight Using a Hilbert Transformer",
        "authors": "Jun Obara, Naoyuki Aikawa",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mwscas57524.2023.10406133"
    },
    {
        "id": 24464,
        "title": "Joint Multi-Dimensional Dynamic Attention and Transformer for Efficient Image Restoration",
        "authors": "Huan Zhang, Xu Zhang, Nian Cai, Jiang-lei Di, Weisi LIN, Yun Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4437069"
    },
    {
        "id": 24465,
        "title": "Hardening Soft Information: A Transformer-Based Approach to Forecasting Stock Return Volatility",
        "authors": "Matthew Caron, Oliver Muller",
        "published": "2020-12-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378134"
    },
    {
        "id": 24466,
        "title": "DUSFormer: Dual-Swin Transformer V2 Aggregate Network for Polyp Segmentation",
        "authors": "Zhangrun Xia, Jingliang Chen, Chengzhun Lu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3352428"
    },
    {
        "id": 24467,
        "title": "Optimization of high frequency transformer based on advanced genetic algorithm",
        "authors": "G. Xiaowei, Y. Zhiting, J. Danchen",
        "published": "2017-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppc.2017.8291261"
    },
    {
        "id": 24468,
        "title": "Text-conditioned Transformer for automatic pronunciation error detection",
        "authors": "Zhan Zhang, Yuehai Wang, Jianyi Yang",
        "published": "2021-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.specom.2021.04.004"
    },
    {
        "id": 24469,
        "title": "Regional Transformer for Image Super-Resolution",
        "authors": "Sen Yang, Jiahong Yang, Dahong Xu, Xi Li",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cmvit57620.2023.00011"
    },
    {
        "id": 24470,
        "title": "Study of Transformer Switching Overvoltages during Power System Restoration Using Delta-Bar-Delta and Directed Random Search Algorithms",
        "authors": "Iman Sadeghkhani, Abbas Ketabi, Rene Feuillet",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00128"
    },
    {
        "id": 24471,
        "title": "The Determination of Kinetic Parameters of Transformer Oil and its Blends by Thermal Analysis",
        "authors": "Veresha Dukhi, Ajay Bissessur, Catherine Jane Ngila, Nelson Mutatina Ijumba",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00133"
    },
    {
        "id": 24472,
        "title": "Improve the Three-phase Unbalance Rate of Railway System Under 2% By Three Kinds of Special Transformer Wiring",
        "authors": "Chien-Hsu Chen, Chin E. Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Because imbalanced power will cause the loss of the propulsion motor of the railway vehicle, and the increase in temperature will shorten the service life of the electric vehicle. Not only this, but also increase the cost of electricity and maintenance. In the past, the industry only focused on methods to improve power quality such as load capacity, relay setting, and harmonic resolution. Now, the consider of three-phase unbalance rate (TPUR) must be applied. I propose special transformers wiring (STW) to improve the three unbalance rates and provide different transformer wiring methods. According to the IEEE Committee, in the future, power companies will need to install balanced relay stations to improve three-phase unbalance rate. the internal regulations of Taipower must be less than 4.5% (voltage unbalance rate (NPSUR)of 2.5% and motor temperature rise of 12.5%). the derivation of the transformer \"three-phase unbalance rate\" model is the focus of the railway system. This research is based on the model derivation of different wiring methods to improve the hot problem caused by the three-phase imbalance and improve the service life of the train. And pointed out that Scott, Le-Blanc, Modified-Woodbridge three wiring methods can be applied to future railway system routes to improve the three-phase unbalance rate, in line with the IEEE standard of less than 2%.",
        "link": "http://dx.doi.org/10.20944/preprints202111.0270.v2"
    },
    {
        "id": 24473,
        "title": "Reliability assessment of transformer insulating oil using accelerated life testing",
        "authors": "Xingchun Wei, Zhiming Wang, Junfeng Guo",
        "published": "2022-12-15",
        "citations": 1,
        "abstract": "AbstractTo improve the reliability and reduce the maintenance cost of transformer oil, a life prediction of transformer oil is needed so that the maintenance of transformer can be performed correctly. However, it is difficult to predict the reliable lifetime of transformer oil accurately because of its unkind operating condition at different environments. To solve this problem, based on the theory of accelerated life testing (ALT), a reliability assessment method for transformer insulating oil on a normal operational conditions is proposed. An inverse power Weibull distribution model for insulating oil lifetime with voltage is built. Numerical procedure of model parameter estimation is presented, the variances of model parameters and reliability indices, which including mean lifetime, reliability, reliable lifetime at given reliability and failure rate, are derived. The feasibility and correctness of the proposed method are validated by real lifetime data of transformer insulating oil in literature. The reliability of transformer insulating oil used at normal usage conditions are predicted by the proposed method, and the point and interval estimations of reliability indices are evaluated. The results show that reliable lifetime and mean lifetime under reliability limit should be considered simultaneously in repair or replacement of transformer insulating oil.",
        "link": "http://dx.doi.org/10.1038/s41598-022-26247-2"
    },
    {
        "id": 24474,
        "title": "A transformer method that predicts human lives from sequences of life events",
        "authors": "",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s43588-023-00586-0"
    },
    {
        "id": 24475,
        "title": "Decision letter for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2019-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v1/decision1"
    },
    {
        "id": 24476,
        "title": "Enhancing ECG Signal Data through Denoising Features with Transformer Generative Adversarial Networks for Model Classification 1D-CNN",
        "authors": "Hendrico Yehezky, Alhadi Bustamam, Hermawan Hermawan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn important component of telemedicine's remote cardiac health monitoring of patients is the use of artificial intelligence (AI) technology to detect electrocardiograph (ECG) signals. Failure to properly diagnose and treat abnormal ECG patterns caused by arrhythmia symptoms can result in a fatal outcome. Given that arrhythmia symptoms contribute significantly to noncommunicable cardiovascular disease (CVD), which is responsible for approximately 32% of global mortality, this concern becomes even more significant. The high sensitivity of ECG signals to both external and internal electrical disturbances makes accurate interpretation of these signals for arrhythmia detection challenging. An effective denoising technique is presented in this method as a substitute approach to reduce noise disturbances in ECG signal data and enhance the quality of the training data for AI detection models. This pre-processing technique combines a synthesis approach with Gaussian filtering, an auto-encoder-decoder (transformer), and generative adversarial networks (GANs). The MIT-BIH dataset is the subject of research for this study, which has been categorized into Normal, Atrial Premature, Premature Ventricular Contraction, Fusion of Ventricular and Normal, and Fusion of Paced and Normal. The research findings show that the quality of the synthesized data is almost identical to that of the original data. It is advised to use a deep neural network (DNN) model instead of the previous prediction model for this enhanced dataset, specifically a one-dimensional convolutional neural network (1D-CNN), which is well suited for training this reconstruction data through this experiment.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3432401/v1"
    },
    {
        "id": 24477,
        "title": "Residual life estimation of power transformer based on Karl Fischer and Adaptive neuro-fuzzy interference system",
        "authors": "Permit Mathuhu Sekatane, Thomas Otieno Olwal",
        "published": "2021-9-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/africon51333.2021.9570981"
    },
    {
        "id": 24478,
        "title": "CNN-Mixer Hierarchical Spectral Transformer for Hyperspectral Image Classification",
        "authors": "Wei Liu, Saurabh Prasad, Melba Crawford",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281792"
    },
    {
        "id": 24479,
        "title": "Infrared target tracking based on transformer",
        "authors": "Zhou Xi, Li XiaoHong",
        "published": "2023-6-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2682473"
    },
    {
        "id": 24480,
        "title": "PanFormer: A Transformer Based Model for Pan-Sharpening",
        "authors": "Huanyu Zhou, Qingjie Liu, Yunhong Wang",
        "published": "2022-7-18",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme52920.2022.9859770"
    },
    {
        "id": 24481,
        "title": "OSVConTramer: A Hybrid CNN and Transformer based Online Signature Verification",
        "authors": "Chandra Sekhar Vorugunti, Avinash Gautam, Viswanath Pulabaigari",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcb57857.2023.10449120"
    },
    {
        "id": 24482,
        "title": "Learning path recommendation based on Transformer reordering",
        "authors": "Yunxiang Liu, Yuanyuan Zhang, Guoqing Zhang",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isctt51595.2020.00025"
    },
    {
        "id": 24483,
        "title": "La pandémie de covid-19, une opportunité pour la souveraineté alimentaire",
        "authors": "Walden Bello, Amal Sall-Benotman",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/syll.cetri.2021.04.0031"
    },
    {
        "id": 24484,
        "title": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
        "authors": "Ananthan Nambiar, Simon Liu, Mark Hopkins, Maeve Heflin, Sergei Maslov, Anna Ritz",
        "published": "No Date",
        "citations": 16,
        "abstract": "AbstractThe scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the art approaches for protein family classification, while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",
        "link": "http://dx.doi.org/10.1101/2020.06.15.153643"
    },
    {
        "id": 24485,
        "title": "Sparse Universal Transformer",
        "authors": "Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.12"
    },
    {
        "id": 24486,
        "title": "CD-Net: Histopathology Representation Learning Using Context-Detail Transformer Network",
        "authors": "Saarthak Kapse, Srijan Das, Prateek Prasanna",
        "published": "2023-4-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230626"
    },
    {
        "id": 24487,
        "title": "CFD Based Sensitivity Study of Cooling Performance of Transformer Radiators",
        "authors": "Bernardo Galletti, Andreas Blaszczyk, Wei Wu",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/arwtr.2019.8930189"
    },
    {
        "id": 24488,
        "title": "Power Transformer Protection using ANN and Wavelet Transforms",
        "authors": "R.Naveena Bhargavi, M.Lakshmi Swarupa, M. Rajitha",
        "published": "2021-3-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaccs51430.2021.9441828"
    },
    {
        "id": 24489,
        "title": "TFE: A Transformer Architecture for Occlusion Aware Facial Expression Recognition",
        "authors": "Jixun Gao, Yuanyuan Zhao",
        "published": "2021-10-25",
        "citations": 6,
        "abstract": "Facial expression recognition (FER) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based FER approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based FER method (TFE) that is capable of adaptatively focusing on the most important and unoccluded facial regions. TFE is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for FER. Compared with traditional transformer, the novelty of TFE is two-fold: (i) To effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (ii) Given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed TFE on the two prevalent in-the-wild facial expression datasets (AffectNet and RAF-DB) and the their modifications with artificial occlusions. Experimental results show that TFE improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art FE methods, TFE obtains consistent improvements. Visualization results show TFE is capable of automatically focusing on the discriminative and non-occluded facial regions for robust FER.",
        "link": "http://dx.doi.org/10.3389/fnbot.2021.763100"
    },
    {
        "id": 24490,
        "title": "Enhancing Vegetable Sales Forecasting with A CNN-LSTM-Transformer Hybrid Model",
        "authors": "Aoxiang Tian",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": " Due to the short shelf life of vegetable products, a significant portion of the inventory cannot be resold the following day. To facilitate more informed procurement decisions in superstores and minimize vegetable wastage, this study proposes a hybrid prediction model based on CNN-LSTM-Transformer for enhancing the accuracy of forecasting vegetable sales volumes. Firstly, an LSTM model is incorporated to account for the recurring and seasonal variations in vegetable sales. Secondly, a CNN model is introduced to address the limitations of LSTM in capturing spatial data components. The convolutional and pooling layers of CNN help establish spatial relationships among different feature values in the dataset. Finally, a Transformer model is integrated to tackle the issue of long-term dependencies, which LSTM alone struggles to resolve. The Transformer model employs a parallel attention mechanism, eliminating temporal dependencies and effectively addressing LSTM's long-term dependency challenge. This integration also accelerates model training. The evaluation metric employed in this paper is RMSE (Root Mean Square Error). The three-year sales volume of 11 vegetable types is predicted using seasonal ARIMA, XGBoost, LSTM, CNN-LSTM, and CNN-LSTM-Transformer models. The results indicate that the CNN-LSTM-Transformer model achieves the lowest RMSE at 0.0758, followed by ARIMA at 0.0792, CNN-LSTM at 0.0830, XGBoost at 0.0858, and LSTM at 0.0913. These findings demonstrate that the CNN-LSTM-Transformer model exhibits superior accuracy in forecasting vegetable sales volumes, yielding more precise predictions. This accurate forecasting not only aids superstores in reducing waste and enhancing profitability but also assists government authorities in rationalizing vegetable subsidy policies and optimizing the vegetable production and marketing system.",
        "link": "http://dx.doi.org/10.54097/rr7t4d15"
    },
    {
        "id": 24491,
        "title": "Implementing Vision Transformer to Model Emotions Recognition from Facial Expressions",
        "authors": "Andry Chowanda,  Nadia,  Diana",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aidas60501.2023.10284712"
    },
    {
        "id": 24492,
        "title": "Lessons Learned from Analysis of Power Transformer Failure Rates",
        "authors": "Ronald D. Hernandez, Benjamin Hancock, Manuel Salmeron",
        "published": "2022-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/concapan48024.2022.9997796"
    },
    {
        "id": 24493,
        "title": "Adaptive Non-Local Regression Prior based on Transformer for Image Deblurring",
        "authors": "Yixing Ji",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3633637.3633713"
    },
    {
        "id": 24494,
        "title": "<i>De novo</i> mass spectrometry peptide sequencing with a transformer model",
        "authors": "Melih Yilmaz, William E. Fondrie, Wout Bittremieux, Sewoong Oh, William Stafford Noble",
        "published": "No Date",
        "citations": 15,
        "abstract": "AbstractTandem mass spectrometry is the only high-throughput method for analyzing the protein content of complex biological samples and is thus the primary technology driving the growth of the field of proteomics. A key outstanding challenge in this field involves identifying the sequence of amino acids—the peptide—responsible for generating each observed spectrum, without making use of prior knowledge in the form of a peptide sequence database. Although various machine learning methods have been developed to address this de novo sequencing problem, challenges that arise when modeling tandem mass spectra have led to complex models that combine multiple neural networks and post-processing steps. We propose a simple yet powerful method for de novo peptide sequencing, Casanovo, that uses a transformer framework to map directly from a sequence of observed peaks (a mass spectrum) to a sequence of amino acids (a peptide). Our experiments show that Casanovo achieves state-of-the-art performance on a benchmark dataset using a standard cross-species evaluation framework which involves testing with spectra with never-before-seen peptide labels. Casanovo not only achieves superior performance but does so at a fraction of the model complexity and inference time required by other methods.",
        "link": "http://dx.doi.org/10.1101/2022.02.07.479481"
    },
    {
        "id": 24495,
        "title": "Transformer-based quality assessment model for generalized user-generated multimedia audio content",
        "authors": "Deebha Mumtaz, Ajit Jena, Vinit Jakhetiya, Karan Nathwani, Sharath Chandra Guntuku",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10386"
    },
    {
        "id": 24496,
        "title": "Comparison of the Effect of the Generative Model on the Performance of Deep Neural Networks and Transformer in Contextual Social Bot Detect",
        "authors": "afsaneh hosseini",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4757423"
    },
    {
        "id": 24497,
        "title": "Application of the Analysis of Variance (ANOVA) in the Interpretation of Power Transformer Faults",
        "authors": "Bonginkosi A. Thango",
        "published": "2022-10-1",
        "citations": 8,
        "abstract": "Electrical power transformers are the most exorbitant and tactically prominent components of the South African electrical power grid. In contrast, they are burdened by internal winding faults predominantly on account of insulation system failure. It is essential that these faults must be swiftly and precisely uncovered and suitable measures should be adopted to separate the faulty unit from the entire system. The frequency response analysis (FRA) is a technique for tracking a transformer’s mechanical integrity. Nevertheless, classifying the category of the fault and its gravity by benchmarking measured FRA responses is still backbreaking and for the most part, anchored in personnel proficiency. This work presents a quantum leap to normalize the FRA interpretation procedure by suggesting an interpretation code criteria based on an empirical survey of transformers ranging from 315 kVA to 40 MVA. The study then proposes an analysis of variance (ANOVA) based interpretation tool for diagnosing the statistical significance of FRA fingerprint and measured profiles. The latter cannot be relied upon by an expert or by the naked eye. Additionally, descriptive FRA frequency sub-region data statistics are proposed to evaluate the shift in both the magnitude and measuring frequency characteristics to formulate the recommended interpretation code criteria. To corroborate the code criteria by incorporating ANOVA and descriptive statistics, the study presents various case studies with unknown FRA profiles for fault diagnosis. The results constitute proof of the reliability of the proposed code criteria and a proposed hybrid of ANOVA and descriptive statistics.",
        "link": "http://dx.doi.org/10.3390/en15197224"
    },
    {
        "id": 24498,
        "title": "Improved shoe recognition model using vision image transformer (VIT)",
        "authors": "I. Y. Garta, R.-C. Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.3222"
    },
    {
        "id": 24499,
        "title": "Hierarchical vector transformer vehicle trajectories prediction with diffusion convolutional neural networks",
        "authors": "Yingjuan Tang, Hongwen He, Yong Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127526"
    },
    {
        "id": 24500,
        "title": "Transformer Fault Condition Prognosis Using Vibration Signals Over Cloud Environment",
        "authors": "Mehdi Bagheri, Amin Zollanvari, Svyatoslav Nezhivenko",
        "published": "2018",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2018.2809436"
    },
    {
        "id": 24501,
        "title": "Local Consensus Transformer for Correspondence Learning",
        "authors": "Gang Wang, Yufei Chen",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00201"
    },
    {
        "id": 24502,
        "title": "Temperature Characteristics of Oil-Immersed Transformer Based on Fiber Bragg Grating Temperature Sensing Technology",
        "authors": "Chuan Luo, Zizhou Li, Bo Yang, Zhengang Zhao, Chuan Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740637"
    },
    {
        "id": 24503,
        "title": "Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography",
        "authors": "Stephane Cuenat, Raphael Couturier",
        "published": "2022-3-18",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790134"
    },
    {
        "id": 24504,
        "title": "HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud",
        "authors": "Zhixing Hou, Yan Yan, Chengzhong Xu, Hui Kong",
        "published": "2022-5-23",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811737"
    },
    {
        "id": 24505,
        "title": "Study of Different Transformer based Networks For Glaucoma Detection",
        "authors": "Siddhartha Mallick, Jayanta Paul, Nandita Sengupta, Jaya Sil",
        "published": "2022-11-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon55691.2022.9977730"
    },
    {
        "id": 24506,
        "title": "MLFT-Net: Point Cloud Completion Using Multi-Level Feature Transformer",
        "authors": "Yueling Du, Jin Xie",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscsic57216.2022.00042"
    },
    {
        "id": 24507,
        "title": "EVALUATING THE DIELECTRIC PERFORMANCE OF TRANSFORMER OIL AND ANTIOXIDANTS UNDER IMPULSE STRESS",
        "authors": "",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets50519"
    },
    {
        "id": 24508,
        "title": "Channel-Spatial Transformer for Efficient Image Super-Resolution",
        "authors": "Jiuqiang Li, Shilei Zhu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446047"
    },
    {
        "id": 24509,
        "title": "Enhancing Parameter Efficiency in Model Inference using an Ultralight Inter-Transformer Linear Structure",
        "authors": "Haoxiang Shi, Tetsuya Sakai",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3378518"
    },
    {
        "id": 24510,
        "title": "Éloge de l’Extra-Terrestre",
        "authors": "Dominiq Jenvrey",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "Rejetée par la figure du terrestre, celle de l’extraterrestre est Moderne : elle est née d’une Fiction se voulant séparée du Réel. L’extraterrestre, pourtant, entraîne les humains à la rencontre avec des entités qu’ils méconnaissent ou ne connaissent pas encore. D’où la nécessité de relier la fiction au réel, en assumant sa puissance effective sur nos réalités par le biais de ce qu’on pourrait appeler une effiction . Cela nous permettrait de passer d’un régime d’imagination moderne à un régime d’imagination terrestre.",
        "link": "http://dx.doi.org/10.3917/mult.094.0238"
    },
    {
        "id": 24511,
        "title": "Small Signal Internal Voltage Transfer Measurements and White-Box Transient Calculations for Non-standard Test Conditions of a Shell-Form Power Transformer",
        "authors": "Bjørn Gustavsen, Ariana Martins, Carlos A. Sá, Luis Braña, Ricardo Castro Lopes, Pedro Lima, Andrea Soto, Mário Soares",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5600-5_15"
    },
    {
        "id": 24512,
        "title": "Study on Electrical Insulation Structure Optimization Design of Converter Transformer Bushing",
        "authors": "Zhang Shiling, Ji Yongliang",
        "published": "2020-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acpee48638.2020.9136452"
    },
    {
        "id": 24513,
        "title": "Education-to-Skill Mapping Using Hierarchical Classification and Transformer Neural Network",
        "authors": "Vilija Kuodytė, Linas Petkevičius",
        "published": "2021-6-24",
        "citations": 2,
        "abstract": "Skills gained from vocational or higher education form an essential component of country’s economy, determining the structure of the national labor force. Therefore, knowledge on how people’s education converts to jobs enables data-driven choices concerning human resources within an ever-changing job market. Moreover, the relationship between education and occupation is also relevant in times of global crises, such as the COVID-19 pandemic. Healthcare system overload and skill shortage on one hand, and job losses related to lock-downs on the other, have exposed a necessity to identify target groups with relevant education backgrounds in order to facilitate their occupational transitions. However, the relationship between education and employment is complex and difficult to model. This study aims to propose the methodology that would allow us to model education-to-skill mapping. Multiple challenges arising from administrative datasets, namely imbalanced data, complex labeling, hierarchical structure and textual data, were addressed using six neural network-based algorithms of incremental complexity. The final proposed mathematical model incorporates the textual data from descriptions of education programs that are transformed into embeddings, utilizing transformer neural networks. The output of the final model is constructed as the hierarchical classification task. The effectiveness of the proposed model is demonstrated using experiments on national level data, which covers whole population of Lithuania. Finally, we provide the recommendations for the usage of proposed model. This model can be used for practical applications and scenario forecasting. Some possible applications for such model usage are demonstrated and described in this article. The code for this research has been made available on GitHub.",
        "link": "http://dx.doi.org/10.3390/app11135868"
    },
    {
        "id": 24514,
        "title": "Query Selector–Efficient transformer with sparse attention",
        "authors": "Jacek Klimek, Jakub Klimek, Witold Kraśkiewicz, Mateusz Topolewski",
        "published": "2022-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.simpa.2021.100187"
    },
    {
        "id": 24515,
        "title": "Design and Evaluation of a Laminated Three-Phase Rotary Transformer for DFIG Applications",
        "authors": "Stefan Botha, Nkosinathi Gule",
        "published": "2022-6-1",
        "citations": 2,
        "abstract": "In doubly fed induction generators (DFIGs), the rotor is excited through slip-ring and brush assemblies. These slip-ring and brush assemblies often require frequent routine maintenance, which affects the reliability of the DFIG. Alternatively, a contact-less energy transfer system, such as a rotary transformer, can be utilized in place of the slip rings. In DFIGs, the rotor frequency is very low, under 5 Hz, and this can lead to a huge rotary transformer since the transformer size is inversely proportional to its operating frequency. However, in a rotor-tied DFIG, whereby the rotor is connected directly to the grid whilst the stator is connected to a back-to-back converter, the rotor frequency becomes the grid frequency and can lead to a reasonably sized rotary transformer. In this paper, the design methodology of a three-phase rotary transformer that can be used in rotor-tied DFIG applications is proposed. The rotary transformer is coupled to the power windings of the rotor-tied DFIG and can improve its reactive power capabilities. The proposed methodology is validated with finite element analysis in 3D and can be used for an efficient design process with the proposed error correction. The proposed methodology is then applied in the design of a 6 kVA rotary transformer. Remarkable practical results are presented to demonstrate the effectiveness of the methodology. The rotary transformer is subsequently coupled to a rotor-tied DFIG and an acceptable performance is demonstrated for the entire system.",
        "link": "http://dx.doi.org/10.3390/en15114061"
    },
    {
        "id": 24516,
        "title": "A Transformer-based Model for Plant miRNA-lncRNA Interaction Prediction",
        "authors": "Wenjian Zhou, Ji Lu, Tang Xiwei",
        "published": "2022-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995426"
    },
    {
        "id": 24517,
        "title": "Skill Transformer: A Monolithic Policy for Mobile Manipulation",
        "authors": "Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00996"
    },
    {
        "id": 24518,
        "title": "Time Series Transformer for Long Term Rainfall Forecasting Towards Water Distribution Management in Smart Cities.",
        "authors": "Aswini Ghosh",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386081"
    },
    {
        "id": 24519,
        "title": "Analysis of failed and working power transformer and their diagnostics",
        "authors": "Niteen. M. Dhanve, S. L. Mhetre",
        "published": "2017-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnte.2017.7947927"
    },
    {
        "id": 24520,
        "title": "MONITORING AND CONTROLLING OF POWER LOSS IN A TRANSFORMER",
        "authors": "",
        "published": "2017-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21090/ijaerd.69316"
    },
    {
        "id": 24521,
        "title": "Design of Optimum Current Transformer",
        "authors": "Onkar Rajendra Aglave",
        "published": "2019-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22214/ijraset.2019.6006"
    },
    {
        "id": 24522,
        "title": "Hybrid Cnn-Transformer Network for Interactive Learning of Challenging Musculoskeletal Images",
        "authors": "Lei Bi, Ulrich Buehner, Xiaohang Fu, Tom Williamson, Peter  F.M. Choong, Jinman Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4535797"
    },
    {
        "id": 24523,
        "title": "Region to Global Vision Transformer for Baggage Re-Identification",
        "authors": "Zhiwei Xing, Shujie Zhu, Tao Zhang, Qian Luo",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240255"
    },
    {
        "id": 24524,
        "title": "Transformer l’excellence en recherche\n                \n   \n                Robert Tijssen,\n                Matthew L. Wallace et Robert McLean",
        "authors": "Erika Kraemer-Mbula, Robert Tijssen, Matthew Wallace, Robert McLean",
        "published": "2021-6-9",
        "citations": 0,
        "abstract": "Chapter 1 ofTransformer l’excellence en recherche",
        "link": "http://dx.doi.org/10.47622/9782954099477_1"
    },
    {
        "id": 24525,
        "title": "Simulation and experimental study of the three-phase three-legged transformer under DC bias",
        "authors": "Dong Xia",
        "published": "2017-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315375120-98"
    },
    {
        "id": 24526,
        "title": "« MÉTANOÏA » : transformer l’esprit de chacun, pour construire l’entreprise de demain",
        "authors": "Yann Goudy, Nicolas Piot",
        "published": "2020-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.saval.2020.02.0221"
    },
    {
        "id": 24527,
        "title": "CFD simulation of pressure loss in HVDC transformer winding",
        "authors": "Ralf Wittmaack",
        "published": "2022-7-4",
        "citations": 0,
        "abstract": "At Siemens the in-house CFD code UniFlow is used to analyse fluid flow and heat transfer in oilimmersed and dry-type transformers, as well as transformer components like windings, cores, tank walls, and radiators. It can be employed to perform steady state as well as transient analyses. This paper describes its physical models and numerical solution methods. Moreover, it presents an application to a valve winding of a HVDC transformer, cooled by mineral oil. This study is aimed at finding the flow induced pressure loss in the winding and the static ring assembly below and above the winding. The investigation includes isothermal runs with different inlet velocity and a conjugate heat transfer run with a conductor representation. In the isothermal simulations a steady state is established and the pressure loss is an almost linear function of the inlet velocity. In the run involving heat transfer, the high buoyancy forces hamper the development of a steady state and the possibility to calculate a flow induced pressure loss.",
        "link": "http://dx.doi.org/10.37798/2014631-4177"
    },
    {
        "id": 24528,
        "title": "Preparation and Characterization of Transformer Oil Based Nano Fluids",
        "authors": "N. Kishore, H.N. Vidyasagar, D.K. Ramesha",
        "published": "2019-11",
        "citations": 0,
        "abstract": "This paper is concerned with the preparation and characterization of transformer oil based nanofluids with the suspensions of Al2O3 and CuO nanoparticles . As a part of experimental study the transformer oil based nanofluids for heat exchangers in transformer cooling application, preparation and characterization has been performed. The preparation of nanofluids is the first key step in experimental studies with nanofluids. One step technique and two step technique are generally used for preparing the nanofluids. This work deals with the preparation methods of (Al2O3-Transformer oil ,CuO-Transformer oil) and characterizing the transformer oil based nanofluids of different volume concentrations (0.05%, 0.1%, 0. 5%, 1.0% and 1.5%).From the results it is revealed that increasing the volume concentration resulted in increase in thermal conductivity, viscosity of the nanofluid and decrease in density and specific heat of nanofluid.",
        "link": "http://dx.doi.org/10.4028/www.scientific.net/amm.895.218"
    },
    {
        "id": 24529,
        "title": "Transformer Based End-to-End Text Recognition System for Korean Handwritten Documents",
        "authors": "Changwoo Ha, Vladimir Tyan, DongBeom Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3976869"
    },
    {
        "id": 24530,
        "title": "Methods and benefits to the application of ultra-high-speed transformer protection",
        "authors": "Roy Moxley, Stefan Nohe",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2017.8090037"
    },
    {
        "id": 24531,
        "title": "T-CVAE: Transformer-Based Conditioned Variational Autoencoder for Story Completion",
        "authors": "Tianming Wang, Xiaojun Wan",
        "published": "2019-8",
        "citations": 38,
        "abstract": "Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than state-of-the-art models in terms of readability, diversity and coherence.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/727"
    },
    {
        "id": 24532,
        "title": "Influence of Temperature and Moisture on Aging Degree of Transformer Oil Paper Insulation",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/iccemm.2018.003"
    },
    {
        "id": 24533,
        "title": "MRST-YOLO: A Novel Microalgae Detection Method Based On YOLOv5s and Mixed Residual Swin Transformer",
        "authors": "Yantong Chen, Yang Liu, Yanyan Zhang, Jialiang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAiming at the problems of small target, variable morphology and dense distribution in the detection of microalgae cells in ship ballast water, this paper proposes a microalgae detection algorithm based on the YOLOv5s and Mixed Residual Swin Transformer(MRST). Firstly, to improve microalgae detection in dense scenes, this paper proposed a novel structure of Swin transformer with residual connectivity combined with feature extraction network of Yolov5s. It can enhance the model's encoding of relative location features to improve global capture performance. Secondly, to refine the extraction of microalgal features, an improved lightweight attention module is proposed to enhance multi-morphic feature recognition. Thirdly, since microalgae are mostly small targets, we fine-tuned the prediction mechanism, which can improve the small target localization ability and reduce the network parameters at the same time. Finally, to further improve the detection in dense microalgae scenarios, a novel prediction box processing paradigm CP-cluster is introduced to optimize the post-processing of the network. The experimental results indicate that under identical conditions, the mAP-IOU@0.5 and mAP-IOU@[0.5:0.95] of the proposed MRST-YOLO algorithm are 99.6% and 83.5%, respectively. The algorithm has the capability to achieve real-time detection, which greatly enhances its ability to identify microalgae species with better accuracy and efficiency.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2729685/v1"
    },
    {
        "id": 24534,
        "title": "Design and Testing of Low Loss Distribution Transformer for Non-Linear Loading Current",
        "authors": "P. Tandeaw, C. Manop, N. Jirasuwankul",
        "published": "2018-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieecon.2018.8712270"
    },
    {
        "id": 24535,
        "title": "Digital Program for Diagnosing the Status of a Power Transformer",
        "authors": "Ivan E. Kolesnikov, Anton V. Korzhov, Konstantin E. Gorshkov",
        "published": "2020-11-17",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glosic50886.2020.9267867"
    },
    {
        "id": 24536,
        "title": "Vision Transformer for Automatic Student Engagement Estimation",
        "authors": "Sandeep Mandia, Kuldeep Singh, Rajendra Mitharwal",
        "published": "2022-12-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipas55744.2022.10052945"
    },
    {
        "id": 24537,
        "title": "A framework-based transformer and knowledge distillation for interior style classification",
        "authors": "Anh H. Vo, Bao T. Nguyen",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126972"
    },
    {
        "id": 24538,
        "title": "Transient analysis of a distribution transformer using ATP-EMTP",
        "authors": "Mujtaba Ali, M. Asghar Saqib",
        "published": "2018-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpesg.2018.8384507"
    },
    {
        "id": 24539,
        "title": "A Topical Keywords Fusion Based on Transformer For Text Summarization",
        "authors": "Shuai Zhao, Fucheng You",
        "published": "2020-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicta51737.2020.00068"
    },
    {
        "id": 24540,
        "title": "Trendformer: Trend Adaptive Transformer for Traffic Flow Prediction",
        "authors": "Lei Huang, Feng Zhu, Zhiheng Li",
        "published": "2022-7-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsit55514.2022.9943858"
    },
    {
        "id": 24541,
        "title": "Application of Support Vector Machines to Locate Minor Short Circuits in Transformer Windings",
        "authors": "Arash Moradzadeh, Kazem Pourhossein",
        "published": "2019-9",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/upec.2019.8893542"
    },
    {
        "id": 24542,
        "title": "Design and Modelling of Thermal Energy Harvesting System for Power Transformer",
        "authors": "Abhijith R Prasad, V.P Mini",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct42659.2018.9058223"
    },
    {
        "id": 24543,
        "title": "Transformer based power combining for outphasing power amplifiers",
        "authors": "Soroush Moallemi, Kevin Grout, Jennifer Kitchen",
        "published": "2018-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wmcas.2018.8400624"
    },
    {
        "id": 24544,
        "title": "Real-Time Multi-Scale Pothole Detection using Transformer",
        "authors": "Anam Bibi, Khizer Ali, Ahmad Raza, Sumaira Kausar",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fit60620.2023.00030"
    },
    {
        "id": 24545,
        "title": "Mobile MicroRNAs: Potential for MicroRNA Biogenesis",
        "authors": "Yoichi Robertus Fujii",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3165-1_3"
    },
    {
        "id": 24546,
        "title": "Privacy Protection in Transformer-based Neural Network",
        "authors": "Jiaqi Lang, Linjing Li, Weiyun Chen, Daniel Zeng",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isi.2019.8823346"
    },
    {
        "id": 24547,
        "title": "Pure large kernel convolutional neural network transformer for medical image registration",
        "authors": "Zhao Fang, Wenming Cao",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "Deformable medical image registration is a fundamental and critical task in medical image analysis. Recently, deep learning-based methods have rapidly developed and have shown impressive results in deformable image registration. However, existing approaches still suffer from limitations in registration accuracy or generalization performance. To address these challenges, in this paper, we propose a pure convolutional neural network module (CVTF) to implement hierarchical transformers and enhance the registration performance of medical images. CVTF has a larger convolutional kernel, providing a larger global effective receptive field, which can improve the network’s ability to capture long-range dependencies. In addition, we introduce the spatial interaction attention (SIA) module to compute the interrelationship between the target feature pixel points and all other points in the feature map. This helps to improve the semantic understanding of the model by emphasizing important features and suppressing irrelevant ones. Based on the proposed CVTF and SIA, we construct a novel registration framework named PCTNet. We applied PCTNet to generate displacement fields and register medical images, and we conducted extensive experiments and validation on two public datasets, OASIS and LPBA40. The experimental results demonstrate the effectiveness and generality of our method, showing significant improvements in registration accuracy and generalization performance compared to existing methods. Our code has been available at https://github.com/fz852/PCTNet.",
        "link": "http://dx.doi.org/10.3233/ida-230197"
    },
    {
        "id": 24548,
        "title": "Transformer l’économie, favoriserle développement durable",
        "authors": " ",
        "published": "2020-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/49a492f0-fr"
    },
    {
        "id": 24549,
        "title": "Thyristor Controlled Transformer-type Tuned Filter Design",
        "authors": "HONG-SHENG SU, YU-SHUANG ZHOU",
        "published": "2018-2-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/dteees/epee2017/18169"
    },
    {
        "id": 24550,
        "title": "Fuzzy logic-based management of hybrid distribution transformer using LoRa technology",
        "authors": "",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.06440"
    },
    {
        "id": 24551,
        "title": "Bird's Eye View Semantic Segmentation based on Improved Transformer for Automatic Annotation",
        "authors": "",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2023.08.002"
    },
    {
        "id": 24552,
        "title": "An Ensemble Novel Architecture for Bangla Mathematical Entity Recognition Using Transformer Based Learning",
        "authors": "Tanjim Taharat Aurpa, Md Shoaib Ahmed, Mohammad  Aman Ullah, Maria Mehzabin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4531227"
    },
    {
        "id": 24553,
        "title": "SkeFormer: Sign Language Segmentation Based on Skeleton Input Transformer",
        "authors": "Gangqing Yu, Tiantian Yuan",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icetci57876.2023.10176896"
    },
    {
        "id": 24554,
        "title": "Integration of electrical data and transformer gas analysis for full asset monitoring",
        "authors": "Terrence Smith, Chris White",
        "published": "2018-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2018.8349802"
    },
    {
        "id": 24555,
        "title": "A magnetization hysteresis-based power transformer fault detection algorithm",
        "authors": "Zongbo Li, Zaibin Jiao, Yifei Wang, Feng Ma",
        "published": "2017-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2017.8273825"
    },
    {
        "id": 24556,
        "title": "Equivalent Circuits of the Transformer in Automated Systems Contactless Discharge",
        "authors": "Aleksei F. Burkov, Mikhail V. Kraskovskiy, Valery N. Yurin",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867676"
    },
    {
        "id": 24557,
        "title": "Aesthetic Assessment of Packaging Design Based on Con-Transformer",
        "authors": "Wei Li",
        "published": "2023-1-27",
        "citations": 0,
        "abstract": "Different from the traditional natural images' aesthetic assessment task, the aesthetic assessment of packaging design should not only pay attention to artistic beauty, but also pay attention to functional beauty, that is, the attraction of the packaging design to consumers. In this paper, the authors propose a con-transformer packaging design aesthetic assessment method, which takes advantage of convolutional operations and self-attention mechanisms for enhanced representation learning, resulting in an effective aesthetic assessment of the packaging design images. Specifically, con-transformer integrates convolution network branch and transformer network branch to extract local representation features and global representation features of the packaging design images respectively. Finally, the fused representation features are used for aesthetic assessment. Experimental results show that the proposed method can not only effectively assess the aesthetic of packaging design images, but also be applied to the aesthetic assessment of natural images.",
        "link": "http://dx.doi.org/10.4018/ijec.316873"
    },
    {
        "id": 24558,
        "title": "Effect of Load Switching on Induced e.m.f. of a Transformer",
        "authors": "S. Bhowmik, A. Mitra, P. B. Deb",
        "published": "2021-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compe53109.2021.9752120"
    },
    {
        "id": 24559,
        "title": "Romat: Role-Based Multi-Agent Transformer for Generalizable Heterogeneous Cooperation",
        "authors": "Dongzi Wang, Fangwei Zhong, Muning Wen, Minglong Li, Yuanxi Peng, Teng Li, Yaodong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4437059"
    },
    {
        "id": 24560,
        "title": "A Reluctance-Based Electromagnetic Transient Model for the Sen Transformer with Inter-Turn Fault",
        "authors": "Wei Li, Song Han, Xi Guo, Shufan Xie, Na Rong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4480801"
    },
    {
        "id": 24561,
        "title": "Power Transformer Vibration Study and its Application in Winding Deformation Detection",
        "authors": "Amir Esmaeili Nezhad, Mohammad Hamed Samimi",
        "published": "2022-5-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee55646.2022.9827437"
    },
    {
        "id": 24562,
        "title": "AU-Shaped Fourier Unit Transformer for Single Image Deblurring",
        "authors": "Hongmei Zhang, Yongde Guo",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisce58541.2023.10142927"
    },
    {
        "id": 24563,
        "title": "Synthesizer Preset Interpolation Using Transformer Auto-Encoders",
        "authors": "Gwendal Le Vaillant, Thierry Dutoit",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096397"
    },
    {
        "id": 24564,
        "title": "Detecting Earthquakes in SAR Interferogram with Vision Transformer",
        "authors": "Bruno Silva, Joaquim J. Sousa, Antonio Cunha",
        "published": "2022-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883523"
    },
    {
        "id": 24565,
        "title": "Chapitre 4. La transformation culturelle : l’orientation réclamation",
        "authors": "William Sabadie, Daniel Ray, David Gotteland",
        "published": "2017-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/vuib.meyro.2017.01.0075"
    },
    {
        "id": 24566,
        "title": "Résister au marché par la consommation responsable en vue de le transformer",
        "authors": "Abdelmajid AMINE, Mouna BENHALLAM",
        "published": "2021",
        "citations": 1,
        "abstract": "Ce chapitre s’intéresse aux pratiques de résistance organisées en ligne par des collectifs de consommateurs adeptes d’une démarche de consommation responsable. Les résultats d’une étude netnographique menée auprès de trois communautés virtuelles, montrent la dimension performative des actions menées par ces communautés sur le plan des comportements individuels, des pratiques marketing des entreprises et des processus de régulation par les pouvoirs publics.",
        "link": "http://dx.doi.org/10.51926/iste.9036.ch1"
    },
    {
        "id": 24567,
        "title": "Tunable Impedance Transformer Based on Split Strip Lines",
        "authors": "N.D. Malyutin, A.V. Andreev, G.A. Malyutin, R.M. Sharabudinov",
        "published": "2019-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sibcon.2019.8729638"
    },
    {
        "id": 24568,
        "title": "Deformation Diagnostic Methods for Transformer Winding through System Identification",
        "authors": "R. Venkataswamy, K. Uma Rao, P. Meena",
        "published": "2019-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icondsc.2019.8816967"
    },
    {
        "id": 24569,
        "title": "Transformer la société et réinventer l’Église",
        "authors": "Yann Raison du Cleuziou",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/books.pur.163951"
    },
    {
        "id": 24570,
        "title": "Monitoring Transformer Condition with MLP Machine Learning Model",
        "authors": "Dino Žanić, Alan Župan",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "Failures of large power transformers in transmission system are always followed by significant costs, which is especially problematic because they present an unplanned expenditure. Aside from derailing financial plans, these events can lead to lower system reliability. This paper describes the development and potential application of transformer model based on multilayer perceptron class of artificial neural networks. Model is built in Python programming language and data collected over the span of one year for a single transformer. Three input features (oil temperature, winding current and outside temperature) are used in the input layer, with the goal of predicting the winding temperature in the transformer. Predicted temperature of the windings can then be compared with the actual winding temperature, which can serve as an indicator of transformers internal condition. Two types of transformer condition degradation are simulated to show the model in action, and certain indicators are explored.",
        "link": "http://dx.doi.org/10.37798/2023722417"
    },
    {
        "id": 24571,
        "title": "Crater-DETR: A Novel Transformer Network for Crater Detection Based on Dense Supervision and Multiscale Fusion",
        "authors": "Yue Guo, Hao Wu, Shuojin Yang, Zhanchuan Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170258969.92657652/v1"
    },
    {
        "id": 24572,
        "title": "DAT: Domain Adaptive Transformer for Domain Adaptive Semantic Segmentation",
        "authors": "Jinyoung Park, Minseok Son, Sumin Lee, Changick Kim",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897293"
    },
    {
        "id": 24573,
        "title": "Research on Optimization of Distribution Network based on Power Electronic Transformer",
        "authors": "Yanheng Lv",
        "published": "2023-4-11",
        "citations": 0,
        "abstract": "As the core equipment of hybrid microgrid, power electronic transformer is a new type of multifunctional transformer which integrates power electronic technology, information communication technology and control protection technology. On the background of the Beijing Low-Carbon Winter Olympics Smart Grid Demonstration Project, this paper proposed an AC/DC hybrid microgrid topology with dual-end power supply based on power electronic transformer. Moreover, a coordinated optimization control for AC/DC hybrid microgrid based on power electronic transformer is studied. The content of the paper mainly includes: The common topology of power electronic transformer is studied, and a three stage four-port power electronic transformer is proposed according to the demand of demonstration project. The interface converter topology structure and control method of distributed generation and energy storage in hybrid microgrid are analyzed, and the system model of hybrid microgrid is established. Simulation results show that MPPT control can maximize the power output of distributed generation, and piecewise droop control can realize seamless switching between power control and voltage control in the grid-tied or autonomous mode.",
        "link": "http://dx.doi.org/10.54097/hset.v35i.7030"
    },
    {
        "id": 24574,
        "title": "Tradformer: A Transformer Model of Traditional Music Transcriptions",
        "authors": "Luca Casini, Bob L. T. Sturm",
        "published": "2022-7",
        "citations": 1,
        "abstract": "We explore the transformer neural network architecture for modeling music, \n\nspecifically Irish and Swedish traditional dance music.\n\nGiven the repetitive structures of these kinds of music, the transformer should be as successful with fewer parameters and complexity as the hitherto most successful model, a vanilla long short-term memory network.\n\nWe find that achieving good performance with the transformer is not straightforward,\n\nand careful consideration is needed for the sampling strategy, \n\nevaluating intermediate outputs in relation to engineering choices, \n\nand finally analyzing what the model learns.\n\nWe discuss these points with several illustrations, \n\nproviding reusable insights for engineering other music generation systems.\n\nWe also report the high performance of our final transformer model \n\nin a competition of music generation systems\n\nfocused on a type of Swedish dance.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/681"
    },
    {
        "id": 24575,
        "title": "Improving the real power capacity of a furnace transformer through capacitance injection",
        "authors": "Nonhlanhla Mahlangu, Agha Francis Nnachi, Aloys Oriedi Akumu, Wonderful Mubatanhema",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/africon46755.2019.9133861"
    },
    {
        "id": 24576,
        "title": "The Remaining Life of Distribution Transformer Prediction by Using Neuro-Wavelet Method",
        "authors": "Rosmaliati Rosmaliati",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15199/48.2023.02.19"
    },
    {
        "id": 24577,
        "title": "Diagnosing the Condition of Transformer Oils Using the Trajectory Method",
        "authors": "Shutenko Oleg, Ponomarenko Serhii",
        "published": "2021-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mees52427.2021.9598490"
    },
    {
        "id": 24578,
        "title": "One-to-many pattern comparison combining fully-connected autoencoder with spatial transformer for ornament investigation",
        "authors": "Sayan Chaki, Simon Steinlin, Remi Emonet, Thierry Fournel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nComparing a query image to some representative of a set of unaligned imagesfrom a class is a cornerstone task for the investigation of ancient ornaments. Whileconvolutional autoencoders provide a level of invariance to translation, they canonly handle a limited range of transformations and often incur blurriness. Wepropose to increase the invariance to linear transformations and standard fluctuationsby using a spatial transformer, then increase reproduction sharpness byusing a fully-connected autoencoder. We evaluate our approach on challengingancient ornament images with synthetic abnormal distortions. This approach significantlyimproves the accuracy of change localization. This information makesit possible to precisely highlight signed changes in image comparators, helpingdomain experts for more efficient analysis of ornaments.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3573134/v1"
    },
    {
        "id": 24579,
        "title": "Attention-Aligned Transformer for Image Captioning",
        "authors": "Zhengcong Fei",
        "published": "2022-6-28",
        "citations": 8,
        "abstract": "Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance. However, some researchers have argued “deviated focus” problem of existing attention mechanisms in determining the effective and influential image features. In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead. Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation. We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight. Then, we propose four aligned strategies to use this information to refine attention weight distribution. Under such a pattern, image regions are attended correctly with the output words. Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation. Trained models and code for reproducing the experiments are publicly available.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i1.19940"
    },
    {
        "id": 24580,
        "title": "Portfolio Optimization with 2D Relative-Attentional Gated Transformer",
        "authors": "Tae Wan Kim, Matloob Khushi",
        "published": "2020-12-16",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csde50874.2020.9411635"
    },
    {
        "id": 24581,
        "title": "An Improved Vision Transformer for Early Detection of Diabetic Foot Using Thermogram",
        "authors": "Huasong Shao",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248732"
    },
    {
        "id": 24582,
        "title": "DC Transformer for Optimal Power Flow with Interior Point Algorithm",
        "authors": "K. Sebaa, O. Kahouli",
        "published": "2018-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssd.2018.8570368"
    },
    {
        "id": 24583,
        "title": "Decision letter for \"Optimal MV/LV transformer allocation in distribution network for power losses reduction and cost minimization: A new multi-objective framework\"",
        "authors": "",
        "published": "2019-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12361/v2/decision1"
    },
    {
        "id": 24584,
        "title": "Improving transformer model translation for low resource South African languages using BERT",
        "authors": "Paddington Chiguvare, Christopher W Cleghorn",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659923"
    },
    {
        "id": 24585,
        "title": "A Transformer-based Joint Prediction in Hybrid Mechanism with Kalman Filter for 3D-MOT",
        "authors": "Rahmad Sadli, soheyb Ribouh, Atika Rivenq, Abdenour Hadid, Abdelmalik Taleb Ahmed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a novel 3D MOT system exploiting a hybrid mechanism combining Kalman Filters and Transformers for high-performance MOT. We investigate the use of Transformers in 3D Multi-Object Tracking. We propose a new Transformer- based Joint Prediction approach in the Hybrid Mechanism with Kalman Filter for 3D-MOT. To assess the validity of our proposed approach, we compare our experimental results against those of state-of-the-art methods in 3D MOT. Our approach achieves interesting performance and clearly outperforms the PC3T and other methods in terms of tracking precision.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24148149.v1"
    },
    {
        "id": 24586,
        "title": "A Lightweight Context-Aware Feature Transformer Network for Human Pose Estimation",
        "authors": "Yanli Ma, Qingxuan Shi, Fan Zhang",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "We propose a Context-aware Feature Transformer Network (CaFTNet), a novel network for human pose estimation. To address the issue of limited modeling of global dependencies in convolutional neural networks, we design the Transformerneck to strengthen the expressive power of features. Transformerneck directly substitutes 3×3 convolution in the bottleneck of HRNet with a Contextual Transformer (CoT) block while reducing the complexity of the network. Specifically, the CoT first produces keys with static contextual information through 3×3 convolution. Then, relying on query and contextualization keys, dynamic contexts are generated through two concatenated 1×1 convolutions. Static and dynamic contexts are eventually fused as an output. Additionally, for multi-scale networks, in order to further refine the features of the fusion output, we propose an Attention Feature Aggregation Module (AFAM). Technically, given an intermediate input, the AFAM successively deduces attention maps along the channel and spatial dimensions. Then, an adaptive refinement module (ARM) is exploited to activate the obtained attention maps. Finally, the input undergoes adaptive feature refinement through multiplication with the activated attention maps. Through the above procedures, our lightweight network provides powerful clues for the detection of keypoints. Experiments are performed on the COCO and MPII datasets. The model achieves a 76.2 AP on the COCO val2017 dataset. Compared to other methods with a CNN as the backbone, CaFTNet has a 72.9% reduced number of parameters. On the MPII dataset, our method uses only 60.7% of the number of parameters, acquiring similar results to other methods with a CNN as the backbone.",
        "link": "http://dx.doi.org/10.3390/electronics13040716"
    },
    {
        "id": 24587,
        "title": "Analysis on Withstand Voltage of DC bias Device at 500kV Transformer Neutral Point",
        "authors": "Ren Hongtao",
        "published": "2020-10-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpea49807.2020.9280141"
    },
    {
        "id": 24588,
        "title": "Heuristic Control of Neutral DC Compensation Method to Moderate DC bias in Power Transformer",
        "authors": "Olanrewaju Lasabi, Andrew Swanson, Leigh Jarvis",
        "published": "2022-8-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powerafrica53997.2022.9905399"
    },
    {
        "id": 24589,
        "title": "Relphormer: Relational Graph Transformer for Knowledge Graph Representations",
        "authors": "Zhen Bi, Siyuan Cheng, Jing Chen, Xiaozhuan Liang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466291"
    },
    {
        "id": 24590,
        "title": "Accurate Calculation of Eddy Current Loss in Litz-Wired High Frequency Transformer Windings",
        "authors": "Z. Liu, L. Zhu, J. Zhu",
        "published": "2018-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intmag.2018.8508822"
    },
    {
        "id": 24591,
        "title": "Conv-Transformer Architecture for Unconstrained Off-LineUrdu Handwriting Recognition",
        "authors": "Nauman Riaz, Haziq Arbab, Arooba Maqsood, Khuzaeymah Bin Nasir, Adnan Ul-Hasan, Faisal Shafait",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nUnconstrained off-line handwriting text recognition in general and for Arabic-like scripts in particular is a challenging task and is still an active researcharea. Transformer based models for English handwriting recognition have recently shown promising results.In this paper, we have explored the use of transformerarchitecture for Urdu handwriting recognition. The useof a Convolution Neural Network before a vanilla fullTransformer and using Urdu printed text-lines alongwith handwritten text lines during the training are thehighlights of the proposed work. The Convolution Layers act to reduce the spatial resolutions and compensate for the n2 complexity of transformer multi-head attention layers. Moreover, the printed text images inthe training phase help the model in learning a greaternumber of ligatures (a prominent feature of Arabiclike scripts) and a better language model. Our modelachieved state-of-the-art accuracy (CER of 5.31%) onpublicly available NUST-UHWR dataset [1].",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1514700/v1"
    },
    {
        "id": 24592,
        "title": "Epilepsy seizure detection using Transformer",
        "authors": "Hangyi Pan, Shuaikui Gong, Fang Dong, Lurong Jiang",
        "published": "2022-6-14",
        "citations": 2,
        "abstract": "Electroencephalogram (EEG) is a general examination method for doctors to diagnose epilepsy, and it is also an important tool for studying brain activity. Due to the time-consuming and uncertainty of manually extracting features from EEG signals, this paper will be based on an end-to-end deep learning method different from the classic CNN and RNN network structure. This paper uses a relatively novel Transformer network structure to identify EEG whether the signal is epileptic. The experiment in this paper was carried out on the public CHBMIT data set, and finally, the average result of the five-fold cross-validation was 94.46%, the specificity was 93.97%, and the sensitivity was 94.96%. The experimental results show that the Transformer model has a higher performance improvement than the classic Resnet and Bi-LSTM networks, and it has greater potential in future epilepsy detection applications.",
        "link": "http://dx.doi.org/10.54097/hset.v1i.482"
    },
    {
        "id": 24593,
        "title": "Emotion Recognition from Brain Connectivity Based on Multi-Head Attention Transformer and Autoencoder Network",
        "authors": "Farzad Saffari, Mikkel N Schmidt, Massimo Buscema, Luis E Bruni",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170420879.92145482/v1"
    },
    {
        "id": 24594,
        "title": "Solution of nonlinear stiff differential equations for a three-phase no-load transformer using a Runge–Kutta implicit method",
        "authors": "",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/aee.2022.142126"
    },
    {
        "id": 24595,
        "title": "Transformer Fault Diagnosis Based on Adversarial Generative Networks and Deep Stacked Autoencoder",
        "authors": "Lei Zhang, Zhongyang Xu, Yu Liu, Tianjiao Qiao, Hongzhi Su, Yazhou Luo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4715615"
    },
    {
        "id": 24596,
        "title": "Generative AI on a Budget: Processing Transformer- based Neural Networks at the Edge",
        "authors": "Y. Tanurhan, P. Paulin, T. Michiels",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iedm45741.2023.10413675"
    },
    {
        "id": 24597,
        "title": "The Effect of Transformer Interwinding Capacitance on Hard-Switched Converter Operation",
        "authors": "Claus S. Kjeldsen, Christian Østergaard",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vppc55846.2022.10003371"
    },
    {
        "id": 24598,
        "title": "Progressive Multi-Scale Vision Transformer for Facial Action Unit Detection",
        "authors": "Chongwen Wang, Zicheng Wang",
        "published": "2022-1-12",
        "citations": 4,
        "abstract": "Facial action unit (AU) detection is an important task in affective computing and has attracted extensive attention in the field of computer vision and artificial intelligence. Previous studies for AU detection usually encode complex regional feature representations with manually defined facial landmarks and learn to model the relationships among AUs via graph neural network. Albeit some progress has been achieved, it is still tedious for existing methods to capture the exclusive and concurrent relationships among different combinations of the facial AUs. To circumvent this issue, we proposed a new progressive multi-scale vision transformer (PMVT) to capture the complex relationships among different AUs for the wide range of expressions in a data-driven fashion. PMVT is based on the multi-scale self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for AUs. Compared with previous AU detection methods, the benefits of PMVT are 2-fold: (i) PMVT does not rely on manually defined facial landmarks to extract the regional representations, and (ii) PMVT is capable of encoding facial regions with adaptive receptive fields, thus facilitating representation of different AU flexibly. Experimental results show that PMVT improves the AU detection accuracy on the popular BP4D and DISFA datasets. Compared with other state-of-the-art AU detection methods, PMVT obtains consistent improvements. Visualization results show PMVT automatically perceives the discriminative facial regions for robust AU detection.",
        "link": "http://dx.doi.org/10.3389/fnbot.2021.824592"
    },
    {
        "id": 24599,
        "title": "Transfer of supraharmonics through a MV/LV transformer",
        "authors": "T. Slangen, E. de Jong, V. Cuk, S. Cobben",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2023.0254"
    },
    {
        "id": 24600,
        "title": "Hybrid Transformer Network for Deepfake Detection",
        "authors": "Sohail Ahmed Khan, Duc-Tien Dang-Nguyen",
        "published": "2022-9-14",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3549555.3549588"
    },
    {
        "id": 24601,
        "title": "Parallel Implementation of Vision Transformer on a Multi-FPGA Cluster",
        "authors": "Yasuyu Fukushima, Kensuke Iizuka, Hideharu Amano",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/candar60563.2023.00020"
    },
    {
        "id": 24602,
        "title": "Load Forecast Based on RF-PSO-VMD-Transformer-LSTM",
        "authors": "Kaiyao Tan, Enkai Zhang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsca59871.2023.10392512"
    },
    {
        "id": 24603,
        "title": "TRANSFORMER BUSHING – A PART OF MEASUREMENT SYSTEM",
        "authors": " Rajko Gardijan,  Alen Keller",
        "published": "2022-7-19",
        "citations": 1,
        "abstract": "Power transformer is one of the most important and most expensive components in the electric power system (EPS) and requires, from its production and throughout lifetime, continuous monitoring and checks of the availability in the power system. All the measurements and tests on the transformer which manufacturers attempt, in order to determine the quality of their products, are carried out through the connection of different types of bushings, and these measurements and tests are conducted in accordance with requirements of applicable standards. Also, the owner of the transformer during the exploitation phase wishes and needs to know the status and availability of transformers for future work. The only available points of the transformer inside are the bushings. The technical practice from the early beginnings of the transformers are periodic off-line measurements. Following that practice and experience, the need arose for continuous supervision of transformer operation (on-line). In the 90’s the first simple systems for the transformer on-line monitoring appeared. Today, it is an established fact that the modern systems for on-line monitoring of transformers provide a complete insight in the transformer state including alarms in the case of critical states. It is realistic to expect in the future that these same systems, apart from providing diagnostics and giving alarms, will be authorized to switch off transformers in the case of necessity with high degree of confidence. An important role in on-line monitoring, as a part of measurement system, has a transformer bushing. For the purpose of both off-line and on-line measurements, manufacturers of the bushings are equipping them with the measuring tap. Unfortunately, despite intentions to standardize the components of the power system, this is not the case with the measuring tap, and the intention of this paper is to draw attention to these problems and to try to find a solution.",
        "link": "http://dx.doi.org/10.37798/2012611-4247"
    },
    {
        "id": 24604,
        "title": "TweetACE: A Fine-grained Classification of Disaster Tweets using Transformer Model",
        "authors": "Ademola Adesokan, Sanjay Madria, Long Nguyen",
        "published": "2023-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aipr60534.2023.10440656"
    },
    {
        "id": 24605,
        "title": "Capsule Transformer Network for Dynamic Hand Gesture Recognition Using Multimodal Data",
        "authors": "Alexandre Lebas, Rim Slama, Hazem Wannous",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222370"
    },
    {
        "id": 24606,
        "title": "A transformer-based approach for Arabic offline handwritten text recognition",
        "authors": "Saleh Momeni, Bagher BabaAli",
        "published": "2024-1-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11760-023-02970-9"
    },
    {
        "id": 24607,
        "title": "Efficient Vision Transformer for Accurate Traffic Sign Detection",
        "authors": "Javad Mirzapour Kaleybar, Hooman Khaloo, Avaz Naghipour",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke60553.2023.10326242"
    },
    {
        "id": 24608,
        "title": "Anchor free based Siamese network tracker with transformer for RGB-T tracking",
        "authors": "Liangsong Fan, Pyeoungkee Kim",
        "published": "2023-8-16",
        "citations": 1,
        "abstract": "AbstractIn recent years, many RGB-THERMAL tracking methods have been proposed to meet the needs of single object tracking under different conditions. However, these trackers are based on ANCHOR-BASED algorithms and feature cross-correlation operations, making it difficult to improve the success rate of target tracking. We propose a siamAFTS tracking network, which is based on ANCHOR-FREE and utilizes a fully convolutional training network with a Transformer module, suitable for RGB-THERMAL target tracking. This model addresses the issue of low success rate in current mainstream algorithms. We also incorporate channel and channel spatial attention modules into the network to reduce background interference on predicted bounding boxes. Unlike current ANCHOR-BASED trackers such as MANET, DAPNet, SGT, and ADNet, the proposed framework eliminates the use of anchor points, avoiding the challenges of anchor hyperparameter tuning and reducing human intervention. Through repeated experiments on three datasets, we ultimately demonstrate the improved success rate of target tracking achieved by our proposed tracking network.",
        "link": "http://dx.doi.org/10.1038/s41598-023-39978-7"
    },
    {
        "id": 24609,
        "title": "Multiscale Audio Spectrogram Transformer for Efficient Audio Classification",
        "authors": "Wentao Zhu, Mohamed Omar",
        "published": "2023-6-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096513"
    },
    {
        "id": 24610,
        "title": "Conditional variational transformer for bearing remaining useful life prediction",
        "authors": "Yupeng Wei, Dazhong Wu",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aei.2023.102247"
    },
    {
        "id": 24611,
        "title": "Insulation System optimization in Dry-Type Transformer Using Finite Element Method",
        "authors": "Shohreh Saberi, Mehdi Bigdeli, Davood Azizian",
        "published": "2022-5-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee55646.2022.9827001"
    },
    {
        "id": 24612,
        "title": "A Driving Area Detection Algorithm Based on Swin Transformer",
        "authors": "Shuang Liu, Ying Li",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456095"
    },
    {
        "id": 24613,
        "title": "Transformer in Computer Vision",
        "authors": "Jiarui Bi, Zengliang Zhu, Qinglong Meng",
        "published": "2021-9-24",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cei52496.2021.9574462"
    },
    {
        "id": 24614,
        "title": "SUNet: Swin Transformer UNet for Image Denoising",
        "authors": "Chi-Mao Fan, Tsung-Jung Liu, Kuan-Hsien Liu",
        "published": "2022-5-28",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas48785.2022.9937486"
    },
    {
        "id": 24615,
        "title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network For Speech Separation",
        "authors": "Jian Luo, Jianzong Wang, Ning Cheng, Edward Xiao, Xulong Zhang, Jing Xiao",
        "published": "2022-9-18",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-66"
    },
    {
        "id": 24616,
        "title": "Transformer Model Compression for End-to-End Speech Recognition on Mobile Devices",
        "authors": "Leila Ben Letaifa, Jean-Luc Rouas",
        "published": "2022-8-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco55093.2022.9909765"
    },
    {
        "id": 24617,
        "title": "Rice Transformer: A Novel Integrated Management System for Controlling Rice Diseases",
        "authors": "Rutuja Rajendra Patil, Sumit Kumar",
        "published": "2022",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3200688"
    },
    {
        "id": 24618,
        "title": "An End-to-End Transformer Model for 3D Object Detection",
        "authors": "Ishan Misra, Rohit Girdhar, Armand Joulin",
        "published": "2021-10",
        "citations": 188,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00290"
    },
    {
        "id": 24619,
        "title": "HFT6D: Multimodal 6D object pose estimation based on hierarchical feature transformer",
        "authors": "Yunnan An, Dedong Yang, Mengyuan Song",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.measurement.2023.113848"
    },
    {
        "id": 24620,
        "title": "MUSIQ: Multi-scale Image Quality Transformer",
        "authors": "Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, Feng Yang",
        "published": "2021-10",
        "citations": 115,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00510"
    },
    {
        "id": 24621,
        "title": "Configuration of Three-phase to Eleven-phase Power Transformer",
        "authors": "Abdelmalik Djebli, Omar Touhami, Rachid Ibtiouen",
        "published": "2018-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cistem.2018.8613556"
    },
    {
        "id": 24622,
        "title": "Magneto Optic Current Transformer Technology (MOCT)",
        "authors": "Attish Jain",
        "published": "2017-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9790/1676-1201044650"
    },
    {
        "id": 24623,
        "title": "The Location of Partial Discharge Source in the Power Transformer with UHF Sensors",
        "authors": "Naifan Xue, Junjie Yang",
        "published": "2018-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsai.2018.8599407"
    },
    {
        "id": 24624,
        "title": "Accelerating Transformer for Neural Machine Translation",
        "authors": "Li Huang, Wenyu Chen, Hong Qu",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3457682.3457711"
    },
    {
        "id": 24625,
        "title": "Conv-Vit-Hgr: A Transformer Based Hybrid Model for Hand Gesture Recognition in Complex Background",
        "authors": "Sheetal Agarwal, Ratnakar Dash",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4084524"
    },
    {
        "id": 24626,
        "title": "A Controlled Switching Approach to Reduction of Three-Phase Transformer Inrush Currents",
        "authors": "Joydeep Mitra, Xufeng Xu, Mohammed Benidris",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias.2018.8544470"
    },
    {
        "id": 24627,
        "title": "Transformer design according to criteria and load profile",
        "authors": "Josifs Survilo",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rtucon.2017.8124744"
    },
    {
        "id": 24628,
        "title": "Wide &amp; Deep Machine Learning Model for Transformer Health Analysis",
        "authors": "P. Sarajcev, D. Jakus, M. Nikolic",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/splitech.2019.8783122"
    },
    {
        "id": 24629,
        "title": "Impedance-based Stability Analysis of Cascaded Dual-Active-Bridge Converters in Smart Transformer Microgrids",
        "authors": "Jiajun Yang, Sandro Guenter, Giampaolo Buticchi",
        "published": "2022-10-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/aeit56783.2022.9951773"
    },
    {
        "id": 24630,
        "title": "CELL-E 2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer",
        "authors": "Emaad Khwaja, Yun S. Song, Aaron Agarunov, Bo Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractWe present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (andvice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enablingde novoprotein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured athttps://bohuanglab.github.io/CELL-E_2/.",
        "link": "http://dx.doi.org/10.1101/2023.10.05.561066"
    },
    {
        "id": 24631,
        "title": "Impact of Antioxidant and Degasification on a New Liquid Dielectric Suitable for Transformer Applications",
        "authors": "D M Srinivasa, Usha Surendra",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3711922"
    },
    {
        "id": 24632,
        "title": "Tightly Coupled Dipole Array with Guanella Transformer and Balun",
        "authors": "Conrad Andrews, Dejan Filipovic, Riley Pack, Alan Brannon",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceaa52647.2021.9539746"
    },
    {
        "id": 24633,
        "title": "A Transformer Based Network Using Micro-Doppler Features for Continuous Human Motion Recognition",
        "authors": "Liubing Jiang, Minyang wu, Li Che, Xiaoyong Xu, Yujie Mu, Yongman Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Radar-based human motion recognition has received extensive attention in\nrecent years. Most current recognition methods generate a heat map of\nfeatures through simple signal processing and then feed into a\nclassification-based neural network for recognition. Such an approach\ncan only identify a single action. When a set of data contains\ninformation about multiple movements it can also only be recognized as a\nsingle movement. Therefore, in order to solve the problem that\ncontinuous human motion cannot be recognized, we propose a continuous\naction recognition method based on micro-Doppler features and\nTransformer, which translates the micro-Doppler features of continuous\nactions into machine translation tasks, and uses the idea of natural\nlanguage processing (NLP) to identify continuous action.",
        "link": "http://dx.doi.org/10.22541/au.166792505.55382878/v1"
    },
    {
        "id": 24634,
        "title": "Blood Cell Classification: Convolutional Neural Network (CNN) and Vision Transformer (ViT) under Medical Microscope",
        "authors": "Mohamad Abou Ali, Fadi Dornaika, Ignacio Arganda-Carreras",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep Learning (DL) has made significant advances in computer vision with the advent of Vision Transformers (ViT). Unlike Convolutional Neural Networks (CNNs), ViTs use self-attention to extract both local and global features from image data, and then use residual connections to feed these features directly into a fully networked multilayer perceptron head. In hospitals, hematologists prepare peripheral blood smears (PBSs) and read them under a medical microscope to detect abnormalities in blood counts such as leukemia. However, this task is time-consuming and prone to human error. This study investigates the transfer learning process of Google ViT and ImageNet CNNs to automate the reading of PBSs. The study used two online PBS datasets, PBC and BCCD, and transferred them into balanced datasets to investigate the influence of data amount and noise immunity on both neural networks. The PBC results show that Google ViT is an excellent DL neural solution for data scarcity. The BCCD results show that Google ViT is superior to ImageNet CNNs in dealing with unclean, noisy image data because it is able to extract both global and local features and use residual connections, despite the additional time and computational overhead.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1753.v1"
    },
    {
        "id": 24635,
        "title": "Power Quality Problems Due to Transformer Inrush Current",
        "authors": " Amir Tokić,  Ivo Uglešić",
        "published": "2022-8-22",
        "citations": 0,
        "abstract": "Transformer energization can produce a large nonsinuoidal inrush current which contains both odd and higher order harmonic components that can put transformer winding under mechanical stress. Additionally, they can cause irregular tripping of harmonic protection relays. Furthermore, in relatively weak power systems, such as is the Bosnian system, the superposition of harmonic components with system resonance frequencies may produce temporary overvoltages (TOV). Transformer winding failures and metal oxide surge arrester (MOA) energy stresses can occur due to TOV. The paper demonstrates a case study of an energization of a 220/110 kV transformer and power quality problems that can appear due to higher harmonics. Energy stresses of MOA provoked by transformer energization are considered in the paper.",
        "link": "http://dx.doi.org/10.37798/2010591-4272"
    },
    {
        "id": 24636,
        "title": "Synchronous machine field excitation utilizing a single phase matrix converter excited rotary transformer",
        "authors": "Jianyang Liu, Thomas A. Lipo",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2017.8095925"
    },
    {
        "id": 24637,
        "title": "Fractal Hilbert sensor to detect partial discharge on transformer",
        "authors": "Herman Halomoan Sinaga",
        "published": "2020-4-10",
        "citations": 1,
        "abstract": "The design of fractal Hilbert sensor is presented in this paper. The sensor is intended to detect partial discharge (PD) in transformer insulation. The fractal Hilbert sensor designed using 4 order fractal and etching on single layer PCB board. The Hilbert fractal type is chosen as this type of sensor can be built on smaller area compare to other types. The sensor overall dimension is limited to 5x5 cm as it is proposed to be use to detect the PD presence in transformer. The sensor sensitivity is tested using a closed type TEM cell. After the sensitivity of the sensor is tested the sensor then applied to detect the PD signals emitted by void PD defect model. The results show the sensor has sensitivity as high as 10 dB. The sensor also has capability to detect the PD signals generated by the PD defect source. The waveforms captured by the sensor show the sensors can capture high frequency pulse generated by the PD source.  Keywords: Partial Discharge (PD), Fractal Hilbert Sensor UHF, Sensor Sensitivity",
        "link": "http://dx.doi.org/10.23960/jesr.v1i2.31"
    },
    {
        "id": 24638,
        "title": "Thérapies du deuil : établir des dialogues entre vivants et morts pour transformer leurs relations",
        "authors": "Laura Perichon",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "Nous nous appuierons ici sur une vision du processus de deuil en deux temps distincts, celui de l’adaptation à un quotidien où le défunt n’est plus et celui de l’évolution du lien avec le défunt. Ce second temps correspond à la dimension relationnelle du deuil et renvoie à tout ce qui peut rester en suspens, au cours du temps, entre les vivants et leurs proches défunts. Certaines thérapies du deuil invitent ainsi les personnes endeuillées à des formes d’interactions et de dialogues avec le défunt afin d’offrir la possibilité de pouvoir dire, sentir, faire ou entendre ce qui importe à chacun. Grâce à la mise en place d’interactions significatives entre vivants et morts, tant leur lien que les images du défunt se modifient, conduisant à davantage d’apaisement. Par ailleurs, ces interactions peuvent venir soulever la question du mode d’existence du défunt, certains considérant qu’il est constitué de souvenirs et de représentations et d’autres estimant qu’il poursuit une existence invisible et autonome par-delà la mort.",
        "link": "http://dx.doi.org/10.3917/tf.232.0165"
    },
    {
        "id": 24639,
        "title": "Transformer less Bidirectional Grid-Connected Single Power Conversion Converter",
        "authors": " Saima S,  ",
        "published": "2020-6-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17577/ijertv9is060607"
    },
    {
        "id": 24640,
        "title": "The influence of electric stress on the transformer oil-conductivity",
        "authors": "Cheng Zhidong, Yang Lijun, Zhou Kuiyu, Lu Yuncai",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd.2018.8514797"
    },
    {
        "id": 24641,
        "title": "Effect of Rectifier on Transient Characteristics of Transformer at DC Substation for Electric Railway",
        "authors": "Kenji TANOUE, Hiroki TANAKA",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iclp.2018.8503478"
    },
    {
        "id": 24642,
        "title": "A Multi-Step Water Quality Prediction Model Based on the Savitzky-Golay Filter and Transformer Optimized Network",
        "authors": "Ruiqi Wang, Ying Qi, Qiang Zhang, Fei Wen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4471898"
    },
    {
        "id": 24643,
        "title": "Effective Automated Transformer Model based Sarcasm Detection Using Multilingual Data",
        "authors": "Vidyullatha Sukhavasi, Venkatesulu Dondeti",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-17302-9"
    },
    {
        "id": 24644,
        "title": "A Squeeze-and-Excitation and Transformer based Cross-task System for Environmental Sound Recognition",
        "authors": "Jisheng Bai, Jianfeng Chen, Mou Wang, Muhammad Saad Ayub",
        "published": "No Date",
        "citations": 0,
        "abstract": "Environmental sound recognition (ESR) is an emerging research topic in audio pattern recognition.Many tasks are presented to resort to computational systems for ESR in real-life applications.However, current systems are usually designed for individual tasks, and are not robust and applicable to other tasks.Cross-task systems, which promote unified knowledge modeling across various tasks, have not been thoroughly investigated.In this paper, we propose a cross-task system for three different tasks of ESR: acoustic scene classification, urban sound tagging, and anomalous sound detection.An architecture named SE-Trans is presented that uses attention mechanism-based Squeeze-and-Excitation and Transformer encoder modules to learn channel-wise relationship and temporal dependencies of the acoustic features.FMix is employed as the data augmentation method that improves the performance of ESR.Evaluations for the three tasks are conducted on the recent databases of DCASE challenges. The experimental results show that the proposed cross-task system achieves state-of-the-art performance on all tasks. Further analysis demonstrates that the proposed cross-task system can effectively utilize acoustic knowledge across different ESR tasks.",
        "link": "http://dx.doi.org/10.31219/osf.io/mek7v"
    },
    {
        "id": 24645,
        "title": "An  Experimental Setup for Monitoring Distribution Transformer Health",
        "authors": "Giri Rajanbabu Venkatakrishnan, Ramasubbu Rengaraj, Arvindswamy Velumani",
        "published": "2021-4-21",
        "citations": 1,
        "abstract": "The Distribution transformers are the most expensive and important component which are used for transmission and distribution of electrical energy. It is imperative that the transformers function correctly without any faults, and should any faults occur, the same should be detected and corrected as soon as possible to prevent the failure of the power system to supply power. Health monitoring systems of distribution transformers are used to diagnose the distribution transformer and to deduce its working condition under the occurrence of incipient faults. This paper presents a model of a health monitoring system for distribution transformers in a laboratory environment. The proposed model ensures that faults do not disrupt the regular supply of power.",
        "link": "http://dx.doi.org/10.13052/dgaej2156-3306.3532"
    },
    {
        "id": 24646,
        "title": "Design and Analysis of Solid State DC-AC Transformer",
        "authors": "K. Suresh, I. Kumaraswamy, R. Arulmozhiyal",
        "published": "2023-3-3",
        "citations": 0,
        "abstract": "Multimodal single/dual stage conversion (BSDC) based on a new solid-state DC-AC transformer (SDAT) is proposed. The suggested conversion can perform variable or constant DC voltage operation. A variable DC voltage is controlled dynamically in response to changes in DC input side voltage, allowing the DC-DC conversion stage to always operate at its best. New DC-AC power conversion of 2 port two-way choppers (TBDC) and two-way rectifier/inverter conversion (BRIC) was planned for the implementation of the proposed SDAT. SDAT, consisting of a bi-directional step-down-step-up converter and a bi-directional step-up-step-down converter, has been used as a prototype model to check the suggestion system. Functional concepts of this BSDC converter with TBDC are designed and simulated on the MATLAB/simulink environment network. PWM and control methods have been applied for SDAT to accomplish a wide variety of voltage and power loss reduction in the proposed device. Test bench model hardware is designed and tested to validate the effectiveness and benefits of the proposed approach.",
        "link": "http://dx.doi.org/10.13052/dgaej2156-3306.3832"
    },
    {
        "id": 24647,
        "title": "State evaluation model of distribution transformer considering environmental factors and operation data",
        "authors": "Chen Yingyu, Zhang Yubo, Wang Youyuan, Fang Jian",
        "published": "2020-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic47619.2020.9158769"
    },
    {
        "id": 24648,
        "title": "A Transformer-based Network with Differential Feature Triple Refinement for Bitemporal Remote Sensing Image Change Detection",
        "authors": "Hao Chang, Xian Sun, Peijin Wang, Wenhui Diao, Guangluan Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In change detection (CD), how to reduce the interferences of pseudo changes and accurately recognize the change of interest (COI) are two important challenges. Recently, considering the powerful long-distance modeling ability of the transformer, some methods try to introduce the transformer into CD and have already proposed several useful CD strategies. However, the existing strategies either do not directly work on the change of interest (COI) or are difficult to give full play to the advantages of the transformer. Therefore, in this paper, we propose a new CD strategy to tackle the above challenges. Specifically, we focus on the difference domain and propose the differential feature triple refinement strategy to precisely characterize COI. We first adopt a CNN-based differential feature extraction (DFET) module to extract the possible detail differences between bitemporal images. Then, we introduce a transformer-based differential feature enhancement (DFEH) module to capture and enhance the COI regions from the preliminarily extracted differences. Finally, we utilize a CNN-based differential feature fusion (DFFS) module to integrate the fine-grained information into the enhanced COI regions. Based on the proposed strategy, we design a new network named DiFormer. We verify six effective hyperparameter configurations and conduct experiments on four commonly researched CD datasets. Extensive experiment results indicate that our proposed strategy has the outstanding generalization ability and obtains the better balance between computation costs and model performance. Peculiarly, when only adopting the Natural Scene Image Pretraining (NSIP), our method still exceeds the recently proposed CD methods which especially focus on the improvement of Remote Sensing Image Pretraining (RSIP).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23119658.v1"
    },
    {
        "id": 24649,
        "title": "Research on application of multivariate cumulative chart in transformer fault warning",
        "authors": "HuiYing Zhou, Song Bin",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceect55960.2022.10030733"
    },
    {
        "id": 24650,
        "title": "KiUT: Knowledge-injected U-Transformer for Radiology Report Generation",
        "authors": "Zhongzhen Huang, Xiaofan Zhang, Shaoting Zhang",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01897"
    },
    {
        "id": 24651,
        "title": "Knowledge Blended Open Domain Visual Question Answering using Transformer",
        "authors": "Dipali Koshti, Ashutosh Gupta, Mukesh Kalla",
        "published": "2023-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icais56108.2023.10073911"
    },
    {
        "id": 24652,
        "title": "Transformer Equivalent Model of Sheathed Cable",
        "authors": "Manika Khadka, Pranav Johri, C. C. Reddy",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/catcon56237.2022.10077696"
    },
    {
        "id": 24653,
        "title": "Video Person Re-identification Based on Transformer-CNN Model",
        "authors": "Liang Zhao, Qiongfang Yu, Yi Yang",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiam57466.2022.00091"
    },
    {
        "id": 24654,
        "title": "Feasibility of Instrument Transformer Calibration using PMU Data based upon Innovation Approach",
        "authors": "Hemantkumar Goklani, Gopal Gajjar, S. A. Soman",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc57038.2022.10069078"
    },
    {
        "id": 24655,
        "title": "Small Sample Phase Selection Method Based on GASF and Swin-Transformer",
        "authors": "Liang Zhao, Qiongfang Yu",
        "published": "2022-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiam57466.2022.00065"
    },
    {
        "id": 24656,
        "title": "Multiple partial discharge source discrimination in a high voltage transformer winding",
        "authors": "N.H. Nik Ali, P. Rapisarda, P.L. Lewin",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/insucon.2017.8097209"
    },
    {
        "id": 24657,
        "title": "Effect of relationship between magnetic moment energy variation and knee point squareness ratio on transformer noise",
        "authors": "Chang-Hung Hsu",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "This paper presents the effects of the magnetostriction of core loss and noise (sound level) on power transformers between 15 and 120 MVA and the results of an analysis relative to the parameters of the squareness ratio (SR). Three types of soft magnetic silicon steel were used in this study: 30ZH105, 27ZH100, and 27PH100. The hysteresis loop and magnetostriction were measured for various magnetic parameters of sound level and core loss. Moreover, finite element analysis was performed to indicate the flux density and the parameter of ΔBr variation dependent on transformer loss and noise. The design values for transformers with several capacities, with respect to sound level and core loss performance were compared. Studies have indicated that transformer noise includes hysteresis and the volume of ferromagnetic materials. The sound level performance displayed a lower noise result because of the core with a higher SR. Besides, it can be observed that the material has innate magnetic anisotropy and the greater the magnetic moment flip energy then induced higher the noise. Additionally, it was observed that a higher SR. of 27ZH100 for core loss was not significant. When the SR. with respect to the sound level was high, the experimental and design values were very similar by using fast Fourier transform to observe the noise operated at a different frequency spectrum between 50 Hz and 10 kHz. The design values for transformers with varying capacities, with respect to the sound level and core loss performance were analyzed and compared with the experimental values.",
        "link": "http://dx.doi.org/10.1063/9.0000739"
    },
    {
        "id": 24658,
        "title": "Temperature dependent mechanical properties of pressboard in transformer winding structure",
        "authors": "Lakshitha Naranpanawe, Chandima Ekanayake, Tapan Saha",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceidp.2017.8257470"
    },
    {
        "id": 24659,
        "title": "A transformer based 60 GHz CMOS LNA for mm-wave applications",
        "authors": "",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22587/ajbas.2019.13.1.10"
    },
    {
        "id": 24660,
        "title": "Using Data from SCADA for Centralized Transformer Monitoring Applications",
        "authors": "Domagoj Peharda, Igor Ivanković, Nikola Jaman",
        "published": "2017",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.proeng.2017.09.695"
    },
    {
        "id": 24661,
        "title": "Transformers",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904.011"
    },
    {
        "id": 24662,
        "title": "Fuzzy control study on a transformer vibration isolation system",
        "authors": "Anxin Zou, Luwen Xu, Mi Zhu, Song Qi, Miao Yu",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2018.8407633"
    },
    {
        "id": 24663,
        "title": "Optimization of Filler Loading of Multi-Particle Mineral Oil Nanofluid for Transformer Insulation",
        "authors": "S Sarov Mohan, P Preetha",
        "published": "2020-7-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd46958.2020.9341838"
    },
    {
        "id": 24664,
        "title": "Improving Few-Shot Learning with Vision Transformer",
        "authors": "Di Qi",
        "published": "2022-4-1",
        "citations": 0,
        "abstract": "Abstract\nVision Transformer (ViT) is emerging as an alternative to convolutional neural network (CNN) for visual recognition and has achieved impressive results, however, due to its data-hungry nature, ViT encoder in few-shot setting remains rarely explored. In this paper, we first propose a simple yet effective baseline that exploits the complementarity of self-supervised learning (SSL) and ViT, enabling the use of standard ViT model as the few-shot learner. Second, based on the baseline, we introduce a novel regularized fine-tuning framework, where the Parametric Instance discrimination (PID) and Base-Novel (BN) regularizations are proposed to reduce the intra-class variance and calibrate the biased distribution of novel classes to further enhance few-shot recognition. We conduct extensive experiments and show that our method can achieve new state-of-the-art performances on two widely used benchmarks.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2253/1/012025"
    },
    {
        "id": 24665,
        "title": "Enhanced Context Learning with Transformer for Human Parsing",
        "authors": "Jingya Song, Qingxuan Shi, Yihang Li, Fang Yang",
        "published": "2022-8-4",
        "citations": 0,
        "abstract": "Human parsing is a fine-grained human semantic segmentation task in the field of computer vision. Due to the challenges of occlusion, diverse poses and a similar appearance of different body parts and clothing, human parsing requires more attention to learn context information. Based on this observation, we enhance the learning of global and local information to obtain more accurate human parsing results. In this paper, we introduce a Global Transformer Module (GTM) via a self-attention mechanism to capture long-range dependencies for effectively extracting context information. Moreover, we design a Detailed Feature Enhancement (DFE) architecture to exploit spatial semantics for small targets. The low-level visual features from CNN intermediate layers are enhanced by using channel and spatial attention. In addition, we adopt an edge detection module to refine the prediction. We conducted extensive experiments on three datasets (i.e., LIP, ATR, and Fashion Clothing) to show the effectiveness of our method, which achieves 54.55% mIoU on the LIP dataset, 80.26% on the average F-1 score on the ATR dataset and 55.19% on the average F-1 score on the Fashion Clothing dataset.",
        "link": "http://dx.doi.org/10.3390/app12157821"
    },
    {
        "id": 24666,
        "title": "Dual-Frequency Impedance Transformer with Ultra- High Impedance Transformation",
        "authors": "Rahul Gupta, Olzhas Shaikenov, Sabina Kairatova, Kassen Dautov, Mohammad Hashmi",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc46564.2019.9038459"
    },
    {
        "id": 24667,
        "title": "Flexible Power Transfer in Smart Transformer Interconnected Microgrids",
        "authors": "V M Hrishikesan, Chandan Kumar, Marco Liserre",
        "published": "2018-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2018.8591115"
    },
    {
        "id": 24668,
        "title": "Frequency-Split Inception Transformer for Image Super-Resolution",
        "authors": "Weibo Xu",
        "published": "2023-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3609703.3609708"
    },
    {
        "id": 24669,
        "title": "Semantic-aligned Fusion Transformer for One-shot Object Detection",
        "authors": "Yizhou Zhao, Xun Guo, Yan Lu",
        "published": "2022-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00745"
    },
    {
        "id": 24670,
        "title": "Model of an Air Transformer for Analyses of Wireless Power Transfer Systems",
        "authors": "Kalina Detka, Krzysztof Górecki, Przemysław Ptak",
        "published": "2023-1-30",
        "citations": 5,
        "abstract": "This article presents a new model of a dedicated air transformer for computer analyses of wireless power transfer systems. This model includes a form of subcircuit for SPICE. It takes into account the electric, magnetic and thermal properties of the modeled device. The form of the elaborated model is presented and the results of its experimental verification are shown. Some results from measurements and computations of an air transformer and a wireless power transfer system containing this transformer are shown and discussed. The structure of the tested system and the measuring setup used are also described. The results of measurements and computations illustrating the influence of the distance between the windings of the air transformer and the displacement between its windings on the output voltage of the power transfer system are presented and discussed. The influence of load resistance on the properties of the considered system is analyzed.",
        "link": "http://dx.doi.org/10.3390/en16031391"
    },
    {
        "id": 24671,
        "title": "Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection",
        "authors": "Ao Li, Yaqin Zhao, Zhaoxiang Zheng",
        "published": "2022-11-30",
        "citations": 14,
        "abstract": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
        "link": "http://dx.doi.org/10.3390/f13122032"
    },
    {
        "id": 24672,
        "title": "Transforming Scene Text Detection and Recognition: A Multi-Scale End-to-End Approach with Transformer Framework",
        "authors": "Tianyu Geng",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3375497"
    },
    {
        "id": 24673,
        "title": "Research on Transmission Characteristic of Electronic Current Transformer Shunt for Flexible HVDC",
        "authors": "Hongxing Wang, Shihao Zhou, Pandian Luo",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced.2018.8592239"
    },
    {
        "id": 24674,
        "title": "Transformer Based Multimodal Scene Recognition in Soccer Videos",
        "authors": "Yaozong Gan, Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmew56448.2022.9859304"
    },
    {
        "id": 24675,
        "title": "Temperature Monitoring System on Distribution Transformer with Web Thermal Camera",
        "authors": "Slamet Widodo, Didik Bayu Prasetya, Sri Anggraeni",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "<p><em>Temperature monitoring using a thermal camera is one type of light inspection that is used as an indicator of the health of a transformer. The system can detect the temperature of a transformer without having to stick directly to the surface of the transformer. The results of temperature measurements from the sensor are displayed in the form of images that can be displayed on a web. In this final project, a transformer temperature monitoring system is made that continues to work in real time to measure the temperature of the transformer. This is because changes in the temperature of the transformer can occur at any time. With this system, workers are expected to always monitor the condition of the transformer without having to make onsite visits. In this final project, ESP32 is used as a microcontroller and MLX90641 thermal camera. The use of a thermal camera in this system is caused by several advantages, namely the thermal camera can take the temperature of all parts of the transformer captured on the camera. In addition, the thermal camera has a range of up to seven meters. In this system, the temperature capture is set at the hottest point of all visible camera pixels. This system is also equipped with a notification system that will work when the temperature of the hamster transformer exceeds 60</em><em>℃</em><em>. The normal temperature threshold of the distribution transformer is 30</em><em>℃</em><em>-80</em><em>℃</em><em>. Or the maximum increase in one day of 40</em><em>℃</em><em> </em><em>from the initial temperature. The temperature of the transformer's hottest point will trigger a notification display in a web. So that after receiving a notification the officer can immediately know the temperature of the transformer and immediately take action. From the results of experiments that have been carried out, it is known that the results of measuring the temperature value of the thermometer and sensor have an accuracy rate of 98.8%. In addition, the system also works well, this is indicated by the color difference that occurs when the transformer temperature is in normal and hot conditions. So that from the image displayed, the officer can already know that the measured transformer is in normal or overheated conditions.</em></p>",
        "link": "http://dx.doi.org/10.32497/jaict.v8i1.4317"
    },
    {
        "id": 24676,
        "title": "Research on Error Evaluation of Capacitive Voltage Transformer Based on EMD-LSTM",
        "authors": "Huanhuan Fang",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisce58541.2023.10142695"
    },
    {
        "id": 24677,
        "title": "Phase-shifting transformer control and protection settings verification",
        "authors": "Bill Cook, Michael J. Thompson, Kamal Garg, Milind Malichkar",
        "published": "2018-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre.2018.8349815"
    },
    {
        "id": 24678,
        "title": "Molecular Representation Contrastive Learning Via Transformer Embedding to Graph Neural Networks",
        "authors": "liu yunwu, Ruisheng Zhang, Tongfeng Li, jing jiang, Jun Ma, Yongna Yuan, Ping Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4713442"
    },
    {
        "id": 24679,
        "title": "Advancement in Power Transformer Infrastructure and Digital Protection",
        "authors": "Nilesh Chothani, Maulik Raichura, Dharmesh Patel",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3870-4"
    },
    {
        "id": 24680,
        "title": "Author response for \"Transformer Fault Diagnosis based on MPA-RF Algorithm and LIF Technology\"",
        "authors": " Pengcheng Yan,  JingBao Wang,  Wenchang Wang,  Li Guodong,  Yuting Zhao,  Ziming Wen",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6501/ad0ad6/v2/response1"
    },
    {
        "id": 24681,
        "title": "STTR-3D: Stereo Transformer 3D Network for Video-Based Disparity Change Estimation",
        "authors": "QiTong Yang, Lionel Rakai, ShiJie Sun, HuanSheng Song, MingTao Feng, Naveed Akhtar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the field of computer vision and stereo disparity estimation, there has been little research in obtaining high accuracy disparity change maps from two-dimensional images. This map offers information that fills the gap between optical flow and depth which is desirable for numerous academic research problems and industrial applications, such as navigation systems, driving assistance, and autonomous systems. We introduce STTR3D, a 3D extension of the STereo TRansformer (STTR) which leverages transformers and an attention mechanism to handle stereo disparity change estimation. We further make use of the Scene Flow FlyingThings3D dataset which openly includes data for disparity change and apply 1) refinements through the use of MLP over relative position encoding and 2) regression head with an entropy-regularized optimal transport to obtain a disparity change map. This model consistently demonstrates superior performance for depth estimation as compared to the original model. In comparison to the existing supervised learning methods for stereo and depth estimation, our technique handles both depth estimation and the disparity change problem with an end-to-end network, also establishing that the addition of our transformer yields improved performance that achieves high precision for both problems. The open-source implementation of our work is available at https://github.com/Vp-SoLo/STTR3D.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2451146/v1"
    },
    {
        "id": 24682,
        "title": "Enhanced Rotary Transformer-Based Field Excitation System for Wound Rotor Synchronous Motor",
        "authors": "Josiah Haruna, Tsarafidy Raminosoa, Jonathan Wilkins",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2019.8912673"
    },
    {
        "id": 24683,
        "title": "Towards Transformer-Based Real-Time Object Detection at the Edge: A Benchmarking Study",
        "authors": "Colin Samplawski, Benjamin M. Marlin",
        "published": "2021-11-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/milcom52596.2021.9653052"
    },
    {
        "id": 24684,
        "title": "A Differential Transformer for Noninvasive Continuous Sodium Monitoring During Dialysis Treatment",
        "authors": "Marc Berger, Flora Sellering, Hannes Rohrich, Hussam Mansour, Thorsten Perl, Stefan Zimmermann",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sensors43011.2019.8956610"
    },
    {
        "id": 24685,
        "title": "Single-Channel Blind Image Separation Based on Transformer-Guided GAN",
        "authors": "Yaya Su, Dongli Jia, Yankun Shen, Lin Wang",
        "published": "2023-5-10",
        "citations": 0,
        "abstract": "Blind source separation (BSS) has been a great challenge in the field of signal processing due to the unknown distribution of the source signal and the mixing matrix. Traditional methods based on statistics and information theory use prior information such as source distribution independence, non-Gaussianity, sparsity, etc. to solve this problem. Generative adversarial networks (GANs) learn source distributions through games without being constrained by statistical properties. However, the current blind image separation methods based on GANs ignores the reconstruction of the structure and details of the separated image, resulting in residual interference source information in the generated results. This paper proposes a Transformer-guided GAN guided by an attention mechanism. Through the adversarial training of the generator and the discriminator, U-shaped Network (UNet) is used to fuse the convolutional layer features to reconstruct the structure of the separated image, and Transformer is used to calculate the position attention and guide the detailed information. We validate our method with quantitative experiments, showing that it outperforms previous blind image separation algorithms in terms of PSNR and SSIM.",
        "link": "http://dx.doi.org/10.3390/s23104638"
    },
    {
        "id": 24686,
        "title": "Power transformer demand forecast with Box Jenkins ARIMA model",
        "authors": "Özlem KUVAT, Ege ADALI",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "Demand forecasting is based on the principle of trying to forecast the demand for the outputs of enterprises in the field of manufacturing or service for the next periods. It requires the estimation of various future scenarios, if necessary, taking measures and taking steps, and during the application phase, the technique that is most suitable for the characteristics of the examined data set is selected and used. As a result of a healthy analysis carried out in this way, detailed plans and strict measures can be taken for the unknown, negative scenarios of the future.\nThis study analyzes the characteristics of a series of power transformers of a company operating in the electromechanical industry in the past years, and as a result of this analysis, the Box Jenkins Autoregressive Integrated Moving Average method (ARIMA), which best fits the results, is expected to occur for power transformers in the future. It was made to estimate the amount of demand.\nWithin the scope of this study, firstly, the most suitable model was tried to be determined by taking into consideration the past 132 months data of PTS. It was decided that the best choice among the alternative models was the ARMA (4,4) x (0,1) 12 model. The model was found to be stable and it was decided that the root mean square error (RMSE), mean absolute percentage error (MAPE) and Theil inequality coefficient values determined in the performance measurements were appropriate.",
        "link": "http://dx.doi.org/10.31593/ijeat.771010"
    },
    {
        "id": 24687,
        "title": "Guided Local Feature Matching with Transformer",
        "authors": "Siliang Du, Yilin Xiao, Jingwei Huang, Mingwei Sun, Mingzhong Liu",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "GLFNet is proposed to be utilized for the detection and matching of local features among remote-sensing images, with existing sparse feature points being leveraged as guided points. Local feature matching is a crucial step in remote-sensing applications and 3D reconstruction. However, existing methods that detect feature points in image pairs and match them separately may fail to establish correct matches among images with significant differences in lighting or perspectives. To address this issue, the problem is reformulated as the extraction of corresponding features in the target image, given guided points from the source image as explicit guidance. The approach is designed to encourage the sharing of landmarks by searching for regions in the target image with features similar to the guided points in the source image. For this purpose, GLFNet is developed as a feature extraction and search network. The main challenge lies in efficiently searching for accurate matches, considering the massive number of guided points. To tackle this problem, the search network is divided into a coarse-level match network-based guided point transformer that narrows the search space and a fine-level regression network that produces accurate matches. The experimental results on challenging datasets demonstrate that the proposed method provides robust matching and benefits various applications, including remote-sensing image registration, optical flow estimation, visual localization, and reconstruction registration. Overall, a promising solution is offered by this approach to the problem of local feature matching in remote-sensing applications.",
        "link": "http://dx.doi.org/10.3390/rs15163989"
    },
    {
        "id": 24688,
        "title": "Efficiency of Rock Destruction by a Pulse Generator Based on a Linear Pulse Transformer",
        "authors": "D. Molchanov, I. Lavrinovich",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ppps34859.2019.9009799"
    },
    {
        "id": 24689,
        "title": "Depth-Based 6DoF Object Pose Estimation Using Swin Transformer",
        "authors": "Zhujun Li, Ioannis Stamos",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342215"
    },
    {
        "id": 24690,
        "title": "Testing IEC-61850 Sampled Values-Based Transformer Differential Protection Scheme",
        "authors": "Mohit Sharma, Lam Nguyen, Sughosh Kuber, Dinesh Baradi",
        "published": "2021-3-22",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cpre48231.2021.9429855"
    },
    {
        "id": 24691,
        "title": "Chapitre 5. Télétravail : dépasser ses paradoxes et transformer la crise en opportunité",
        "authors": "Chantal Fuhrer",
        "published": "2021-12-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3917/ems.kalik.2021.01.0087"
    },
    {
        "id": 24692,
        "title": "Current Transformer Accuracy Improvement by Digital Compensation Technique",
        "authors": "Makarand Sudhakar Ballal, Manish Ganesh Wath",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12647-019-00304-0"
    },
    {
        "id": 24693,
        "title": "Transformer fault analysis using instantaneous symmetrical components",
        "authors": "Daniil I. Ivanchenko, Belsky A. Aleksey",
        "published": "2018-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eiconrus.2018.8317178"
    },
    {
        "id": 24694,
        "title": "Transformer-Based De Novo Peptide Sequencing for Data-Independent Acquisition Mass Spectrometry",
        "authors": "Shiva Ebrahimi, Xuan Guo",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibe60311.2023.00013"
    },
    {
        "id": 24695,
        "title": "Adaptive Hyperspectral Siamese Network in Transformer",
        "authors": "Chang Liu, Jiawei Zhou, Yanni Dong",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/whispers61460.2023.10431291"
    },
    {
        "id": 24696,
        "title": "Transformer Neural Networks, Information Biology, and Alzheimer’s Disease",
        "authors": "Melanie Swan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractBackgroundAlzheimer’s disease affects one in ten people older than 65 years, with the prevalence expected to triple by 2030, together with commensurate increases in cost. A comprehensive causal model of Alzheimer’s disease etiology is lacking despite more than three decades of study. Modern computational methods provide a new approach to multiscalar biosystems by modeling neuropathology in its native three‐dimensional character in topological biophysics and quantum computational models.MethodThis work describes how an emerging standard in machine learning, responsible for the AlphaFold (protein folding structure) and Gato generalist agent projects, transformer neural networks, may be applied to the study of Alzheimer’s disease per proven results in bot text chat, image recognition and creation, video generation, software programming, and autonomy in robotics and driving. The self‐attention and self‐learning mechanisms of transformer neural networks are discussed in application to multifaceted Alzheimer’s disease data sets involving neuroimaging scans, whole human genome data, and blood and CSF biomarker data.ResultA machine learning‐based topological biophysics model of Alzheimer’s disease neuropathology hypothesis‐testing (misfolded proteins, gene expression, diabetes type‐3, and peptide immune system stimulation) is described.ConclusionThere is an opportunity to apply state‐of‐the‐art transformer neural network machine learning techniques to the automated analysis of large accruing data stores of Alzheimer’s information (imaging, genomic, proteomic, and self‐reported data) towards the causal understanding of the neuropathology.",
        "link": "http://dx.doi.org/10.1002/alz.071502"
    },
    {
        "id": 24697,
        "title": "Paraphrase Generation Model Using Transformer Based Architecture",
        "authors": "Mosima Anna Masethe, Hlaudi Daniel Masethe, Sunday Olusegun Ojo, Pius A Owolawi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683780"
    },
    {
        "id": 24698,
        "title": "Gex'ez-English Bi-Directional Neural Machine Translation Using Transformer",
        "authors": "Sefineh Getachew, Yirga Yayeh",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ict4da59526.2023.10302254"
    },
    {
        "id": 24699,
        "title": "Vision Transformer in Oral Cancer Detection",
        "authors": "Pinky Agarwal, Naman Gupta, Yashita Bharadwaj, Anju Yadav, Pratistha Mathur",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icssas57918.2023.10331857"
    },
    {
        "id": 24700,
        "title": "Study of the Transformer Rectifier Unit Compatible with the Concept of a More Electric Aircraft",
        "authors": "Lucjan Setlak, Rafal Kowalik",
        "published": "2018-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paee.2018.8441100"
    },
    {
        "id": 24701,
        "title": "Investigation Of Electromagnetic Forces In Converter Transformer",
        "authors": "Jayesh U. Kothavade, Prasanta Kundu",
        "published": "2021-12-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/stpec52385.2021.9718676"
    },
    {
        "id": 24702,
        "title": "MM-HAT: Transformer for Millimeter-Wave Sensing Based Human Activity Recognition",
        "authors": "Jie Yan, Xianlin Zeng, Anfu Zhou, Huadong Ma",
        "published": "2022-12-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10000673"
    },
    {
        "id": 24703,
        "title": "Research on a Multi-line Electromagnetic Sen Transformer Suitable for Distribution Network",
        "authors": "Shunkai Xu, Jiaxin Yuan, Shan Yin",
        "published": "2020-10-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce44975.2020.9235478"
    },
    {
        "id": 24704,
        "title": "An Early Prediction Model on Systemic Risk Under Global Risk: Using Finbert and Temporal Fusion Transformer to Multimodal Data Fusion Framework",
        "authors": "XIAO JIN, ShuLing Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706654"
    },
    {
        "id": 24705,
        "title": "Decoder Transformer for Temporally-Embedded Health Outcome Predictions",
        "authors": "Omar Boursalie, Reza Samavi, Thomas E. Doyle",
        "published": "2021-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00235"
    },
    {
        "id": 24706,
        "title": "3D-VRVT: 3D Voxel Reconstruction from A Single Image with Vision Transformer",
        "authors": "Xi Li, Ping Kuang",
        "published": "2021-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccst53801.2021.00078"
    },
    {
        "id": 24707,
        "title": "Molecular Graph Transformer: Stepping Beyond ALIGNN Into Long-Range Interactions",
        "authors": "Marco Anselmi, Greg Slabaugh, Rachel Crespo-Otero, Devis Di Tommaso",
        "published": "No Date",
        "citations": 0,
        "abstract": "Graph Neural Networks (GNNs) have revolutionized material property prediction by learning directly from the structural information of molecules and materials. However, conventional GNN models rely solely on local atomic interactions, such as bond lengths and angles, neglecting crucial longrange electrostatic forces that aect certain properties. To address this, we introduce the Molecular Graph Transformer (MGT), a novel GNN architecture that combines local attention mechanisms with message passing on both bond graphs and their line graphs, explicitly capturing long-range interactions. Benchmarking on MatBench and Quantum MOF (QMOF) datasets demonstrates that MGT's improved understanding of electrostatic interactions signicantly enhances the prediction accuracy of properties like exfoliation energy and refractive index, while maintaining state-of-theart performance on all other properties. This breakthrough paves the way for the development of highly accurate and efficient materials design tools across diverse applications. Code is available at: https://github.com/MolecularGraphTransformer/MGT",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2024-rxwbc"
    },
    {
        "id": 24708,
        "title": "Context-Aware Hierarchical Transformer for Fine-Grained Video-Text Retrieval",
        "authors": "Mingliang Chen, Weimin Zhang, Yurui Ren, Ge Li",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897206"
    },
    {
        "id": 24709,
        "title": "Potential cognitive risks of generative transformer-based AI chatbots on higher order executive functions.",
        "authors": "Umberto León-Domínguez",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/neu0000948"
    },
    {
        "id": 24710,
        "title": "Transformer insulation structure for dielectric liquids with higher permittivity",
        "authors": "A. Sbravati, K. Rapp, P. Schmitt, Ch. Krause",
        "published": "2017-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl.2017.8124705"
    },
    {
        "id": 24711,
        "title": "Semi-supervised EEG emotion recognition with Discriminative Graph Transformer Model",
        "authors": "Dae Hyeon Kim, Young-Seok Choi",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-asia59966.2023.10326427"
    },
    {
        "id": 24712,
        "title": "A transformer-less modular multilevel DC-DC converter with DC fault blocking capability",
        "authors": "Fei Zhang, Geza Joos, Wei Li",
        "published": "2017-12",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec.2017.8333576"
    },
    {
        "id": 24713,
        "title": "Black-Box Power Transformer Winding Model",
        "authors": "M. F. Horvat, Z. Jurkovic, B. Jurisic, T. Zupan, B. Cucic",
        "published": "2022-10-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/arwtr54586.2022.9959949"
    },
    {
        "id": 24714,
        "title": "Study on Artificial Intelligence Approaches for Power Transformer Health Index Assessment",
        "authors": "Dhanu Rediansyah, Rahman Azis Prasojo,  Suwarno",
        "published": "2021-10-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceei52609.2021.9611109"
    },
    {
        "id": 24715,
        "title": "A transformer-based model for effective and exportable IoMT-based stress detection",
        "authors": "Moudy Sharaf Alshareef, Badraddin Alturki, Mona Jaber",
        "published": "2022-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001083"
    },
    {
        "id": 24716,
        "title": "Machine Learning Based Online Monitoring of Step-Up Transformer Assets in Electrical Generating Stations",
        "authors": "Julia Penfield, Matt Holland",
        "published": "2021-6-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsmartgrid52357.2021.9551264"
    },
    {
        "id": 24717,
        "title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation",
        "authors": "Chenze Shao, Zhengrui Ma, Yang Feng",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.322"
    },
    {
        "id": 24718,
        "title": "Flexible and Innovative Transformer Technologies (DOE)",
        "authors": "",
        "published": "2024-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/fgc.33557"
    },
    {
        "id": 24719,
        "title": "Breakdown Characteristics of Printed Circuit Board Based Transformer Windings",
        "authors": "R. E. P. Frost, P. L. Lewin",
        "published": "2022-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipmhvc51093.2022.10099413"
    },
    {
        "id": 24720,
        "title": "Universal Graph Transformer Self-Attention Networks",
        "authors": "Dai Quoc Nguyen, Tu Dinh Nguyen, Dinh Phung",
        "published": "2022-4-25",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3487553.3524258"
    },
    {
        "id": 24721,
        "title": "Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation",
        "authors": "Shuta Taniguchi, Tsuneo Kato, Akihiro Tamura, Keiji Yasuda",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-448"
    },
    {
        "id": 24722,
        "title": "Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene",
        "authors": "Soonshin Seo, Donghyun Lee, Ji-Hwan Kim",
        "published": "2021-8-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1308"
    },
    {
        "id": 24723,
        "title": "DFDT: An End-to-End DeepFake Detection Framework Using Vision Transformer",
        "authors": "Aminollah Khormali, Jiann-Shiun Yuan",
        "published": "2022-3-14",
        "citations": 14,
        "abstract": "The ever-growing threat of deepfakes and large-scale societal implications has propelled the development of deepfake forensics to ascertain the trustworthiness of digital media. A common theme of existing detection methods is using Convolutional Neural Networks (CNNs) as a backbone. While CNNs have demonstrated decent performance on learning local discriminative information, they fail to learn relative spatial features and lose important information due to constrained receptive fields. Motivated by the aforementioned challenges, this work presents DFDT, an end-to-end deepfake detection framework that leverages the unique characteristics of transformer models, for learning hidden traces of perturbations from both local image features and global relationship of pixels at different forgery scales. DFDT is specifically designed for deepfake detection tasks consisting of four main components: patch extraction & embedding, multi-stream transformer block, attention-based patch selection followed by a multi-scale classifier. DFDT’s transformer layer benefits from a re-attention mechanism instead of a traditional multi-head self-attention layer. To evaluate the performance of DFDT, a comprehensive set of experiments are conducted on several deepfake forensics benchmarks. Obtained results demonstrated the surpassing detection rate of DFDT, achieving 99.41%, 99.31%, and 81.35% on FaceForensics++, Celeb-DF (V2), and WildDeepfake, respectively. Moreover, DFDT’s excellent cross-dataset & cross-manipulation generalization provides additional strong evidence on its effectiveness.",
        "link": "http://dx.doi.org/10.3390/app12062953"
    },
    {
        "id": 24724,
        "title": "Visualizing Paired Image Similarity in Transformer Networks",
        "authors": "Samuel Black, Abby Stylianou, Robert Pless, Richard Souvenir",
        "published": "2022-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv51458.2022.00160"
    },
    {
        "id": 24725,
        "title": "A Three-Phase Adjustable-Voltage-Ratio Transformer Based on Magnetic Flux Valves",
        "authors": "Junwei Cui, Liyan Qu, Wei Qiao",
        "published": "2018-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2018.8558375"
    },
    {
        "id": 24726,
        "title": "A Study of Vision Transformer for Lung Diseases Classification",
        "authors": "Manh-Hung Nguyen, Khai Ngo Quang",
        "published": "2022-7-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gtsd54989.2022.9989100"
    },
    {
        "id": 24727,
        "title": "WITHDRAWN: Long-Term Prediction of Network Security Situation Through the Use of the Transformer-Based Model",
        "authors": "Kun Yin, Yu Yang, Chengpeng Yao, Jinwei Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn recent years, the Internet has shown rapid development, and network security issue has gradually become the focus of research by scholars and enterprises. Network security time series is a reliable source to obtain future network security situation, so as to develop network security defense strategy by exploring the correlation of time series. The network security time series is a reliable source to obtain the future network security situation, and it is the main direction of current network security defense by exploring the correlation of time series, and analyzing the future network security situation so as to formulate network security defense strategies. This is the main direction of network security defense. The existing research focuses on the short-term prediction of network attacks, and the robustness and accuracy of long-term prediction still have big problems. To fuse the information from different data sources and capture the correlation between sequences, we design a data source selection module based on the similarity of measurement curves. We then model the network security situation prediction based on deep learning models and propose a situation prediction model based on Temporal Convolutional Network (TCN)-combined Transformer, which focuses on the time series long-term prediction problem, combining the network condition and attack situation to obtain the future network security situation. Our proposed model is divided into three parts, which are the information encoding module, the information synthesis module, and situation value calculation and prediction accuracy evaluation module. The selected multi-dimensional situations element data are used as model input, and the TCN-combined Transformer is employed as the network security situational data processing unit to complete the information fusion and prediction tasks. Finally, the role of data source selection on prediction accuracy is evaluated using an ablation study. We experimented and evaluated the model at different prediction horizon lengths using five existing baseline models and three performance metrics. The experimental results show that our proposed prediction model has better robustness and accuracy in most of the metrics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1553264/v1"
    },
    {
        "id": 24728,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 24729,
        "title": "Video Frame Interpolation Transformer",
        "authors": "Zhihao Shi, Xiangyu XU, Xiaohong Liu, Jun Chen, Ming-Hsuan Yang",
        "published": "2022-6",
        "citations": 57,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.01696"
    },
    {
        "id": 24730,
        "title": "500kVA Hybrid Solid State Transformer (HSST): Modelling and Control",
        "authors": "Sanjay Rajendran, Soumik Sen, Zhicheng Guo, Alex Q. Huang",
        "published": "2021-10-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce47101.2021.9595105"
    },
    {
        "id": 24731,
        "title": "Multi-scale feature fusion pedestrian detection algorithm based on Transformer",
        "authors": "Huan Chen, Xiaoming Guo",
        "published": "2023-5-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166718"
    },
    {
        "id": 24732,
        "title": "Metro Ridership Forecasting using Inter-Station-Aware Transformer Networks",
        "authors": "Khaled Saleh, Adriana-Simona Mihaita, Yuming Ou",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421998"
    },
    {
        "id": 24733,
        "title": "Hybrid Convolution-Transformer for Lightweight Single Image Super-Resolution",
        "authors": "Jiuqiang Li, Yutong Ke",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446977"
    },
    {
        "id": 24734,
        "title": "ViViT: A Video Vision Transformer",
        "authors": "Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid",
        "published": "2021-10",
        "citations": 810,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00676"
    },
    {
        "id": 24735,
        "title": "Cure Models",
        "authors": "Catherine Legrand",
        "published": "2021-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429054167-4"
    },
    {
        "id": 24736,
        "title": "Frailty Models",
        "authors": "Catherine Legrand",
        "published": "2021-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429054167-3"
    },
    {
        "id": 24737,
        "title": "A Novel Action Transformer Network for Hybrid Multimodal Sign Language Recognition",
        "authors": "Sameena Javaid, Safdar Rizvi",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.031924"
    },
    {
        "id": 24738,
        "title": "Influence of Arrester Parameters on Overvoltage Characteristics on Protected Transformer",
        "authors": "Abugalia A",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4172/2332-0796.1000284"
    },
    {
        "id": 24739,
        "title": "Review on Health Care Monitor of Substation Transformer with Overload Protection",
        "authors": "Rajesh Kudale, Rakesh Shriwatava, Somnath Hadpe, Shridhar Khule",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4043565"
    },
    {
        "id": 24740,
        "title": "Transformer Neural Network for Structure Constrained Molecular Optimization",
        "authors": "Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, eva nittinger, Christian Tyrchan, Werngard Czechtizky",
        "published": "No Date",
        "citations": 3,
        "abstract": "Finding molecules with a desirable balance of multiple properties is a main challenge in drug discovery. Here, we focus on the task of molecular optimization, where a starting molecule with promising properties needs to be further optimized towards the desirable properties. Typically, chemists would apply chemical transformations to the starting molecule based on their intuition. A widely used strategy is the concept of matched molecular pairs where two molecules differ by a single transformation. In particular, a chemist would be interested in keeping one part of the starting molecule (core) constant, while substituting the other part (R-group), to optimize the starting molecule towards desirable properties. Motivated by this, we train a Transformer model, Transformer-R, to generate R-groups given the starting molecule (with its core and R-group specified) and the specified desirable properties. The generated R-groups will be attached to the core to form the final molecules, which are guaranteed to keep the core of interest and are expected to satisfy the desirable properties in the input. Our model could accelerate the process of optimizing antiviral drug candidates in terms of various properties of interest, e.g. pharmacokinetics.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14416133.v1"
    },
    {
        "id": 24741,
        "title": "VOLTAGE REGULATED DISTRIBUTION TRANSFORMER WITH NEW VACUUM OLTC",
        "authors": "S. Mikulić, B. Ćućić",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.1207"
    },
    {
        "id": 24742,
        "title": "Multiport Energy Router for Satellite Based on High-Frequency Transformer",
        "authors": "Kang Qing, Zhang Xuan, Xing Jie, Li Feng, Shi Haiping",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/espc.2019.8932025"
    },
    {
        "id": 24743,
        "title": "COMPUTER SIMULATION MODEL OF THE TRACTION TRANSFORMER OF THE AC ELECTRIC LOCOMOTIVE OF THE O'ZBEKISTON SERIES",
        "authors": "Tulagan, Nazirkhonov, Rakhimbergan Polvonov",
        "published": "2022-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32743/26870142.2022.21.244.341834"
    },
    {
        "id": 24744,
        "title": "Transformer-based visual inspection algorithm for surface defects",
        "authors": "Qinmiao Zhu, Jingyun Chang, Teng Liu, Yuhui Wang, Hua Yang",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.oe.62.9.094102"
    },
    {
        "id": 24745,
        "title": "INTERFACING OF WIND ENERGY CONVERSION SYSTEM WITH SOLID STATE TRANSFORMER FOR SEAMLESS FAULT RIDE THROUGH OPERATION",
        "authors": "",
        "published": "2020-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31838/jcr.07.06.156"
    },
    {
        "id": 24746,
        "title": "Advanced Leakage Inductance Model for Transformer Transient Simulation",
        "authors": "Alejandro Avendano, Bruce A. Mork, Dmitry Ishchenko, Francisco Gonzalez",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2018.8586015"
    },
    {
        "id": 24747,
        "title": "Study on surface crack detection of low voltage current transformer",
        "authors": "Gao Xiaozheng, Lei Huan, Zhou Guangbin, Yang Wenjie",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2017.7978598"
    },
    {
        "id": 24748,
        "title": "Simulation Modeling of Linear Phase-shifting Transformer Inverter System",
        "authors": "Hou Xinguo, Bi Min",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciba50161.2020.9277276"
    },
    {
        "id": 24749,
        "title": "Transformer with sequence relative position for continuous sign language translation",
        "authors": "Jiwei Hu, Lian Ni",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2637117"
    },
    {
        "id": 24750,
        "title": "Mitigating Electrical Disturbances with Hybrid Distribution Transformer",
        "authors": "Fajer Alelaj, Mohamed Dahidah, Haris Patsios",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Hybrid transformers (HT) have the advantages of the conventional transformer, the regulatory abilities of power electronic converters, and reduce the impact of the grid. The impacts of the existing grid are voltage sag, voltage swell, harmonic distortion, and voltage unbalanced. The power electronic converter has a controllable advantage such as regulating the voltage and can transfer only a fraction of the power. The aim of the paper is to augment the conventional power distribution transformer with a partially rated power electronic module to enhance flexibility and introduce new features to the distribution transformer. In this paper, the proposed back-to-back converter included an active front rectifier and a modular multilevel converter (MMC) was simulated by MATLAB/Simulink software. The proposed back-to-back converter was used at the primary side of the distribution transformer to compensate for the voltage sag and swell issues. The simulation results were obtained under different conditions such as various supply voltages and various loads. Hence, the proposed system has the ability to regulate the output voltage under various conditions with ±10%.",
        "link": "http://dx.doi.org/10.5121/eeij.2023.10101"
    },
    {
        "id": 24751,
        "title": "Study Of Measurements Leakage Current In Oil Transformer",
        "authors": "Hassan J. Mohammed.",
        "published": "2020-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24237/djes.2020.13101"
    },
    {
        "id": 24752,
        "title": "Transformer Safety Monitoring System",
        "authors": "V. Gomathi, D. P. Suryaprakash, R Srimathi, S. Uma, R. Valarmathi, V. Sudha",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon55714.2022.9972460"
    },
    {
        "id": 24753,
        "title": "Microcontroller based differential relay using fuzzy logic for transformer protection",
        "authors": "Madhura S. Deshmukh, V. T. Barhate",
        "published": "2017-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccons.2017.8250556"
    },
    {
        "id": 24754,
        "title": "Multi-inverter drive with symmetrical multilevel winding voltage of transformer during overmodulation",
        "authors": "Valentin Oleschuk, Vladimir Ermuratskii",
        "published": "2017-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iseee.2017.8170638"
    },
    {
        "id": 24755,
        "title": "Sound signal-based transformer operation status monitoring system",
        "authors": "Haifeng Xu, Jidong Cai, Long Xu, Fei Xu",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135233"
    },
    {
        "id": 24756,
        "title": "Investigation of Persea Americana Oil as an Alternative Transformer Insulation Oil",
        "authors": "Benard M. Makaa, George K. Irungu, David K. Murage",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic43217.2019.9046539"
    },
    {
        "id": 24757,
        "title": "Experimental Analysis of Some Natural Esters and Their Mixtures for Transformer Application",
        "authors": "Sachin Vishwakarma, Ranjana Singh",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uemgreen46813.2019.9221564"
    },
    {
        "id": 24758,
        "title": "A Modified Transformer Neural Network (MTNN) for Robust Intrusion Detection in IoT Networks",
        "authors": "Syed Wahaj Ahmed, Fabio Kientz, Rasha Kashef",
        "published": "2023-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itc-egypt58155.2023.10206134"
    },
    {
        "id": 24759,
        "title": "Analysis of Transformer Vibration Signal",
        "authors": "Keqi Ma",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2991/fmsmt-17.2017.204"
    },
    {
        "id": 24760,
        "title": "Auto-transformer-based power amplifier with totem-pole driver",
        "authors": "Elena Sobotta, Robert Wolf, Frank Ellinger",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssd.2017.8166916"
    },
    {
        "id": 24761,
        "title": "Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer",
        "authors": "Honglin Mu, Wentian Xia, Wanxiang Che",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00193"
    },
    {
        "id": 24762,
        "title": "Life management concepts of transformer investigations",
        "authors": "K Vinoth Kumar, Nithin Matthew Sam, Kevin Sony",
        "published": "2018-3-19",
        "citations": 0,
        "abstract": "This paper shows the importance of specifications and life management concepts for reliable, uninterrupted and intended application of transformers throughout the life cycle. The transformers are tested, manufactured and designed based on the specification.",
        "link": "http://dx.doi.org/10.14419/ijet.v7i2.8.10414"
    },
    {
        "id": 24763,
        "title": "Integrated Busbar, Transformer And Feeder Backup Protection Based On Wide Area Voltage Measurements",
        "authors": "Patrick T. Manditereza",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sges51519.2020.00030"
    },
    {
        "id": 24764,
        "title": "Learning English to Chinese Character: Calligraphic Art Production based on Transformer",
        "authors": "Yifan Jin, Yi Zhang, Xi Yang",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3476124.3488642"
    },
    {
        "id": 24765,
        "title": "Transformer Based Image-Text Consistency Analysis for Infographic Articles",
        "authors": "Yuwei Chen, Ming-Ching Chang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mipr59079.2023.00023"
    },
    {
        "id": 24766,
        "title": "Transient Modeling and Switching Logic Analysis of a Power-Electronic-Assisted Oltc Based Sen Transformer",
        "authors": "Wei Li, Song Han, Xi Guo, Shufan Xie, Na Rong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4521918"
    },
    {
        "id": 24767,
        "title": "Transformer Neural Network-Based Molecular Optimization Using General Transformations",
        "authors": "Jiazhen He, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Atanas Patronov, Esben Jannik Bjerrum, Ola Engkvist",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMolecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug discovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the large chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimicking the chemist's intuition in terms of matched molecular pairs (MMPs). Although MMPs is a typical and widely used strategy by medicinal chemists, it offers limited capability in terms of exploring the space of solutions. There are more options to modify a starting molecule to achieve desirable properties,  e.g.   one can simultaneously modify the molecule at different places including changing the scaffold. This study trains the same Transformer architecture on different datasets. These datasets consist of a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets reflecting general transformations are constructed from ChEMBL based on two approaches: Tanimoto similarity (allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold constant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while keeping the same model architecture. Our results show that the models trained on differently prepared datasets transform a given starting molecule in a way that it reflects the nature of the dataset used for training the model. These models could complement each other and  unlock the capability for the chemists to pursue different options for improving a starting molecule.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1097104/v1"
    },
    {
        "id": 24768,
        "title": "Thermal Modes Analysis of Operation of Transformer with Rotating Magnetic Field",
        "authors": "Elena Limonnikova, Alexander Cherevko, Sergey Platonenkov",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uralcon.2019.8877610"
    },
    {
        "id": 24769,
        "title": "Push–Pull Converter Transformer Maximum Efficiency Optimization",
        "authors": "Jan Martis",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-65960-2_27"
    },
    {
        "id": 24770,
        "title": "Entretien: Ibrahim Thiaw, conseiller spécial pour le Sahel, Transformer le Sahel en une terre d’opportunités",
        "authors": "Ibrahim Thiaw",
        "published": "2018-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/e70e3833-fr"
    },
    {
        "id": 24771,
        "title": "Review on Health Care Monitor of Substation Transformer with Overload Protection",
        "authors": "Rajesh Kudale, Rakesh Shriwatava, Somnath Hadpe, Shridhar Khule",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4074622"
    },
    {
        "id": 24772,
        "title": "A Hybrid CNN-Transformer Architecture for Semantic Segmentation of Radar Sounder data",
        "authors": "Raktim Ghosh, Francesca Bovolo",
        "published": "2022-7-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883124"
    },
    {
        "id": 24773,
        "title": "Hierarchical volumetric transformer with comprehensive attention for medical image segmentation",
        "authors": "Zhuang Zhang, Wenjie Luo",
        "published": "2022",
        "citations": 0,
        "abstract": "<abstract>\n <p>Transformer is widely used in medical image segmentation tasks due to its powerful ability to model global dependencies. However, most of the existing transformer-based methods are two-dimensional networks, which are only suitable for processing two-dimensional slices and ignore the linguistic association between different slices of the original volume image blocks. To solve this problem, we propose a novel segmentation framework by deeply exploring the respective characteristic of convolution, comprehensive attention mechanism, and transformer, and assembling them hierarchically to fully exploit their complementary advantages. Specifically, we first propose a novel volumetric transformer block to help extract features serially in the encoder and restore the feature map resolution to the original level in parallel in the decoder. It can not only obtain the information of the plane, but also make full use of the correlation information between different slices. Then the local multi-channel attention block is proposed to adaptively enhance the effective features of the encoder branch at the channel level, while suppressing the invalid features. Finally, the global multi-scale attention block with deep supervision is introduced to adaptively extract valid information at different scale levels while filtering out useless information. Extensive experiments demonstrate that our proposed method achieves promising performance on multi-organ CT and cardiac MR image segmentation.</p>\n </abstract>",
        "link": "http://dx.doi.org/10.3934/mbe.2023149"
    },
    {
        "id": 24774,
        "title": "Einleitung",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904.001"
    },
    {
        "id": 24775,
        "title": "Standard schemes and engineering cases of transformer noise reduction for typical distribution room in residential area",
        "authors": "J. Luo, K. Zhang, S. Chen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2069"
    },
    {
        "id": 24776,
        "title": "Intercell Transformer Coupled Buck Converter in One-of-Three Rectifier",
        "authors": "Yuxiang Shi, Jing Xu, Goran Mandic, Sandeep Bala",
        "published": "2020-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce44975.2020.9236027"
    },
    {
        "id": 24777,
        "title": "Study of High Frequency Rotary Transformer Structures for Contactless Inductive Power Transfer",
        "authors": "Xu ZU, Quan JIANG",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8921812"
    },
    {
        "id": 24778,
        "title": "Phrase Grounding Algorithm Based on Transformer Multilevel Feature Fusion",
        "authors": "Xiangdong Meng, Juxiang zhou, Jianhou Gan, Jun Wang, Ken Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4289646"
    },
    {
        "id": 24779,
        "title": "Fast Calculation of Hot Spot Temperature of Onan Transformer Based on Rational Fraction Regression Model",
        "authors": "Ruiyang Xiao, Dongping Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4116248"
    },
    {
        "id": 24780,
        "title": "Improved scheme based on memory voltage for transformer differential protection considering the effects of PLL",
        "authors": "Tao Zheng, Ruozhu Zhang, Ying Chen, jingwen Ai, yilin Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "Phase-locked loop (PLL) technique is a critical control module applied\nin photovoltaic (PV) grid-connected control system to synchronize with\ngrid voltage. The PLL error directly affects the response characteristic\nof PV grid-connected inverter under faults, leading to a mass of\nharmonic components in the output current of the PV inverter. Especially\nin the scenario of short-circuit fault with deep voltage sag, the PLL\ndetection error is too significant to be ignored, posing challenges to\ntransformer differential protection. This paper proposes an improved\nscheme that utilizes a digital computer algorithm to record and store\npre-fault voltage, aiming to address the complexity and implementation\nchallenges for phase-lock error in practical engineering applications.\nBy utilizing the memory voltage, the PLL detection error is eliminated,\nmitigating harmonic distortion in the PV output current and ensuring the\nreliability of the transformer differential protection. However,\nintroducing memory voltage may increase the short-circuit current of the\nPV output. Therefore, it is recommended to reduce the amplitude of the\nshort-circuit current by multiplying the inverter port voltage reference\nvalue by an appropriate limiting coefficient k and outputting it to the\nphysical system. Finally, the effectiveness of the proposed scheme is\nverified through MATLAB/Simulink simulation.",
        "link": "http://dx.doi.org/10.22541/au.169081517.70776459/v1"
    },
    {
        "id": 24781,
        "title": "Local-constraint transformer network for stock movement prediction",
        "authors": "Jincheng Hu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcse.2021.10039986"
    },
    {
        "id": 24782,
        "title": "Simulation Research on Full-scale Transformer Water Fire Extinguishing System",
        "authors": "Yu Liu, Bi-chen Pan, Bao-hui Chen",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced56215.2022.9928881"
    },
    {
        "id": 24783,
        "title": "Prompt Engineering for Transformer-Based Chemical Similarity Search Identifies Structurally Distinct Functional Analogues",
        "authors": "Clayton  Walter Kosonocky, Aaron  L. Feller, Claus  O. Wilke, Andrew  D. Ellington",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4458489"
    },
    {
        "id": 24784,
        "title": "Using electromagnetic waves for mechanical defects monitoring",
        "authors": "Gevork B. Gharehpetian, Hossein Karami, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00009-4"
    },
    {
        "id": 24785,
        "title": "Three-Phase Air-Core Rotary Transformer with Halbach AC Windings for Wound-Field Motors",
        "authors": "Masahiro Aoyama",
        "published": "2022-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ipec-himeji2022-ecce53331.2022.9806914"
    },
    {
        "id": 24786,
        "title": "Vocoder-free End-to-End Voice Conversion with Transformer Network",
        "authors": "June-Woo Kim, Ho-Young Jung, Minho Lee",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207653"
    },
    {
        "id": 24787,
        "title": "Unsupervised Full Transformer for Pose, Depth and Optical Flow Joint Learning",
        "authors": "Xiaochen Liu, Tao Zhang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacre58689.2023.10208360"
    },
    {
        "id": 24788,
        "title": "Multidomain transformer-based deep learning for early detection of network intrusion",
        "authors": "Jinxin Liu, Murat Simsek, Michele Nogueira, Burak Kantarci",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436976"
    },
    {
        "id": 24789,
        "title": "Percevoir et transformer la frontière. L’œuvre de Günter Grass comme processus de borderscaping",
        "authors": "Dorothée Cailleux",
        "published": "2017-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/rg.284"
    },
    {
        "id": 24790,
        "title": "Large Power Transformer Overload Detection using Sound Analysis",
        "authors": "Nguyen Cong-Phuong",
        "published": "2021-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3483207.3483230"
    },
    {
        "id": 24791,
        "title": "TSVT: Token Sparsification Vision Transformer for RGB-D Salient Object Detection",
        "authors": "Lina Gao, Bing Liu, Ping Fu, Mingzhu Xu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342326"
    },
    {
        "id": 24792,
        "title": "Optimization of the Electric Arc Furnace Transformer Power",
        "authors": "Yu. M. Mironov",
        "published": "2020-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1134/s0036029520060142"
    },
    {
        "id": 24793,
        "title": "State of Charge Estimation for Lithium-Ion Battery Using Time Series Transformer with De-Noise De-Stationary Inception Network",
        "authors": "Zhihao Yi, Liwei Wang, Kaitai Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545965"
    },
    {
        "id": 24794,
        "title": "Exploring Hugging Face Transformer Library Impact on Sentiment Analysis",
        "authors": "Aashita Chhabra, Kiran Chaudhary, Mansaf Alam",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781032614083-6"
    },
    {
        "id": 24795,
        "title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog",
        "authors": "Erik Ekstedt, Gabriel Skantze",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.268"
    },
    {
        "id": 24796,
        "title": "Spatial-Temporal Unfold Transformer for Skeleton-based Human Action Recognition",
        "authors": "HU CUI, Tessai Hayama",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.52731/liir.v004.167"
    },
    {
        "id": 24797,
        "title": "A patrol mobile robot for power transformer substations based on ROS",
        "authors": "Yang Gao, Sheng Li, Xiang Wang, Qingwei Chen",
        "published": "2018-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2018.8407176"
    },
    {
        "id": 24798,
        "title": "Image Caption Generation With Adaptive Transformer",
        "authors": "Wei Zhang, Wenbo Nie, Xinle Li, Yao Yu",
        "published": "2019-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac.2019.8787715"
    },
    {
        "id": 24799,
        "title": "Voiceprint Recognition of Transformer Fault Based on Blind Source Separation and Convolutional Neural Network",
        "authors": "Li Min, Zhan Huamao, Qiu Annan",
        "published": "2021-6-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic49891.2021.9612322"
    },
    {
        "id": 24800,
        "title": "Patch-Range Attention and Visual Transformer for Facial Expression Recognition",
        "authors": "Zhiwei Lu, Xuesong Tang",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eiect58010.2022.00044"
    },
    {
        "id": 24801,
        "title": "ActionFormer: Pose-based Action Recognition via Transformer and CNN",
        "authors": "Caojie Xu, Yue Zhou, Aichun Zhu, Zixuan Wang, Yifeng Li, Fangqiang Hu",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055176"
    },
    {
        "id": 24802,
        "title": "Transformer Based Technique for High Resolution Image Restoration",
        "authors": "Debajyoty Banik, Sangsaptak Pal, M. Nazma B. Naskar, Anjan Bandyopadhyay",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ocit56763.2022.00109"
    },
    {
        "id": 24803,
        "title": "Reliability Enhancement of High Voltage Power Transformer Using Online Oil Dehydration",
        "authors": "Badr A. Attiyah, Abdullah A. Alnujaimi, Mohammad A. Alghamdi",
        "published": "2019-9-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/meps46793.2019.9395044"
    },
    {
        "id": 24804,
        "title": "Transformer and CNN Hybrid Neural Network for Seismic Impedance Inversion",
        "authors": "Chunyu Ning, Bangyu Wu, Zhaolin Zhu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281611"
    },
    {
        "id": 24805,
        "title": "Low-voltage ride through of multi-port power electronic transformer",
        "authors": "Wusong Wen, Zhengming Zhao, Shiqi Ji, Liqiang Yuan",
        "published": "2022-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ien.2022.0033"
    },
    {
        "id": 24806,
        "title": "Forensic Investigation of Drone Malfunctions with Transformer",
        "authors": "Arda Surya Editya, Tohari Ahmad, Hudan Studiawan",
        "published": "2023-7-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsses58299.2023.10199237"
    },
    {
        "id": 24807,
        "title": "Modeling Image Virality with Pairwise Spatial Transformer Networks",
        "authors": "Abhimanyu Dubey, Sumeet Agarwal",
        "published": "2017-10-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3123266.3123333"
    },
    {
        "id": 24808,
        "title": "Fine Tuning an AraT5 Transformer for Arabic Abstractive Summarization",
        "authors": "Yasmin Einieh, Amal Almansour, Amani Jamal",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cicn56167.2022.10008272"
    },
    {
        "id": 24809,
        "title": "Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings",
        "authors": "Parker Seegmiller, Sarah Preum",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.596"
    },
    {
        "id": 24810,
        "title": "Digital estimating and balancing of transformer magnetizing current in an isolated full bridge converter",
        "authors": "Karsten Holm Andersen, Morten Nymand",
        "published": "2017-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spec.2017.8333616"
    },
    {
        "id": 24811,
        "title": "Analysis and Design of Rotary Transformer for Wireless Power Transmission",
        "authors": "Roman Manko, Selma Corovic, Damijan Miljavec",
        "published": "2020-9-21",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/paep49887.2020.9240870"
    },
    {
        "id": 24812,
        "title": "Idling Mode Simulation of Single-Phase Transformer",
        "authors": "Dmytro Yarymbash, Mykhailo Kotsur, Yevheniia Kulanina, Tetyana Divchuk",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mees.2019.8896637"
    },
    {
        "id": 24813,
        "title": "Multimodal attention-based transformer for video captioning",
        "authors": "Hemalatha Munusamy, Chandra Sekhar C",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-023-04597-2"
    },
    {
        "id": 24814,
        "title": "Multi-Scale Transformer-Based Feature Combination for Image Retrieval",
        "authors": "Carlos Roig Mari, David Varas Gonzalez, Elisenda Bou-Balust",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897512"
    },
    {
        "id": 24815,
        "title": "Modulation classification using convolutional neural networks and spatial transformer networks",
        "authors": "Moein Mirmohammadsadeghi, Samer S. Hanna, Danijela Cabric",
        "published": "2017-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acssc.2017.8335486"
    },
    {
        "id": 24816,
        "title": "Understanding transfer learning for chest radiograph clinical report generation with modified transformer architectures",
        "authors": "Edward Vendrow, Ethan Schonfeld",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2023.e17968"
    },
    {
        "id": 24817,
        "title": "Study on axial vibration characteristics of transformer winding",
        "authors": "Dajian Li, Zhangting Yu, Jian Zhao, Lei Zhang",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciea.2019.8834297"
    },
    {
        "id": 24818,
        "title": "Ultra-Compact Low-Loss Integrated Transformer-Based Ku-Band Quadrature Hybrid Coupler",
        "authors": "Manuel Potercau, Nathalie Deltimple, Anthony Ghiotto",
        "published": "2018-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eumic.2018.8539914"
    },
    {
        "id": 24819,
        "title": "Advance technology using solid state transformer in power grids",
        "authors": "M.S. Poojari, P.M. Joshi",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.matpr.2021.11.107"
    },
    {
        "id": 24820,
        "title": "Efficient Convolution and Transformer-based Network for Video Frame Interpolation",
        "authors": "Issa Khalifeh, Luka Murn, Marta Mrak, Ebroul Izquierdo",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222296"
    },
    {
        "id": 24821,
        "title": "Research on Multi-Objective Optimization Design Method for High-Frequency Transformer",
        "authors": "Pengning Zhang, Pengyang Li, Hailong Zhu, Bofan Li, Ning Wang, Wei Li, Lei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685485"
    },
    {
        "id": 24822,
        "title": "An Evolved Transformer Model for ADME/Tox Prediction",
        "authors": "Changheng Shao, Fengjing Shao, Song Huang, Rencheng Sun, Tao Zhang",
        "published": "2024-2-2",
        "citations": 0,
        "abstract": "Drug discovery aims to keep fueling new medicines to cure and palliate many ailments and some untreatable diseases that still afflict humanity. The ADME/Tox (absorption, distribution, metabolism, excretion/toxicity) properties of candidate drug molecules are key factors that determine the safety, uptake, elimination, metabolic behavior and effectiveness of drug research and development. The predictive technique of ADME/Tox drastically reduces the fraction of pharmaceutics-related failure in the early stages of drug development. Driven by the expectation of accelerated timelines, reduced costs and the potential to reveal hidden insights from vast datasets, artificial intelligence techniques such as Graphormer are showing increasing promise and usefulness to perform custom models for molecule modeling tasks. However, Graphormer and other transformer-based models do not consider the molecular fingerprint, as well as the physicochemicals that have been proved effective in traditional computational drug research. Here, we propose an enhanced model based on Graphormer which uses a tree model that fully integrates some known information and achieves better prediction and interpretability. More importantly, the model achieves new state-of-the-art results on ADME/Tox properties prediction benchmarks, surpassing several challenging models. Experimental results demonstrate an average SMAPE (Symmetric Mean Absolute Percentage Error) of 18.9 and a PCC (Pearson Correlation Coefficient) of 0.86 on ADME/Tox prediction test sets. These findings highlight the efficacy of our approach and its potential to enhance drug discovery processes. By leveraging the strengths of Graphormer and incorporating additional molecular descriptors, our model offers improved predictive capabilities, thus contributing to the advancement of ADME/Tox prediction in drug development. The integration of various information sources further enables better interpretability, aiding researchers in understanding the underlying factors influencing the predictions. Overall, our work demonstrates the potential of our enhanced model to expedite drug discovery, reduce costs, and enhance the success rate of our pharmaceutical development efforts.",
        "link": "http://dx.doi.org/10.3390/electronics13030624"
    },
    {
        "id": 24823,
        "title": "Analysis of Internal Overvoltages in Transformer Windings during Transients in Electrical Networks",
        "authors": "Jakub Furgał, Maciej Kuniewski, Piotr Pająk",
        "published": "2020-5-22",
        "citations": 6,
        "abstract": "Due to the increasing requirements for the reliability of electrical power supply and associated apparatus, it is necessary to provide a detailed analysis of the overvoltage risk of power transformer insulation systems and equipment connected to their terminals. Exposure of transformer windings to overvoltages is the result of the propagation condition of electromagnetic waves in electrical networks and transformer windings. An analysis of transformer winding responses to transients in power systems is of particular importance, especially when protection against surges by typical overvoltage protection systems is applied. The analysis of internal overvoltages in transformers during a typical transient related to switching operations and selected failures is of great importance, particularly to assess the overvoltage exposure of insulation systems in operating conditions. The random nature of overvoltage phenomena in electrical networks implies the usage of computer simulations for the analysis of overvoltage exposures of electrical devices in operation. This article presents the analysis of the impact of transient phenomena in a model of a medium-voltage electrical network during switching operations and ground faults on overvoltages in the internal insulation systems of transformer windings. The basis of the analysis is simulations of overvoltages in the windings, made in the Electromagnetic Transients Program/Alternative Transients Program (EMTP/ATP) using a model with lumped parameters of transformer windings. The analysis covers the impact of the cable line length and the ground fault resistance value on internal overvoltage distributions.",
        "link": "http://dx.doi.org/10.3390/en13102644"
    },
    {
        "id": 24824,
        "title": "Transformer-Based Deep Learning Method for the Prediction of Ventilator Pressure",
        "authors": "Ruizhe Fan",
        "published": "2022-3-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicse55337.2022.9828926"
    },
    {
        "id": 24825,
        "title": "Fault Diagnosis of a Transformer using Fuzzy Model and PSO optimized SVM",
        "authors": "Akshita Dhiman, Rajesh Kumar",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126260"
    },
    {
        "id": 24826,
        "title": "A Novel Transformer Less Grid-Tied Multi Level Boost PV Inverter",
        "authors": "Ankur Srivastava, Jeevanand Seshadrinath",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etfg55873.2023.10408373"
    },
    {
        "id": 24827,
        "title": "An air insulated linear pulse transformer for electrodischarge technology",
        "authors": "A.A. Zherlitsyn, V.M. Alexeenko, S.S. Kondratiev",
        "published": "2021-12-1",
        "citations": 1,
        "abstract": "Abstract\nA linear pulse transformer with air insulation at\n  atmospheric pressure was created and tested under both constant\n  resistance and non linear loads.  The maximum power of the\n  transformer output pulse reached ∼500 MW at a matched load with\n  a charge voltage 50 kV.  The transformer transferred ∼60% of\n  the stored energy to the load over a characteristic time of about\n  1 μs.  The scalability the generator was studied by connecting\n  two identical transformers in series which gave a power output of\n  ∼850 MW with doubled output voltage and reduced current.  The\n  frequency mode of operation was studied using one and two\n  transformers with a charge voltage of 50 kV and a load that was,\n  close to matched.  In both cases, the power maximum and jitter\n  showed no significant changes at any of the frequencies tested (up\n  to 5 Hz).  These results mean that the use of this generator can be\n  recommended for a wide field of applications due to its scalability\n  and low internal impedance.",
        "link": "http://dx.doi.org/10.1088/1748-0221/16/12/p12006"
    },
    {
        "id": 24828,
        "title": "Analysis Method of Transformer Insulation Aging",
        "authors": "Feiyu Jiang, Songtao Gao, Jiti Zhang, Shuying Wang, Xin Liu, Bo Zhou",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pandafpe57779.2023.10140773"
    },
    {
        "id": 24829,
        "title": "WSD based Ontology Learning from Unstructured Text using Transformer",
        "authors": "Akshay Hari, Priyanka Kumar",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.01.019"
    },
    {
        "id": 24830,
        "title": "Transformer Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
        "authors": "Daoquan Chen, Weicong Hong, Xiuze Zhou",
        "published": "2022",
        "citations": 82,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3151975"
    },
    {
        "id": 24831,
        "title": "Under Load Tap Changer Diagnostics Based on Transformer DGA and DC Resistance Tests",
        "authors": "Ali Naderian Jahromi, Mohsen Hosseinkhanloo, Laurent Lamare",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic.2018.8481090"
    },
    {
        "id": 24832,
        "title": "Design and Modeling of Millimeter-Wave Transformer in Silicon: A Tutorial (Invited)",
        "authors": "Dixian Zhao, Jiecheng Zhong",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rfit.2019.8929123"
    },
    {
        "id": 24833,
        "title": "Hyneter: Hybrid Network Transformer for Object Detection",
        "authors": "Dong Chen, Duoqian Miao, Xuerong Zhao",
        "published": "2023-6-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096922"
    },
    {
        "id": 24834,
        "title": "Set-Transformer BeamsNet for AUV Velocity Forecasting in Complete DVL Outage Scenarios",
        "authors": "Nadav Cohen, Zeev Yampolsky, Itzik Klein",
        "published": "2023-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ut49729.2023.10103453"
    },
    {
        "id": 24835,
        "title": "TransGait: Vision Transformer Based Gait Recognition Network",
        "authors": "Kaixuan Li, Steven Meng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424880"
    },
    {
        "id": 24836,
        "title": "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders",
        "authors": "Naina Dhingra",
        "published": "2021-12-15",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fg52635.2021.9667080"
    },
    {
        "id": 24837,
        "title": "Cptnn: Cross-Parallel Transformer Neural Network For Time-Domain Speech Enhancement",
        "authors": "Kai Wang, Bengbeng He, Wei-Ping Zhu",
        "published": "2022-9-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwaenc53105.2022.9914777"
    },
    {
        "id": 24838,
        "title": "Transformer-Based Microbubble Localization",
        "authors": "Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz",
        "published": "2022-10-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ius54386.2022.9958313"
    },
    {
        "id": 24839,
        "title": "Video Frame Interpolation with Transformer",
        "authors": "Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, Jiaya Jia",
        "published": "2022-6",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00352"
    },
    {
        "id": 24840,
        "title": "Prediction of Electric Vehicles Charging Demand: A Transformer-Based Deep Learning Approach",
        "authors": "Sahar Koohfar, Wubeshet Woldemariam, Amit Kumar",
        "published": "2023-1-22",
        "citations": 12,
        "abstract": "Electric vehicles have been gaining attention as a cleaner means of transportation that is low-carbon and environmentally friendly and can reduce greenhouse gas emissions and air pollution. Despite EVs’ many advantages, widespread adoption will negatively affect the electric grid due to their random and volatile nature. Consequently, predicting the charging demand for electric vehicles is becoming a priority to maintain a steady supply of electric energy. Time series methodologies are applied to predict the charging demand: traditional and deep learning. RNN, LSTM, and transformers represent deep learning approaches, while ARIMA and SARIMA are traditional techniques. This research represents one of the first attempts to use the Transformer model for predicting EV charging demand. Predictions for 3-time steps are considered: 7 days, 30 days, and 90 days to address both short-term and long-term forecasting of EV charging load. RMSE and MAE were used to compare the model’s performance. According to the results, the Transformer outperforms the other mentioned models in terms of short-term and long-term predictions, demonstrating its ability to address time series problems, especially EV charging predictions. The proposed Transformers framework and the obtained results can be used to manage electricity grids efficiently and smoothly.",
        "link": "http://dx.doi.org/10.3390/su15032105"
    },
    {
        "id": 24841,
        "title": "Impulse performance of synthetic esters based-nanofluid for power transformer",
        "authors": "Anu Kumar Das, Saibal Chatterjee",
        "published": "2018-9-28",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2053-1591/aae237"
    },
    {
        "id": 24842,
        "title": "IOT Based Transformer Health Monitoring System: A Survey",
        "authors": "K. Hazarika, Gauri Katiyar, Noorul Islam",
        "published": "2021-3-4",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacite51222.2021.9404657"
    },
    {
        "id": 24843,
        "title": "TRAC: Compilation-Based Design of Transformer Accelerators for FPGAs",
        "authors": "Patrick Plagwitz, Frank Hannig, Jurgen Teich",
        "published": "2022-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fpl57034.2022.00015"
    },
    {
        "id": 24844,
        "title": "Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding",
        "authors": "Keval Doshi, Yasin Yilmaz",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00514"
    },
    {
        "id": 24845,
        "title": "Dual Global Enhanced Transformer for image captioning",
        "authors": "Tiantao Xian, Zhixin Li, Canlong Zhang, Huifang Ma",
        "published": "2022-4",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.01.011"
    },
    {
        "id": 24846,
        "title": "Parasitic Egg Detection and Classification with Transformer-Based Architectures",
        "authors": "Anibal Pedraza, Jesus Ruiz-Santaquiteria, Oscar Deniz, Gloria Bueno",
        "published": "2022-10-16",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897846"
    },
    {
        "id": 24847,
        "title": "Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer",
        "authors": "Lipeng Zhuang",
        "published": "2021-12-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3508259.3508270"
    },
    {
        "id": 24848,
        "title": "Heterogeneous Graph Transformer for Graph-to-Sequence Learning",
        "authors": "Shaowei Yao, Tianming Wang, Xiaojun Wan",
        "published": "2020",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.640"
    },
    {
        "id": 24849,
        "title": "An Efficient Grid Connected Photovoltaic System Based on H6 Transformer-less Inverter",
        "authors": "Essam Hendawi",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "Grid-connected photovoltaic (PV) systems based on transformer-less inverters have been widely used. Various topologies of transformer-less inverters are presented. The requirements of minimizing leakage current through transformer-less inverters and maximizing the power generated from PV arrays are of great importance. This paper presents a complete system that can effectively minimize the leakage current to safe values. In addition, operation at maximum PV array power point is realized. The system is based on modified H6 transformer-less inverter to minimize the leakage current. Perturbation and Observation P&O maximum power point tracker in addition to a dc-dc boost converter are utilized to achieve the second target. Analysis of each part and system controllers in the proposed system are presented in detail. Losses and system efficiency are introduced. Effects of sun irradiance variations are included. Simulation results prove the effectiveness of the proposed system.",
        "link": "http://dx.doi.org/10.24018/ejenergy.2023.3.3.125"
    },
    {
        "id": 24850,
        "title": "Longitudinal Beam-Shaping Simulation for Enhanced Transformer Ratio in Beam-Driven Accelerators",
        "authors": "Wei-Hou Tan, Philippe Piot, Alexander Zholents",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aac.2018.8659429"
    },
    {
        "id": 24851,
        "title": "Local-constraint transformer network for stock movement prediction",
        "authors": "Jincheng Hu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcse.2021.117030"
    },
    {
        "id": 24852,
        "title": "Applying HF and VHF/UHF Partial Discharge Detection for Distribution Transformer",
        "authors": "Sakda Maneerot, Masaaki Kando, Norasage Pattanadech",
        "published": "2020-8-8",
        "citations": 0,
        "abstract": "This paper represents application of high frequency (HF) and very high frequency/ultrahigh frequency (VHF/UHF) partial discharge (PD) detection for a distribution transformer. A capacitive sensor is used to detect the HF electric field caused by charge transfer inside oil–paper insulation due to PD at the defect site, and an electromagnetic sensor or antenna is used for detecting electromagnetic PD transients in the air outside the investigated transformer in the near-field region. Three types of artificial PD sources in air and insulating liquid, which are corona discharge, surface discharge and air void discharge in pressboard, were investigated. Three identical distribution transformers were rated at 22 kV, 400 V and 50 kVA, and were designed and constructed. The first transformer was filled with mineral oil, the second was filled with natural ester and the third was filled with palm oil. The PD generated by the air-filled voids in the insulating papers and pressboards of these transformers with five different conditions were investigated, i.e., non-impregnated paper, impregnated paper for 3 hours, 6 hours, 9 hours and 12 hours. The impregnation process was done with 65°C liquid temperature, and the pressure in the oven was around 5 mbar. From the experimental results, it can be concluded that the electromagnetic PD transients radiated from the corona discharge of both high-voltage (HV) and low-voltage sides in the air are in the VHF range, and surface discharge frequency is extended up to the UHF range. For the PD in the insulating liquid, the phase resolved PD (PRPD) pattern in the HF range is a valuable tool to characterize the PD sources. The PD in an air-filled void inside the insulating paper of the mineral oil transformer is obviously different compared with those of the natural ester transformer and the palm oil transformer. For the manufacturing of distribution transformers in this research, it is found that after the paper insulation is dried out, the impregnation process for a period of 9 hours is suitable for improving the oil–paper insulation with an acceptable PD level. This paper is the cross-field application by applying the antenna and communication theory for detecting the discharge problems in HV equipment.",
        "link": "http://dx.doi.org/10.13052/jmm1550-4646.1545"
    },
    {
        "id": 24853,
        "title": "P2.05 - Continuous non-invasive cell growth monitoring in disposable bioreactors using an active compensated differential transformer",
        "authors": "M. Allers, S. Schönewald, K. K. Bakes, S. Zimmermann",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5162/13dss2017/p2.05"
    },
    {
        "id": 24854,
        "title": "ANALYSIS ON THE OPERATION PARAMETERS OF THE NEW CONVERTER TRANSFORMER",
        "authors": "THANH NGOC TRAN",
        "published": "2020-11-11",
        "citations": 0,
        "abstract": "This paper presents a new converter transformer which is applied in multipulse diode/SCR rectifiers/inverter. Comparison of operational characteristics of a conventional converter transformer and the proposed transformer was studied. Results show that the new converter transformer can not only decrease the grid side current, increase the valve side voltage but also greatly reduce the harmonics of the grid side currents. Simulation results verify the correctness of the theoretical analysis.",
        "link": "http://dx.doi.org/10.46242/jst-iuh.v28i04.229"
    },
    {
        "id": 24855,
        "title": "Catnic:A Feature Relevance Based Transformer Model for Automatic Image Caption Generation",
        "authors": "Tingting Zhang, Tao Zhang, Yanhong Zhuo, Feng Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4272712"
    },
    {
        "id": 24856,
        "title": "Automation of transformer substations of industrial enterprises with modern programmed logical controllers",
        "authors": "Aslanova Gulnoz Nasriddinovna",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5958/2278-4853.2021.00760.6"
    },
    {
        "id": 24857,
        "title": "News Recommendation Based On Multi-Feature Sequence Transformer",
        "authors": "Chenghao Wang, Jin Gou, Zongwen Fan",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itme53901.2021.00037"
    },
    {
        "id": 24858,
        "title": "PV-Battery System Interconnected Through Transformer and Bidirectional DC-DC Converter",
        "authors": "Antony J. Manjila, V. Mini",
        "published": "2019-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccs45141.2019.9065478"
    },
    {
        "id": 24859,
        "title": "Influence of Transformer Axial-Clamping Loss on the Vibration of Transformers",
        "authors": "Andre Wurde, Jannis Nikolas Kahlen, Nils Langenberg, Albert Moser",
        "published": "2022-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic51169.2022.9833153"
    },
    {
        "id": 24860,
        "title": "Optimum Design Approach of High Frequency Transformer: Including the Effects of Eddy Currents",
        "authors": "Sobhi Barqi",
        "published": "2018-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssd.2018.8570390"
    },
    {
        "id": 24861,
        "title": "Transformer condition diagnostics based on expert opinions and fuzzy logic",
        "authors": "Vadim Manusov, Dzhavod Ahyoev",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18635/2071-2219-2017-2-37-40"
    },
    {
        "id": 24862,
        "title": "ELECTROMAGNETIC PROCESSES IN A THREE-PHASE TRANSFORMER DURING HEATING",
        "authors": "Igor Grigorievich Strizhkov, Evgeny Nikolaevich Chesnyuk",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55618/20756704_2022_15_2_73-80"
    },
    {
        "id": 24863,
        "title": "Data Augmentation for Question Answering Using Transformer-based VAE with Negative Sampling",
        "authors": "Wataru Kano, Koichi Takeuchi",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiaiaai55812.2022.00097"
    },
    {
        "id": 24864,
        "title": "Investigating the Influence of Different Types of Nanoparticles on Thermal and Dielectric Properties of Insulation in Converter Transformer",
        "authors": "Boxue Du",
        "published": "2017-5-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/67432"
    },
    {
        "id": 24865,
        "title": "Dtcm: Deep Transformer Capsule Mutual Distillation for Multivariate Time Series Classification",
        "authors": "Zhiwen Xiao, Xin Xu, Huanlai Xing, Bowen Zhao, Xinhan Wang, Fuhong Song, Rong Qu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327154"
    },
    {
        "id": 24866,
        "title": "Fall Event Detection using Vision Transformer",
        "authors": "Ankita Dey, Sreeraman Rajan, George Xiao, Jianping Lu",
        "published": "2022-10-30",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sensors52175.2022.9967352"
    },
    {
        "id": 24867,
        "title": "Influence of Low Frequency Oscillation on the Fuse of Potential Transformer",
        "authors": "Hongtao Ren, Ying Zhang",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epee59859.2023.10351951"
    },
    {
        "id": 24868,
        "title": "BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language Generation",
        "authors": "Masaki Asada, Makoto Miwa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.34"
    },
    {
        "id": 24869,
        "title": "Compact Convolutional Transformer based on Sharpness-Aware Minimization for Image Classification",
        "authors": "Li-Hua Li, Radius Tanone",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icast57874.2023.10359276"
    },
    {
        "id": 24870,
        "title": "Pengaruh Error Current Transformer (CT) terhadap Tagihan Pembayaran Listrik pada Pelanggan 20KV",
        "authors": "Sri Muzzayanah, Arnisa Stefanie",
        "published": "2022-9-15",
        "citations": 0,
        "abstract": "Energi Listrik bukan lagi hal yang tabu di telinga kita semua, listrik di jaman teknologi sekarang menjadi salah satu bahan utama yang dibutuhkan oleh manusia, segala kegiatan yang dilakukan oleh manusia dari berbagai usia menggunakan listrik. Dari memasak, melakukan pekerjaan, mengerjakan tugas hingga berpergian. Banyak sekali yang diuntungkan karena adanya ketersediaan listrik. Apalagi dalam sektor industri khususnya pelanggan Tegangan Menengah yang membutuhkan suplpay listrik banyak sehingga membutuhkan daya yang cukup besar, dengan adanya Current Transformator (CT) atau Trafo Arus hal tersebut bisa di dapatkan untuk supplay pada input Kwh meter di indonesia, yaitu sebesar 5A pada sisi primer. Dikarenakan membutuhkan daya yang cukup besar CT yang dipasang harus sesuai dengan rasio yang sesuai dengan daya kontrak. dalam hal ini penertiban pemakaian tenaga listrik (P2TL) dilaksanakan secara langsung kelapangan oleh  PT PLN (Persero) untuk memastikan penyaluran listrik yang aman pada pelanggan sehingga pelanggan mendapatkan kenyamanan secara teknis maupun non teknis. Hasil pelaksanaan (P2TL) mendapatkan error pada CT yang terpasang, namun error yang terdeteksi merupakan kesalahan wiring atau Human error. Sehingga dapat menyebabkan Tagihan Listrik melonjak dan pelanggan merasa dirugikan, dengan adanya kebijakan tentang Penyesuaian Rekening Pemakaian Tenaga Listrik (PRPTL) oleh PT PLN (Persero)  maka tidak ada yang dirugikan dalam kasus ini.",
        "link": "http://dx.doi.org/10.30736/je-unisla.v7i2.816"
    },
    {
        "id": 24871,
        "title": "Internal model control of solid state transformer for microgrid stability enhancement",
        "authors": "M. Monika, R. Meshram, S. Wagh, A. M. Stankovic",
        "published": "2017-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps.2017.8107221"
    },
    {
        "id": 24872,
        "title": "Target-aware transformer tracking with hard occlusion instance generation",
        "authors": "Dingkun Xiao, Zhenzhong Wei, Guangjun Zhang",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "Visual tracking is a crucial task in computer vision that has been applied in diverse fields. Recently, transformer architecture has been widely applied in visual tracking and has become a mainstream framework instead of the Siamese structure. Although transformer-based trackers have demonstrated remarkable accuracy in general circumstances, their performance in occluded scenes remains unsatisfactory. This is primarily due to their inability to recognize incomplete target appearance information when the target is occluded. To address this issue, we propose a novel transformer tracking approach referred to as TATT, which integrates a target-aware transformer network and a hard occlusion instance generation module. The target-aware transformer network utilizes an encoder-decoder structure to facilitate interaction between template and search features, extracting target information in the template feature to enhance the unoccluded parts of the target in the search features. It can directly predict the boundary between the target region and the background to generate tracking results. The hard occlusion instance generation module employs multiple image similarity calculation methods to select an image pitch in video sequences that is most similar to the target and generate an occlusion instance mimicking real scenes without adding an extra network. Experiments on five benchmarks, including LaSOT, TrackingNet, Got10k, OTB100, and UAV123, demonstrate that our tracker achieves promising performance while running at approximately 41 fps on GPU. Specifically, our tracker achieves the highest AUC scores of 65.5 and 61.2% in partial and full occlusion evaluations on LaSOT, respectively.",
        "link": "http://dx.doi.org/10.3389/fnbot.2023.1323188"
    },
    {
        "id": 24873,
        "title": "Fault diagnosis of power transformer using method of graphic images",
        "authors": "Shutenko Oleg, Jakovenko Ivan",
        "published": "2017-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ysf.2017.8126594"
    },
    {
        "id": 24874,
        "title": "Fundamental Research on the Impact of Surfactants on the Electrical Performance of Transformer Oil-Based Nanofluids",
        "authors": "Chinnapat Suriyasakulpong, Pichai Muangpratoom",
        "published": "2023-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieecon56657.2023.10126981"
    },
    {
        "id": 24875,
        "title": "Time domain analysis of EMWs in transformers",
        "authors": "Hossein Karami, Gevork B. Gharehpetian, Seyed-Alireza Ahmadi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822801-2.00007-0"
    },
    {
        "id": 24876,
        "title": "THE INFLUENCE OF MAGNETIC TEXTURE OF TRANSFORMER STEELS ON MAGNETIC FIELD OF LAMINATED CORES",
        "authors": "E.V. Kalinin, A.I. Chivenkov",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46960/2658-6754_2020_3_35"
    },
    {
        "id": 24877,
        "title": "Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks",
        "authors": "Longqing Ye",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3532213.3532222"
    },
    {
        "id": 24878,
        "title": "Harmonic optimal power flow with transformer excitation",
        "authors": "Frederik Geth, Tom Van Acker",
        "published": "2022-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.epsr.2022.108604"
    },
    {
        "id": 24879,
        "title": "Investigation of size and morphology effects of MgO nanostructures on the properties of MgO/transformer oil-based nanofluids",
        "authors": "Mansoor Farbod, Narges Saki, Ameneh Ahangarpour",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMgO nanoparticles and nanoflakes were prepared using sol-gel and hydrothermal methods and the effect of size and morphology of MgO nanostructures on the thermal conductivity and stability of the transformer oil-based nanofluid containing these nanostructures was investigated. The structural properties of the samples were examined using X-ray diffraction (XRD) and field emission scanning electron microscopy (FESEM). The results showed that the nanostructure’s shape depends on the fabrication method, pH, temperature, and synthesis time. Also, the measurements showed that the thermal conductivity of nanofluids containing MgO nanoflakes with different wt.% has higher values than that of nanofluids containing MgO nanoparticles. It was found that by increasing the concentration of nano additives up to 1 wt.%, the thermal conductivity increased and then decreased for higher concentrations. The maximum increase of 11.3% was measured for nanofluid containing 1 wt.% of nanoflakes. Likewise, the stability of nanoflake’s nanofluid was observed to be higher.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2651608/v1"
    },
    {
        "id": 24880,
        "title": "LETR: An End-to-End Detector of Reconstruction Area in Blade’s Adaptive Machining with Transformer",
        "authors": "Zikai Yin, Yonghou Liang, Junxue Ren, Jungang An, Famei He",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the leading/trailing edge&rsquo;s adaptive machining of the near-net-shaped blade, a small portion of the theoretical part is retained for securing aerodynamic performance by manual work. However, this procedure is time-consuming and depends on the human experience. In this paper, we defined retained theoretical leading/trailing edge as the reconstruction area. To accelerate the reconstruction process, an anchor-free neural network model based on Transformer was proposed, named LETR (Leading/trailing Edge Transformer). LETR extracts image features from an aspect of mixed frequency and channel domain. We also integrated LETR with the newest meta-Acon activation function. We tested our model on the self-made dataset LDEG2021 on a single GPU and got an mAP of 91.9\\%, which surpassed our baseline model, Deformable DETR by 1.1\\%. Furthermore, we modified LETR&rsquo;s convolution layer and named the new model after GLETR (Ghost Leading/trailing Edge Transformer) as a lightweight model for real-time detection. It is proved that GLETR has fewer weight parameters and converges faster than LETR with an acceptable decrease in mAP (0.1\\%) by test results.",
        "link": "http://dx.doi.org/10.20944/preprints202109.0332.v2"
    },
    {
        "id": 24881,
        "title": "Image Cognition-based Power Transformer Protection Scheme Using Convolutional Neural Network",
        "authors": "Zongbo Li, Zaibin Jiao, Anyang He",
        "published": "2020-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281450"
    },
    {
        "id": 24882,
        "title": "Consensus Phase Shifting Transformer Model",
        "authors": "Md Rejwanur R. Mojumdar, Jose M. Cano, Mohsen Assadi, Gonzalo A. Orcajo",
        "published": "2020-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281681"
    },
    {
        "id": 24883,
        "title": "Field Coating of 765kV Transformer Bushings with RTV HVIC: Planning, Implementation and Results",
        "authors": "Neelesh Arora",
        "published": "2022-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33686/pwj.v17i1.167352"
    },
    {
        "id": 24884,
        "title": "Glass Objects Detection Based on Transformer Encoder-Decoder",
        "authors": "Xiaonan Hou, Minghao Zhan, Chunlei Wang, Chunhui Fan",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacr55854.2022.9935562"
    },
    {
        "id": 24885,
        "title": "A Contextualized Transformer-Based Method for Cyberbullying Detection",
        "authors": "Nabi Rezvani, Amin Beheshti, Xuyun Zhang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302478"
    },
    {
        "id": 24886,
        "title": "Control of solid-state-transformer for minimized energy storage capacitors",
        "authors": "Takanori Isobe, Hiroshi Tadano, Zijin He, Yang Zou",
        "published": "2017-10",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce.2017.8096672"
    },
    {
        "id": 24887,
        "title": "Investigation of Transformer Lockout Event Caused by Breaker Failure Protection Misoperation",
        "authors": "Ibukunoluwa O. Korede, Jacob Midkiff, Brian Starling",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias54024.2023.10406222"
    },
    {
        "id": 24888,
        "title": "A Survey On: IoT based Transformer Health Monitoring System",
        "authors": "Naman Mishra, Avinash Yadav, Pritam Yadav, Arzoo Yadav, Pinki Yadav",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4159209"
    },
    {
        "id": 24889,
        "title": "A Quantization Method Based on Lightweight Transformer Model Architecture for Automatic Classification of Lung Sounds",
        "authors": "Qiuhao Wang, Yun Chu, EnZe Zhou, Gang Zheng, Qian Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4482035"
    },
    {
        "id": 24890,
        "title": "A modelling of AC voltage stabilizer based on a hybrid transformer with matrix converter",
        "authors": "Paweł Szcześniak",
        "published": "2017-6-27",
        "citations": 5,
        "abstract": "AbstractThis article presents a study of an AC voltage stabilizer based on a three-phase hybrid transformer combined with a matrix converter. The proposed solution is used to control AC voltage amplitude and phase shift. By adjustment of these voltage parameters we can reduce the effects of overvoltage, voltage dips or lamp flicker. Such negative phenomena are very significant, particularly from the perspective of the final consumer and sensitive loads connected to the power network. Often the voltage in the power system can be adjusted using a mechanical or thyristor controlled regulator, which in a stepwise manner switches the taps of the electromagnetic transformer. The method for obtaining continuous control of the voltage magnitude and phase shift with the use of a conventional transformer with two output windings and a matrix converter is presented in this paper. The operating principles, mathematical model and properties of the proposed voltage stabilizers are discussed in this paper. The main part of the article will be devoted to the mathematical model which is based on an averaged equation. Computer simulation results are presented and compared with the results of a mathematical study.",
        "link": "http://dx.doi.org/10.1515/aee-2017-0028"
    },
    {
        "id": 24891,
        "title": "Research on Credit Scoring Based on Transformer-CatBoost Network Structure",
        "authors": "Zhengyuan Zhang, Zhanquan Wang",
        "published": "2022-7-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceiec54567.2022.9835063"
    },
    {
        "id": 24892,
        "title": "Calibration of a Digital Current Transformer Measuring Bridge: Metrological Challenges and Uncertainty Contributions",
        "authors": "Guglielmo Frigo, Marco Agustoni",
        "published": "2021-10-3",
        "citations": 6,
        "abstract": "In this paper, we consider the calibration of measuring bridges for non-conventional instrument transformers with digital output. In this context, the main challenge is represented by the necessity of synchronization between analog and digital outputs. To this end, we propose a measurement setup that allows for monitoring and quantifying the main quantities of interest. A possible laboratory implementation is presented and the main sources of uncertainty are discussed. From a metrological point of view, technical specifications and statistical analysis are employed to draw up a rigorous uncertainty budget of the calibration setup. An experimental validation is also provided through the thorough characterization of the measurement accuracy of a commercial device in use at METAS laboratories. The proposed analysis proves how the calibration of measuring bridges for non-conventional instrument transformers requires ad hoc measurement setups and identifies possible space for improvement, particularly in terms of outputs’ synchronization and flexibility of the generation process.",
        "link": "http://dx.doi.org/10.3390/metrology1020007"
    },
    {
        "id": 24893,
        "title": "Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition",
        "authors": "Akihiko Takashima, Ryo Masumura, Atsushi Ando, Yoshihiro Yamazaki, Mihiro Uchida, Shota Orihashi",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11307"
    },
    {
        "id": 24894,
        "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
        "authors": "Jianyuan Sun, Xubo Liu, Xinhao Mei, Volkan Kılıç, Mark D. Plumbley, Wenwu Wang",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-943"
    },
    {
        "id": 24895,
        "title": "CALCULATION OF POWER TRANSFORMER INDUCTANCE MATRIX",
        "authors": "Aleksandr I. Orlov, Sergei V. Volkov, Ilsur Kh. Garipov",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "Practical calculations of electric equipment and electric networks are performed using equivalent circuits. Without taking into account saturation phenomena, electromagnetic processes in a power transformer are described by a system of linear equations, which can be represented in matrix form. The transformer inductance matrix contains self and mutual winding inductances, whose values for one phase are determined directly from the passport data containing the results of no-load and short circuit experiments. Mutual inductances between phases depending on the type and size of the magnetic core are not usually given in the transformer documentation. It does not allow to simulate the operating modes that differ from the steady state with uniform phase load.\r\n\r\nThe purpose of the work is to develop an algorithm for calculating elements of the power transformer inductance matrix according to its passport data and magnetic circuit parameters.\r\n\r\nThe scientific novelty lies in use of information about magnetic circuit type and size to determine elements of the inductance matrix.\r\n\r\nMaterials and methods. Methods of linear electric and magnetic circuit theory are used in the work.\r\n\r\nResults. The proposed algorithm for calculation of power transformer inductance matrix includes three stages: calculation of self and mutual inductances for one phase; construction of an equivalent circuit for a magnetic circuit and calculation of magnetic fluxes in order to take into account the mutual influence of windings on various phases; assembly of the inductance matrix of three-phase transformer. A method is proposed for constructing the transformer inductance matrix with an arbitrary type of magnetic circuit, phases and windings quantity. The method involves preliminary calculation of the coefficient matrix that characterizes the magnetic circuit and depends mainly on its type and, to a less, on the ratio of geometric dimensions in rods and yokes. The coefficient matrix can be calculated in advance and used to determine the inductance matrix of a wide range of transformers.\r\n\r\nConclusions. 1. The passport data of the transformer do not allow to model transformer operation at unbalanced load validly. 2. An algorithm for determining the inductance matrix of transformers has been developed. 3. A method for compilation of a transformer inductance matrix with an arbitrary type of magnetic circuit, the number of phases and windings is introduced. The practical significance of the algorithm is determined by the simplicity of algorithmizing, as well as by the possibility of using it in computer simulation of electrical circuits with transformers, for example, using the modified nodal analysis method.",
        "link": "http://dx.doi.org/10.47026/1810-1909-2023-4-120-129"
    },
    {
        "id": 24896,
        "title": "Review of critical analysis for life estimation of power transformer",
        "authors": "Kanika Chitnavis, N. R. Bhasme",
        "published": "2017-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpces.2017.8117632"
    },
    {
        "id": 24897,
        "title": "Importer, transformer, diffuser les savoirs infirmiers.",
        "authors": "Kevin Toffel, Philippe Longchamp",
        "published": "2017-5-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/anthropologiesante.2536"
    },
    {
        "id": 24898,
        "title": "Transformer-based Siamese and Triplet Networks for Facial Expression Intensity Estimation",
        "authors": "Motaz SABRI",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5057/ijae.ijae-d-22-00011"
    },
    {
        "id": 24899,
        "title": "Penser le commun au cœur de l’intime",
        "authors": "Alice Sternberg",
        "published": "2021-3-3",
        "citations": 0,
        "abstract": "Peut-on concevoir l’enfermement dans notre vie quotidienne et notre sphère privée, qu’ont induit les politiques de gestion de la pandémie de covid-19, comme une aubaine pour revitaliser la démocratie directe ? S’appuyant sur Henri Lefebvre et André Gorz, philosophes et du quotidien et de la vie, Alice Sternberg explore les possibilités de développer, à l’aune du monde vécu et à l’échelle locale, des communs fondés sur le travail pour soi et la coopération .",
        "link": "http://dx.doi.org/10.3917/ecorev.050.0139"
    },
    {
        "id": 24900,
        "title": "Finger-vein feature extraction method based on vision transformer",
        "authors": "Zhiying Lu, Runhao Wu, Jianfeng Zhang",
        "published": "2022-7-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.jei.31.4.043010"
    },
    {
        "id": 24901,
        "title": "Multi-dimensional Attention Spiking Transformer for Event-based Image Classification",
        "authors": "Lin Li, Yang Liu",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisce58541.2023.10142563"
    },
    {
        "id": 24902,
        "title": "Cavit: An Integrated Method for Image Style Transfer Using Parallel Cnn and Vision Transformer",
        "authors": "Zhang Zai-Fang, Shunlu Lu, Qing Guo, Nan Gao, Yuxiang Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4557551"
    },
    {
        "id": 24903,
        "title": "Dielectric Strength of Nanofluid-Impregnated Transformer Solid Insulation",
        "authors": "Daniel Pérez-Rosa, Andrés Montero, Belén García, Juan Carlos Burgos",
        "published": "2022-11-22",
        "citations": 2,
        "abstract": "The interest in developing new fluids that can be used as dielectric liquids for transformers has driven the research on dielectric nanofluids in the last years. A number of authors have reported promising results on the electrical and thermal properties of dielectric nanofluids. Less attention has been paid to the interaction of these fluids with the cellulose materials that constitute the solid insulation of the transformers. In the present study, the dielectric strength of cellulose insulation is investigated, comparing its behavior when it is impregnated with transformer mineral oil and when it is impregnated with a dielectric nanofluid. The study includes the analysis of the AC breakdown voltage and the impulse breakdown voltage of the samples. Large improvements were observed on the AC breakdown voltages of the specimens impregnated with nanofluids, while the enhancements were lower in the case of the impulse tests. The reasons for the increase in AC breakdown voltage were investigated, considering the dielectric properties of the nanofluids used to impregnate the samples of cellulose. The analysis was completed with a finite element study that revealed the effect of the nanoparticles on the electric field distribution within the test cell, and its role in the observed enhancement.",
        "link": "http://dx.doi.org/10.3390/nano12234128"
    },
    {
        "id": 24904,
        "title": "DPATD: Dual-Phase Audio Transformer for Denoising",
        "authors": "Junhui Li, Pu Wang, Jialu Li, Xinzhe Wang, Youshan Zhang",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddp60485.2023.00018"
    },
    {
        "id": 24905,
        "title": "Face Detection and Recognition Using OpenCV and Vision Transformer",
        "authors": "Krish Kumar, Nilesh Pingale, Bhawana Rudra",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10306821"
    },
    {
        "id": 24906,
        "title": "A Vision Transformer Architecture for the Automated Segmentation of Retinal Fluids in Spectral Domain Optical Coherence Tomography Images",
        "authors": "Daniel Philippi, Kai Rothaus, Mauro Castelli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNeovascular age-related macular degeneration (nAMD) is one of the major causes of irreversible blindness and is characterized by accumulations of different fluids inside the retina. An early detection and activity monitoring of predominately three types of fluids, namely intra-retinal fluid (IRF), sub-retinal fluid (SRF), and pigment epithelium detachment (PED), is critical for a successful treatment. Spectral-domain optical coherence tomography (SD-OCT) revolutionized nAMD treatment by providing cross-sectional, high-resolution images of the retina. Automatic segmentation and quantification of IRF, SRF, and PED in SD-OCT images can be extremely useful for clinical decision-making. Despite the use of state-of-the-art convolutional neural network (CNN)-based methods, the task remains challenging due to relevant variations in the location, size, shape, and texture of the fluids. This work is the first to adopt a transformer-based method to automatically segment retinal fluid from SD-OCT images and qualitatively and quantitatively evaluate its performance against CNN-based methods. The method combines the efficient long-range feature extraction and aggregation capabilities of Vision Transformers (ViTs) with data-efficient training of CNNs. The proposed method was tested on a private dataset containing 3842 2-dimensional SD-OCT retina images, manually labeled by experts of the Franziskus-Eye-Hospital. While one of the competitors presents a better performance in terms of Dice score, the proposed method is significantly less computationally expensive. Thus, future research will focus on the proposed network's architecture to increase its segmentation performance while maintaining its computational efficiency.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2262988/v1"
    },
    {
        "id": 24907,
        "title": "Power Transformer Fault Diagnosis System Based on Internet of Things",
        "authors": "Guoshi Wang, Ying Liu, Xiaowen Chen, Qing Yan, Haibin Sui, Chao Ma, Junfei Zhang",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nTransformer is the most important equipment in the power system. The research and development of fault diagnosis technology for Internet of things equipment can effectively detect the operation status of equipment and eliminate hidden faults in time, which is conducive to reducing the incidence of accidents and improving people's life safety index. Objective: To explore the utility of Internet of things in power transformer fault diagnosis system. Methods: A total of 30 groups of transformer fault samples were selected, and 10 groups were randomly selected for network training, and the rest samples were used for testing. The matter-element extension mathematical model of power transformer fault diagnosis was established, and the correlation function was improved according to the characteristics of three ratio method. Each group of power transformer was diagnosed for four months continuously, and the monitoring data and diagnosis were recorded and analyzed result. GPRS communication network is used to complete the communication between data acquisition terminal and monitoring terminal. According to the parameters of the database, the working state of the equipment is set, and various sensors are controlled by the instrument driver module to complete the diagnosis of transformer fault system. Results: The detection success rate of the power transformer fault diagnosis system model established in this paper is as high as 95.6%, the training error is less than 0.000 l, and it can correctly identify the fault types of the non-training samples. It can be seen that the technical support of the Internet of things is helpful to the upgrading and maintenance of the power transformer fault diagnosis system.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-71379/v1"
    },
    {
        "id": 24908,
        "title": "Power Transformer Fault Diagnosis System Based on Internet of Things",
        "authors": "Guoshi Wang, Ying Liu, Xiaowen Chen, Qing Yan, Haibin Sui, Chao Ma, Junfei Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformer is the most important equipment in the power system. The research and development of fault diagnosis technology for Internet of things equipment can effectively detect the operation status of equipment and eliminate hidden faults in time, which is conducive to reducing the incidence of accidents and improving people's life safety index. Objective: To explore the utility of Internet of things in power transformer fault diagnosis system. Methods: A total of 30 groups of transformer fault samples were selected, and 10 groups were randomly selected for network training, and the rest samples were used for testing. The matter-element extension mathematical model of power transformer fault diagnosis was established, and the correlation function was improved according to the characteristics of three ratio method. Each group of power transformer was diagnosed for four months continuously, and the monitoring data and diagnosis were recorded and analyzed result. GPRS communication network is used to complete the communication between data acquisition terminal and monitoring terminal. According to the parameters of the database, the working state of the equipment is set, and various sensors are controlled by the instrument driver module to complete the diagnosis of transformer fault system. Results: The detection success rate of the power transformer fault diagnosis system model established in this paper is as high as 95.6%, the training error is less than 0.000l, and it can correctly identify the fault types of the non training samples. It can be seen that the technical support of the Internet of things is helpful to the upgrading and maintenance of the power transformer fault diagnosis system.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-71379/v2"
    },
    {
        "id": 24909,
        "title": "Natural Language Processing based Automated Essay Scoring with Parameter-Efficient Transformer Approach",
        "authors": "Angad Sethi, Kavinder Singh",
        "published": "2022-3-29",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccmc53470.2022.9753760"
    },
    {
        "id": 24910,
        "title": "Piezoelectric transformer as a power supply for GM radiation detector",
        "authors": "Pavel Valenta, Vaclav Koucky, Jiri Hammerbauer",
        "published": "2018-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/epe.2018.8395991"
    },
    {
        "id": 24911,
        "title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks",
        "authors": "Yekun Chai, Shuo Jin, Xinwen Hou",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.616"
    },
    {
        "id": 24912,
        "title": "CMOS Power Amplifier with Inter-Stage Matching Network Using Tournament-Shape Transformer",
        "authors": "Donghwan Seo, Jaeyong Lee, Jinho Yoo, Changkun Park",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc57107.2023.10439809"
    },
    {
        "id": 24913,
        "title": "Image Compression with Deeper Learned Transformer",
        "authors": "Licheng Xiao, Hairong Wang, Nam Ling",
        "published": "2019-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apsipaasc47483.2019.9023342"
    },
    {
        "id": 24914,
        "title": "Novel Integrated Charger Concept Using an Induction Machine as Transformer at Standstill",
        "authors": "Erik Hoevenaars, Tobias Illg, Marc Hiller",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vppc49601.2020.9330995"
    },
    {
        "id": 24915,
        "title": "ShuffleSR: Image Deepfake Detection Using Shuffle Transformer",
        "authors": "Cheng Yang, Chen Zhu, Chao Deng, Zeyu Xiao",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ickii58656.2023.10332600"
    },
    {
        "id": 24916,
        "title": "Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction",
        "authors": "V.S.D.S.Mahesh Akavarapu, Arnab Bhattacharya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.423"
    },
    {
        "id": 24917,
        "title": "Multi-Physics Reliability Modeling of High-Frequency Electromagnetic Transformers in Solid-State Transformer Systems",
        "authors": "Reza Ilka, JiangBiao He",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itec55900.2023.10187006"
    },
    {
        "id": 24918,
        "title": "Deformation Study of a Transformer Winding using Frequency-Response Analysis and Finite-Element Analysis",
        "authors": "Sibabrata Pradhan, Sisir Kumar Nayak",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc57038.2022.10069362"
    },
    {
        "id": 24919,
        "title": "PV-Fed Transformer-Less Five-Level Grid-Tied Inverter",
        "authors": "Amirhossein Zeinaly, Kazem Varesi, Jaber Fallah Ardashir",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictem56862.2023.10083895"
    },
    {
        "id": 24920,
        "title": "Generative Video Compression with a Transformer-Based Discriminator",
        "authors": "Pengli Du, Ying Liu, Nam Ling, Yongxiong Ren, Lingzhi Liu",
        "published": "2022-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pcs56426.2022.10018030"
    },
    {
        "id": 24921,
        "title": "Sequence and Distance Aware Transformer for Recommendation Systems",
        "authors": "Runqiang Zang, Meiyun Zuo, Jilei Zhou, Yining Xue, Keman Huang",
        "published": "2021-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icws53863.2021.00027"
    },
    {
        "id": 24922,
        "title": "Multimodal Phased Transformer for Sentiment Analysis",
        "authors": "Junyan Cheng, Iordanis Fostiropoulos, Barry Boehm, Mohammad Soleymani",
        "published": "2021",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.189"
    },
    {
        "id": 24923,
        "title": "Dependency-Based Self-Attention for Transformer NMT",
        "authors": "Hiroyuki Deguchi,  , Akihiro Tamura, Takashi Ninomiya",
        "published": "2019-10-22",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26615/978-954-452-056-4_028"
    },
    {
        "id": 24924,
        "title": "Suitability of Alternate Fluids as Transformer Insulation - A Comprehensive Review",
        "authors": "Subhadip Bhattacharya, Sovan Dalai, Biswendu Chatterjee",
        "published": "2020-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece48148.2020.9223086"
    },
    {
        "id": 24925,
        "title": "Word-level Morpheme segmentation using Transformer neural network",
        "authors": "Tsolmon Zundi, Chinbat Avaajargal",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.sigmorphon-1.15"
    },
    {
        "id": 24926,
        "title": "DPE-BoTNeT: Dual Position Encoding Bottleneck Transformer Network for Skin Lesion Classification",
        "authors": "Katsuhiro Nakai, Xian-Hua Han",
        "published": "2022-3-28",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isbi52829.2022.9761578"
    },
    {
        "id": 24927,
        "title": "Vision Relation Transformer for Unbiased Scene Graph Generation",
        "authors": "Gopika Sudhakaran, Devendra Singh Dhami, Kristian Kersting, Stefan Roth",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.02000"
    },
    {
        "id": 24928,
        "title": "Investigating the Impact of Time Series Structure in Performance of Transformer-Based Model for River Streamflow Forecasting",
        "authors": "Nikolaos Tepetidis, Theano Iliopoulou, Panayiotis Dimitriadis, Demetris Koutsoyiannis",
        "published": "No Date",
        "citations": 0,
        "abstract": "River discharge forecasting plays a pivotal role in water resource management and environmental planning. Understanding the long-term dependence or changes in these processes is crucial for accurate predictions. Deep-learning methodologies have garnered significant scientific interest and are progressively becoming more prevalent across water-resources-related endeavors. Transformer models, a novel architecture that aims to track relationships in sequential data through attention mechanism, have increasing popularity last years. Through comprehensive experiments and analysis on real-world river discharge datasets, we aim to elucidate the impact of long-term dependence detection, as facilitated by the climacogram and Hurst coefficient, on the predictive capabilities of a transformer-based model. Insights from this investigation are anticipated to contribute to the advancement of river discharge forecasting methodologies, enhancing our understanding of long-term dependencies in these environmental processes.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-19687"
    },
    {
        "id": 24929,
        "title": "Captioning Transformer with Stacked Attention Modules",
        "authors": "Xinxin Zhu, Lixiang Li, Jing Liu, Haipeng Peng, Xinxin Niu",
        "published": "2018-5-7",
        "citations": 56,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/app8050739"
    },
    {
        "id": 24930,
        "title": "IRSTFormer: A Hierarchical Vision Transformer for Infrared Small Target Detection",
        "authors": "Gao Chen, Weihua Wang, Sirui Tan",
        "published": "2022-7-6",
        "citations": 15,
        "abstract": "Infrared small target detection occupies an important position in the infrared search and track system. The most common size of infrared images has developed to 640×512. The field-of-view (FOV) also increases significantly. As the result, there is more interference that hinders the detection of small targets in the image. However, the traditional model-driven methods do not have the capability of feature learning, resulting in poor adaptability to various scenes. Owing to the locality of convolution kernels, recent convolutional neural networks (CNN) cannot model the long-range dependency in the image to suppress false alarms. In this paper, we propose a hierarchical vision transformer-based method for infrared small target detection in larger size and FOV images of 640×512. Specifically, we design a hierarchical overlapped small patch transformer (HOSPT), instead of the CNN, to encode multi-scale features from the single-frame image. For the decoder, a top-down feature aggregation module (TFAM) is adopted to fuse features from adjacent scales. Furthermore, after analyzing existing loss functions, a simple yet effective combination is exploited to optimize the network convergence. Compared to other state-of-the-art methods, the normalized intersection-over-union (nIoU) on our IRST640 dataset and public SIRST dataset reaches 0.856 and 0.758. The detailed ablation experiments are conducted to validate the effectiveness and reasonability of each component in the method.",
        "link": "http://dx.doi.org/10.3390/rs14143258"
    },
    {
        "id": 24931,
        "title": "DC Bias Impact Analysis on Hydropower Plant Excitation Transformer",
        "authors": "Subramanya K, Thanga Raj Chelliah",
        "published": "2022-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pedes56012.2022.10080732"
    },
    {
        "id": 24932,
        "title": "AnimeTransGAN: Animation Image Super-Resolution Transformer via Deep Generative Adversarial Network",
        "authors": "Chang-De Peng, Li-Wei Kang",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315278"
    },
    {
        "id": 24933,
        "title": "Oil-Immersed Power Transformer Condition Monitoring Methodologies: A Review",
        "authors": "Lan Jin, Dowon Kim, Ahmed Abu-Siada, Shantanu Kumar",
        "published": "2022-5-6",
        "citations": 32,
        "abstract": "A power transformer is one of the most critical and expensive assets in electric power systems. Failure of a power transformer would not only result in a downtime to the entire transmission and distribution networks but may also cause personnel and environmental hazards due to oil leak and fire. Hence, to enhance a transformer’s reliability and extend its lifespan, a cost-effective and reliable condition monitoring technique should be adopted from day one of its installation. This will help detect incipient faults, extend a transformer’s operational life, and avoid potential consequences. With the global trend to establish digital substation automation systems, transformer online condition monitoring has been given much attention by utilities and researchers alike. Several online and offline condition monitoring techniques have been recently proposed for oil-immersed power transformers. This paper is aimed at providing a state-of-the-art review for the various condition monitoring technologies used for oil-immersed power transformers. Concept of measurements and analysis of the results along with the future trend of condition monitoring techniques are presented.",
        "link": "http://dx.doi.org/10.3390/en15093379"
    },
    {
        "id": 24934,
        "title": "Intelligent Classifiers in Distinguishing Transformer Faults Using Frequency Response Analysis",
        "authors": "Mehdi Bigdeli, Pierluigi Siano, Hassan Haes Alhelou",
        "published": "2021",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3052144"
    },
    {
        "id": 24935,
        "title": "Transformer-Based Bidirectional Encoder Representations for Emotion Detection from Text",
        "authors": "Ashok Kumar J, Erik Cambria, Tina Esther Trueman",
        "published": "2021-12-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660152"
    },
    {
        "id": 24936,
        "title": "Energy Consumption Prediction Strategy for Electric Vehicle Based on Lstm-Transformer Framework",
        "authors": "Zhanyu Feng, Jian  Ronald Zhang, Han Jiang, Xuejian Yao, Yu Qian, Haiyan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4747046"
    },
    {
        "id": 24937,
        "title": "Experiment 31 For the Given Current Transformer and Burden to Find the Ratio and Phase Angle Error",
        "authors": "",
        "published": "2017-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683922797-031"
    },
    {
        "id": 24938,
        "title": "Investigating Transformer-based Approach for Small-sample Object Detection",
        "authors": "Tiantian Lv, Yaoxuan Yuan, Jianguo Wang",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccnea60107.2023.00046"
    },
    {
        "id": 24939,
        "title": "Experiment 15 Separation of Losses in a Single-Phase Transformer (Separation of Eddy Current and Hysteresis Loss)",
        "authors": "",
        "published": "2017-12-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683922797-015"
    },
    {
        "id": 24940,
        "title": "Transformer-based structural seismic response prediction",
        "authors": "Qingyu Zhang, Maozu Guo, Lingling Zhao, Yang Li, Xinxin Zhang, Miao Han",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.istruc.2024.105929"
    },
    {
        "id": 24941,
        "title": "Stock Price Prediction Using a Frequency Decomposition Based GRU Transformer Neural Network",
        "authors": "Chengyu Li, Guoqi Qian",
        "published": "2022-12-24",
        "citations": 17,
        "abstract": "Stock price prediction is crucial but also challenging in any trading system in stock markets. Currently, family of recurrent neural networks (RNNs) have been widely used for stock prediction with many successes. However, difficulties still remain to make RNNs more successful in a cluttered stock market. Specifically, RNNs lack power to retrieve discerning features from a clutter of signals in stock information flow. Making it worse, by RNN a single long time cell from the market is often fused into a single feature, losing all the information about time which is essential for temporal stock prediction. To tackle these two issues, we develop in this paper a novel hybrid neural network for price prediction, which is named frequency decomposition induced gate recurrent unit (GRU) transformer, abbreviated to FDGRU-transformer or FDG-trans). Inspired by the success of frequency decomposition, in FDG-transformer we apply empirical model decomposition to decompose the complete ensemble of cluttered data into a trend component plus several informative and independent mode components. Equipped with the decomposition, FDG-transformer has the capacity to extract the discriminative insights from the cluttered signals. To retain the temporal information in the observed cluttered data, FDG-transformer utilizes hybrid neural network of GRU, long short term memory (LSTM) and multi-head attention (MHA) transformers. The integrated transformer network is capable of encoding the impact of different weights from each past time step to the current one, resulting in the establishment of a time series model from a deeper fine-grained level. We appy the developed FDG-transformer model to analyze Limit Order Book data and compare the results with that obtained from other state-of-the-art methods. The comparison shows that our model delivers effective price forecasting. Moreover, an ablation study is conducted to validate the importance and necessity of each component in the proposed model.",
        "link": "http://dx.doi.org/10.3390/app13010222"
    },
    {
        "id": 24942,
        "title": "Techniques for improved CityGML models",
        "authors": "Steffen Goebbels, Regina Pohle-Fröhlich",
        "published": "2019-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.gmod.2019.101044"
    },
    {
        "id": 24943,
        "title": "Models, models, models: a deflationary view",
        "authors": "Jay Odenbaugh",
        "published": "2021-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11229-017-1665-8"
    },
    {
        "id": 24944,
        "title": "Business Models and Circular Business Models",
        "authors": "Roberta De Angelis",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-75127-6_3"
    },
    {
        "id": 24945,
        "title": "A Novel Action Transformer Network for Hybrid Multimodal Sign Language Recognition",
        "authors": "Sameena Javaid, Safdar Rizvi",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.031924"
    },
    {
        "id": 24946,
        "title": "Influence of Arrester Parameters on Overvoltage Characteristics on Protected Transformer",
        "authors": "Abugalia A",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4172/2332-0796.1000284"
    },
    {
        "id": 24947,
        "title": "A Novel Transformer Less Grid-Tied Multi Level Boost PV Inverter",
        "authors": "Ankur Srivastava, Jeevanand Seshadrinath",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etfg55873.2023.10408373"
    },
    {
        "id": 24948,
        "title": "An air insulated linear pulse transformer for electrodischarge technology",
        "authors": "A.A. Zherlitsyn, V.M. Alexeenko, S.S. Kondratiev",
        "published": "2021-12-1",
        "citations": 1,
        "abstract": "Abstract\nA linear pulse transformer with air insulation at\n  atmospheric pressure was created and tested under both constant\n  resistance and non linear loads.  The maximum power of the\n  transformer output pulse reached ∼500 MW at a matched load with\n  a charge voltage 50 kV.  The transformer transferred ∼60% of\n  the stored energy to the load over a characteristic time of about\n  1 μs.  The scalability the generator was studied by connecting\n  two identical transformers in series which gave a power output of\n  ∼850 MW with doubled output voltage and reduced current.  The\n  frequency mode of operation was studied using one and two\n  transformers with a charge voltage of 50 kV and a load that was,\n  close to matched.  In both cases, the power maximum and jitter\n  showed no significant changes at any of the frequencies tested (up\n  to 5 Hz).  These results mean that the use of this generator can be\n  recommended for a wide field of applications due to its scalability\n  and low internal impedance.",
        "link": "http://dx.doi.org/10.1088/1748-0221/16/12/p12006"
    },
    {
        "id": 24949,
        "title": "Life management concepts of transformer investigations",
        "authors": "K Vinoth Kumar, Nithin Matthew Sam, Kevin Sony",
        "published": "2018-3-19",
        "citations": 0,
        "abstract": "This paper shows the importance of specifications and life management concepts for reliable, uninterrupted and intended application of transformers throughout the life cycle. The transformers are tested, manufactured and designed based on the specification.",
        "link": "http://dx.doi.org/10.14419/ijet.v7i2.8.10414"
    },
    {
        "id": 24950,
        "title": "Integrated Busbar, Transformer And Feeder Backup Protection Based On Wide Area Voltage Measurements",
        "authors": "Patrick T. Manditereza",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sges51519.2020.00030"
    },
    {
        "id": 24951,
        "title": "VOLTAGE REGULATED DISTRIBUTION TRANSFORMER WITH NEW VACUUM OLTC",
        "authors": "S. Mikulić, B. Ćućić",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.1207"
    },
    {
        "id": 24952,
        "title": "Multiport Energy Router for Satellite Based on High-Frequency Transformer",
        "authors": "Kang Qing, Zhang Xuan, Xing Jie, Li Feng, Shi Haiping",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/espc.2019.8932025"
    },
    {
        "id": 24953,
        "title": "COMPUTER SIMULATION MODEL OF THE TRACTION TRANSFORMER OF THE AC ELECTRIC LOCOMOTIVE OF THE O'ZBEKISTON SERIES",
        "authors": "Tulagan, Nazirkhonov, Rakhimbergan Polvonov",
        "published": "2022-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32743/26870142.2022.21.244.341834"
    },
    {
        "id": 24954,
        "title": "INTERFACING OF WIND ENERGY CONVERSION SYSTEM WITH SOLID STATE TRANSFORMER FOR SEAMLESS FAULT RIDE THROUGH OPERATION",
        "authors": "",
        "published": "2020-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31838/jcr.07.06.156"
    },
    {
        "id": 24955,
        "title": "Advanced Leakage Inductance Model for Transformer Transient Simulation",
        "authors": "Alejandro Avendano, Bruce A. Mork, Dmitry Ishchenko, Francisco Gonzalez",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm.2018.8586015"
    },
    {
        "id": 24956,
        "title": "Study on surface crack detection of low voltage current transformer",
        "authors": "Gao Xiaozheng, Lei Huan, Zhou Guangbin, Yang Wenjie",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2017.7978598"
    },
    {
        "id": 24957,
        "title": "Investigation of Persea Americana Oil as an Alternative Transformer Insulation Oil",
        "authors": "Benard M. Makaa, George K. Irungu, David K. Murage",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eic43217.2019.9046539"
    },
    {
        "id": 24958,
        "title": "Experimental Analysis of Some Natural Esters and Their Mixtures for Transformer Application",
        "authors": "Sachin Vishwakarma, Ranjana Singh",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uemgreen46813.2019.9221564"
    },
    {
        "id": 24959,
        "title": "Study Of Measurements Leakage Current In Oil Transformer",
        "authors": "Hassan J. Mohammed.",
        "published": "2020-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24237/djes.2020.13101"
    },
    {
        "id": 24960,
        "title": "Transformer Safety Monitoring System",
        "authors": "V. Gomathi, D. P. Suryaprakash, R Srimathi, S. Uma, R. Valarmathi, V. Sudha",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mysurucon55714.2022.9972460"
    },
    {
        "id": 24961,
        "title": "Microcontroller based differential relay using fuzzy logic for transformer protection",
        "authors": "Madhura S. Deshmukh, V. T. Barhate",
        "published": "2017-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccons.2017.8250556"
    },
    {
        "id": 24962,
        "title": "Multi-inverter drive with symmetrical multilevel winding voltage of transformer during overmodulation",
        "authors": "Valentin Oleschuk, Vladimir Ermuratskii",
        "published": "2017-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iseee.2017.8170638"
    },
    {
        "id": 24963,
        "title": "Sound signal-based transformer operation status monitoring system",
        "authors": "Haifeng Xu, Jidong Cai, Long Xu, Fei Xu",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135233"
    },
    {
        "id": 24964,
        "title": "Analysis of Transformer Vibration Signal",
        "authors": "Keqi Ma",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2991/fmsmt-17.2017.204"
    },
    {
        "id": 24965,
        "title": "Auto-transformer-based power amplifier with totem-pole driver",
        "authors": "Elena Sobotta, Robert Wolf, Frank Ellinger",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssd.2017.8166916"
    },
    {
        "id": 24966,
        "title": "Idling Mode Simulation of Single-Phase Transformer",
        "authors": "Dmytro Yarymbash, Mykhailo Kotsur, Yevheniia Kulanina, Tetyana Divchuk",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mees.2019.8896637"
    },
    {
        "id": 24967,
        "title": "Review on Health Care Monitor of Substation Transformer with Overload Protection",
        "authors": "Rajesh Kudale, Rakesh Shriwatava, Somnath Hadpe, Shridhar Khule",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4043565"
    },
    {
        "id": 24968,
        "title": "Transformer Neural Network for Structure Constrained Molecular Optimization",
        "authors": "Jiazhen He, Felix Mattsson, Marcus Forsberg, Esben Jannik Bjerrum, Ola Engkvist, eva nittinger, Christian Tyrchan, Werngard Czechtizky",
        "published": "No Date",
        "citations": 3,
        "abstract": "Finding molecules with a desirable balance of multiple properties is a main challenge in drug discovery. Here, we focus on the task of molecular optimization, where a starting molecule with promising properties needs to be further optimized towards the desirable properties. Typically, chemists would apply chemical transformations to the starting molecule based on their intuition. A widely used strategy is the concept of matched molecular pairs where two molecules differ by a single transformation. In particular, a chemist would be interested in keeping one part of the starting molecule (core) constant, while substituting the other part (R-group), to optimize the starting molecule towards desirable properties. Motivated by this, we train a Transformer model, Transformer-R, to generate R-groups given the starting molecule (with its core and R-group specified) and the specified desirable properties. The generated R-groups will be attached to the core to form the final molecules, which are guaranteed to keep the core of interest and are expected to satisfy the desirable properties in the input. Our model could accelerate the process of optimizing antiviral drug candidates in terms of various properties of interest, e.g. pharmacokinetics.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14416133.v1"
    },
    {
        "id": 24969,
        "title": "Forensic Investigation of Drone Malfunctions with Transformer",
        "authors": "Arda Surya Editya, Tohari Ahmad, Hudan Studiawan",
        "published": "2023-7-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsses58299.2023.10199237"
    },
    {
        "id": 24970,
        "title": "Modeling Image Virality with Pairwise Spatial Transformer Networks",
        "authors": "Abhimanyu Dubey, Sumeet Agarwal",
        "published": "2017-10-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3123266.3123333"
    },
    {
        "id": 24971,
        "title": "Fine Tuning an AraT5 Transformer for Arabic Abstractive Summarization",
        "authors": "Yasmin Einieh, Amal Almansour, Amani Jamal",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cicn56167.2022.10008272"
    },
    {
        "id": 24972,
        "title": "Learning English to Chinese Character: Calligraphic Art Production based on Transformer",
        "authors": "Yifan Jin, Yi Zhang, Xi Yang",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3476124.3488642"
    },
    {
        "id": 24973,
        "title": "Transformer Based Image-Text Consistency Analysis for Infographic Articles",
        "authors": "Yuwei Chen, Ming-Ching Chang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mipr59079.2023.00023"
    },
    {
        "id": 24974,
        "title": "A Modified Transformer Neural Network (MTNN) for Robust Intrusion Detection in IoT Networks",
        "authors": "Syed Wahaj Ahmed, Fabio Kientz, Rasha Kashef",
        "published": "2023-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itc-egypt58155.2023.10206134"
    },
    {
        "id": 24975,
        "title": "Large Power Transformer Overload Detection using Sound Analysis",
        "authors": "Nguyen Cong-Phuong",
        "published": "2021-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3483207.3483230"
    },
    {
        "id": 24976,
        "title": "TSVT: Token Sparsification Vision Transformer for RGB-D Salient Object Detection",
        "authors": "Lina Gao, Bing Liu, Ping Fu, Mingzhu Xu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342326"
    },
    {
        "id": 24977,
        "title": "Optimization of the Electric Arc Furnace Transformer Power",
        "authors": "Yu. M. Mironov",
        "published": "2020-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1134/s0036029520060142"
    },
    {
        "id": 24978,
        "title": "Transformer Neural Network-Based Molecular Optimization Using General Transformations",
        "authors": "Jiazhen He, Eva Nittinger, Christian Tyrchan, Werngard Czechtizky, Atanas Patronov, Esben Jannik Bjerrum, Ola Engkvist",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMolecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug discovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the large chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimicking the chemist's intuition in terms of matched molecular pairs (MMPs). Although MMPs is a typical and widely used strategy by medicinal chemists, it offers limited capability in terms of exploring the space of solutions. There are more options to modify a starting molecule to achieve desirable properties,  e.g.   one can simultaneously modify the molecule at different places including changing the scaffold. This study trains the same Transformer architecture on different datasets. These datasets consist of a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets reflecting general transformations are constructed from ChEMBL based on two approaches: Tanimoto similarity (allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold constant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while keeping the same model architecture. Our results show that the models trained on differently prepared datasets transform a given starting molecule in a way that it reflects the nature of the dataset used for training the model. These models could complement each other and  unlock the capability for the chemists to pursue different options for improving a starting molecule.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1097104/v1"
    },
    {
        "id": 24979,
        "title": "Thermal Modes Analysis of Operation of Transformer with Rotating Magnetic Field",
        "authors": "Elena Limonnikova, Alexander Cherevko, Sergey Platonenkov",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uralcon.2019.8877610"
    },
    {
        "id": 24980,
        "title": "Push–Pull Converter Transformer Maximum Efficiency Optimization",
        "authors": "Jan Martis",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-65960-2_27"
    },
    {
        "id": 24981,
        "title": "Entretien: Ibrahim Thiaw, conseiller spécial pour le Sahel, Transformer le Sahel en une terre d’opportunités",
        "authors": "Ibrahim Thiaw",
        "published": "2018-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18356/e70e3833-fr"
    },
    {
        "id": 24982,
        "title": "Review on Health Care Monitor of Substation Transformer with Overload Protection",
        "authors": "Rajesh Kudale, Rakesh Shriwatava, Somnath Hadpe, Shridhar Khule",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4074622"
    },
    {
        "id": 24983,
        "title": "A Hybrid CNN-Transformer Architecture for Semantic Segmentation of Radar Sounder data",
        "authors": "Raktim Ghosh, Francesca Bovolo",
        "published": "2022-7-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883124"
    },
    {
        "id": 24984,
        "title": "Hierarchical volumetric transformer with comprehensive attention for medical image segmentation",
        "authors": "Zhuang Zhang, Wenjie Luo",
        "published": "2022",
        "citations": 0,
        "abstract": "<abstract>\n <p>Transformer is widely used in medical image segmentation tasks due to its powerful ability to model global dependencies. However, most of the existing transformer-based methods are two-dimensional networks, which are only suitable for processing two-dimensional slices and ignore the linguistic association between different slices of the original volume image blocks. To solve this problem, we propose a novel segmentation framework by deeply exploring the respective characteristic of convolution, comprehensive attention mechanism, and transformer, and assembling them hierarchically to fully exploit their complementary advantages. Specifically, we first propose a novel volumetric transformer block to help extract features serially in the encoder and restore the feature map resolution to the original level in parallel in the decoder. It can not only obtain the information of the plane, but also make full use of the correlation information between different slices. Then the local multi-channel attention block is proposed to adaptively enhance the effective features of the encoder branch at the channel level, while suppressing the invalid features. Finally, the global multi-scale attention block with deep supervision is introduced to adaptively extract valid information at different scale levels while filtering out useless information. Extensive experiments demonstrate that our proposed method achieves promising performance on multi-organ CT and cardiac MR image segmentation.</p>\n </abstract>",
        "link": "http://dx.doi.org/10.3934/mbe.2023149"
    },
    {
        "id": 24985,
        "title": "Einleitung",
        "authors": "Jochen Hirschle",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446473904.001"
    },
    {
        "id": 24986,
        "title": "Standard schemes and engineering cases of transformer noise reduction for typical distribution room in residential area",
        "authors": "J. Luo, K. Zhang, S. Chen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2069"
    },
    {
        "id": 24987,
        "title": "Intercell Transformer Coupled Buck Converter in One-of-Three Rectifier",
        "authors": "Yuxiang Shi, Jing Xu, Goran Mandic, Sandeep Bala",
        "published": "2020-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce44975.2020.9236027"
    },
    {
        "id": 24988,
        "title": "Study of High Frequency Rotary Transformer Structures for Contactless Inductive Power Transfer",
        "authors": "Xu ZU, Quan JIANG",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icems.2019.8921812"
    },
    {
        "id": 24989,
        "title": "Phrase Grounding Algorithm Based on Transformer Multilevel Feature Fusion",
        "authors": "Xiangdong Meng, Juxiang zhou, Jianhou Gan, Jun Wang, Ken Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4289646"
    },
    {
        "id": 24990,
        "title": "Fast Calculation of Hot Spot Temperature of Onan Transformer Based on Rational Fraction Regression Model",
        "authors": "Ruiyang Xiao, Dongping Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4116248"
    },
    {
        "id": 24991,
        "title": "Improved scheme based on memory voltage for transformer differential protection considering the effects of PLL",
        "authors": "Tao Zheng, Ruozhu Zhang, Ying Chen, jingwen Ai, yilin Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "Phase-locked loop (PLL) technique is a critical control module applied\nin photovoltaic (PV) grid-connected control system to synchronize with\ngrid voltage. The PLL error directly affects the response characteristic\nof PV grid-connected inverter under faults, leading to a mass of\nharmonic components in the output current of the PV inverter. Especially\nin the scenario of short-circuit fault with deep voltage sag, the PLL\ndetection error is too significant to be ignored, posing challenges to\ntransformer differential protection. This paper proposes an improved\nscheme that utilizes a digital computer algorithm to record and store\npre-fault voltage, aiming to address the complexity and implementation\nchallenges for phase-lock error in practical engineering applications.\nBy utilizing the memory voltage, the PLL detection error is eliminated,\nmitigating harmonic distortion in the PV output current and ensuring the\nreliability of the transformer differential protection. However,\nintroducing memory voltage may increase the short-circuit current of the\nPV output. Therefore, it is recommended to reduce the amplitude of the\nshort-circuit current by multiplying the inverter port voltage reference\nvalue by an appropriate limiting coefficient k and outputting it to the\nphysical system. Finally, the effectiveness of the proposed scheme is\nverified through MATLAB/Simulink simulation.",
        "link": "http://dx.doi.org/10.22541/au.169081517.70776459/v1"
    },
    {
        "id": 24992,
        "title": "Research on Multi-Objective Optimization Design Method for High-Frequency Transformer",
        "authors": "Pengning Zhang, Pengyang Li, Hailong Zhu, Bofan Li, Ning Wang, Wei Li, Lei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685485"
    },
    {
        "id": 24993,
        "title": "Analysis Method of Transformer Insulation Aging",
        "authors": "Feiyu Jiang, Songtao Gao, Jiti Zhang, Shuying Wang, Xin Liu, Bo Zhou",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pandafpe57779.2023.10140773"
    },
    {
        "id": 24994,
        "title": "Efficient Convolution and Transformer-based Network for Video Frame Interpolation",
        "authors": "Issa Khalifeh, Luka Murn, Marta Mrak, Ebroul Izquierdo",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222296"
    },
    {
        "id": 24995,
        "title": "WSD based Ontology Learning from Unstructured Text using Transformer",
        "authors": "Akshay Hari, Priyanka Kumar",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.01.019"
    },
    {
        "id": 24996,
        "title": "Impulse performance of synthetic esters based-nanofluid for power transformer",
        "authors": "Anu Kumar Das, Saibal Chatterjee",
        "published": "2018-9-28",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2053-1591/aae237"
    },
    {
        "id": 24997,
        "title": "Prediction of Electric Vehicles Charging Demand: A Transformer-Based Deep Learning Approach",
        "authors": "Sahar Koohfar, Wubeshet Woldemariam, Amit Kumar",
        "published": "2023-1-22",
        "citations": 12,
        "abstract": "Electric vehicles have been gaining attention as a cleaner means of transportation that is low-carbon and environmentally friendly and can reduce greenhouse gas emissions and air pollution. Despite EVs’ many advantages, widespread adoption will negatively affect the electric grid due to their random and volatile nature. Consequently, predicting the charging demand for electric vehicles is becoming a priority to maintain a steady supply of electric energy. Time series methodologies are applied to predict the charging demand: traditional and deep learning. RNN, LSTM, and transformers represent deep learning approaches, while ARIMA and SARIMA are traditional techniques. This research represents one of the first attempts to use the Transformer model for predicting EV charging demand. Predictions for 3-time steps are considered: 7 days, 30 days, and 90 days to address both short-term and long-term forecasting of EV charging load. RMSE and MAE were used to compare the model’s performance. According to the results, the Transformer outperforms the other mentioned models in terms of short-term and long-term predictions, demonstrating its ability to address time series problems, especially EV charging predictions. The proposed Transformers framework and the obtained results can be used to manage electricity grids efficiently and smoothly.",
        "link": "http://dx.doi.org/10.3390/su15032105"
    },
    {
        "id": 24998,
        "title": "Local-constraint transformer network for stock movement prediction",
        "authors": "Jincheng Hu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcse.2021.10039986"
    },
    {
        "id": 24999,
        "title": "Simulation Research on Full-scale Transformer Water Fire Extinguishing System",
        "authors": "Yu Liu, Bi-chen Pan, Bao-hui Chen",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced56215.2022.9928881"
    },
    {
        "id": 25000,
        "title": "Simulation Modeling of Linear Phase-shifting Transformer Inverter System",
        "authors": "Hou Xinguo, Bi Min",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciba50161.2020.9277276"
    }
]