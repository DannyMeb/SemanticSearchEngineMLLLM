[
    {
        "id": 1401,
        "title": "Fading memory as inductive bias in residual recurrent networks",
        "authors": "Igor Dubinin, Felix Effenberger",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106179"
    },
    {
        "id": 1402,
        "title": "Two-timescale recurrent neural networks for distributed minimax optimization",
        "authors": "Zicong Xia, Yang Liu, Jiasen Wang, Jun Wang",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.003"
    },
    {
        "id": 1403,
        "title": "Spatial–temporal recurrent reinforcement learning for autonomous ships",
        "authors": "Martin Waltz, Ostap Okhrin",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.015"
    },
    {
        "id": 1404,
        "title": "DyVGRNN: DYnamic mixture Variational Graph Recurrent Neural Networks",
        "authors": "Ghazaleh Niknam, Soheila Molaei, Hadi Zare, Shirui Pan, Mahdi Jalili, Tingting Zhu, David Clifton",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.05.048"
    },
    {
        "id": 1405,
        "title": "User Re-Authentication via Mouse Movements and Recurrent Neural Networks",
        "authors": "Paul Houssel, Luis Leiva",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296600003648"
    },
    {
        "id": 1406,
        "title": "Novel criteria of sampled-data synchronization controller design for gated recurrent unit neural networks under mismatched parameters",
        "authors": "Seungyong Han, Suneel Kumar Kommuri, Yongsik Jin",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.035"
    },
    {
        "id": 1407,
        "title": "Explainability Insights to Cellular Simultaneous Recurrent Neural Networks for Classical Planning",
        "authors": "Michaela Urbanovská, Antonín Komenda",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012375800003636"
    },
    {
        "id": 1408,
        "title": "Warming up recurrent neural networks to maximise reachable multistability greatly improves learning",
        "authors": "Gaspard Lambrechts, Florent De Geeter, Nicolas Vecoven, Damien Ernst, Guillaume Drion",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.023"
    },
    {
        "id": 1409,
        "title": "Time series prediction and anomaly detection with recurrent spiking neural networks",
        "authors": "Yann Cherdo, Benoit Miramond, Alain Pegatoquet",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191614"
    },
    {
        "id": 1410,
        "title": "Generalisation of Feed-Forward Neural Networks and  Recurrent Neural Networks",
        "authors": "Rui Wang",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "This paper presents an in-depth analysis of Feed-Forward Neural Networks (FNNs) and Recurrent Neural Networks (RNNs), two powerful models in the field of artificial intelligence. Understanding these models and their applications is crucial for harnessing their potential. The study addresses the need to comprehend the unique characteristics and architectures of FNNs and RNNs. These models excel at processing sequential and temporal data, making them indispensable in tasks. Furthermore, the paper emphasises the importance of variables in FNNs and proposes a novel method to rank the importance of independent variables in predicting the output variable. By understanding the relationship between inputs and outputs, valuable insights can be gained into the underlying patterns and mechanisms driving the system being modelled. Additionally, the research explores the impact of initial weights on model performance. Contrary to conventional beliefs, the study provides evidence that neural networks with random weights can achieve competitive performance, particularly in situations with limited training datasets. This finding challenges the traditional notion that careful initialization is necessary for neural networks to perform well. In summary, this paper provides a comprehensive analysis of FNNs and RNNs while highlighting the importance of understanding the relationship between variables and the impact of initial weights on model performance. By shedding light on these crucial aspects, this research contributes to the advancement and effective utilisation of neural networks, paving the way for improved predictions and insights in various domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/40/20230659"
    },
    {
        "id": 1411,
        "title": "Quantum recurrent neural networks for sequential learning",
        "authors": "Yanan Li, Zhimin Wang, Rongbing Han, Shangshang Shi, Jiaxin Li, Ruimin Shang, Haiyong Zheng, Guoqiang Zhong, Yongjian Gu",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.003"
    },
    {
        "id": 1412,
        "title": "Cascade Prediction with Recurrent Neural Networks and Diffusion Depth Distributions",
        "authors": "Shao Huang, Wangyang Yu",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105676"
    },
    {
        "id": 1413,
        "title": "Dynamic Analysis and an Eigen Initializer for Recurrent Neural Networks",
        "authors": "Ran Dou, Jose Principe",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191986"
    },
    {
        "id": 1414,
        "title": "Sentiment Analysis With Lipschitz Recurrent Neural Networks",
        "authors": "Mahmudul Hasan, Sachin Shetty",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isncc58260.2023.10323619"
    },
    {
        "id": 1415,
        "title": "DRRNets: Dynamic Recurrent Routing via Low-Rank Regularization in Recurrent Neural Networks",
        "authors": "Dongjing Shan, Yong Luo, Xiongwei Zhang, Chao Zhang",
        "published": "2023-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3105818"
    },
    {
        "id": 1416,
        "title": "Traffic forecasting with graph spatial–temporal position recurrent network",
        "authors": "Yibi Chen, Kenli Li, Chai Kiat Yeo, Keqin Li",
        "published": "2023-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.009"
    },
    {
        "id": 1417,
        "title": "Modeling Batch Tasks Using Recurrent Neural Networks in Co-Located Alibaba Workloads",
        "authors": "Hifza Khalid, Arunselvan Ramaswamy, Simone Ferlin, Alva Couch",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012392700003654"
    },
    {
        "id": 1418,
        "title": "Different Types of Neural Networks and Applications: Evidence from Feedforward, Convolutional and Recurrent Neural Networks",
        "authors": "Yumin Pan",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "Neural networks have achieved great process in the 90 years since they were officially introduced in 1943. Because of its wide application and huge research and development potential, this technology attracts more and more scientific and technological workers to the research of neural networks. Neural network technology is an essential component of AI development, and it is a significant indicator of a country's overall strength. In this paper, this study will demonstrate Feedforward Neural Network, Convolution Neural Network and Recurrent Neural networks and evaluate them through datasets from kaggle.com. and CSDN (China IT community). Through this paper, readers can have a better outlook and understanding of the operating principles of each type of neural network as well as their specific jobs (what kind of jobs they specialized in) and each application of these neural networks. So that this paper can promote readers' thoughts and help them start learning neural networks or be a supplement or reference for future scholars. In the end, this paper will present the outcome, which is the evaluation of the accuracy, loss curve, and accuracy curve of neural networks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/6rn1wd81"
    },
    {
        "id": 1419,
        "title": "Exploring crude oil price movements as a complex time series using recurrent neural networks",
        "authors": "Rida El Abassi, Mohamed Oubraime, Jaafar Idrais, Abderrahim Sabour",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607720.3607731"
    },
    {
        "id": 1420,
        "title": "Least Redundant Gated Recurrent Neural Network",
        "authors": "Łukasz Neumann, Łukasz Lepak, Paweł Wawrzyński",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191895"
    },
    {
        "id": 1421,
        "title": "RECURRENT QUANTUM NEURAL NETWORKS: A REVIEW",
        "authors": "Gleydson Fernandes de Jesus, Valéria Loureiro da Silva",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5151/siintec2023-306159"
    },
    {
        "id": 1422,
        "title": "Predicting Opinions in Social Networks Using Recurrent Neural Networks",
        "authors": "Mohamed N. Zareer, Rastko R. Selmic",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/med59994.2023.10185814"
    },
    {
        "id": 1423,
        "title": "An Inexact Sequential Quadratic Programming Method for Learning and Control of Recurrent Neural Networks",
        "authors": "Adeyemi D. Adeoye, Alberto Bemporad",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3354855"
    },
    {
        "id": 1424,
        "title": "Continual learning with attentive recurrent neural networks for temporal data classification",
        "authors": "Shao-Yu Yin, Yu Huang, Tien-Yu Chang, Shih-Fang Chang, Vincent S. Tseng",
        "published": "2023-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.10.031"
    },
    {
        "id": 1425,
        "title": "Ensemble Recurrent Graph Neural Networks for Availability Prediction in Cellular Networks",
        "authors": "Ming-Yen Wu",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ickii58656.2023.10332614"
    },
    {
        "id": 1426,
        "title": "Fog-cloud based intrusion detection system using Recurrent Neural Networks and feature selection for IoT networks",
        "authors": "Naeem Firdous Syed, Mengmeng Ge, Zubair Baig",
        "published": "2023-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2023.109662"
    },
    {
        "id": 1427,
        "title": "Semantically Layered Representation for Planning Problems and Its Usage for Heuristic Computation Using Cellular Simultaneous Recurrent Neural Networks",
        "authors": "Michaela Urbanovská, Antonín Komenda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011691000003393"
    },
    {
        "id": 1428,
        "title": "Design of continuous-time recurrent neural networks with piecewise-linear activation function for generation of prescribed sequences of bipolar vectors",
        "authors": "Norikazu Takahashi, Tsuyoshi Yamakawa, Yasuhiro Minetoma, Tetsuo Nishi, Tsuyoshi Migita",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.05.013"
    },
    {
        "id": 1429,
        "title": "Exploring Neural Network Structure through Sparse Recurrent Neural Networks: A Recasting and Distillation of Neural Network Hyperparameters",
        "authors": "Quincy Hershey, Randy Paffenroth, Harsh Pathak",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00026"
    },
    {
        "id": 1430,
        "title": "Adversarial Attacks with Defense Mechanisms on Convolutional Neural Networks and Recurrent Neural Networks for Malware Classification",
        "authors": "Sharoug Alzaidy, Hamad Binsalleeh",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "In the field of behavioral detection, deep learning has been extensively utilized. For example, deep learning models have been utilized to detect and classify malware. Deep learning, however, has vulnerabilities that can be exploited with crafted inputs, resulting in malicious files being misclassified. Cyber-Physical Systems (CPS) may be compromised by malicious files, which can have catastrophic consequences. This paper presents a method for classifying Windows portable executables (PEs) using Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). To generate malware executable adversarial examples of PE, we conduct two white-box attacks, Jacobian-based Saliency Map Attack (JSMA) and Carlini and Wagner attack (C&W). An adversarial payload was injected into the DOS header, and a section was added to the file to preserve the PE functionality. The attacks successfully evaded the CNN model with a 91% evasion rate, whereas the RNN model evaded attacks at an 84.6% rate. Two defense mechanisms based on distillation and training techniques are examined in this study for overcoming adversarial example challenges. Distillation and training against JSMA resulted in the highest reductions in the evasion rates of 48.1% and 41.49%, respectively. Distillation and training against C&W resulted in the highest decrease in evasion rates, at 48.1% and 49.9%, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14041673"
    },
    {
        "id": 1431,
        "title": "Differentiating brain states via multi-clip random fragment strategy-based interactive bidirectional recurrent neural network",
        "authors": "Shu Zhang, Enze Shi, Lin Wu, Ruoyang Wang, Sigang Yu, Zhengliang Liu, Shaochen Xu, Tianming Liu, Shijie Zhao",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.040"
    },
    {
        "id": 1432,
        "title": "Next Word Prediction using Recurrent Neural Networks",
        "authors": "",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.58257/ijprems32232"
    },
    {
        "id": 1433,
        "title": "Image Captioning System Using Merge Conventional and Recurrent Neural Networks",
        "authors": "Rasha Talib Gdeeb",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Image captioning system is that operation where we create a group of captions defines the image content and depends specially on deep learning. This technology is used for a wide range in recent time because it can help as example the blind people by telling them about all the objects around them which Nvidia company has the lead of it. Many searches were done in this subject and may be the most important one that search done by Andrej Karpathy leader of artificial intelligent in Tesla company depending on Flicker database which gives a special result used by next researches. ICS (Image Captioning Systems) are end-to-end Sequence-to-Sequence systems where we can convert a series of image pixels descriptions into a series of words. For images objects recognition we can use conventional neural networks (CNN) and for the words recognition and text build we will use recurrent neural network",
        "keywords": "",
        "link": "http://dx.doi.org/10.47832/minarcongress9-17"
    },
    {
        "id": 1434,
        "title": "DESIGN AND DEVELOPMENT OF EFFICIENT WATER QUALITY PREDICTION MODELS USING VARIANTS OF RECURRENT NEURAL NETWORKS",
        "authors": "",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/ecb/2023.12.si5.0143"
    },
    {
        "id": 1435,
        "title": "Soil Nutrient Evaluation System Based on Improved Recurrent Neural Network",
        "authors": "Yingbo Bu, Xinjie Yu, Yao Yang",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105705"
    },
    {
        "id": 1436,
        "title": "SGORNN: Combining scalar gates and orthogonal constraints in recurrent networks",
        "authors": "Will Taylor-Melanson, Martha Dais Ferreira, Stan Matwin",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.028"
    },
    {
        "id": 1437,
        "title": "Exploring a Basis Set of Intrinsic Functions Underlying Neural Computation by Symbolically Programming Recurrent Neural Networks.",
        "authors": "Daniel Calbick, Ilker Yildirim, Jason Kim",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1692-0"
    },
    {
        "id": 1438,
        "title": "Integrating Explicit Contexts with Recurrent Neural Networks for Improving Prognostic Models",
        "authors": "Rashmi Dutta Baruah, Mario Muñoz Organero",
        "published": "2023-3-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aero55745.2023.10115751"
    },
    {
        "id": 1439,
        "title": "A direct discretization recurrent neurodynamics method for time-variant nonlinear optimization with redundant robot manipulators",
        "authors": "Yang Shi, Wangrong Sheng, Shuai Li, Bin Li, Xiaobing Sun, Dimitrios K. Gerontitis",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.04.040"
    },
    {
        "id": 1440,
        "title": "ENHANCING SEQUENCE LEARNING WITH MASKING IN RECURRENT NEURAL NETWORKS",
        "authors": "",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets48489"
    },
    {
        "id": 1441,
        "title": "Characterising representation dynamics in recurrent neural networks for object recognition",
        "authors": "Sushrut Thorat, Adrien Doerig, Tim Kietzmann",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1088-0"
    },
    {
        "id": 1442,
        "title": "Advancing Recurrent Neural Networks and Generative Adversarial Networks: A Technical Framework for Enhanced Effectiveness Evaluation",
        "authors": "Xia Yu",
        "published": "2023-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciscae59047.2023.10393296"
    },
    {
        "id": 1443,
        "title": "Stability and Limit Cycles of Fuzzy Inferences in a Recurrent Petri-like Neural Network",
        "authors": "Lidia Ghosh, Dipanjan Konar, Amit Konar, Atulya K. Nagar",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191279"
    },
    {
        "id": 1444,
        "title": "Recurrent Neural Networks With More Flexible Memory: Better Predictions Than Rough Volatility",
        "authors": "Damien Challet, Vincent Ragel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4514331"
    },
    {
        "id": 1445,
        "title": "Recurrent Neural Networks for Forecasting Social Processes",
        "authors": "Angelin Lalev, Alexandrina Alexandrova",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bdkcse59280.2023.10339767"
    },
    {
        "id": 1446,
        "title": "Trajectory-Based State-of-Charge Prediction Using LSTM Recurrent Neural Networks",
        "authors": "Adan Ernesto Vela",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasc58513.2023.10311254"
    },
    {
        "id": 1447,
        "title": "‘Seeing’ the Future: Improving Macroeconomic Forecasts with Spatial Data Using Recurrent Convolutional Neural Networks",
        "authors": "Jonathan Leslie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4350048"
    },
    {
        "id": 1448,
        "title": "Load Margin Assessment of Power Systems Using Recurrent Neural Networks",
        "authors": "Murilo E. C. Bento",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/induscon58041.2023.10374951"
    },
    {
        "id": 1449,
        "title": "Retracted: Note Detection in Music Teaching Based on Intelligent Bidirectional Recurrent Neural Network",
        "authors": "",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9850239"
    },
    {
        "id": 1450,
        "title": "Emotion Recognition in Reddit Comments Using Recurrent Neural\nNetworks",
        "authors": "Mahdi Rezapour",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "\nBackground:\nReddit comments are a valuable source of natural language data\nwhere emotion plays a key role in human communication. However, emotion recognition is a\ndifficult task that requires understanding the context and sentiment of the texts. In this paper,\nwe aim to compare the effectiveness of four recurrent neural network (RNN) models for classifying the emotions of Reddit comments.\n\n\nMethods:\nWe use a small dataset of 4,922 comments labeled with four emotions: approval,\ndisapproval, love, and annoyance. We also use pre-trained Glove.840B.300d embeddings as\nthe input representation for all models. The models we compare are SimpleRNN, Long ShortTerm Memory (LSTM), bidirectional LSTM, and Gated Recurrent Unit (GRU). We experiment with different text preprocessing steps, such as removing stopwords and applying stemming, removing negation from stopwords, and the effect of setting the embedding layer as\ntrainable on the models.\n\n\nResults:\nWe find that GRU outperforms all other models, achieving an accuracy of 74%. Bidirectional LSTM and LSTM are close behind, while SimpleRNN performs the worst. We observe that the low accuracy is likely due to the presence of sarcasm, irony, and complexity in\nthe texts. We also notice that setting the embedding layer as trainable improves the performance of LSTM but increases the computational cost and training time significantly. We analyze some examples of misclassified texts by GRU and identify the challenges and limitations\nof the dataset and the models\n\n\nConclusion:\nIn our study GRU was found to be the best model for emotion classification of\nReddit comments among the four RNN models we compared. We also discuss some future directions for research to improve the emotion recognition task on Reddit comments. Furthermore, we provide an extensive discussion of the applications and methods behind each technique in the context of the paper.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2174/0126662558273325231201051141"
    },
    {
        "id": 1451,
        "title": "Memory capacity of recurrent neural networks with matrix representation",
        "authors": "Animesh Renanse, Alok Sharma, Rohitash Chandra",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126824"
    },
    {
        "id": 1452,
        "title": "Learning of Cognitive Control during Task Switching in Recurrent Neural Networks",
        "authors": "Shengjie Xu, Tom Verguts, Senne Braem",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1253-0"
    },
    {
        "id": 1453,
        "title": "Phish-armour: phishing detection using deep recurrent neural networks",
        "authors": "P. Dhanavanthini, S. Sibi Chakkravarthy",
        "published": "2023-3-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-07962-y"
    },
    {
        "id": 1454,
        "title": "Quantifying and Maximizing the Information Flux in Recurrent Neural Networks",
        "authors": "Claus Metzner, Marius E. Yamakou, Dennis Voelkl, Achim Schilling, Patrick Krauss",
        "published": "2024-2-16",
        "citations": 1,
        "abstract": "Abstract\nFree-running recurrent neural networks (RNNs), especially probabilistic models, generate an ongoing information flux that can be quantified with the mutual information I[x→(t),x→(t+1)] between subsequent system states x→. Although previous studies have shown that I depends on the statistics of the network’s connection weights, it is unclear how to maximize I systematically and how to quantify the flux in large systems where computing the mutual information becomes intractable. Here, we address these questions using Boltzmann machines as model systems. We find that in networks with moderately strong connections, the mutual information I is approximately a monotonic transformation of the root-mean-square averaged Pearson correlations between neuron pairs, a quantity that can be efficiently computed even in large systems. Furthermore, evolutionary maximization of I[x→(t),x→(t+1)] reveals a general design principle for the weight matrices enabling the systematic construction of systems with a high spontaneous information flux. Finally, we simultaneously maximize information flux and the mean period length of cyclic attractors in the state-space of these dynamical networks. Our results are potentially useful for the construction of RNNs that serve as short-time memories or pattern generators.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/neco_a_01651"
    },
    {
        "id": 1455,
        "title": "Improved Recurrent Neural Networks for Text Classification and Dynamic Sylvester Equation Solving",
        "authors": "Weijie Chen, Jie Jin, Dimitrios Gerontitis, Lixin Qiu, Jingcan Zhu",
        "published": "2023-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11176-6"
    },
    {
        "id": 1456,
        "title": "Motion Prediction Of Traffic Agents With Hybrid Recurrent-Convolutional Neural Networks",
        "authors": "Vasileios Lagoutaris, Konstantinos Moustakas",
        "published": "2023-6-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsp58604.2023.10167941"
    },
    {
        "id": 1457,
        "title": "Fracture Estimation Based on Deformation History with Recurrent Neural Networks",
        "authors": "Muhammed Adil Yatkin, Mihkel Kõrgesaar",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00152"
    },
    {
        "id": 1458,
        "title": "Algorithm for Mobile Robot Localization Based on Recurrent Convolutional Neural Networks",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/wcse.2023.06.029"
    },
    {
        "id": 1459,
        "title": "Prediction of Deep Ice Layer Thickness Using Adaptive Recurrent Graph Neural Networks",
        "authors": "Benjamin Zalatan, Maryam Rahnemoonfar",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222391"
    },
    {
        "id": 1460,
        "title": "Comparisons of Stock Prediction Methods Based on Recurrent Neural Networks",
        "authors": "George Shao",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "These instructions give you guidelines for preparing papers for DRP. Use this document as a template if you are using Microsoft Word 6.0 or later. Otherwise, use this document as an instruction set. The electronic file of your paper will be formatted further at DRP. Paper titles should be written in uppercase and lowercase letters, not all uppercase. Avoid writing long formulas with subscripts in the title; short formulas that identify the elements are fine (e.g., \"Nd-Fe-B\"). Do not write “(Invited)” in the title. Full names of authors are preferred in the author field, but are not required. Put a space between authors’ initials. The abstract must be a concise yet comprehensive reflection of what is in your article. In particular, the abstract must be self-contained, without abbreviations, footnotes, or references. It should be a microcosm of the full article. The abstract must be between 100 - 300 words. Be sure that you adhere to these limits; otherwise, you will need to edit your abstract accordingly. The abstract must be written as one paragraph, and should not contain displayed mathematical equations or tabular material. The abstract should include three or four different keywords or phrases, as this will help readers to find it. It is important to avoid over-repetition of such phrases as this can result in a page being rejected by search engines. Ensure that your abstract reads well and is grammatically correct.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5376"
    },
    {
        "id": 1461,
        "title": "Investigating the Use of Spatial Transformer Networks and Recurrent Neural Networks for Medical Image Segmentation",
        "authors": "Vineet Saxena, M N Nachappa, Ritu Shree",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470837"
    },
    {
        "id": 1462,
        "title": "Convolutional Recurrent Neural Networks for Medical Image Recognition",
        "authors": "Pankaj Saraswat, Rohaila Naaz, Kavitha R",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470932"
    },
    {
        "id": 1463,
        "title": "Universal Recurrent Event Memories for Streaming Data",
        "authors": "Ran Dou, Jose Principe",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191277"
    },
    {
        "id": 1464,
        "title": "A Combined Model Based on Recurrent Neural Networks and Graph Convolutional Networks for Financial Time Series Forecasting",
        "authors": "Ana Lazcano, Pedro Javier Herrera, Manuel Monge",
        "published": "2023-1-2",
        "citations": 22,
        "abstract": "Accurate and real-time forecasting of the price of oil plays an important role in the world economy. Research interest in forecasting this type of time series has increased considerably in recent decades, since, due to the characteristics of the time series, it was a complicated task with inaccurate results. Concretely, deep learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have appeared in this field with promising results compared to traditional approaches. To improve the performance of existing networks in time series forecasting, in this work two types of neural networks are brought together, combining the characteristics of a Graph Convolutional Network (GCN) and a Bidirectional Long Short-Term Memory (BiLSTM) network. This is a novel evolution that improves existing results in the literature and provides new possibilities in the analysis of time series. The results confirm a better performance of the combined BiLSTM-GCN approach compared to the BiLSTM and GCN models separately, as well as to the traditional models, with a lower error in all the error metrics used: the Root Mean Squared Error (RMSE), the Mean Squared Error (MSE), the Mean Absolute Percentage Error (MAPE) and the R-squared (R2). These results represent a smaller difference between the result returned by the model and the real value and, therefore, a greater precision in the predictions of this model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11010224"
    },
    {
        "id": 1465,
        "title": "Artificial intelligence-based Medicine Recognition System using faster Recurrent Convolutional Neural Networks",
        "authors": "P.Ranjith Kumar, Tarun Jaiswal, Satyabrata Jena",
        "published": "2023",
        "citations": 0,
        "abstract": "Recently, chronic patients are taking multiple medications incorrectly and taking the wrong medications due to similarity of drugs.It is possible that taking the improper medication can result in hazardous interactions with other medications or they will counteract the intended benefits of the medications, resulting in extra severe repercussions such as acute complications. The conventional methods are failed to provide the maximum efficiency. Therefore, this article is focused on implementation of faster recurrent convolutional neural networks (FR-CNN), which is capable of extracting the features from images. FR-CNN mainly used to analyze the patterns of the medicines and extracts the deep features. Further, classification of medicines is carried out by comparing with ground truth labels. The simulation results shows that the proposed system resulted in superior performance as compared to state of art approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.58599/ijsmien.2023.1105"
    },
    {
        "id": 1466,
        "title": "Uncertainty-Aware QoT Forecasting in Optical Networks with Bayesian Recurrent Neural Networks",
        "authors": "Nicola Di Cicco, Jacopo Talpini, Mëmëdhe Ibrahimi, Marco Savi, Massimo Tornatore",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278767"
    },
    {
        "id": 1467,
        "title": "Performing decision-making tasks through dynamics of recurrent neural networks trained with reinforcement learning",
        "authors": "Roman Kononov, Oleg Maslennikov",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dcna59899.2023.10290321"
    },
    {
        "id": 1468,
        "title": "Fraud Detection using Recurrent Neural Networks for Digital Wallet Security",
        "authors": "Can Iscan, Fatma Patlar Akbulut",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ubmk59864.2023.10286651"
    },
    {
        "id": 1469,
        "title": "Automating Medical Image Segmentation with Recurrent Neural Networks",
        "authors": "Neeraj Das, Ajay Kumar Upadhyay, Deepak Mehta",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470816"
    },
    {
        "id": 1470,
        "title": "Improving Wind Speed Uncertainty Forecasts Using Recurrent Neural Networks",
        "authors": "Juri Backes, Wolfgang Renz",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "For integration of growing amounts of volatile renewable energy in the European electricity system, reliable weather prognosis gains importance. But, depending on weather conditions, forecast reliability of wind speed for predicting wind power can vary drastically with time. Thus, relevance of risk-aware system operation strategies is increasing based on wind speed uncertainty a measure of which is provided by the standard deviations of ensemble forecasts of the German Weather Service. However, lacking validity of this measure is known as a long-standing problem.Therefore, this work investigates how machine learning based on a suitably selected set of physical quantities of weather ensemble data as well as historic wind data allows for a more realistic uncertainty quantification. A recurrent neural network (RNN) based sequence-to-sequence architecture is implemented and probabilistic wind speed forecasts are generated for a region in northern Germany.The results are evaluated and compared with the forecasts of the German Weather Service thereby revealing improved validity of such deep-learning based uncertainty measures.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/18.6806"
    },
    {
        "id": 1471,
        "title": "Graph Generation with Recurrent and Graph Neural Networks",
        "authors": "Xikun Huang, Yangyang Li, Chaoqun Fei, Chuanqing Wang",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/medai59581.2023.00033"
    },
    {
        "id": 1472,
        "title": "Reconstructing computational system dynamics from neural data with recurrent neural networks",
        "authors": "Daniel Durstewitz, Georgia Koppe, Max Ingo Thurm",
        "published": "2023-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41583-023-00740-7"
    },
    {
        "id": 1473,
        "title": "Recurrent Neural Networks (RNNs) to improve EEG-based person identification",
        "authors": "Youssef Mohamed, Ahmed M. Anter, Ahmed B. Zaky",
        "published": "2023-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217750"
    },
    {
        "id": 1474,
        "title": "Recurrent Neural Networks for Improved Medical Image Classification",
        "authors": "Umesh Kumar Singh, Kavitha R, Pankaj Saraswat",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470908"
    },
    {
        "id": 1475,
        "title": "A Comparative Study of Convolutional Neural Networks and Recurrent Neural Networks for Chord Recognition",
        "authors": "Hania Nawaz Khan, Sibghatullah Bazai, Zubair Zaland, Sibghatullah Durrani, Saad Aslam, Angela Amphawan, Fatima Ali, Tse-Kian Neo",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "Using Mel-spectrograms, this study evaluates the effectiveness of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs). Mel-spectrograms are justified by their non- linearity and similarity to the human hearing system. This study uses over 200 tracks by The Beatles and Queen collected through the Music Information Retrieval Evaluation Exchange. Data augmentation approaches are used to increase accuracy on unusual chords. This paper presents a 3-layer 2D CNN model trained on major and minor chords and then expanded to different types of chords. The dataset demonstrates that both models can recognize musical chords across various genres. We compare the proposed results to the existing literature and demonstrate the effectiveness of the proposed methodology. As a result of our analysis, we found that the CNN and RNN models were 79% and 76% accurate, respectively. The presented findings suggest that CNNs and RNNs are suitable models for chord recognition using Mel-spectrograms. Data augmentation can be an effective technique for improving accuracy on rare chords.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15379/ijmst.v10i2.1837"
    },
    {
        "id": 1476,
        "title": "Combining recurrent and Graph Neural Networks to predict the next place’s category",
        "authors": "Cláudio G.S. Capanema, Guilherme S. de Oliveira, Fabrício A. Silva, Thais R.M.B. Silva, Antonio A.F. Loureiro",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.adhoc.2022.103016"
    },
    {
        "id": 1477,
        "title": "Multi-Level Node Authorization using Recurrent Neural Networks for Secure Health Monitoring System",
        "authors": "V. Lakshman Narayana, P. Syamalatha, P. Vatsalya, V. SriCharitha, V. Akhila",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscna58489.2023.10370543"
    },
    {
        "id": 1478,
        "title": "Efficient Extraction and Automated Thyroid Prediction with an Optimized Gated Recurrent Unit in Recurrent Neural Networks",
        "authors": "Et al. Nagavali Saka",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Computer-aided tools are becoming increasingly important in medical diagnostics. This paper introduces the Efficient Feature Extraction Based Recurrent Neural Network (FERNN) for computer-aided thyroid disease prediction. The FERNN model uses a Gated Recurrent Unit Recurrent Neural Network (GRU-RNN) optimized with the COOT Optimization Algorithm.The study begins by gathering data from an open-source system and preprocessing it using min-max normalization to address missing values. The preprocessed data undergoes a two-level feature extraction (TLFE) procedure. In the first level, a ranked filter feature set technique is used to prioritize features based on medical expert recommendations. In the second level, a variety of metrics, including information gain, gain ratio, chi-square, and relief, are used to rank and select features. A composite measure guided by fuzzy logic is then used to select a judicious subset of features. The FERNN model uses the GRU-RNN to classify thyroid diseases in the databases. To optimise, the COOT optimization method is employed. The model's weights. The FERNN model was put into practise in MATLAB and assessed with a variety of statistical metrics, including kappa, accuracy, precision, recall, sensitivity, specificity, and the F-measure. The proposed methodology was benchmarked against traditional techniques, including the deep belief neural network (DBN), artificial neural network (ANN), and support vector machine (SVM).",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i9.9607"
    },
    {
        "id": 1479,
        "title": "Optimal Energy Forecasting Using Hybrid Recurrent Neural Networks",
        "authors": "Elumalaivasan Poongavanam, Padmanathan Kasinathan, Kulothungan Kanagasabai",
        "published": "2023",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.030101"
    },
    {
        "id": 1480,
        "title": "Methane Concentration Forecasting Based on Sentinel-5P Products and Recurrent Neural Networks",
        "authors": "Theofani Psomouli, Ioannis Kansizoglou, Antonios Gasteratos",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "The increase in the concentration of geological gas emissions in the atmosphere and particularly the increase of methane is considered by the majority of the scientific community as the main cause of global climate change. The main reasons that place methane at the center of interest, lie in its high global warming potential (GWP) and its lifetime in the atmosphere. Anthropogenic processes, like engineering geology ones, highly affect the daily profile of gasses in the atmosphere. Should direct measures be taken to reduce emissions of methane, immediate global warming mitigation could be achieved. Due to its significance, methane has been monitored by many space missions over the years and as of 2017 by the Sentinel-5P mission. Considering the above, we conclude that monitoring and predicting future methane concentration based on past data is of vital importance for the course of climate change over the next decades. To that end, we introduce a method exploiting state-of-the-art recurrent neural networks (RNNs), which have been proven particularly effective in regression problems, such as time-series forecasting. Aligned with the green artificial intelligence (AI) initiative, the paper at hand investigates the ability of different RNN architectures to predict future methane concentration in the most active regions of Texas, Pennsylvania and West Virginia, by using Sentinel-5P methane data and focusing on computational and complexity efficiency. We conduct several empirical studies and utilize the obtained results to conclude the most effective architecture for the specific use case, establishing a competitive prediction performance that reaches up to a 0.7578 mean squared error on the evaluation set. Yet, taking into consideration the overall efficiency of the investigated models, we conclude that the exploitation of RNN architectures with less number of layers and a restricted number of units, i.e., one recurrent layer with 8 neurons, is able to better compensate for competitive prediction performance, meanwhile sustaining lower computational complexity and execution time. Finally, we compare RNN models against deep neural networks along with the well-established support vector regression, clearly highlighting the supremacy of the recurrent ones, as well as discuss future extensions of the introduced work.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/geosciences13060183"
    },
    {
        "id": 1481,
        "title": "The Smart Intuitive Natural Language Understanding with Recurrent Neural Networks",
        "authors": "R Raghavendra, Megha Pandeya, Deepak Kumar",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470796"
    },
    {
        "id": 1482,
        "title": "Active Hypothesis Testing in Unknown Environments Using Recurrent Neural Networks and Model Free Reinforcement Learning",
        "authors": "George Stamatelis, Nicholas Kalouptsidis",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289731"
    },
    {
        "id": 1483,
        "title": "Location Estimation of Moving Targets by Passive Sonobuoy and Recurrent Deep Neural Network",
        "authors": "Hooshang Abbaspour, Ghazal Najafi",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isncc58260.2023.10323838"
    },
    {
        "id": 1484,
        "title": "Deep Learning for Marginal Bayesian Posterior Inference with Recurrent Neural Networks",
        "authors": "Thayer Fisher, Alex Luedtke, Marco Carone, Noah Simon",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5705/ss.202020.0348"
    },
    {
        "id": 1485,
        "title": "Recurrent Neural Networks for Solving Photovoltaic System Dynamics",
        "authors": "Md Rifat Hossain, Sumit Paudyal, Tuyen Vu",
        "published": "2023-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isgt-la56058.2023.10328292"
    },
    {
        "id": 1486,
        "title": "The Smart Improving of Translation Models Using Recurrent Neural Networks",
        "authors": "Anjali Singh, Mayur Agarwal, Gowrishankar J",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470485"
    },
    {
        "id": 1487,
        "title": "A Deep Convolutional Gated Recurrent Unit for CT Image Reconstruction",
        "authors": "Masaki Ikuta, Jun Zhang",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3169569"
    },
    {
        "id": 1488,
        "title": "Retracted: Prediction of IoT Traffic Using the Gated Recurrent Unit Neural Network- (GRU-NN-) Based Predictive Model",
        "authors": "",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9820840"
    },
    {
        "id": 1489,
        "title": "Stability Analysis of Recurrent Neural Networks With Time-Varying Delay by Flexible Terminal Interpolation Method",
        "authors": "Zhanshan Wang, Yufeng Tian",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3188161"
    },
    {
        "id": 1490,
        "title": "User2Vec: A Novel Representation for the Information of the Social Networks for Stock Market Prediction Using Convolutional and Recurrent Neural Networks",
        "authors": "Pegah Eslamieh, Mehdi Shajari, Ahmad Nickabadi",
        "published": "2023-7-1",
        "citations": 1,
        "abstract": "Predicting stock market trends is an intriguing and complex problem, which has drawn considerable attention from the research community. In recent years, researchers have employed machine learning techniques to develop prediction models by using numerical market data and textual messages on social networks as their primary sources of information. In this article, we propose User2Vec, a novel approach to improve stock market prediction accuracy, which contributes to more informed investment decision making. User2Vec is a unique method that recognizes the unequal impact of different user opinions on specific stocks, and it assigns weights to these opinions based on the accuracy of their associated social metrics. The User2Vec model begins by encoding each message as a vector. These vectors are then fed into a convolutional neural network (CNN) to generate an aggregated feature vector. Following this, a stacked bi-directional long short-term memory (LSTM) model provides the final representation of the input data over a period. LSTM-based models have shown promising results by effectively capturing the temporal patterns in time series market data. Finally, the output is fed into a classifier that predicts the trend of the target stock price for the next day. In contrast to previous attempts, User2Vec considers not only the sentiment of the messages, but also the social information associated with the users and the text content of the messages. It has been empirically proven that this inclusion provides valuable information for predicting stock direction, thereby significantly enhancing prediction accuracy. The proposed model was rigorously evaluated using various combinations of market data, encoded messages, and social features. The empirical studies conducted on the Dow Jones 30 stock market showed the model’s superiority over existing state-of-the-art models. The findings of these experiments reveal that including social information about users and their tweets, in addition to the sentiment and textual content of their messages, significantly improves the accuracy of stock market prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11132950"
    },
    {
        "id": 1491,
        "title": "Maneuver Prediction Using Traffic Scene Graphs via Graph Neural Networks and Recurrent Neural Networks",
        "authors": "Petrit Rama, Naim Bajcinca",
        "published": "2023-9",
        "citations": 0,
        "abstract": " The driving process involves many layers of planning and navigation, in order to enable tractable solutions for the otherwise highly complex problem of autonomous driving. One such layer involves an inherent discrete layer of decision-making corresponding to tactical maneuvers. Inspired by this, the focus of this work is predicting high-level maneuvers for the ego-vehicle. As maneuver prediction is fundamentally feedback-structured, it requires modeling techniques that take into consideration the interaction awareness of the traffic agents involved. This work addresses this challenge by modeling the traffic scenario as an interaction graph and proposing three deep learning architectures for interaction-aware tactical maneuver prediction of the ego-vehicle. These architectures are based on graph neural networks (GNNs) for extracting spatial features among traffic agents and recurrent neural networks (RNNs) for extracting dynamic motion patterns of surrounding agents. These proposed architectures have been trained and evaluated using BLVD dataset. Moreover, this dataset is expanded using data augmentation, data oversampling and data undersampling approaches, to strengthen model’s resilience and enhance the learning process. Lastly, we compare proposed learning architectures for ego-vehicle maneuver prediction in various driving circumstances with various numbers of surrounding traffic agents in order to effectively verify the proposed architectures. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s1793351x23620040"
    },
    {
        "id": 1492,
        "title": "Data-Driven Tabulation for Chemistry Integration Using Recurrent Neural Networks",
        "authors": "Yu Zhang, Qingguo Lin, Wenli Du, Feng Qian",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3175301"
    },
    {
        "id": 1493,
        "title": "Daily Prediction Model of Photovoltaic Power Generation Using a Hybrid Architecture of Recurrent Neural Networks and Shallow Neural Networks",
        "authors": "Wilson Castillo-Rojas, Juan Bekios-Calfa, César Hernández",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "In recent years, photovoltaic energy has become one of the most implemented electricity generation options to help reduce environmental pollution suffered by the planet. Accuracy in this photovoltaic energy forecasting is essential to increase the amount of renewable energy that can be introduced to existing electrical grid systems. The objective of this work is based on developing various computational models capable of making short-term forecasting about the generation of photovoltaic energy that is generated in a solar plant. For the implementation of these models, a hybrid architecture based on recurrent neural networks (RNN) with long short-term memory (LSTM) or gated recurrent units (GRU) structure, combined with shallow artificial neural networks (ANN) with multilayer perceptron (MLP) structure, is established. RNN models have a particular configuration that makes them efficient for processing ordered data in time series. The results of this work have been obtained through controlled experiments with different configurations of its hyperparameters for hybrid RNN-ANN models. From these, the three models with the best performance are selected, and after a comparative analysis between them, the forecasting of photovoltaic energy production for the next few hours can be determined with a determination coefficient of 0.97 and root mean square error (RMSE) of 0.17. It is concluded that the proposed and implemented models are functional and capable of predicting with a high level of accuracy the photovoltaic energy production of the solar plant, based on historical data on photovoltaic energy production.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/2592405"
    },
    {
        "id": 1494,
        "title": "Modeling an intrusion detection using recurrent neural networks",
        "authors": "Mariam Ibrahim, Ruba Elhafiz",
        "published": "2023-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jer.2023.100013"
    },
    {
        "id": 1495,
        "title": "Building Performance Prediction Model Using CAD Technology and Recurrent Neural Networks",
        "authors": "Nian Wu, Zhao Ye",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s18.66-80"
    },
    {
        "id": 1496,
        "title": "Structural health monitoring of steel moment frame buildings via sequence-based recurrent neural networks",
        "authors": "Khashayar Heydarpour, Doeun Choe, Kyungyong Chung",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00154"
    },
    {
        "id": 1497,
        "title": "Research on Photovoltaic Power Generation Load Prediction based on Recurrent Neural Networks",
        "authors": "Zhitao Zhang, Yujia Dong",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnepe60694.2023.10429183"
    },
    {
        "id": 1498,
        "title": "ES-dRNN: A Hybrid Exponential Smoothing and Dilated Recurrent Neural Network Model for Short-Term Load Forecasting",
        "authors": "Slawek Smyl, Grzegorz Dudek, Paweł Pełka",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3259149"
    },
    {
        "id": 1499,
        "title": "ICOR: improving codon optimization with recurrent neural networks",
        "authors": "Rishab Jain, Aditya Jain, Elizabeth Mauro, Kevin LeShane, Douglas Densmore",
        "published": "2023-4-4",
        "citations": 5,
        "abstract": "Abstract\nBackground\nIn protein sequences—as there are 61 sense codons but only 20 standard amino acids—most amino acids are encoded by more than one codon. Although such synonymous codons do not alter the encoded amino acid sequence, their selection can dramatically affect the expression of the resulting protein. Codon optimization of synthetic DNA sequences is important for heterologous expression. However, existing solutions are primarily based on choosing high-frequency codons only, neglecting the important effects of rare codons. In this paper, we propose a novel recurrent-neural-network based codon optimization tool, ICOR, that aims to learn codon usage bias on a genomic dataset of Escherichia coli. We compile a dataset of over 7,000 non-redundant, high-expression, robust genes which are used for deep learning. The model uses a bidirectional long short-term memory-based architecture, allowing for the sequential context of codon usage in genes to be learned. Our tool can predict synonymous codons for synthetic genes toward optimal expression in Escherichia coli.\n\nResults\nWe demonstrate that sequential context achieved via RNN may yield codon selection that is more similar to the host genome. Based on computational metrics that predict protein expression, ICOR theoretically optimizes protein expression more than frequency-based approaches. ICOR is evaluated on 1,481 Escherichia coli genes as well as a benchmark set of 40 select DNA sequences whose heterologous expression has been previously characterized. ICOR’s performance is measured across five metrics: the Codon Adaptation Index, GC-content, negative repeat elements, negative cis-regulatory elements, and codon frequency distribution.\n\nConclusions\nThe results, based on in silico metrics, indicate that ICOR codon optimization is theoretically more effective in enhancing recombinant expression of proteins over other established codon optimization techniques. Our tool is provided as an open-source software package that includes the benchmark set of sequences used in this study.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12859-023-05246-8"
    },
    {
        "id": 1500,
        "title": "ODRNN: optimized deep recurrent neural networks for automatic detection of leukaemia",
        "authors": "K. Dhana Shree, S. Logeswari",
        "published": "2024-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11760-024-03062-y"
    }
]