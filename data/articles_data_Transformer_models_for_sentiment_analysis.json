[
    {
        "id": 20371,
        "title": "Effectiveness of Data Augmentation and Ensembling Using Transformer-Based Models for Sentiment Analysis: Software Engineering Perspective",
        "authors": "Zubair Tusar, Sadat Sharfuddin, Muhtasim Abid, Md. Haque, Md. Mostafa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012092500003538"
    },
    {
        "id": 20372,
        "title": "CrudeBERT: Applying Economic Theory Towards Fine-Tuning Transformer-Based Sentiment Analysis Models to the Crude Oil Market",
        "authors": "Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011749600003467"
    },
    {
        "id": 20373,
        "title": "Sentiment Analysis on Code-Mixed Tamil-English Corpus: A Comprehensive Study of Transformer-Based Models",
        "authors": "M. Sangeetha, K. Nimala",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNLP, or natural language processing, is a subfield of AI that aims to equip computers with the ability to understand and analyze human language. Sentiment analysis is a widely used application of NLP, particularly for examining attitudes expressed in online conversations. Nevertheless, many social media comments are written in languages that are not native to the authors, making sentiment analysis more difficult, especially for languages with limited resources, such as Tamil. To tackle this issue, a code-mixed and sentiment-annotated corpus in Tamil and English was created. This article will explain how the corpus was established, including the process of data collection and the assignment of polarities. The article will also explore the agreement between annotators and the results of sentiment analysis performed on the corpus. This work signifies various performance metrics such as precision, recall, support, and F1-score for the transformer-based model such as BERT, RoBerta, and XLM-RoBerta. Among the various models, XLM-Robert shows slightly significant positive results on the code-mixed corpus when compared to the state of art models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3418283/v1"
    },
    {
        "id": 20374,
        "title": "Sentiment Analysis of Thai Stock Reviews Using Transformer Models",
        "authors": "Pongsatorn Harnmetta, Taweesak Samanchuen",
        "published": "2022-6-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jcsse54890.2022.9836278"
    },
    {
        "id": 20375,
        "title": "Comparative Analysis of Sentiment in Original and Summarized Tweets: Leveraging Transformer Models for Enhanced NLP Insights",
        "authors": "Kun Bu, Kandethody Ramachandran",
        "published": "2024-2-24",
        "citations": 0,
        "abstract": "This paper investigates the sentiments of Twitter users towards the emergent topic of ChatGPT, leveraging advanced techniques in natural language processing (NLP) and sentiment analysis (SA). Our approach uniquely incorporates a dual setting for sentiment analysis: one analyzes the sentiments of original, full-length tweets, while the other first condenses these tweets into succinct summaries before performing sentiment analysis. By employing this dual approach, we are able to offer a comparative analysis of sentiment assessment pre- and post-text summarization, exploring the accuracy and reliability of the summarized sentiments. Central to our methodology is the application of Transformer models, specifically ProphetNet, which facilitates a deeper and more nuanced understanding of the original text. Unlike traditional methods that rely on keyword extraction and aggregation, our approach generates coherent and contextually rich summaries, providing a novel lens for sentiment analysis. This research contributes to the field by presenting a comprehensive study comparing sentiment analysis outcomes between original texts and their summarized counterparts, and examining the effectiveness of different NLP techniques, namely NLTK and the Transformer-based ProphetNet model. The findings offer valuable insights into the dynamics of sentiment analysis in the context of social media and the efficacy of state-of-the-art NLP technologies in processing complex, real-world data.",
        "link": "http://dx.doi.org/10.5121/csit.2024.140404"
    },
    {
        "id": 20376,
        "title": "Sentiment Analysis of StockTwits Using Transformer Models",
        "authors": "Aysun Bozanta, Sabrina Angco, Mucahit Cevik, Ayse Basar",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00204"
    },
    {
        "id": 20377,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109"
    },
    {
        "id": 20378,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109.v2"
    },
    {
        "id": 20379,
        "title": "Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable communication scholars to adopt modern computational methods for valid and reliable sentiment analysis, we propose a free and open web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates different sentiment analysis tools including transformer-based deep learning models and sentiment dictionaries. TLSA enables users with limited computational resources to use these tools and validate their results at once. Two cases showed that transformer-based models provided more accurate measurement of sentiments in tweets, compared to lexicon-based methods. The performance of different tools varied to a large extent based on the dataset, suggesting the importance to validate different sentiment tools in a specific study. TLSA is expected to empower communication scholars to reap the benefit of state-of-the-art computational methods for valid measurement and insight discovery. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21781109.v1"
    },
    {
        "id": 20380,
        "title": "Transformer-based deep learning models for the sentiment analysis of social media data",
        "authors": "Sayyida Tabinda Kokab, Sohail Asghar, Shehneela Naz",
        "published": "2022-7",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.array.2022.100157"
    },
    {
        "id": 20381,
        "title": "Efficient Transformer based Sentiment Classification Models",
        "authors": "Leeja Mathew, Bindu V R",
        "published": "2022-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31449/inf.v46i8.4332"
    },
    {
        "id": 20382,
        "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
        "authors": "Fanfei Meng, David Demeter",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24638745"
    },
    {
        "id": 20383,
        "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
        "authors": "Fanfei Meng, David Demeter",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24638745.v1"
    },
    {
        "id": 20384,
        "title": "Comparison of Transformer Based and Traditional Models on Sentiment Analysis on Social Media Datasets",
        "authors": "Arif Ridho Lubis, Yulia Fatmi, Deden Witarsyah",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2ie60547.2023.10331232"
    },
    {
        "id": 20385,
        "title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.34"
    },
    {
        "id": 20386,
        "title": "Convolutional Transformer with Sentiment-aware Attention for Sentiment Analysis",
        "authors": "Pengfei Li, Peixiang Zhong, Jiaheng Zhang, Kezhi Mao",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206796"
    },
    {
        "id": 20387,
        "title": "Novelty fused image and text models based on deep neural network and transformer for multimodal sentiment analysis",
        "authors": "Bui Thanh Hung, Nguyen Hoang Minh Thu",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-18105-8"
    },
    {
        "id": 20388,
        "title": "Evaluation of transformer models for financial targeted sentiment analysis in Spanish",
        "authors": "Ronghao Pan, José Antonio García-Díaz, Francisco Garcia-Sanchez, Rafael Valencia-García",
        "published": "2023-5-9",
        "citations": 2,
        "abstract": "Nowadays, financial data from social media plays an important role to predict the stock market. However, the exponential growth of financial information and the different polarities of sentiment that other sectors or stakeholders may have on the same information has led to the need for new technologies that automatically collect and classify large volumes of information quickly and easily for each stakeholder. In this scenario, we conduct a targeted sentiment analysis that can automatically extract the main economic target from financial texts and obtain the polarity of a text towards such main economic target, other companies and society in general. To this end, we have compiled a novel corpus of financial tweets and news headlines in Spanish, constituting a valuable resource for the Spanish-focused research community. In addition, we have carried out a performance comparison of different Spanish-specific large language models, with MarIA and BETO achieving the best results. Our best result has an overall performance of 76.04%, 74.16%, and 68.07% in macro F1-score for the sentiment classification towards the main economic target, society, and other companies, respectively, and an accuracy of 69.74% for target detection. We have also evaluated the performance of multi-label classification models in this context and obtained a performance of 71.13%.",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1377"
    },
    {
        "id": 20389,
        "title": "Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models",
        "authors": "David Rozado, Ruth Hughes, Jamin Halberstadt",
        "published": "2022-10-18",
        "citations": 14,
        "abstract": "This work describes a chronological (2000–2019) analysis of sentiment and emotion in 23 million headlines from 47 news media outlets popular in the United States. We use Transformer language models fine-tuned for detection of sentiment (positive, negative) and Ekman’s six basic emotions (anger, disgust, fear, joy, sadness, surprise) plus neutral to automatically label the headlines. Results show an increase of sentiment negativity in headlines across written news media since the year 2000. Headlines from right-leaning news media have been, on average, consistently more negative than headlines from left-leaning outlets over the entire studied time period. The chronological analysis of headlines emotionality shows a growing proportion of headlines denoting anger, fear, disgust and sadness and a decrease in the prevalence of emotionally neutral headlines across the studied outlets over the 2000–2019 interval. The prevalence of headlines denoting anger appears to be higher, on average, in right-leaning news outlets than in left-leaning news media.",
        "link": "http://dx.doi.org/10.1371/journal.pone.0276367"
    },
    {
        "id": 20390,
        "title": "Fine-Grained Sentiment Analysis of College Email Texts Based on Transformer",
        "authors": "Jiangning Xie, Zhen Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4529274"
    },
    {
        "id": 20391,
        "title": "An Ensemble of Arabic Transformer-based Models for Arabic Sentiment Analysis",
        "authors": "Ikram El Karfi, Sanaa El Fkihi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2022.0130865"
    },
    {
        "id": 20392,
        "title": "Can Lexicon-Based Sentiment Analysis Boost Performances of Transformer-Based Models?",
        "authors": "Lindung Parningotan Manik, Harry Susianto, Arawinda Dinakaramani, Niken Pramanik, Totok Suhardijanto",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conmedia60526.2023.10428401"
    },
    {
        "id": 20393,
        "title": "NLP-LISAC at SemEval-2023 Task 12: Sentiment Analysis for Tweets expressed in African languages via Transformer-based Models",
        "authors": "Abdessamad Benlahbib, Achraf Boumhidi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.28"
    },
    {
        "id": 20394,
        "title": "Building an Ensemble of Transformer Models for Arabic Dialect Classification and Sentiment Analysis",
        "authors": "Abdullah Salem Khered, Ingy Yasser Hassan Abdou Abdelhalim, Riza Batista-Navarro",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.53"
    },
    {
        "id": 20395,
        "title": "The relationship between stockholder sentiment lag and stock price prediction accuracy: an empirical analysis based on LSTM and Transformer models",
        "authors": "Haoqian Guo, Yuxin Xu",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "This comprehensive paper investigates the nuanced relationship between retail investor sentiment and stock prices in the Chinese stock market, with a special focus on the role of sentiment time lags. Using advanced time-series models, specifically Long Short-Term Memory (LSTM) and Transformer models, the study takes a detailed look at the stock price of Oriental Finance (Ticker: 300059A). The research employs varying time lags of stockholder sentiment (ranging from 0 to 4 days) as well as technical indicators to predict stock prices. Our experimental design involves comparative analysis under these two models to isolate the impact of sentiment time lags on prediction accuracy. The results reveal that the LSTM model consistently outperforms the Transformer model, particularly when a 4-day lag in stockholder sentiment is considered. Interestingly, the prediction accuracy did not uniformly improve with increased sentiment lags, suggesting a complex relationship between investor sentiment and stock prices.",
        "link": "http://dx.doi.org/10.54097/hset.v70i.13888"
    },
    {
        "id": 20396,
        "title": "Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?",
        "authors": "Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, Lingxiao Jiang",
        "published": "2020-9",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsme46990.2020.00017"
    },
    {
        "id": 20397,
        "title": "Advancement in Bangla Sentiment Analysis: A Comparative Study of Transformer-Based and Transfer Learning Models for E-commerce Sentiment Classification",
        "authors": "Zishan Ahmed, Shakib Sadat Shanto, Akinul Islam Jony",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Background: As a direct result of the Internet's expansion, the quantity of information shared by Internet users across its numerous platforms has increased. Sentiment analysis functions at a higher level when there are more available perspectives and opinions. However, the lack of labeled data significantly complicates sentiment analysis utilizing Bangla natural language processing (NLP). In recent years, nevertheless, due to the development of more effective deep learning models, Bangla sentiment analysis has improved significantly.\nObjective: This article presents a curated dataset for Bangla e-commerce sentiment analysis obtained solely from the \"Daraz\" platform. We aim to conduct sentiment analysis in Bangla for binary and understudied multiclass classification tasks.\nMethods: Transfer learning (LSTM, GRU) and Transformers (Bangla-BERT) approaches are compared for their effectiveness on our dataset. To enhance the overall performance of the models, we fine-tuned them.\nResults: The accuracy of Bangla-BERT was highest for both binary and multiclass sentiment classification tasks, with 94.5% accuracy for binary classification and 88.78% accuracy for multiclass sentiment classification.\nConclusion: Our proposed method performs noticeably better classifying multiclass sentiments in Bangla than previous deep learning techniques.\nKeywords: Bangla-BERT, Deep Learning, E-commerce, NLP, Sentiment Analysis",
        "link": "http://dx.doi.org/10.20473/jisebi.9.2.181-194"
    },
    {
        "id": 20398,
        "title": "Sentiment Analysis using Transformer Based Pre-Trained Models for the Hindi Language",
        "authors": "Akshat Verma, Shivam Walbe, Ishwar Wani, Ritesh Wankhede, Radha Thakare, Sanika Patankar",
        "published": "2022-2-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sceecs54111.2022.9741028"
    },
    {
        "id": 20399,
        "title": "ADAPTATION OF DOMAIN-SPECIFIC TRANSFORMER MODELS WITH TEXT OVERSAMPLING FOR SENTIMENT ANALYSIS OF SOCIAL MEDIA POSTS ON COVID-19 VACCINE",
        "authors": "Anmol Bansal, Arjun Choudhry, Anubhav Sharma, Seba Susan",
        "published": "2023-3-10",
        "citations": 3,
        "abstract": "Covid-19 has spread across the world and many different vaccines have been developed to counter its surge. To identify the correct sentiments associated with the vaccines from social media posts, this paper aims to fine-tune pre-trained transformer models on tweets associated with different Covid vaccines, specifically RoBERTa, XLNet and BERT which are recently introduced state-of-the-art bi-directional transformer models, and domain-specific transformer models BERTweet and CT-BERT that are pre-trained on Covid-19 tweets. We further explore the option of data augmentation by text oversampling using LMOTE to improve the accuracies of these models, specifically, for small sample datasets where there is an imbalanced class distribution among the positive, negative and neutral sentiment classes. Our results summarize our findings on the suitability of text oversampling for imbalanced, small sample datasets that are used to fine-tune state-of-the-art pre-trained transformer models, and the utility of having domain-specific transformer models for the classification task.",
        "link": "http://dx.doi.org/10.7494/csci.2023.24.2.4761"
    },
    {
        "id": 20400,
        "title": "Arabic Dialect Identification and Sentiment Classification using Transformer-based Models",
        "authors": "Joseph Attieh, Fadi Hassan",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wanlp-1.54"
    },
    {
        "id": 20401,
        "title": "Semantics Squad at BLP-2023 Task 2: Sentiment Analysis of Bangla Text with Fine Tuned Transformer Based Models",
        "authors": "Krishno Dey, Md. Arid Hasan, Prerona Tarannum, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.41"
    },
    {
        "id": 20402,
        "title": "BpHigh at WASSA 2023: Using Contrastive Learning to build Sentence Transformer models for Multi-Class Emotion Classification in Code-mixed Urdu",
        "authors": "Bhavish Pahwa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.59"
    },
    {
        "id": 20403,
        "title": "A Novel Approach for Sentiment Analysis on social media using BERT &amp; ROBERTA Transformer-Based Models",
        "authors": "Kundeti Naga Prasanthi, Rallabandi Eswari Madhavi, Degala Naga Sai Sabarinadh, Battula Sravani",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126206"
    },
    {
        "id": 20404,
        "title": "Sentiment analysis using machine learning  classification models",
        "authors": "Umair Hassan",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is written to compare different classification models for sentiment analysis",
        "link": "http://dx.doi.org/10.36227/techrxiv.19783384"
    },
    {
        "id": 20405,
        "title": "Sentiment analysis using machine learning  classification models",
        "authors": "Umair Hassan",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is written to compare different classification models for sentiment analysis",
        "link": "http://dx.doi.org/10.36227/techrxiv.19783384.v1"
    },
    {
        "id": 20406,
        "title": "Tweets Topic Classification and Sentiment Analysis Based on Transformer-Based Language Models",
        "authors": "Ranju Mandal, Jinyan Chen, Susanne Becken, Bela Stantic",
        "published": "2023-5",
        "citations": 3,
        "abstract": "People provide information on their thoughts, perceptions, and activities through a wide range of channels, including social media. The wide acceptance of social media results in vast volume of valuable data, in variety of format as well as veracity. Analysis of such ‘big data’ allows organizations and analysts to make better and faster decisions. However, this data had to be quantified and information has to be extracted, which can be very challenging because of possible data ambiguity and complexity. To address information extraction, many analytic techniques, such as text mining, machine learning, predictive analytics, and diverse natural language processing, have been proposed in the literature. Recent advances in Natural Language Understanding-based techniques more specifically transformer-based architectures can solve sequence-to-sequence modeling tasks while handling long-range dependencies efficiently. In this work, we applied transformer-based sequence modeling on short texts’ topic classification and sentiment analysis from user-posted tweets. Applicability of models is investigated on posts from the Great Barrier Reef tweet dataset and obtained findings are encouraging providing insight that can be valuable for researchers working on classification of large datasets as well as large number of target classes.",
        "link": "http://dx.doi.org/10.1142/s2196888822500269"
    },
    {
        "id": 20407,
        "title": "A Fine Line Between Irony and Sincerity: Identifying Bias in Transformer Models for Irony Detection",
        "authors": "Aaron Maladry, Els Lefever, Cynthia Van Hee, Veronique Hoste",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.28"
    },
    {
        "id": 20408,
        "title": "Automated measures of sentiment via transformer- and lexicon-based sentiment analysis (TLSA)",
        "authors": "Xinyan Zhao, Chau-Wai Wong",
        "published": "2023-11-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42001-023-00233-8"
    },
    {
        "id": 20409,
        "title": "Leveraging Readability and Sentiment in Spam Review Filtering Using Transformer Models",
        "authors": "Sujithra Kanmani, Surendiran Balasubramanian",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/csse.2023.029953"
    },
    {
        "id": 20410,
        "title": "Improving Sentiment Analysis in Online Course Reviews with BERT and Transformer Attention Mechanism",
        "authors": "Raisa Fairooz Meem, Khandaker Tabin Hasan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the field of text mining, sentiment analysis has grown significantly in importance for understanding user reactions. This knowledge contributes to improving several features of given goods or services. Considering its adequacy, sentiment analysis can play a significant role in the educational field, where student input is essential for enhancing curricula and recommending courses, resources, and other elements. Likewise, because of the higher accuracy and decreased complexity, transfer learning models are increasingly being used in a variety of domains, which suggests that the educational sector will substantially benefit from incorporating them into its related domains.\nTo leverage the advantages of both sentiment analysis and transfer learning techniques, this study suggests a Transfer Learning Model (BERT-base-uncased) based on BERT (Bidirectional Encoder Representations from Transformers) using the Transformer Attention Mechanism to analyse the sentiment of student feedback from various online courses. Two applications of the suggested model have been demonstrated- one with a sequence classifier and another without it. The result shows a significant accuracy of 76% with a high recall value with the sequence classifier.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3741963/v1"
    },
    {
        "id": 20411,
        "title": "Saccade Inspired Attentive Visual Patch Transformer for Image Sentiment Analysis",
        "authors": "Jing Zhang, Jiangpei Liu, Xinzhou Zhang, Han Sun, Zhe Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685795"
    },
    {
        "id": 20412,
        "title": "PICT-CLRL at WASSA 2023 Empathy, Emotion and Personality Shared Task: Empathy and Distress Detection using Ensembles of Transformer Models",
        "authors": "Tanmay Chavan, Kshitij Deshpande, Sheetal Sonawane",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.wassa-1.52"
    },
    {
        "id": 20413,
        "title": "Adapting Transformer Networks for Document Summarization and Sentiment Analysis",
        "authors": "Nidhi Mehra",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "Due to the increasing number of text-based content sources, the demand for effective sentiment analysis and document summarization techniques has been increasing. Several transformer-based models, including “ELECTRA, BERT, XLNet, RoBERTa, DistilBERT, and ALBERT” have emerged as promising alternatives to traditional methods. This paper aims to study the effectiveness of the different transformer models for performing sentiment analysis and document summarization on the Yelp dataset. The paper aims to analyze the various transformer models' performance on the tasks, identify their weaknesses, and suggest possible improvements. It also thoroughly studies the Yelp dataset, which has over 5 million reviews. The paper introduces the different transformer models that are used for performing document summarization and analysis on the Yelp dataset. We then perform evaluation on these models using various metrics to measure their performance. Some of these include ROUGE, F1-score, AUC-ROC, and accuracy. According to the paper's experimental results, the RoBERTa and BERT models perform better than the other transformer models when it comes to document summarization. In addition, we identified the weaknesses and strengths of each model. We suggest implementing domain-specific training and fine-tuning techniques to improve their performance. The results of the experiment revealed that the RoBERTa and BERT models perform better than the other ones when it comes to document summarization. We also found that the models have weaknesses and strengths, and we suggest using domain-specific training and fine-tuning techniques to improve their performance. The paper contributes to the literature related to the use of transformer-based models in sentiment analysis and documents summarization by providing an extensive analysis of the different models' performance in the Yelp dataset. It also suggests various modifications to improve their capabilities.",
        "link": "http://dx.doi.org/10.17762/msea.v70i2.2325"
    },
    {
        "id": 20414,
        "title": "Sentiment Analysis in Turkish Using Transformer-Based Deep Learning Models",
        "authors": "Oktay Ozturk, Alper Ozcan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-31956-3_1"
    },
    {
        "id": 20415,
        "title": "Stacking BERT based Models for Arabic Sentiment Analysis",
        "authors": "Hasna Chouikhi, Hamza Chniter, Fethi Jarray",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010648400003064"
    },
    {
        "id": 20416,
        "title": "Switch-Transformer Sentiment Analysis Model for Arabic Dialects that Utilizes Mixture of Experts Mechanism",
        "authors": "Laith H. Baniata, Sangwoo Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent times, models like the Transformer have showcased remarkable prowess in tasks related to natural language processing. However, these models tend to be excessively intricate and demand extensive training. Additionally, while the multi-head self-attention mechanism in the Transformer model aims to capture semantic connections between words in a sequence, it encounters limitations when handling short sequences, thereby limiting its effectiveness in 5-polarity Arabic sentiment analysis tasks. The switch-transformer model has recently emerged as a high-performing alternative. Nevertheless, when these models are trained using single-task learning, they often fall short of achieving exceptional performance and struggle to generate robust latent feature representations, especially when working with compact datasets. This challenge is particularly pronounced in the case of the Arabic dialect, which is considered a low-resource language. Given these constraints, this research introduces a novel approach to sentiment analysis in Arabic text. This method leverages multitask learning in tandem with the switch-transformer shared encoder to enhance model adaptability and refine sentence representation. By introducing a mixture of expert (MoE) mechanism that break down the problem into smaller, more manageable sub-problems, the model becomes adept at handling lengthy sequences and intricate input-output relationships, benefiting both five-point and three-polarity Arabic sentiment analysis tasks. This proposed model effectively discerns sentiment in Arabic dialect sentences. The empirical results highlight the outstanding performance of the suggested model, as evidenced in evaluations on the Hotel Arabic-Reviews Dataset, the Book Reviews Arabic Dataset, and the LARB dataset.",
        "link": "http://dx.doi.org/10.20944/preprints202311.0187.v1"
    },
    {
        "id": 20417,
        "title": "DeepTweet: Leveraging Transformer-based Models for Enhanced Fake News Detection in Twitter Sentiment Analysis",
        "authors": "N. Vallileka, P. Sundaravadivel, U. Karthikeyan, R. Santhana Krishnan, K. Lakshmi Narayanan, S. Sundararajan",
        "published": "2023-10-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i-smac58438.2023.10290217"
    },
    {
        "id": 20418,
        "title": "Chinese text sentiment analysis based on transformer model",
        "authors": "Huang Jing, Cai Yang",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwecai55315.2022.00043"
    },
    {
        "id": 20419,
        "title": "Knowledge-guided Transformer for Joint Topic and Sentiment Analysis of Chinese Classical Poetry",
        "authors": "Bin Wu, Yuting Wei, Yangfu Zhu, Linmei Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe analyses of the topic and sentiment are essential for understanding Chinese classical poetry and historical culture. Existing works fail to consider the lexical knowledge mined from poem annotations, which partly contains some information about the topic and sentiment. In addition, most works ignore the interdependence and diversity of the topic and sentiment in one poem. Hence, in this paper, we propose a Knowledge-guided Transformer Model (KTM) for joint multiple topic and sentiment analysis of Chinese classical poetry. Specifically, we first respectively construct two lexical dictionaries for the topic and sentiment based on the poem annotations. Then we take full advantage of the lexical dictionaries with a knowledge-based mask-transformer to represent poems. Furthermore, considering the correlations between the topic and sentiment, our model jointly classifies the multiple topics and sentiments in Chinese classical poetry by stacking the two subtasks. Considering there is no public dataset, we release a new Chinese classical poetry dataset CCPD for joint multiple topic and sentiment analysis. Extensive experiments demonstrate that our model achieves state-of-the-art performance on both topic and sentiment analyses, especially on tail labels.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2725320/v1"
    },
    {
        "id": 20420,
        "title": "Forecasting the S&amp;P 500 Index Using Mathematical-Based Sentiment Analysis and Deep Learning Models: A FinBERT Transformer Model and LSTM",
        "authors": "Jihwan Kim, Hui-Sang Kim, Sun-Yong Choi",
        "published": "2023-8-29",
        "citations": 2,
        "abstract": "Stock price prediction has been a subject of significant interest in the financial mathematics field. Recently, interest in natural language processing models has increased, and among them, transformer models, such as BERT and FinBERT, are attracting attention. This study uses a mathematical framework to investigate the effects of human sentiment on stock movements, especially in text data. In particular, FinBERT, a domain-specific language model based on BERT tailored for financial language, was employed for the sentiment analysis on the financial texts to extract sentiment information. In this study, we use “summary” text data extracted from The New York Times, representing concise summaries of news articles. Accordingly, we apply FinBERT to the summary text data to calculate sentiment scores. In addition, we employ the LSTM (Long short-term memory) methodology, one of the machine learning models, for stock price prediction using sentiment scores. Furthermore, the LSTM model was trained by stock price data and the estimated sentiment scores. We compared the predictive power of LSTM models with and without sentiment analysis based on error measures such as MSE, RMSE, and MAE. The empirical results demonstrated that including sentiment scores through the LSTM model led to improved prediction accuracy for all three measures. These findings indicate the significance of incorporating news sentiment into stock price predictions, shedding light on the potential impact of psychological factors on financial markets. By using the FinBERT transformer model, this study aimed to investigate the interplay between sentiment and stock price predictions, contributing to a deeper understanding of mathematical-based sentiment analysis in finance and its role in enhancing forecasting in financial mathematics. Furthermore, we show that using summary data instead of entire news articles is a useful strategy for mathematical-based sentiment analysis.",
        "link": "http://dx.doi.org/10.3390/axioms12090835"
    },
    {
        "id": 20421,
        "title": "Linked Data Models for Sentiment and Emotion Analysis in Social Networks",
        "authors": "C.A. Iglesias, J.F. Sánchez-Rada, G. Vulcu, P. Buitelaar",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-804412-4.00004-8"
    },
    {
        "id": 20422,
        "title": "Persian Sentiment Analysis via a Transformer Model concerning Banking Sector",
        "authors": "Seyed Jamal Haddadi, Elham Khoeini, Pezhman Salmani, Mehdi Beygi, Mehrdad Haddad Khoshkar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe competitive landscape of a country's banking sector necessitates an in-depth understanding of customer satisfaction levels concerning the services provided. Presently, customers predominantly express their feedback via social media platforms in the form of posts and comments. This study endeavors to create a highly accurate sentiment detection algorithm for the Iranian banking system, utilizing a transformer model. In the initial stages, we collected data by crawling comments from Twitter, which are subsequently labeled and filtered according to the names of Iranian banks, dating from 2019. Following this, an optimized Deep Neural Network (DNN)-based pre-trained ParsBERT model, a monolingual Persian model, is fine-tuned using this data. Finally, our model is evaluated on a test dataset, and the results are validated by comparing them with the original multilingual BERT, Bidirectional Long Short-Term Memory (Bi-LSTM) network, and four other classification methods. To address the Out-Of-Vocabulary (OOV) issue, a character-level embedding is incorporated in conjunction with the word-level embedding. This approach aids in tackling the multitude of variations observed in non-native words, extracting character-level features using a character-level Bi-LSTM. The proposed model highlights the statistical superiority of our method when compared to the other methods evaluated.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3211293/v1"
    },
    {
        "id": 20423,
        "title": "Cgt: A Clause Graph Transformer Structure for Aspect-Based Sentiment Analysis",
        "authors": "Zelong Su, Bin Gao, Xiaoou Pan, Yu Ji, Shutian Liu, Zhengjun Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697938"
    },
    {
        "id": 20424,
        "title": "Attention! Transformer with Sentiment on Cryptocurrencies Price Prediction",
        "authors": "Huali Zhao, Martin Crane, Marija Bezbradica",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011103400003197"
    },
    {
        "id": 20425,
        "title": "Transformer based Twitter Trending Topics Sentiment Drift Analysis in Real Time",
        "authors": "E Susi, A P Shanthi",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoac59537.2023.10249424"
    },
    {
        "id": 20426,
        "title": "Comparative Analysis of Different Transformer Based Architectures Used in Sentiment Analysis",
        "authors": "Keval Pipalia, Rahul Bhadja, Madhu Shukla",
        "published": "2020-12-4",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smart50582.2020.9337081"
    },
    {
        "id": 20427,
        "title": "Empathy and Distress Prediction using Transformer Multi-output Regression and Emotion Analysis with an Ensemble of Supervised and Zero-Shot Learning Models",
        "authors": "Flor Miriam Del Arco, Jaime Collado-Montañez, L. Alfonso Ureña, María-Teresa Martín-Valdivia",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wassa-1.23"
    },
    {
        "id": 20428,
        "title": "Generative Models for Sentiment Analysis and Opinion Mining",
        "authors": "Hongning Wang, ChengXiang Zhai",
        "published": "2017",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-55394-8_6"
    },
    {
        "id": 20429,
        "title": "Overcoming Transformer Fine-Tuning process to improve Twitter Sentiment Analysis for Spanish Dialects",
        "authors": "Daniel Palomino",
        "published": "2020-12-12",
        "citations": 0,
        "abstract": "Is there an effective Spanish Sentiment Analysis algorithm? The aim of this paper is to answer this question. The task is challenging because there are several dialects for the Spanish Language. Thus, identically written words could have several meanings and polarities regarding Spanish speaking countries. To tackle this multidialect issue we rely on a transfer learning approach. To do so, we train a BERT language model to “transfer” general features of the Spanish language. Then, we fine-tune the language model to specific dialects. BERT is also used to generate contextual data augmentation aimed to prevent overfitting. Finally, we build the polarity classifier and propose a fine-tuning step using groups of layers. Our design choices allow us to achieve state-of-the-art results regarding multidialect benchmark datasets.",
        "link": "http://dx.doi.org/10.52591/lxai202012124"
    },
    {
        "id": 20430,
        "title": "Implementation of sentiment analysis in stock market prediction using variants of GARCH models",
        "authors": "V. Vijayalakshmi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-443-22009-8.00002-1"
    },
    {
        "id": 20431,
        "title": "A Comparative Analysis of Sentiment Classification Models for Improved Performance Optimization",
        "authors": "Varun Iyer",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.171073040.03879551/v1"
    },
    {
        "id": 20432,
        "title": "Aspect Based Sentiment Analysis using French Pre-Trained Models",
        "authors": "Abderrahman Essebbar, Bamba Kane, Ophélie Guinaudeau, Valeria Chiesa, Ilhem Quénel, Stéphane Chau",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010382705190525"
    },
    {
        "id": 20433,
        "title": "Multimodal Sentiment Analysis via Efficient Multimodal Transformer and Modality-Aware Adaptive Training Strategy",
        "authors": "Chaoyue Ding, Daoming Zong, Baoxiang Li, Song Zhang, Xiaoxu Zhu, Guiping Zhong, Dinghao Zhou",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3606039.3613113"
    },
    {
        "id": 20434,
        "title": "Lexicon Integrated CNN Models with Attention for Sentiment Analysis",
        "authors": "Bonggun Shin, Timothy Lee, Jinho D. Choi",
        "published": "2017",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-5220"
    },
    {
        "id": 20435,
        "title": "Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models",
        "authors": "Batsergelen Myagmar, Jie Li, Shigetomo Kimura",
        "published": "2019",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2952360"
    },
    {
        "id": 20436,
        "title": "A Hybrid Sentiment Analysis Method of Transformer and Capsule Network for Hotel Reviews",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2021.040110"
    },
    {
        "id": 20437,
        "title": "Multimodal Phased Transformer for Sentiment Analysis",
        "authors": "Junyan Cheng, Iordanis Fostiropoulos, Barry Boehm, Mohammad Soleymani",
        "published": "2021",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.189"
    },
    {
        "id": 20438,
        "title": "Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach",
        "authors": "Luca Bacco, Andrea Cimino, Felice Dell’Orletta, Mario Merone",
        "published": "2021-9-8",
        "citations": 10,
        "abstract": "In recent years, the explainable artificial intelligence (XAI) paradigm is gaining wide research interest. The natural language processing (NLP) community is also approaching the shift of paradigm: building a suite of models that provide an explanation of the decision on some main task, without affecting the performances. It is not an easy job for sure, especially when very poorly interpretable models are involved, like the almost ubiquitous (at least in the NLP literature of the last years) transformers. Here, we propose two different transformer-based methodologies exploiting the inner hierarchy of the documents to perform a sentiment analysis task while extracting the most important (with regards to the model decision) sentences to build a summary as the explanation of the output. For the first architecture, we placed two transformers in cascade and leveraged the attention weights of the second one to build the summary. For the other architecture, we employed a single transformer to classify the single sentences in the document and then combine the probability scores of each to perform the classification and then build the summary. We compared the two methodologies by using the IMDB dataset, both in terms of classification and explainability performances. To assess the explainability part, we propose two kinds of metrics, based on benchmarking the models’ summaries with human annotations. We recruited four independent operators to annotate few documents retrieved from the original dataset. Furthermore, we conducted an ablation study to highlight how implementing some strategies leads to important improvements on the explainability performance of the cascade transformers model.",
        "link": "http://dx.doi.org/10.3390/electronics10182195"
    },
    {
        "id": 20439,
        "title": "Exploring Hugging Face Transformer Library Impact on Sentiment Analysis",
        "authors": "Aashita Chhabra, Kiran Chaudhary, Mansaf Alam",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781032614083-6"
    },
    {
        "id": 20440,
        "title": "EmptyMind at BLP-2023 Task 2: Sentiment Analysis of Bangla Social Media Posts using Transformer-Based Models",
        "authors": "Karnis Fatema, Udoy Das, Md Ayon Mia, Md Sajidul Mowla, Mahshar Yahan, Md Fayez Ullah, Arpita Sarker, Hasan Murad",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.39"
    },
    {
        "id": 20441,
        "title": "Arabic Sentiment Analysis based on Neural Network Models: Overview and Comparison",
        "authors": "Youssra Zahidi, Yacine El Younoussi, Yassine Al-Amrani",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010728700003101"
    },
    {
        "id": 20442,
        "title": "Video Review Analysis via Transformer-Based Sentiment Change Detection",
        "authors": "Zilong Wu, Siyuan Huang, Rui Zhang, Lin Li",
        "published": "2020-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mipr49039.2020.00074"
    },
    {
        "id": 20443,
        "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
        "authors": "Fanfei Meng, Chen-Ao Wang",
        "published": "2024-3-25",
        "citations": 0,
        "abstract": "We propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model. ",
        "link": "http://dx.doi.org/10.54254/2755-2721/50/20241326"
    },
    {
        "id": 20444,
        "title": "Multimodal Sentiment Recognition Based on Sentiment Fusion Transformer",
        "authors": "Xinyu Guan, Yuan Wang",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccasit58768.2023.10351509"
    },
    {
        "id": 20445,
        "title": "TransSentLog: Interpretable Anomaly Detection Using Transformer and Sentiment Analysis on Individual Log Event",
        "authors": "Tuan-Anh Pham, Jong-Hoon Lee",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3311146"
    },
    {
        "id": 20446,
        "title": "Incorporating Transformer Models for Sentiment Analysis and News Classification in Khmer",
        "authors": "Md Rifatul Islam Rifat, Abdullah Al Imran",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-91434-9_10"
    },
    {
        "id": 20447,
        "title": "Cross-modal sentiment analysis based on Transformer and image-text collaborative interaction",
        "authors": "Yuanyuan Zhang, Liang Wang, Haicheng Wang",
        "published": "2023-6-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cedl60560.2023.00024"
    },
    {
        "id": 20448,
        "title": "A Survey of Transformer and GNN for Aspect-based Sentiment Analysis",
        "authors": "Wenqing Luo, Wei Zhang, Yihang Zhao",
        "published": "2021-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisai54367.2021.00074"
    },
    {
        "id": 20449,
        "title": "TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis",
        "authors": "Zilong Wang, Zhaohong Wan, Xiaojun Wan",
        "published": "2020-4-20",
        "citations": 58,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3366423.3380000"
    },
    {
        "id": 20450,
        "title": "Comparing Deep Neural Networks to Traditional Models for Sentiment Analysis in Turkish Language",
        "authors": "Savaş Yildirim",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-1216-2_12"
    },
    {
        "id": 20451,
        "title": "Assessing State-of-the-Art Sentiment Models on State-of-the-Art\n            Sentiment Datasets",
        "authors": "Jeremy Barnes, Roman Klinger, Sabine Schulte im Walde",
        "published": "2017",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-5202"
    },
    {
        "id": 20452,
        "title": "Multimodal Sentiment Analysis Based on Pre-LN Transformer Interaction",
        "authors": "Huihui Song, Jianping Li, Zhiping Xia, Zongping Yang, Xiao Du",
        "published": "2022-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec53115.2022.9734328"
    },
    {
        "id": 20453,
        "title": "Peer Review #1 of \"Context-based sentiment analysis on customer reviews using machine learning linear models (v0.1)\"",
        "authors": "",
        "published": "2021-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.813v0.1/reviews/1"
    },
    {
        "id": 20454,
        "title": "Transformer based ensemble for emotion detection",
        "authors": "Aditya Kane, Shantanu Patankar, Sahil Khose, Neeraja Kirtane",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.wassa-1.25"
    },
    {
        "id": 20455,
        "title": "Transformer-Based Indonesian Language Model for Emotion Classification and Sentiment Analysis",
        "authors": "Hendri Ahmadian, Taufik Fuadi Abidin, Hammam Riza, Kahlil Muchtar",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitcom60176.2023.10442970"
    },
    {
        "id": 20456,
        "title": "An ensemble transformer-based model for Arabic sentiment analysis",
        "authors": "Omar Mohamed, Aly M. Kassem, Ali Ashraf, Salma Jamal, Ensaf Hussein Mohamed",
        "published": "2022-12-24",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13278-022-01009-0"
    },
    {
        "id": 20457,
        "title": "OPI at SemEval-2022 Task 10: Transformer-based Sequence Tagging with Relation Classification for Structured Sentiment Analysis",
        "authors": "Rafał Poświata",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.190"
    },
    {
        "id": 20458,
        "title": "Rumor detection methodology based on sentiment analysis and the transformer model with decision-level fusion",
        "authors": "Zhengqiao Zhong",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "The use of transformer models in natural language processing (NLP) has gained significant attention in recent years due to their exceptional performance in various language tasks. This paper explores the application of transformer models in rumor detection, the relevant research on rumor detection, the use of transformer models, and the techniques used to boost the models performance. Ultimately, the purpose of this paper is to provide insight into the potential of transformer models in detecting rumors on social media. Unlike other rumor-detecting models, the author adds a sentiment analysis model as a supplement to rumor detection. Also, to address the issue of insufficient information in early-stage comments on rumors, this paper proposes a decision-level fusion method before the output layer, which effectively utilizes information from different sources and minimizes the negative impact of insufficient data sources. The early-stage rumor detection accuracy of the model is greatly enhanced by this method, therefore, the articles main contributions can be regarded as follows: First, this paper proposes a combination of an aspect level text sentiment analysis method according to syntactic features, gated recurrent units, and a self-attention mechanism. Experimental findings demonstrate that, compared to the original model without taking the sentiment analysis method into account, the proposed network model has advantages in accuracy and Macro F1 evaluation indexes. Second, a cross-text rumor-detecting method based on Decision-level fusion is proposed. Its advantage is that when the cross-text data source is incomplete and a certain text is missing, another text can be used to continue the analysis. Experimental findings show the effectiveness of this method in improving the accuracy of emotion recognition by integrating data from different modes. Third, a comparison is conducted between the performance of the Transformer-sentiment model and other related models, Text-CNN, Bi-LSTM, etc. The result shows that this integrated Transformer-sentiment model can not only solve the rumor detection tasks at higher accuracy, but can also overcome the shortcomings of the lack of datasets, which means that the model is more robust, and is able to detect rumors at the early stage of the rumor spreading process.",
        "link": "http://dx.doi.org/10.54254/2755-2721/27/20230200"
    },
    {
        "id": 20459,
        "title": "Multimodal transformer with adaptive modality weighting for multimodal sentiment analysis",
        "authors": "Yifeng Wang, Jiahao He, Di Wang, Quan Wang, Bo Wan, Xuemei Luo",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.127181"
    },
    {
        "id": 20460,
        "title": "Validity of Large Language Models for Sentiment Analysis: Evidence of performance comparable to human coders",
        "authors": "James Elsey",
        "published": "No Date",
        "citations": 0,
        "abstract": "Assessing the sentiment of content is a major focus of communication science. Previous research has compared the performance of ‘gold-standard’ methods – exemplified by trained human coders – with approaches such as crowd-sourcing, data dictionaries, and machine learning. Aggregated crowd-sourced assessments and trained human coders perform best, but trained coders may not be scalable and crowd-sourcing may raise financial, ethical, and practical issues. Large Language Models (LLMs) – artificial intelligence systems trained on vast datasets to process and generate human-like text – may combine the in-depth understanding of human coders with the speed and scalability of automated methods. We used the gold-standard sentiment coding for a corpus of Dutch economic news headlines from previous research. We found that two commercially available LLMs, Anthropic’s Claude 2 and OpenAI’s ChatGPT-4, could perform at levels approaching or matching single trained coders and crowdsourced ratings, and reliably exceeded previously reported machine learning and automated approaches. Using variation in prompts (‘prompt engineering’), we show that prompt details can affect performance, but even with minimal instruction the performance of Claude 2 approached human-level. LLMs are promising tools for scalable and high-quality sentiment analysis. Future research can consider their applications on long-form text and images.",
        "link": "http://dx.doi.org/10.31234/osf.io/kcuwy"
    },
    {
        "id": 20461,
        "title": "An Enhanced Transformer-Based Approach with Meta-Ensemble Learning for Arabic Sentiment Analysis",
        "authors": "Mohamed Ashraf, Mustafa Marzouk, Rania Kora, Ammar Mohammed",
        "published": "2023-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/miucc58832.2023.10278312"
    },
    {
        "id": 20462,
        "title": "Transformer-based Relation Detect Model for Aspect-based Sentiment Analysis",
        "authors": "Zixi Wei, Xiaofei Xu, Lijian Li, Kaixin Qin, Li Li",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533453"
    },
    {
        "id": 20463,
        "title": "Analyzing Public Sentiment on the Amazon Website: A GSK-Based Double Path Transformer Network Approach for Sentiment Analysis",
        "authors": "Lella Kranthi Kumar, Venkata Nagaraju Thatha, Pamula Udayaraju, D. Siri, G. Uday Kiran, B. N. Jagadesh, Ramesh Vatambeti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3368441"
    },
    {
        "id": 20464,
        "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
        "authors": "Jean-Benoit Delbrouck, Noé Tits, Mathilde Brousmiche, Stéphane Dupont",
        "published": "2020",
        "citations": 39,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.challengehml-1.1"
    },
    {
        "id": 20465,
        "title": "Analysis of comparative performance of deep learning models for sentiment analysis",
        "authors": "Mirza Murtaza",
        "published": "2021-4-19",
        "citations": 0,
        "abstract": "Abstract\r\nSentiment analysis of text can be performed using machine learning and natural language processing methods. However, there is no single tool or method that is effective in all cases. The objective of this research project is to determine the effectiveness of neural network-based architecture to perform sentiment analysis of customer comments and reviews, such as the ones on Amazon site. A typical sentiment analysis process involves text preparation (of acquired content), sentiment detection, sentiment classification and analysis of results. In this research, the objective is to a) identify the best approach for text preparation in a given application (text filtering approach to remove errors in data), and, most importantly, b) what is the best machine learning (feed forward neural nets, convolutional neural nets, Long Short-Term Memory networks) approach that provides best classification accuracy.\r\nIn this research, a set of three thousand two hundred reviews of food related products were used to train and experiment with a neural network-based sentiment analysis system. The neural network implementation of six different models provided close to one-hundred percent accuracy of test data, and a decent test accuracy in mid-80%. The results of the research would be useful to businesses in evaluating customer preferences for products or services.\r\n ",
        "link": "http://dx.doi.org/10.32473/flairs.v34i1.128739"
    },
    {
        "id": 20466,
        "title": "Sentiment Analysis of Product Reviews Using Deep Learning and Transformer Models: A Comparative Study",
        "authors": "Sheetal Kusal, Shruti Patil, Aashna Gupta, Harsh Saple, Devashish Jaiswal, Vaishnavi Deshpande, Ketan Kotecha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-8476-3_15"
    },
    {
        "id": 20467,
        "title": "Virtual Agents using Sequence to Sequence Models for Sentiment Analysis to deal with Anxiety and Depression",
        "authors": "Bishal Thapaliya, Swagat Niroula, Bikash Lamsal",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/iciae2019.034"
    },
    {
        "id": 20468,
        "title": "Sentiment Analysis of Steam Reviews Using Transformer Models",
        "authors": "Raghunath Reddy, Ahmed Abdul Naoman, Gollapudi Venkata Sriram Charan, Syed Naveed Fazal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-7137-4_70"
    },
    {
        "id": 20469,
        "title": "Sentiment Analysis of ChatGPT Tweets Using Transformer Algorithms",
        "authors": "Sugeng Winardi, Mohammad Diqi, Arum Kurnia Sulistyowati, Jelina Imlabla",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "This study explores the application of the Transformer model in sentiment analysis of tweets generated by ChatGPT. We used a Kaggle dataset consisting of 217,623 instances labeled as \"Good\", \"Bad\", and \"Neutral\". The Transformer model demonstrated high accuracy (90%) in classifying sentiments, particularly predicting \"Bad\" tweets. However, it showed slightly lower performance for the \"Good\" and \"Neutral\" categories, indicating areas for future research and model refinement. Our findings contribute to the growing body of evidence supporting deep learning methods in sentiment analysis and underscore the potential of AI models like Transformers in handling complex natural language processing tasks. This study broadens the scope for AI applications in social media sentiment analysis.",
        "link": "http://dx.doi.org/10.36499/jinrpl.v5i2.8632"
    },
    {
        "id": 20470,
        "title": "A Comparison of Machine Learning Algorithms and Transformer-based Methods for Multiclass Sentiment Analysis on Twitter",
        "authors": "Aakash Singh, Suresh Kumar",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10306507"
    }
]