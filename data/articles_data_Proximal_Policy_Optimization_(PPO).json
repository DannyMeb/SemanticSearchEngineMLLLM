[
    {
        "id": 16471,
        "title": "Exploring PID Control Constants Using Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)",
        "authors": "Dong-Kyu Lee, Hyeun-Jun Moon",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21086/ksles.2023.12.30.6.642"
    },
    {
        "id": 16472,
        "title": "PPO-ABR: Proximal Policy Optimization based Deep Reinforcement Learning for Adaptive BitRate streaming",
        "authors": "Mandan Naresh, Paresh Saxena, Manik Gupta",
        "published": "2023-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10182379"
    },
    {
        "id": 16473,
        "title": "Proposing Camera Calibration Method Using PPO (Proximal Policy Optimization) for Improving Camera Pose Estimations",
        "authors": "Haitham Al-Jabri, Takafumi Matsumaru",
        "published": "2018-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio.2018.8665088"
    },
    {
        "id": 16474,
        "title": "Fast-PPO: Proximal Policy Optimization with Optimal Baseline Method",
        "authors": "Zhu Xiao, Ning Xie, Guobiao Yang, Zhenjiang Du",
        "published": "2020-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pic50277.2020.9350833"
    },
    {
        "id": 16475,
        "title": "PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation",
        "authors": "Perttu Hamalainen, Amin Babadi, Xiaoxiao Ma, Jaakko Lehtinen",
        "published": "2020-9",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp49062.2020.9231618"
    },
    {
        "id": 16476,
        "title": "Proximal Policy Optimization (PPO)-Based Resource Allocation for Energy Harvesting Industrial Wireless Sensor",
        "authors": "Rongzhen Li, Lei Xu, Chengming Tang, Ping Wang, Wanli Liu, Junjie Gu, Zhicheng Cai, Rui Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nFor the purpose of overcoming the challenges of charging wireless sensors in the complicated industrial environment, researchers are concentrating more and more on sensor networks that can harvest energy.This paper looks at a wirelessly powered industrial sensor network where each sensor harvests energy from a specific radio frequency (RF) energy source and uses it to transmit data to a receiver.Two working modes are discussed of in this paper.One is the frequency division multiplexing (FDM) working mode, where the sensor simultaneously transmits data over orthogonal frequency bands while harvesting RF energy.Time division multiplexing (TDM), which divides each time slot into two successive intervals, is the second working mode.Data is transmitted and energy is harvested in the same frequency band, but at distinct intervals.Because the channel condition and energy harvesting process are unpredictable, an efficient resource allocation algorithm is required for the sensors.We propose a novel resource allocation algorithm based on reinforcement learning.The proposed algorithm achieves continuous resource allocation and is applicable for continuous states by using Proximal Policy Optimization (PPO).We also utilize entropy regularization, online normalization of state, reward scaling, and advantage normalization to improve the performance of resource allocation algorithm in real-world scenarios.In both FDM and TDM working modes, the proposed algorithm outperforms the greedy algorithm and random algorithm in terms of long-term throughput, according to the results of numerical simulations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3210671/v1"
    },
    {
        "id": 16477,
        "title": "PPO-RM: Proximal Policy Optimization Based Route Mutation for Multimedia Services",
        "authors": "Jiahao Shen, Tao Zhang, Bingchi Zhang, Weixiao Ji, Xiaohui Kuang, Changqiao Xu",
        "published": "2021-6-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc51323.2021.9498706"
    },
    {
        "id": 16478,
        "title": "Implementing action mask in proximal policy optimization (PPO) algorithm",
        "authors": "Cheng-Yen Tang, Chien-Hung Liu, Woei-Kae Chen, Shingchern D. You",
        "published": "2020-9",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.icte.2020.05.003"
    },
    {
        "id": 16479,
        "title": "PPO-TA: Adaptive task allocation via Proximal Policy Optimization for spatio-temporal crowdsourcing",
        "authors": "Bingxu Zhao, Hongbin Dong, Yingjie Wang, Tingwei Pan",
        "published": "2023-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110330"
    },
    {
        "id": 16480,
        "title": "Deep Reinforcement Learning Techniques For Solving Hybrid Flow Shop Scheduling Problems: Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C)",
        "authors": "Abdulrahman Nahhas, Andrey Kharitonov, Klaus Turowski",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24251/hicss.2022.206"
    },
    {
        "id": 16481,
        "title": "Value-Decomposition Multi-Agent Proximal Policy Optimization",
        "authors": "Yanhao Ma, Jie Luo",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10054763"
    },
    {
        "id": 16482,
        "title": "GAA-PPO: A novel graph adversarial attack method by incorporating proximal policy optimization",
        "authors": "Shuxin Yang, Xiaoyang Chang, Guixiang Zhu, Jie Cao, Weiping Qin, Youquan Wang, Zhendong Wang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126707"
    },
    {
        "id": 16483,
        "title": "Proximal Policy Optimization for Radiation Source Search",
        "authors": "Philippe Proctor, Christof Teuscher, Adam Hecht, Marek Osiński",
        "published": "No Date",
        "citations": 0,
        "abstract": "Rapid search and localization for nuclear sources can be an important aspect in preventing human harm from illicit material in dirty bombs or from contamination. In the case of a single mobile radiation detector, there are numerous challenges to overcome such as weak source intensity, multiple sources, background radiation, and the presence of obstructions, i.e., a non-convex environment. In this work, we investigate the sequential decision making capability of deep reinforcement learning in the nuclear source search context. A novel neural network architecture (RAD-A2C) based on the actor critic (A2C) framework and a particle filter gated recurrent unit for localization is proposed.  Performance is studied in a randomized 20 x 20 m convex and non-convex environment across a range of signal-to-noise ratio (SNR)s for a single detector and single source.  RAD-A2C performance is compared to both an information-driven controller that uses a bootstrap particle filter and to a gradient search (GS) algorithm. We find that the RAD-A2C has comparable performance to the information-driven controller across SNR in a convex environment and at lower computational complexity per action. The RAD-A2C far outperforms the GS algorithm in the non-convex environment with greater than 95% median completion rate for up to seven obstructions.",
        "link": "http://dx.doi.org/10.20944/preprints202108.0018.v1"
    },
    {
        "id": 16484,
        "title": "Combustion Optimization Study of Pulverized Coal Boiler Based on Proximal Policy Optimization Algorithm",
        "authors": "Xuecheng Wu, Hongnan Zhang, Huafeng Chen, Shifeng Wang, Lingling Gong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4542814"
    },
    {
        "id": 16485,
        "title": "Proximal Policy Optimization with Relative Pearson Divergence",
        "authors": "Taisuke Kobayashi",
        "published": "2021-5-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9560856"
    },
    {
        "id": 16486,
        "title": "Preferential Proximal Policy Optimization",
        "authors": "Tamilselvan Balasuntharam, Heidar Davoudi, Mehran Ebrahimi",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00048"
    },
    {
        "id": 16487,
        "title": "Integrated Path Planning and Control Through Proximal Policy Optimization for a Marine Current Turbine ⋆",
        "authors": "Arezoo Hasankhani, Yufei Tang, James VanZwieten",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4349106"
    },
    {
        "id": 16488,
        "title": "Quantum architecture search via truly proximal policy optimization",
        "authors": "Xianchao Zhu, Xiaokai Hou",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "AbstractQuantum Architecture Search (QAS) is a process of voluntarily designing quantum circuit architectures using intelligent algorithms. Recently, Kuo et al. (Quantum architecture search via deep\nreinforcement learning. arXiv preprint arXiv:2104.07715, 2021) proposed a deep reinforcement learning-based QAS (QAS-PPO) method, which used the Proximal Policy Optimization (PPO) algorithm to automatically generate the quantum circuit without any expert knowledge in physics. However, QAS-PPO can neither strictly limit the probability ratio between old and new policies nor enforce well-defined trust domain constraints, resulting in poor performance. In this paper, we present a new deep reinforcement learning-based QAS method, called Trust Region-based PPO with Rollback for QAS (QAS-TR-PPO-RB), to automatically build the quantum gates sequence from the density matrix only. Specifically, inspired by the research work of Wang, we employ an improved clipping function to implement the rollback behavior to limit the probability ratio between the new strategy and the old strategy. In addition, we use the triggering condition of the clipping based on the trust domain to optimize the policy by restricting the policy within the trust domain, which leads to guaranteed monotone improvement. Experiments on several multi-qubit circuits demonstrate that our presented method achieves better policy performance and lower algorithm running time than the original deep reinforcement learning-based QAS method.",
        "link": "http://dx.doi.org/10.1038/s41598-023-32349-2"
    },
    {
        "id": 16489,
        "title": "Real-Time Optimal Energy Management of Microgrid Based on Multi-Agent Proximal Policy Optimization",
        "authors": "Danlu Wang, Qiuye Sun, Hanguang Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4575022"
    },
    {
        "id": 16490,
        "title": "TEMPPO: Twin Entropy Maximized Proximal Policy Optimization",
        "authors": "S.A. Shahrokhi, Ali Ahmadi",
        "published": "2022-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csicc55295.2022.9780488"
    },
    {
        "id": 16491,
        "title": "Model-Based Reinforcement Learning via Proximal Policy Optimization",
        "authors": "Yuewen Sun, Xin Yuan, Wenzhang Liu, Changyin Sun",
        "published": "2019-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8996875"
    },
    {
        "id": 16492,
        "title": "A Study on Load Distribution of Gaming Server Using Proximal Policy Optimization",
        "authors": "Jung-min Park, Hye-young Kim, Sung Hyun Cho",
        "published": "2019-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7583/jkgs.2019.19.3.5"
    },
    {
        "id": 16493,
        "title": "Inventory Control with Lateral Transshipment Using Proximal Policy Optimization",
        "authors": "Ziang Liu, Tatsushi Nishi",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/docs60977.2023.10294547"
    },
    {
        "id": 16494,
        "title": "Mixed-Autonomy Traffic Control with Proximal Policy Optimization",
        "authors": "Haoran Wei, Xuanzhang Liu, Lena Mashayekhy, Keith Decker",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vnc48660.2019.9062809"
    },
    {
        "id": 16495,
        "title": "A New Approach for Drone Tracking with Drone Using Proximal Policy Optimization Based Distributed Deep Reinforcement Learning",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4393763"
    },
    {
        "id": 16496,
        "title": "Proximal Policy Optimization based computations offloading for delay optimization in UAV-assisted mobile edge computing",
        "authors": "Praveen Kumar,  Priyadarshni, Shivani Tripathi, Rajiv Misra",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386861"
    },
    {
        "id": 16497,
        "title": "Off-Policy Proximal Policy Optimization",
        "authors": "Wenjia Meng, Qian Zheng, Gang Pan, Yilong Yin",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Proximal Policy Optimization (PPO) is an important reinforcement learning method, which has achieved great success in sequential decision-making problems. However, PPO faces the issue of sample inefficiency, which is due to the PPO cannot make use of off-policy data. In this paper, we propose an Off-Policy Proximal Policy Optimization method (Off-Policy PPO) that improves the sample efficiency of PPO by utilizing off-policy data. Specifically, we first propose a clipped surrogate objective function that can utilize off-policy data and avoid excessively large policy updates. Next, we theoretically clarify the stability of the optimization process of the proposed surrogate objective by demonstrating the degree of policy update distance is consistent with that in the PPO. We then describe the implementation details of the proposed Off-Policy PPO which iteratively updates policies by optimizing the proposed clipped surrogate objective. Finally, the experimental results on representative continuous control tasks validate that our method outperforms the state-of-the-art methods on most tasks.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i8.26099"
    },
    {
        "id": 16498,
        "title": "Covariance Matrix Adaptation for Multi-Agent Proximal Policy Optimization",
        "authors": "Yiou Shen, Xiang Gao, Zhiwei Liang",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327347"
    },
    {
        "id": 16499,
        "title": "Proximal Policy Optimization Algorithm for Dynamic Pricing with Online Reviews",
        "authors": "Chao Wu, bi wenjie, Haiying Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4179218"
    },
    {
        "id": 16500,
        "title": "Decaying Clipping Range in Proximal Policy Optimization",
        "authors": "Monika Farsang, Luca Szegletes",
        "published": "2021-5-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saci51354.2021.9465602"
    },
    {
        "id": 16501,
        "title": "Counterfactual Misleading Inference Generation via Reinforced Proximal Policy Optimization",
        "authors": "Hsien-Yung Peng, Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, we propose to investigate Misleading Inference Generation, a new natural language generation task. The goal is to generate a counterfactual sentence for a context and a factual sentence. This paper proposes a framework based on BART and reinforcement learning for the misleading inference generation task. The experiment results show our model significantly outperforms the compared models, making our solution a necessary and strong baseline for future research toward misleading inference generation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1809942/v1"
    },
    {
        "id": 16502,
        "title": "Distributed Proximal Policy Optimization for Contention-Based Spectrum Access",
        "authors": "Akash Doshi, Jeffrey G. Andrews",
        "published": "2021-10-31",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf53345.2021.9723270"
    },
    {
        "id": 16503,
        "title": "Proximal Policy Optimization for User Association in Hybrid LiFi/WiFi Indoor Networks",
        "authors": "Peijun Hou, Nan Cen",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437559"
    },
    {
        "id": 16504,
        "title": "Resource Management Scheduling-Based on Proximal Policy Optimization",
        "authors": "Jingjing Xu, Huanhuan Fang",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlise57402.2022.00021"
    },
    {
        "id": 16505,
        "title": "Job Shop Scheduling Problem Using Proximal Policy Optimization",
        "authors": "Ziqing Wang, Wenzhu Liao",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieem58616.2023.10406397"
    },
    {
        "id": 16506,
        "title": "Proximal Policy Optimization for Explainable Recommended Systems",
        "authors": "Qian Feng, Geyang Xiao, Yuan Liang, Huifeng Zhang, Linlin Yan, Xiaoyu Yi",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/docs55193.2022.9967709"
    },
    {
        "id": 16507,
        "title": "Proximal policy optimization with adaptive threshold for symmetric relative density ratio",
        "authors": "Taisuke Kobayashi",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.rico.2022.100192"
    },
    {
        "id": 16508,
        "title": "Traffic Signal Control Method Based on Modified Proximal Policy Optimization",
        "authors": "Yaohui An, Jing Zhang",
        "published": "2022-8-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictle55577.2022.9901894"
    },
    {
        "id": 16509,
        "title": "Recurrent Proximal Policy Optimization Based Tractor-Trailer Wheeled Robot Automatic Parking Algorithm",
        "authors": "",
        "published": "2023-9-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14738/tecs.114.15355"
    },
    {
        "id": 16510,
        "title": "Proximal Policy Optimization-based Join Order Optimization with Spark SQL",
        "authors": "Kyeong-Min Lee, InA Kim, Kyu-Chul Lee",
        "published": "2021-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5573/ieiespc.2021.10.3.227"
    },
    {
        "id": 16511,
        "title": "Research on TCP Congestion Control Strategy Based on Proximal Policy Optimization",
        "authors": "Yuyu Yuan",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itaic58329.2023.10408879"
    },
    {
        "id": 16512,
        "title": "CamerAI: Chase Camera in a Dense Environment using a Proximal Policy Optimization-trained Neural Network",
        "authors": "James Rucks, Nikolaos Katzakis",
        "published": "2021-8-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619120"
    },
    {
        "id": 16513,
        "title": "Mobile Robotic Arm for Opening Doors Using Proximal Policy Optimization",
        "authors": "M Kokila, G Amalredge",
        "published": "2023-2-1",
        "citations": 0,
        "abstract": "The traditional robotic arm control method has strong dependence on the application scenario. To improve the reliability of the mobile robotic arm control when the scene is disturbed, this paper proposes a control method based on an improved proximal policy optimization algorithm. This study researches mobile robotic arms for opening doors. At first, the door handle position is obtained through an image-recognition method based on YOLOv5. Second, the simulation platform CoppeliaSim is used to realize the interaction between the robotic arm and the environment. Third, a control strategy based on a reward function is designed to train the robotic arm and applied to the opening-door task in the real environment. In this paper PPO algorithm is used to solve the result. The experimental results show that the proposed method can accelerate the convergence of the training process. Besides, our method can effectively reduce the jitter of the robotic arm and improve the stability of control.",
        "link": "http://dx.doi.org/10.46632/daai/3/2/20"
    },
    {
        "id": 16514,
        "title": "Angle of Arrival Passive Location Algorithm Based on Proximal Policy Optimization",
        "authors": "Yao Zhang, Zhongliang Deng, Yuhui Gao",
        "published": "2019-12-17",
        "citations": 10,
        "abstract": "Location technology is playing an increasingly important role in urban life. Various active and passive wireless positioning technologies for mobile terminals have attracted research attention. However, positioning signals experience serious interference in high-density residential areas or in the interior of large buildings. The main type of interference is that caused by non-line-of-sight (NLOS) propagation. In this paper, we present a new method for optimizing the angle of arrival (AOA) measurement to obtain high accuracy location results based on proximal policy optimization (PPO). PPO is a new family of policy gradient methods for reinforcement learning, which can be used to adjust the sampling data under different environments using stochastic gradient ascent. Therefore, PPO can correct the NLOS propagation errors to produce a clear AOA measurement data set without building an offline fingerprinting database. Then, we used the least square method to calculate the location. The simulation result shows that the AOA passive location algorithm based on PPO produced more accurate location information.",
        "link": "http://dx.doi.org/10.3390/electronics8121558"
    },
    {
        "id": 16515,
        "title": "Multi-agent Proximal Policy Optimization via Non-fixed Value Clipping",
        "authors": "Chiqiang Liu, Dazi Li",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10167264"
    },
    {
        "id": 16516,
        "title": "Research on the job shop scheduling problem based on digital twin and proximal policy optimization",
        "authors": "Lilan Liu, Kai Guo, Zenggui Gao, Jiaying Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nJob shop scheduling plays an important role in intelligent manufacturing. Effectively solving the scheduling problem is the key to realizing intelligent manufacturing. In the actual production process, there are various dynamic events, such as the problem of inserting orders when new orders arrive, which seriously affect the efficiency of scheduling execution. The traditional job shop scheduling algorithm has poor real-time response ability. When encountering emergencies, it needs to run again, and the time complexity is high. A traditional scheduling scheme cannot solve this problem well. To solve the above problems, we use the digital twin (DT) workshop to accurately capture the abnormal events in the production process in real time. Combined with the fast response ability of deep reinforcement learning (DRL), we propose a method to establish a digital twin workshop combined with deep reinforcement learning. In this method, we use a disjunctive graph to represent the state in reinforcement learning, a graph neural network (GNN) as the network structure, and the proximal policy optimization (PPO) algorithm to find the optimal scheduling scheme. We compare the scheduling results with heuristic scheduling rules and a meta heuristic algorithm to verify the feasibility of the algorithm. Finally, in the laboratory environment, aiming at the common order insertion problem in the actual production process, we verify the effectiveness of this job shop scheduling method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1355780/v1"
    },
    {
        "id": 16517,
        "title": "Evaluation of Proximal Policy Optimization with Extensions in Virtual Environments of Various Complexity",
        "authors": "Robert Rauch, Stefan Korecko, Juraj Gazda",
        "published": "2022-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radioelektronika54537.2022.9764924"
    },
    {
        "id": 16518,
        "title": "Usage of Proximal Policy Optimization Algorithm for Personnel Assignment in Railway Nodes",
        "authors": "Andrea Galadíková, Norbert Adamko",
        "published": "2023-6-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/idt59031.2023.10194403"
    },
    {
        "id": 16519,
        "title": "Advantage policy update based on proximal policy optimization",
        "authors": "Zilin Zeng, Junwei Wang, Zhigang Hu, Dongnan Su, Peng Shang",
        "published": "2023-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2667235"
    },
    {
        "id": 16520,
        "title": "Drones Tracking Adaptation Using Reinforcement Learning: Proximal Policy optimization",
        "authors": "Esra Alhadhrami, Amal El Fallah Seghrouchni, Frederic Barbaresco, Raed Abu Zitar",
        "published": "2023-5-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/irs57608.2023.10172435"
    },
    {
        "id": 16521,
        "title": "Multi-Agent Formation Control With Obstacle Avoidance Using Proximal Policy Optimization",
        "authors": "Priyam Sadhukhan, Rastko R. Selmic",
        "published": "2021-10-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9658635"
    },
    {
        "id": 16522,
        "title": "Data-Driven Smart Home Energy Management Based on Proximal Policy Optimization",
        "authors": "Ma Aoxiang, Jun Cao, Pedro Rodriguez",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327581"
    },
    {
        "id": 16523,
        "title": "Proximal Policy Optimization with Advantage Reuse Competition",
        "authors": "Yuhu Cheng, Qingbang Guo, Xuesong Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2024.3354694"
    },
    {
        "id": 16524,
        "title": "Pairs Trading Strategy Optimization Using Proximal Policy Optimization Algorithms",
        "authors": "Yi-Feng Chen, Wen-Yueh Shih, Hsu-Chao Lai, Hao-Chun Chang, Jiun-Long Huang",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigcomp57234.2023.00015"
    },
    {
        "id": 16525,
        "title": "Robust observer and proximal policy optimization-based VTOL vehicle attitude stabilization research",
        "authors": "Yanling Li, Feizhou Luo, Zhilei Ge",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsi58851.2023.10303824"
    },
    {
        "id": 16526,
        "title": "Dynamic Adjustment of Reward Function for Proximal Policy Optimization with Imitation Learning: Application to Automated Parking Systems",
        "authors": "Mohamad Albilani, Amel Bouzeghoub",
        "published": "2022-6-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827194"
    },
    {
        "id": 16527,
        "title": "Bipedal walking using deep reinforcement learning and proximal policy optimization",
        "authors": "Jhon Paul Feliciano Charaja Casas, Luca Borgonovi, Adriano Siqueira",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26678/abcm.enebi2022.eeb22-0007"
    },
    {
        "id": 16528,
        "title": "Low Carbon Economic Dispatch of Integrated Electricity-Gas Energy System Based on Proximal Policy Optimization",
        "authors": "Jiaming Lei, Aihua Jiang, Xinfei Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4051657"
    },
    {
        "id": 16529,
        "title": "A Proximal Policy Optimization Approach for Food Delivery Problem with Reassignment Due to Order Cancellation",
        "authors": "Yang Deng, Yimo Yan, Andy H.F. Chow, Zhili Zhou, Yong-Hong Kuo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4669849"
    },
    {
        "id": 16530,
        "title": "Accelerating Proximal Policy Optimization on CPU-FPGA Heterogeneous Platforms",
        "authors": "Yuan Meng, Sanmukh Kuppannagari, Viktor Prasanna",
        "published": "2020-5",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fccm48280.2020.00012"
    },
    {
        "id": 16531,
        "title": "Distributionally Robust Proximal Policy Optimization for Unit Commitment with Wind Power Uncertainty",
        "authors": "Yancheng Lu, Bo Wang",
        "published": "2023-1-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsie55183.2023.10175277"
    },
    {
        "id": 16532,
        "title": "Strategies for Using Proximal Policy Optimization in Mobile Puzzle Games",
        "authors": "Jeppe Theiss Kristensen, Paolo Burelli",
        "published": "2020-9-15",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3402942.3402944"
    },
    {
        "id": 16533,
        "title": "pH stablizing control of a neutral leaching process based on proximal policy optimization",
        "authors": "Jiaxin Li, Yonggang Li, Bei Sun, Fengrun Tang, Yanting Luo",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055264"
    },
    {
        "id": 16534,
        "title": "Multi-Objective Exploration for Proximal Policy Optimization",
        "authors": "Nguyen Do Hoang Khoi, Cuong Pham Van, Hoang Vu Tran, Cao Dung Truong",
        "published": "2021-3-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/atigb50996.2021.9423319"
    },
    {
        "id": 16535,
        "title": "Use of Proximal Policy Optimization for the Joint Replenishment Problem",
        "authors": "Nathalie Vanvuchelen, Joren Gijsbrechts, Robert Boute",
        "published": "2020-8",
        "citations": 64,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compind.2020.103239"
    },
    {
        "id": 16536,
        "title": "Proximal Policy Optimization with Continuous Bounded Action Space via the Beta Distribution",
        "authors": "Irving G. B. Petrazzini, Eric A. Antonelo",
        "published": "2021-12-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660123"
    },
    {
        "id": 16537,
        "title": "Meta Proximal Policy Optimization for Cooperative Multi-Agent Continuous Control",
        "authors": "Boli Fang, Zhenghao Peng, Hao Sun, Qin Zhang",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892004"
    },
    {
        "id": 16538,
        "title": "A new approach for drone tracking with drone using Proximal Policy Optimization based distributed deep reinforcement learning",
        "authors": "Ziya Tan, Mehmet Karaköse",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.softx.2023.101497"
    },
    {
        "id": 16539,
        "title": "Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task",
        "authors": "Reuf Kozlica, Stefan Wegenkittl, Simon Hiränder",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie51358.2023.10228056"
    },
    {
        "id": 16540,
        "title": "A proximal policy optimization with curiosity algorithm for virtual drone navigation",
        "authors": "Rupayan Das, Angshuman Khan, Gunjan Paul",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "Abstract\nThe drone sector is witnessing a surge in demand for advanced models tailored to address critical applications such as disaster management and intelligent warehouse deliveries. Employing simulation-based experiments with virtual drone navigation is considered a best practice before deploying physical models. Nonetheless, the current state-of-the-art virtual drone navigation system lacks accuracy and introduces notable increments in simulation time. In order to mitigate these issues, this paper introduces a deep reinforcement learning-based drone agent, designed to autonomously navigate within a constrained virtual environment. The proposed drone agent utilizes realistic drone physics in order to ensure flight within the virtual environment. The work uniquely combines & optimizes both control algorithms and physical dynamics, making the model more robust and versatile than others. The integration of curiosity-driven learning with physics-based modeling potentially increases the model's readiness for real-world application, compared to theoretical approaches. The extensive simulation results validate the remarkable speed and accuracy of the proposed scheme compared to baseline works. The trained agent exhibits strength and versatility, enabling it to deal with the numerous targets and obstacles encountered in human environments.",
        "link": "http://dx.doi.org/10.1088/2631-8695/ad1f14"
    },
    {
        "id": 16541,
        "title": "Cloud Task Scheduling Based on Proximal Policy Optimization Algorithm for Lowering Energy Consumption of Data Center",
        "authors": "",
        "published": "2022-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2022.06.006"
    },
    {
        "id": 16542,
        "title": "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization",
        "authors": "Siyu Lin, Peter A. Beling",
        "published": "2020-7",
        "citations": 11,
        "abstract": "In this article, we propose an end-to-end adaptive framework for optimal trade execution based on Proximal Policy Optimization (PPO). We use two methods to account for the time dependencies in the market data based on two different neural network architecture: 1) Long short-term memory (LSTM) networks, 2) Fully-connected networks (FCN) by stacking the most recent limit orderbook (LOB) information as model inputs. The proposed framework can make trade execution decisions based on level-2 limit order book (LOB) information such as bid/ask prices and volumes directly without manually designed attributes as in previous research. Furthermore, we use a sparse reward function, which gives the agent reward signals at the end of each episode as an indicator of its relative performances against the baseline model, rather than implementation shortfall (IS) or a shaped reward function. The experimental results have demonstrated advantages over IS and the shaped reward function in terms of performance and simplicity. The proposed framework has outperformed the industry commonly used baseline models such as TWAP, VWAP, and AC as well as several Deep Reinforcement Learning (DRL) models on most of the 14 US equities in our experiments.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/627"
    },
    {
        "id": 16543,
        "title": "Automated Lane Change Strategy using Proximal Policy Optimization-based Deep Reinforcement Learning",
        "authors": "Fei Ye, Xuxin Cheng, Pin Wang, Ching-Yao Chan, Jiucai Zhang",
        "published": "2020-10-19",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv47402.2020.9304668"
    },
    {
        "id": 16544,
        "title": "Joint Forest Fire Rescue Strategy Based on Multi-Agent Proximal Policy Optimization",
        "authors": "Jingda Zhang, Yuxian Zhang, Likui Qiao",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901908"
    },
    {
        "id": 16545,
        "title": "Improving proximal policy optimization with alpha divergence",
        "authors": "Haotian Xu, Zheng Yan, Junyu Xuan, Guangquan Zhang, Jie Lu",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.02.008"
    },
    {
        "id": 16546,
        "title": "Proximal policy optimization based dynamic path planning algorithm for mobile robots",
        "authors": "Xin Jin, Zhengxiao Wang",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/ell2.12342"
    },
    {
        "id": 16547,
        "title": "Research on Proximal Policy Optimization Algorithm Based on N-step Update",
        "authors": "Zhao Guoqing, Xu Junming, Liu Aidong, Yu Jing",
        "published": "2021-5-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisce52179.2021.9445929"
    },
    {
        "id": 16548,
        "title": "Layered Learning in a Quadrotor Drone: Simultaneous Controlling and Path Planning Using Optimal Fuzzy Fractional-Order Proportional- Integral-Derivative and Proximal Policy Optimization",
        "authors": "Hamed Shahbazi, Vahid Tikani, Roholamin Fattahi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4767238"
    },
    {
        "id": 16549,
        "title": "Rocket Powered Landing Guidance Using Proximal Policy Optimization",
        "authors": "Yifan Chen, Lin Ma",
        "published": "2019-7-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3351917.3351935"
    },
    {
        "id": 16550,
        "title": "Proximal Policy Optimization Based Continuous Intelligent Power Control in Cognitive Radio Network",
        "authors": "Fan Zishen, Xie Xianzhong, Shi Zhaoyuan, Chen Xiping",
        "published": "2020-12-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc51575.2020.9345062"
    },
    {
        "id": 16551,
        "title": "An Efficient Hyperparameter Control Method for a Network Intrusion Detection System Based on Proximal Policy Optimization",
        "authors": "Hyojoon Han, Hyukho Kim, Yangwoo Kim",
        "published": "2022-1-14",
        "citations": 21,
        "abstract": "The complexity of network intrusion detection systems (IDSs) is increasing due to the continuous increases in network traffic, various attacks and the ever-changing network environment. In addition, network traffic is asymmetric with few attack data, but the attack data are so complex that it is difficult to detect one. Many studies on improving intrusion detection performance using feature engineering have been conducted. These studies work well in the dataset environment; however, it is challenging to cope with a changing network environment. This paper proposes an intrusion detection hyperparameter control system (IDHCS) that controls and trains a deep neural network (DNN) feature extractor and k-means clustering module as a reinforcement learning model based on proximal policy optimization (PPO). An IDHCS controls the DNN feature extractor to extract the most valuable features in the network environment, and identifies intrusion through k-means clustering. Through iterative learning using the PPO-based reinforcement learning model, the system is optimized to improve performance automatically according to the network environment, where the IDHCS is used. Experiments were conducted to evaluate the system performance using the CICIDS2017 and UNSW-NB15 datasets. In CICIDS2017, an F1-score of 0.96552 was achieved and UNSW-NB15 achieved an F1-score of 0.94268. An experiment was conducted by merging the two datasets to build a more extensive and complex test environment. By merging datasets, the attack types in the experiment became more diverse and their patterns became more complex. An F1-score of 0.93567 was achieved in the merged dataset, indicating 97% to 99% performance compared with CICIDS2017 and UNSW-NB15. The results reveal that the proposed IDHCS improved the performance of the IDS by automating learning new types of attacks by managing intrusion detection features regardless of the network environment changes through continuous learning.",
        "link": "http://dx.doi.org/10.3390/sym14010161"
    },
    {
        "id": 16552,
        "title": "Riemannian Proximal Policy Optimization",
        "authors": "Shijun Wang, Baocheng Zhu, Chen Li, Mingzhe Wu, James Zhang, Wei Chu, Yuan Qi",
        "published": "2020-7-30",
        "citations": 0,
        "abstract": "In this paper, we propose a general Riemannian proximal optimization algorithm with guaranteed convergence to solve Markov decision process (MDP) problems. To model policy functions in MDP, we employ Gaussian mixture model (GMM) and formulate it as a non-convex optimization problem in the Riemannian space of positive semidefinite matrices. For two given policy functions, we also provide its lower bound on policy improvement by using bounds derived from the Wasserstein distance of GMMs. Preliminary experiments show the efficacy of our proposed Riemannian proximal policy optimization algorithm.",
        "link": "http://dx.doi.org/10.5539/cis.v13n3p93"
    },
    {
        "id": 16553,
        "title": "Motion control of unmanned surface vehicle based on improved reinforcement learning proximal policy optimization algorithm",
        "authors": "Shuai Wu",
        "published": "2022-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2653441"
    },
    {
        "id": 16554,
        "title": "Step climbing method for crawler type rescue robot using reinforcement learning with Proximal Policy Optimization",
        "authors": "Mifu Totani, Noritaka Sato, Yoshifumi Morita",
        "published": "2019-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/romoco.2019.8787360"
    },
    {
        "id": 16555,
        "title": "Autonomous collision avoidance system in a multi-ship environment based on proximal policy optimization method",
        "authors": "Zheng Rongcai, Xie Hongwei, Yuan Kexin",
        "published": "2023-3",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.113779"
    },
    {
        "id": 16556,
        "title": "Proximal Policy Optimization-Based Anti-Jamming UAV-Assisted Data Collection",
        "authors": "Ze Chen, Ping Yang, Yue Xiao, Liangxin Qian",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437913"
    },
    {
        "id": 16557,
        "title": "The Temperature Prediction of Permanent Magnet Synchronous Machines Based on Proximal Policy Optimization",
        "authors": "Yuefeng Cen, Chenguang Zhang, Gang Cen, Yulai Zhang, Cheng Zhao",
        "published": "2020-10-23",
        "citations": 3,
        "abstract": "Accurate temperature prediction plays an important role in the thermal protection of permanent magnet synchronous motors. A temperature prediction method of permanent magnet synchronous machines (PMSMs) based on proximal policy optimization is proposed. In the proposed method, the actor-critic framework of reinforcement learning is introduced to model the effective temperature prediction mechanism, and the correlations between the input features are then analyzed to select the appropriate input features. Finally, the simplified proximal policy optimization algorithm is introduced to optimize the value of the prediction temperature of PMSMs. Experimental results reveal the high accuracy and reliability of the proposed method compared with an exponential weighted moving average method (EWMA), a recurrent neural network (RNN), and long short-term memory (LSTM).",
        "link": "http://dx.doi.org/10.3390/info11110495"
    },
    {
        "id": 16558,
        "title": "Deep Reinforcement Learning Attitude Control of Fixed-Wing UAVs Using Proximal Policy optimization",
        "authors": "Eivind Bohn, Erlend M. Coates, Signe Moe, Tor Ame Johansen",
        "published": "2019-6",
        "citations": 93,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas.2019.8798254"
    },
    {
        "id": 16559,
        "title": "Temporal Graph Traversals Using Reinforcement Learning With Proximal Policy Optimization",
        "authors": "Samuel Henrique Silva, Adel Alaeddini, Peyman Najafirad",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.2985295"
    },
    {
        "id": 16560,
        "title": "Proximal Policy Optimization for Energy Management of Electric Vehicles and PV Storage Units",
        "authors": "Monica Alonso, Hortensia Amaris, David Martin, Arturo de la Escalera",
        "published": "2023-7-29",
        "citations": 1,
        "abstract": "Connected autonomous electric vehicles (CAEVs) are essential actors in the decarbonization process of the transport sector and a key aspect of home energy management systems (HEMSs) along with PV units, CAEVs and battery energy storage systems. However, there are associated uncertainties which present new challenges to HEMSs, such as aleatory EV arrival and departure times, unknown EV battery states of charge at the connection time, and stochastic PV production due to weather and passing cloud conditions. The proposed HEMS is based on proximal policy optimization (PPO), which is a deep reinforcement learning algorithm suitable for continuous complex environments. The optimal solution for HEMS is a tradeoff between CAEV driver’s range anxiety, batteries degradation, and energy consumption, which is solved by means of incentives/penalties in the reinforcement learning formulation. The proposed PPO algorithm was compared to conventional methods such as business-as-usual (BAU) and value iteration (VI) solutions based on dynamic programming. Simulation results indicate that the proposed PPO’s performance showed a daily energy cost reduction of 54% and 27% compared to BAU and VI, respectively. Finally, the developed PPO algorithm is suitable for real-time operations due to its fast execution and good convergence to the optimal solution.",
        "link": "http://dx.doi.org/10.3390/en16155689"
    },
    {
        "id": 16561,
        "title": "Enhanced Multi-Agent Proximal Policy Optimization for Multi-UAV Target Offensive-Defensive Decision",
        "authors": "Yifan Zheng, Bin Xin, Keming Jiao, Zhixin Zhao, Yuyang Wang, Yunming Zhao",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240070"
    },
    {
        "id": 16562,
        "title": "vegIMPACT: Knowledge Transfer : Improving vegetable production by smallholder farmers in Indonesia",
        "authors": "C. Plaisier,  , Y. Dijkxhoorn, J. Medah, J. Dengerink, T. Koster, H. Hengsdijk, F. van Koesveld,  ,  ,  ,  ",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18174/426707"
    },
    {
        "id": 16563,
        "title": "On Douglas-Rachford splitting that generally fails to be a proximal mapping: a degenerate proximal point analysis",
        "authors": "Feng Xue",
        "published": "2023-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/02331934.2023.2187256"
    },
    {
        "id": 16564,
        "title": "TCP-CFCC: Proximal Policy Optimization for Congestion Control",
        "authors": "Xiang Huang, Mengting Li, Hongyu Liu, Jiancheng Liu, Kei Bai, Shenghong He",
        "published": "2022-1-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpeca53709.2022.9718574"
    },
    {
        "id": 16565,
        "title": "A Portable Accelerator of Proximal Policy Optimization for Robots",
        "authors": "Weiyi Zhang, Yancao Jiang, Fasih Ud Din Farrukh, Chun Zhang, Xiang Xie",
        "published": "2021-11-24",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icta53157.2021.9661840"
    },
    {
        "id": 16566,
        "title": "Improving traffic signal control operations using proximal policy optimization",
        "authors": "Liben Huang, Xiaohui Qu",
        "published": "2023-3",
        "citations": 2,
        "abstract": "AbstractExisting traffic signal control systems present many limitations including fixed signal timing schemes, insufficient efficiency and flexibility, and difficulty in adapting to the changing traffic flows. In recent years, the development of deep reinforcement learning (RL) has shown great research potential and application prospects. This paper proposes an intersection signal control method based on the proximal policy optimization (PPO) method. Specifically, this paper uses the value vector representation method of traffic characteristics to encode the traffic state and then feeds the state encoding into the long short‐term memory (LSTM) network to obtain the signal phase output. Finally, to obtain the optimal signal control strategy, the PPO algorithm is utilized to train the neural network and adjust the signal phase. The proposed algorithm is benchmarked against other classic RL and adaptive signal control schemes in an experimental environment based on the traffic simulation software simulation of urban mobility (SUMO). Experimental results showed that the proposed algorithm greatly improves traffic efficiency. Specifically, the mean velocity of the vehicles increases by 45.09%, and the mean occupancy rate of each lane, the length of the longest jams during each step, and the mean halting duration dropped by 21.38%, 25.86%, and 12.94%, respectively.",
        "link": "http://dx.doi.org/10.1049/itr2.12286"
    },
    {
        "id": 16567,
        "title": "Proximal policy optimization with model-based methods",
        "authors": "Shuailong Li, Wei Zhang, Huiwen Zhang, Xin Zhang, Yuquan Leng",
        "published": "2022-4-28",
        "citations": 0,
        "abstract": "Model-free reinforcement learning methods have successfully been applied to practical applications such as decision-making problems in Atari games. However, these methods have inherent shortcomings, such as a high variance and low sample efficiency. To improve the policy performance and sample efficiency of model-free reinforcement learning, we propose proximal policy optimization with model-based methods (PPOMM), a fusion method of both model-based and model-free reinforcement learning. PPOMM not only considers the information of past experience but also the prediction information of the future state. PPOMM adds the information of the next state to the objective function of the proximal policy optimization (PPO) algorithm through a model-based method. This method uses two components to optimize the policy: the error of PPO and the error of model-based reinforcement learning. We use the latter to optimize a latent transition model and predict the information of the next state. For most games, this method outperforms the state-of-the-art PPO algorithm when we evaluate across 49 Atari games in the Arcade Learning Environment (ALE). The experimental results show that PPOMM performs better or the same as the original algorithm in 33 games.",
        "link": "http://dx.doi.org/10.3233/jifs-211935"
    },
    {
        "id": 16568,
        "title": "Multi-Agent Proximal Policy Optimization for a Deadlock Capable Transport System in a Simulation-Based Learning Environment",
        "authors": "Marcel Müller, Lorena S. Reyes-Rubiano, Tobias Reggelin, Hartmut Zadek",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408110"
    },
    {
        "id": 16569,
        "title": "Proximal-Policy-Optimization-based Intra-day Scheduling of Hydrogen Fueling Station",
        "authors": "Binrui Cao, Xiong Wu, Mingkang He, Xiaofei Li, Wenwen He, Shengyao Chen",
        "published": "2022-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cieec54735.2022.9846726"
    },
    {
        "id": 16570,
        "title": "Proximal Policy Optimization Based Reinforcement Learning for Joint Bidding in Energy and Frequency Regulation Markets",
        "authors": "Muhammad Anwar, Changlong Wang, Frits de Nijs, Hao Wang",
        "published": "2022-7-17",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm48719.2022.9917082"
    }
]