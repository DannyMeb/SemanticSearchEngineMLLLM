[
    {
        "id": 8801,
        "title": "Informer, an Information Organization Transformer Architecture",
        "authors": "Cristian Ojeda, Cayetano Artal, Francisco Tejera",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010372703810389"
    },
    {
        "id": 8802,
        "title": "Chapter 3: Transformer Architecture Introduction",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-004"
    },
    {
        "id": 8803,
        "title": "Chapter 4: Transformer Architecture in Greater Depth",
        "authors": "",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683928973-005"
    },
    {
        "id": 8804,
        "title": "Time Series Forecasting of Air Pollutant PM2.5 Using Transformer Architecture",
        "authors": "K. Azhahudurai, V. Veeramanikandan",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21275/sr231125192357"
    },
    {
        "id": 8805,
        "title": "Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks",
        "authors": "Yuliang Cai, Mohammad Rostami",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4713352"
    },
    {
        "id": 8806,
        "title": "Defect transformer: An efficient hybrid transformer architecture for surface defect detection",
        "authors": "Junpu Wang, Guili Xu, Fuju Yan, Jinjin Wang, Zhengsheng Wang",
        "published": "2023-4",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.measurement.2023.112614"
    },
    {
        "id": 8807,
        "title": "Rethinking position embedding methods in the Transformer architecture",
        "authors": "Xin Zhou, Zhaohui Ren, Tianzhuang Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the transformer architecture, as self-attention reads entire image patches at once, the context of the sequence between patches is omitted. Therefore, the position embedding method is employed to assist the self-attention layers in computing the ordering information of tokens. Although most works directly add the position vector into the corresponding token vector instead of concatenating, fewer papers give a detailed explanation except for dimensional reduction. Actually, the addition method makes no sense because token vectors and position vectors are different physical quantities and can not be simply added together. Thus, we investigate the discrepancy of learned position information in both embedding methods(concatenation and addition) and compare their performance for models. Experiments demonstrate that the concatenation method can learn more spatial information(horizontal, vertical, and angle) than the addition method and learn smoother position information in the multi-scale model. Also, an appropriate concatenating strategy can easily bring 0.1% to 0.4% performance gain for soft-of-the-art models without extra computation overhead.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2525471/v1"
    },
    {
        "id": 8808,
        "title": "EEG-Transformer: Self-attention from Transformer Architecture for Decoding EEG of Imagined Speech",
        "authors": "Young-Eun Lee, Seo-Hyun Lee",
        "published": "2022-2-21",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bci53720.2022.9735124"
    },
    {
        "id": 8809,
        "title": "DiVIT: Algorithm and architecture co-design of differential attention in vision transformer",
        "authors": "Yangfan Li, Yikun Hu, Fan Wu, Kenli Li",
        "published": "2022-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sysarc.2022.102520"
    },
    {
        "id": 8810,
        "title": "Tolstoy’s Genius Explored by Deep Learning using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "Artificial Narrow Intelligence is in the phase of moving towards the AGN, which will attempt to decide as a human being. We are getting closer to it by each day, but AI actually is indefinite to many, although it is no different than any other set of mathematically defined computer operations in its core. Generating new data from a pre-trained model introduces new challenges to science & technology. In this work, the design of such an architecture from scratch, solving problems, and introducing alternative approaches are what has been conducted. Using a deep thinker, Tolstoy, as an object of study is a source of motivation for the entire research.",
        "link": "http://dx.doi.org/10.5121/csit.2023.132306"
    },
    {
        "id": 8811,
        "title": "Tolstoy’s Genius Explored by Deep Learning Using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4670984"
    },
    {
        "id": 8812,
        "title": "Ghostformer: Efficiently Amalgamated Cnn-Transformer Architecture for Object Detection",
        "authors": "Xin Xie, Dengquan Wu, Mingye Xie, Zixi Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532338"
    },
    {
        "id": 8813,
        "title": "XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples",
        "authors": "Shorya Consul, John Robertson, Haris Vikalo",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTIt is estimated that approximately 15% of cancers world-wide can be linked to viral infections. The viruses that can cause or increase the risk of cancer include human papillomavirus, hepatitis B and C viruses, Epstein-Barr virus, and human immunodeficiency virus, to name a few. The computational analysis of the massive amounts of tumor DNA data, whose collection is enabled by the recent advancements in sequencing technologies, have allowed studies of the potential association between cancers and viral pathogens. However, the high diversity of oncoviral families makes reliable detection of viral DNA difficult and thus, renders such analysis challenging. In this paper, we introduce XVir, a data pipeline that relies on a transformer-based deep learning architecture to reliably identify viral DNA present in human tumors. In particular, XVir is trained on genomic sequencing reads from viral and human genomes and may be used with tumor sequence information to find evidence of viral DNA in human cancers. Results on semi-experimental data demonstrate that XVir is capable of achieving high detection accuracy, generally outperforming state-of-the-art competing methods while being more compact and less computationally demanding.CCS CONCEPTS•Computer systems organization→Embedded systems;Redundancy; Robotics; •Networks→ Network reliability.ACM Reference FormatShorya Consul, John Robertson, and Haris Vikalo. 2023. XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples. InProceedings of The Eighth International Workshop on Computational Network Biology: Modeling, Analysis, and Control (CNB-MAC ’23). ACM, New York, NY, USA, 8 pages.",
        "link": "http://dx.doi.org/10.1101/2023.08.28.555020"
    },
    {
        "id": 8814,
        "title": "Optimizing Transformer Architecture for Time Series Analysis in Cardiovascular Signal Processing",
        "authors": "Kahriman Oruç, Kranz Dominik, Wessel Niels, Krämer Jan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47952/gro-publ-197"
    },
    {
        "id": 8815,
        "title": "A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining",
        "authors": "Cheng Wang, Wei Li",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nImage deraining is a challenging task that involves restoring degraded images affected by rain streaks. While Convolutional Neural Networks (CNNs) have been commonly used for this task, existing approaches often rely on stacked convolutional basic blocks with limited performance and compromised spatial detail. Furthermore, the limited receptive field of convolutional layers leads to incomplete processing of non-uniform rain streaks. To address these concerns, we propose a novel image deraining network that combines CNNs and trans- formers. Our network comprises two stages: an encoder-decoder architecture with a triple attention mechanism to capture valuable features and residual dual branch transformer blocks that enhance local information modeling. To address the transformer’s lack of local information modeling capability, we intro- duce convolution in the self-attentive mechanism of the transformer block and feed-forward network. Additionally, we employ a frequency domain contrastive learning method to enhance contrastive sample information, ensuring that the restored image closely resembles the clear image  in the frequency domain space, while still retaining a distinction from the rainy image. Extensive quantitative and qualitative experiments demonstrate that our proposed deraining network outperforms existing methods on public datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3240803/v1"
    },
    {
        "id": 8816,
        "title": "ECONOMIC DAMAGE ASSESSMENT METHODOLOGY IN CASE OF TRANSFORMER LOAD RATIO DEVIATION",
        "authors": "Anton S. LUKOVENKO",
        "published": "2021-12-15",
        "citations": 0,
        "abstract": "The aim of the study is to develop a methodology for assessing economic damage when the transformer load factor deviates. In the process of research, a method was used to determine the optimal load of a power transformer. A method for assessing economic damage in case of a deviation of its load factor has been developed. The optimal load factor of a power transformer has been determined according to the criterion of the minimum cost of money for the transformation of electrical energy. The load level of power transformers can be assessed by two criteria: by the maximum integral value of the effi ciency and by the minimum loss of money during the transformation of electricity. According to the proposed methodology, the assessment of damage is carried out when the load factor of the transformer deviates from the optimal values, for which the relative losses of electrical energy are calculated. The results obtained when evaluating energy and fi nancial and economic effi ciency show that when the load factor is overestimated relative to the optimal values, it is much more preferable to overestimate than underestimate. The reliability of the results is confi rmed by the satisfactory agreement of the calculated results with the experimental data obtained at the operating power plant.",
        "link": "http://dx.doi.org/10.17673/vestnik.2021.03.22"
    },
    {
        "id": 8817,
        "title": "Racing with Vision Transformer Architecture",
        "authors": "Chengwen Tian, Liang Song",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ishc56805.2022.00055"
    },
    {
        "id": 8818,
        "title": "Transformer la cité de transit de Beutre : pour une conception ouverte en architecture",
        "authors": "Marion Howa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56698/metropolitiques.1943"
    },
    {
        "id": 8819,
        "title": "Molecule Generation for Drug Discovery with New Transformer Architecture",
        "authors": "Yu-Bin Hong, Kyung-Jun Lee, DongNyeong Heo, Heeyoul Choi",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4195528"
    },
    {
        "id": 8820,
        "title": "HTC-Grasp: A Hybrid Transformer-CNN Architecture for Robotic Grasp Detection",
        "authors": "Qiang Zhang, Jianwei Zhu, Xueying Sun, Mingmin Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "We introduce a novel hybrid Transformer-CNN architecture for robotic grasp detection, designed to enhance the accuracy of grasping unknown objects. Our proposed architecture has two key designs. Firstly, we develop a hierarchical transformer as the encoder, incorporating the external attention to effectively capture the correlation features across the data. Secondly, the decoder is constructed with cross-layer connections to efficiently fuse multi-scale features. Channel attention is introduced in the decoder to model the correlation between channels and to adaptively recalibrate the channel correlation feature response, thereby increasing the weight of the effective channels. Our method is evaluated on the Cornell and Jacquard public datasets, achieving an image-wise detection accuracy of 98.3% and 95.8% on each dataset, respectively. Additionally, we achieve object-wise detection accuracy of 96.9% and 92.4% on the same datasets. A physical experiment is also performed using the Elite 6Dof robot, with a grasping accuracy rate of 93.3%, demonstrating the proposed method's ability to grasp unknown objects in real-world scenarios. The results of this study show that our proposed method outperforms other state-of-the-art methods.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0382.v1"
    },
    {
        "id": 8821,
        "title": "Multilingual Cyber Abuse Detection using Advanced Transformer Architecture",
        "authors": "Aditya Malte, Pratik Ratadiya",
        "published": "2019-10",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929493"
    },
    {
        "id": 8822,
        "title": "HTC-Grasp: A Hybrid Transformer-CNN Architecture for Robotic Grasp Detection",
        "authors": "Qiang Zhang, Jianwei Zhu, Xueying Sun, Mingmin Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "We introduce a novel hybrid Transformer-CNN architecture for robotic grasp detection, designed to enhance the accuracy of grasping unknown objects. Our proposed architecture has two key designs. Firstly, we develop a hierarchical transformer as the encoder, incorporating the external attention to effectively capture the correlation features across the data. Secondly, the decoder is constructed with cross-layer connections to efficiently fuse multi-scale features. Channel attention is introduced in the decoder to model the correlation between channels and to adaptively recalibrate the channel correlation feature response, thereby increasing the weight of the effective channels. Our method is evaluated on the Cornell and Jacquard public datasets, achieving an image-wise detection accuracy of 98.3% and 95.8% on each dataset, respectively. Additionally, we achieve object-wise detection accuracy of 96.9% and 92.4% on the same datasets. A physical experiment is also performed using the Elite 6Dof robot, with a grasping accuracy rate of 93.3%, demonstrating the proposed method's ability to grasp unknown objects in real-world scenarios. The results of this study show that our proposed method outperforms other state-of-the-art methods.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0382.v2"
    },
    {
        "id": 8823,
        "title": "Construction material classification on imbalanced datasets using Vision Transformer architecture (ViT)",
        "authors": "Maryam Soleymani, Mahdi Bonyani, Hadi Mahami, Farnad Nasirzadeh",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThis research proposes a reliable model for identifying different construction materials with the highest accuracy, which is exploited as an advantageous tool for a wide range of construction applications such as automated progress monitoring. In this study, a novel deep learning architecture called Vision Transformer (ViT) is used for detecting and classifying construction materials. The robustness of the proposed method is assessed by utilizing different image datasets. For this purpose, the model is trained and tested on two large imbalanced datasets, namely Construction Material Library (CML) and Building Material Dataset (BMD). A third dataset is also generated by combining CML and BMD to create a more imbalanced dataset and assess the capabilities of the proposed method. The achieved results reveal an accuracy of 100 percent in evaluation metrics such as accuracy, precision, recall, and f1-score for each material category of three different datasets. It is believed that the suggested model accomplishes a novel and robust tool for detecting and classifying different material types. To date, a number of studies have attempted to automatically classify a variety of building materials, which still have some errors. This research will address the mentioned shortcoming and proposes a model to detect the material type with higher accuracy. The proposed model is also capable of being generalized to different datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1948162/v1"
    },
    {
        "id": 8824,
        "title": "Empirical Assessment of Transformer Based Neural Network Architecture in Forecasting Pollution Trends",
        "authors": "Sarbani Roy, Pritthijit Nath, Asif Iqbal Middya",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWith rising pollution concerns in recent times, producing refined and accurate predictions as a part of United Nations Sustainable Development Goals 11 (Sustainable Cities and Communities) and 13 (Climate Action) have gained utmost importance. As new computational methods become available, it becomes difficult for an average policymaker to evaluate various techniques comprehensively. One such state-of-the-art architecture is the Transformer neural network which has shown an exceptional rise in various areas such as natural language processing and computer vision. This paper reviews the current literature and performance of such a Transformer based neural network (referred in the paper as PolTrans) in the domain of pollution forecasting. Experiments based on four univariate city pollution datasets (Delhi, Seoul, Skopje and Ulaanbaatar) and two multivariatedatasets (Beijing PM2:5 and Beijing PM10) are performed against other computational methods consisting of widely used statistical, machine learning and deep learning techniques. Experimental findings show that although PolTrans performs comparatively better compared to existing deeplearning methods such as BiDirectional long short-term memory networks (LSTM), LSTM AutoEncoder, etc. for modelling pollution in cities such as Beijing, Delhi and Ulaanbaatar, in the majority of cases, the PolTrans architecture lags behind statistical and machine learning methods such as AutoRegressive Integrated Moving Average (ARIMA), Random Forest Regression, Standard Vector Regression (SVR), etc. by a range of 1.5 - 15 units in terms of Root Mean Square Error.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1793953/v1"
    },
    {
        "id": 8825,
        "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
        "authors": "Masakiyo Fujimoto, Hisashi Kawai",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-225"
    },
    {
        "id": 8826,
        "title": "Captioning Remote Sensing Images Using Transformer Architecture",
        "authors": "Wrucha Nanal, Mohammadreza Hajiarbabi",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067039"
    },
    {
        "id": 8827,
        "title": "Dyformer: A Dynamic Transformer-Based Architecture for Multivariate Time Series Classification",
        "authors": "Chao Yang, Xianzhi Wang, Lina Yao, Guodong Long, Guandong Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4362666"
    },
    {
        "id": 8828,
        "title": "Architecture and Topology Overview of Modular Smart Solid-State Transformer",
        "authors": "Rongwu Zhu, Marco Liserre",
        "published": "2021-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/egrid52793.2021.9662131"
    },
    {
        "id": 8829,
        "title": "Application of Open Agent Architecture and Data Mining Techniques to Transformer Condition Assessment System",
        "authors": "Lizeng Wu, Zhu Yongli, Jinsha Yuan, Xueyu Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00189"
    },
    {
        "id": 8830,
        "title": "Unveiling the Potential of Vision Transformer Architecture for Person Re-identification",
        "authors": "N. Perwaiz, M. Shahzad, M.M. Fraz",
        "published": "2022-10-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inmic56986.2022.9972908"
    },
    {
        "id": 8831,
        "title": "A COVID-19 Search Engine (CO-SE) with Transformer-based architecture",
        "authors": "Shaina Raza",
        "published": "2022-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.health.2022.100068"
    },
    {
        "id": 8832,
        "title": "Style Augmented Transformer Architecture for Automatic Essay Assessment",
        "authors": "Tirthankar Dasgupta, Gaurav K. Singh, Lipika Dey",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt58122.2023.00105"
    },
    {
        "id": 8833,
        "title": "Low-light image enhancement based on Transformer and CNN architecture",
        "authors": "Keyuan Chen, Bin Chen, Shiqian Wu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326484"
    },
    {
        "id": 8834,
        "title": "Evolutionary neural architecture search combining multi-branch ConvNet and improved transformer",
        "authors": "Yang Xu, Yongjie Ma",
        "published": "2023-9-22",
        "citations": 3,
        "abstract": "AbstractDeep convolutional neural networks (CNNs) have achieved promising performance in the field of deep learning, but the manual design turns out to be very difficult due to the increasingly complex topologies of CNNs. Recently, neural architecture search (NAS) methods have been proposed to automatically design network architectures, which are superior to handcrafted counterparts. Unfortunately, most current NAS methods suffer from either highly computational complexity of generated architectures or limitations in the flexibility of architecture design. To address above issues, this article proposes an evolutionary neural architecture search (ENAS) method based on improved Transformer and multi-branch ConvNet. The multi-branch block enriches the feature space and enhances the representational capacity of a network by combining paths with different complexities. Since convolution is inherently a local operation, a simple yet powerful “batch-free normalization Transformer Block” (BFNTBlock) is proposed to leverage both local information and long-range feature dependencies. In particular, the design of batch-free normalization (BFN) and batch normalization (BN) mixed in the BFNTBlock blocks the accumulation of estimation shift ascribe to the stack of BN, which has favorable effects for performance improvement. The proposed method achieves remarkable accuracies, 97.24 $$\\%$$\n%\n and 80.06 $$\\%$$\n%\n on CIFAR10 and CIFAR100, respectively, with high computational efficiency, i.e. only 1.46 and 1.53 GPU days. To validate the universality of our method in application scenarios, the proposed algorithm is verified on two real-world applications, including the GTSRB and NEU-CLS dataset, and achieves a better performance than common methods.",
        "link": "http://dx.doi.org/10.1038/s41598-023-42931-3"
    },
    {
        "id": 8835,
        "title": "Evolved Speech-Transformer: Applying Neural Architecture Search to End-to-End Automatic Speech Recognition",
        "authors": "Jihwan Kim, Jisung Wang, Sangki Kim, Yeha Lee",
        "published": "2020-10-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1233"
    },
    {
        "id": 8836,
        "title": "A Transformer Architecture for the Prediction of Cognate Reflexes",
        "authors": "Giuseppe Celano",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.sigtyp-1.10"
    },
    {
        "id": 8837,
        "title": "Deepsem-Net: Enhancing Sem Defect Analysis in Semiconductor Manufacturing with a Dual-Branch Cnn-Transformer Architecture",
        "authors": "Yibo Qiao, Zhouzhouzhou Mei, Yuening Luo, Yining Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4765199"
    },
    {
        "id": 8838,
        "title": "Research on Chinese Text Error Correction Based on Transformer Enhanced Architecture",
        "authors": "靖翔 杨",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2022.123057"
    },
    {
        "id": 8839,
        "title": "Building Blocks for a Complex-Valued Transformer Architecture",
        "authors": "Florian Eilers, Xiaoyi Jiang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095349"
    },
    {
        "id": 8840,
        "title": "Super-resolution for satellite imagery: uncovering details using a new Cross Band Transformer architecture",
        "authors": "Jasper S. Wijnands, Nikolaos Ntantis, Jan Fokke Meirink, Domenica Dibenedetto",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recent advances in artificial intelligence (AI) techniques have enabled the processing and analysis of vast datasets, such as archives of satellite observations. In the geosciences, remote sensing has transformed the way in which the atmosphere and surface are observed. Traditionally, substantial funding is directed towards the development of new satellites to improve observation accuracy. Nowadays, novel methods based on AI could become a complementary approach to further enhance the resolution of observations. Therefore, we developed a new, state-of-the-art super-resolution methodology.\nSatellites commonly measure electromagnetic radiation, reflected or emitted by the earth's surface and atmosphere, in different parts of the spectrum. Many instruments capture both panchromatic (PAN) and low-resolution multi-spectral (LRMS) images. While PAN typically covers a broad spectral range, LRMS focuses on details in narrow bands within that range. Pansharpening is the task of fusing the spatial details of PAN with the spectral richness of LRMS, to obtain high-resolution multi-spectral (HRMS) images. This has proven to be valuable in many areas of the geosciences, leading to new capabilities such as detecting small-sized marine plastic litter and identifying buried archaeological remains. Although HRMS images are not directly captured by the satellite, they can provide enhanced visual clarity, uncover intricate patterns and allow for more accurate and detailed analyses.\nTechnically, pansharpening is closely related to the single image super-resolution task, where attention-based models have achieved excellent results. In our study a new Cross Band Transformer (CBT) for pansharpening was developed, incorporating and adapting successful features of vision transformer architectures. Information sharing between the panchromatic and multi-spectral input streams was enabled through two novel components: the Shifted Cross-Band Attention Block and the Overlapping Cross-Band Attention Block, implementing mechanisms for shifted and overlapping cross-attention. Each block led to a more accurate fusion of panchromatic and multi-spectral data. For evaluation, CBT was also compared to seven competitive benchmark methods, including MDCUN, PanFormer and ArbRPN. Our model produced state-of-the-art results on the widely used GaoFen-2 and WorldView-3 pansharpening datasets. Based on peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) scores of the generated images, CBT outperformed all benchmark methods. Our AI method can be integrated in existing remote sensing pipelines, as CBT converts actual observations into a high-resolution equivalent for use in downstream tasks. A PyTorch implementation of CBT is available at https://github.com/VisionVoyagerX/CBT.\nFurthermore, we developed the Sev2Mod dataset, available at https://zenodo.org/record/8360458. Unlike conventional benchmark datasets, Sev2Mod acquired input and target pairs from two different satellite instruments: (i) SEVIRI onboard the Meteosat Second Generation (MSG) satellite in geostationary orbit and (ii) MODIS onboard the Terra satellite in polar, sun-synchronous orbit. SEVIRI measures a fixed field of view quasi-continuously, while MODIS passes only twice a day but observes at a much higher spatial resolution. Our study investigated image generation at the spatial resolution of MODIS, while preserving SEVIRI's high temporal resolution. Since Sev2Mod is better aligned with actual situations one may encounter in applications of pansharpening methods (e.g., noise, bias, approximate temporal matching), it provides a solid foundation to design robust pansharpening models for real-world applications.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-340"
    },
    {
        "id": 8841,
        "title": "Multimodal Very Short-Term Solar Irradiance Forecasting Using Sky Image-Numerical Fusion: Advanced Fusion Method Using the Gate Architecture and the Transformer Architecture",
        "authors": "Liwenbo Zhang, Robin Wilson, Mark Sumner, Yupeng Wu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4371894"
    },
    {
        "id": 8842,
        "title": "TFE: A Transformer Architecture for Occlusion Aware Facial Expression Recognition",
        "authors": "Jixun Gao, Yuanyuan Zhao",
        "published": "2021-10-25",
        "citations": 6,
        "abstract": "Facial expression recognition (FER) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based FER approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based FER method (TFE) that is capable of adaptatively focusing on the most important and unoccluded facial regions. TFE is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for FER. Compared with traditional transformer, the novelty of TFE is two-fold: (i) To effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (ii) Given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed TFE on the two prevalent in-the-wild facial expression datasets (AffectNet and RAF-DB) and the their modifications with artificial occlusions. Experimental results show that TFE improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art FE methods, TFE obtains consistent improvements. Visualization results show TFE is capable of automatically focusing on the discriminative and non-occluded facial regions for robust FER.",
        "link": "http://dx.doi.org/10.3389/fnbot.2021.763100"
    },
    {
        "id": 8843,
        "title": "An Ensemble Novel Architecture for Bangla Mathematical Entity Recognition Using Transformer Based Learning",
        "authors": "Tanjim Taharat Aurpa, Md Shoaib Ahmed, Mohammad  Aman Ullah, Maria Mehzabin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4531227"
    },
    {
        "id": 8844,
        "title": "Conv-Transformer Architecture for Unconstrained Off-LineUrdu Handwriting Recognition",
        "authors": "Nauman Riaz, Haziq Arbab, Arooba Maqsood, Khuzaeymah Bin Nasir, Adnan Ul-Hasan, Faisal Shafait",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nUnconstrained off-line handwriting text recognition in general and for Arabic-like scripts in particular is a challenging task and is still an active researcharea. Transformer based models for English handwriting recognition have recently shown promising results.In this paper, we have explored the use of transformerarchitecture for Urdu handwriting recognition. The useof a Convolution Neural Network before a vanilla fullTransformer and using Urdu printed text-lines alongwith handwritten text lines during the training are thehighlights of the proposed work. The Convolution Layers act to reduce the spatial resolutions and compensate for the n2 complexity of transformer multi-head attention layers. Moreover, the printed text images inthe training phase help the model in learning a greaternumber of ligatures (a prominent feature of Arabiclike scripts) and a better language model. Our modelachieved state-of-the-art accuracy (CER of 5.31%) onpublicly available NUST-UHWR dataset [1].",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1514700/v1"
    },
    {
        "id": 8845,
        "title": "MFVT:An Anomaly Traffic Detection Method Merging Feature Fusion Network and Vision Transformer Architecture",
        "authors": "Ming Li, Dezhi Han, Dun Li, Han Liu, Chin- Chen Chang",
        "published": "No Date",
        "citations": 4,
        "abstract": "Abstract\nNetwork intrusion detection, which takes the extraction and analysis of network traffic features as the main method, plays a vital role in network security protection. The current network traffic feature extraction and analysis for network intrusion detection mostly uses deep learning algorithms. Currently, deep learning requires a lot of training resources, and have weak processing capabilities for imbalanced data sets. In this paper, a deep learning model (MFVT) based on feature fusion network and Vision Transformer architecture is proposed, to which improves the processing ability of imbalanced data sets and reduces the sample data resources needed for training. Besides, to improve the traditional raw traffic features extraction methods, a new raw traffic features extraction method (CRP) is proposed, the CPR uses PCA algorithm to reduce all the processed digital traffic features to the specified dimension. On the IDS 2017 dataset and the IDS 2012 dataset, the ablation experiments show that the performance of the proposed MFVT model is significantly better than other network intrusion detection models, and the detection accuracy can reach the state-of-the-art level. And, When MFVT model is combined with CRP algorithm, the detection accuracy is further improved to 99.99%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-877144/v1"
    },
    {
        "id": 8846,
        "title": "Segmentation of Nuclei using Transformer based Architecture",
        "authors": "Pritam Rao, Ashutosh Naik, Chirag Rana, Sunil Ghane",
        "published": "2022-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt54827.2022.9984414"
    },
    {
        "id": 8847,
        "title": "A Vision Transformer Architecture for the Automated Segmentation of Retinal Fluids in Spectral Domain Optical Coherence Tomography Images",
        "authors": "Daniel Philippi, Kai Rothaus, Mauro Castelli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNeovascular age-related macular degeneration (nAMD) is one of the major causes of irreversible blindness and is characterized by accumulations of different fluids inside the retina. An early detection and activity monitoring of predominately three types of fluids, namely intra-retinal fluid (IRF), sub-retinal fluid (SRF), and pigment epithelium detachment (PED), is critical for a successful treatment. Spectral-domain optical coherence tomography (SD-OCT) revolutionized nAMD treatment by providing cross-sectional, high-resolution images of the retina. Automatic segmentation and quantification of IRF, SRF, and PED in SD-OCT images can be extremely useful for clinical decision-making. Despite the use of state-of-the-art convolutional neural network (CNN)-based methods, the task remains challenging due to relevant variations in the location, size, shape, and texture of the fluids. This work is the first to adopt a transformer-based method to automatically segment retinal fluid from SD-OCT images and qualitatively and quantitatively evaluate its performance against CNN-based methods. The method combines the efficient long-range feature extraction and aggregation capabilities of Vision Transformers (ViTs) with data-efficient training of CNNs. The proposed method was tested on a private dataset containing 3842 2-dimensional SD-OCT retina images, manually labeled by experts of the Franziskus-Eye-Hospital. While one of the competitors presents a better performance in terms of Dice score, the proposed method is significantly less computationally expensive. Thus, future research will focus on the proposed network's architecture to increase its segmentation performance while maintaining its computational efficiency.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2262988/v1"
    },
    {
        "id": 8848,
        "title": "Indian Peak Power demand Forecasting: Transformer Based Implementation of Temporal Architecture",
        "authors": "Shashwat Jha, Vishvaditya Luhach",
        "published": "2022-9-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globconpt57482.2022.9938155"
    },
    {
        "id": 8849,
        "title": "A Quantization Method Based on Lightweight Transformer Model Architecture for Automatic Classification of Lung Sounds",
        "authors": "Qiuhao Wang, Yun Chu, EnZe Zhou, Gang Zheng, Qian Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4482035"
    },
    {
        "id": 8850,
        "title": "A Hybrid CNN-Transformer Architecture for Semantic Segmentation of Radar Sounder data",
        "authors": "Raktim Ghosh, Francesca Bovolo",
        "published": "2022-7-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883124"
    },
    {
        "id": 8851,
        "title": "Paraphrase Generation Model Using Transformer Based Architecture",
        "authors": "Mosima Anna Masethe, Hlaudi Daniel Masethe, Sunday Olusegun Ojo, Pius A Owolawi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683780"
    },
    {
        "id": 8852,
        "title": "Development of a Model for Detection and Classification of Diseases in Tomato Plants Using Swin Transformer Architecture",
        "authors": "Oscar David Valencia, Maria José Capera Firigua",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis research focuses on developing a model for the detection and classification of diseases in tomato plants using the Swin Transformer architecture. The model aims to surpass the accuracy limitations of current Convolutional Neural Networks (CNN) methods. The study involves constructing a balanced dataset for various tomato plant diseases, evaluating the model's predictions, and comparing its accuracy with CNN-based models.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3618075/v1"
    },
    {
        "id": 8853,
        "title": "Foundations of ANNs: Tolstoy’s Genius Explored using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "Artificial Narrow Intelligence is in the phase of moving towards the AGN, which will attempt to decide as a human being. We are getting closer to it by each day, but AI actually is indefinite to many, although it is no different than any other set of mathematically defined computer operations in its core. Generating new data from a pre-trained model introduces new challenges to science & technology. In this work, the design of such an architecture from scratch, solving problems, and introducing alternative approaches are what has been conducted. Using a deep thinker, Tolstoy, as an object of study is a source of motivation for the entire research.",
        "link": "http://dx.doi.org/10.5121/ijaia.2024.15105"
    },
    {
        "id": 8854,
        "title": "An Improved Transformer Transducer Architecture for Hindi-English Code Switched Speech Recognition",
        "authors": "Ansen Antony, Sumanth Reddy Kota, Akhilesh Lade, Spoorthy V, Shashidhar G. Koolagudi",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10763"
    },
    {
        "id": 8855,
        "title": "Detecting Fine-Grained Emotions from COVID-19 Tweets Using Transformer-Based Architecture",
        "authors": "Rida Javed Kutty, Nazura Javed, Rahul Mallya",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itis59651.2023.10420305"
    },
    {
        "id": 8856,
        "title": "Classification of Phonocardiogram Recordings using Vision Transformer Architecture",
        "authors": "\"Joonyeob Kim, Gibeom Park, Bongwon Suh\"",
        "published": "2022-12-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22489/cinc.2022.084"
    },
    {
        "id": 8857,
        "title": "Evolving transformer architecture for neural machine translation",
        "authors": "Ben Feng, Dayiheng Liu, Yanan Sun",
        "published": "2021-7-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449726.3459441"
    },
    {
        "id": 8858,
        "title": "Sika Deer Trajectory Prediction Considering Environmental Factors by Timeseries Transformer-Based Architecture",
        "authors": "Kentaro Kazama, Katsuhide Fujita, Yushin Shinoda, Shinsuke Koike",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4276307"
    },
    {
        "id": 8859,
        "title": "Evaluation of the Coherence of Ukrainian Texts Using a Transformer Architecture",
        "authors": "Artem Kramov, Sergiy Pogorilyy",
        "published": "2020-11-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/atit50783.2020.9349355"
    },
    {
        "id": 8860,
        "title": "A Transformer Network Architecture for Dermoscopy Image Segmentation",
        "authors": "Lixing Wei",
        "published": "2022-7-1",
        "citations": 0,
        "abstract": "Abstract\nAiming at the problems of irregular shape and blurred boundary of skin lesions in skin lesions images, this paper proposes a skin lesion segmentation algorithm combining CNN and Transformer. Firstly, Resnet is used as the backbone feature extraction network to extract features, and the extracted feature map sequence is used as the input of Transformer. A new structural boundary attention gate is added to Transformer to extract enough local details to deal with fuzzy boundaries. Finally, DenseASPP is used to enhance features Represents and processes multi-scale information, and proposes an improved loss function, the purpose of which is to make the model pay attention to the boundary region when calculating the loss function. The experimental results show that the dice value and JI value of the network on the ISIC2017 dataset are 0.854534 and 0.767901, respectively, and the dice value and JI value on the ISIC2018 dataset are 0.908548 and 0.843689, respectively, which achieves good results compared to other advanced models. Its effectiveness is proved by comparing with different models and showing the effect.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2303/1/012043"
    },
    {
        "id": 8861,
        "title": "A Transformer-based Neural Architecture Search Method",
        "authors": "Shang Wang, Huanrong Tang, Jianquan Ouyang",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3583133.3590735"
    },
    {
        "id": 8862,
        "title": "Load Autoformer: A Transformer architecture for short-term load forecasting",
        "authors": "Yuzhe Huang, Fan Mo, Zitong Zhang, Chenghan Li, Kan Li",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispec58282.2023.10402999"
    },
    {
        "id": 8863,
        "title": "Joint Transformer/RNN Architecture for Gesture Typing in Indic Languages",
        "authors": "Emil Biju, Anirudh Sriram, Mitesh M. Khapra, Pratyush Kumar",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.87"
    },
    {
        "id": 8864,
        "title": "Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer",
        "authors": " Mohanad Sameer,  Ahmed Talib,  Alla Hussein",
        "published": "2023-3-21",
        "citations": 2,
        "abstract": "Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when utilizing the Common Voice dataset from Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 hours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition systems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training requires a very large amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language model.",
        "link": "http://dx.doi.org/10.51173/jt.v5i1.749"
    },
    {
        "id": 8865,
        "title": "A novel multi-port solid state transformer enabled isolated hybrid microgrid architecture",
        "authors": "Arun Chandrasekharan Nair, B.G. Fernandes",
        "published": "2017-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2017.8216113"
    },
    {
        "id": 8866,
        "title": "Design and Experimental Analysis of a Modular Smart Transformer Architecture",
        "authors": "Levy F. Costa, Youngjong Ko, Marco Liserre",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecce53617.2023.10362128"
    },
    {
        "id": 8867,
        "title": "Memory-Efficient Differentiable Transformer Architecture Search",
        "authors": "Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, Weizhu Chen",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.372"
    },
    {
        "id": 8868,
        "title": "Should we Hard-Code the Recurrence Concept or Learn it Instead ? Exploring the Transformer Architecture for Audio-Visual Speech Recognition",
        "authors": "George Sterpu, Christian Saam, Naomi Harte",
        "published": "2020-10-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2480"
    },
    {
        "id": 8869,
        "title": "Ct-Gan: A Conditional Generative Adversarial Network of Transformer Architecture for Text-to-Image",
        "authors": "Bing Wang, Xin Zhang, Wentao Jiao, Xuedong Tian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4238357"
    },
    {
        "id": 8870,
        "title": "Pocformer: A Lightweight Transformer Architecture For Detection Of Covid-19 Using Point Of Care Ultrasound",
        "authors": "Shehan Perera, Srikar Adhikari, Alper Yilmaz",
        "published": "2021-9-19",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506353"
    },
    {
        "id": 8871,
        "title": "A Convolutional Transformer Architecture for Remaining Useful Life Estimation",
        "authors": "Yifei Ding, Minping Jia",
        "published": "2021-10-15",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/phm-nanjing52125.2021.9612814"
    },
    {
        "id": 8872,
        "title": "Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture for Complex NER",
        "authors": "Amit Pandey, Swayatta Daw, Vikram Pudi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.224"
    },
    {
        "id": 8873,
        "title": "An Efficient ConvNet for Learned Image Compression with Transformer-Style Architecture",
        "authors": "Haihang Ruan, Feng Wang, Yan Wang",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vcip59821.2023.10402780"
    },
    {
        "id": 8874,
        "title": "Searching Efficient Neural Architecture with Multi-resolution Fusion Transformer for Appearance-based Gaze Estimation",
        "authors": "Vikrant Nagpure, Kenji Okuma",
        "published": "2023-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00095"
    },
    {
        "id": 8875,
        "title": "SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics",
        "authors": "Da Yin, Tao Meng, Kai-Wei Chang",
        "published": "2020",
        "citations": 51,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.341"
    },
    {
        "id": 8876,
        "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
        "authors": "Kangjie Cao, Ting Zhang, Jueqiao Huang",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "AbstractIn the field of engineering systems—particularly in underground drilling and green stormwater management—real-time predictions are vital for enhancing operational performance, ensuring safety, and increasing efficiency. Addressing this niche, our study introduces a novel LSTM-transformer hybrid architecture, uniquely specialized for multi-task real-time predictions. Building on advancements in attention mechanisms and sequence modeling, our model integrates the core strengths of LSTM and Transformer architectures, offering a superior alternative to traditional predictive models. Further enriched with online learning, our architecture dynamically adapts to variable operational conditions and continuously incorporates new field data. Utilizing knowledge distillation techniques, we efficiently transfer insights from larger, pretrained networks, thereby achieving high predictive accuracy without sacrificing computational resources. Rigorous experiments on sector-specific engineering datasets validate the robustness and effectiveness of our approach. Notably, our model exhibits clear advantages over existing methods in terms of predictive accuracy, real-time adaptability, and computational efficiency. This work contributes a pioneering predictive framework for targeted engineering applications, offering actionable insights into.",
        "link": "http://dx.doi.org/10.1038/s41598-024-55483-x"
    },
    {
        "id": 8877,
        "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture",
        "authors": "Christopher Brix, Parnia Bahar, Hermann Ney",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.acl-main.360"
    },
    {
        "id": 8878,
        "title": "End-to-End Automated Speech Recognition Using a Character Based Small Scale Transformer Architecture",
        "authors": "Alexander Loubser, Allan De Freitas, Pieter de Villiers",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4290605"
    },
    {
        "id": 8879,
        "title": "A Transformer Architecture for Stress Detection from ECG",
        "authors": "Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Paul Hungler, Ali Etemad",
        "published": "2021-9-21",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3460421.3480427"
    },
    {
        "id": 8880,
        "title": "An ensemble novel architecture for Bangla Mathematical Entity Recognition (MER) using transformer based learning",
        "authors": "Tanjim Taharat Aurpa, Md Shoaib Ahmed",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e25467"
    },
    {
        "id": 8881,
        "title": "Neural network architecture «transformer»: Artificial Intelligence and its role in Natural Language Processing",
        "authors": "A.M Evstropov, E.A Tarlakovskaya, I.A Sidorov",
        "published": "2023",
        "citations": 0,
        "abstract": "The transformer neural architecture has become a cornerstone of natural language processing (NLP) models. It is a powerful tool for language understanding and generation, enabling machines to process human language that is close to human understanding. NLP has significantly improved in recent years due to pro-gress in artificial intelligence (AI). One of the key developments that has enabled these improvements is the transformer neural network architecture. In this paper, I will explore the transformer architecture, its main concepts, and its application in NLP.",
        "link": "http://dx.doi.org/10.18411/trnio-05-2023-633"
    },
    {
        "id": 8882,
        "title": "High-Efficiency Solid State Transformer Architecture for Large-scale PV Application",
        "authors": "Kangan Wang, Rongwu Zhu, Youngjong Ko, Marco Liserre",
        "published": "2019-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iecon.2019.8926910"
    },
    {
        "id": 8883,
        "title": "An Augmented Transformer Architecture for Natural Language Generation Tasks",
        "authors": "Hailiang Li, Adele, Y.C. Wang, Yang Liu, Du Tang, Zhibin Lei, Wenye Li",
        "published": "2019-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw48858.2019.9024754"
    },
    {
        "id": 8884,
        "title": "CTPlantNet: A Hybrid CNN-Transformer Architecture for Plant Disease Classification",
        "authors": "Adnane Ait Nasser, Moulay A. Akhloufi",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icm56065.2022.10005433"
    },
    {
        "id": 8885,
        "title": "Siamese Transformer Network: Building an autonomous real-time target tracking system for UAV",
        "authors": "Xiaolou Sun, Qi Wang, Fei Xie, Zhibin Quan, Wei Wang, Hao Wang, Yuncong Yao, Wankou Yang, Satoshi Suzuki",
        "published": "2022-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sysarc.2022.102675"
    },
    {
        "id": 8886,
        "title": "TransUSleepNet:&amp;nbsp;A Deep Learning Architecture for Sleep Scoring Based Transformer and U-Net",
        "authors": "Tianxing Li, Yulin Gong, Chang Liu, Yudan Lv, Jiahao Zhang, Bo Han, Chuankang Leng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4661673"
    },
    {
        "id": 8887,
        "title": "TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer",
        "authors": "Minxuan Zhou, Weihong Xu, Jaeyoung Kang, Tajana Rosing",
        "published": "2022-4",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hpca53966.2022.00082"
    },
    {
        "id": 8888,
        "title": "Towards Lightweight Transformer Architecture: an Analysis on Semantic Segmentation",
        "authors": "Bohdan Ivaniuk-Skulskyi, Nadiya Shvai, Arcadi Llanza, Amir Nakib",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467926"
    },
    {
        "id": 8889,
        "title": "A Transformer Architecture with Adaptive Attention for Fine-Grained Visual Classification",
        "authors": "Changli Cai, Tiankui Zhang, Zhewei Weng, Chunyan Feng, Yapeng Wang",
        "published": "2021-12-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc54389.2021.9674560"
    },
    {
        "id": 8890,
        "title": "An enhancement of transformer-based architecture with randomized regularization for wind speed prediction",
        "authors": "Tham Vo",
        "published": "2023-1-30",
        "citations": 0,
        "abstract": "The wind power is considered as a potential renewable energy resource which requires less management cost and effort than the others like as tidal, geothermal, etc. However, the natural randomization and volatility aspects of wind in different regions have brought several challenges for efficiently as well as reliably operating the wind-based power supply grid. Thus, it is necessary to have centralized monitoring centers for managing as well as optimizing the performance of wind power farms. Among different management task, wind speed prediction is considered as an important task which directly support for further wind-based power supply resource planning/optimization, hence towards power shortage risk and operating cost reductions. Normally, considering as traditional time-series based prediction problem, most of previous deep learning-based models have demonstrated significant improvement in accuracy performance of wind speed prediction problem. However, most of recurrent neural network (RNN) as well as sequential auto-encoding (AE) based architectures still suffered several limitations related to the capability of sufficient preserving the spatiotemporal and long-range time dependent information of complex time-series based wind datasets. Moreover, previous RNN-based wind speed predictive models also perform poor prediction results within high-complex/noised time-series based wind speed datasets. Thus, in order to overcome these limitations, in this paper we proposed a novel integrated convolutional neural network (CNN)-based spatiotemporal randomization mechanism with transformer-based architecture for wind speed prediction problem, called as: RTrans-WP. Within our RTrans-WP model, we integrated the deep neural encoding component with a randomized CNN learning mechanism to softy align temporal feature within the long-range time-dependent learning context. The utilization of randomized CNN component at the data encoding part also enables to reduce noises and time-series based observation uncertainties which are occurred during the data representation learning and wind speed prediction-driven fine-tuning processes.",
        "link": "http://dx.doi.org/10.3233/jifs-222446"
    },
    {
        "id": 8891,
        "title": "New Architecture of Transformer Networks for Generating Natural Dialogues",
        "authors": "V. A. Ryndyuk, Y. S. Varakin, E. A. Pisarenko",
        "published": "2022-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/weconf55058.2022.9803724"
    },
    {
        "id": 8892,
        "title": "Hybrid UNet transformer architecture for ischemic stoke segmentation with MRI and CT datasets",
        "authors": "Wei Kwek Soh, Jagath C. Rajapakse",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "A hybrid UNet and Transformer (HUT) network is introduced to combine the merits of the UNet and Transformer architectures, improving brain lesion segmentation from MRI and CT scans. The HUT overcomes the limitations of conventional approaches by utilizing two parallel stages: one based on UNet and the other on Transformers. The Transformer-based stage captures global dependencies and long-range correlations. It uses intermediate feature vectors from the UNet decoder and improves segmentation accuracy by enhancing the attention and relationship modeling between voxel patches derived from the 3D brain volumes. In addition, HUT incorporates self-supervised learning on the transformer network. This allows the transformer network to learn by maintaining consistency between the classification layers of the different resolutions of patches and augmentations. There is an improvement in the rate of convergence of the training and the overall capability of segmentation. Experimental results on benchmark datasets, including ATLAS and ISLES2018, demonstrate HUT's advantage over the state-of-the-art methods. HUT achieves higher Dice scores and reduced Hausdorff Distance scores in single-modality and multi-modality lesion segmentation. HUT outperforms the state-the-art network SPiN in the single-modality MRI segmentation on Anatomical Tracings of lesion After Stroke (ATLAS) dataset by 4.84% of Dice score and a large margin of 40.7% in the Hausdorff Distance score. HUT also performed well on CT perfusion brain scans in the Ischemic Stroke Lesion Segmentation (ISLES2018) dataset and demonstrated an improvement over the recent state-of-the-art network USSLNet by 3.3% in the Dice score and 12.5% in the Hausdorff Distance score. With the analysis of both single and multi-modalities datasets (ATLASR12 and ISLES2018), we show that HUT can perform and generalize well on different datasets.Code is available at: https://github.com/vicsohntu/HUT_CT.",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1298514"
    },
    {
        "id": 8893,
        "title": "QGFORMER: Quantum-Classical Hybrid Transformer Architecture for Gravitational Wave Detection",
        "authors": "Hu Jiaxiang, Liu Jiale",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip60502.2023.10387052"
    },
    {
        "id": 8894,
        "title": "Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation",
        "authors": "Yuliang Cai, Jesse Thomason, Mohammad Rostami",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.466"
    },
    {
        "id": 8895,
        "title": "Spatio-Temporal Analysis of Transformer based Architecture for Attention Estimation from EEG",
        "authors": "Victor Delvigne, Hazem Wannous, Jean-Philippe Vandeborre, Laurence Ris, Thierry Dutoit",
        "published": "2022-8-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956610"
    },
    {
        "id": 8896,
        "title": "Self-Supervised Transformer Architecture for Change Detection in Radio Access Networks",
        "authors": "Igor Kozlov, Dmitriy Rivkin, Wei-Di Chang, Di Wu, Xue Liu, Gregory Dudek",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278818"
    },
    {
        "id": 8897,
        "title": "AI-Generated Text Detector for Arabic Language Using Encoder-Based Transformer Architecture",
        "authors": "Hamed Alshammari, Ahmed El-Sayed, Khaled Elleithy",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "The effectiveness of existing AI detectors is notably hampered when processing Arabic texts. This study introduces a novel AI text classifier designed specifically for Arabic, tackling the distinct challenges inherent in processing this language. A particular focus is placed on accurately recognizing human-written texts (HWTs), an area where existing AI detectors have demonstrated significant limitations. To achieve this goal, this paper utilized and fine-tuned two Transformer-based models, AraELECTRA and XLM-R, by training them on two distinct datasets: a large dataset comprising 43,958 examples and a custom dataset with 3078 examples that contain HWT and AI-generated texts (AIGTs) from various sources, including ChatGPT 3.5, ChatGPT-4, and BARD. The proposed architecture is adaptable to any language, but this work evaluates these models’ efficiency in recognizing HWTs versus AIGTs in Arabic as an example of Semitic languages. The performance of the proposed models has been compared against the two prominent existing AI detectors, GPTZero and OpenAI Text Classifier, particularly on the AIRABIC benchmark dataset. The results reveal that the proposed classifiers outperform both GPTZero and OpenAI Text Classifier with 81% accuracy compared to 63% and 50% for GPTZero and OpenAI Text Classifier, respectively. Furthermore, integrating a Dediacritization Layer prior to the classification model demonstrated a significant enhancement in the detection accuracy of both HWTs and AIGTs. This Dediacritization step markedly improved the classification accuracy, elevating it from 81% to as high as 99% and, in some instances, even achieving 100%.",
        "link": "http://dx.doi.org/10.3390/bdcc8030032"
    },
    {
        "id": 8898,
        "title": "Modelling and Experimental Evaluation of Ideal Transformer Algorithm Interface for Power Hardware in the Loop Architecture",
        "authors": "Mandip Pokharel, Carl Ngai Man Ho",
        "published": "2020-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apec39645.2020.9124046"
    },
    {
        "id": 8899,
        "title": "A Vision Transformer Architecture for Open Set Recognition",
        "authors": "Feiyang Cai, Zhenkai Zhang, Jie Liu, Xenofon Koutsoukos",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00034"
    },
    {
        "id": 8900,
        "title": "Genetic Algorithm-based Transformer Architecture Design for Neural Machine Translation",
        "authors": "Jie Wu, Ben Feng, Yanan Sun",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3568199.3568215"
    }
]