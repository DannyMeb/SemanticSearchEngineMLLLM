[
    {
        "id": 10671,
        "title": "Fine-Tuning Fine-Tuning",
        "authors": "Yoaav Isaacs",
        "published": "2018-3-22",
        "citations": 10,
        "abstract": "This chapter argues that the fine-tuning argument for the existence of God is a straightforwardly legitimate argument. The fine-tuning argument takes certain features of fundamental physics to confirm the existence of God because these features of fundamental physics are more likely given the existence of God than they are given the non-existence of God. And any such argument is straightforwardly legitimate, as such arguments follow a canonically legitimate form of empirical argumentation. The chapter explores various objections to the fine-tuning argument: that it requires an ill-defined notion of small changes in the laws of physics, that it over-generalizes, that it requires implausible presuppositions about divine intentions, and that it is debunked by anthropic reasoning. In each case it finds either that the putatively objectionable feature of the fine-tuning argument is inessential to it or that the putatively objectionable feature of the fine-tuning argument is not actually objectionable.",
        "link": "http://dx.doi.org/10.1093/oso/9780198798705.003.0008"
    },
    {
        "id": 10672,
        "title": "Fine-Tuning: From Star to Galaxies Formation",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.010"
    },
    {
        "id": 10673,
        "title": "Naturalness, Fine-tuning, and Observer Selection in Cosmology",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.003"
    },
    {
        "id": 10674,
        "title": "Fine-Tuning, Complexity, and Life in the Multiverse",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.001"
    },
    {
        "id": 10675,
        "title": "Idealism and Fine-Tuning",
        "authors": "Jacob Ross",
        "published": "2018-1-18",
        "citations": 0,
        "abstract": "This chapter argues that, given certain background assumptions, a kind of idealism follows from a version of the fine-tuning thesis. The kind of idealism in question ascribes explanatory priority, not ontological priority, to the mental. The version of the fine-tuning thesis in question is the strong fine-tuning for consciousness thesis, according to which (i) the values of the fundamental physical parameters are fine-tuned for consciousness and (ii) this fine-tuning for consciousness is not the inevitable by-product of fine-tuning for something more basic than consciousness, such as life. The chapter argues that, assuming a particular account of the nature of explanation—namely, the unificationist account—the strong fine-tuning for consciousness thesis entails that consciousness plays a fundamental explanatory role in nature, and so this thesis entails explanatory idealism. The chapter concludes by arguing that similar reasoning leads to the conclusion that consciousness is the final cause of the universe.",
        "link": "http://dx.doi.org/10.1093/oso/9780198746973.003.0015"
    },
    {
        "id": 10676,
        "title": "Privacy-Preserving Fine-Tuning of Artificial Intelligence (AI) Foundation Models with Federated Learning, Differential Privacy, Offsite Tuning, and Parameter-Efficient Fine-Tuning (PEFT)",
        "authors": "Jun Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Artificial Intelligence (AI) Foundation Models (FMs), pre-trained on massive datasets, have recently emerged as a pivotal asset in a wide array of tasks. Examples of FMs include Large Language Models (LLMs), Large Vision Models (LVMs), and Large Multimodal Models (LMMs). The adaptability of FMs, achieved through finetuning, enables these models to perform exceptionally across diverse domains. However, the fine-tuning process often entails data centralization, which raises privacy concerns. For instance, in healthcare, hospitals might want to fine-tune an AI model on patient records. Sending this data to a central server for fine-tuning can raise privacy concerns. To mitigate the privacy challenges, this research seeks to employ privacy-preserving technologies such as federated learning (FL), differential privacy (DP), and emulator-based tuning (i.e., offsite tuning) in combination with parameter-efficient fine-tuning (PEFT) techniques to refine FMs without compromising data privacy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24191886.v1"
    },
    {
        "id": 10677,
        "title": "Privacy-Preserving Fine-Tuning of Artificial Intelligence (AI) Foundation Models with Federated Learning, Differential Privacy, Offsite Tuning, and Parameter-Efficient Fine-Tuning (PEFT)",
        "authors": "Jun Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Artificial Intelligence (AI) Foundation Models (FMs), pre-trained on massive datasets, have recently emerged as a pivotal asset in a wide array of tasks. Examples of FMs include Large Language Models (LLMs), Large Vision Models (LVMs), and Large Multimodal Models (LMMs). The adaptability of FMs, achieved through finetuning, enables these models to perform exceptionally across diverse domains. However, the fine-tuning process often entails data centralization, which raises privacy concerns. For instance, in healthcare, hospitals might want to fine-tune an AI model on patient records. Sending this data to a central server for fine-tuning can raise privacy concerns. To mitigate the privacy challenges, this research seeks to employ privacy-preserving technologies such as federated learning (FL), differential privacy (DP), and emulator-based tuning (i.e., offsite tuning) in combination with parameter-efficient fine-tuning (PEFT) techniques to refine FMs without compromising data privacy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24191886"
    },
    {
        "id": 10678,
        "title": "Climbing Up the Theories of Nature: Fine-Tuning and Biological Molecules",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.013"
    },
    {
        "id": 10679,
        "title": "Our Fine-Tuned Universe?",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-3"
    },
    {
        "id": 10680,
        "title": "Our Fine-Tuned Actual World?",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-6"
    },
    {
        "id": 10681,
        "title": "Cosmological Fine-Tuning Arguments",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537"
    },
    {
        "id": 10682,
        "title": "A Theological Critique of the Fine-Tuning Argument",
        "authors": "Hans Halvorson",
        "published": "2018-3-22",
        "citations": 0,
        "abstract": "The fine-tuning argument attempts to use data from contemporary physics as evidence for God’s existence. In particular, contemporary physics suggests that—in absence of any divine intervention—there was little chance that a universe like ours would come into existence. The chapter points out a theological problem with the fine-tuning argument: since God can choose the laws of nature, God can set the chances that a universe like ours would come into existence. It argues, however, that if God could be expected to create a nice universe, then God could also be expected to set favourable chances for a nice universe. Therefore, the fine-tuning argument defeats itself.",
        "link": "http://dx.doi.org/10.1093/oso/9780198798705.003.0007"
    },
    {
        "id": 10683,
        "title": "Self-Attention Factor-Tuning for Parameter Efficient Fine-Tuning",
        "authors": "Jason Abohwo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformers have revolutionized the fields of Natural Language Processing and Computer Vision - a result of their ability to capture long-range dependencies with their key innovation: the attention mechanism. Despite the success of these models, their growing complexity has led to an ever-increasing need for processing power, making their practical applications less feasible. In recent years, tensor decomposition-based parameter-efficient fine-tuning techniques have emerged as a promising solution to the computational bottleneck. In this research, we investigate the use of a modified version of Factor Tuning that lessens inter-layer associations that the original Factor Tuning creates and focuses exclusively on attention mechanisms. We refer to this method as Self-Attention Factor-Tuning. To evaluate the effectiveness of our approach, we conduct experiments with Vision Transformers using all 19 datasets from the VTAB-1k benchmark for image classification. The results demonstrate that the proposed framework effectively reduces the number of parameters required to fine-tune a transformer, achieving new state-of-the-art performance on three of the 19 datasets in the benchmark and outperforming the original Factor-Tuning paradigm as well as various other competitive techniques, whilst using significantly fewer parameters.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3487308/v2"
    },
    {
        "id": 10684,
        "title": "Introduction",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-1"
    },
    {
        "id": 10685,
        "title": "Self-Attention Factor-Tuning for Parameter Efficient Fine-Tuning",
        "authors": "Jason Abohwo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTransformers have revolutionized the fields of natural language processing and computer vision - a result of their ability to capture long-range dependencies with their key innovation: the attention mechanism. Despite the success of these models, their growing complexity has led to an ever-increasing need for processing power, making their practical applications less feasible. In recent years, tensor decomposition based parameter-efficient fine-tuning techniques have emerged as a promising solution to the computational bottleneck. In this research, we investigate the use of a specialized version of Factor Tuning that focuses exclusively on tensor decomposition for attention layers, which we refer to as Self-Attention Factor-Tuning. This selective decomposition not only allows for more inter-layer weight redundancy reduction, but also improves the model's efficiency and scalability. To evaluate the effectiveness of our approach, we conduct experiments with Vision Transformers using several benchmark datasets for image classification. The results demonstrate that the proposed framework effectively reduces the number of parameters required to train a transformer, outperforming the original Factor-Tuning paradigm as well as various other state-of-the-art techniques such as Low-Rank Adaptation whilst using significantly less parameters.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3487308/v1"
    },
    {
        "id": 10686,
        "title": "Hierarchy of Fine-Structure Consta",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.002"
    },
    {
        "id": 10687,
        "title": "Fine-Tunings at Particle Scales",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.008"
    },
    {
        "id": 10688,
        "title": "fine tuning, n.",
        "authors": "",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/oed/4737120488"
    },
    {
        "id": 10689,
        "title": "Four Preliminary Investigations",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-2"
    },
    {
        "id": 10690,
        "title": "Mapping the Logical Space",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-5"
    },
    {
        "id": 10691,
        "title": "Learning From the Critics",
        "authors": "Jason Waller",
        "published": "2019-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315182537-4"
    },
    {
        "id": 10692,
        "title": "Infinite Cardinalities, Measuring Knowledge, and Probabilities in Fine-Tuning Arguments",
        "authors": "Isaac Choi",
        "published": "2018-3-22",
        "citations": 0,
        "abstract": "This chapter deals with two different problems in which infinity plays a central role. It first responds to a claim that infinity renders counting knowledge-level beliefs an infeasible approach to measuring and comparing how much we know. There are two methods of comparing sizes of infinite sets, using the one-to-one correspondence principle or the subset principle, and it argues that we should use the subset principle for measuring knowledge. The chapter then turns to the normalizability and coarse tuning objections to fine-tuning arguments for the existence of God or a multiverse. These objections center on the difficulty of talking about the epistemic probability of a physical constant falling within a finite life-permitting range when the possible range of that constant is infinite. Applying the lessons learned regarding infinity and the measurement of knowledge, the chapter hopes to blunt much of the force of these objections to fine-tuning arguments.",
        "link": "http://dx.doi.org/10.1093/oso/9780198798705.003.0006"
    },
    {
        "id": 10693,
        "title": "Realignment",
        "authors": "Evan Osborne",
        "published": "2018-1-23",
        "citations": 0,
        "abstract": "The later nineteenth and early twentieth centuries witnessed arguments from social reformers and artists and economists that the new, spontaneously evolving society was deficient. It worsened poverty, and it impoverished the soul. The tool of political regulation, exercised in the growing political power of the emerging organization known as the nation, was called in to polish the rough edges of the self-regulating society. As time went on, political regulation gradually came to be seen as the default, and self-regulation needed to be justified. The chapter particularly emphasizes the growth in such thinking among socialists and progressives in the United States and Western Europe. The catastrophe of the Great Depression, combined with admiration for a Soviet Union, Italy, and Germany, where political regulators said they were rationally designing a better society, meant that by the onset of World War II, this presumption was firmly in place throughout the West.",
        "link": "http://dx.doi.org/10.11126/stanford/9780804796446.003.0006"
    },
    {
        "id": 10694,
        "title": "ROS2 Fine Tuning",
        "authors": "Jaime Losa",
        "published": "2017-9-21",
        "citations": 0,
        "abstract": "\n",
        "link": "http://dx.doi.org/10.36288/roscon2017-900266"
    },
    {
        "id": 10695,
        "title": "ROS2 Fine Tuning",
        "authors": "Jaime Losa",
        "published": "2017-9-21",
        "citations": 0,
        "abstract": "\n",
        "link": "http://dx.doi.org/10.36288/roscon2017-900810"
    },
    {
        "id": 10696,
        "title": "Is the Universal Matter-Antimatter Asymmetry Fine-Tuned?",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.005"
    },
    {
        "id": 10697,
        "title": "Design Inferences, Fine-Tuning, and the Prior Probability of Divine Intelligent Agency: What the Fine-Tuning Argument Shows 1",
        "authors": "Kenneth Einar Himma",
        "published": "2017-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781351150408-9"
    },
    {
        "id": 10698,
        "title": "Index",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.014"
    },
    {
        "id": 10699,
        "title": "Synopsis: Fine-tuning the EIF Model",
        "authors": "Sarah Buschfeld",
        "published": "2020-9-1",
        "citations": 4,
        "abstract": "Chapter 18 offers a synopsis of the preceding chapters. In the light of chapters 2 to 17, the editor sums up and comments on the advantages, shortcomings, and potential pitfalls of the Extra- and Intra-territorial Forces Model and thus evaluates its suitability for both postcolonial and non-postcolonial contexts. The synopsis provides an extended list of extra- and intra-territorial forces which can be identified in the case studies discussed (for a first list of potential forces, see Buschfeld and Kautzsch 2017). It also meets the critical voices raised in some of the contributions and offers some alternations and modifications to the original conception of the model, viz. the revised Extra- and Intra-territorial Forces Model.",
        "link": "http://dx.doi.org/10.3366/edinburgh/9781474445863.003.0018"
    },
    {
        "id": 10700,
        "title": "Matching Pre-Trained Language Models with Specific Tasks: Fine-Tuning and Prompt-Tuning Strategies",
        "authors": "Ahmad Pouramini, Hesham Faili",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4702919"
    },
    {
        "id": 10701,
        "title": "Dark Matter",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.009"
    },
    {
        "id": 10702,
        "title": "Fine Tuning Treatment",
        "authors": "",
        "published": "2021-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1542/9781610025478-9"
    },
    {
        "id": 10703,
        "title": "Structure Formation",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.006"
    },
    {
        "id": 10704,
        "title": "Fine-tuning and Humean laws: fine-tuning as argument for a non-governing account of laws rather than for God or multiverse",
        "authors": "John F. Halpin",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11229-022-03712-z"
    },
    {
        "id": 10705,
        "title": "Fine-Tuning",
        "authors": "Barış Çakır",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-81861-6_18"
    },
    {
        "id": 10706,
        "title": "Fine-tuning human for LLM projects",
        "authors": "Rehan Guha",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent years after emerging of LLM in the space of NLP, tools like ChatGPT, BARD, DALL-E, took over the market and daily lives. We will be focusing on the human like output capability from LLM’s . There is a huge reservation of this technology due to multiple scenarios like security, accuracy, relevance, etc... In this paper, I will talk about a method which I have designed to fine tune a human being to lower the expectation from LLM outputs and increase the acceptance rate of the final product. This technique is more of a psychological method than a technological way to improve the models output to be more human like.",
        "link": "http://dx.doi.org/10.31219/osf.io/9js3b"
    },
    {
        "id": 10707,
        "title": "The Fine-Tuning Considerations",
        "authors": "",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108765947.003"
    },
    {
        "id": 10708,
        "title": "Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER",
        "authors": "Qingkai Zeng, Wenhao Yu, Mengxia Yu, Tianwen Jiang, Tim Weninger, Meng Jiang",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.429"
    },
    {
        "id": 10709,
        "title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained",
        "authors": "Hongliang Dai, Ziqian Zeng",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.126"
    },
    {
        "id": 10710,
        "title": "Multi-phase Fine-Tuning: A New Fine-Tuning Approach for Sign Language Recognition",
        "authors": "Noha Sarhan, Mikko Lauri, Simone Frintrop",
        "published": "2022-3",
        "citations": 3,
        "abstract": "AbstractIn this paper, we propose multi-phase fine-tuning for tuning deep networks from typical object recognition to sign language recognition (SLR). It extends the successful idea of transfer learning by fine-tuning the network’s weights over several phases. Starting from the top of the network, layers are trained in phases by successively unfreezing layers for training. We apply this novel training approach to SLR, since in this application, training data is scarce and differs considerably from the datasets which are usually used for pre-training. Our experiments show that multi-phase fine-tuning can reach significantly better accuracy in fewer training epochs compared to previous fine-tuning techniques",
        "link": "http://dx.doi.org/10.1007/s13218-021-00746-2"
    },
    {
        "id": 10711,
        "title": "A Genetic Algorithm for HMI Test Infrastructure Fine Tuning",
        "authors": "Lukas Rosenbauer, Anthony Stein, Jörg Hähner",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010512800002994"
    },
    {
        "id": 10712,
        "title": "Grapevine Leaves Classification Using Transfer Learning and Fine Tuning",
        "authors": "Muhammet Çakmak",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374623"
    },
    {
        "id": 10713,
        "title": "Deep Discriminative Fine-Tuning for Cancer Type Classification",
        "authors": "Alena Harley",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractDetermining the primary site of origin for metastatic tumors is one of the open problems in cancer care because the efficacy of treatment often depends on the cancer tissue of origin. Classification methods that can leverage tumor genomic data and predict the site of origin are therefore of great value.Because tumor DNA point mutation data is very sparse, only limited accuracy (64.5% for 12 tumor classes) was previously demonstrated by methods that rely on point mutations as features (1). Tumor classification accuracy can be greatly improved (to over 90% for 33 classes) by relying on gene expression data (2). However, this additional data is often not readily available in clinical setting, because point mutations are better profiled and targeted by clinical mutational profiling.Here we sought to develop an accurate deep transfer learning and fine-tuning method for tumor sub-type classification, where predicted class is indicative of the primary site of origin. Our method significantly outperforms the state-of-the-art for tumor classification using DNA point mutations, reducing the error by more than 30% at the same time discriminating over many more classes on The Cancer Genome Atlas (TCGA) dataset. Using our method, we achieve state-of-the-art tumor type classification accuracy of 78.3% for 29 tumor classes relying on DNA point mutations in the tumor only.",
        "link": "http://dx.doi.org/10.1101/841056"
    },
    {
        "id": 10714,
        "title": "A Genetic Algorithm for HMI Test Infrastructure Fine Tuning",
        "authors": "Lukas Rosenbauer, Anthony Stein, Jörg Hähner",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010512803670374"
    },
    {
        "id": 10715,
        "title": "Fine-Tuning Air Pollution Models",
        "authors": "Sarah Derouin",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "InMAP estimates air pollution within cities, but its predictions are flawed for specific chemicals. Now, scientists are addressing that shortcoming.",
        "link": "http://dx.doi.org/10.1029/2023eo230181"
    },
    {
        "id": 10716,
        "title": "Transfer Learning Gaussian Anomaly Detection by Fine-tuning Representations",
        "authors": "Oliver Rippel, Arnav Chavan, Chucai Lei, Dorit Merhof",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011063900003209"
    },
    {
        "id": 10717,
        "title": "HEp-2 Intensity Classification based on Deep Fine-tuning",
        "authors": "Vincenzo Taormina, Donato Cascio, Leonardo Abbene, Giuseppe Raso",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008954501430149"
    },
    {
        "id": 10718,
        "title": "Fine-tuning your brand",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781350009851.ch-003"
    },
    {
        "id": 10719,
        "title": "Fine-Tuning the Red-Black Tree Delete Algorithm",
        "authors": "Djamel Eddine ZEGOUR",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRed-Black trees have gained popularity as a data structure widely used for implementing dictionaries, associative arrays, and symbol tables in compilers like C++ and Java, as well as other systems. This paper introduces an enhanced delete algorithm for this type of binary search tree. The proposed algorithm shows promise by introducing a different coloring scheme to the tree, resulting in a significant reduction of approximately 29% and 11% in color changes and maintenance operations, respectively. The new coloring emphasizes the presence of red nodes, effectively accelerating the deletion operation. Moreover, it achieves a 4% improvement in running time when insert and delete operations are performed together, while preserving the search performance of the standard algorithm.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1194654/v6"
    },
    {
        "id": 10720,
        "title": "VGG Fine-tuning for Cooking State Recognition",
        "authors": "Juan Wilches",
        "published": "2019-4-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32555/2019.dl.007"
    },
    {
        "id": 10721,
        "title": "Fine-tuning the Optoelectronic Properties of Borophene by Strain",
        "authors": "Lyudmyla Adamska, Sahar Sharifzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>\n\t\t\t<div>\n\t\t\t\t<div>\n\t\t\t\t\t<p>Here, we present an extensive first-\nprinciples study of the structural and optoelectronic properties of the two proposed structures of\nborophene under strain. With a density functional theory analysis, we determine that\nthe optical absorbance and electronic band structure are continuously tunable upon application of\nfew percent of strain. While both structures remain metallic with moderate strains of up to 6%, key features of the band structure, as well as the in-plane anisotropy of the complex\ndielectric function and optical absorption can be significantly modified. </p>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.5418727"
    },
    {
        "id": 10722,
        "title": "Fine-Tuning for Life and Design",
        "authors": "",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108765947.004"
    },
    {
        "id": 10723,
        "title": "Fine-tuning Siamese Networks to Assess Sport Gestures Quality",
        "authors": "Mégane Millan, Catherine Achard",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008924600570065"
    },
    {
        "id": 10724,
        "title": "Entanglement Fine-Tuning in Composite Quantum Systems",
        "authors": "Albert  A.I. Huber, Paul Schreivogl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4484571"
    },
    {
        "id": 10725,
        "title": "ChemBERTa-2: Fine-Tuning for Molecule’s HIV Replication Inhibition Prediction",
        "authors": "Sylwia Nowakowska",
        "published": "No Date",
        "citations": 0,
        "abstract": "Two versions of Large Language ChemBERTa-2 models, pre-trained with two different methods, were fine-tuned in this work for HIV replication inhibition prediction. The best model achieved AUROC of 0.793. The changes in distributions of molecular embeddings prior to and following fine-tuning reveal models’ enhanced ability to differentiate between active and inactive HIV molecules.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-b57vx"
    },
    {
        "id": 10726,
        "title": "Cosmic Inflation: Trick or Treat?",
        "authors": "",
        "published": "2020-9-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.004"
    },
    {
        "id": 10727,
        "title": "Light Waves",
        "authors": "David K. Reynolds",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094"
    },
    {
        "id": 10728,
        "title": "Fine Tuning MobileNet Neural Networks for Oil Spill Detection",
        "authors": "Caixia Wang, Andrew Coulson",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5703/1288284317671"
    },
    {
        "id": 10729,
        "title": "Fine-tuning the Optoelectronic Properties of Borophene by Strain",
        "authors": "Lyudmyla Adamska, Sahar Sharifzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here, we present an extensive first- principles study of the structural and optoelectronic properties of the two proposed structures of borophene under strain. With a density functional theory analysis, we determine that the optical absorbance and electronic band structure are continuously tunable upon application of few percent of strain. While both structures remain metallic with moderate strains of up to 6%, key features of the band structure, as well as the in-plane anisotropy of the complex dielectric function and optical absorption can be significantly modified.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.5418727.v1"
    },
    {
        "id": 10730,
        "title": "Peer Review #3 of Fine-tuning FAM161A gene augmentation therapy to restore retinal function",
        "authors": "",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15252/rc.2024091403"
    },
    {
        "id": 10731,
        "title": "Fine-tuning FDI screening can propel Singapore and ASEAN forward",
        "authors": "Faizal Bin Yahya",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59425/eabc.1706281200"
    },
    {
        "id": 10732,
        "title": "A mother’s touch: a key player in fine tuning the function of our genome",
        "authors": "",
        "published": "2019-7-11",
        "citations": 0,
        "abstract": "There is debate as to the importance of genetics in determining our behaviour. This debate has become enshrined perhaps due to the early focus of genetics on searching for DNA variation in our genome (termed a polymorphism) that affected protein structure, the hypothesis being that such a protein variant would not be working optimally in our body throughout our life.",
        "link": "http://dx.doi.org/10.13056/acamh.10640"
    },
    {
        "id": 10733,
        "title": "Fine Tuning",
        "authors": "Francis M. Bator",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1057/978-1-349-95189-5_11"
    },
    {
        "id": 10734,
        "title": "Peer Review #2 of Fine-tuning FAM161A gene augmentation therapy to restore retinal function",
        "authors": "",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15252/rc.2024454206"
    },
    {
        "id": 10735,
        "title": "A Two Step Fine-tuning Approach for Text Recognition on Identity Documents",
        "authors": "Francesco Visalli, Antonio Patrizio, Massimo Ruffolo",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010252208370844"
    },
    {
        "id": 10736,
        "title": "A Third Way to Explain Fine Tuning",
        "authors": "Francesco Riva",
        "published": "2021-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1103/physics.14.157"
    },
    {
        "id": 10737,
        "title": "Optimal Bayesian Price Fine-Tuning",
        "authors": "Jue Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3328801"
    },
    {
        "id": 10738,
        "title": "Author response: Controlling protein function by fine-tuning conformational flexibility",
        "authors": "Sonja Schmid, Thorsten Hugel",
        "published": "2020-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.57180.sa2"
    },
    {
        "id": 10739,
        "title": "Fine-tuning your brand",
        "authors": "Lisa Pritchard",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003103882-3"
    },
    {
        "id": 10740,
        "title": "Less Invasive Surfactant Administration: Fine Tuning a Disparate Practice",
        "authors": "Steven Conlon, Tara Glenn",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.168120004.45006612/v1"
    },
    {
        "id": 10741,
        "title": "How Special Is the Solar System?",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.011"
    },
    {
        "id": 10742,
        "title": "Peer Review #1 of Fine-tuning FAM161A gene augmentation therapy to restore retinal function",
        "authors": "",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15252/rc.2024395122"
    },
    {
        "id": 10743,
        "title": "On the Temporal Habitability of Our Universe",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108614023.012"
    },
    {
        "id": 10744,
        "title": "Fine-Tuning Feel",
        "authors": "Carlin Wing",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5040/9781501375378.ch-002"
    },
    {
        "id": 10745,
        "title": "Contents",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094-toc"
    },
    {
        "id": 10746,
        "title": "Review for \"Fine‐tuning biodiversity assessments: A framework to pair eDNA metabarcoding and morphological approaches\"",
        "authors": "",
        "published": "2021-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13718/v2/review1"
    },
    {
        "id": 10747,
        "title": "Unsupervised Fine-tuning of Optical Flow for Better Motion Boundary Estimation",
        "authors": "Taha Alhersh, Heiner Stuckenschmidt",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007343707760783"
    },
    {
        "id": 10748,
        "title": "Fine-Tuning Your Fresh Start",
        "authors": "Angela Duckworth,  ",
        "published": "2019-1",
        "citations": 0,
        "abstract": "What's the best time of year to set goals for self-improvement?\n\nMy answer: Right now. \n\nJanuary is a fresh start: an opportunity to leave our imperfect 2018 selves behind and begin anew with the hope that our 2019 selves will be better.\n\nHence the time-honored tradition of New Year's resolutions, which most American adults report making.\n\nFor students, January is also often the beginning of a new marking period. It is at this equinox—midway between the first day of school and the last—when students have enough feedback to know what they need to improve and enough time to feel like effort might change their final grades.\n\nThe science of fresh starts is intuitive: temporal landmarks like the New Year or a new semester spur us to set goals for improving our performance, no matter what it is we want to improve.\n\nWhat may not be obvious is how fickle hope can be.",
        "link": "http://dx.doi.org/10.53776/tips-fine-tuning-your-fresh-start"
    },
    {
        "id": 10749,
        "title": "Brain Tumor Classification with Selective Fine Tuning Using Transfer Learning",
        "authors": "Deepa AB, Varghese Paul",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4690086"
    },
    {
        "id": 10750,
        "title": "Review for \"Fine‐tuning biodiversity assessments: A framework to pair eDNA metabarcoding and morphological approaches\"",
        "authors": "",
        "published": "2021-2-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13718/v1/review1"
    },
    {
        "id": 10751,
        "title": "Fine-Tuning Human for LLM Projects",
        "authors": "Rehan Guha",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4574477"
    },
    {
        "id": 10752,
        "title": "The Standard Fine-Tuning Argument for the Multiverse",
        "authors": "",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108765947.005"
    },
    {
        "id": 10753,
        "title": "A New Fine-Tuning Argument for the Multiverse",
        "authors": "",
        "published": "2021-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108765947.007"
    },
    {
        "id": 10754,
        "title": "Deep Learning of COVID-19 Chest X-Rays: New Models or Fine Tuning?",
        "authors": "Tuan Pham",
        "published": "No Date",
        "citations": 1,
        "abstract": "Chest X-rays have been found to be very promising for assessing COVID-19 patients, especially for resolving emergency-department and urgent-care-center overcapacity.  Deep-learning (DL) methods in artificial intelligence (AI) play a dominant role as high-performance classifiers in the detection of the disease using chest X-rays. While many new DL models have been being developed for this purpose, this study aimed to investigate the fine tuning of pretrained convolutional neural networks (CNNs) for the classification of COVID-19 using chest X-rays. Three pretrained CNNs, which are AlexNet, GoogleNet, and SqueezeNet, were selected and fine-tuned without data augmentation to carry out 2-class and 3-class classification tasks using 3 public chest X-ray databases. In comparison with other recently developed DL models, the 3 pretrained CNNs achieved very high classification results in terms of accuracy, sensitivity, specificity, precision, F1 score, and area under the receiver-operating-characteristic curve.  AlexNet, GoogleNet, and SqueezeNet require the least training time among pretrained DL models, but with suitable selection of training parameters, excellent classification results can be achieved without data augmentation by these networks. The findings contribute to the urgent need for harnessing the pandemic by facilitating the deployment of AI tools that are fully automated and readily available in the public domain for rapid implementation.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12656948"
    },
    {
        "id": 10755,
        "title": "Acknowledgments",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094-002"
    },
    {
        "id": 10756,
        "title": "Unsupervised Fine-tuning of Optical Flow for Better Motion Boundary Estimation",
        "authors": "Taha Alhersh, Heiner Stuckenschmidt",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007343700002108"
    },
    {
        "id": 10757,
        "title": "How Fine Tuning Affects Contextual Embeddings: A Negative Result Explanation",
        "authors": "Ha-Thanh Nguyen, Vu Tran, Minh-Phuong Nguyen, Le-Minh Nguyen, Ken Satoh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011714200003393"
    },
    {
        "id": 10758,
        "title": "Fine tuning plots",
        "authors": "Ewen Harrison, Pius Riinu",
        "published": "2020-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367855420-5"
    },
    {
        "id": 10759,
        "title": "Fine-tuning Nanog expression heterogeneity by altering MicroRNA regulation",
        "authors": "Tagari Samanta, Sandip Kar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractNanog exhibits a robust heterogeneous expression pattern within a population of embryonic stem cells (ESCs) under varied culture conditions. However, an efficient way to fine-tune the Nanog expression heterogeneity remains elusive. Herein, by employing a stochastic modeling approach, we demonstrate that Nanog expression heterogeneity can be controlled by modulating the regulatory features of a Nanog transcript specific microRNA. We demonstrate how and why the extent of origin dependent fluctuations in Nanog expression level can be altered by varying either the binding efficiency of the microRNA-mRNA complex or the expression level of the respective microRNA. Moreover, our model makes experimentally feasible and insightful predictions to maneuver the Nanog expression heterogeneity explicitly to achieve cell-type specific differentiation of ESCs.",
        "link": "http://dx.doi.org/10.1101/2020.08.28.273177"
    },
    {
        "id": 10760,
        "title": "Preface",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094-001"
    },
    {
        "id": 10761,
        "title": "Review for \"Fine‐tuning biodiversity assessments: A framework to pair eDNA metabarcoding and morphological approaches\"",
        "authors": "",
        "published": "2021-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13718/v1/review2"
    },
    {
        "id": 10762,
        "title": "Deep Learning of COVID-19 Chest X-Rays: New Models or Fine Tuning?",
        "authors": "Tuan Pham",
        "published": "No Date",
        "citations": 1,
        "abstract": "Chest X-rays have been found to be very promising for assessing COVID-19 patients, especially for resolving emergency-department and urgent-care-center overcapacity.  Deep-learning (DL) methods in artificial intelligence (AI) play a dominant role as high-performance classifiers in the detection of the disease using chest X-rays. While many new DL models have been being developed for this purpose, this study aimed to investigate the fine tuning of pretrained convolutional neural networks (CNNs) for the classification of COVID-19 using chest X-rays. Three pretrained CNNs, which are AlexNet, GoogleNet, and SqueezeNet, were selected and fine-tuned without data augmentation to carry out 2-class and 3-class classification tasks using 3 public chest X-ray databases. In comparison with other recently developed DL models, the 3 pretrained CNNs achieved very high classification results in terms of accuracy, sensitivity, specificity, precision, F1 score, and area under the receiver-operating-characteristic curve.  AlexNet, GoogleNet, and SqueezeNet require the least training time among pretrained DL models, but with suitable selection of training parameters, excellent classification results can be achieved without data augmentation by these networks. The findings contribute to the urgent need for harnessing the pandemic by facilitating the deployment of AI tools that are fully automated and readily available in the public domain for rapid implementation.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12656948.v1"
    },
    {
        "id": 10763,
        "title": "A Moral Fine-Tuning Argument",
        "authors": "Martin Jakobsen",
        "published": "2023-12-24",
        "citations": 0,
        "abstract": "This paper develops Mark D. Linville’s brief description of “a sort of moral fine-tuning argument”. I develop the argument in four ways: I unpack the argument and give it a clear formulation, I unpack the theistic explanation of why a somewhat reliable moral capacity is expected, I point to the significance of not seeking to explain a perfect moral capacity, and I put the argument up against the recent work on non-theistic moral epistemology by Derek Parfit, David Enoch, and Erik Wielenberg.",
        "link": "http://dx.doi.org/10.3390/rel15010031"
    },
    {
        "id": 10764,
        "title": "Frontmatter",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094-fm"
    },
    {
        "id": 10765,
        "title": "fine tuning",
        "authors": "",
        "published": "2020-4-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9783527809080.cataz06916"
    },
    {
        "id": 10766,
        "title": "GenGradAttack: Efficient and Robust Targeted Adversarial Attacks Using Genetic Algorithms and Gradient-Based Fine-Tuning",
        "authors": "Naman Agarwal, James Pope",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012314700003636"
    },
    {
        "id": 10767,
        "title": "21. Storytelling",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780824845094-023"
    },
    {
        "id": 10768,
        "title": "Fine-Tuning Nhc Ligands for Ruthenium-Catalyzed Hydrogenation of Carboxylic Esters to Alcohols",
        "authors": "Renat Kadyrov",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4460110"
    },
    {
        "id": 10769,
        "title": "Review for \"Fine-tuning GPT-3 for machine learning electronic and functional properties of organic molecules\"",
        "authors": "",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1039/d3sc04610a/v2/review2"
    },
    {
        "id": 10770,
        "title": "SynFine: Boosting Image Segmentation Accuracy Through Synthetic Data Generation and Surgical Fine-Tuning",
        "authors": "Mehdi Mounsif, Yassine Motie, Mohamed Benabdelkrim, Florent Brondolo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011848300003411"
    }
]