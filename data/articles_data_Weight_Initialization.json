[
    {
        "id": 5001,
        "title": "Investigation of weight initialization using Fibonacci Sequence on the performance of neural networks*",
        "authors": "Dipanwita Sinha Mukherjee, Naveen Yeri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Initializing weights are important for fast convergence and performance improvement of Artificial Neural Network models. This study proposes a heuristic method to initialize weights for Neural Network with Fibonacci sequence. Experiments have been carried out with different network structures and datasets and results have been compared with other initialization techniques such as Zero, Random, Xavier and He. It has been observed that for small sized datasets, Fibonacci initialization technique reports 94% of test accuracy which is better than Random (85%) and close to Xavier (93%) and He (96%) initialization methods. Also, for medium sized dataset, we have noted that performance of Fibonacci weight initialization method is comparable with the same for Random, Xavier and He initialization techniques.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14546022.v1"
    },
    {
        "id": 5002,
        "title": "Investigation of weight initialization using Fibonacci Sequence on the performance of neural networks*",
        "authors": "Dipanwita Sinha Mukherjee, Naveen Yeri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Initializing weights are important for fast convergence and performance improvement of Artificial Neural Network models. This study proposes a heuristic method to initialize weights for Neural Network with Fibonacci sequence. Experiments have been carried out with different network structures and datasets and results have been compared with other initialization techniques such as Zero, Random, Xavier and He. It has been observed that for small sized datasets, Fibonacci initialization technique reports 94% of test accuracy which is better than Random (85%) and close to Xavier (93%) and He (96%) initialization methods. Also, for medium sized dataset, we have noted that performance of Fibonacci weight initialization method is comparable with the same for Random, Xavier and He initialization techniques.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14546022"
    },
    {
        "id": 5003,
        "title": "Modified weight initialization in the self-organizing map using Nguyen-Widrow initialization algorithm",
        "authors": "M N Linan, B Gerardo, R Medina",
        "published": "2019-6-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1742-6596/1235/1/012055"
    },
    {
        "id": 5004,
        "title": "Do different weight initialization strategies have an impact on transfer learning for plant disease detection?",
        "authors": "DUYGU SINANC TERZI",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe concept of weight initialization technique for transfer learning refers to the practice of using pre-trained models that can be modified to solve new problems, instead of starting the training process from scratch. By using pre-trained models as a starting point, the network can learn from patterns and features present in the original data, improving overall accuracy and allowing for faster convergence during training. In this study, four different transfer learning weight initialization strategies are proposed for plant disease detection: random initialization, pre-trained model on different domain (ImageNet), model trained on related domain (ISIC 2019), and model trained on same domain (PlantVillage). Weights from each strategy are transferred to a target dataset, Plant Pathology 2021. These strategies were implemented using four state-of-the-art CNN-based architectures: AlexNet, DenseNet, MobileNetV2, and VGG. The best result was obtained when both the target and source datasets included images of plant diseases. In this case, VGG was used and resulted in an 85.9% weighted f-score, which is a 9% improvement from random initialization. The transfer of knowledge from small-sized, related domain data (skin cancer data) was almost as successful as the transfer from ImageNet. Transferring from ImageNet yielded an f-score of 85.7%, while transferring from skin cancer data resulted in an f-score of 85.2%. This indicates that ImageNet, which is widely favored in the literature, may not necessarily represent the most optimal transfer source for the given context. Finally, the classifications made by the proposed models were visualized using Grad-CAM to better understand the decision-making process.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3249850/v1"
    },
    {
        "id": 5005,
        "title": "Comparison of Random Weight Initialization to New Weight Initialization CONEXP",
        "authors": "Apeksha Mittal, Amit Prakash Singh, Pravin Chandra",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5830-6_24"
    },
    {
        "id": 5006,
        "title": "Deep ConvNet: Non-Random Weight Initialization for Repeatable Determinism, examined with FSGM",
        "authors": "Richard Niall Mark Rudd-Orthner, Lyudmila Mihaylova",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper presents a non-random weight initialization method in convolutional layers of neural networks examined with the Fast Gradient Sign Method (FSGM) attack. This paper's focus is convolutional layers, and are the layers that have been responsible for better than human performance in image categorization. The proposed method induces earlier learning through the use of striped forms, and as such has less unlearning of the existing random number speckled methods, consistent with the intuitions of Hubel and Wiesel. The proposed method provides a higher performing accuracy in a single epoch, with improvements of between 3-5% in a well known benchmark model, of which the first epoch is the most relevant as it is the epoch after initialization. The proposed method is also repeatable and deterministic, as a desirable quality for safety critical applications in image classification within sensors. That method is robust to Glorot/Xavier and He initialization limits as well. The proposed non-random initialization was examined under adversarial perturbation attack through the FGSM approach with transferred learning, as a technique to measure the affect in transferred learning with controlled distortions, and finds that the proposed method is less compromised to the original validation dataset, with higher distorted datasets.",
        "link": "http://dx.doi.org/10.20944/preprints202105.0780.v1"
    },
    {
        "id": 5007,
        "title": "Improving Neural Language Models with Weight Norm Initialization and Regularization",
        "authors": "Christian Herold, Yingbo Gao, Hermann Ney",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6310"
    },
    {
        "id": 5008,
        "title": "Analysis on the Weight initialization Problem in Fully-connected Multi-layer Perceptron Neural Network",
        "authors": "Li Wanchen",
        "published": "2020-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaice51518.2020.00035"
    },
    {
        "id": 5009,
        "title": "Audio surveillance of roads using deep learning and autoencoder-based sample weight initialization",
        "authors": "Zied Mnasri, Stefano Rovetta, Francesco Masulli",
        "published": "2020-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/melecon48756.2020.9140594"
    },
    {
        "id": 5010,
        "title": "Investigation of Weight Initialization Using Fibonacci Sequence on the Performance of Neural Networks",
        "authors": "Dipanwita Sinha Mukherjee, Naveen Gururaja Yeri",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/punecon52575.2021.9686532"
    },
    {
        "id": 5011,
        "title": "Deep ConvNet: Non-Random Weight Initialization for Repeatable Determinism, Examined with FSGM",
        "authors": "Richard N. M. Rudd-Orthner, Lyudmila Mihaylova",
        "published": "2021-7-13",
        "citations": 0,
        "abstract": "A repeatable and deterministic non-random weight initialization method in convolutional layers of neural networks examined with the Fast Gradient Sign Method (FSGM). Using the FSGM approach as a technique to measure the initialization effect with controlled distortions in transferred learning, varying the dataset numerical similarity. The focus is on convolutional layers with induced earlier learning through the use of striped forms for image classification. Which provided a higher performing accuracy in the first epoch, with improvements of between 3–5% in a well known benchmark model, and also ~10% in a color image dataset (MTARSI2), using a dissimilar model architecture. The proposed method is robust to limit optimization approaches like Glorot/Xavier and He initialization. Arguably the approach is within a new category of weight initialization methods, as a number sequence substitution of random numbers, without a tether to the dataset. When examined under the FGSM approach with transferred learning, the proposed method when used with higher distortions (numerically dissimilar datasets), is less compromised against the original cross-validation dataset, at ~31% accuracy instead of ~9%. This is an indication of higher retention of the original fitting in transferred learning.",
        "link": "http://dx.doi.org/10.3390/s21144772"
    },
    {
        "id": 5012,
        "title": "Quantitative measures to evaluate neural network weight initialization strategies",
        "authors": "Ernesto Zamora Ramos, Masanori Nakakuni, Evangelos Yfantis",
        "published": "2017-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccwc.2017.7868389"
    },
    {
        "id": 5013,
        "title": "Weight initialization algorithm for physics-informed neural networks using finite differences",
        "authors": "Homayoon Tarbiyati, Behzad Nemati Saray",
        "published": "2023-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00366-023-01883-y"
    },
    {
        "id": 5014,
        "title": "Suitable CNN Weight Initialization and Activation Function for Javanese Vowels Classification",
        "authors": "Chandra Kusuma Dewa,  Afiahayati",
        "published": "2018",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2018.10.512"
    },
    {
        "id": 5015,
        "title": "Weight Initialization on Neural Network for Neuro PID Controller -Case study-",
        "authors": "Theertham Akilesh Sai, Hee-hyol Lee",
        "published": "2018-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ict-robot.2018.8549904"
    },
    {
        "id": 5016,
        "title": "FFANN Weight Initialization: A New Method",
        "authors": "Rijul Singh Malik, Ashish Payal, Pravin Chandra",
        "published": "2020-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icrito48877.2020.9197988"
    },
    {
        "id": 5017,
        "title": "Weight and Bias Initialization of ANN for Load Forecasting using Cuckoo Search Algorithm",
        "authors": "Vedanshu Kumar, Madan Mohan Tripathi",
        "published": "2019-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icghit.2019.00021"
    },
    {
        "id": 5018,
        "title": "Research and Application of Deep Belief Network Based on Local Binary Pattern and Improved Weight Initialization",
        "authors": "Longyang Wang, Junfei Qiao",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isass.2019.8757780"
    },
    {
        "id": 5019,
        "title": "Performance of neural network for indoor airflow prediction: Sensitivity towards weight initialization",
        "authors": "Qi Zhou, Ryozo Ooka",
        "published": "2021-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2021.111106"
    },
    {
        "id": 5020,
        "title": "A method for the detection of electricity theft behavior based on Xavier weight initialization",
        "authors": "Jiaqi Liang",
        "published": "2023-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2685710"
    },
    {
        "id": 5021,
        "title": "A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers",
        "authors": "Ingeborg de Pater, Mihaela Mitici",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.035"
    },
    {
        "id": 5022,
        "title": "AutoInit: Analytic Signal-Preserving Weight Initialization for Neural Networks",
        "authors": "Garrett Bingham, Risto Miikkulainen",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Neural networks require careful weight initialization to prevent signals from exploding or vanishing.  Existing initialization schemes solve this problem in specific cases by assuming that the network has a certain activation function or topology.  It is difficult to derive such weight initialization strategies, and modern architectures therefore often use these same initialization schemes even though their assumptions do not hold. This paper introduces AutoInit, a weight initialization algorithm that automatically adapts to different neural network architectures.  By analytically tracking the mean and variance of signals as they propagate through the network, AutoInit appropriately scales the weights at each layer to avoid exploding or vanishing signals.  Experiments demonstrate that AutoInit improves performance of convolutional, residual, and transformer networks across a range of activation function, dropout, weight decay, learning rate, and normalizer settings, and does so more reliably than data-dependent initialization methods.  This flexibility allows AutoInit to initialize models for everything from small tabular tasks to large datasets such as ImageNet.  Such generality turns out particularly useful in neural architecture search and in activation function discovery.  In these settings, AutoInit initializes each candidate appropriately, making performance evaluations more accurate. AutoInit thus serves as an automatic configuration tool that makes design of new neural network architectures more robust. The AutoInit package provides a wrapper around TensorFlow models and is available at https://github.com/cognizant-ai-labs/autoinit.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25836"
    },
    {
        "id": 5023,
        "title": "Autoencoders as Weight Initialization of Deep Classification Networks Applied to Papillary Thyroid Carcinoma",
        "authors": "Mafalda Falcao Ferreira, Rui Camacho, Luis Filipe Teixeira",
        "published": "2018-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm.2018.8621356"
    },
    {
        "id": 5024,
        "title": "Weight and bias initialization routines for Sigmoidal Feedforward Network",
        "authors": "Apeksha Mittal, Amit Prakash Singh, Pravin Chandra",
        "published": "2021-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-020-01960-5"
    },
    {
        "id": 5025,
        "title": "Denoising Autoencoder and Weight Initialization of CNN Model for ERP Classification",
        "authors": "Madina Kudaibergenova, Adnan Yazici, Sung-Jun Lee, Min-Ho Lee",
        "published": "2022-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945157"
    },
    {
        "id": 5026,
        "title": "Accelerating the evolution of convolutional neural networks with node-level mutations and epigenetic weight initialization",
        "authors": "Travis Desell",
        "published": "2018-7-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205651.3205792"
    },
    {
        "id": 5027,
        "title": "Beware of the beginnings: intermediate and higher-level representations in deep neural networks are strongly affected by weight initialization",
        "authors": "Johannes Mehrer, Nikolaus Kriegeskorte, Tim C. Kietzmann",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1172-0"
    },
    {
        "id": 5028,
        "title": "A Weight Initialization Method Associated with Samples for Deep Feedforward Neural Network",
        "authors": "Yanli Yang, Yichuan He",
        "published": "2020-1-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3379247.3379253"
    },
    {
        "id": 5029,
        "title": "Opposition-based initialization and a modified pattern for Inertia Weight (IW) in PSO",
        "authors": "Mehr Umer Farooq, Akhlaque Ahmad, Abdul Hameed",
        "published": "2017-7",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inista.2017.8001139"
    },
    {
        "id": 5030,
        "title": "Improving convergence speed of the neural network model using Meta heuristic algorithms for weight initialization",
        "authors": "V Vishnu Priya, P. Natesan, K. Venu, E. Gothai",
        "published": "2021-1-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci50826.2021.9402415"
    },
    {
        "id": 5031,
        "title": "A weight initialization method for fuzzy neural network based on rule partition",
        "authors": "Xuefeng Wang, Wenjing Li, Junfei Qiao",
        "published": "2021-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc52312.2021.9601986"
    },
    {
        "id": 5032,
        "title": "UNDERSTANDING AND STUDY OF WEIGHT INITIALIZATION IN ARTIFICAL NEURAL NETWORKS WITH BACK PROPAGATION ALGORITHM",
        "authors": "Farhana Kausar,  Dr. Aishwarya P., Dr. Gopal Krishna Shyam",
        "published": "2021-3-19",
        "citations": 0,
        "abstract": "There are various important choices that need to be assumed when building and training a neural network. One has to determine which loss function to be used, how many layers to be include, what stride and kernel size to use for each layer, which optimization algorithm is best suited for the network and so on. Assuming all the above condition, it decided to initialize the neural network training by different weight initialization techniques. This process is carried out in affiliation or with respect to with random learning rate so that we can get better result. We have calculated the mean test error for newly proposed paradigm and traditional approach. The newly proposed paradigm Xavier Weight Initialization less error in comparison to the traditional approach of Uniform and Gaussian Weight initialization (Random Initialization).",
        "link": "http://dx.doi.org/10.17762/itii.v9i1.290"
    },
    {
        "id": 5033,
        "title": "A new weight initialization method for sigmoidal FFANN",
        "authors": "M.P.S. Bhatia,  Veenu, Pravin Chandra",
        "published": "2018-11-20",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/jifs-169803"
    },
    {
        "id": 5034,
        "title": "A review on weight initialization strategies for neural networks",
        "authors": "Meenal V. Narkhede, Prashant P. Bartakke, Mukul S. Sutaone",
        "published": "2022-1",
        "citations": 77,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10462-021-10033-z"
    },
    {
        "id": 5035,
        "title": "A weight initialization based on the linear product structure for neural networks",
        "authors": "Qipin Chen, Wenrui Hao, Juncai He",
        "published": "2022-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.amc.2021.126722"
    },
    {
        "id": 5036,
        "title": "Medical Image Classification Algorithm Based on Weight Initialization-Sliding Window Fusion Convolutional Neural Network",
        "authors": "Ankit Kumar, Pankaj Dadheech, S. R. Dogiwal, Sandeep Kumar, Rajani Kumari",
        "published": "2021-3-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003121152-9"
    },
    {
        "id": 5037,
        "title": "Neural network control of networked redundant manipulator system with weight initialization method",
        "authors": "Naijing Jiang, Jian Xu, Shu Zhang",
        "published": "2018-9",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2018.04.039"
    },
    {
        "id": 5038,
        "title": "Initialization protocol",
        "authors": "",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5194/gmd-2023-123-cc2"
    },
    {
        "id": 5039,
        "title": "A Weight Initialization Method for Compressed Video Action Recognition in Compressed Domain",
        "authors": "Rogeany Kanza, Chenyu Huang, Allah Rakhio Junejo, Zhuoming Li",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3573942.3574089"
    },
    {
        "id": 5040,
        "title": "How training of sigmoidal FFANN affected by weight initialization",
        "authors": " Veenu",
        "published": "2021-6-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003150664-5"
    },
    {
        "id": 5041,
        "title": "A weight initialization method based on neural network with asymmetric activation function",
        "authors": "Jingjing Liu, Yefeng Liu, Qichun Zhang",
        "published": "2022-4",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.01.088"
    },
    {
        "id": 5042,
        "title": "About Influence of Weight Initialization Algorithms on Accuracy of the Forecast with LSTM-net for Harmonic Signals",
        "authors": "Yulia Timoshenkova, Nikolai Safiullin, Sergey Porshnev",
        "published": "2021-5-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/usbereit51232.2021.9455014"
    },
    {
        "id": 5043,
        "title": "A Comparative Study of Weight Initialization Techniques for Convolutional Neural Networks in COVID-19 Classification from X-ray Images.",
        "authors": "Abdelrahman Ezzeldin Nagib, Mostafa Saeed, Shereen Fathy El-Feky, Ali Khater Mohamed",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217655"
    },
    {
        "id": 5044,
        "title": "Attribution-aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation",
        "authors": "Dipam Goswami, Rene Schuster, Joost van de Weijer, Didier Stricker",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00321"
    },
    {
        "id": 5045,
        "title": "Medical Image Classification Algorithm Based on Weight Initialization-Sliding Window Fusion Convolutional Neural Network",
        "authors": "Feng-Ping An",
        "published": "2019-10-20",
        "citations": 10,
        "abstract": "Due to the complexity of medical images, traditional medical image classification methods have been unable to meet actual application needs. In recent years, the rapid development of deep learning theory has provided a technical approach for solving medical image classification tasks. However, deep learning has the following problems in medical image classification. First, it is impossible to construct a deep learning model hierarchy for medical image properties; second, the network initialization weights of deep learning models are not well optimized. Therefore, this paper starts from the perspective of network optimization and improves the nonlinear modeling ability of the network through optimization methods. A new network weight initialization method is proposed, which alleviates the problem that existing deep learning model initialization is limited by the type of the nonlinear unit adopted and increases the potential of the neural network to handle different visual tasks. Moreover, through an in-depth study of the multicolumn convolutional neural network framework, this paper finds that the number of features and the convolution kernel size at different levels of the convolutional neural network are different. In contrast, the proposed method can construct different convolutional neural network models that adapt better to the characteristics of the medical images of interest and thus can better train the resulting heterogeneous multicolumn convolutional neural networks. Finally, using the adaptive sliding window fusion mechanism proposed in this paper, both methods jointly complete the classification task of medical images. Based on the above ideas, this paper proposes a medical classification algorithm based on a weight initialization/sliding window fusion for multilevel convolutional neural networks. The methods proposed in this study were applied to breast mass, brain tumor tissue, and medical image database classification experiments. The results show that the proposed method not only achieves a higher average accuracy than that of traditional machine learning and other deep learning methods but also is more stable and more robust.",
        "link": "http://dx.doi.org/10.1155/2019/9151670"
    },
    {
        "id": 5046,
        "title": "Gamma-ray blazar classification using machine learning with advanced weight initialization and self-supervised learning techniques",
        "authors": "Gopal Bhatta, Sarvesh Gharat, Abhimanyu Borthakur, Aman Kumar",
        "published": "2024-1-13",
        "citations": 0,
        "abstract": "ABSTRACT\nMachine learning has emerged as a powerful tool in the field of gamma-ray astrophysics. The algorithms can distinguish between different source types, such as blazars and pulsars, and help uncover new insights into the high-energy universe. The Large Area Telescope onboard the Fermi gamma-ray telescope has significantly advanced our understanding of the Universe. The instrument has detected a large number of gamma-ray-emitting sources, among which a significant number of objects have been identified as active galactic nuclei. The sample is primarily composed of blazars; however, more than one-third of these sources are either of an unknown class or lack a definite association with a low-energy counterpart. In this work, we employ multiple machine learning algorithms to classify the sources based on their other physical properties. In particular, we utilized smart initialization techniques and self-supervised learning for classifying blazars into BL Lacertae (BL Lac, also BLL) objects and flat-spectrum radio quasars (FSRQs). The core advantage of the algorithm is its simplicity, usage of minimum number of features and easy deployment due to lesser number of parameters without compromising on the performance along with increase in inference speed (at least seven times more than existing algorithms). As a result, the best-performing model is deployed on multiple platforms so that any user irrespective of their coding background can use the tool. The model predicts that out of the 1115 sources of uncertain type in the 4FGL-DR3 catalogue, 820 can be classified as BL Lacs and 295 can be classified as FSRQs.",
        "link": "http://dx.doi.org/10.1093/mnras/stae028"
    },
    {
        "id": 5047,
        "title": "An Efficient DDoS Attack Detection Using Chaos Henry Gas Solubility Optimization Weight Initialization Based Rectified Linear Unit",
        "authors": "Selvam Lakshmanan, Uma Maheswari Gnaniyan Ponnusamy, Senthilkumar Andi",
        "published": "2023-3-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/01969722.2023.2175140"
    },
    {
        "id": 5048,
        "title": "A Modification to the Nguyen–Widrow Weight Initialization Method",
        "authors": "Apeksha Mittal, Amit Prakash Singh, Pravin Chandra",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-6095-4_11"
    },
    {
        "id": 5049,
        "title": "An analysis of weight initialization methods in connection with different activation functions for feedforward neural networks",
        "authors": "Kit Wong, Rolf Dornberger, Thomas Hanne",
        "published": "2022-11-24",
        "citations": 2,
        "abstract": "AbstractThe selection of weight initialization in an artificial neural network is one of the key aspects and affects the learning speed, convergence rate and correctness of classification by an artificial neural network. In this paper, we investigate the effects of weight initialization in an artificial neural network. Nguyen-Widrow weight initialization, random initialization, and Xavier initialization method are paired with five different activation functions. This paper deals with a feedforward neural network, consisting of an input layer, a hidden layer, and an output layer. The paired combination of weight initialization methods with activation functions are examined and tested and compared based on their best achieved loss rate in training. This work aims to better understand how weight initialization methods in neural networks, in combination with activation functions, affect the learning speed in comparison after a fixed number of training epochs.",
        "link": "http://dx.doi.org/10.1007/s12065-022-00795-y"
    },
    {
        "id": 5050,
        "title": "Rainfall Prediction Using Fuzzy Neural Network with Genetically Enhanced Weight Initialization",
        "authors": "V. S. Felix Enigo",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-32150-5_102"
    },
    {
        "id": 5051,
        "title": "Using autoencoders as a weight initialization method on deep neural networks for disease detection",
        "authors": "Mafalda Falcão Ferreira, Rui Camacho, Luís F. Teixeira",
        "published": "2020-8",
        "citations": 17,
        "abstract": "Abstract\nBackground\nAs of today, cancer is still one of the most prevalent and high-mortality diseases, summing more than 9 million deaths in 2018. This has motivated researchers to study the application of machine learning-based solutions for cancer detection to accelerate its diagnosis and help its prevention. Among several approaches, one is to automatically classify tumor samples through their gene expression analysis.\n\nMethods\nIn this work, we aim to distinguish five different types of cancer through RNA-Seq datasets: thyroid, skin, stomach, breast, and lung. To do so, we have adopted a previously described methodology, with which we compare the performance of 3 different autoencoders (AEs) used as a deep neural network weight initialization technique. Our experiments consist in assessing two different approaches when training the classification model — fixing the weights after pre-training the AEs, or allowing fine-tuning of the entire network — and two different strategies for embedding the AEs into the classification network, namely by only importing the encoding layers, or by inserting the complete AE. We then study how varying the number of layers in the first strategy, the AEs latent vector dimension, and the imputation technique in the data preprocessing step impacts the network’s overall classification performance. Finally, with the goal of assessing how well does this pipeline generalize, we apply the same methodology to two additional datasets that include features extracted from images of malaria thin blood smears, and breast masses cell nuclei. We also discard the possibility of overfitting by using held-out test sets in the images datasets.\n\nResults\nThe methodology attained good overall results for both RNA-Seq and image extracted data. We outperformed the established baseline for all the considered datasets, achieving an average F1 score of 99.03, 89.95, and 98.84 and an MCC of 0.99, 0.84, and 0.98, for the RNA-Seq (when detecting thyroid cancer), the Malaria, and the Wisconsin Breast Cancer data, respectively.\n\nConclusions\nWe observed that the approach of fine-tuning the weights of the top layers imported from the AE reached higher results, for all the presented experiences, and all the considered datasets. We outperformed all the previous reported results when comparing to the established baselines.\n",
        "link": "http://dx.doi.org/10.1186/s12911-020-01150-w"
    },
    {
        "id": 5052,
        "title": "initialization, n.",
        "authors": "",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/oed/9193556022"
    },
    {
        "id": 5053,
        "title": "Incorrect Application of Yilmaz–Poli (2022) Initialisation Method in dePater–Mitici 2023 paper entitled “A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers”",
        "authors": "Riccardo Poli, Ahmet Yilmaz",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.09.017"
    },
    {
        "id": 5054,
        "title": "Physics-informed interpretable wavelet weight initialization and balanced dynamic adaptive threshold for intelligent fault diagnosis of rolling bearings",
        "authors": "Chao He, Hongmei Shi, Jin Si, Jianbo Li",
        "published": "2023-10",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jmsy.2023.08.014"
    },
    {
        "id": 5055,
        "title": "Where Should We Begin? A Low-Level Exploration of Weight Initialization Impact on Quantized Behaviour of Deep Neural Networks",
        "authors": "Stone Yun, Alexander Wong",
        "published": "2021-1-15",
        "citations": 0,
        "abstract": "With the proliferation of deep convolutional neural network (CNN) algorithms for mobile processing, limited precision quantization has become an essential tool for CNN efficiency. Consequently, various works have sought to design fixed precision quantization algorithms and quantization-focused optimization techniques that minimize quantization induced performance degradation. However, there is little concrete understanding of how various CNN design decisions/best practices affect quantized inference behaviour. Weight initialization strategies are often associated with solving issues such as vanishing/exploding gradients but an often-overlooked aspect is their impact on the final trained distributions of each layer. We present an in-depth, fine-grained ablation study of the effect of different weights initializations on the final distributions of weights and activations of different CNN architectures. The fine-grained, layerwise analysis enables us to gain deep insights on how initial weights distributions will affect final accuracy and quantized behaviour. To our best knowledge, we are the first to perform such a low-level, in-depth quantitative analysis of weights initialization and its effect on quantized behaviour.",
        "link": "http://dx.doi.org/10.15353/jcvis.v6i1.3538"
    },
    {
        "id": 5056,
        "title": "Weight Initialization based Partial Training Algorithm for Fast Learning in Neural Network",
        "authors": "Jung-Jae Kim, Min-woo Ryu, Si-Ho Cha, Kuk-Hyun Cho",
        "published": "2017-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14257/ijdta.2017.10.8.03"
    },
    {
        "id": 5057,
        "title": "Improved content-based brain tumor retrieval for magnetic resonance images using weight initialization framework with densely connected deep neural network",
        "authors": "Vibhav Prakash Singh, Aman Verma, Dushyant Kumar Singh, Ritesh Maurya",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00521-023-09149-w"
    },
    {
        "id": 5058,
        "title": "An approach combining a new weight initialization method and constructive algorithm to configure a single Feedforward Neural Network for multi-class classification",
        "authors": "Cristiano Hora Fontes, Marcelo Embiruçu",
        "published": "2021-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.engappai.2021.104495"
    },
    {
        "id": 5059,
        "title": "A novel weight initialization with adaptive hyper-parameters for deep semantic segmentation",
        "authors": "Nuhman Ul Haq, Ahmad Khan, Zia ur Rehman, Ahmad Din, Ling Shao, Sajid Shah",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-021-10510-1"
    },
    {
        "id": 5060,
        "title": "Effect of Weight Initialization on Training of Sigmoidal FFANN for Back Propagation Algorithms",
        "authors": " Veenu, M. P. S. Bhatia, Pravin Chandra",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-30577-2_40"
    },
    {
        "id": 5061,
        "title": "Deep Learning-Based Weight Initialization on Multi-layer Perceptron for Image Recognition",
        "authors": "Sourabrata Mukherjee, Prasenjit Dey",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-1472-2_17"
    },
    {
        "id": 5062,
        "title": "A new method for building single feedforward neural network models for multivariate static regression problems: a combined weight initialization and constructive algorithm",
        "authors": "Ghabriel A. Gomes de Sá, Cristiano Hora Fontes, Marcelo Embiruçu",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12065-022-00813-z"
    },
    {
        "id": 5063,
        "title": "Heuristic Weight Initialization for Diagnosing Heart Diseases Using Feature Ranking",
        "authors": "Musulmon Lolaev, Shraddha M. Naik, Anand Paul, Abdellah Chehri",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "The advent of Artificial Intelligence (AI) has had a broad impact on life to solve various tasks. Building AI models and integrating them with modern technologies is a central challenge for researchers. These technologies include wearables and implants in living beings, and their use is known as human augmentation, using technology to enhance human abilities. Combining human augmentation with artificial intelligence (AI), especially after the recent successes of the latter, is the most significant advancement in their applicability. In the first section, we briefly introduce these modern applications in health care and examples of their use cases. Then, we present a computationally efficient AI-driven method to diagnose heart failure events by leveraging actual heart failure data. The classifier model is designed without conventional models such as gradient descent. Instead, a heuristic is used to discover the optimal parameters of a linear model. An analysis of the proposed model shows that it achieves an accuracy of 84% and an F1 score of 0.72 with only one feature. With five features for diagnosis, the accuracy achieved is 83%, and the F1 score is 0.74. Moreover, the model is flexible, allowing experts to determine which variables are more important than others when implementing diagnostic systems.",
        "link": "http://dx.doi.org/10.3390/technologies11050138"
    },
    {
        "id": 5064,
        "title": "Variance-Aware Weight Initialization for Point Convolutional Neural Networks",
        "authors": "Pedro Hermosilla, Michael Schelling, Tobias Ritschel, Timo Ropinski",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-19815-1_5"
    },
    {
        "id": 5065,
        "title": "Decorelated Weight Initialization by Backpropagation",
        "authors": "Alexander Kovalenko, Pavel Kordík",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44207-0_46"
    },
    {
        "id": 5066,
        "title": "Behavior Analysis of a Deep Feedforward Neural Network by Varying the Weight Initialization Methods",
        "authors": "Geetika Srivastava, Shubham Vashisth, Ishika Dhall, Shipra Saraswat",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-5345-5_15"
    },
    {
        "id": 5067,
        "title": "Delving into Feature Maps: An Explanatory Analysis to Evaluate Weight Initialization",
        "authors": "Meenal Narkhede, Prashant P. Bartakke, Mukul S. Sutaone",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-73689-7_29"
    },
    {
        "id": 5068,
        "title": "Domain adaptation and weight initialization of neural networks for diagnosing interstitial lung diseases",
        "authors": "Onkar Thorat, Siddharth Salvi, Shrey Dedhia, Chetashri Bhadane, Deepika Dongre",
        "published": "2022-9",
        "citations": 2,
        "abstract": "AbstractInterstitial lung diseases (ILDs) are adverse disorders, damaging the lung tissues, thus making timely diagnosis imperative. To counter the scarcity of publicly available high‐resolution computed tomography (HRCT) data, architecture employing HRCT and x‐ray data have been proposed for diagnosing ILDs. A model is first trained to diagnose three ILDs and a healthy lung from x‐ray data and then with a small amount of HRCT data of the same four classes. We introduce an EfficientNet+AlexNet model and a custom deep convolutional neural network and compare their performances with different weight initialization at the start of x‐ray image training. The EfficientNet+AlexNet model initialized with ImageNet weights performed best on the x‐ray dataset and was later used for domain adaptation. We also compare our model's performance to a single pre‐trained EfficientNetB0 model (trained on the same HRCT data). Our model gave an accuracy of 97.4% on the testing dataset, a notable increase in the performance compared to the recently proposed methodologies. The experimental results and graphical plots depict the superiority of domain adaptation and the potentials of weights that are a starting point for training the models.",
        "link": "http://dx.doi.org/10.1002/ima.22716"
    },
    {
        "id": 5069,
        "title": "Hybrid Fuzzy Classification Algorithm with Modifed Initialization and Crossover",
        "authors": "Tatiana Pleshkova, Vladimir Stanovov",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011587500003332"
    },
    {
        "id": 5070,
        "title": "DPReLU: Dynamic Parametric Rectified Linear Unit and Its Proper Weight Initialization Method",
        "authors": "Donghun Yang, Kien Mai Ngoc, Iksoo Shin, Myunggwon Hwang",
        "published": "2023-2-11",
        "citations": 2,
        "abstract": "AbstractActivation functions are essential in deep learning, and the rectified linear unit (ReLU) is the most widely used activation function to solve the vanishing gradient problem. However, owing to the dying ReLU problem and bias shift effect, deep learning models using ReLU cannot exploit the potential benefits of negative values. Numerous ReLU variants have been proposed to address this issue. In this study, we propose Dynamic Parametric ReLU (DPReLU), which can dynamically control the overall functional shape of ReLU with four learnable parameters. The parameters of DPReLU are determined by training rather than by humans, thereby making the formulation more suitable and flexible for each model and dataset. Furthermore, we propose an appropriate and robust weight initialization method for DPReLU. To evaluate DPReLU and its weight initialization method, we performed two experiments on various image datasets: one using an autoencoder for image generation and the other using the ResNet50 for image classification. The results show that DPReLU and our weight initialization method provide faster convergence and better accuracy than the original ReLU and the previous ReLU variants.",
        "link": "http://dx.doi.org/10.1007/s44196-023-00186-w"
    },
    {
        "id": 5071,
        "title": "Review for \"Fluctuation-driven initialization for spiking neural network training\"",
        "authors": "",
        "published": "2022-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2634-4386/ac97bb/v2/review2"
    },
    {
        "id": 5072,
        "title": "Review for \"Fluctuation-driven initialization for spiking neural network training\"",
        "authors": "",
        "published": "2022-10-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2634-4386/ac97bb/v2/review1"
    },
    {
        "id": 5073,
        "title": "A KM-Net Model Based on k-Means Weight Initialization for Images Classification",
        "authors": "Miao Ma, Lin Liu, Yuli Chen",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hpcc/smartcity/dss.2018.00188"
    },
    {
        "id": 5074,
        "title": "Initialization Data",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-815563-9.00022-7"
    },
    {
        "id": 5075,
        "title": "Depth from Small Motion using Rank-1 Initialization",
        "authors": "Peter Fasogbon",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007381400002108"
    },
    {
        "id": 5076,
        "title": "K-modes and Entropy Cluster Centers Initialization Methods",
        "authors": "Doaa S. Ali, Ayman Ghoneim, Mohamed Saleh",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006245504470454"
    },
    {
        "id": 5077,
        "title": "Depth from Small Motion using Rank-1 Initialization",
        "authors": "Peter Fasogbon",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007381405210528"
    },
    {
        "id": 5078,
        "title": "Improving Weight Initialization of ReLU and Output Layers",
        "authors": "Diego Aguirre, Olac Fuentes",
        "published": "2019",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-30484-3_15"
    },
    {
        "id": 5079,
        "title": "K-modes and Entropy Cluster Centers Initialization Methods",
        "authors": "Doaa S. Ali, Ayman Ghoneim, Mohamed Saleh",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006245500001482"
    },
    {
        "id": 5080,
        "title": "Towards Fast and Automatic Map Initialization for Monocular SLAM Systems",
        "authors": "Blake Troutman, Mihran Tuceryan",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010640600003061"
    },
    {
        "id": 5081,
        "title": "Cluster Initialization",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100058"
    },
    {
        "id": 5082,
        "title": "Cluster-Based Input Weight Initialization for Echo State Networks",
        "authors": "Peter Steiner, Azarakhsh Jalalvand, Peter Birkholz",
        "published": "2023-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3145565"
    },
    {
        "id": 5083,
        "title": "More Accurate Pose Initialization with Redundant Measurements",
        "authors": "Ksenia Klionovska, Heike Benninghoff, Felix Huber",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007378001460153"
    },
    {
        "id": 5084,
        "title": "More Accurate Pose Initialization with Redundant Measurements",
        "authors": "Ksenia Klionovska, Heike Benninghoff, Felix Huber",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007378000002108"
    },
    {
        "id": 5085,
        "title": "Template_ini_files_BBGC5",
        "authors": "Teagan Baiotto",
        "published": "2021-7-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4211/hs.b76183ceb87d43a3aac5e63c03f6f234"
    },
    {
        "id": 5086,
        "title": "Review for \"Fluctuation-driven initialization for spiking neural network training\"",
        "authors": " Bojian Yin",
        "published": "2022-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2634-4386/ac97bb/v1/review1"
    },
    {
        "id": 5087,
        "title": "Review for \"Fluctuation-driven initialization for spiking neural network training\"",
        "authors": "Cecilia Jarne",
        "published": "2022-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2634-4386/ac97bb/v1/review2"
    },
    {
        "id": 5088,
        "title": "On Robustness of Deep Neural Networks: A Comprehensive Study on the Effect of Architecture and Weight Initialization to Susceptibility and Transferability of Adversarial Attacks",
        "authors": "Ibrahim Ben Daya, Mohammad Javad Shaifee, Michelle Karg, Christian Scharfenderger, Alexander Wong",
        "published": "2018-12-24",
        "citations": 1,
        "abstract": "Neural network models have shown state of the art performance inseveral applications. However it has been observed that they aresusceptible to adversarial attacks: small perturbations to the inputthat fool a network model into mislabelling the input data. Theseattacks can also transfer from one network model to another, whichraises concerns over their applicability, particularly when there areprivacy and security risks involved. In this work, we conduct a studyto analyze the effect of network architectures and weight initial-ization on the robustness of individual network models as well astransferability of adversarial attacks. Experimental results demon-strate that while weight initialization has no affect on the robustnessof a network model, it does have an affect on attack transferabilityto a network model. Results also show that the complexity of anetwork model as indicated by the total number of parameters andMAC number is not indicative of a network’s robustness to attackor transferability, but accuracy can be; within the same architec-ture, higher accuracy usually indicates a more robust network, butacross architectures there is no strong link between accuracy androbustness.",
        "link": "http://dx.doi.org/10.15353/jcvis.v4i1.329"
    },
    {
        "id": 5089,
        "title": "Toward Efficient Image Recognition in Sensor-Based IoT: A Weight Initialization Optimizing Method for CNN Based on RGB Influence Proportion",
        "authors": "Zile Deng, Yuanlong Cao, Xinyu Zhou, Yugen Yi, Yirui Jiang, Ilsun You",
        "published": "2020-5-18",
        "citations": 8,
        "abstract": "As the Internet of Things (IoT) is predicted to deal with different problems based on big data, its applications have become increasingly dependent on visual data and deep learning technology, and it is a big challenge to find a suitable method for IoT systems to analyze image data. Traditional deep learning methods have never explicitly taken the color differences of data into account, but from the experience of human vision, colors play differently significant roles in recognizing things. This paper proposes a weight initialization method for deep learning in image recognition problems based on RGB influence proportion, aiming to improve the training process of the learning algorithms. In this paper, we try to extract the RGB proportion and utilize it in the weight initialization process. We conduct several experiments on different datasets to evaluate the effectiveness of our proposal, and it is proven to be effective on small datasets. In addition, as for the access to the RGB influence proportion, we also provide an expedient approach to get the early proportion for the following usage. We assume that the proposed method can be used for IoT sensors to securely analyze complex data in the future.",
        "link": "http://dx.doi.org/10.3390/s20102866"
    },
    {
        "id": 5090,
        "title": "A New Initialization Method for Neural Networks with Weight Sharing",
        "authors": "Xiaofeng Ding, Hongfei Yang, Raymond H. Chan, Hui Hu, Yaxin Peng, Tieyong Zeng",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-2701-9_9"
    },
    {
        "id": 5091,
        "title": "Reinforcement Learning Controller Design for Discrete-Time-Constrained Nonlinear Systems With Weight Initialization Method",
        "authors": "Jiahui Xu, Jingcheng Wang, Yanjiu Zhong, Jun Rao, Shunyu Wu",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsmc.2023.3344883"
    },
    {
        "id": 5092,
        "title": "Autoencoder Extreme Learning Machine for Fingerprint-Based Positioning: A Good Weight Initialization is Decisive",
        "authors": "Darwin P. Quezada Gaibor, Lucie Klus, Roman Klus, Elena Simona Lohan, Jari Nurmi, Mikko Valkama, Joaquín Huerta, Joaquín Torres-Sospedra",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jispin.2023.3299433"
    },
    {
        "id": 5093,
        "title": "Initialization of Recursive Mixture-based Clustering with Uniform Components",
        "authors": "Evgenia Suzdaleva, Ivan Nagy, Pavla Pecherková, Raissa Likhonina",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006417104490458"
    },
    {
        "id": 5094,
        "title": "AN EFFICIENT PRIMARY POPULATION INITIALIZATION METHOD FOR METAHEURISTIC ALGORITHMS",
        "authors": "A.V. VARDUMYAN",
        "published": "2022",
        "citations": 0,
        "abstract": "It is widely recognized that convergence capabilities of metaheuristic optimization\nalgorithms can be enhanced by properly chosen initial population. Most of the state-of-theart initialization techniques are suffering from numerous shortcomings, that ultimately\nmake them non-viable or not efficient enough. To overcome this, this paper proposes a new algorithm for population initialization, which adopts the approach of dividing the search space into nested cubes and picking edge-points for sampling. Based on the data obtained after testing the method on 8 complex benchmark functions against other popular initialization strategies in the scope of WOA algorithm, the proposed approach outperformed all of the candidates in finding the global optimum.",
        "link": "http://dx.doi.org/10.53297/0002306x-2022.v75.3-431"
    },
    {
        "id": 5095,
        "title": "Weight Initialization Techniques for Deep Learning Algorithms in Remote Sensing: Recent Trends and Future Perspectives",
        "authors": "Wadii Boulila, Maha Driss, Eman Alshanqiti, Mohamed Al-Sarem, Faisal Saeed, Moez Krichen",
        "published": "2022",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-5559-3_39"
    },
    {
        "id": 5096,
        "title": "Comparison of K-means Clustering Initialization Approaches with Brute-Force Initialization",
        "authors": "Martin Golasowski, Jan Martinovič, Kateřina Slaninová",
        "published": "2017",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-3409-1_7"
    },
    {
        "id": 5097,
        "title": "System Initialization and Services",
        "authors": "Richard Fox",
        "published": "2021-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003203322-9"
    },
    {
        "id": 5098,
        "title": "Decision letter for \"Fluctuation-driven initialization for spiking neural network training\"",
        "authors": "",
        "published": "2022-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2634-4386/ac97bb/v2/decision1"
    },
    {
        "id": 5099,
        "title": "Rheological Bridge Zone: Initialization of Localization",
        "authors": "He Feng, Christopher Gerbi, Scott Johnson",
        "published": "No Date",
        "citations": 0,
        "abstract": "&lt;p&gt;Strain localization occurs throughout the crust, in both the brittle and viscous regimes. The causes of strain localization remain under discussion. However, realistic rock records indicate that variations of material properties (e.g. active deformation mechanisms, crystallographic orientation, phase distribution, grain shapes, etc.) are likely to be the dominant factor for weakening. Determining the cause(s) of localization requires investigation of the earliest stages of strain concentration in different P-T conditions. Our study focuses on two rocks that experienced low macroscale strain at amphibolite and/or granulite facies conditions yet exhibit localization on the millimeter and smaller scale. We combine optical and electron beam petrography with chemical mapping and electron backscatter diffraction to characterize these rheologically important domains. Morphologically, these localized zones appear to mechanically link rheologically weak phases or domains. These &amp;#8220;bridge&amp;#8221; zones typically comprise a band of relatively fine grains with weak crystallographic preferred orientation. The major element compositions of like phases inside and outside the bridge zone are similar, but the modal mineralogy and trace elements vary somewhat. Bridge zones result from not only in-situ grain size reduction (due to, for example, nucleation, recrystallization, or cataclasis), but also chemical processes resulting in phase mixing or element mobility on a short spatial scale. Their spatial distribution suggests that the small modal fraction of microstructural change represented by the bridge zones can lead to a high degree of bulk weakening.&lt;/p&gt;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu21-7917"
    },
    {
        "id": 5100,
        "title": "Entropic Alternatives to Initialization",
        "authors": "Daniele Musso",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4057054"
    }
]