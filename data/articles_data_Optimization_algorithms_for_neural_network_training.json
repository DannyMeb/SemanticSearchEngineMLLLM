[
    {
        "id": 21771,
        "title": "Comparison of Search Optimization Algorithms in Two-Stage Artificial Neural Network Training for Handwritten Digits Recognition",
        "authors": "Patrik Gilley, Yanjun Yan",
        "published": "2020-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon44009.2020.9249759"
    },
    {
        "id": 21772,
        "title": "On neural-network training algorithms",
        "authors": "Jonathan Barzilai",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-820543-3.00015-8"
    },
    {
        "id": 21773,
        "title": "Reactive power optimization for distribution network with distributed generators by improved driving training‑based optimization",
        "authors": "Songlin Du, Tao Hai, Jianfeng Lu, Jun Wang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3004751"
    },
    {
        "id": 21774,
        "title": "New optimization algorithms for neural network training using operator splitting techniques",
        "authors": "Cristian Daniel Alecsa, Titus Pinţa, Imre Boros",
        "published": "2020-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2020.03.018"
    },
    {
        "id": 21775,
        "title": "Attribute Optimization: Genetic Algorithms and Neural Network for Voice Analysis Classification of Parkinson's Disease",
        "authors": "Yudi Ramadhani, Ade Mubarok, Syarif Hidayatullah, Wildan Wiguna",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009947030743079"
    },
    {
        "id": 21776,
        "title": "Artificial Neural Network Training Algorithms in Modeling of Radial Overcut in EDM",
        "authors": "Raja Das, Mohan Kumar Pradhan",
        "published": "2022",
        "citations": 0,
        "abstract": "This chapter describes with the comparison of the most used back propagations training algorithms neural networks, mainly Levenberg-Marquardt, conjugate gradient and Resilient back propagation are discussed. In the present study, using radial overcut prediction as illustrations, comparisons are made based on the effectiveness and efficiency of three training algorithms on the networks. Electrical Discharge Machining (EDM), the most traditional non-traditional manufacturing procedures, is growing attraction, due to its not requiring cutting tools and permits machining of hard, brittle, thin and complex geometry. Hence it is very popular in the field of modern manufacturing industries such as aerospace, surgical components, nuclear industries. But, these industries surface finish has the almost importance. Based on the study and test results, although the Levenberg-Marquardt has been found to be faster and having improved performance than other algorithms in training, the Resilient back propagation algorithm has the best accuracy in testing period. ",
        "link": "http://dx.doi.org/10.4018/978-1-6684-2408-7.ch015"
    },
    {
        "id": 21777,
        "title": "A Comparison Study on Training Optimization Algorithms in the biLSTM Neural Network for Classification of PCG Signals",
        "authors": "Mahmoud Fakhry, Abeer FathAllah Brery",
        "published": "2022-3-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iraset52964.2022.9738309"
    },
    {
        "id": 21778,
        "title": "Comparative Analysis of Artificial Neural Network Training Algorithms",
        "authors": "Charos Khidirova",
        "published": "2020-11-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icisct50599.2020.9351395"
    },
    {
        "id": 21779,
        "title": "Neural Network Training Based Particle Swarm Optimization (PSO)",
        "authors": "Ardashir Mohammadzadeh, Mohammad Hosein Sabzalian, Oscar Castillo, Rathinasamy Sakthivel, Fayez F. M. El-Sousy, Saleh Mobayen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-14571-1_6"
    },
    {
        "id": 21780,
        "title": "NSGA-PINN: A Multi-Objective Optimization Method for Physics-Informed Neural Network Training",
        "authors": "Binghang Lu, Christian Moya, Guang Lin",
        "published": "2023-4-3",
        "citations": 3,
        "abstract": "This paper presents NSGA-PINN, a multi-objective optimization framework for the effective training of physics-informed neural networks (PINNs). The proposed framework uses the non-dominated sorting genetic algorithm (NSGA-II) to enable traditional stochastic gradient optimization algorithms (e.g., ADAM) to escape local minima effectively. Additionally, the NSGA-II algorithm enables satisfying the initial and boundary conditions encoded into the loss function during physics-informed training precisely. We demonstrate the effectiveness of our framework by applying NSGA-PINN to several ordinary and partial differential equation problems. In particular, we show that the proposed framework can handle challenging inverse problems with noisy data.",
        "link": "http://dx.doi.org/10.3390/a16040194"
    },
    {
        "id": 21781,
        "title": "COMPUTATIONAL MODELING OF ELECTRICITY CONSUMPTION USING ECONOMETRIC VARIABLES BASED ON NEURAL NETWORK TRAINING ALGORITHMS",
        "authors": "T. M. Usha, S. Appavu alias Balamurugan",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14311/nnw.2017.27.007"
    },
    {
        "id": 21782,
        "title": "Complexity of training ReLU neural network",
        "authors": "Digvijay Boob, Santanu S. Dey, Guanghui Lan",
        "published": "2022-5",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.disopt.2020.100620"
    },
    {
        "id": 21783,
        "title": "Optimization of Enhanced TIG Welding Process Using Artificial Neural Network and Heuristic Algorithms",
        "authors": "Masoud Azadi Moghaddam, Farhad Kolahan",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nUsing conventional gas tungsten arc welding (C-GTAW) process includes some demerits, shallow penetration has been considered as the most important ones. Recently, in order to cope with the mentioned disadvantage (low penetration), using a paste like coating of activating flux during welding process known as activated GTAW (AGTAW) has been proposed. In this paper, effect of A-GTAW process input adjusting parameters including welding speed (S), welding current (C) and percentage of activating fluxes (TiO2 and SiO2) combination (F) on weld bead width (WBW), depth of penetration (DOP), and consequently aspect ratio (ASR) (the most important quality characteristics) in welding of AISI316L parts have been studied. Box-behnken design (BBD) of experiments has been used to prepare the required experimental matrix for modeling and optimization objectives. Back propagation neural network (BPNN), architecture (hidden layers number and their corresponding neurons/nodes) of which has been determined using heuristic algorithm employed to model the process outputs, the most fitted ones have been optimized using simulated annealing (SA), and particle swarm optimization (PSO) algorithms in order to obtain the desired aspect ratio, maximum depth of penetration, and minimum weld bead width. Finally, confirmation experimental tests have been carried out to evaluate the performance of the proposed method. Due to the obtained results, the suggested method for modeling and optimization of A-GTAW process is quite efficient (with less than 4% error).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-680478/v1"
    },
    {
        "id": 21784,
        "title": "Modeling of Multilayer Perceptron Neural Network Hyperparameter Optimization and Training",
        "authors": "Taoufyq Elansari, Mohammed Ouanan, Hamid Bourray",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe multilayer perceptron (MLP) is an artificial neural network composed of one or more hidden layers. MLP has an extensive array of classification and regression applications in a wide range of domains. Selecting the right neural network architecture to solve a specific problem is one of the major areas of neural network research. The number of neurons in the hidden layers and the initial weights have a great effect on the convergence of MLP training algorithms. This paper presents a technique termed hyperparameter optimization model for the multilayer perceptron. We model in terms of mixed-variable optimization problems with nonlinear constraints. To solve the obtained model, a hybrid algorithm is used, based on the genetic algorithm and the backpropagation algorithm. The numerical results demonstrate the efficiency of the technique presented in this paper, and the advantages of the proposed model vis-a-vis the previous state-of-the-art model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2570112/v1"
    },
    {
        "id": 21785,
        "title": "FETA: Fairness Enforced Verifying, Training, and Predicting Algorithms for Neural Networks",
        "authors": "Kiarash Mohammadi, Aishwarya Sivaraman, Golnoosh Farnadi",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3617694.3623243"
    },
    {
        "id": 21786,
        "title": "Optimizing neural network training with Genetic Algorithms",
        "authors": "Junen Chai",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In modern society, computer plays an important role among all human beings. Through the increasing development of technology, some problems happened gradually. In order to solve and regenerate the country, individuals should test their strengths. This paper discusses how to use genetic algorithms to optimize neural network training. As an important tool of machine learning, neural networks have made remarkable achievements in dealing with complex tasks. However, the training process of neural networks involves a lot of hyperparameter adjustment and weight optimization, which often requires a lot of time and computing resources. In order to improve the efficiency and performance of neural network training, humans should introduce genetic algorithms as an optimization method. Experiments are conducted on several common datasets to compare the performance of neural network training with Genetic Algorithm optimization against the traditional method. The results indicate that using Genetic Algorithms significantly improves the convergence speed and performance of neural networks while reducing the time and effort spent on hyperparameter tuning. Neural networks optimized using the Genetic Algorithm outperform their counterparts trained under the same time frame.",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230780"
    },
    {
        "id": 21787,
        "title": "Sonar Data Classification using Neural Network Trained by Hybrid Dragonfly and Chimp Optimization Algorithms",
        "authors": "F. Mousavipour, Mohammad Reza Mosavi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper proposes a hybrid Dragonfly Algorithm (DA) for training Multi-Layer Perceptron Neural Network (MLP NN) to design the classifier for solving complicated problems and distinguishing the real target from liars’ targets in sonar applications. Due to improving the cost computation and reducing the waste of time, a modified low-cost DA is designed for evaluation. To assess the accuracy of the technique, some well-known meta-heuristic trainers include Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), DA, and Chimp Optimization Algorithm (ChoA) compared to show the accuracy of similar algorithms. DA and ChoA algorithms have remarkable features and a hybrid algorithm of them is proposed. The performance of the proposed classifier will be evaluated by two standard benchmark datasets. The results show that the modified hybrid DA-ChoA has 15% less time consuming and 4% better performance rather than the original dragonfly method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1673592/v2"
    },
    {
        "id": 21788,
        "title": "Sonar Data Classification using Neural Network Trained by Hybrid Dragonfly and Chimp Optimization Algorithms",
        "authors": "F. Mousavipour, M. R. Mosavi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper proposes a hybrid Dragonfly Algorithm (DA) for training Multi-Layer Perceptron Neural Network (MLP NN) to design the classifier for solving complicated problems and distinguishing the real target from liars’ targets in sonar applications. Due to improving the cost computation and reducing the waste of time, a modified low-cost DA is designed for evaluation. To assess the accuracy of the technique, some well-known meta-heuristic trainers include Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), DA, and Chimp Optimization Algorithm (ChoA) compared to show the accuracy of similar algorithms. DA and ChoA algorithms have remarkable features and a hybrid algorithm of them is proposed. The performance of the proposed classifier will be evaluated by two standard benchmark datasets. The results show that the modified hybrid DA-ChoA has 15% less time consuming and 4% better performance rather than the original dragonfly method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1673592/v1"
    },
    {
        "id": 21789,
        "title": "Activated Gas Tungsten Arc Welding Process Optimization Using Artificial Neural Network and Heuristic Algorithms",
        "authors": "Masoud Azadi Moghaddam, Farhad Kolahan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nApart from different merits of using conventional gas tungsten arc welding (C-GTAW) process, shallow penetration has been considered as the most important drawback of the process. Recently, in order to cope with the low penetration, using a paste like coating of activating flux during welding process known as activated GTAW (A-GTAW) has been proposed. In this paper, effect of A-GTAW process input parameters (welding speed (S), welding current (C)) and percentage of activating fluxes (TiO2 and SiO2) combination (F)) on the most important quality characteristics (weld bead width (WBW), depth of penetration (DOP), and consequently aspect ratio (ASR)) for AISI316L parts have been considered. The data needed for the modeling and optimization objectives, box-behnken design (BBD) of experiments, back propagation neural network (BPNN), simulated annealing (SA), and particle swarm optimization (PSO) algorithms have been employed. Moreover, PSO algorithm has been used to determine the proper ANN architecture (hidden layers number and their corresponding neurons/nodes) and optimize the proper ANN model to obtain the desired aspect ratio, maximum depth of penetration, and minimum weld bead width. Next, SA algorithm has been used to avoid getting trapped in local minima. Finally, confirmation experimental tests have been carried out to evaluate the performance of the proposed method. Due to the obtained results, the suggested method for modeling and optimization of A-GTAW process is quite efficient (with less than 4% error).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-526655/v1"
    },
    {
        "id": 21790,
        "title": "DAPID: A Differential-adaptive PID Optimization Strategy for Neural Network Training",
        "authors": "Yulin Cai, Haoqian Wang",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892746"
    },
    {
        "id": 21791,
        "title": "Reduction of training computation by network optimization of Integration Neural Network approximator",
        "authors": "Yoshiharu Iwata, Hidefumi Wakamatsu",
        "published": "2023-1-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii55687.2023.10039273"
    },
    {
        "id": 21792,
        "title": "Analysis of Neural Network Training Algorithms for Implementation of the Prescriptive Maintenance Strategy",
        "authors": "P. LEMPA",
        "published": "2022-8-5",
        "citations": 0,
        "abstract": "Abstract. This paper presents a proposal to combine supervised and semi-supervised training strategies to obtain a neural network for use in the prescriptive maintenance approach. It is required in this approach because of only partially labelled data for use in supervised learning, and additionally, this data is predicted to expand quickly. The main issue is the decision on which are suitable training methodologies for supervised learning, having in mind using this data and methods for semi-supervised learning. The proposed methods of training neural networks with supervised and semi-supervised training to receive the best results will be tested and compared in further work. ",
        "link": "http://dx.doi.org/10.21741/9781644902059-41"
    },
    {
        "id": 21793,
        "title": "Neural network surgery: Combining training with topology optimization",
        "authors": "Elisabeth J. Schiessler, Roland C. Aydin, Kevin Linka, Christian J. Cyron",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2021.08.034"
    },
    {
        "id": 21794,
        "title": "Energy Optimization of Wireless Sensor Network Using Neuro-Fuzzy Algorithms",
        "authors": "Mohammed Ali Adem",
        "published": "2019-12-8",
        "citations": 0,
        "abstract": "Wireless sensor network (WSN) is one of the recent technologies in communication and engineering world to assist various civilian and military applications. They are deployed remotely in sever environment which doesn’t have an infrastructure. Energy is a limited resource that needs efficient management to work without any failure. Energy efficient clustering of WSN is the ultimate mechanism to conserve energy for longtime. The major objective of this research is to efficiently consume energy based on the Neuro-Fuzzy approach particularly adaptive Neuro fuzzy inference system (ANFIS). The significance of this study is to examine the challenges of energy efficient algorithms and the network lifetime on WSN so that they can assist several applications. Clustering is one of the hierarchical based routing protocols, which manage the communication between sensor nodes and sink via Cluster Head (CH), CH is responsible to send and receive information from multiple sensor nodes and multiple base stations (BS). There are various algorithms that can efficiently select appropriate CH and localize the membership of cluster with fuzzy logic classification parameters to minimize periodic clustering which consumes more energy and we have applied neural network learning algorithm to learn various patterns based on the fuzzy rules and measured how much energy has saved from random clustering. Finally, we have compared to our Neuro-Fuzzy logic and consequently demonstrated that our Neuro-Fuzzy model outperforms than random model.",
        "link": "http://dx.doi.org/10.52591/lxai2019120814"
    },
    {
        "id": 21795,
        "title": "Short-term prediction of BP neural network optimized by improved particle swarm optimization algorithm",
        "authors": "Quan Sun, Yuan Sun",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2637168"
    },
    {
        "id": 21796,
        "title": "Prediction of Polycaprolactone Molecular Weight Synthesized via Enzymatic Polymerization Process: Comparing Training Algorithms of Artificial Neural Network Modeling",
        "authors": "Mohammad Asad Tariq, Senthil Kumar Arumugasamy",
        "published": "2022-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s41660-022-00240-8"
    },
    {
        "id": 21797,
        "title": "Optimization of Optical Machine Structure by Backpropagation Neural Network Based on Particle Swarm Optimization and Bayesian Regularization Algorithms",
        "authors": "Xinyong Zhang, Liwei Sun",
        "published": "2021-6-1",
        "citations": 4,
        "abstract": "Fit of the highly nonlinear functional relationship between input variables and output response is important and challenging for the optical machine structure optimization design process. The backpropagation neural network method based on particle swarm optimization and Bayesian regularization algorithms (called BMPB) is proposed to solve this problem. A prediction model of the mass and first-order modal frequency of the supporting structure is developed using the supporting structure as an example. The first-order modal frequency is used as the constraint condition to optimize the lightweight design of the supporting structure’s mass. Results show that the prediction model has more than 99% accuracy in predicting the mass and the first-order modal frequency of the supporting structure, and converges quickly in the supporting structure’s mass-optimization process. The supporting structure results demonstrate the advantages of the method proposed in the article in terms of high accuracy and efficiency. The study in this paper provides an effective method for the optimized design of optical machine structures.",
        "link": "http://dx.doi.org/10.3390/ma14112998"
    },
    {
        "id": 21798,
        "title": "A Comparative Study of Neural Network Training Algorithms for the Intelligent Security Monitoring of Industrial Control Systems",
        "authors": "Jaedeok Kim, Guillermo Francia",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-58424-9_30"
    },
    {
        "id": 21799,
        "title": "Parameter Optimization of a Convolutional Neural Network Using Particle Swarm Optimization",
        "authors": "Jonathan Fregoso, Claudia I. Gonzalez, Gabriela E. Martinez",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68776-2_9"
    },
    {
        "id": 21800,
        "title": "Swarm Based Algorithms for Neural Network Training",
        "authors": "Reginald McLean, Beatrice Ombuki-Berman, Andries P. Engelbrecht",
        "published": "2020-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc42975.2020.9283242"
    },
    {
        "id": 21801,
        "title": "Training Optimization of Feedforward Neural Network for Binary Classification",
        "authors": "Omkar Thawakar, Pranav Gajjewar",
        "published": "2019-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci.2019.8822184"
    },
    {
        "id": 21802,
        "title": "Analyzing Training Exercises with Mathematical Algorithms and Convolutional Neural Network",
        "authors": "Szabolcs Róbert Bakos, Miklós Sipos",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cinti59972.2023.10382077"
    },
    {
        "id": 21803,
        "title": "Acceleration of Neural Network training algorithms via FPGA devices",
        "authors": "Gyulai-Nagy Zoltán-Valentin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.10.259"
    },
    {
        "id": 21804,
        "title": "FUZZY MULTI-OBJECTIVE OPTIMIZATION ALGORITHMS FOR SOLVING MULTI-MODE AUTOMATED GUIDED VEHICLES BY CONSIDERING MACHINE BREAK TIME AND ARTIFICIAL NEURAL NETWORK",
        "authors": "Hojat Nabovati, Hassan Haleh, Behnam Vahdani",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14311/nnw.2018.28.016"
    },
    {
        "id": 21805,
        "title": "Development and optimization of artificial neural network algorithms for the prediction of building specific local temperature for HVAC control",
        "authors": "Gulsun Demirezen, Alan Fung, Mathieu Deprez",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This research accounts for the outcome of a major cloud-based smart dual fuel switching system (SDFSS) project, which is a dual-fuel integrated hybrid heating, ventilation, and air conditioning (HVAC) system in residential homes. The SDFSS was developed to enable optimized, flexible, and cost-effective switching between the natural gas furnace and electric air source heat pump (ASHP). In order to meet the optimal energy consumption requirements in the house and provide thermal comfort for the residents, various high-quality sensors and meters were installed to record multiple data points inside and outside the house. The performance of the system was monitored in the long term, which is a common practice in energy monitoring projects. Outdoor temperature data plays the most crucial role in operating HVAC systems and also is a key variable in the decision-making algorithm of the SDFSS controller. Therefore, this study introduces an innovative and unique approach to obtain the outdoor temperature that could potentially replace high precision sensors with a data-driven model utilizing weather station data at a time resolution of 2 minutes and 1 hour. In this work, a series of artificial neural network algorithms were developed, optimized, and implemented to predict the outdoor temperature with an average of 0.99 coefficient of correlation (<em>R</em>), 1.011 mean absolute error (MAE), and 1.315 root mean square error (RMSE). It has been demonstrated that the developed ANN is a reliable and powerful tool in predicting outdoor temperature. Thus, the proposed model is strongly suggested to be implemented as an alternative to temperature sensors in hybrid energy systems or similar systems requiring accurate ambient temperature measurements.</p>",
        "link": "http://dx.doi.org/10.32920/23811252"
    },
    {
        "id": 21806,
        "title": "APPLICATION OF NEURAL NETWORKS AND EVOLUTIONARY ALGORITHMS TO SOLVE ENERGY OPTIMIZATION AND UNIT COMMITMENT FOR A SMART CITY",
        "authors": "Bohumír Garlík",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14311/nnw.2018.28.022"
    },
    {
        "id": 21807,
        "title": "Development and optimization of artificial neural network algorithms for the prediction of building specific local temperature for HVAC control",
        "authors": "Gulsun Demirezen, Alan Fung, Mathieu Deprez",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This research accounts for the outcome of a major cloud-based smart dual fuel switching system (SDFSS) project, which is a dual-fuel integrated hybrid heating, ventilation, and air conditioning (HVAC) system in residential homes. The SDFSS was developed to enable optimized, flexible, and cost-effective switching between the natural gas furnace and electric air source heat pump (ASHP). In order to meet the optimal energy consumption requirements in the house and provide thermal comfort for the residents, various high-quality sensors and meters were installed to record multiple data points inside and outside the house. The performance of the system was monitored in the long term, which is a common practice in energy monitoring projects. Outdoor temperature data plays the most crucial role in operating HVAC systems and also is a key variable in the decision-making algorithm of the SDFSS controller. Therefore, this study introduces an innovative and unique approach to obtain the outdoor temperature that could potentially replace high precision sensors with a data-driven model utilizing weather station data at a time resolution of 2 minutes and 1 hour. In this work, a series of artificial neural network algorithms were developed, optimized, and implemented to predict the outdoor temperature with an average of 0.99 coefficient of correlation (<em>R</em>), 1.011 mean absolute error (MAE), and 1.315 root mean square error (RMSE). It has been demonstrated that the developed ANN is a reliable and powerful tool in predicting outdoor temperature. Thus, the proposed model is strongly suggested to be implemented as an alternative to temperature sensors in hybrid energy systems or similar systems requiring accurate ambient temperature measurements.</p>",
        "link": "http://dx.doi.org/10.32920/23811252.v1"
    },
    {
        "id": 21808,
        "title": "Fuzzification of training data class membership binary values for neural network algorithms",
        "authors": "Tibor Tajti",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33039/ami.2020.10.001"
    },
    {
        "id": 21809,
        "title": "Diffuse Neural and Neuro-Fuzzy Networks Training Algorithms",
        "authors": "Boris A. Skorohod",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-812609-7.00004-4"
    },
    {
        "id": 21810,
        "title": "Co-Optimization of Carbon Dioxide Storage in Aquifers Using Genetic Algorithms Based on Artificial Neural Network",
        "authors": "P. Vaziri, B. Sedaee",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202210633"
    },
    {
        "id": 21811,
        "title": "An Improved Cuckoo Search Algorithm for Optimization of Artificial Neural Network Training",
        "authors": "Pedda Nagyalla Maddaiah, Pournami Pulinthanathu Narayanan",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-023-11411-0"
    },
    {
        "id": 21812,
        "title": "FPGA IMPLEMENTATION OF ANN TRAINING USING LEVENBERG AND MARQUARDT ALGORITHMS",
        "authors": "Mehmet Ali Çavuşlu, Suhap Şahin",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14311/nnw.2018.28.010"
    },
    {
        "id": 21813,
        "title": "A Comprehensive Comparison of the Performance of Metaheuristic Algorithms in Neural Network Training for Nonlinear System Identification",
        "authors": "Ebubekir Kaya",
        "published": "2022-5-9",
        "citations": 8,
        "abstract": "Many problems in daily life exhibit nonlinear behavior. Therefore, it is important to solve nonlinear problems. These problems are complex and difficult due to their nonlinear nature. It is seen in the literature that different artificial intelligence techniques are used to solve these problems. One of the most important of these techniques is artificial neural networks. Obtaining successful results with an artificial neural network depends on its training process. In other words, it should be trained with a good training algorithm. Especially, metaheuristic algorithms are frequently used in artificial neural network training due to their advantages. In this study, for the first time, the performance of sixteen metaheuristic algorithms in artificial neural network training for the identification of nonlinear systems is analyzed. It is aimed to determine the most effective metaheuristic neural network training algorithms. The metaheuristic algorithms are examined in terms of solution quality and convergence speed. In the applications, six nonlinear systems are used. The mean-squared error (MSE) is utilized as the error metric. The best mean training error values obtained for six nonlinear systems were 3.5×10−4, 4.7×10−4, 5.6×10−5, 4.8×10−4, 5.2×10−4, and 2.4×10−3, respectively. In addition, the best mean test error values found for all systems were successful. When the results were examined, it was observed that biogeography-based optimization, moth–flame optimization, the artificial bee colony algorithm, teaching–learning-based optimization, and the multi-verse optimizer were generally more effective than other metaheuristic algorithms in the identification of nonlinear systems.",
        "link": "http://dx.doi.org/10.3390/math10091611"
    },
    {
        "id": 21814,
        "title": "Investigation of the Evolutionary Optimization Algorithms for the Neural Network Solution of the Optimal Control Problems",
        "authors": "Irina Bolodurina, Lyubov Zabrodina",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon49822.2020.9208192"
    },
    {
        "id": 21815,
        "title": "Hyper-parameter optimization for convolutional neural network committees based on evolutionary algorithms",
        "authors": "Erik Bochinski, Tobias Senst, Thomas Sikora",
        "published": "2017-9",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2017.8297018"
    },
    {
        "id": 21816,
        "title": "Investigation of neural network and fuzzy inference neural network and their optimization using meta-algorithms in river flood routing",
        "authors": "Mohammad R. Hassanvand, Hojat Karami, Sayed-Farhad Mousavi",
        "published": "2018-12",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11069-018-3456-z"
    },
    {
        "id": 21817,
        "title": "An Improved Particle Swarm optimization based Neural Network Training for Classification",
        "authors": "Palash Mondal, Arijit Nandi, Nanda Dulal Jana",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp46218.2019.8971232"
    },
    {
        "id": 21818,
        "title": "Music melody generation and LIF supervised training based on spiking neural network",
        "authors": "Yang Gao",
        "published": "2022-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2659783"
    },
    {
        "id": 21819,
        "title": "HYBRID NEURAL NETWORK OPTIMIZATION SYSTEM BASED ON ANT ALGORITHMS",
        "authors": "V. M. Sineglazov, O. I. Chumachenko, D. M. Omelchenko",
        "published": "2020-7-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18372/1990-5548.64.14857"
    },
    {
        "id": 21820,
        "title": "Evaluating Convolution Neural Network optimization Algorithms for Classification of Cervical Cancer Macro Images",
        "authors": "Suleiman Mustafa, Mohammed Dauda",
        "published": "2019-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecco48375.2019.9043255"
    },
    {
        "id": 21821,
        "title": "Investigation of Optimization Algorithms for Neural Network Solutions of Optimal Control Problems with Mixed Constraints",
        "authors": "Irina Bolodurina, Lyubov Zabrodina",
        "published": "2021-5-17",
        "citations": 1,
        "abstract": "In this paper, we consider the problem of selecting the most efficient optimization algorithm for neural network approximation—solving optimal control problems with mixed constraints. The original optimal control problem is reduced to a finite-dimensional optimization problem by applying the necessary optimality conditions, the Lagrange multiplier method and the least squares method. Neural network approximation models are presented for the desired control functions, trajectory and conjugate factors. The selection of the optimal weight coefficients of the neural network approximation was carried out using the gravitational search algorithm and the basic particle swarm algorithm and the genetic algorithm. Computational experiments showed that evolutionary optimization algorithms required the smallest number of iterations for a given accuracy in comparison with the classical gradient optimization method; however, the multi-agent optimization methods were performed later for each operation. As a result, the genetic algorithm showed a faster convergence rate relative to the total execution time.",
        "link": "http://dx.doi.org/10.3390/machines9050102"
    },
    {
        "id": 21822,
        "title": "Performance Analysis of Traffic Congestion Using Designated Neural Network Training Algorithms",
        "authors": "Ituabhor Odesanya, Joseph Femi Odesanya",
        "published": "2021-4-20",
        "citations": 0,
        "abstract": "A lot of neural network training algorithms on prediction exist and these algorithms are being used by researchers to solve evaluation, forecasting, clustering, function approximation etc. problems in traffic volume congestion. This study is aimed at analysing the performance of traffic congestion using some designated neural network training algorithms on traffic flow in some selected corridors within Akure, Ondo state, Nigeria. The selected corridors were Oba Adesida road, Oyemekun road and Oke Ijebu road all in Akure. The traffic flow data were collected manually with the help of field observers who monitored and record traffic movement along the corridors. To accomplish this, three common training algorithms were selected to train the traffic flow data. The data were trained using Bayesian Regularization (BR), Scaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) training algorithms. The outputs/performances of these training functions were evaluated by using the Mean Square Error (MSE) and Coefficient of Regression (R) to find the best training algorithms. The results show that, the Bayesian regularization algorithm, performs better with MSE of 2.37e-13 and R of 0.9999 than SCG and LM algorithms.",
        "link": "http://dx.doi.org/10.56431/p-nof752"
    },
    {
        "id": 21823,
        "title": "Performance Analysis of Traffic Congestion Using Designated Neural Network Training Algorithms",
        "authors": "Ituabhor Odesanya, Joseph Femi Odesanya",
        "published": "2021-4",
        "citations": 0,
        "abstract": "A lot of neural network training algorithms on prediction exist and these algorithms are being used by researchers to solve evaluation, forecasting, clustering, function approximation etc. problems in traffic volume congestion. This study is aimed at analysing the performance of traffic congestion using some designated neural network training algorithms on traffic flow in some selected corridors within Akure, Ondo state, Nigeria. The selected corridors were Oba Adesida road, Oyemekun road and Oke Ijebu road all in Akure. The traffic flow data were collected manually with the help of field observers who monitored and record traffic movement along the corridors. To accomplish this, three common training algorithms were selected to train the traffic flow data. The data were trained using Bayesian Regularization (BR), Scaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) training algorithms. The outputs/performances of these training functions were evaluated by using the Mean Square Error (MSE) and Coefficient of Regression (R) to find the best training algorithms. The results show that, the Bayesian regularization algorithm, performs better with MSE of 2.37e-13 and R of 0.9999 than SCG and LM algorithms.",
        "link": "http://dx.doi.org/10.18052/www.scipress.com/ijet.20.23"
    },
    {
        "id": 21824,
        "title": "Design Optimization of Truss Structures Using a Graph Neural Network-Based Surrogate Model",
        "authors": "Navid Nourian, Mamdouh El-Badry, Maziar Jamshidi",
        "published": "2023-8-7",
        "citations": 1,
        "abstract": "One of the primary objectives of truss structure design optimization is to minimize the total weight by determining the optimal sizes of the truss members while ensuring structural stability and integrity against external loads. Trusses consist of pin joints connected by straight members, analogous to vertices and edges in a mathematical graph. This characteristic motivates the idea of representing truss joints and members as graph vertices and edges. In this study, a Graph Neural Network (GNN) is employed to exploit the benefits of graph representation and develop a GNN-based surrogate model integrated with a Particle Swarm Optimization (PSO) algorithm to approximate nodal displacements of trusses during the design optimization process. This approach enables the determination of the optimal cross-sectional areas of the truss members with fewer finite element model (FEM) analyses. The validity and effectiveness of the GNN-based optimization technique are assessed by comparing its results with those of a conventional FEM-based design optimization of three truss structures: a 10-bar planar truss, a 72-bar space truss, and a 200-bar planar truss. The results demonstrate the superiority of the GNN-based optimization, which can achieve the optimal solutions without violating constraints and at a faster rate, particularly for complex truss structures like the 200-bar planar truss problem.",
        "link": "http://dx.doi.org/10.3390/a16080380"
    },
    {
        "id": 21825,
        "title": "Evaluation of Parameter Settings for Training Neural Networks Using Backpropagation Algorithms",
        "authors": " Leema N., Khanna H. Nehemiah,  Elgin Christo V. R.,  Kannan A.",
        "published": "2022",
        "citations": 0,
        "abstract": "Artificial neural networks (ANN) are widely used for classification, and the training algorithm commonly used is the backpropagation (BP) algorithm. The major bottleneck faced in the backpropagation neural network training is in fixing the appropriate values for network parameters. The network parameters are initial weights, biases, activation function, number of hidden layers and the number of neurons per hidden layer, number of training epochs, learning rate, minimum error, and momentum term for the classification task. The objective of this work is to investigate the performance of 12 different BP algorithms with the impact of variations in network parameter values for the neural network training. The algorithms were evaluated with different training and testing samples taken from the three benchmark clinical datasets, namely, Pima Indian Diabetes (PID), Hepatitis, and Wisconsin Breast Cancer (WBC) dataset obtained from the University of California Irvine (UCI) machine learning repository. ",
        "link": "http://dx.doi.org/10.4018/978-1-6684-2408-7.ch009"
    },
    {
        "id": 21826,
        "title": "Neural Network Training Using Particle Swarm Optimization - a Case Study",
        "authors": "Marcin Kaminski",
        "published": "2019-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mmar.2019.8864679"
    },
    {
        "id": 21827,
        "title": "Optimized Training for Convolutional Neural Network Using Enhanced Grey Wolf Optimization Algorithm",
        "authors": "Akram Guernine, Mohamed Tahar Kimour",
        "published": "2021-8-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31449/inf.v45i5.3497"
    },
    {
        "id": 21828,
        "title": "Comparative study of various training algorithms of artificial neural network",
        "authors": "Pragati Jaiswal, Nikhil Kumar Gupta, A. Ambikapathy",
        "published": "2018-10",
        "citations": 63,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacccn.2018.8748660"
    },
    {
        "id": 21829,
        "title": "Iterative CT image reconstruction using neural network optimization algorithms",
        "authors": "Hongquan Zuo, Jun Zhang",
        "published": "2019-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2512329"
    },
    {
        "id": 21830,
        "title": "Neural Network Training Techniques Regularize Optimization Trajectory: An Empirical Study",
        "authors": "Cheng Chen, Junjie Yang, Yi Zhou",
        "published": "2020-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378359"
    },
    {
        "id": 21831,
        "title": "Logarithm Decreasing Inertia Weight Particle Swarm Optimization Algorithms for Convolutional Neural Network",
        "authors": "Murinto Murinto, Miftahurrahma Rosyda",
        "published": "2022-5-27",
        "citations": 1,
        "abstract": "The convolutional neural network (CNN) is a technique that is often used in deep learning. Various models have been proposed and improved for learning on CNN. When learning with CNN, it is important to determine the optimal parameters. This paper proposes an optimization of CNN parameters using logarithm decreasing inertia weight (LogDIW). This paper is used two datasets, i.e., MNIST and CIFAR-10 dataset. The MNIST learning experiment, the CIFAR-10 dataset, compared its accuracy with the CNN standard based on the LeNet-5 architectural model. When using the MNIST dataset, CNN's baseline was 94.02% at the 5th epoch, compared to CNN's LogDIWPSO, which improves accuracy. When using the CIFAR-10 dataset, the CNN baseline was 28.07% at the 10th epoch, compared to the LogDIWPSO CNN accuracy of 69.3%, which increased the accuracy.",
        "link": "http://dx.doi.org/10.30595/juita.v10i1.12573"
    },
    {
        "id": 21832,
        "title": "COMPARATIVE ANALYSIS OF BIO-INSPIRED OPTIMIZATION ALGORITHMS IN NEURAL NETWORK BASED DATA MINING CLASSIFICATION",
        "authors": "",
        "published": "2022-1",
        "citations": 0,
        "abstract": "It always helps to determine optimal solutions for stochastic problems thereby maintaining good balance between its key elements. Nature inspired algorithms are meta-heuristics that mimic the natural activities for solving optimization issues in the era of computation. In the past decades, several research works have been presented for optimization especially in the field of data mining. This paper addresses the implementation of bio-inspired optimization techniques for machine learning based data mining classification by four different optimization algorithms. The stochastic problems are overcome by training the neural network model with techniques such as barnacles mating , black widow optimization, cuckoo algorithm and elephant herd optimization. The experiments are performed on five different datasets, and the outcomes are compared with existing methods with respect to runtime, mean square error and classification rate. From the experimental analysis, the proposed bio-inspired optimization algorithms are found to be effective for classification with neural network training.",
        "link": "http://dx.doi.org/10.4018/ijsir.2022010114"
    },
    {
        "id": 21833,
        "title": "Network Security Intrusion Detection Methods Combining Optimization Algorithms and Neural Networks",
        "authors": "Lan Xia, Xuefei Xia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.11.067"
    },
    {
        "id": 21834,
        "title": "Evaluation on Training Algorithms of Back Propagation Neural Network for a Solar Photovoltaic Based DSTATCOM System",
        "authors": "Nor Hanisah Baharudin, Tunku Muhammad Nizar Tunku Mansur, Rosnazri Ali, Muhammad Irwanto Misrun",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-6151-7_9"
    },
    {
        "id": 21835,
        "title": "Hyperparameters optimization for neural network training using Fractal Decomposition-based Algorithm",
        "authors": "Leo Souquet, Nadiya Shvai, Arcadi Llanza, Amir Nakib",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec48606.2020.9185599"
    },
    {
        "id": 21836,
        "title": "Accelerated Gradient-free Neural Network Training by Multi-convex Alternating Optimization",
        "authors": "Junxiang Wang, Hongyi Li, Liang Zhao",
        "published": "2022-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.02.039"
    },
    {
        "id": 21837,
        "title": "Study of PSO and Firefly algorithm based Feed-forward neural network training algorithms",
        "authors": "Sudarsan Nandy, Anirban Mitra, Tamoghna Mukherjee",
        "published": "2020-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spin48934.2020.9070809"
    },
    {
        "id": 21838,
        "title": "A new brain tumor diagnostic model: Selection of textural feature extraction algorithms and convolution neural network features with optimization algorithms",
        "authors": "Erdal Başaran",
        "published": "2022-9",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2022.105857"
    },
    {
        "id": 21839,
        "title": "Thermal Stress Analysis in Two-Directional Functionally Graded Plates with Artificial Neural Network Training Algorithms",
        "authors": "Munise Didem Demirbaş, Didem Çakır",
        "published": "2019-6-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29137/umagd.485604"
    },
    {
        "id": 21840,
        "title": "Variational Algorithms, Quantum Approximate Optimization Algorithm, and Neural Network Quantum States with Two Qubits",
        "authors": "Claudio Conti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-44226-1_6"
    },
    {
        "id": 21841,
        "title": "Network Optimization Models at Greater Kuparuk Area Using Neural Networks and Genetic Algorithms",
        "authors": "Rodney L. Murray, Reese S. Hopkins, Douglas K. Valentine",
        "published": "2020-10-19",
        "citations": 0,
        "abstract": "Abstract\nThe Greater Kuparuk Area, located on the North Slope of Alaska, began production in 1981 and has produced over 2.5 billion barrels to date. The area contains six oil fields flowing into three central processing facilities with 47 drillsites and over 1,200 production and injection wells. The facilities are primarily gas constrained due to limits from the gas lift compressors, and secondarily water constrained due to injection pump capacity. An optimization program using the equal slope concept is currently in use for lift gas allocation. A previous attempt to more rigorously optimize the production system using commercial software resulted in better lift gas allocation, but computation time lead to the cessation of its use for daily optimization.\nThe objective of this work was to develop a fast, flexible optimization model that recommends well status, lift gas rates, and water injection rates. The model uses field data and data generated by the previous surface models to develop the hydraulic models as well as current facility conditions and constraints. The model contains four components. The first was a function that estimates producer and injector performance. The second is a function that gathers and interpolates well performance models with physics-based models. Third, the drillsite header pressures were estimated using a neural network. Finally, a genetic algorithm is used to search for the optimal well status, lift gas rate, and water injection rate for each well. Connections were made to databases to run the model using field conditions at any time over the last five years.\nThe hydraulic model for three phase flow utilizes a neural network, whereas a simpler linear based model is used for the water injection system. The hydraulic model was rigorously back tested using field data over a two-year period with weekly model retraining. Drillsite header pressures deviated 6 psi on average from actuals, which is on par with commercial software. The optimization converges in under 90 seconds in a single facility optimization run. The recommendations from the optimization program are expected to increase oil rate 1.5% in the existing system.\nWhile production optimization using genetic algorithms and neural networks has been presented for over 20 years, there are not many, if any, known industry applications of optimizing the production and injection networks simultaneously using neural network models. The program was written in Python and deployed on cloud computing. The tool is used to calculate daily net oil benefit per well, prioritize shut-in wells when the facility is constrained, and optimize injection pumps and drillsite configurations. Additionally, the model is designed to accept new engineer-specified source wells to understand the impact of backout when developing new projects. Overall, the model has provided a platform for engineers to make optimization decisions in a complex, interdependent system.",
        "link": "http://dx.doi.org/10.2118/201760-ms"
    },
    {
        "id": 21842,
        "title": "Artificial Neural Network Based Prediction Of Wind Turbine Power Curve Using Various Training Algorithms",
        "authors": "Muhammad U Saram, Jianming Yang, Zaheer Ahmad, Sadaf Zahoor",
        "published": "2021-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32393/csme.2021.136"
    },
    {
        "id": 21843,
        "title": "New optimization methods for designing rain stations network using new neural network, election, and whale optimization algorithms by combining the Kriging method",
        "authors": "Maryam Safavi, Abbas Khashei Siuki, Seyed Reza Hashemi",
        "published": "2021-1",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10661-020-08726-z"
    },
    {
        "id": 21844,
        "title": "Wireless sensor network coverage optimization strategy based on improved cuckoo search algorithm",
        "authors": "Siyang Li",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3004648"
    },
    {
        "id": 21845,
        "title": "OPTIMIZATION OF ELECTRIC POWER SYSTEMS USING FUZZY NEURAL NETWORK ALGORITHMS",
        "authors": "Alexey L Rutskov",
        "published": "2020-4-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26782/jmcms.spl.8/2020.04.00020"
    },
    {
        "id": 21846,
        "title": "Genetic algorithm based hyper-parameters optimization for transfer convolutional neural network",
        "authors": "Chen Li, Jinzhe Jiang, Yaqian Zhao, Rengang Li, Endong Wang, Xin Zhang, Kun Zhao",
        "published": "2022-6-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2637170"
    },
    {
        "id": 21847,
        "title": "Optimization Analysis of Neural Network Algorithms Using Bagging Techniques on Classification of Date Fruit Types",
        "authors": "Rully Pramudita,  Solikin, Nadya Safitri",
        "published": "2022-12-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icic56845.2022.10006986"
    },
    {
        "id": 21848,
        "title": "Genetic Optimization of Ensemble Neural Network Architectures for Prediction of COVID-19 Confirmed and Death Cases",
        "authors": "Julio C. Mónica, Patricia Melin, Daniela Sánchez",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68776-2_5"
    },
    {
        "id": 21849,
        "title": "Random Regrouping and Factorization in Cooperative Particle Swarm Optimization Based Large-Scale Neural Network Training",
        "authors": "Cody Dennis, Beatrice M. Ombuki-Berman, Andries P. Engelbrecht",
        "published": "2020-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-019-10112-x"
    },
    {
        "id": 21850,
        "title": "Early Stopping Criteria for Levenberg-Marquardt Based Neural Network Training Optimization",
        "authors": "Azizah Suliman, Batyrkhan Omarov",
        "published": "2018-12-9",
        "citations": 2,
        "abstract": "In this research we train a direct distributed neural network using Levenberg-Marquardt algorithm. In order to prevent overtraining, we proposed correctly recognized image percentage based on early stop condition and conduct the experiments with different stop thresholds for image classification problem. Experiment results show that the best early stop condition is 93% and other increase in stop threshold can lead to decrease in the quality of the neural network. The correct choice of early stop condition can prevent overtraining which led to the training of a neural network with considerable number of hidden neurons.  ",
        "link": "http://dx.doi.org/10.14419/ijet.v7i4.36.25382"
    },
    {
        "id": 21851,
        "title": "Filter Size Optimization on a Convolutional Neural Network Using FGSA",
        "authors": "Yutzil Poma, Patricia Melin, Claudia I. González, Gabriela E. Martínez",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-35445-9_29"
    },
    {
        "id": 21852,
        "title": "Accelerations of neural network training.",
        "authors": "Anastasia Ermakova",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nNeural networks are widely used for scientific and commercial applications across the world. Neural networks show themselves as a fast and effective instrument for natural text processing. The downside of neural network engagement for scientific or commercial purposes is a time and resources consuming process for training and testing.\n\n\nThe training could take up to 7 days and occupy more than 2500 MB of memory. High CPU, GPU, and memory consumption make it impossible to do high-quality training on the personal PC. It creates difficulties for students, young professionals, and researchers who lack easy access to powerful machines to conduct experiments or learn how to train and use neural networks.\n\n\nIn this article, I summarize methods to train neural networks with large training sets without consuming too much memory and time.\n",
        "link": "http://dx.doi.org/10.14293/pr2199.000462.v1"
    },
    {
        "id": 21853,
        "title": "A Convolutional Neural Network (\n            <scp>CNN</scp>\n            ) Architecture and Training",
        "authors": "",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781394209118.ch7"
    },
    {
        "id": 21854,
        "title": "A Study of Artificial Neural Network Training Algorithms for Classification of Cardiotocography Signals",
        "authors": "Zafer CÖMERT, Adnan KOCAMAZ",
        "published": "2017-12-26",
        "citations": 58,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17678/beuscitech.338085"
    },
    {
        "id": 21855,
        "title": "Using Knowledge Transfer for Neural Network Architecture Optimization with Improved Training Efficiency",
        "authors": "Marius Gavrilescu, Florin Leon",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc55426.2022.9931898"
    },
    {
        "id": 21856,
        "title": "Performance Optimization by Using Artificial Neural Network Algorithms in VANETs",
        "authors": "Muhammet Ali Karabulut, A. F. M. Shahen Shah, Haci Ilhan",
        "published": "2019-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsp.2019.8768830"
    },
    {
        "id": 21857,
        "title": "Transport Stream Optimization Based on Neural Network Learning Algorithms",
        "authors": "Yaroslav Shamlitskiy, Anatoly Popov, Nazar Saidov, Kristina Moiseeva",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.trpro.2023.02.056"
    },
    {
        "id": 21858,
        "title": "Optimization of neural network for software effort estimation",
        "authors": "P Sankara Rao, Kiran Kumar Reddi, R Usha Rani",
        "published": "2017-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icammaet.2017.8186696"
    },
    {
        "id": 21859,
        "title": "Comparison of Feedforward Neural Network with Different Training Algorithms for Bitcoin Price Forecasting",
        "authors": "Eng Chuen Loh, Shuhaida Ismail, Azme Khamis, Aida Mustapha",
        "published": "2020-4-10",
        "citations": 1,
        "abstract": "Bitcoin is the most popular cryptocurrency with the highest market value. It was said to have potential in changing the way of trading in future. However, Bitcoin price prediction is a hard task and difficult for investors to make decision. This is caused by nonlinearity property of the Bitcoin price. Hence, a better forecasting method are essential to minimize the risk from inaccuracy decision. The aim of this paper is to compare two different training algorithms which are Levenberg-Marquardt (LM) backpropagation algorithm and Scaled Conjugate Gradient (SCG) backpropagation algorithm using Feedforward Neural Network (FNN) to forecast the Bitcoin price. After obtaining the forecasting result, forecast accuracy measurement will be carried out to identify the best model to forecast Bitcoin price. The result showed that the performance of Bitcoin price forecasting increased after the application of FNN – LM model. It is proven that Levenberg-Marquardt backpropagation algorithm is better compared to Scaled Conjugate Gradient backpropagation when forecasting Bitcoin price using FNN. The resulting model provides new insights into Bitcoin forecasting using FNN – LM model which directly benefits the investors and economists in lowering the risk of making wrong decision when it comes to invest in Bitcoin.\nKeywords: Bitcoin Price; Artificial Neural Network; Forecasting",
        "link": "http://dx.doi.org/10.32802/asmscj.2020.sm26(1.5)"
    },
    {
        "id": 21860,
        "title": "Fundamentals of optimization of training algorithms for artificial neural networks",
        "authors": "P.A. Kornev, A.N. Pylkin",
        "published": "2020",
        "citations": 0,
        "abstract": "In the modern IT industry, the basis for the nearest progress is artificial intelligence technologies and, in particular, artificial neuron systems. The so-called neural networks are constantly being improved within the framework of their many learning algorithms for a wide range of tasks. In the paper, a class of approximation problems is distinguished as one of the most common classes of problems in artificial intelligence systems. The aim of the paper is to study the most recommended learning algorithms, select the most optimal one and find ways to improve it according to various characteristics. Several of the most commonly used learning algorithms for approximation are considered. In the course of computational experiments, the most advantageous aspects of all the presented algorithms are revealed. A method is proposed for improving the computational characteristics of the algorithms under study.",
        "link": "http://dx.doi.org/10.1051/e3sconf/202022401022"
    },
    {
        "id": 21861,
        "title": "Special Issue “Neural Network for Traffic Forecasting”",
        "authors": "Weiwei Jiang",
        "published": "2023-9-2",
        "citations": 0,
        "abstract": "Traffic forecasting is an important research topic in intelligent transportation systems and smart cities [...]",
        "link": "http://dx.doi.org/10.3390/a16090421"
    },
    {
        "id": 21862,
        "title": "Review of convolutional neural network optimization and training in image processing",
        "authors": "Yong Ren, Xuemin Cheng",
        "published": "2019-3-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2512087"
    },
    {
        "id": 21863,
        "title": "Improving Training time in Capsule Neural Network",
        "authors": "Onyeachonam Dominic-Mario Chiadika, Moazhen Li, Jaewoong Choi",
        "published": "2021-6-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hora52670.2021.9461340"
    },
    {
        "id": 21864,
        "title": "Study on Optimization Method of Hidden Layer Nodes and Training Times in Artificial Neural Network",
        "authors": "Yiran Zhao",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceitsa54226.2021.00081"
    },
    {
        "id": 21865,
        "title": "Optimization of dynamic frame slot ALOHA algorithm based on back propagation neural network",
        "authors": "yulu wu, shaopeng li, xiangpeng kong, yang li, yunfeng sun, fuxiang zhang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3004669"
    },
    {
        "id": 21866,
        "title": "Network Optimization",
        "authors": "",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119898948.ch4"
    },
    {
        "id": 21867,
        "title": "Prediction of Breast Tumor Malignancy Using Neural Network and Whale Optimization Algorithms (WOA)",
        "authors": "Ali Sharifi",
        "published": "2019-11-1",
        "citations": 2,
        "abstract": "Introduction: Breast cancer is the most prevalent cause of cancer mortality among women. Early diagnosis of breast cancer gives patients greater survival time. The present study aims to provide an algorithm for more accurate prediction and more effective decision-making in the treatment of patients with breast cancer.\nMethods: The present study was applied, descriptive-analytical, based on the use of computerized methods. We obtained 699 independent records containing nine clinical variables from the UCI machine learning. The EM algorithm was used to analyze the data before normalizing them. Following that, a combination of neural network model based on multilayer perceptron structure with the Whale Optimization Algorithm (WOA) was used to predict the breast tumor malignancy.\nResults: After preprocessing the disease data set and reducing data dimensions, the accuracy of the proposed algorithm for training and testing data was 99.6% and 99%, respectively. The prediction accuracy of the proposed model was 99.4%, which would be a satisfying result compared to different methods of machine learning in other studies.\nConclusion: Considering the importance of early diagnosis of breast cancer, the results of this study may have highly useful implications for health care providers and planners so as to achieve the early diagnosis of the disease.",
        "link": "http://dx.doi.org/10.30699/acadpub.ijbd.12.3.26"
    },
    {
        "id": 21868,
        "title": "TOWARDS GLOBAL OPTIMIZATION OF NEURAL NETWORK: A COMPARATIVE ANALYSIS USING GENETIC AND WHALE OPTIMIZATION ALGORITHMS",
        "authors": "E. C. Igodan, K. C. Ukaoha, S. O. P. Oliomogbe",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "The intelligence and adaptability features of the neural network has made it a technique that is widely used to solve problems in diverse areas such as; detection, monitoring, prediction, diagnostics, data mining, classification, recognition, robotics, biomedicine, etc. However, determination of the optimal number of hidden layers of neural network and other parameters are still a difficult task. Usually, these parameters are decided by trial-and-error which increases the computational complexity and it is human dependent in obtaining the optimal model and parameters alike for any particular task. Optimization has received enormous attention in recent years, primarily because of the rapid progress in computer technology, including the development and availability of user-friendly software, high-speed and parallel processors, and artificial neural networks. This research work is to propose a neuro-evolutionary model using the computational intelligence techniques by combining ANN, GA and WOA for binary classification problems. The proposed optimized ANN-GA and WOA models is to circumvent the problem that is characterized in the trade-off between smoothness and accuracies in selecting the models and optimal parameters of neural network.",
        "link": "http://dx.doi.org/10.14738/jbemi.86.11004"
    },
    {
        "id": 21869,
        "title": "Enhancing Performance of a Deep Neural Network: A Comparative Analysis of Optimization Algorithms",
        "authors": "Noor Fatima",
        "published": "2020-6-20",
        "citations": 23,
        "abstract": "Adopting the most suitable optimization algorithm (optimizer) for a Neural Network Model is among the most important ventures in Deep Learning and all classes of Neural Networks. It’s a case of trial and error experimentation. In this paper, we will experiment with seven of the most popular optimization algorithms namely: sgd, rmsprop, adagrad, adadelta, adam, adamax and nadam on four unrelated datasets discretely, to conclude which one dispenses the best accuracy, efficiency and performance to our deep neural network. This work will provide insightful analysis to a data scientist in choosing the best optimizer while modelling their deep neural network.",
        "link": "http://dx.doi.org/10.14201/adcaij2020927990"
    },
    {
        "id": 21870,
        "title": "Neural Network Optimization and Implications of High and Low Gradient Results in Training Vanilla and Hybrid Adaptive Neural Network Models for Effective Signal Power Loss Prediction",
        "authors": "Virginia Chika Ebhota, Thokozani Shongwe",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15866/iremos.v16i5.23622"
    }
]