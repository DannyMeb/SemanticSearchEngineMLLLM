[
    {
        "id": 6001,
        "title": "Prognostic Model Evaluation Metrics",
        "authors": "Shashvat Prakash, Katarina Vuckovic, Sanket Amin",
        "published": "2023-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero55745.2023.10115952"
    },
    {
        "id": 6002,
        "title": "ARM Data-Oriented Metrics and Diagnostics Package for Climate Model Evaluation",
        "authors": "Chengzhu Zhang, Shaocheng Xie, Cheng Tao",
        "published": "2020-10-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1396238"
    },
    {
        "id": 6003,
        "title": "Improved ANN-based earthquake prediction system with reference model engaged in the evaluation metrics",
        "authors": "Ying Zhang, Qinghua Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Artificial Neural Networks (ANNs) are well known for their ability to find hidden patterns in data. This technique has also been widely used for predicting the time, location, and magnitude of future earthquakes. Using various data and neural networks, previous works claimed that their models are effective for predicting earthquakes. However, these scores provided by the evaluation metrics with poor reference models, which are non-professional in statistical seismology, are not robust. In this work, we first take the Nighttime Light Map (NLM) as the input of Long-short Term Memory (LSTM) networks to predict the earthquakes with M>=5.0 for the whole Chinese Mainland, and NLM records the lumens of nighttime artificial light, and it is retrieved from the nighttime satellite imagery. The NLM is not physically related to earthquakes; however, the scores provided by Receiver Operating Characteristics curve, Precision-Recall plot, and Molchan diagram with spatial invariant Poisson model indicated that NLM is effective for predicting earthquakes. These results reaffirmed that researchers should be cautious when using these evaluation metrics with poor reference models to evaluate earthquake prediction models. Moreover, the original loss functions of ANNs, such as Cross Entropy (CE), Balanced Cross Entropy (BCE), Focal Loss (FL), and Focal Loss alpha (FL-alpha), contain no knowledge about seismology. To differentiate the hard and easy examples of earthquake prediction models during the training steps of ANNs, the punishment of CE, BCE, FL, and FL-alpha for positive examples will be further weighted by P0 and the punishment for negative examples will be weighted by P1, where P1/P0 is the prior probability provided by the reference model that at least one or no earthquakes will occur for the given example and P1+P0=1. The reference models are supposed to be as close to the real spatial-temporal distribution of earthquakes as possible, and the spatial variable Poisson (SVP) model is the simplest version which is also friendly to data mining experts. In this work, we choose the SVP as the reference model to revise these previous loss functions and take the estimated cumulative earthquake energy in the time-space unit (1 degree*1 degree*10 days) as the input of the LSTM to predict the earthquakes with M>=5.0 in the whole Chinese Mainland, and we use the Molchan diagram (SVP) and Area Skill Score (ASS) to evaluate the performance of these models. Results show that the majority of models (134 out of 144) trained by original loss functions are ineffective for predicting earthquakes; however, the scores of models trained using the revised loss functions have been obviously improved, and 83 out of 144 models are proved to be better than SVP in predicting earthquakes. Our results indicate that designing a more complex structure for ANN and neuron is not the only way to improve the performance of ANNs for predicting earthquakes, and how combining the professional knowledge of data mining experts and seismologists deserves more attention for the future development of ANN-based earthquake prediction models.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-11684"
    },
    {
        "id": 6004,
        "title": "Faithful Model Evaluation for Model-Based Metrics",
        "authors": "Qian Hu, Palash Goyal, Rahul Gupta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.464"
    },
    {
        "id": 6005,
        "title": "Useful AI/ML model metrics and an application of a QSAR model to repurpose some marketed drugs for therapeutic evaluation pertinent to INSR",
        "authors": "Subhas J Chakravorty",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/scimeetings.0c02337"
    },
    {
        "id": 6006,
        "title": "Useful AI/ML model metrics and an application of a QSAR model to repurpose some marketed drugs for therapeutic evaluation pertinent to INSR",
        "authors": "Subhas J Chakravorty",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1021/scimeetings.0c02335"
    },
    {
        "id": 6007,
        "title": "Defining Data Model Quality Metrics for Data Vault 2.0 Model Evaluation",
        "authors": "Heli Helskyaho, Laura Ruotsalainen, Tomi Männistö",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "Designing a database is a crucial step in providing businesses with high-quality data for decision making. The quality of a data model is the key to the quality of its data. Evaluating the quality of a data model is a complex and time-consuming task. Having suitable metrics for evaluating the quality of a data model is an essential requirement for automating the design process of a data model. While there are metrics available for evaluating data warehouse data models to some degree, there is a distinct lack of metrics specifically designed to assess how well a data model conforms to the rules and best practices of Data Vault 2.0. The quality of a Data Vault 2.0 data model is considered suboptimal if it fails to adhere to these principles. In this paper, we introduce new metrics that can be used for evaluating the quality of a Data Vault 2.0 data model, either manually or automatically. This methodology involves defining a set of metrics based on the best practices of Data Vault 2.0, evaluating five representative data models using both metrics and manual assessments made by a human expert. Finally, a comparative analysis of both evaluations was conducted to validate the consistency of the metrics with the judgments made by a human expert.",
        "link": "http://dx.doi.org/10.3390/inventions9010021"
    },
    {
        "id": 6008,
        "title": "ARM Data-Oriented Metrics and Diagnostics Package for Climate Model Evaluation",
        "authors": "Chengzhu Zhang,  , Shaocheng Xie, Cheng Tao,  ,  ,  ",
        "published": "2020-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1728458"
    },
    {
        "id": 6009,
        "title": "A Class of Model Predictive Safety Performance Metrics for Driving Behavior Evaluation",
        "authors": "Bowen Weng",
        "published": "2021-9-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9565013"
    },
    {
        "id": 6010,
        "title": "An Evaluation of Validation Metrics for Probabilistic Model Outputs",
        "authors": "Paul Gardner, Charles Lord, Robert J. Barthorpe",
        "published": "2018-5-16",
        "citations": 2,
        "abstract": "Probabilistic modelling methods are increasingly being employed in engineering applications. These approaches make inferences about the distribution, or summary statistical moments, for output quantities. A challenge in applying probabilistic models is validating output distributions. An ideal validation metric is one that intuitively provides information on key divergences between the output and validation distributions. Furthermore, it should be interpretable across different problems in order to informatively select the appropriate statistical method. In this paper, two families of measures for quantifying differences between distributions are compared: f-divergence and integral probability metrics (IPMs). Discussions and evaluation of these measures as validation metrics are performed with comments on ease of computation, interpretability and quantity of information provided.",
        "link": "http://dx.doi.org/10.1115/vvs2018-9327"
    },
    {
        "id": 6011,
        "title": "Understanding Large Language Model Based Metrics for Text Summarization",
        "authors": "Abhishek Pradhan, Ketan Todi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.eval4nlp-1.12"
    },
    {
        "id": 6012,
        "title": "ARM data-oriented metrics and diagnostics package for climate model evaluation version 3 (ARM-DIAGS-V3)",
        "authors": "X Zheng, C Tao, C Zhang, S Xie",
        "published": "2022-10-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2318779"
    },
    {
        "id": 6013,
        "title": "Classification Model Evaluation Metrics",
        "authors": "Željko Ð. Vujovic",
        "published": "2021",
        "citations": 51,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2021.0120670"
    },
    {
        "id": 6014,
        "title": "System Description for the CommonGen task with the POINTER model",
        "authors": "Anna Shvets",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.gem-1.15"
    },
    {
        "id": 6015,
        "title": "Evaluation of Prediction-Oriented Model Selection Metrics for Extended Redundancy Analysis",
        "authors": "Sunmee Kim, Heungsun Hwang",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "Extended redundancy analysis (ERA) is a statistical method that relates multiple sets of predictors to response variables. In ERA, the conventional approach of model evaluation tends to overestimate the performance of a model since the performance is assessed using the same sample used for model development. To avoid the overly optimistic assessment, we introduce a new model evaluation approach for ERA, which utilizes computer-intensive resampling methods to assess how well a model performs on unseen data. Specifically, we suggest several new model evaluation metrics for ERA that compute a model’s performance on out-of-sample data, i.e., data not used for model development. Although considerable work has been done in machine learning and statistics to examine the utility of cross-validation and bootstrap variants for assessing such out-of-sample predictive performance, to date, no research has been carried out in the context of ERA. We use simulated and real data examples to compare the proposed model evaluation approach with the conventional one. Results show the conventional approach always favor more complex ERA models, thereby failing to prevent the problem of overfitting in model selection. Conversely, the proposed approach can select the true ERA model among many mis-specified (i.e., underfitted and overfitted) models.",
        "link": "http://dx.doi.org/10.3389/fpsyg.2022.821897"
    },
    {
        "id": 6016,
        "title": "Developing Evaluation Metrics for Active Reading Support",
        "authors": "Nanna Inie, Louise Barkhuus",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010406701770188"
    },
    {
        "id": 6017,
        "title": "Online Adaptive Metrics for Model Evaluation on Non-representative Offline Test Data",
        "authors": "Enrico Piovano, Dieu-Thu Le, Bei Chen, Melanie Bradford",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956048"
    },
    {
        "id": 6018,
        "title": "Using a Markov cyberattack model for evaluation of security metrics",
        "authors": "A. A. Kassenov, A. A. Magazev, E. V. Trapeznikov",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "This paper presents a description of a Markov model of cyberattacks by which two security metrics are constructed. An algorithm is given for estimating the input parameters of the model based on a limited number of empirical data. An example that illustrates the use of the proposed security metrics is considered.",
        "link": "http://dx.doi.org/10.24147/2222-8772.2020.2.129-144"
    },
    {
        "id": 6019,
        "title": "Towards a Model-based Fuzzy Software Quality Metrics",
        "authors": "Omar Masmali, Omar Badreddin",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008913701390148"
    },
    {
        "id": 6020,
        "title": "Theory → Concepts → Measures but Policies → Metrics",
        "authors": "Robert R. Hoffman",
        "published": "2018-9-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315593173-2"
    },
    {
        "id": 6021,
        "title": "Evaluation summary and metrics: “Title of paper” (template)",
        "authors": "David Reinstein",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.3272dd18"
    },
    {
        "id": 6022,
        "title": "Evaluation Metrics and Evaluation",
        "authors": "Hercules Dalianis",
        "published": "2018",
        "citations": 89,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-78503-5_6"
    },
    {
        "id": 6023,
        "title": "Evaluation of keyness metrics: Reliability and interpretability",
        "authors": "Lukas Sönning",
        "published": "No Date",
        "citations": 2,
        "abstract": "While keyword analysis has become an essential tool in corpus-based work, the question of how to quantify keyness has been subject to considerable methodological debate. This has given rise to a variety of computerized metrics for detecting and ranking candidate items based on the comparison of a target to a reference corpus. Building on previous work, the present paper starts out by delineating four dimensions of keyness, which distinguish between frequency- and dispersion-related perspectives and identify substantively different aspects of typicalness. Existing measures are then organized according to these dimensions and evaluated with regard to two specific criteria, their interpretability and reliability. The first of these, which has been neglected in previous work, is a critical feature if metrics are to offer informative indications of keyness. The second criterion is performance-oriented and reflects the degree to which a metric produces stable and replicable rankings across repeated studies on the same pair of text varieties. Our illustrative analysis, which deals with the identification of key verbs in academic writing, shows considerable differences among indicators with regard to these two criteria. Our findings provide further support for the superiority of the Wilcoxon rank sum test and allow us to identify, within each dimension of keyness, metrics that may be given preference in applied work in light of our criteria.",
        "link": "http://dx.doi.org/10.31234/osf.io/eb2n9"
    },
    {
        "id": 6024,
        "title": "An ESG investment evaluation system based on metrics and model innovation",
        "authors": "Guanjun Xiao, Xueyan Wang, Yichen Hu, Xucheng Hu, Chirui Pan",
        "published": "2022-11-20",
        "citations": 0,
        "abstract": "In recent years, with the acceleration of the economic transformation in China, environmentally friendly and sustainable enterprises are more conducive the construction of a green economy. Environmental, Social and Governance (ESG) which can quantify the relationship between enterprises and the environment, has emerged as the times require. Therefore, based on the \"scoring model system for ESG information disclosure\", we simplified and reformulated the ESG investment evaluation system, and built iESG, an integrated green financial service platform for small, medium and micro enterprises. It fills the gap in the field of ESG evaluation in the market, and provides targeted ESG evaluation and related analysis and green financial services for small, medium and micro enterprises and independent investors.",
        "link": "http://dx.doi.org/10.54691/bcpbm.v33i.2789"
    },
    {
        "id": 6025,
        "title": "Longitudinal Evaluation of Software Quality Metrics in Open-Source Applications",
        "authors": "Arthur-Jozsef Molnar, Alexandra Neamţu, Simona Motogna",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007725600800091"
    },
    {
        "id": 6026,
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "authors": "Ziang Xiao, Susu Zhang, Vivian Lai, Q. Vera Liao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.676"
    },
    {
        "id": 6027,
        "title": "Evaluation as Power",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.002"
    },
    {
        "id": 6028,
        "title": "Delving into Evaluation Metrics for Generation: A Thorough Assessment of How Metrics Generalize to Rephrasing Across Languages",
        "authors": "Yixuan Wang, Qingyan Chen, Duygu Ataman",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.eval4nlp-1.3"
    },
    {
        "id": 6029,
        "title": "Evaluation summary and metrics: “Artificial Intelligence and Economic Growth”",
        "authors": "",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.10e0adbf"
    },
    {
        "id": 6030,
        "title": "Playing the Evaluation Game",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.007"
    },
    {
        "id": 6031,
        "title": "Benchmarking Hayai-Annotation Plants: A Re-evaluation Using Standard Evaluation Metrics",
        "authors": "Andrea Ghelfi, Kenta Shirasawa, Sachiko Isobe",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe rapid growth of next-generation sequencing (NGS) technology has led to a surge in the determination of whole genome sequences in plants. This has created a need for functional annotation of newly predicted gene sequences in the assembled genomes. To address this, “Hayai-Annotation Plants” was developed as a gene functional annotation tool for plant species. In this report, we compared Hayai-Annotation Plants with Blast2GO and TRAPID, focusing on the three primary gene-ontology (GO) domains: Biological Process (BP), Molecular Function (MF), and Cellular Component (CC). Using theArabidopsis thalianaGO annotation as a benchmark, we evaluated each tool using two approaches: the area under the precision-recall curve (AUC-PR) and the metrics used at the critical assessment of functional annotation (CAFA). In the latter case, a CAFA-evaluator, was used to determine the F-score, weighted F-score, and S-score for each domain. Hayai-Annotation Plants showed better performances in all three GO domains. Our results thus reaffirm the effectiveness of Hayai-Annotation Plants for functional gene annotation in plant species. In this era of extensive whole genome sequencing, Hayai-Annotation Plants will serve as a valuable tool that facilitates simplified and accurate gene function annotation for numerous users, thereby making a significant contribution to plant research.",
        "link": "http://dx.doi.org/10.1101/2023.09.08.556781"
    },
    {
        "id": 6032,
        "title": "Methods of data sample metrics evaluation based on fractal dimension for computational intelligence model buiding",
        "authors": "Sergey Subbotin",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocommst.2017.8246136"
    },
    {
        "id": 6033,
        "title": "Untold Histories of Research Evaluation",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.004"
    },
    {
        "id": 6034,
        "title": "Detecting Manuscript Annotations in Historical Print: Negative Evidence and Evaluation Metrics",
        "authors": "Jacob Murel, David Smith",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012365600003654"
    },
    {
        "id": 6035,
        "title": "Numerical Relationship Between Non-visual Metrics and Brightness Metrics - Implications for the Evaluation of Integrative Lighting Systems",
        "authors": "Khanh Quoc Tran, Vinh Quang Trinh, Peter Bodrogi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nFrom the beginning of the 21st century until today, the demand for lighting systems includes not only visual parameters (brightness, contrast perception, color quality), but also non-visual parameters. It is necessary to define the new non-visualparameters for the realization of the new concept of Human Centric Lighting (HCL) or Integrative Lighting. As a contribution tothis approach, many international research groups have tried to quantify the non-visual parameters such as Circadian Stimulus by M. Rea et. al. in USA (CS2018, CS2021), Melanopic Equivalent Daylight (D65) illuminance, mEDI of the CIE S 026/E:2018 or the latest formula by Giménez et al. for the nocturnal melatonin suppression. Therefore, it is necessary to analyze the relationship between these non-visual metrics and brightness metrics such as the equivalent luminance of Fotios et al. or the latest brightness model of TU Darmstadt so that scientists, lighting engineers and lighting system users can correctly apply them in their work. In this context, this paper attempts to investigate and analyze these relationships between the three metric groups based on the database of 884 light sources of different light source technologies and daylight spectra. The obtained results show that the latest Circadian Stimulus model of M. Rea et. al. CS2021 with the improvement of Circadian Light CLA,2021 (CLA 2.0) has solved the disadvantage of CS2018, especially for the interrupted point between warm and cold white (about 3710 K) or the junction between negative and positive signal of the opponent channel (B-(L + M)). Moreover,these three metrics of the three research groups contain a high correlation coefficient, so that one metric can be transformed by linear functions to the other two parameters.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2812113/v1"
    },
    {
        "id": 6036,
        "title": "An Off-line Evaluation of Users’ Ranking Metrics in Group Recommendation",
        "authors": "Silvia Rossi, Francesco Cervone, Francesco Barile",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006200702520259"
    },
    {
        "id": 6037,
        "title": "Measuring the Measuring Tools: An Automatic Evaluation of Semantic Metrics for Text Corpora",
        "authors": "George Kour, Samuel Ackerman, Eitan Daniel Farchi, Orna Raz, Boaz Carmeli, Ateret Anaby Tavor",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gem-1.35"
    },
    {
        "id": 6038,
        "title": "DELETE ME Evaluation summary and metrics: “Artificial Intelligence and Economic Growth",
        "authors": "Ezekiel Washburne",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.b24c1b96"
    },
    {
        "id": 6039,
        "title": "empirical evaluation of metrics on aspect-oriented programs",
        "authors": "Mazen Ismaeel Ghareb, Garry Allen",
        "published": "2019-10-23",
        "citations": 0,
        "abstract": " \r\n The quality evaluation of software metrics measurement is considered as the primary indicator of imperfection prediction and software maintenance in various empirical studies of software products. However, there is no agreement on which metrics are compelling quality pointers for new software development approaches such as aspect-oriented programming (AOP) techniques. AOP intends to enhance programming quality by providing fundamentally different parts of the systems, for example, pointcuts, advice, and intertype relationships. Hence, it is not evident if quality characteristics for AOP could be extracted from direct expansions of traditional object-oriented programming (OOP) measurements. Then again, investigations of AOP do regularly depend on established static and dynamic metrics measurement; notwithstanding the late research of AOP in empirical studies, few analyses been adopted using the International Organization for Standardization 9126 quality model as useful markers of flaw inclination in this context. This paper examination we have considered different programming quality models given by various authors every once in a while and distinguished that adaptability was deficient in the current model. We have testing 10 projects developed by AOP. We have used many applications to extract the metrics, but none of them could extract all AOP Metrics. It only can measure some of AOP Metrics, not all of them. This study investigates the suitable framework for extract AOP Metrics, for instance, static and dynamic metrics measurement for hybrid application systems (AOP and OOP) or only AOP application.",
        "link": "http://dx.doi.org/10.21928/uhdjst.v3n2y2019.pp74-86"
    },
    {
        "id": 6040,
        "title": "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gem-1"
    },
    {
        "id": 6041,
        "title": "Metrics and Marketing--Model",
        "authors": "Pravin Nath",
        "published": "2021-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/t81775-000"
    },
    {
        "id": 6042,
        "title": "Model-Based Metrics for Bulk Transport Capacity",
        "authors": "M. Mathis, A. Morton",
        "published": "2018-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17487/rfc8337"
    },
    {
        "id": 6043,
        "title": "Were You Successful? Evaluation and Metrics",
        "authors": "",
        "published": "2019-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108377577.018"
    },
    {
        "id": 6044,
        "title": "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.gem-1"
    },
    {
        "id": 6045,
        "title": "An Experimental Evaluation of Relations Between Architectural and Runtime Metrics in Microservices Systems",
        "authors": "Niels Knoll, Robin Lichtenthäler",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011728600003488"
    },
    {
        "id": 6046,
        "title": "False data injection attack (FDIA): an overview and new metrics for fair evaluation of its countermeasure",
        "authors": "Mohiuddin Ahmed, Al-Sakib Khan Pathan",
        "published": "2020-12",
        "citations": 61,
        "abstract": "AbstractThe concept of false data injection attack (FDIA) was introduced originally in the smart grid domain. While the term sounds common, it specifically means the case when an attacker compromises sensor readings in such tricky way that undetected errors are introduced into calculations of state variables and values. Due to the rapid growth of the Internet and associated complex adaptive systems, cyber attackers are interested in exploiting similar attacks in other application domains such as healthcare, finance, defense, governance, etc. In today’s increasingly perilous cyber world of complex adaptive systems, FDIA has become one of the top-priority issues to deal with. It is a necessity today for greater awareness and better mechanism to counter such attack in the cyberspace. Hence, this work presents an overview of the attack, identifies the impact of FDIA in critical domains, and talks about the countermeasures. A taxonomy of the existing countermeasures to defend against FDIA is provided. Unlike other works, we propose some evaluation metrics for FDIA detection and also highlight the scarcity of benchmark datasets to validate the performance of FDIA detection techniques.",
        "link": "http://dx.doi.org/10.1186/s40294-020-00070-w"
    },
    {
        "id": 6047,
        "title": "On the nature of information access evaluation metrics: a unifying framework",
        "authors": "Enrique Amigó, Stefano Mizzaro",
        "published": "2020-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10791-020-09374-0"
    },
    {
        "id": 6048,
        "title": "Evaluation Summary and Metrics: \"Advance Market Commitments: Insights from Theory and Experience\"",
        "authors": "David Reinstein",
        "published": "2023-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.652a843a"
    },
    {
        "id": 6049,
        "title": "Metrics Model to Complement the Evaluation of DevOps in Software Companies",
        "authors": "Carlos-Eduardo Orozco-Garcés, César-Jesús Pardo-Calvache, Elizabeth Suescún-Monsalve",
        "published": "2022-10-4",
        "citations": 0,
        "abstract": "This article presents a model to complement the evaluation of DevOps in software companies. It was designed by harmonizing the elements of the DevOps process identified through a systematic mapping of the literature and aimed to know the state of the art of methodological solutions and tools to evaluate DevOps in the industry. The process elements were identified, compared, and integrated into a common process structure that was used to establish a total of 11 metrics using the Goal-Question-Metric approach. The model was evaluated by a focus group of expert DevOps professionals. They determined that the model is clear, easy to apply, and provides valuable information to companies to improve their DevOps practices.",
        "link": "http://dx.doi.org/10.19053/01211129.v31.n62.2022.14766"
    },
    {
        "id": 6050,
        "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
        "authors": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.501"
    },
    {
        "id": 6051,
        "title": "Integration of Hawk for Model Metrics in the MEASURE Platform",
        "authors": "Orjuwan Al-Wadeai, Antonio Garcia-Domínguez, Alessandra Bagnato, Antonin Abherve, Konstantinos Barmpis",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006732207190730"
    },
    {
        "id": 6052,
        "title": "Alternative article‐level metrics",
        "authors": "Lutz Bornmann, Robin Haunschild",
        "published": "2018-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15252/embr.201847260"
    },
    {
        "id": 6053,
        "title": "Evaluation of Quality Control in Clinical Chemistry Using Sigma Metrics",
        "authors": "Caroline Njeru, Angela Amayo",
        "published": "No Date",
        "citations": 0,
        "abstract": "IntroductionSix sigma improves the quality of outputs by analyzing and abolishing the source of defectsand reducing variability in the manufacturing industry. It can be used as a self -assessmenttool for laboratories when making their quality control frequency and strategy. Clinicallaboratories tests with low sigma values (&lt; 3σ) indicate that the analytical quality of the labneeds to be improved.AimThe aim of the study was to evaluate clinical chemistry laboratory performance using sixsigma metrics to improve quality.Materials and methodsFive parameters from renal and liver function tests were studied over a period of 6 months(Dec 2016-May2017). Data from IQC and EQA participation was used. The analytes wereplasma creatinine, aspartate transaminase(AST), alanine transaminase(ALT), total serumprotein, total and direct bilirubin.Sigma metrics was calculated using total allowable error as per CLIA recommendations. Biaswas calculated from HUQAS EQA participation while coefficient of variation was calculatedfrom IQC data collected during the above mentioned months.Results.Clinical chemistry had sigma metrics below 3, the highest sigma value was 2.01 while thelowest sigma value was 0.85.ConclusionClinical chemistry analytes had sigma levels less than 3, method performance improvementwith stringent internal quality control and correct setting of control limits need to be applied.Application of sigma metrics in addition to daily internal quality control can identifyanalytical deficits and improvement in clinical laboratories",
        "link": "http://dx.doi.org/10.31730/osf.io/5gjc2"
    },
    {
        "id": 6054,
        "title": "Experimenting with Linear Optimization of Metrics for Single-Document Summarization Evaluation",
        "authors": "Jonathan Rojas-Simon, Yulia Ledeneva, Rene Arnulfo Garcia-Hernandez",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-07214-7_7"
    },
    {
        "id": 6055,
        "title": "Experimenting with Linear Optimization of Metrics for Multi-document Summarization Evaluation",
        "authors": "Jonathan Rojas-Simon, Yulia Ledeneva, Rene Arnulfo Garcia-Hernandez",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-07214-7_8"
    },
    {
        "id": 6056,
        "title": "Introduction",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.001"
    },
    {
        "id": 6057,
        "title": "Conclusions",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.008"
    },
    {
        "id": 6058,
        "title": "Evaluation Metrics for Galaxy Image Generators",
        "authors": "Stefan Hackstein, Vitaliy Kinakh, Christian Bailer, Martin Melchior",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4276472"
    },
    {
        "id": 6059,
        "title": "The Evaluation Game",
        "authors": "Emanuel Kulczycki",
        "published": "2023-4-27",
        "citations": 6,
        "abstract": "Scientific research is communicated, organized, financed, governed, and evaluated through the process of publication. The result of this process is a highly competitive academic environment that rewards researchers for high volume publication, preferably in high-impact journals, leading to the popularised expression 'publish or perish'. Universities and other scientific institutions are under similar pressure, with their aggregated research output being under constant scrutiny. This innovative text provides a detailed introduction to the origin and development of the scholarly metrics used to measure academic productivity, and the effect they have upon the quality and diversity of scientific research. With its careful attention to both the positive and negative outcomes of research evaluation and their distinct expressions around the globe, The Evaluation Game guides the way to a more grounded understanding of metrics, and the diverse academic cultures they give rise to.",
        "link": "http://dx.doi.org/10.1017/9781009351218"
    },
    {
        "id": 6060,
        "title": "Semantic Similarity as a Window into Vector- and Graph-Based Metrics",
        "authors": "Wai Ching Leung, Shira Wein, Nathan Schneider",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gem-1.8"
    },
    {
        "id": 6061,
        "title": "Evaluation of Automatic Segmentation Model With Dosimetric Metrics for Radiotherapy of Esophageal Cancer",
        "authors": "Ji Zhu, Xinyuan Chen, Bining Yang, Nan Bi, Tao Zhang, Kuo Men, Jianrong Dai",
        "published": "2020-9-29",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3389/fonc.2020.564737"
    },
    {
        "id": 6062,
        "title": "Altmetrics: alternative metrics for scientific, technological and innovation evaluation",
        "authors": "Roelvis Ortiz Núñez",
        "published": "2021-7-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20935/al1658"
    },
    {
        "id": 6063,
        "title": "Evaluation Summary and Metrics: “The Environmental Effects of Economic Production: Evidence from Ecological Observations”",
        "authors": "Tanya O'Garra",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.a3e2b6b7"
    },
    {
        "id": 6064,
        "title": "REAM♯: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation",
        "authors": "Jun Gao, Wei Bi, Ruifeng Xu, Shuming Shi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.220"
    },
    {
        "id": 6065,
        "title": "Evaluation of Semantic Answer Similarity Metrics",
        "authors": "Farida Mustafazade, Peter F. Ebbinghaus",
        "published": "2022-6-25",
        "citations": 0,
        "abstract": "There are several issues with the existing general machine translation or natural language generation evaluation metrics, and question-answering (QA) systems are indifferent in that context. To build robust QA systems, we need the ability to have equivalently robust evaluation systems to verify whether model predictions to questions are similar to ground-truth annotations. The ability to compare similarity based on semantics as opposed to pure string overlap is important to compare models fairly and to indicate more realistic acceptance criteria in real-life applications. We build upon the first to our knowledge paper that uses transformer-based model metrics to assess semantic answer similarity and achieve higher correlations to human judgement in the case of no lexical overlap. We propose cross-encoder augmented bi-encoder and BERTScore models for semantic answer similarity, trained on a new dataset consisting of name pairs of US-American public figures. As far as we are concerned, we provide the first dataset of co-referent name string pairs along with their similarities, which can be used both for training and as a benchmark.",
        "link": "http://dx.doi.org/10.5121/csit.2022.121109"
    },
    {
        "id": 6066,
        "title": "Challenging Recommendation Engines Evaluation Metrics and Mitigating Bias Problem of Information Cascades and Confirmation Biases",
        "authors": "Guillaume Blot, Francis Rousseaux, Pierre Saurel",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006581803930400"
    },
    {
        "id": 6067,
        "title": "Application-Based Usability Evaluation Metrics",
        "authors": "Noura A. Sayed, Hesham Hassan, Khaled Wassif, Hanaa Bayomi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTesting is one of the critical stages in the software development life cycle (SDLC). Usability testing is a very important field that helps the applications be usable and easy for the users. Because of the importance of usability testing, a metrics has been developed to help in measuring the usability through converting the main qualitative usability attributes in ISO to quantitative steps that provide the developer a framework to follow in developing to achieve usability of their applications and helps the tester with a checklist and a tool to measure the usability percentage of their application. The framework provides a set of steps to achieve the usability attributes and answers the question of how you could measure this attribute with the defined steps. The framework results in a 95% average accuracy in the high-rate application and a 59% average accuracy in the low-rate application. Finally, the framework is programmed in a tool to measure the usability percentage of the application through a checklist and provides a scheme to help the developer achieve the best results in usability.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1769489/v1"
    },
    {
        "id": 6068,
        "title": "Introduction to an open-source tool for collective Earth System Model evaluation and benchmarking: PCMDI Metrics Package (PMP)",
        "authors": "Jiwoo Lee, Ana Ordonez, Peter Gleckler, Paul Ullrich, Yann Planton, Bo Dong, Kristin Chang, Paul Durack, Elina Valkonen, Julie Caron",
        "published": "No Date",
        "citations": 0,
        "abstract": "The PCMDI Metrics Package (PMP) is an open-source Python-based framework that enables objective \"quick-look\" comparisons and benchmarking of Earth System Models (ESMs) against observation-based reference datasets. The PMP, which is focused primarily on atmospheric quantities, has been used for routine and systematic evaluation of thousands of simulations from the Coupled Model Intercomparison Project (CMIP). Ongoing work aims for seamless application of the tool to the next generation of CMIP (CMIP7), with an aspiration to aid modeling groups during their development cycle. The latest version of PMP offers a diverse suite of evaluation capabilities covering large- to global-scale climatology and annual cycle, variability modes such as tropical and extratropical variability modes e.g., ENSO and MJO, regional monsoons, cloud feedback, and high frequency characteristics e.g., extremes. Current work is expanding the scope of PMP to include the evaluation of the following: (1) Quasi-Biennial Oscillation (QBO) and its teleconnection to MJO, (2) atmospheric blocking and rivers leveraging Machine Learning based pattern detection algorithms, and (3) polar and high-latitude regions by implementing sectional sea-ice area metrics. The PMP is also advancing its evaluation capabilities to help evaluate higher resolution simulations such as those from the HighResMIP, cloud-resolving E3SM experiments, and regionally downscaled products. This presentation will highlight the motivation for routine model evaluation, introduce the PMP, share progress on current polar metrics, and discuss future plans and opportunities to connect with ongoing efforts.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-7132"
    },
    {
        "id": 6069,
        "title": "Constructing Better Evaluation Metrics by Incorporating the Anchoring Effect into the User Model",
        "authors": "Nuo Chen, Fan Zhang, Tetsuya Sakai",
        "published": "2022-7-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3477495.3531953"
    },
    {
        "id": 6070,
        "title": "Social Based Mobility Model with Metrics for Evaluation of Social Behaviour in Mobility Models for MANET-DTN Networks",
        "authors": "David Hrabcak, Martin Matis, Lubomir Dobos, Jan Papaj",
        "published": "2017-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15598/aeee.v15i4.2384"
    },
    {
        "id": 6071,
        "title": "Metrics and Monitoring",
        "authors": "",
        "published": "2017-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b17815-9"
    },
    {
        "id": 6072,
        "title": "A New Evaluation Method: Evaluation Data and Metrics for Chinese Grammatical Error Correction",
        "authors": "Nankai Lin, Yingwen Fu, Xiaotian Lin, Ziyu Yang, Shengyi Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs a fundamental task in natural language processing (NLP), Chinese Grammatical Error Correction (CGEC) [1–3] has gradually received widespread attention and become a research hotspot. However, one obvious deficiency of the existing CGEC evaluation systems is that the evaluation values of the same error correction models are signif- icantly influenced by the Chinese word segmentation (CWS) results or different language models. However, it is expected that these met- rics should be independent of the CWS results and language models for a fair evaluation. To this end, we propose three novel eval- uation metrics for CGEC in two dimensions: reference-based and reference-less. What’s more, according to these three evaluation met- rics, we build a new evaluation metric that can comprehensively evaluate the CGEC model from multiple dimensions. We deeply eval- uate and analyze the reasonableness and validity of the proposed metrics, and we expect them to become a new standard for CGEC.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2299197/v1"
    },
    {
        "id": 6073,
        "title": "Evaluation processes and metrics",
        "authors": "Guy André Boy",
        "published": "2020-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429351686-9"
    },
    {
        "id": 6074,
        "title": "Evaluation Metrics for Structured Text Retrieval",
        "authors": "Jovan Pehcevski, Benjamin Piwowarski",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4614-8265-9_152"
    },
    {
        "id": 6075,
        "title": "Evaluation Metrics for AI",
        "authors": "Lucas D, Hurbert K, Rusell E",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4712915"
    },
    {
        "id": 6076,
        "title": "Advancing Real-world Image Dehazing: A Comprehensive Dataset and Evaluation Metrics",
        "authors": "Luigi Celona, Flavio Piccoli",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe majority of dehazing techniques in the literature have been designed to learn from supervised datasets, which typically comprise pairs of images, one with haze and one without haze, to facilitate the learning process. These datasets often contain synthesized hazy images, either created by leveraging the theoretical model of haze creation, or by exploiting professional haze generation machines.\nHowever, the lack of realism given by these generative processes has led to a limited capacity of dehazing techniques to effectively generalize on real-world images. Recently, a collection of real-world hazy image datasets has been collected, but with a reduced cardinality.\nThis study aims to address the aforementioned issues and introduce the Unpaired Real-world Hazy Image Dataset (URHID). It is a large-scale benchmark dataset containing 13,329 real-world hazy images scraped from the Internet. In addition, a total of eight state-of-the-art dehazing methods were chosen to assess the performance of URHID.\nGiven the lack of no-reference measures in the existing literature for evaluating the quality of unpaired dehazed images, this study aims to address this gap by proposing the Multi-Aspect Dehazed Image Quality Assessment (MADIQA) dataset. A collection of dehazed images annotated in terms of multiple quality-related aspects as well as overall quality. Furthermore, we develop multiple No-Reference Dehazed Image Quality Assessment (NR-DIQA) metrics. The proposed URHID and MADIQA datasets, as well as the proposed DIQA metrics are publicly available for research purposes at: https://celuigi.github.io/arid.github.io/.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3917740/v1"
    },
    {
        "id": 6077,
        "title": "Evaluation Summary and Metrics: \"Do Celebrity Endorsements Matter? A Twitter Experiment Promoting Vaccination In Indonesia\"",
        "authors": "David Reinstein",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.f5b7e124"
    },
    {
        "id": 6078,
        "title": "Power Grid Computational Challenges and Metrics for Hardware Accelerator Evaluation",
        "authors": "Shrirang Abhyankar, Slaven Peles, Draguna Vrabie",
        "published": "2019-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1989488"
    },
    {
        "id": 6079,
        "title": "Measuring post-purchase evaluation",
        "authors": "Jacek Kall",
        "published": "2021-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003167235-5"
    },
    {
        "id": 6080,
        "title": "Economization and Metricization",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.003"
    },
    {
        "id": 6081,
        "title": "Quantifying Uncertainty in Aggregate Evaluation Metrics for Foundation Models [Presentation]",
        "authors": "Rachel Longjohn, Emily Casleton, Giridhar Gopalan",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2007329"
    },
    {
        "id": 6082,
        "title": "Evaluation complacency or evaluation inertia? A study of evaluative metrics and research practices in Irish universities",
        "authors": "Lai Ma, Michael Ladisch",
        "published": "2019-7-1",
        "citations": 19,
        "abstract": "Abstract\nEvaluative metrics have been used for research assessment in most universities and funding agencies with the assumption that more publications and higher citation counts imply increased productivity and better quality of research. This study investigates the understanding and perceptions of metrics, as well as the influences and implications of the use of evaluative metrics on research practices, including choice of research topics and publication channels, citation behavior, and scholarly communication in Irish universities. Semi-structured, in-depth interviews were conducted with researchers from the humanities, the social sciences, and the sciences in various career stages. Our findings show that there are conflicting attitudes toward evaluative metrics in principle and in practice. The phenomenon is explained by two concepts: evaluation complacency and evaluation inertia. We conclude that evaluative metrics should not be standardized and institutionalized without a thorough examination of their validity and reliability and without having their influences on academic life, research practices, and knowledge production investigated. We also suggest that an open and public discourse should be supported for the discussion of evaluative metrics in the academic community.",
        "link": "http://dx.doi.org/10.1093/reseval/rvz008"
    },
    {
        "id": 6083,
        "title": "Forging New Evaluation Paradigms: Beyond Statistical Generalization",
        "authors": "Emilie M. Roth, Robert G. Eggleston",
        "published": "2018-9-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315593173-16"
    },
    {
        "id": 6084,
        "title": "Multilingual Social Media Text Generation and Evaluation with Few-Shot Prompting",
        "authors": "Mack Blackburn",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.gem-1.39"
    },
    {
        "id": 6085,
        "title": "System Evaluation Using the Cognitive Performance Indicators",
        "authors": "Sterling L. Wiggins, Donald A. Cox",
        "published": "2018-9-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315593173-20"
    },
    {
        "id": 6086,
        "title": "Automatic Meta-evaluation of Low-Resource Machine Translation Evaluation Metrics",
        "authors": "Junting Yu, Wuying Liu, Hongye He, Lin Wang",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037658"
    },
    {
        "id": 6087,
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "authors": "Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.674"
    },
    {
        "id": 6088,
        "title": "Players and the Stakes",
        "authors": "",
        "published": "2023-4-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009351218.006"
    },
    {
        "id": 6089,
        "title": "Image Captioning: Methods and Evaluation Metrics",
        "authors": "Himanshu Sharma, Swati Srivastava",
        "published": "2022-6-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit55038.2022.9847918"
    },
    {
        "id": 6090,
        "title": "Comparative Evaluation of Different Spinal Stability Metrics",
        "authors": "Amir Hossein Eskandari, Farshid Ghezelbash, Aboulfazl Shirazi-Adl, Christian Larivière,",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4455773"
    },
    {
        "id": 6091,
        "title": "Evaluation summary and metrics: “Banning wildlife trade can boost demand for unregulated threatened species”",
        "authors": "Tanya O'Garra",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/d28e8e57.ad95c497"
    },
    {
        "id": 6092,
        "title": "Macrocognition Metrics and Scenarios",
        "authors": "",
        "published": "2018-9-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315593173"
    },
    {
        "id": 6093,
        "title": "Empirical UX Evaluation: UX Goals, Metrics, and Targets",
        "authors": "Rex Hartson, Pardha Pyla",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805342-3.00022-9"
    },
    {
        "id": 6094,
        "title": "A metrics for ‘Point-Ellipse’ distance evaluation in 2D",
        "authors": "Fabio Stroppa, Mine Sarac",
        "published": "2018-7-11",
        "citations": 1,
        "abstract": "This document proposes a method to define the distance between a set of points and an ellipse, which can<br/>be used as cost function of a Genetic Algorithm - or any other kind of solvers - to solve the ellipse fitting<br/>problem. This method involves a metrics to retrieve the correct polygon edge to be compared with each<br/>point of the set, featuring linear time complexity.<br/> ",
        "link": "http://dx.doi.org/10.29007/k13p"
    },
    {
        "id": 6095,
        "title": "Evaluation of Industrial Routes to Vinyl Chloride According to Material and Energy Efficiency Green Metrics Analysis",
        "authors": "john andraos",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work presents a detailed analysis of various industrial syntheses of vinyl chloride according to material and energy efficiency green metrics. The routes examined are hydrochlorination of acetylene, oxyhydrochlorination of ethane, and tandem hydrochlorination of ethylene and dehydrochlorination of 1,2-dichloroethane. The green metrics selected are process mass intensity (PMI) and input enthalpy per unit mass of product measured in MJ per ton.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7688183"
    },
    {
        "id": 6096,
        "title": "Validation and Evaluation Metrics",
        "authors": "Jing Wang, Xiaofeng Yang",
        "published": "2023-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003243458-24"
    },
    {
        "id": 6097,
        "title": "Physically Unclonable Functions: Design Principles and Evaluation Metrics",
        "authors": "Basel Halak",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-76804-5_2"
    },
    {
        "id": 6098,
        "title": "Evaluation of Analysis by Cross-Validation. Part I: Using Verification Metrics",
        "authors": "Richard Menard, Martin Deshaies-Jacques",
        "published": "No Date",
        "citations": 3,
        "abstract": "We examine how observations can be used to evaluate an air quality analysis by verifying against passive observations (i.e. cross-validation) that are not used to create the analysis and we compare these verifications to those made against the same set of (active) observations that were used to generate the analysis. The results show that both active and passive observations can be used to evaluate of first moment metrics (e.g. bias) but only passive observations are useful to evaluate second moment metrics such as variance of observed-minus-analysis and correlation between observations and analysis. We derive a set of diagnostics based on passive observation&ndash;minus-analysis residuals and we show that the true analysis error variance can be estimated, without relying on any statistical optimality assumption. This diagnostic is used to obtain near optimal analyses that are then used to evaluate the analysis error using several different methods. We compare the estimates according to the method of Hollingsworth Lonnberg, Desroziers, a diagnostic we introduce, and the perceived analysis error computed from the analysis scheme, to conclude that as long as the analysis is optimal, all estimates agrees within a certain error margin. The analysis error variance at passive observation sites is also obtained.",
        "link": "http://dx.doi.org/10.20944/preprints201801.0217.v1"
    },
    {
        "id": 6099,
        "title": "Metrics of Project Evaluation",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-17299-1_302667"
    },
    {
        "id": 6100,
        "title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics",
        "authors": "Daniil Larionov, Jens Grünwald, Christoph Leiter, Steffen Eger",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.7"
    }
]