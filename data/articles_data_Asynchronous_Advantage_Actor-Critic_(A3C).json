[
    {
        "id": 16871,
        "title": "Actor-Critic Models and the A3C",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_11"
    },
    {
        "id": 16872,
        "title": "A3C in Code",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_12"
    },
    {
        "id": 16873,
        "title": "M-A3C: A Mean-Asynchronous Advantage Actor-Critic Reinforcement Learning Method for Real-Time Gait Planning of Biped Robot",
        "authors": "Jie Leng, Suozhong Fan, Jun Tang, Haiming Mou, Junxiao Xue, Qingdu Li",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3176608"
    },
    {
        "id": 16874,
        "title": "Group Random Access Control Scheme Based on Asynchronous Advantage Actor Critic",
        "authors": "Su Kim, Han-Seung Jang",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.2.258"
    },
    {
        "id": 16875,
        "title": "Asynchronous Advantage Actor-Critic (A3C) Learning for Cognitive Network Security",
        "authors": "Eric Muhati, Danda B. Rawat",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpsisa52974.2021.00012"
    },
    {
        "id": 16876,
        "title": "Deep Reinforcement Learning Techniques For Solving Hybrid Flow Shop Scheduling Problems: Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C)",
        "authors": "Abdulrahman Nahhas, Andrey Kharitonov, Klaus Turowski",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24251/hicss.2022.206"
    },
    {
        "id": 16877,
        "title": "VMP-A3C: Virtual machines placement in cloud computing based on asynchronous advantage actor-critic algorithm",
        "authors": "Pengcheng Wei, Yushan Zeng, Bei Yan, Jiahui Zhou, Elaheh Nikougoftar",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jksuci.2023.04.002"
    },
    {
        "id": 16878,
        "title": "Follow then Forage Exploration: Improving Asynchronous Advantage Actor Critic",
        "authors": "James B. Holliday, T.H. Ngan Le",
        "published": "2020-7-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/csit.2020.100909"
    },
    {
        "id": 16879,
        "title": "Crawling the Deep Web Using Asynchronous Advantage Actor Critic Technique",
        "authors": "Kapil Madan, Rajesh Bhatia",
        "published": "2021-6-10",
        "citations": 0,
        "abstract": "In the digital world, World Wide Web magnitude is expanding very promptly. Now a day, a rising number of data-centric websites require a mechanism to crawl the information. The information accessible through hyperlinks can easily be retrieved with general-purpose search engines. A massive chunk of the structured information is invisible behind the search forms. Such immense information is recognized as the deep web and has structured information as compared to the surface web. Crawling the content of deep web is very challenging and requires filling the search forms with suitable queries. This paper proposes an innovative technique using an Asynchronous Advantage Actor-Critic (A3C) to explore the unidentified deep web pages. It is based on the policy gradient deep reinforcement learning technique that parameterizes the policy and value function based on the reward system. A3C has one coordinator and various agents. These agents learn from different environments, update the local gradients to a coordinator, and produce a more stable system. The proposed technique has been validated with Open Directory Project (ODP). The experimental outcome shows that the proposed technique outperforms most of the prevailing techniques based on various metrics such as average precision-recall, average harvest rate, and coverage ratio.",
        "link": "http://dx.doi.org/10.13052/jwe1540-9589.20314"
    },
    {
        "id": 16880,
        "title": "An accelerated asynchronous advantage actor-critic algorithm applied in papermaking",
        "authors": "Xuechun Wang, Zhiwei Zhuang, Luobao Zou, Weidong Zhang",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866243"
    },
    {
        "id": 16881,
        "title": "Multi-Objective Prioritized Task Scheduler Using Improved Asynchronous Advantage Actor Critic (a3c) Algorithm in Multi Cloud Environment",
        "authors": "S. Sudheer Mangalampalli, Ganesh Reddy Karri, Sachi Nandan Mohanty, Shahid Ali, Muhammad Ijaz Khan, Sherzod Abdullaev, Salman A. AlQahtani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3355092"
    },
    {
        "id": 16882,
        "title": "Data Transmission Evaluation and Allocation Mechanism of the Optimal Routing Path: An Asynchronous Advantage Actor-Critic (A3C) Approach",
        "authors": "Yahui Ding, Jianli Guo, Xu Li, Xiujuan Shi, Peng Yu",
        "published": "2021-9-2",
        "citations": 0,
        "abstract": "The delay tolerant networks (DTN), which have special features, differ from the traditional networks and always encounter frequent disruptions in the process of transmission. In order to transmit data in DTN, lots of routing algorithms have been proposed, like “Minimum Expected Delay,” “Earliest Delivery,” and “Epidemic,” but all the above algorithms have not taken into account the buffer management and memory usage. With the development of intelligent algorithms, Deep Reinforcement Learning (DRL) algorithm can better adapt to the above network transmission. In this paper, we firstly build optimal models based on different scenarios so as to jointly consider the behaviors and the buffer of the communication nodes, aiming to ameliorate the process of the data transmission; then, we applied the Deep Q-learning Network (DQN) and Advantage Actor-Critic (A3C) approaches in different scenarios, intending to obtain end-to-end optimal paths of services and improve the transmission performance. In the end, we compared algorithms over different parameters and find that the models build in different scenarios can achieve 30% end-to-end delay decline and 80% throughput improvement, which show that our algorithms applied in are effective and the results are reliable.",
        "link": "http://dx.doi.org/10.1155/2021/6685722"
    },
    {
        "id": 16883,
        "title": "A Reinforcement Method for Passenger Flow Control Based on Asynchronous Advantage Actor-Critic Neural Network",
        "authors": "Bao Wang, Peter J. Jin, Xia Luo, Qiming Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nEffective passenger flow management is critical for improving service quality and alleviating congestion in metro networks. However, the dynamic nature of travel demand and the complex structure of metro networks present significant challenges in building and solving control models. Additionally, the high computational costs of existing methods limit their practical applications. To address these challenges, this study proposes a new reinforcement learning (RL) based method for passenger flow control. The method has three components: the network state characterization, the control model, and the reinforcement learning model. Then, the study outlines the “action”, “state”, and “reward” concepts in RL based on the definition of decision variables, constraints, and objective functions in the constructed passenger flow control programming model. An iterative interaction mechanism is introduced to synchronize the control schemes generated by the reinforcement learning unit and the network states. Furthermore, effectively utilizing computational resources, the Asynchronous Advantage Actor-Critic Neural Network (A3C-NN) is trained to optimize the complex programming model. Finally, the proposed approach is validated through a case study using data from Chengdu Urban Rail Transit (URT), demonstrating its effectiveness in achieving various objectives, such as minimizing passenger waiting time, maximizing passenger turnover, and maximizing passenger numbers.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3247340/v1"
    },
    {
        "id": 16884,
        "title": "Decentralized Multi-Agent Advantage Actor-Critic",
        "authors": "Scott Barnes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>We present a decentralized advantage actor-critic algorithm that utilizes learning agents in parallel environments with synchronous gradient descent. This approach decorrelates agents’ experiences, stabilizing observations and eliminating the need for a replay buffer, requires no knowledge of the other agents’ internal state during training or execution, and runs on a single multi-core CPU.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19166384.v1"
    },
    {
        "id": 16885,
        "title": "Decentralized Multi-Agent Advantage Actor-Critic",
        "authors": "Scott Barnes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>We present a decentralized advantage actor-critic algorithm that utilizes learning agents in parallel environments with synchronous gradient descent. This approach decorrelates agents’ experiences, stabilizing observations and eliminating the need for a replay buffer, requires no knowledge of the other agents’ internal state during training or execution, and runs on a single multi-core CPU.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19166384"
    },
    {
        "id": 16886,
        "title": "Asynchronous Advantage Actor-Critic Algorithms Based on Residual Networks",
        "authors": "Lili Tang",
        "published": "2019-8-1",
        "citations": 1,
        "abstract": "Abstract\nDeep reinforcement learning is one of the fastest-growing technologies in machine learning. The Asynchronous Advantage Actor-Critic algorithm completely uses the actor-critic framework and utilizes the idea of asynchronous training, which greatly speeds up the training and improves performance. Although A3C algorithm puts actor-critic into multiple threads to train synchronously, effectively utilizes computer resources and improves training effectiveness, it is still difficult to train in deep neural network. Deep networks have proved to be capable of extending to thousands of layers and still have improved performance. However, every one percent increase in accuracy almost doubles the cost of layers, so it is not easy for A3C to train both actor and critic networks. In response to this problem, we innovatively utilize the residual network to apply to the asynchronous advantage actor-critic algorithm and has achieved improvement greatly in the inverted pendulum problem.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1302/2/022061"
    },
    {
        "id": 16887,
        "title": "Research on Automatic Train Operation Performance Optimization of High Speed Railway Based on Asynchronous Advantage Actor-Critic",
        "authors": "Hao Liang, Yong Zhang",
        "published": "2020-11-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327330"
    },
    {
        "id": 16888,
        "title": "Content Caching Policy for 5G Network Based on Asynchronous Advantage Actor-Critic Method",
        "authors": "Zhuoyang Shi, Lixin Li, Yang Xu, Xu Li, Wei Chen, Zhu Han",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014268"
    },
    {
        "id": 16889,
        "title": "A3C-GS: Adaptive Moment Gradient Sharing With Locks for Asynchronous Actor–Critic Agents",
        "authors": "Alfonso B. Labao, Mygel Andrei M. Martija, Prospero C. Naval",
        "published": "2021-3",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.2980743"
    },
    {
        "id": 16890,
        "title": "Statistical arbitrage trading on the intraday market using the asynchronous advantage actor–critic method",
        "authors": "Sumeyra Demir, Bart Stappers, Koen Kok, Nikolaos G. Paterakis",
        "published": "2022-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2022.118912"
    },
    {
        "id": 16891,
        "title": "An Asynchronous Advantage Actor-Critic Approach with Familiarity-Based Experience Replay for Smart Building Energy Management",
        "authors": "yantai huang, jinjiang zhang, qiang lin, Shamsuddeen Nyako Ibrahim, Lu Wang, Zihan Hu, OREFO VICTOR ARINZE",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4749398"
    },
    {
        "id": 16892,
        "title": "Pemanfaatan Asynchronous Advantage Actor-Critic Dalam Pembuatan AI Game Bot Pada Game Arcade",
        "authors": "Evan Kusuma Susanto, Yosi Kristian",
        "published": "2019-12-5",
        "citations": 0,
        "abstract": "Asynchronous Advantage Actor-Critic (A3C) adalah sebuah algoritma deep reinforcement learning yang dikembangkan oleh Google DeepMind. Algoritma ini dapat digunakan untuk menciptakan sebuah arsitektur artificial intelligence yang dapat menguasai berbagai jenis game yang berbeda melalui trial and error dengan mempelajari tempilan layar game dan skor yang diperoleh dari hasil tindakannya tanpa campur tangan manusia. Sebuah network A3C terdiri dari Convolutional Neural Network (CNN) di bagian depan, Long Short-Term Memory Network (LSTM) di tengah, dan sebuah Actor-Critic network di bagian belakang. CNN berguna sebagai perangkum dari citra output layar dengan mengekstrak fitur-fitur yang penting yang terdapat pada layar. LSTM berguna sebagai pengingat keadaan game sebelumnya. Actor-Critic Network berguna untuk menentukan tindakan terbaik untuk dilakukan ketika dihadapkan dengan suatu kondisi tertentu. Dari hasil percobaan yang dilakukan, metode ini cukup efektif dan dapat mengalahkan pemain pemula dalam memainkan 5 game yang digunakan sebagai bahan uji coba.",
        "link": "http://dx.doi.org/10.52985/insyst.v1i2.82"
    },
    {
        "id": 16893,
        "title": "Autonomous Valet Parking with Asynchronous Advantage Actor-Critic Proximal Policy Optimization",
        "authors": "Teckchai Tiong, Ismail Saad, Kenneth Tze Kin Teo, Herwansyah Bin Lago",
        "published": "2022-1-26",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccwc54503.2022.9720881"
    },
    {
        "id": 16894,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 16895,
        "title": "An Asynchronous Advantage Actor-Critic Reinforcement Learning Method for Stock Selection and Portfolio Management",
        "authors": "Qinma Kang, Huizhuo Zhou, Yunfan Kang",
        "published": "2018-10-27",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3291801.3291831"
    },
    {
        "id": 16896,
        "title": "Asynchronous Advantage Actor-Critic Based Approach for Economic Optimization in the Integrated Energy System with Energy Hub",
        "authors": "Bin Zhang, Weihao Hu, Di Cao, Qi Huang, Zhe Chen",
        "published": "2021-3-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeees51875.2021.9403176"
    },
    {
        "id": 16897,
        "title": "Accelerated DRL Agent for Autonomous Voltage Control Using Asynchronous Advantage Actor-critic",
        "authors": "Zhengyuan Xu, Yan Zan, Chunlei Xu, Jin Li, Di Shi, Zhiwei Wang, Bei Zhang, Jiajun Duan",
        "published": "2020-8-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281768"
    },
    {
        "id": 16898,
        "title": "Application of Improved Asynchronous Advantage Actor Critic Reinforcement Learning Model on Anomaly Detection",
        "authors": "Kun Zhou, Wenyong Wang, Teng Hu, Kai Deng",
        "published": "2021-2-25",
        "citations": 8,
        "abstract": "Anomaly detection research was conducted traditionally using mathematical and statistical methods. This topic has been widely applied in many fields. Recently reinforcement learning has achieved exceptional successes in many areas such as the AlphaGo chess playing and video gaming etc. However, there were scarce researches applying reinforcement learning to the field of anomaly detection. This paper therefore aimed at proposing an adaptable asynchronous advantage actor-critic model of reinforcement learning to this field. The performances were evaluated and compared among classical machine learning and the generative adversarial model with variants. Basic principles of the related models were introduced firstly. Then problem definitions, modelling processes and testing were detailed. The proposed model differentiated the sequence and image from other anomalies by proposing appropriate neural networks of attention mechanism and convolutional network for the two kinds of anomalies, respectively. Finally, performances with classical models using public benchmark datasets (NSL-KDD, AWID and CICIDS-2017, DoHBrw-2020) were evaluated and compared. Experiments confirmed the effectiveness of the proposed model with the results indicating higher rewards and lower loss rates on the datasets during training and testing. The metrics of precision, recall rate and F1 score were higher than or at least comparable to the state-of-the-art models. We concluded the proposed model could outperform or at least achieve comparable results with the existing anomaly detection models.",
        "link": "http://dx.doi.org/10.3390/e23030274"
    },
    {
        "id": 16899,
        "title": "Parallel multiple DNA sequence alignment using genetic algorithm and asynchronous advantage actor critic model",
        "authors": "Khadidja Belattar, El Amine Zemali, Sabrina Baouni, Souhila Dehni",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijbra.2022.128236"
    },
    {
        "id": 16900,
        "title": "Design and application of adaptive PID controller based on asynchronous advantage actor–critic learning method",
        "authors": "Qifeng Sun, Chengze Du, Youxiang Duan, Hui Ren, Hongqiang Li",
        "published": "2021-7",
        "citations": 19,
        "abstract": "AbstractTo address the problems of the slow convergence and inefficiency in the existing adaptive PID controllers, we propose a new adaptive PID controller using the asynchronous advantage actor–critic (A3C) algorithm. Firstly, the controller can train the multiple agents of the actor–critic structures in parallel exploiting the multi-thread asynchronous learning characteristics of the A3C structure. Secondly, in order to achieve the best control effect, each agent uses a multilayer neural network to approach the strategy function and value function to search the best parameter-tuning strategy in continuous action space. The simulation results indicate that our proposed controller can achieve the fast convergence and strong adaptability compared with conventional controllers.",
        "link": "http://dx.doi.org/10.1007/s11276-019-02225-x"
    },
    {
        "id": 16901,
        "title": "Asynchronous Advantage Actor-Critic Algorithm Based Cooperative Caching Strategy for Fog Radio Access Networks",
        "authors": "Fan Jiang, Shaojiang Han, Changyin Sun",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc55385.2023.10118683"
    },
    {
        "id": 16902,
        "title": "Parallel Multiple DNA Sequence Alignment using Genetic Algorithm and Asynchronous Advantage Actor Critic Model",
        "authors": "Sabrina BAOUNI, El Amine ZEMALI, Khadidja BELATTAR, Souhila DEHNI",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijbra.2022.10051235"
    },
    {
        "id": 16903,
        "title": "The Implementation of Asynchronous Advantage Actor-Critic with Stigmergy in Network-assisted Multi-agent System",
        "authors": "Kun Chen, Rongpeng Li, Zhifeng Zhao, Honggang Zhang",
        "published": "2020-10-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcsp49889.2020.9299839"
    },
    {
        "id": 16904,
        "title": "Generalized Critic Policy Optimization: A Model For Combining Advantage Estimates In Actor Critic Methods",
        "authors": "Roumeissa Kitouni, Abderrahim Kitouni, Feng Jiang",
        "published": "2020-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190994"
    },
    {
        "id": 16905,
        "title": "An enhanced asynchronous advantage actor-critic-based algorithm for performance optimization in mobile edge computing -enabled internet of vehicles networks",
        "authors": "Komeil Moghaddasi, Shakiba Rajabi, Farhad Soleimanian Gharehchopogh",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12083-024-01633-x"
    },
    {
        "id": 16906,
        "title": "Towards Understanding Asynchronous Advantage Actor-Critic: Convergence and Linear Speedup",
        "authors": "Han Shen, Kaiqing Zhang, Mingyi Hong, Tianyi Chen",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsp.2023.3268475"
    },
    {
        "id": 16907,
        "title": "AQROM: A quality of service aware routing optimization mechanism based on asynchronous advantage actor-critic in software-defined networks",
        "authors": "Wei Zhou, Xing Jiang, Qingsong Luo, Bingli Guo, Xiang Sun, Fengyuan Sun, Lingyu Meng",
        "published": "2022-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.dcan.2022.11.016"
    },
    {
        "id": 16908,
        "title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "link": "http://dx.doi.org/10.18421/tem111-27"
    },
    {
        "id": 16909,
        "title": "Implementation on benchmark of SC2LE environment with advantage actor – critic method",
        "authors": "Huan Hu, Qingling Wang",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas48674.2020.9214032"
    },
    {
        "id": 16910,
        "title": "Learning-based Power Delay Profile Estimation for 5G NR via Advantage Actor-Critic (A2C)",
        "authors": "Hyukjoon Kwon",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-spring54318.2022.9860924"
    },
    {
        "id": 16911,
        "title": "Application of the asynchronous advantage actor–critic machine learning algorithm to real-time accelerator tuning",
        "authors": "Yun Zou, Qing-Zi Xing, Bai-Chuan Wang, Shu-Xin Zheng, Cheng Cheng, Zhong-Ming Wang, Xue-Wu Wang",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s41365-019-0668-1"
    },
    {
        "id": 16912,
        "title": "Locating algorithm of steel stock area with asynchronous advantage actor-critic reinforcement learning",
        "authors": "Young-in Cho, Byeongseop Kim, Hee-Chang Yoon, Jong Hun Woo",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "Abstract\nIn the steel stockyard of the shipyard, the sorting work to relocate the steel plates already stacked to retrieve the target steel plate on the fabrication schedule is labor-consuming work requiring the operation of overhead cranes. To reduce the sorting work, there is a need for a method of stacking the steel plates in order of fabrication schedules when the steel plates arrive at the shipyard from the steel-making companies. However, the conventional optimization algorithm and heuristics have limitations in determining the optimal stacking location of steel plates because the real-world stacking problems in shipyards have vast solution space in addition to the uncertainty in the arrival order of steel plates. In this study, reinforcement learning is applied to the development of a real-time stacking algorithm for steel plates considering the fabrication schedule. Markov decision process suitable for the stacking problem is defined, and the optimal stacking policy is learned using an asynchronous advantage actor-critic algorithm. The learned policy is tested on several problems by varying the number of steel plates. The test results indicate that the proposed method is effective for minimizing the use of cranes compared with other metaheuristics and heuristics for stacking problems.",
        "link": "http://dx.doi.org/10.1093/jcde/qwae002"
    },
    {
        "id": 16913,
        "title": "Towards distributed and autonomous IoT service placement in fog computing using asynchronous advantage actor-critic algorithm",
        "authors": "Mansoureh Zare, Yasser Elmi Sola, Hesam Hasanpour",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jksuci.2022.12.006"
    },
    {
        "id": 16914,
        "title": "A New Advantage Actor-Critic Algorithm For Multi-Agent Environments",
        "authors": "Gabor Paczolay, Istvan Harmati",
        "published": "2020-10-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismcr51255.2020.9263738"
    },
    {
        "id": 16915,
        "title": "Adversarial retraining attack of asynchronous advantage actor‐critic based pathfinding",
        "authors": "Chen Tong, Liu Jiqiang, Xiang Yingxiao, Niu Wenjia, Tong Endong, Wang Shuoru, Li He, Chang Liang, Li Gang, Chen Qi Alfred",
        "published": "2021-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/int.22380"
    },
    {
        "id": 16916,
        "title": "Learning-Based Resource Allocation in Cloud Data Center using Advantage Actor-Critic",
        "authors": "Zheyi Chen, Jia Hu, Geyong Min",
        "published": "2019-5",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761309"
    },
    {
        "id": 16917,
        "title": "Distributed Multi-Agent Approach for Achieving Energy Efficiency and Computational Offloading in MECNs Using Asynchronous Advantage Actor-Critic",
        "authors": "Israr Khan, Salman Raza, Razaullah Khan, Waheed ur Rehman, G. M. Shafiqur Rahman, Xiaofeng Tao",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "Mobile edge computing networks (MECNs) based on hierarchical cloud computing have the ability to provide abundant resources to support the next-generation internet of things (IoT) network, which relies on artificial intelligence (AI). To address the instantaneous service and computation demands of IoT entities, AI-based solutions, particularly the deep reinforcement learning (DRL) strategy, have been intensively studied in both the academic and industrial fields. However, there are still many open challenges, namely, the lengthening convergence phenomena of the agent, network dynamics, resource diversity, and mode selection, which need to be tackled. A mixed integer non-linear fractional programming (MINLFP) problem is formulated to maximize computing and radio resources while maintaining quality of service (QoS) for every user’s equipment. We adopt the advanced asynchronous advantage actor-critic (A3C) approach to take full advantage of distributed multi-agent-based solutions for achieving energy efficiency in MECNs. The proposed approach, which employs A3C for computing offloading and resource allocation, is shown through numerical results to significantly reduce energy consumption and improve energy efficiency. This method’s effectiveness is further shown by comparing it to other benchmarks.",
        "link": "http://dx.doi.org/10.3390/electronics12224605"
    },
    {
        "id": 16918,
        "title": "A new noise network and gradient parallelisation‐based asynchronous advantage actor‐critic algorithm",
        "authors": "Zhengshun Fei, Yanping Wang, Jinglong Wang, Kangling Liu, Bingqiang Huang, Ping Tan",
        "published": "2022-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/csy2.12059"
    },
    {
        "id": 16919,
        "title": "Advantage Actor-Critic for Autonomous Intersection Management",
        "authors": "John Ayeelyan, Guan-Hung Lee, Hsiu-Chun Hsu, Pao-Ann Hsiung",
        "published": "2022-12-12",
        "citations": 2,
        "abstract": "With increasing urban population, there are more and more vehicles, causing traffic congestion. In order to solve this problem, the development of an efficient and fair intersection management system is an important issue. With the development of intelligent transportation systems, the computing efficiency of vehicles and vehicle-to-vehicle communications are becoming more advanced, which can be used to good advantage in developing smarter systems. As such, Autonomous Intersection Management (AIM) proposals have been widely discussed. This research proposes an intersection management system based on Advantage Actor-Critic (A2C) which is a type of reinforcement learning. This method can lead to a fair and efficient intersection resource allocation strategy being learned. In our proposed approach, we design a reward function and then use this reward function to encourage a fair allocation of intersection resources. The proposed approach uses a brake-safe control to ensure that autonomous moving vehicles travel safely. An experiment is performed using the SUMO simulator to simulate traffic at an isolated intersection, and the experimental performance is compared with Fast First Service (FFS) and GAMEOPT in terms of throughput, fairness, and maximum waiting time. The proposed approach increases fairness by 20% to 40%, and the maximum waiting time is reduced by 20% to 36% in high traffic flow. The inflow rates are increased, average waiting time is reduced, and throughput is increased.",
        "link": "http://dx.doi.org/10.3390/vehicles4040073"
    },
    {
        "id": 16920,
        "title": "A dynamic event-triggered network control algorithm combined with gradient-sharing asynchronous advantage actor-critic strategy",
        "authors": "Donghui Zhang, Zehua Ye, Dan Zhang, Qun Lu",
        "published": "2023-3-25",
        "citations": 0,
        "abstract": " In the existing memory event-triggered control (METC) algorithms, the threshold parameters and memory weights are fixed, reducing the system’s adaptability. In this paper, a new intelligent dynamic METC algorithm is proposed to reduce the amount of transmission data and decrease the communication burden in networked control systems. The proposed dynamic METC mechanism applies the gradient-sharing asynchronous advantage actor-critic (A3C-GS) learning algorithm to optimized memory event-triggered function of memory weights and threshold parameters. A time-varying delay system model for dynamic METC is developed by incorporating the A3C-GS for the networked control system with communication delays. Then, by solving two linear matrix inequalities (LMIs), the controller gain parameters of the networked control systems are derived. Finally, the performance of the proposed new METC algorithm is compared with the current three event-triggered control methods. Simulation results show that the proposed intelligent event-triggered algorithm reduces the number of triggers by about 40% compared with the traditional event-triggered algorithm under the pregiven simulation time. Thus, the effectiveness of the main results is verified. ",
        "link": "http://dx.doi.org/10.1177/01423312231159698"
    },
    {
        "id": 16921,
        "title": "Variational value learning in advantage actor-critic reinforcement learning",
        "authors": "Yaozhong Zhang, Jiaqi Han, Xiaofang Hu, Shihao Dan",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327530"
    },
    {
        "id": 16922,
        "title": "Advantage Actor-Critic Reinforcement Learning with Technical Indicators for Stock Trading Decisions",
        "authors": "Rubell Marion Lincy G, Som Sagar, Vishnu Narayanan, Dhanush Binu, Nevin Selby, Sheba  Elizabeth Thomas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4596132"
    },
    {
        "id": 16923,
        "title": "Design of Biped Robot Using Reinforcement Learning and Asynchronous Actor-Critical Agent (A3C) Algorithm",
        "authors": "M. Navaneethakrishnan, P Pushpa, Tamilarasan T, T A Mohanaprakash, Batini Dhanwanth, Faraz Ahmed A S",
        "published": "2023-5-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vitecon58111.2023.10156947"
    },
    {
        "id": 16924,
        "title": "Data augmented Approach to Optimizing Asynchronous Actor-Critic Methods",
        "authors": "Sandeep Varma N, Pradyumna Rahul K, Vaishnavi Sinha",
        "published": "2022-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcece53908.2022.9792764"
    },
    {
        "id": 16925,
        "title": "The Adaptive PID Controlling Algorithm Using Asynchronous Advantage Actor-Critic Learning Method",
        "authors": "Qifeng Sun, Hui Ren, Youxiang Duan, Yanan Yan",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-32216-8_48"
    },
    {
        "id": 16926,
        "title": "Optimal Scheduling Framework of Electricity-Gas-Heat Integrated Energy System Based on Asynchronous Advantage Actor-Critic Algorithm",
        "authors": "Jian Dong, Haixin Wang, Junyou Yang, Xinyi Lu, Liu Gao, Xiran Zhou",
        "published": "2021",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3114335"
    },
    {
        "id": 16927,
        "title": "Real time production scheduling based on Asynchronous Advanced Actor Critic and composite dispatching rule",
        "authors": "Juan Liu, Fei Qiao, Yumin Ma",
        "published": "2020-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327198"
    },
    {
        "id": 16928,
        "title": "Adaptive Advantage Estimation for Actor-Critic Algorithms",
        "authors": "Yurou Chen, Fengyi Zhang, Zhiyong Liu",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534005"
    },
    {
        "id": 16929,
        "title": "Neural Architecture Search with Synchronous Advantage Actor-Critic Methods and Partial Training",
        "authors": "George Kyriakides, Konstantinos G. Margaritis",
        "published": "2018-7-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3200947.3208068"
    },
    {
        "id": 16930,
        "title": "Actor-Critic Tracking with Precise Scale Estimation and Advantage Function",
        "authors": "Chuyao Wang, Yuchen Ling",
        "published": "2021-3-1",
        "citations": 0,
        "abstract": "Abstract\nIn this work, a deep reinforcement learning (DRL) method is proposed to address the problem of real-time object tracking. The adopted framework in this paper is based on the ‘Actor-Critic’ tracker (ACT), since ACT only considers the scale change instead of regression object boundary, which cannot adapt the object size variation. To this end, the ACT method is improved by using a more reasonable action space, which contains a left-top and right-bottom corner coordinates. Precise shape estimation is given by regressing the variation of width and height, respectively. Furthermore, to speed up the whole training and tracking process, the Advantage Function (AF) is adopted, and its performance is compared with ACT, ACT with improved action space (IAS), and ACT with IAS and AF. This method is tested on the OTB100 dataset to validate its effectiveness.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1827/1/012064"
    },
    {
        "id": 16931,
        "title": "Robustness Assessment of Asynchronous Advantage Actor-Critic Based on Dynamic Skewness and Sparseness Computation: A Parallel Computing View",
        "authors": "Tong Chen, Ji-Qiang Liu, He Li, Shuo-Ru Wang, Wen-Jia Niu, En-Dong Tong, Liang Chang, Qi Alfred Chen, Gang Li",
        "published": "2021-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11390-021-1217-z"
    },
    {
        "id": 16932,
        "title": "Multi-Agent Advantage Actor-Critic Learning For Message Content Selection in Cooperative Perception Networks",
        "authors": "Imed Ghnaya, Mohamed Mosbah, Hasnaâ Aniss, Toufik Ahmed",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/noms56928.2023.10154436"
    },
    {
        "id": 16933,
        "title": "Noisy Importance Sampling Actor-Critic: An Off-Policy Actor-Critic With Experience Replay",
        "authors": "Norman Tasfi, Miriam Capretz",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207681"
    },
    {
        "id": 16934,
        "title": "Online Service Function Chain Deployment Method Based on Advantage Actor-Critic Learning",
        "authors": "Yuying Wang, Zhuo Chen",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsins60115.2023.10455499"
    },
    {
        "id": 16935,
        "title": "Statistical arbitrage trading across electricity markets using advantage actor–critic methods",
        "authors": "Sumeyra Demir, Koen Kok, Nikolaos G. Paterakis",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101023"
    },
    {
        "id": 16936,
        "title": "Approaches That Use Domain-Specific Expertise: Behavioral-Cloning-Based Advantage Actor-Critic in Basketball Games",
        "authors": "Taehyeok Choi, Kyungeun Cho, Yunsick Sung",
        "published": "2023-2-22",
        "citations": 3,
        "abstract": "Research on the application of artificial intelligence (AI) in games has recently gained momentum. Most commercial games still use AI based on a finite state machine (FSM) due to complexity and cost considerations. However, FSM-based AI decreases user satisfaction given that it performs the same patterns of consecutive actions in the same situations. This necessitates a new AI approach that applies domain-specific expertise to existing reinforcement learning algorithms. We propose a behavioral-cloning-based advantage actor-critic (A2C) that improves learning performance by applying a behavioral cloning algorithm to an A2C algorithm in basketball games. The state normalization, reward function, and episode classification approaches are used with the behavioral-cloning-based A2C. The results of the comparative experiments with the traditional A2C algorithms validated the proposed method. Our proposed method using existing approaches solved the difficulty of learning in basketball games.",
        "link": "http://dx.doi.org/10.3390/math11051110"
    },
    {
        "id": 16937,
        "title": "Optimizing Advantage Actor-Critic with Policy Gradient and Deep Q-learning to Maximize Profit in Forex Trading Prediction",
        "authors": "Abdillah Baradja, Rahmat Gernowo, Adi Wibowo",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ice-smartech59237.2023.10461955"
    },
    {
        "id": 16938,
        "title": "Experimental Evaluation of Proximal Policy Optimization and Advantage Actor-Critic RL Algorithms using MiniGrid Environment",
        "authors": "Wen-Chung (Andy) Cheng, Zhen Ni, Xiangnan Zhong",
        "published": "2021-5-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5038/lztz6050"
    },
    {
        "id": 16939,
        "title": "Roll control of Underwater Vehicle based Reinforcement Learning using Advantage Actor-Critic",
        "authors": "Byungjun Lee",
        "published": "2021-2-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.9766/kimst.2021.24.1.123"
    },
    {
        "id": 16940,
        "title": "End-to-end Reinforcement Learning for Autonomous Longitudinal Control Using Advantage Actor Critic with Temporal Context",
        "authors": "Sampo Kuutti, Richard Bowden, Harita Joshi, Robert de Temple, Saber Fallah",
        "published": "2019-10",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917387"
    },
    {
        "id": 16941,
        "title": "Real-Time Optimal Energy Management of Multimode Hybrid Electric Powertrain With Online Trainable Asynchronous Advantage Actor–Critic Algorithm",
        "authors": "Atriya Biswas, Pier Giuseppe Anselma, Ali Emadi",
        "published": "2022-6",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tte.2021.3138330"
    },
    {
        "id": 16942,
        "title": "Performance Analysis of Deep Q Networks and Advantage Actor Critic Algorithms in Designing Reinforcement Learning-based Self-tuning PID Controllers",
        "authors": "Rajarshi Mukhopadhyay, Soutrik Bandyopadhyay, Ashoke Sutradhar, Paramita Chattopadhyay",
        "published": "2019-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ibssc47189.2019.8973068"
    },
    {
        "id": 16943,
        "title": "Cooperative traffic signal control using Multi-step return and Off-policy Asynchronous Advantage Actor-Critic Graph algorithm",
        "authors": "Shantian Yang, Bo Yang, Hau-San Wong, Zhongfeng Kang",
        "published": "2019-11",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2019.07.026"
    },
    {
        "id": 16944,
        "title": "Selector-Actor-Critic and Tuner-Actor-Critic Algorithms for Reinforcement Learning",
        "authors": "Ala'eddin Masadeh, Zhengdao Wang, Ahmed E. Kamal",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcsp.2019.8928124"
    },
    {
        "id": 16945,
        "title": "Implementation and Evaluation of Advantage Actor-Critic Algorithm on a Desktop Computer with a Multi-Core CPU",
        "authors": "Fredy Martínez, Angélica Rendón",
        "published": "2022-7",
        "citations": 0,
        "abstract": "In this paper, the implementation and evaluation of the Advantage Actor-Critic (A2C) algorithm, one of the most important Deep Reinforcement Learning schemes, is performed. The objective is to determine the behavior of the algorithm on a desktop computer with a multi-core CPU, establishing its behavior, performance, and resource consumption for embedded applications. This algorithm makes use of multiple agents on parallel instances of the environment so that each agent adds knowledge to the system, which is weighted by a value of Advantage that evaluates its interaction in the environment. This assessment is performed on OpenAI's CartPole-v0 playground, so the results are comparable and easily reproducible. The results show a high performance of the algorithm for different instances with fixed-length segments of experience, which allows us to think of successful use on more resource-constrained hardware platforms.",
        "link": "http://dx.doi.org/10.7753/ijcatr1107.1005"
    },
    {
        "id": 16946,
        "title": "An Improved Advantage Actor-Critic Algorithm for Disassembly Line Balancing Problems Considering Tools Deterioration",
        "authors": "WeiBiao Cai, Xiwang Guo, Jiacun Wang, Shujin Qin, Jian Zhao, Yuanyuan Tan",
        "published": "2022-10-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945173"
    },
    {
        "id": 16947,
        "title": "Beam Selection for Energy-Efficient mmWave Network Using Advantage Actor Critic Learning",
        "authors": "Ycaro Dantas, Pedro Enrique Iturria-Rivera, Hao Zhou, Majid Bavand, Medhat Elsayed, Raimundas Gaigalas, Melike Erol-Kantarci",
        "published": "2023-5-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279804"
    },
    {
        "id": 16948,
        "title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning",
        "authors": "Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Yun-Nung Chen, Kam-Fai Wong",
        "published": "2018-4",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2018.8461918"
    },
    {
        "id": 16949,
        "title": "An Advantage Actor-Critic Algorithm with Confidence Exploration for Open Information Extraction",
        "authors": "Guiliang Liu, Xu Li, Miningming Sun, Ping Li",
        "published": "2020-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1137/1.9781611976236.25"
    },
    {
        "id": 16950,
        "title": "Extractive text summarization model based on advantage actor-critic and graph matrix methodology",
        "authors": "Senqi Yang, Xuliang Duan, Xi Wang, Dezhao Tang, Zeyan Xiao, Yan Guo",
        "published": "2022",
        "citations": 1,
        "abstract": "<abstract>\n<p>The automatic text summarization task faces great challenges. The main issue in the area is to identify the most informative segments in the input text. Establishing an effective evaluation mechanism has also been identified as a major challenge in the area. Currently, the mainstream solution is to use deep learning for training. However, a serious exposure bias in training prevents them from achieving better results. Therefore, this paper introduces an extractive text summarization model based on a graph matrix and advantage actor-critic (GA2C) method. The articles were pre-processed to generate a graph matrix. Based on the states provided by the graph matrix, the decision-making network made decisions and sent the results to the evaluation network for scoring. The evaluation network got the decision results of the decision-making network and then scored them. The decision-making network modified the probability of the action based on the scores of the evaluation network. Specifically, compared with the baseline reinforcement learning-based extractive summarization (Refresh) model, experimental results on the CNN/Daily Mail dataset showed that the GA2C model led on Rouge-1, Rouge-2 and Rouge-A by 0.70, 9.01 and 2.73, respectively. Moreover, we conducted multiple ablation experiments to verify the GA2C model from different perspectives. Different activation functions and evaluation networks were used in the GA2C model to obtain the best activation function and evaluation network. Two different reward functions (Set fixed reward value for accumulation (ADD), Rouge) and two different similarity matrices (cosine, Jaccard) were combined for the experiments.</p>\n</abstract>",
        "link": "http://dx.doi.org/10.3934/mbe.2023067"
    },
    {
        "id": 16951,
        "title": "Adaptive bias-variance trade-off in advantage estimator for actor–critic algorithms",
        "authors": "Yurou Chen, Fengyi Zhang, Zhiyong Liu",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.023"
    },
    {
        "id": 16952,
        "title": "Incomplete Information Competition Strategy Based on Improved Asynchronous Advantage Actor Critical Model",
        "authors": "Cong Zhao, Bing Xiao, Lin Zha",
        "published": "2020-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3417188.3417189"
    },
    {
        "id": 16953,
        "title": "A Variation-aware Advantage Actor-critic (A2C) Machine for Optimization of CML-to-CMOS (C2C) Circuit Design",
        "authors": "Tae-Hyun Kim, Jichull Jeong, Jayoung Yang, Hyeonjung Kim, Euihyun Cheon",
        "published": "2022-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-asia57006.2022.9954866"
    },
    {
        "id": 16954,
        "title": "Asynchronous learning for actor–critic neural networks and synchronous triggering for multiplayer system",
        "authors": "Ke Wang, Chaoxu Mu",
        "published": "2022-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.isatra.2022.02.007"
    },
    {
        "id": 16955,
        "title": "Actor-Critic Reinforcement Learning with Neural Networks in Continuous Games",
        "authors": "Gabriel Leuenberger, Marco A. Wiering",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006556500530060"
    },
    {
        "id": 16956,
        "title": "Supervised Advantage Actor-Critic for Recommender Systems",
        "authors": "Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose",
        "published": "2022-2-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3488560.3498494"
    },
    {
        "id": 16957,
        "title": "Collaborative Learning of Human and Computer: Supervised Actor-Critic based Collaboration Scheme",
        "authors": "Ashwin Devanga, Koichiro Yamauchi",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007568407940801"
    },
    {
        "id": 16958,
        "title": "Twin Delayed Stochastic Actor-Critic",
        "authors": "Mohammad Asadolahi, Arash Sharifi, Touraj Banirostam",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn both discrete and continuous domains, model-free reinforcement learning algorithms have been successfully applied to the vast majority of reinforcement learning problems and are the main solution to real world problems. In reinforcement learning problems with continuous action space, state-of-the-art algorithms are extremely sample-inefficient and need lots of training interactions to become proficient, which could be catastrophically expensive and infeasible in real-world problems. So far, frontier algorithms have not used well-defined methods to explore the decision space. Exploring new behaviors is a prerequisite to look for optimal policies. All of the leading algorithms in the field leverage a blind form of exploration added to agent decisions to search for better policies. Such solutions fail to mindfully explore the environment, disrupting the learning process. This makes these algorithms very prone to failing in specific domains. In this research, a novel stochastic Off-Policy Actor-Critic algorithm, TDS for short, is presented. Combining the policy gradient theorem with the deterministic policy gradient, the TDS algorithm can learn how to mindfully explore the environment. The proposed update method enables TDS to learn how to modify the decision stochasticity bonds for each state and action. This is done according to gradients information derived from learning feedbacks. Evaluations in MuJoCo and Box2D tasks show faster convergence or outperform the state-of-the-art algorithms including TD3, SAC, and DDPG in every environment tested.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3041837/v1"
    },
    {
        "id": 16959,
        "title": "Test Point Selection Using Deep Graph Convolutional Networks and Advantage Actor Critic (A2C) Reinforcement Learning",
        "authors": "Shaoqi Wei, Kohei Shiotani, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Hiroshi Takahashi, Gang Wang",
        "published": "2023-6-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itc-cscc58803.2023.10212888"
    },
    {
        "id": 16960,
        "title": "An Applied Framework for Smarter Buildings Exploiting a Self-Adapted Advantage Weighted Actor-Critic",
        "authors": "Ioannis Papaioannou, Asimina Dimara, Christos Korkas, Iakovos Michailidis, Alexios Papaioannou, Christos-Nikolaos Anagnostopoulos, Elias Kosmatopoulos, Stelios Krinidis, Dimitrios Tzovaras",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Smart buildings are rapidly becoming more prevalent, aiming to create energy-efficient and comfortable living spaces. Nevertheless, the design of a smart building is a multifaceted approach that faces numerous challenges, with the primary one being the algorithm needed for energy management. In this paper, the design of a smart building, with a particular emphasis on the algorithm for controlling the indoor environment, is addressed. The implementation and evaluation of the Advantage-Weighted Actor-Critic algorithm is examined in a four-unit residential simulated building. Moreover, a novel self-adapted Advantage-Weighted Actor-Critic algorithm is proposed, tested, and evaluated in both the simulated and real building. The results underscore the effectiveness of the proposed control strategy compared to Rule-Based Controllers, Deep Deterministic Policy Gradient, and Advantage-Weighted Actor-Critic. Experimental results demonstrate a 34.91% improvement compared to the Deep Deterministic Policy Gradient and a 2.50% increase compared to the best Advantage-Weighted Actor-Critic method in the first epoch during a real-life scenario. These findings solidify the Self-Adapted Advantage-Weighted Actor-Critic algorithm’s efficacy, positioning it as a promising and advanced solution in the realm of smart building optimization.",
        "link": "http://dx.doi.org/10.3390/en17030616"
    },
    {
        "id": 16961,
        "title": "Balance Control for the First-order Inverted Pendulum Based on the Advantage Actor-critic Algorithm",
        "authors": "Yan Zheng, Xutong Li, Long Xu",
        "published": "2020-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12555-019-0278-z"
    },
    {
        "id": 16962,
        "title": "Review for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "",
        "published": "2022-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v1/review2"
    },
    {
        "id": 16963,
        "title": "Distributional Safety Critic for Stochastic Latent Actor-Critic",
        "authors": "Thiago S. Miranda, Heder S. Bernardino",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "When employing reinforcement learning techniques in real-world applications, one may desire to constrain the agent by limiting actions that lead to potential damage, harm, or unwanted scenarios. Particularly, recent approaches focus on developing safe behavior under partial observability conditions. In this vein, we develop a method that combines distributional reinforcement learning techniques with methods used to facilitate learning in partially observable environments, called distributional safe stochastic latent actor-critic (DS-SLAC). We evaluate the DS-SLAC performance on four Safety-Gym tasks and DS-SLAC obtained results better than those reached by state-of-the-art algorithms in two of the evaluated environments while being able to develop a safe policy in three of them. Lastly, we also identify the main challenges of performing distributional reinforcement learning in the safety-constrained partially observable setting.",
        "link": "http://dx.doi.org/10.5753/eniac.2023.234620"
    },
    {
        "id": 16964,
        "title": "Multiple Agents Dispatch via Batch Synchronous Actor Critic in Autonomous Mobility on Demand Systems",
        "authors": "Jiyao Li, Vicki Allan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012351700003636"
    },
    {
        "id": 16965,
        "title": "The Proposal of Double Agent Architecture using Actor-critic Algorithm for Penetration Testing",
        "authors": "Hoang Nguyen, Songpon Teerakanok, Atsuo Inomata, Tetsutaro Uehara",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010232504400449"
    },
    {
        "id": 16966,
        "title": "The Effect of Discounting Actor-loss in Actor-Critic Algorithm",
        "authors": "Jordi Yaputra, Suyanto Suyanto",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isriti54043.2021.9702883"
    },
    {
        "id": 16967,
        "title": "Variance Reduction in Actor Critic Methods (ACM)",
        "authors": "Eric Benhamou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3424668"
    },
    {
        "id": 16968,
        "title": "Soft Actor-Critic With Integer Actions",
        "authors": "Ting-Han Fan, Yubo Wang",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867395"
    },
    {
        "id": 16969,
        "title": "Review for \"A survey and comparative evaluation of actor-critic methods in process control\"",
        "authors": "Faisal Khan",
        "published": "2022-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24508/v1/review1"
    },
    {
        "id": 16970,
        "title": "An Optimized Advantage Actor-Critic Algorithm for Disassembly Line Balancing Problem Considering Disassembly Tool Degradation",
        "authors": "Shujin Qin, Xinkai Xie, Jiacun Wang, Xiwang Guo, Liang Qi, Weibiao Cai, Ying Tang, Qurra Tul Ann Talukder",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "The growing emphasis on ecological preservation and natural resource conservation has significantly advanced resource recycling, facilitating the realization of a sustainable green economy. Essential to resource recycling is the pivotal stage of disassembly, wherein the efficacy of disassembly tools plays a critical role. This work investigates the impact of disassembly tools on disassembly duration and formulates a mathematical model aimed at minimizing workstation cycle time. To solve this model, we employ an optimized advantage actor-critic algorithm within reinforcement learning. Furthermore, it utilizes the CPLEX solver to validate the model’s accuracy. The experimental results obtained from CPLEX not only confirm the algorithm’s viability but also enable a comparative analysis against both the original advantage actor-critic algorithm and the actor-critic algorithm. This comparative work verifies the superiority of the proposed algorithm.",
        "link": "http://dx.doi.org/10.3390/math12060836"
    }
]