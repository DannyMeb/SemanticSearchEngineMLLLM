[
    {
        "id": 14105,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 14106,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 14107,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 14108,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 14109,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 14110,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 14111,
        "title": "BGRL: Basal Ganglia inspired Reinforcement Learning based framework for deep brain stimulators",
        "authors": "Harsh Agarwal, Heena Rathore",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artmed.2023.102736"
    },
    {
        "id": 14112,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 14113,
        "title": "Temporal signed gestures segmentation in an image sequence using deep reinforcement learning",
        "authors": "Dawid Kalandyk, Tomasz Kapuściński",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.107879"
    },
    {
        "id": 14114,
        "title": "Artificial Intelligence Gamers Based on Deep Reinforcement Learning",
        "authors": "Haolin Wang",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "This study investigates the design and implementation of Artificial Intelligence (AI) game players based on deep reinforcement learning, offering a novel approach to autonomous decision-making and strategy acquisition in intelligent games. Initially, the fundamental principles and algorithms of deep reinforcement learning are introduced, along with the fusion of deep learning and reinforcement learning. Subsequently, existing research is reviewed, and the pros and cons of current methodologies are examined, highlighting the underlying issues and challenges. The utilization of AI players in mainstream games is then introduced, and the influence of AI players on contemporary games is analyzed. Through this analysis of AI players in mainstream games, the strengths and weaknesses of current AI players are identified, and recommendations for optimizing them are provided. This study holds significant implications for guiding the design and development of intelligent game players, while also enriching the application of deep reinforcement learning within the gaming domain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/p9tv1494"
    },
    {
        "id": 14115,
        "title": "Data-driven hospitals staff and resources allocation using agent-based simulation and deep reinforcement learning",
        "authors": "Teddy Lazebnik",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106783"
    },
    {
        "id": 14116,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 14117,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 14118,
        "title": "A Deep Reinforcement Learning Agent for Snake Game",
        "authors": "Md Meem Hossain, Akinwumi Fakokunde, Omololu Isaac Olaolu",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "After watching AlphaGo a Netflix documentary which presents how AlphaGo is an AI computer game developed by deep-mind technologies based on deep reinforcement learning (DRL). Since then, my interest in reinforcement learning has been growing. In this project, I will apply reinforcement learning to develop an agent to play snake game. Where Deep learning will implement a neural Network to help the agent (snake) to learn what action must take to get a state. If we describe deep reinforcement learning (DRL) model where agent interacts with an environment and chooses an action. Based on action, agents receive feedback from the environment as states (or perceives) and rewards. A state = an array with 11 input values, each input values represent a neural network that provides an output of 3 values, each one represents three possible actions the agent (snake) can take (Straight, Right Turn and Left Turn).",
        "keywords": "",
        "link": "http://dx.doi.org/10.36079/lamintang.ijai-01002.565"
    },
    {
        "id": 14119,
        "title": "Safe resource management of non-cooperative microgrids based on deep reinforcement learning",
        "authors": "Mahdi Shademan, Hamid Karimi, Shahram Jadid",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106865"
    },
    {
        "id": 14120,
        "title": "Teleconsultation dynamic scheduling with a deep reinforcement learning approach",
        "authors": "Wenjia Chen, Jinlin Li",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artmed.2024.102806"
    },
    {
        "id": 14121,
        "title": "Dynamic Resource Allocation Using Deep Reinforcement Learning for 6G Metaverse",
        "authors": "Haesik Kim",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic60209.2024.10463509"
    },
    {
        "id": 14122,
        "title": "A Method to Plan the Path of a Robot Utilizing Deep Reinforcement Learning and Multi-Sensory Information Fusion",
        "authors": "Jieren Tan",
        "published": "2023-12-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/08839514.2023.2224996"
    },
    {
        "id": 14123,
        "title": "DRL4HFC: Deep Reinforcement Learning for Container-Based Scheduling in Hybrid Fog/Cloud System",
        "authors": "Ameni Kallel, Molka Rekik, Mahdi Khemakhem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012356800003636"
    },
    {
        "id": 14124,
        "title": "Causal Deep Reinforcement Learning Using Observational Data",
        "authors": "Wenxuan Zhu, Chao Yu, Qiang Zhang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) requires the collection of interventional data, which is sometimes expensive and even unethical in the real world, such as in the autonomous driving and the medical field. Offline reinforcement learning promises to alleviate this issue by exploiting the vast amount of observational data available in the real world. However, observational data may mislead the learning agent to undesirable outcomes if the behavior policy that generates the data depends on unobserved random variables (i.e., confounders). In this paper, we propose two deconfounding methods in DRL to address this problem. The methods first calculate the importance degree of different samples based on the causal inference technique, and then adjust the impact of different samples on the loss function by reweighting or resampling the offline dataset to ensure its unbiasedness. These deconfounding methods can be flexibly combined with existing model-free DRL algorithms such as soft actor-critic and deep Q-learning, provided that a weak condition can be satisfied by the loss functions of these algorithms. We prove the effectiveness of our deconfounding methods and validate them experimentally.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/524"
    },
    {
        "id": 14125,
        "title": "Wireless Network Design Optimization for Computer Teaching with Deep Reinforcement Learning Application",
        "authors": "Yumei Luo, Deyu Zhang",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/08839514.2023.2218169"
    },
    {
        "id": 14126,
        "title": "Deep Reinforcement Learning (DRL) for Real-Time Traffic Management in Smart Cities",
        "authors": "Dhiraj Singh",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccsai59793.2023.10421359"
    },
    {
        "id": 14127,
        "title": "Intersecting reinforcement learning and deep factor methods for optimizing locality and globality in forecasting: A review",
        "authors": "João Sousa, Roberto Henriques",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108082"
    },
    {
        "id": 14128,
        "title": "Integrating Neural Pathways for Learning in Deep Reinforcement Learning Models",
        "authors": "Varun Ananth",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Considering that the human brain is the most powerful, generalizable, and energy-efficient computer we know of, it makes the most sense to look to neuroscience for ideas regarding deep learning model improvements. I propose one such idea, augmenting a traditional Advantage-Actor-Critic (A2C) model with additional learning signals akin to those in the brain. Pursuing this direction of research should hopefully result in a new reinforcement learning (RL) control paradigm that can learn from fewer examples, train with greater stability, and possibly consume less energy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30541"
    },
    {
        "id": 14129,
        "title": "Dynamic Job Shop Scheduling via Deep Reinforcement Learning",
        "authors": "Xinjie Liang, Wen Song, Pengfei Wei",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00060"
    },
    {
        "id": 14130,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 14131,
        "title": "Deep Reinforcement Learning for Communication Networks",
        "authors": "Raffaele Galliera",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "This research explores optimizing communication tasks with (Multi-Agent) Reinforcement Learning (RL/MARL) in Point-to-Point and Group Communication (GC) networks. The study initially applied RL for Congestion Control in networks with dynamic link properties, yielding competitive results. Then, it focused on the challenge of effective message dissemination in GC networks, by framing a novel game-theoretic formulation and designing methods to solve the task based on MARL and Graph Convolution. Future research will deepen the exploration of MARL in GC. This will contribute to both academic knowledge and practical advancements in the next generation of communication protocols.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30394"
    },
    {
        "id": 14132,
        "title": "Hardware accelerators for deep reinforcement learning",
        "authors": "Vinod K. Mishra, Kanad Basu, Ayush Arunachalam",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663175"
    },
    {
        "id": 14133,
        "title": "A deep reinforcement learning algorithm to control a two-wheeled scooter with a humanoid robot",
        "authors": "Jacky Baltes, Guilherme Christmann, Saeed Saeedvand",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106941"
    },
    {
        "id": 14134,
        "title": "Deep Reinforcement Learning algorithms for Low Latency Edge Computing Systems",
        "authors": "K. Kumaran, E. Sasikala",
        "published": "2023-3-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aisp57993.2023.10134928"
    },
    {
        "id": 14135,
        "title": "Reward Design for Deep Reinforcement Learning Towards Imparting Commonsense Knowledge in Text-Based Scenario",
        "authors": "Ryota Kubo, Fumito Uwano, Manabu Ohta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012456900003636"
    },
    {
        "id": 14136,
        "title": "An explainable deep reinforcement learning algorithm for the parameter configuration and adjustment in the consortium blockchain",
        "authors": "Zhonghao Zhai, Subin Shen, Yanqin Mao",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107606"
    },
    {
        "id": 14137,
        "title": "Deep Reinforcement Learning for Advanced Persistent Threat Detection in Wireless Networks",
        "authors": "Kazeem Saheed, Shagufta Henna",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aics60730.2023.10470498"
    },
    {
        "id": 14138,
        "title": "Research on Tackle Recognition of Football Players Based on Deep Reinforcement Learning",
        "authors": "Xiaodong Pang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiars59518.2023.00107"
    },
    {
        "id": 14139,
        "title": "Adversarial deep reinforcement learning based robust depth tracking control for underactuated autonomous underwater vehicle",
        "authors": "Zhao Wang, Xianbo Xiang, Yu Duan, Shaolong Yang",
        "published": "2024-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107728"
    },
    {
        "id": 14140,
        "title": "Resource optimization of MEC systems based on deep reinforcement learning",
        "authors": "Mengyuan Zhou",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icbase59196.2023.10303158"
    },
    {
        "id": 14141,
        "title": "Proposal of a Signal Control Method Using Deep Reinforcement Learning with Pedestrian Traffic Flow",
        "authors": "Akimasa Murata, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011665000003393"
    },
    {
        "id": 14142,
        "title": "Deep Reinforcement Learning for Autonomous Ground Vehicle Exploration Without A-Priori Maps",
        "authors": "Shathushan Sivashangaran, Azim Eskandarian",
        "published": "2023",
        "citations": 1,
        "abstract": "Autonomous Ground Vehicles (AGVs) are essential tools for a wide range of applications stemming from their ability to operate in hazardous environments with minimal human operator input. Effective motion planning is paramount for successful operation of AGVs. Conventional motion planning algorithms are dependent on prior knowledge of environment characteristics and offer limited utility in information poor, dynamically altering environments such as areas where emergency hazards like fire and earthquake occur, and unexplored subterranean environments such as tunnels and lava tubes on Mars. We propose a Deep Reinforcement Learning (DRL) framework for intelligent AGV exploration without a-priori maps utilizing Actor-Critic DRL algorithms to learn policies in continuous and high-dimensional action spaces directly from raw sensor data. The DRL architecture comprises feedforward neural networks for the critic and actor representations in which the actor network strategizes linear and angular velocity control actions given current state inputs, that are evaluated by the critic network which learns and estimates Q-values to maximize an accumulated reward. Three off-policy DRL algorithms, DDPG, TD3 and SAC, are trained and compared in two environments of varying complexity, and further evaluated in a third with no prior training or knowledge of map characteristics. The agent is shown to learn optimal policies at the end of each training period to chart quick, collision-free exploration trajectories, and is extensible, capable of adapting to an unknown environment without changes to network architecture or hyperparameters. The best algorithm is further evaluated in a realistic 3D environment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54364/aaiml.2023.1170"
    },
    {
        "id": 14143,
        "title": "Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness",
        "authors": "Ezgi Korkmaz",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Learning from raw high dimensional data via interaction with a given environment has been effectively achieved through the utilization of deep neural networks. Yet the observed degradation in policy performance caused by imperceptible worst-case policy dependent translations along high sensitivity directions (i.e. adversarial perturbations) raises concerns on the robustness of deep reinforcement learning policies. In our paper, we show that these high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box setting. Furthermore, we show that vanilla training techniques intriguingly result in learning more robust policies compared to the policies learnt via the state-of-the-art adversarial training techniques. We believe our work lays out intriguing properties of the deep reinforcement learning policy manifold and our results can help to build robust and generalizable deep reinforcement learning policies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i7.26009"
    },
    {
        "id": 14144,
        "title": "Optimizing Forensic Investigation and Security Surveillance with Deep Reinforcement Learning Techniques",
        "authors": "T J Nandhini, K Thinakaran",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452551"
    },
    {
        "id": 14145,
        "title": "Survey on Object Detection Using Deep Reinforcement Learning",
        "authors": "Najla Musthafa, Naseeha Abdullah",
        "published": "2023-4-25",
        "citations": 0,
        "abstract": "Deep Reinforcement Learning (DRL) is a  method that is a combination of  Reinforcement Learning framework and deep  neural networks. It is observed that DRL  achieved a remarkable victory over the fields  such as video games, robotics, finance,  computer vision, health care etc. Comparing  other domains, the medicine and healthcare  field has benefitted a lot from DRL. In this  paper, we study the role of DRL in object  detection using the works of various authors.  Here we focus on object detection in medicine  and the healthcare field. It is observed that  the authors experience higher speed in the  DRL algorithm compared to classic methods.  The respective methods are more efficient  and accurate working on CT/MRI images.  Most authors use an updated DRL algorithm  in the stage of feature extraction and also club  it with some machine learning techniques.  DQN (Deep Q Network), Double DQN,  TRPO(Trust Region Policy Optimization) etc  are some common DRL algorithms used by  researchers. This literature survey  emphasizes methodologies of application of  DRL algorithms for more efficient object  detection. This review helps the futuristic way  to develop a DRL algorithm for better object  detection in the healthcare domain and  similar ones.",
        "keywords": "",
        "link": "http://dx.doi.org/10.46610/rtaia.2023.v02i01.005"
    },
    {
        "id": 14146,
        "title": "Advancements in Deep Reinforcement Learning and Inverse Reinforcement Learning for Robotic Manipulation: Toward Trustworthy, Interpretable, and Explainable Artificial Intelligence",
        "authors": "Recep Ozalp, Aysegul Ucar, Cuneyt Guzelis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3385426"
    },
    {
        "id": 14147,
        "title": "Toward an adaptive deep reinforcement learning agent for maritime platform defense",
        "authors": "Jared Markowitz, Edward W. Staley",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663831"
    },
    {
        "id": 14148,
        "title": "Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making",
        "authors": "Jeff Shuford",
        "published": "2024-2-2",
        "citations": 0,
        "abstract": "Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.60087/jaigs.v1i1.36"
    },
    {
        "id": 14149,
        "title": "An application of deep reinforcement learning and vendor-managed inventory in perishable supply chain management",
        "authors": "Navid Mohamadi, Seyed Taghi Akhavan Niaki, Mahdi Taher, Ali Shavandi",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107403"
    },
    {
        "id": 14150,
        "title": "A Low Latency Adaptive Coding Spike Framework for Deep Reinforcement Learning",
        "authors": "Lang Qin, Rui Yan, Huajin Tang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/340"
    },
    {
        "id": 14151,
        "title": "MAGNET: Multi-Interest Attentive Group Recommender with Deep Reinforcement Learning",
        "authors": "Yaqi Shi, Xiaoqiang Ren, Xiaofan Wang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiiip61647.2023.00060"
    },
    {
        "id": 14152,
        "title": "Research on Unmanned Surface Vehicle Collision Avoidance Based on Deep Reinforcement Learning",
        "authors": "Hua Xiao, Qingnian Zhang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489693"
    },
    {
        "id": 14153,
        "title": "Balanced incremental deep reinforcement learning based on variational autoencoder data augmentation for customer credit scoring",
        "authors": "Yadong Wang, Yanlin Jia, Yu Zhong, Jing Huang, Jin Xiao",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106056"
    },
    {
        "id": 14154,
        "title": "Deep Reinforcement Learning Based Efficient and Robust Navigation Method For Autonomous Applications",
        "authors": "Nathan Hemming, Vineetha Menon",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00049"
    },
    {
        "id": 14155,
        "title": "A Survey of Zero-shot Generalisation in Deep Reinforcement Learning",
        "authors": "Robert Kirk, Amy Zhang, Edward Grefenstette, Tim Rocktäschel",
        "published": "2023-1-9",
        "citations": 24,
        "abstract": "The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We rely on a unifying formalism and terminology for discussing different ZSG problems, building upon previous works. We go on to categorise existing benchmarks for ZSG, as well as current methods for tackling these problems. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG, and we recommend building benchmarks in underexplored problem settings such as offline RL ZSG and reward-function variation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1613/jair.1.14174"
    },
    {
        "id": 14156,
        "title": "Research and Design of an Autonomous Underwater Vehicle Path Planning Method Based on Deep Reinforcement Learning",
        "authors": "Yunpeng Li",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405588"
    },
    {
        "id": 14157,
        "title": "Artificial intelligence process control: deep reinforcement learning for Ga2O3 wafer production",
        "authors": "Sarah Constantin, Matthew Putman, Valerie Bordelanne",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2668706"
    },
    {
        "id": 14158,
        "title": "DJ-Agent: music theory directed a cappella accompaniment generation using deep reinforcement learning",
        "authors": "Jiuming Jiang",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2667902"
    },
    {
        "id": 14159,
        "title": "Transferable dynamics models for efficient object-oriented reinforcement learning",
        "authors": "Ofir Marom, Benjamin Rosman",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2024.104079"
    },
    {
        "id": 14160,
        "title": "Deep-Q-Based Reinforcement Learning Method to Predict Accuracy of Atari Gaming Set Classification",
        "authors": "N Gobinathan, R Ponnusamy",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452644"
    },
    {
        "id": 14161,
        "title": "Bi-level optimization of charging scheduling of a battery swap station based on deep reinforcement learning",
        "authors": "Mao Tan, Zhuocen Dai, Yongxin Su, Caixue Chen, Ling Wang, Jie Chen",
        "published": "2023-2",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105557"
    },
    {
        "id": 14162,
        "title": "Multi-UAV trajectory optimizer: A sustainable system for wireless data harvesting with deep reinforcement learning",
        "authors": "Mincheol Seong, Ohyun Jo, Kyungseop Shin",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105891"
    },
    {
        "id": 14163,
        "title": "An imbalanced classification approach for establishment of cause-effect relationship between Heart-Failure and Pulmonary Embolism using Deep Reinforcement Learning",
        "authors": "Naira Firdous, Nusrat Mohi Ud Din, Assif Assad",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107004"
    },
    {
        "id": 14164,
        "title": "A survey on deep reinforcement learning approaches for traffic signal control",
        "authors": "Haiyan Zhao, Chengcheng Dong, Jian Cao, Qingkui Chen",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108100"
    },
    {
        "id": 14165,
        "title": "Deep Reinforcement Learning for Congestion Control and Routing Optimization in LEO Satellite Networks",
        "authors": "Jiacheng Zhu, Xuebin Sun, Dianjun Chen",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405470"
    },
    {
        "id": 14166,
        "title": "End-to-end UAV Intelligent Training via Deep Reinforcement Learning",
        "authors": "Xin Liu, Caizheng Wang, Qiuquan Guo, Jun Yang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489264"
    },
    {
        "id": 14167,
        "title": "Deep Reinforcement Learning-based Building Energy Management using Electric Vehicles for Demand Response",
        "authors": "Daeyoung Kang, Seunghyun Yoon, Hyuk Lim",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10066975"
    },
    {
        "id": 14168,
        "title": "GQN: Multi-Agent Deep Reinforcement Learning based on Graph Networks",
        "authors": "Zeyu Zhou, Mideng Qian, Hao Zhang, Xinkun Chu",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itaic58329.2023.10408958"
    },
    {
        "id": 14169,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 14170,
        "title": "Hierarchical Mean-Field Deep Reinforcement Learning for Large-Scale Multiagent Systems",
        "authors": "Chao Yu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Learning for efficient coordination in large-scale multiagent systems suffers from the problem of the curse of dimensionality due to the exponential growth of agent interactions. Mean-Field (MF)-based methods address this issue by transforming the interactions within the whole system into a single agent played with the average effect of its neighbors. However, considering the neighbors merely by their average may ignore the varying influences of each neighbor, and learning with this kind of local average effect would likely lead to inferior system performance due to lack of an efficient coordination mechanism in the whole population level. In this work, we propose a Hierarchical Mean-Field (HMF) learning framework to further improve the performance of existing MF methods. The basic idea is to approximate the average effect for a sub-group of agents by considering their different influences within the sub-group, and realize population-level coordination through the interactions among different sub-groups. Empirical studies show that HMF significantly outperforms existing baselines on both challenging cooperative and mixed cooperative-competitive tasks with different scales of agent populations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i10.26387"
    },
    {
        "id": 14171,
        "title": "A SAC-Based Deep Reinforcement Learning Approach for Autonomous Underwater Vehicle Combat",
        "authors": "Kai Zhang, Yang Xu, Junjie Zhu",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489144"
    },
    {
        "id": 14172,
        "title": "Learning team-based navigation: a review of deep reinforcement learning techniques for multi-agent pathfinding",
        "authors": "Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "AbstractMulti-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation indicators and providing comprehensive clarification on these indicators. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified indicators for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10462-023-10670-6"
    },
    {
        "id": 14173,
        "title": "SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning",
        "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf",
        "published": "2023-8",
        "citations": 4,
        "abstract": "We present a novel recommendation system designed to provide real-time treatment strategies to therapists during psychotherapy sessions. Our system utilizes a turn-level rating mechanism that forecasts the therapeutic outcome by calculating a similarity score between the profound representation of a scoring inventory and the patient's current spoken sentence. By transcribing and segmenting the continuous audio stream into patient and therapist turns, our system conducts immediate evaluation of their therapeutic working alliance. The resulting dialogue pairs, along with their computed working alliance ratings, are then utilized in a deep reinforcement learning recommendation system. In this system, the sessions are treated as users, while the topics are treated as items. To showcase the system's effectiveness, we not only evaluate its performance using an existing dataset of psychotherapy sessions but also demonstrate its practicality through a web app. Through this demo, we aim to provide a tangible and engaging experience of our recommendation system in action.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/837"
    },
    {
        "id": 14174,
        "title": "Pursuit-Evasion Game of Unmanded Surface Vehicles Based on Deep Reinforcement Learning",
        "authors": "Xin Wang, Yueying Wang, Weixiang Zhou, Jiaming Zhang",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icecai58670.2023.10176487"
    },
    {
        "id": 14175,
        "title": "A deep reinforcement learning approach to energy management control with connected information for hybrid electric vehicles",
        "authors": "Peng Mei, Hamid Reza Karimi, Hehui Xie, Fei Chen, Cong Huang, Shichun Yang",
        "published": "2023-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106239"
    },
    {
        "id": 14176,
        "title": "Clinical knowledge-guided deep reinforcement learning for sepsis antibiotic dosing recommendations",
        "authors": "Yuan Wang, Anqi Liu, Jucheng Yang, Lin Wang, Ning Xiong, Yisong Cheng, Qin Wu",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artmed.2024.102811"
    },
    {
        "id": 14177,
        "title": "Deep Reinforcement Learning Object Tracking Based on Actor-Double Critic Network",
        "authors": "Jing Xin, Jianglei Zhou, Xinhong Hei, Pengyu Yue, Jia Zhao",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26599/air.2023.9150013"
    },
    {
        "id": 14178,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 14179,
        "title": "Controlling fracture propagation using deep reinforcement learning",
        "authors": "Yuteng Jin, Siddharth Misra",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106075"
    },
    {
        "id": 14180,
        "title": "Container stacking optimization based on Deep Reinforcement Learning",
        "authors": "Xin Jin, Zhentang Duan, Wen Song, Qiqiang Li",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106508"
    },
    {
        "id": 14181,
        "title": "Decentralized fused-learner architectures for Bayesian reinforcement learning",
        "authors": "Augustin A. Saucan, Subhro Das, Moe Z. Win",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2024.104094"
    },
    {
        "id": 14182,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 14183,
        "title": "Deep Hierarchical Communication Graph in Multi-Agent Reinforcement Learning",
        "authors": "Zeyang Liu, Lipeng Wan, Xue Sui, Zhuoran Chen, Kewu Sun, Xuguang Lan",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Sharing intentions is crucial for efficient cooperation in communication-enabled multi-agent reinforcement learning. Recent work applies static or undirected graphs to determine the order of interaction. However, the static graph is not general for complex cooperative tasks, and the parallel message-passing update in the undirected graph with cycles cannot guarantee convergence. To solve this problem, we propose Deep Hierarchical Communication Graph (DHCG) to learn the dependency relationships between agents based on their messages. The relationships are formulated as directed acyclic graphs (DAGs), where the selection of the proper topology is viewed as an action and trained in an end-to-end fashion. To eliminate the cycles in the graph, we apply an acyclicity constraint as intrinsic rewards and then project the graph in the admissible solution set of DAGs. As a result, DHCG removes redundant communication edges for cost improvement and guarantees convergence. To show the effectiveness of the learned graphs, we propose policy-based and value-based DHCG. Policy-based DHCG factorizes the joint policy in an auto-regressive manner, and value-based DHCG factorizes the joint value function to individual value functions and pairwise payoff functions. Empirical results show that our method improves performance across various cooperative multi-agent tasks, including Predator-Prey, Multi-Agent Coordination Challenge, and StarCraft Multi-Agent Challenge.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/24"
    },
    {
        "id": 14184,
        "title": "Hierarchical Deep Reinforcement Learning with Neural Turing Machines for Treatment Path Optimization",
        "authors": " Mamta, Sujata V. Patil, Mahendra Alate, Ashutosh Pagrotra",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiihi57871.2023.10489355"
    },
    {
        "id": 14185,
        "title": "Deep learning based simulators for the phosphorus removal process control in wastewater treatment via deep reinforcement learning algorithms",
        "authors": "Esmaeel Mohammadi, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Per Halkjær Nielsen, Daniel Ortiz-Arroyo, Petar Durdevic",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.107992"
    },
    {
        "id": 14186,
        "title": "Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient",
        "authors": "Maheed H. Ahmed, Abdullah AboHussien, Aly El-Shafei, Ahmed M. Darwish, Ahmed H. Abdel-Gawad",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106593"
    },
    {
        "id": 14187,
        "title": "Reinforcement learning with multimodal advantage function for accurate advantage estimation in robot learning",
        "authors": "Jonghyeok Park, Soohee Han",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107019"
    },
    {
        "id": 14188,
        "title": "Terrain information-involved power allocation optimization for fuel cell/battery/ultracapacitor hybrid electric vehicles via an improved deep reinforcement learning",
        "authors": "Fazhan Tao, Huixian Gong, Zhumu Fu, Zhengyu Guo, Qihong Chen, Shuzhong Song",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106685"
    },
    {
        "id": 14189,
        "title": "Policy-Independent Behavioral Metric-Based Representation for Deep Reinforcement Learning",
        "authors": "Weijian Liao, Zongzhang Zhang, Yang Yu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Behavioral metrics can calculate the distance between states or state-action pairs from the rewards and transitions difference. By virtue of their capability to filter out task-irrelevant information in theory, using them to shape a state embedding space becomes a new trend of representation learning for deep reinforcement learning (RL), especially when there are explicit distracting factors in observation backgrounds. However, due to the tight coupling between the metric and the RL policy, such metric-based methods may result in less informative embedding spaces which can weaken their aid to the baseline RL algorithm and even consume more samples to learn. We resolve this by proposing a new behavioral metric. It decouples the learning of RL policy and metric owing to its independence on RL policy. We theoretically justify its scalability to continuous state and action spaces and design a practical way to incorporate it into an RL procedure as a representation learning target. We evaluate our approach on DeepMind control tasks with default and distracting backgrounds. By statistically reliable evaluation protocols, our experiments demonstrate our approach is superior to previous metric-based methods in terms of sample efficiency and asymptotic performance in both backgrounds.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i7.26052"
    },
    {
        "id": 14190,
        "title": "An Overview of Environmental Features that Impact Deep Reinforcement Learning in Sparse-Reward Domains",
        "authors": "Jim Martin Catacora Ocana, Roberto Capobianco, Daniele Nardi",
        "published": "2023-4-26",
        "citations": 1,
        "abstract": "Deep reinforcement learning has achieved impressive results in recent years; yet, it is still severely troubled by environments showcasing sparse rewards. On top of that, not all sparse-reward environments are created equal, i.e., they can differ in the presence or absence of various features, with many of them having a great impact on learning. In light of this, the present work puts together a literature compilation of such environmental features, covering particularly those that have been taken advantage of and those that continue to pose a challenge. We expect this effort to provide guidance to researchers for assessing the generality of their new proposals and to call their attention to issues that remain unresolved when dealing with sparse rewards.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1613/jair.1.14390"
    },
    {
        "id": 14191,
        "title": "Multi-agent quantum-inspired deep reinforcement learning for real-time distributed generation control of 100% renewable energy systems",
        "authors": "Dan Liu, Yingzi Wu, Yiqun Kang, Linfei Yin, Xiaotong Ji, Xinghui Cao, Chuangzhi Li",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105787"
    },
    {
        "id": 14192,
        "title": "A Deep Reinforcement Learning Method for Accurate and Efficient Anomaly Detection",
        "authors": "Dongcheng Zhang, Liyuan Zheng, Xin Xie, Chen Wang",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciba56860.2023.10165364"
    },
    {
        "id": 14193,
        "title": "Offloading strategy for UAV power inspection task based on deep reinforcement learning",
        "authors": "Jin Tong, Minghao Gu, Yun Shan, Fangming Deng",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671522"
    },
    {
        "id": 14194,
        "title": "Obstacle-Avoiding Rectilinear Steiner Minimal Tree Algorithm Based on Deep Reinforcement Learning",
        "authors": "Zhenkun Lin, Yuhan Zhu, Xing Huang, Liliang Yang, Genggeng Liu",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiotsys58602.2023.00044"
    },
    {
        "id": 14195,
        "title": "Arrhythmia Detection from Electrocardiogram Signal Data Based on Wavelet Transform and Deep Reinforcement Learning",
        "authors": "Muran Zhu, Xiaotong Huo, Zuozhen Zhang, Jinduo Liu, Junzhong Ji",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/medai59581.2023.00051"
    },
    {
        "id": 14196,
        "title": "Deep Reinforcement Learning Based Cognitive Equalization Algorithm Research in Underwater Communication",
        "authors": "Yiwen He, Yi Tao",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccai57533.2023.10201283"
    },
    {
        "id": 14197,
        "title": "Autonomous driving system using proximal policy optimization in deep reinforcement learning",
        "authors": "Imam Noerhenda Yazid, Ema Rachmawati",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "<span>Autonomous driving is one solution that can minimize and even prevent accidents. In autonomous driving, the vehicle must know the surrounding environment and move under the provisions and situations. We build an autonomous driving system using proximal policy optimization (PPO) in deep reinforcement learning, with PPO acting as an instinct for the agent to choose an action. The instinct will be updated continuously until the agent reaches the destination from the initial point. We use five sensory inputs for the agent to accelerate, turn the steer, hit the brakes, avoid the walls, detect the initial point, and reach the destination point. We evaluated our proposed autonomous driving system in a simulation environment with several branching tracks, reflecting a real-world setting. For our driving simulation purpose in this research, we use the Unity3D engine to construct the dataset (in the form of a road track) and the agent model (in the form of a car). Our experimental results firmly indicate our agent can successfully control a vehicle to navigate to the destination point. </span>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i1.pp422-431"
    },
    {
        "id": 14198,
        "title": "Safety Aware Neural Pruning for Deep Reinforcement Learning (Student Abstract)",
        "authors": "Briti Gangopadhyay, Pallab Dasgupta, Soumyajit Dey",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Neural network pruning is a technique of network compression by removing weights of lower importance from an optimized neural network. Often, pruned networks are compared\nin terms of accuracy, which is realized in terms of rewards for Deep Reinforcement Learning (DRL) networks. However, networks that estimate control actions for safety-critical tasks, must also adhere to safety requirements along with obtaining rewards. We propose a methodology to iteratively refine the weights of a pruned neural network such that we get a sparse high-performance network without significant side effects on safety.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26966"
    },
    {
        "id": 14199,
        "title": "No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning",
        "authors": "Dianyu Zhong, Yiqin Yang, Qianchuan Zhao",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The large action space is one fundamental obstacle to deploying Reinforcement Learning methods in the real world. The numerous redundant actions will cause the agents to make repeated or invalid attempts, even leading to task failure. Although current algorithms conduct some initial explorations for this issue, they either suffer from rule-based systems or depend on expert demonstrations, which significantly limits their applicability in many real-world settings. In this work, we examine the theoretical analysis of what action can be eliminated in policy optimization and propose a novel redundant action filtering mechanism. Unlike other works, our method constructs the similarity factor by estimating the distance between the state distributions, which requires no prior knowledge. In addition, we combine the modified inverse model to avoid extensive computation in high-dimensional state space. We reveal the underlying structure of action spaces and propose a simple yet efficient redundant action filtering mechanism named No Prior Mask (NPM) based on the above techniques. We show the superior performance of our method by conducting extensive experiments on high-dimensional, pixel-input, and stochastic problems with various action redundancy tasks. Our code is public online at https://github.com/zhongdy15/npm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29652"
    },
    {
        "id": 14200,
        "title": "Toward Artificial General Intelligence: Deep Reinforcement Learning Method to AI in Medicine",
        "authors": "Daniel Schilling Weiss Nguyen, Richard Odigie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4236/jcc.2023.119006"
    },
    {
        "id": 14201,
        "title": "Heuristic deep reinforcement learning for online packing",
        "authors": "Chengbo Yang, Yonggui Lü, Jing Du, Ting Liu, Ronggui Dao",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3011479"
    },
    {
        "id": 14202,
        "title": "Deep Curious Feature Selection: A Recurrent, Intrinsic-Reward Reinforcement Learning Approach to Feature Selection",
        "authors": "Michal Moran, Goren Gordon",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2023.3282564"
    },
    {
        "id": 14203,
        "title": "A Satellite Adaptive Modulation Coding Method Based on Deep Reinforcement Learning",
        "authors": "Xin Zhou, Wenfeng Li, Kanglian Zhao",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaii59460.2023.10497307"
    },
    {
        "id": 14204,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 14205,
        "title": "Playing Various Strategies in Dominion with Deep Reinforcement Learning",
        "authors": "Jasper Gerigk, Steve Engels",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "Deck-building games, like Dominion, present an unsolved challenge for game AI research. The complexity arising from card interactions and the relative strength of strategies depending on the game configuration result in computer agents being limited to simple strategies. This paper describes the first application of recent advances in Geometric Deep Learning to deck-building games. We utilize a comprehensive multiset-based game representation and train the policy using a Soft Actor-Critic algorithm adapted to support variable-size sets of actions. The proposed model is the first successful learning-based agent that makes all decisions without relying on heuristics and supports a broader set of game configurations. It exceeds the performance of all previous learning-based approaches and is only outperformed by search-based approaches in certain game configurations. In addition, the paper presents modifications that induce agents to exhibit novel human-like play strategies. Finally, we show that learning strong strategies based on card combinations requires a reinforcement learning algorithm capable of discovering and executing a precise strategy while ignoring simpler suboptimal policies with higher immediate rewards.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aiide.v19i1.27518"
    },
    {
        "id": 14206,
        "title": "Airline dynamic pricing with patient customers using deep exploration-based reinforcement learning",
        "authors": "Seongbae Jo, Gyu M. Lee, Ilkyeong Moon",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108073"
    },
    {
        "id": 14207,
        "title": "Toward complete coverage planning using deep reinforcement learning by trapezoid-based transformable robot",
        "authors": "Dinh Tung Vo, Anh Vu Le, Tri Duc Ta, Minh Tran, Phan Van Duc, Minh Bui Vu, Nguyen Huu Khanh Nhan",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105999"
    },
    {
        "id": 14208,
        "title": "A deep reinforcement learning method for job shop scheduling integrated with automated guided vehicles",
        "authors": "Shuting Huang, Baigang Du, Yuying Rong",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3025432"
    },
    {
        "id": 14209,
        "title": "TEAMSTER: Model-based reinforcement learning for ad hoc teamwork",
        "authors": "João G. Ribeiro, Gonçalo Rodrigues, Alberto Sardinha, Francisco S. Melo",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104013"
    },
    {
        "id": 14210,
        "title": "End-to-End Deep Reinforcement Learning for Conversation Disentanglement",
        "authors": "Karan Bhukar, Harshit Kumar, Dinesh Raghu, Ajay Gupta",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Collaborative Communication platforms (e.g., Slack) support multi-party conversations which contain a large number of messages on shared channels. Multiple conversations intermingle within these messages. The task of conversation disentanglement is to cluster these intermingled messages into conversations. Existing approaches are trained using loss functions that optimize only local decisions, i.e. predicting reply-to links for each message and thereby creating clusters of conversations. In this work, we propose an end-to-end reinforcement learning (RL) approach that directly optimizes a global metric. We observe that using existing global metrics such as variation of information and adjusted rand index as a reward for the RL agent deteriorates its performance. This behaviour is because these metrics completely ignore the reply-to links between messages (local decisions) during reward computation. Therefore, we propose a novel thread-level reward function that captures the global metric without ignoring the local decisions. Through experiments on the Ubuntu IRC dataset, we demonstrate that the proposed RL model improves the performance on both link-level and conversation-level metrics.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26480"
    },
    {
        "id": 14211,
        "title": "Deep Reinforcement Learning for Early Diagnosis of Lung Cancer",
        "authors": "Yifan Wang, Qining Zhang, Lei Ying, Chuan Zhou",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Lung cancer remains the leading cause of cancer-related death worldwide, and early diagnosis of lung cancer is critical for improving the survival rate of patients. Performing annual low-dose computed tomography (LDCT) screening among high-risk populations is the primary approach for early diagnosis. However, after each screening, whether to continue monitoring (with follow-up screenings) or to order a biopsy for diagnosis remains a challenging decision to make. Continuing with follow-up screenings may lead to delayed diagnosis but ordering a biopsy without sufficient evidence incurs unnecessary risk and cost. In this paper, we tackle the problem by an optimal stopping approach. Our proposed algorithm, called EarlyStop-RL, utilizes the structure of the Snell envelope for optimal stopping, and model-free deep reinforcement learning for making diagnosis decisions. Through evaluating our algorithm on a commonly used clinical trial dataset (the National Lung Screening Trial), we demonstrate that EarlyStop-RL has the potential to greatly enhance risk assessment and early diagnosis of lung cancer, surpassing the performance of two widely adopted clinical models, namely the Lung-RADS and the Brock model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30248"
    },
    {
        "id": 14212,
        "title": "Obstacle-Avoidance X-Architecture Steiner Minimal Tree Algorithm Based on Deep Reinforcement Learning",
        "authors": "Jie You, Yuhan Zhu, Xing Huang, Liliang Yang, Genggeng Liu",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiotsys58602.2023.00046"
    },
    {
        "id": 14213,
        "title": "Distributed deep reinforcement learning-based gas supply system coordination management method for solid oxide fuel cell",
        "authors": "Jiawen Li, Haoyang Cui, Wei Jiang",
        "published": "2023-4",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105818"
    },
    {
        "id": 14214,
        "title": "Combination of Reinforcement and Deep Learning for EEG Channel Optimization on Brain-Machine Interface Systems",
        "authors": "Goragod Pongthanisorn, Aya Shirai, Satoki Sugiyama, Genci Capi",
        "published": "2023-2-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10066973"
    },
    {
        "id": 14215,
        "title": "Distributed web hacking by adaptive consensus-based reinforcement learning",
        "authors": "Nemanja Ilić, Dejan Dašić, Miljan Vučetić, Aleksej Makarov, Ranko Petrović",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104032"
    },
    {
        "id": 14216,
        "title": "Deep Q-Network (DQN): Reinforcement Learning Based Approach for Secure Social Distancing Adherence with SARS-CoV-2 in Public Places",
        "authors": "",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "The estimates taken far and wide to deal with the SARS-CoV-2 pandemic, limiting travel, shuttering superfluous organizations and implementing all social separating arrangements, are having serious monetary consequences. a noteworthy decrease in economic action spread over the economy the world, lasting in excess of a few months, typically clear in genuine GDP. Where it is formally announced a downturn. To quicken a strong expected recuperation with rising protectionism and unilateralism. There is a requirement for individuals to come out and face the circumstance. Despite the fact that it is established that separating individuals and investigating their contacts would be inadequate to control the SARS-CoV-2 pandemic, in light of the fact that there would be an excess of deferral between the beginning of indications and seclusion. Consequently, in these sorts of conditions it is to keep people groups from infection influence and early anticipation of these tainted individuals may prompt re development the economy too. We built up a numerical model utilizing profound Deep reinforcement learning (DRL) which is poised to revolutionize the field of artificial intelligence and the use of central algorithms in deep RL, specifically the deep Q-network (DQN), trust region policy optimization (TRPO). The proposed astute checking framework can be utilized as a reciprocal apparatus to be introduced at better places and consequently screen individuals receiving the security rules. With these prudent estimations, people will have the option to win this battle against SARS-CoV-2.",
        "keywords": "",
        "link": "http://dx.doi.org/10.46632/jdaai/2/3/11"
    },
    {
        "id": 14217,
        "title": "Controlling Neural Style Transfer with Deep Reinforcement Learning",
        "authors": "Chengming Feng, Jing Hu, Xin Wang, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Siwei Lyu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Controlling the degree of stylization in the Neural Style Transfer (NST) is a little tricky since it usually needs hand-engineering on hyper-parameters. In this paper, we propose the first deep Reinforcement Learning (RL) based architecture that splits one-step style transfer into a step-wise process for the NST task. Our RL-based method tends to preserve more details and structures of the content image in early steps, and synthesize more style patterns in later steps. It is a user-easily-controlled style-transfer method. Additionally, as our RL-based model performs the stylization progressively, it is lightweight and has lower computational complexity than existing one-step Deep Learning (DL) based models. Experimental results demonstrate the effectiveness and robustness of our method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/12"
    },
    {
        "id": 14218,
        "title": "Learning reward machines: A study in partially observable reinforcement learning",
        "authors": "Rodrigo Toro Icarte, Toryn Q. Klassen, Richard Valenzano, Margarita P. Castro, Ethan Waldie, Sheila A. McIlraith",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.103989"
    },
    {
        "id": 14219,
        "title": "Fractional Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing",
        "authors": "Lyudong Jin, Ming Tang, Meng Zhang, Hao Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Mobile edge computing (MEC) is a promising paradigm for real-time applications with intensive computational needs (e.g., autonomous driving), as it can reduce the processing delay. In this work, we focus on the timeliness of computational-intensive updates, measured by Age-of-Information (AoI), and study how to jointly optimize the task updating and offloading  policies for AoI with fractional form. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The uncertain edge load dynamics, the nature of the fractional objective, and hybrid continuous-discrete action space (due to the joint optimization) make this problem challenging and existing approaches not directly applicable. To this end, we propose a fractional reinforcement learning (RL) framework and prove its convergence. We further design a model-free fractional deep RL (DRL) algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 57.6% compared with several non-fractional benchmarks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i11.29192"
    },
    {
        "id": 14220,
        "title": "Metrics for Assessing Generalization of Deep Reinforcement Learning in Parameterized Environments",
        "authors": "Maciej Aleksandrowicz, Joanna Jaworek-Korjakowska",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nIn this work, a study focusing on proposing generalization metrics for Deep Reinforcement Learning (DRL) algorithms was performed. The experiments were conducted in DeepMind Control (DMC) benchmark suite with parameterized environments. The performance of three DRL algorithms in selected ten tasks from the DMC suite has been analysed with existing generalization gap formalism and the proposed ratio and decibel metrics. The results were presented with the proposed methods: average transfer metric and plot for environment normal distribution. These efforts allowed to highlight major changes in the model’s performance and add more insights about making decisions regarding models’ requirements.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2478/jaiscr-2024-0003"
    },
    {
        "id": 14221,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 14222,
        "title": "Deep reinforcement learning based planning method in state space for lunar rovers",
        "authors": "Ai Gao, Siyao Lu, Rui Xu, Zhaoyu Li, Bang Wang, Shengying Zhu, Yuhui Gao, Bo Pan",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107287"
    },
    {
        "id": 14223,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 14224,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 14225,
        "title": "Learning to schedule job shop scheduling problem with maintenance time using graph node embedding and deep reinforcement learning",
        "authors": "Xiangzhen Fang, Jin Li, Yilei Wang",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2684742"
    },
    {
        "id": 14226,
        "title": "Gamma and vega hedging using deep distributional reinforcement learning",
        "authors": "Jay Cao, Jacky Chen, Soroush Farghadani, John Hull, Zissis Poulos, Zeyu Wang, Jun Yuan",
        "published": "2023-2-22",
        "citations": 3,
        "abstract": "We show how reinforcement learning can be used in conjunction with quantile regression to develop a hedging strategy for a trader responsible for derivatives that arrive stochastically and depend on a single underlying asset. We assume that the trader makes the portfolio delta-neutral at the end of each day by taking a position in the underlying asset. We focus on how trades in options can be used to manage gamma and vega. The option trades are subject to transaction costs. We consider three different objective functions. We reach conclusions on how the optimal hedging strategy depends on the trader's objective function, the level of transaction costs, and the maturity of the options used for hedging. We also investigate the robustness of the hedging strategy to the process assumed for the underlying asset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/frai.2023.1129370"
    },
    {
        "id": 14227,
        "title": "A reinforcement learning approach to Automatic Voltage Regulator system",
        "authors": "Mustafa Sinasi Ayas, Ali Kivanc Sahin",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106050"
    },
    {
        "id": 14228,
        "title": "Recent Studies on Deep Reinforcement Learning in RIS-UAV Communication Networks",
        "authors": "Tri-Hai Nguyen, Heejae Park, Laihyuk Park",
        "published": "2023-2-20",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067052"
    },
    {
        "id": 14229,
        "title": "Instance-Wise Laplace Mechanism via Deep Reinforcement Learning (Student Abstract)",
        "authors": "Sehyun Ryu, Hosung Joo, Jonggyu Jang, Hyun Jong Yang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Recent research has shown a growing interest in per-instance differential privacy (pDP), highlighting the fact that each data instance within a dataset may incur distinct levels of privacy loss.\nHowever, conventional additive noise mechanisms apply identical noise to all query outputs, thereby deteriorating data statistics.\nIn this study, we propose an instance-wise Laplace mechanism, which adds non-identical Laplace noises to the query output for each data instance.\nA challenge arises from the complex interaction of additive noise, where the noise introduced to individual instances impacts the pDP of other instances, adding complexity and resilience to straightforward solutions.\nTo tackle this problem, we introduce an instance-wise Laplace mechanism algorithm via deep reinforcement learning and validate its ability to better preserve data statistics on a real dataset, compared to the original Laplace mechanism.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30506"
    },
    {
        "id": 14230,
        "title": "I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives",
        "authors": "R. Teal Witter, Lucas Rosenblatt",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The open streets initiative \"opens\" streets to pedestrians and bicyclists by closing them to cars and trucks. The initiative, adopted by many cities across North America, increases community space in urban environments. But could open streets also make cities safer and less congested? We study this question by framing the choice of which streets to open as a reinforcement learning problem. In order to simulate the impact of opening streets, we first compare models for predicting vehicle collisions given network and temporal data. We find that a recurrent graph neural network, leveraging the graph structure and the short-term temporal dependence of the data, gives the best predictive performance. Then, with the ability to simulate collisions and traffic, we frame a reinforcement learning problem to find which streets to open. We compare the streets in the open streets initiative to those proposed by a Q-learning algorithm. We find that the streets proposed by the Q-learning algorithm have reliably better outcomes, while streets already selected by the open streets initiative have similar outcomes to randomly selected streets. We present our work as a step toward principally choosing which streets to open for safer and less congested cities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30250"
    },
    {
        "id": 14231,
        "title": "Reinforcement learning from expert demonstrations with application to redundant robot control",
        "authors": "Jorge Ramirez, Wen Yu",
        "published": "2023-3",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105753"
    },
    {
        "id": 14232,
        "title": "Reinforcement Learning-Based Simulation of Seal Engraving Robot in the Context of Artificial Intelligence",
        "authors": "Ran Tan,  Khayril Anwar Bin Khairudin",
        "published": "2024-3-21",
        "citations": 0,
        "abstract": "The rapid development of robotics technology has made people's lives and work more convenient and efficient. The research and simulation of robots combined with reinforcement learning intelligent algorithms have become a hotspot in various fields of robot applications. In view of this, this study is based on deep reinforcement learning convolutional neural networks, combined with point cloud models, proximal strategy optimization algorithms, and flexible action evaluation algorithms. A seal cutting robot based on deep reinforcement learning has been proposed. The final results show that the descent speed of the seal cutting robot with the root mean square difference as the performance standard is about 1% faster than the flexible action evaluation algorithm. About 2% faster than the proximal strategy optimization algorithm. It is about 4% faster than the deep deterministic strategy gradient algorithm. This indicates that the research model has certain advantages in terms of actual accuracy after cutting. The fluctuation of this model is about 10% smaller than the evaluation of flexible actions and about 60% smaller than the gradient of deep deterministic strategies. Therefore, the research model has the highest overall stability without falling into local optima. In addition, compared to the near end strategy optimization algorithm, it falls into local optima, resulting in a low coincidence degree of about 17%. The deep deterministic strategy gradient algorithm has a large fluctuation amplitude during the seal cutting process, and the overall curve is relatively slow, with a final overlap of about 70%. The overlap degree of flexible action evaluation is slightly higher by about 83%. The maximum stability of the model's overlap is best around 90%. Through experiments, it can be found that the seal cutting robot proposed in the study based on deep reinforcement learning maintains certain advantages in performance indicators in various types of tests.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37965/jait.2024.0453"
    },
    {
        "id": 14233,
        "title": "FanoutNet: A Neuralized PCB Fanout Automation Method Using Deep Reinforcement Learning",
        "authors": "Haiyun Li, Jixin Zhang, Ning Xu, Mingyu Liu",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "In modern electronic manufacturing processes, multi-layer Printed Circuit Board (PCB) routing requires connecting more than hundreds of nets with perplexing topology under complex routing constraints and highly limited resources, so that takes intense effort and time of human engineers. PCB fanout as a pre-design of PCB routing has been proved to be an ideal technique to reduce the complexity of PCB routing by pre-allocating resources and pre-routing. However, current PCB fanout design heavily relies on the experience of human engineers, and there is no existing solution for PCB fanout automation in industry, which limits the quality of PCB routing automation. To address the problem, we propose a neuralized PCB fanout method by deep reinforcement learning. To the best of our knowledge, we are the first in the literature to propose the automation method for PCB fanout. We combine with Convolution Neural Network (CNN) and attention-based network to train our fanout policy model and value model. The models learn representations of PCB layout and netlist to make decisions and evaluations in place of human engineers. We employ Proximal Policy Optimization (PPO) to update the parameters of the models. In addition, we apply our PCB fanout method to a PCB router to improve the quality of PCB routing. Extensive experimental results on real-world industrial PCB benchmarks demonstrate that our approach achieves 100% routability in all industrial cases and improves wire length by an average of 6.8%, which makes a significant improvement compared with the state-of-the-art methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i7.26030"
    },
    {
        "id": 14234,
        "title": "Decision-making for Overtaking in Specific Unmanned Driving Scenarios based on Deep Reinforcement Learning",
        "authors": "Chenyang Zhang, Zhiqing Huang, Shuqing Wang, Yan Hong",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciba56860.2023.10164889"
    },
    {
        "id": 14235,
        "title": "Deep learning and ensemble deep learning for circRNA-RBP interaction prediction in the last decade: A review",
        "authors": "Dilan Lasantha, Sugandima Vidanagamachchi, Sam Nallaperuma",
        "published": "2023-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106352"
    },
    {
        "id": 14236,
        "title": "Self-learning swimming of a three-disk microrobot in a viscous and stochastic environment using reinforcement learning",
        "authors": "Hossein Abdi, Hossein Nejat Pishkenari",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106188"
    },
    {
        "id": 14237,
        "title": "Reinforcement Learning for Stock Option Trading",
        "authors": "James Garza",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aics60730.2023.10470596"
    },
    {
        "id": 14238,
        "title": "Know Your Enemy: Identifying Adversarial Behaviours in Deep Reinforcement Learning Agents (Student Abstract)",
        "authors": "Seán Caulfield Curley, Karl Mason, Patrick Mannion",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "It has been shown that an agent can be trained with an adversarial policy which achieves high degrees of success against a state-of-the-art DRL victim despite taking unintuitive actions. This prompts the question: is this adversarial behaviour detectable through the observations of the victim alone? We find that widely used classification methods such as random forests are only able to achieve a maximum of ≈71% test set accuracy when classifying an agent for a single timestep. However, when the classifier inputs are treated as time-series data, test set classification accuracy is increased significantly to ≈98%. This is true for both classification of episodes as a whole, and for “live” classification at each timestep in an episode. These classifications can then be used to “react” to incoming attacks and increase the overall win rate against Adversarial opponents by approximately 17%. Classification of the victim’s own internal activations in response to the adversary is shown to achieve similarly impressive accuracy while also offering advantages like increased transferability to other domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26948"
    },
    {
        "id": 14239,
        "title": "Deep Anomaly Detection and Search via Reinforcement Learning (Student Abstract)",
        "authors": "Chao Chen, Dawei Wang, Feng Mao, Zongzhang Zhang, Yang Yu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Semi-supervised anomaly detection is a data mining task which aims at learning features from partially-labeled datasets. We propose Deep Anomaly Detection and Search (DADS) with reinforcement learning. During the training process, the agent searches for possible anomalies in unlabeled dataset to enhance performance. Empirically, we compare DADS with several methods in the settings of leveraging known anomalies to detect both other known and unknown anomalies. Results show that DADS achieves good performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26950"
    },
    {
        "id": 14240,
        "title": "Distributed multi-robot obstacle avoidance via logarithmic map-based deep reinforcement learning",
        "authors": "Jiafeng Ma, Guangda Chen, Peng Jiang, ZhiWeng Zhang, Jinyu Cao, Jianming Zhang",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671168"
    },
    {
        "id": 14241,
        "title": "An Automated Deep Reinforcement Learning Pipeline for Dynamic Pricing",
        "authors": "Reza Refaei Afshar, Jason Rhuggenaath, Yingqian Zhang, Uzay Kaymak",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3186292"
    },
    {
        "id": 14242,
        "title": "Cheaper and Faster: Distributed Deep Reinforcement Learning with Serverless Computing",
        "authors": "Hanfei Yu, Jian Li, Yang Hua, Xu Yuan, Hao Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) has gained immense success in many applications, including gaming AI, robotics, and system scheduling. Distributed algorithms and architectures have been vastly proposed (e.g., actor-learner architecture) to accelerate DRL training with large-scale server-based clusters. However, training on-policy algorithms with the actor-learner architecture unavoidably induces resource wasting due to synchronization between learners and actors, thus resulting in significantly extra billing. As a promising alternative, serverless computing naturally fits on-policy synchronization and alleviates resource wasting in distributed DRL training with pay-as-you-go pricing. Yet, none has leveraged serverless computing to facilitate DRL training.  This paper proposes MinionsRL, the first serverless distributed DRL training framework that aims to accelerate DRL training- and cost-efficiency with dynamic actor scaling. We prototype MinionsRL on top of Microsoft Azure Container Instances and evaluate it with popular DRL tasks from OpenAI Gym. Extensive experiments show that MinionsRL reduces total training time by up to 52% and training cost by 86% compared to latest solutions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29592"
    },
    {
        "id": 14243,
        "title": "Distributed Deep Reinforcement Learning for Autonomous Iot Healthcare Devices in the Cloud",
        "authors": "Aasheesh Shukla, Hemant Singh Pokhariya, Jacob Michaelson, K Laxminarayanamma, Mukesh Kumar, Om Krishna",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiihi57871.2023.10488976"
    },
    {
        "id": 14244,
        "title": "Axillary Lymph Node Metastasis prediction Using Deep Reinforcement Learning on Primary Tumor Biopsy Slides",
        "authors": "Nusrat Mohi Ud Din, Saqib Ul Sabha, Muzafar Rasool Bhat, Assif Assad",
        "published": "2023-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aisp57993.2023.10135039"
    },
    {
        "id": 14245,
        "title": "Robustness Verification of Deep Reinforcement Learning Based Control Systems Using Reward Martingales",
        "authors": "Dapeng Zhi, Peixin Wang, Cheng Chen, Min Zhang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Deep Reinforcement Learning (DRL) has gained prominence as an effective approach for control systems. However, its practical deployment is impeded by state perturbations that can severely impact system performance. Addressing this critical challenge requires robustness verification about system performance, which involves tackling two quantitative questions: (i) how to establish guaranteed bounds for expected cumulative rewards, and (ii) how to determine tail bounds for cumulative rewards. In this work, we present the first approach for robustness verification of DRL-based control systems by introducing  reward martingales, which offer a rigorous mathematical foundation to characterize the impact of state perturbations on system performance in terms of cumulative rewards. Our verified results provide provably quantitative certificates for the two questions. We then show that reward martingales can be implemented and trained via neural networks, against different types of control policies. Experimental results demonstrate that our certified bounds tightly enclose simulation outcomes on various DRL-based control systems, indicating the effectiveness and generality of the proposed approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i18.29976"
    },
    {
        "id": 14246,
        "title": "Deep reinforcement learning-PID based supervisor control method for indirect-contact heat transfer processes in energy systems",
        "authors": "Xuan Wang, Jinwen Cai, Rui Wang, Gequn Shu, Hua Tian, Mingtao Wang, Bowen Yan",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105551"
    },
    {
        "id": 14247,
        "title": "A deep reinforcement learning model for dynamic job-shop scheduling problem with uncertain processing time",
        "authors": "Xinquan Wu, Xuefeng Yan, Donghai Guan, Mingqiang Wei",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107790"
    },
    {
        "id": 14248,
        "title": "Subtask-masked curriculum learning for reinforcement learning with application to UAV maneuver decision-making",
        "authors": "Yueqi Hou, Xiaolong Liang, Maolong Lv, Qisong Yang, Yang Li",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106703"
    },
    {
        "id": 14249,
        "title": "Fragility, robustness and antifragility in deep learning",
        "authors": "Chandresh Pravin, Ivan Martino, Giuseppe Nicosia, Varun Ojha",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104060"
    },
    {
        "id": 14250,
        "title": "Safety Margins for Reinforcement Learning",
        "authors": "Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00026"
    },
    {
        "id": 14251,
        "title": "Robust and efficient task scheduling for robotics applications with reinforcement learning",
        "authors": "Mateusz Tejer, Rafal Szczepanski, Tomasz Tarczewski",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107300"
    },
    {
        "id": 14252,
        "title": "Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language",
        "authors": "Zhourui Guo, Meng Yao, Yang Yu, Qiyue Yin",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3638584.3638661"
    },
    {
        "id": 14253,
        "title": "PiCor: Multi-Task Deep Reinforcement Learning with Policy Correction",
        "authors": "Fengshuo Bai, Hongming Zhang, Tianyang Tao, Zhiheng Wu, Yanna Wang, Bo Xu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Multi-task deep reinforcement learning (DRL) ambitiously aims to train a general agent that masters multiple tasks simultaneously. However, varying learning speeds of different tasks compounding with negative gradients interference makes policy learning inefficient. In this work, we propose PiCor, an efficient multi-task DRL framework that splits learning into policy optimization and policy correction phases. The policy optimization phase improves the policy by any DRL algothrim on the sampled single task without considering other tasks. The policy correction phase first constructs an adaptive adjusted performance constraint set. Then the intermediate policy learned by the first phase is constrained to the set, which controls the negative interference and balances the learning speeds across tasks. Empirically, we demonstrate that PiCor outperforms previous methods and significantly improves sample efficiency on simulated robotic manipulation and continuous control tasks. We additionally show that adaptive weight adjusting can further improve data efficiency and performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25825"
    },
    {
        "id": 14254,
        "title": "Utilizing Multi-Agent Deep Reinforcement Learning for Autonomous Intersection Management Systems: A Promising Approach",
        "authors": "Mostafa K. Ghaith, Mohamed M. Rehaan, N. Shouman, Y. Abdalla, Omar M. Shehata",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/airc57904.2023.10303022"
    },
    {
        "id": 14255,
        "title": "IoT-based pest detection and classification using deep features with enhanced deep learning strategies",
        "authors": "Prasath B., M. Akila",
        "published": "2023-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105985"
    },
    {
        "id": 14256,
        "title": "Delay Optimization of IoT-Edge Computing in Smart Grid Using Deep Reinforcement Learning",
        "authors": "Muhammad Akmal Jafri, Fauzun Abdullah Asuhaimi, Ahmad Fikri Dahlan, Mohd Khairil Mohd Hatta",
        "published": "2023-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iicaiet59451.2023.10292063"
    },
    {
        "id": 14257,
        "title": "Multi-agent Hierarchical Deep Reinforcement Learning for Operation Optimization of Grid-interactive Efficient Commercial Buildings",
        "authors": "Zhiqiang Chen, Liang Yu, Shuang zhang, Shushan Hu, Chao Shen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2024.3366869"
    },
    {
        "id": 14258,
        "title": "Deep reinforcement learning for the rapid on-demand design of mechanical metamaterials with targeted nonlinear deformation responses",
        "authors": "Nathan K. Brown, Anthony P. Garland, Georges M. Fadel, Gang Li",
        "published": "2023-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106998"
    },
    {
        "id": 14259,
        "title": "Autonomous intelligent control of earth pressure balance shield machine based on deep reinforcement learning",
        "authors": "Xuanyu Liu, Wenshuai Zhang, Cheng Shao, Yudong Wang, Qiumei Cong",
        "published": "2023-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106702"
    },
    {
        "id": 14260,
        "title": "Automated function development for emission control with deep reinforcement learning",
        "authors": "Lucas Koch, Mario Picerno, Kevin Badalian, Sung-Yong Lee, Jakob Andert",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105477"
    },
    {
        "id": 14261,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 14262,
        "title": "A reinforcement learning based computational intelligence approach for binary optimization problems: The case of the set-union knapsack problem",
        "authors": "Fehmi Burcin Ozsoydan, İlker Gölcük",
        "published": "2023-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105688"
    },
    {
        "id": 14263,
        "title": "Anti-drifting Feature Selection via Deep Reinforcement Learning (Student Abstract)",
        "authors": "Aoran Wang, Hongyang Yang, Feng Mao, Zongzhang Zhang, Yang Yu, Xiaoyang Liu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Feature selection (FS) is a crucial procedure in machine learning pipelines for its significant benefits in removing data redundancy and mitigating model overfitting. Since concept drift is a widespread phenomenon in streaming data and could severely affect model performance, effective FS on concept drifting data streams is imminent. However, existing state-of-the-art FS algorithms fail to adjust their selection strategy adaptively when the effective feature subset changes, making them unsuitable for drifting streams. In this paper, we propose a dynamic FS method that selects effective features on concept drifting data streams via deep reinforcement learning. Specifically, we present two novel designs: (i) a skip-mode reinforcement learning environment that shrinks action space size for high-dimensional FS tasks; (ii) a curiosity mechanism that generates intrinsic rewards to address the long-horizon exploration problem. The experiment results show that our proposed method outperforms other FS methods and can dynamically adapt to concept drifts.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.27038"
    },
    {
        "id": 14264,
        "title": "A general motion controller based on deep reinforcement learning for an autonomous underwater vehicle with unknown disturbances",
        "authors": "Fei Huang, Jian Xu, Di Wu, Yunfei Cui, Zheping Yan, Wen Xing, Xun Zhang",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105589"
    },
    {
        "id": 14265,
        "title": "Deep-Reinforcement-Learning-Based Resource Allocation for Energy Harvesting D2D Communication",
        "authors": "Yuan Qi, Shuqin Geng",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icecai58670.2023.10176743"
    },
    {
        "id": 14266,
        "title": "Learning Pessimism for Reinforcement Learning",
        "authors": "Edoardo Cetin, Oya Celiktutan",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25852"
    },
    {
        "id": 14267,
        "title": "An Efficient Deep Reinforcement Learning Algorithm for Solving Imperfect Information Extensive-Form Games",
        "authors": "Linjian Meng, Zhenxing Ge, Pinzhuo Tian, Bo An, Yang Gao",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "One of the most popular methods for learning Nash equilibrium (NE) in large-scale imperfect information extensive-form games (IIEFGs) is the neural variants of counterfactual regret minimization (CFR). CFR is a special case of Follow-The-Regularized-Leader (FTRL). At each iteration, the neural variants of CFR update the agent's strategy via the estimated counterfactual regrets. Then, they use neural networks to approximate the new strategy, which incurs an approximation error. These approximation errors will accumulate since the counterfactual regrets at iteration t are estimated using the agent's past approximated strategies. Such accumulated approximation error causes poor performance. To address this accumulated approximation error, we propose a novel FTRL algorithm called FTRL-ORW, which does not utilize the agent's past strategies to pick the next iteration strategy. More importantly, FTRL-ORW can update its strategy via the trajectories sampled from the game, which is suitable to solve large-scale IIEFGs since sampling multiple actions for each information set is too expensive in such games. However, it remains unclear which algorithm to use to compute the next iteration strategy for FTRL-ORW when only such sampled trajectories are revealed at iteration t. To address this problem and scale FTRL-ORW to large-scale games, we provide a model-free method called Deep FTRL-ORW, which computes the next iteration strategy using model-free Maximum Entropy Deep Reinforcement Learning. Experimental results on two-player zero-sum IIEFGs show that Deep FTRL-ORW significantly outperforms existing model-free neural methods and OS-MCCFR.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i5.25722"
    },
    {
        "id": 14268,
        "title": "Mimicking Electronic Gaming Machine Player Behavior Using Reinforcement Learning",
        "authors": "Gaurav Jariwala, Vlado Keselj",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21428/594757db.6b6b324c"
    },
    {
        "id": 14269,
        "title": "Autonomous Path Optimization in Unfamiliar Map Through Deep Reinforcement Learning",
        "authors": "Longxin Wei, Kit Guan Lim, Min Keng Tan, Chung Fan Liau, Tianlei Wang, Kenneth Tze Kin Teo",
        "published": "2023-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iicaiet59451.2023.10291248"
    },
    {
        "id": 14270,
        "title": "Air defense intelligent weapon target assignment method based on deep reinforcement learning",
        "authors": "Qiang Fu, Chengli Fan, Yong Heng",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3580219.3580247"
    },
    {
        "id": 14271,
        "title": "A Transfer Approach Using Graph Neural Networks in Deep Reinforcement Learning",
        "authors": "Tianpei Yang, Heng You, Jianye Hao, Yan Zheng, Matthew E. Taylor",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Transfer learning (TL) has shown great potential to improve Reinforcement Learning (RL) efficiency by leveraging prior knowledge in new tasks. However, much of the existing TL research focuses on transferring knowledge between tasks that share the same state-action spaces.  Further, transfer from multiple source tasks that have different state-action spaces is more challenging and needs to be solved urgently to improve the generalization and practicality of the method in real-world scenarios. This paper proposes TURRET (Transfer Using gRaph neuRal nETworks), to utilize the generalization capabilities of Graph Neural Networks (GNNs) to facilitate efficient and effective multi-source policy transfer learning in the state-action mismatch setting.  TURRET learns a semantic representation by accounting for the intrinsic property of the agent through GNNs, which leads to a unified state embedding space for all tasks. As a result, TURRET achieves more efficient transfer with strong generalization ability between different tasks and can be easily combined with existing Deep RL algorithms. Experimental results show that TURRET significantly outperforms other TL methods on multiple continuous action control tasks, successfully transferring across robots with different state-action spaces.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29571"
    },
    {
        "id": 14272,
        "title": "Energy Efficiency of IoT-Edge Computing in Smart Grid Using Deep Reinforcement Learning",
        "authors": "Ahmad Fikri bin Dahari, Fauzun Abdullah Asuhaimi, Muhammad Akmal bin Jafri, Mohd Khairil Mohd Hatta",
        "published": "2023-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iicaiet59451.2023.10291227"
    },
    {
        "id": 14273,
        "title": "A Novel Deep Reinforcement Learning (DRL) Algorithm to Apply Artificial Intelligence-Based Maintenance in Electrolysers",
        "authors": "Abiodun Abiola, Francisca Segura Manzano, José Manuel Andújar",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "Hydrogen provides a clean source of energy that can be produced with the aid of electrolysers. For electrolysers to operate cost-effectively and safely, it is necessary to define an appropriate maintenance strategy. Predictive maintenance is one of such strategies but often relies on data from sensors which can also become faulty, resulting in false information. Consequently, maintenance will not be performed at the right time and failure will occur. To address this problem, the artificial intelligence concept is applied to make predictions on sensor readings based on data obtained from another instrument within the process. In this study, a novel algorithm is developed using Deep Reinforcement Learning (DRL) to select the best feature(s) among measured data of the electrolyser, which can best predict the target sensor data for predictive maintenance. The features are used as input into a type of deep neural network called long short-term memory (LSTM) to make predictions. The DLR developed has been compared with those found in literatures within the scope of this study. The results have been excellent and, in fact, have produced the best scores. Specifically, its correlation coefficient with the target variable was practically total (0.99). Likewise, the root-mean-square error (RMSE) between the experimental sensor data and the predicted variable was only 0.1351.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16120541"
    },
    {
        "id": 14274,
        "title": "Role of Artificial Intelligence, Machine Learning, Deep Learning for Sericulture: A Technological Perspective",
        "authors": "Pradeepto Pal, Devendra Singh, Rajesh Singh, Anita Gehlot, Shaik Vaseem Akram",
        "published": "2023-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aisc56616.2023.10085573"
    },
    {
        "id": 14275,
        "title": "Competitive-Cooperative Multi-Agent Reinforcement Learning for Auction-based Federated Learning",
        "authors": "Xiaoli Tang, Han Yu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Auction-based Federated Learning (AFL) enables open collaboration among self-interested data consumers and data owners. Existing AFL approaches cannot manage the mutual influence among multiple data consumers competing to enlist data owners. Moreover, they cannot support a single data owner to join multiple data consumers simultaneously. To bridge these gaps, we propose the Multi-Agent Reinforcement Learning for AFL (MARL-AFL) approach to steer data consumers to bid strategically\n\ntowards an equilibrium with desirable overall system characteristics. We design a temperature-based reward reassignment scheme to make tradeoffs between cooperation and competition among AFL data consumers. In this way, it can reach an equilibrium state that ensures individual data consumers can achieve good utility, while preserving system-level social welfare. To circumvent potential collusion behaviors among data consumers, we introduce a bar agent to set a personalized bidding\n\nlower bound for each data consumer. Extensive experiments on six commonly adopted benchmark datasets show that MARL-AFL is significantly more advantageous compared to six state-of-the-art approaches, outperforming the best by 12.2%, 1.9% and 3.4% in terms of social welfare, revenue and accuracy, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/474"
    },
    {
        "id": 14276,
        "title": "Reward-respecting subtasks for model-based reinforcement learning",
        "authors": "Richard S. Sutton, Marlos C. Machado, G. Zacharias Holland, David Szepesvari, Finbarr Timbers, Brian Tanner, Adam White",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104001"
    },
    {
        "id": 14277,
        "title": "Causal reinforcement learning based on Bayesian networks applied to industrial settings",
        "authors": "Gabriel Valverde, David Quesada, Pedro Larrañaga, Concha Bielza",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106657"
    },
    {
        "id": 14278,
        "title": "Learning Activities that Influence Deep Active Learning in Reading Circles Learning",
        "authors": "Qihui Hu",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaie56796.2023.00023"
    },
    {
        "id": 14279,
        "title": "Voltage control of DC–DC converters through direct control of power switches using reinforcement learning",
        "authors": "Omid Zandi, Javad Poshtan",
        "published": "2023-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105833"
    },
    {
        "id": 14280,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 14281,
        "title": "Traffic management approaches using machine learning and deep learning techniques: A survey",
        "authors": "Hanan Almukhalfi, Ayman Noor, Talal H. Noor",
        "published": "2024-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108147"
    },
    {
        "id": 14282,
        "title": "Balancing therapeutic effect and safety in ventilator parameter recommendation: An offline reinforcement learning approach",
        "authors": "Bo Zhang, Xihe Qiu, Xiaoyu Tan",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107784"
    },
    {
        "id": 14283,
        "title": "Construction of English Teaching System Based on Deep Learning Models and Artificial Intelligence Technology",
        "authors": "Zheng Zhang",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aicit59054.2023.10277704"
    },
    {
        "id": 14284,
        "title": "Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning",
        "authors": "Chenjia Bai, Lingxiao Wang, Jianye Hao, Zhuoran Yang, Bin Zhao, Zhen Wang, Xuelong Li",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104048"
    },
    {
        "id": 14285,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 14286,
        "title": "Optimal drug-dosing of cancer dynamics with fuzzy reinforcement learning and discontinuous reward function",
        "authors": "Chidentree Treesatayapun, Aldo Jonathan Muñoz-Vázquez",
        "published": "2023-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.105851"
    },
    {
        "id": 14287,
        "title": "Safe Exploration in Reinforcement Learning for Learning from Human Experts",
        "authors": "Jorge Ramirez, Wen Yu",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aibthings58340.2023.10292489"
    },
    {
        "id": 14288,
        "title": "The Boundaries of Copyright Protection for Deep Learning Technologies of Artificial Intelligence",
        "authors": "Jikuan Xu",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icbaie59714.2023.10281312"
    },
    {
        "id": 14289,
        "title": "Advances in Artificial Intelligence, Machine Learning and Deep Learning Applications",
        "authors": "Muhammad Salman Haleem",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "Recent advances in the field of artificial intelligence (AI) have been pivotal in enhancing the effectiveness and efficiency of many systems and in all fields of knowledge, including medical diagnosis [...]",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12183780"
    },
    {
        "id": 14290,
        "title": "Explain the Explainer: Interpreting Model-Agnostic Counterfactual Explanations of a Deep Reinforcement Learning Agent",
        "authors": "Ziheng Chen, Fabrizio Silvestri, Gabriele Tolomei, Jia Wang, He Zhu, Hongshik Ahn",
        "published": "2024-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3223892"
    },
    {
        "id": 14291,
        "title": "DeepValve: Development and experimental testing of a Reinforcement Learning control framework for occupant-centric heating in offices",
        "authors": "Amirreza Heidari, Dolaana Khovalyg",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106310"
    },
    {
        "id": 14292,
        "title": "Removing Radiographic Markers Using Deep Learning to Enable Image                     Sharing",
        "authors": "Ken Chang, Matthew D. Li",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1148/ryai.230369"
    },
    {
        "id": 14293,
        "title": "Guided Hierarchical Reinforcement Learning for Safe Urban Driving",
        "authors": "Mohamad Albilani, Amel Bouzeghoub",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00115"
    },
    {
        "id": 14294,
        "title": "Online Reinforcement Learning in Periodic MDP",
        "authors": "Ayush Aniket, Arpan Chattopadhyay",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2024.3375258"
    },
    {
        "id": 14295,
        "title": "A Scale-Independent Deep Reinforcement Learning Framework for Multi-UAV Communication Resource Allocation in Unmanned Logistics",
        "authors": "Lizhen Huang, Feipeng Wang, Chunhui Liu",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itaic58329.2023.10409035"
    },
    {
        "id": 14296,
        "title": "A deep reinforcement learning method for multi-heterogeneous robot systems based on global status",
        "authors": "Yanjiang Chen, Junqing Lin, Zhiyuan Yu, Zaiping Zheng, Chunyu Fu, Kui Huang",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3026373"
    },
    {
        "id": 14297,
        "title": "Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees",
        "authors": "Kai-Chieh Hsu, Allen Z. Ren, Duy P. Nguyen, Anirudha Majumdar, Jaime F. Fisac",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2022.103811"
    },
    {
        "id": 14298,
        "title": "Demand response model: A cooperative-competitive multi-agent reinforcement learning approach",
        "authors": "Eduardo J. Salazar, Veronica Rosero, Jawana Gabrielski, Mauricio E. Samper",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108273"
    },
    {
        "id": 14299,
        "title": "Solving combined economic and emission dispatch problems using reinforcement learning-based adaptive differential evolution algorithm",
        "authors": "Wenguan Luo, Xiaobing Yu, Yifan Wei",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107002"
    },
    {
        "id": 14300,
        "title": "Home Energy Management with V2X Capability using Reinforcement Learning",
        "authors": "Zachary Tchir, Marek Z. Reformat, Petr Musilek",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00046"
    },
    {
        "id": 14301,
        "title": "Siamese-Discriminant Deep Reinforcement Learning for Solving Jigsaw Puzzles with Large Eroded Gaps",
        "authors": "Xingke Song, Jiahuan Jin, Chenglin Yao, Shihe Wang, Jianfeng Ren, Ruibin Bai",
        "published": "2023-6-26",
        "citations": 6,
        "abstract": "Jigsaw puzzle solving has recently become an emerging research area. The developed techniques have been widely used in applications beyond puzzle solving. This paper focuses on solving Jigsaw Puzzles with Large Eroded Gaps (JPwLEG). We formulate the puzzle reassembly as a combinatorial optimization problem and propose a Siamese-Discriminant Deep Reinforcement Learning (SD2RL) to solve it. A Deep Q-network (DQN) is designed to visually understand the puzzles, which consists of two sets of Siamese Discriminant Networks, one set to perceive the pairwise relations between vertical neighbors and another set for horizontal neighbors. The proposed DQN considers not only the evidence from the incumbent fragment but also the support from its four neighbors. The DQN is trained using replay experience with carefully designed rewards to guide the search for a sequence of fragment swaps to reach the correct puzzle solution. Two JPwLEG datasets are constructed to evaluate the proposed method, and the experimental results show that the proposed SD2RL significantly outperforms state-of-the-art methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i2.25325"
    },
    {
        "id": 14302,
        "title": "Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning",
        "authors": "Guy Azran, Mohamad H. Danesh, Stefano V. Albrecht, Sarah Keren",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RMs), state machine abstractions that induce subtasks based on the current task’s rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Empirical results show that our representations improve sample efficiency and few-shot transfer in a variety of domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i10.28970"
    },
    {
        "id": 14303,
        "title": "ConcaveQ: Non-monotonic Value Function Factorization via Concave Representations in Deep Multi-Agent Reinforcement Learning",
        "authors": "Huiqun Li, Hanhan Zhou, Yifei Zou, Dongxiao Yu, Tian Lan",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Value function factorization has achieved great success in multi-agent reinforcement learning by optimizing joint action-value functions through the maximization of factorized per-agent utilities. To ensure Individual-Global-Maximum property, existing works often focus on value factorization using monotonic functions, which are known to result in restricted representation expressiveness. In this paper, we analyze the limitations of monotonic factorization and present ConcaveQ, a novel non-monotonic value function factorization approach that goes beyond monotonic mixing functions and employs neural network representations of concave mixing functions. Leveraging the concave property in factorization, an iterative action selection scheme is developed to obtain optimal joint actions during training. It is used to update agents’ local policy networks, enabling fully decentralized execution. The effectiveness of the proposed ConcaveQ is validated across scenarios involving multi-agent predator-prey environment and StarCraft II micromanagement tasks. Empirical results exhibit significant improvement of ConcaveQ over state-of-the-art multi-agent reinforcement learning approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i16.29695"
    },
    {
        "id": 14304,
        "title": "MA2CL:Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning",
        "authors": "Haolin Song, Mingxiao Feng, Wengang Zhou, Houqiang Li",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Recent approaches have utilized self-supervised auxiliary tasks as representation learning to improve the performance and sample efficiency of vision-based reinforcement learning algorithms in single-agent settings. However, in multi-agent reinforcement learning (MARL), these techniques face challenges because each agent only receives partial observation from an environment influenced by others, resulting in correlated observations in the agent dimension. So it is necessary to consider agent-level information in representation learning for MARL. In this paper, we propose an effective framework called Multi-Agent Masked Attentive Contrastive Learning (MA2CL), which encourages learning representation to be both temporal and agent-level predictive by reconstructing the masked agent observation in latent space. Specifically, we use an attention reconstruction model for recovering and the model is trained via contrastive learning. MA2CL allows better utilization of contextual information at the agent level, facilitating the training of MARL agents for cooperation tasks. Extensive experiments demonstrate that our method significantly improves the performance and sample efficiency of different MARL algorithms and outperforms other methods in various vision-based and state-based scenarios.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/470"
    }
]