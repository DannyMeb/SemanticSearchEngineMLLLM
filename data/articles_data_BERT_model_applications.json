[
    {
        "id": 201,
        "title": "An Advanced BERT LayerSum Model for Sentiment Classification of COVID-19 Tweets",
        "authors": "Areeba Umair, Elio Masciari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012128900003541"
    },
    {
        "id": 202,
        "title": "Bert Model for Position Estimation in Indoor Environments",
        "authors": "Qun Kong, Fuxiang Wang, Guiwen Zhang, Xue Wang, Zhuang Wang",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405530"
    },
    {
        "id": 203,
        "title": "A novel framework for aspect based sentiment analysis using a hybrid BERT (HybBERT) model",
        "authors": "Anushree Goud, Bindu Garg",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17647-1"
    },
    {
        "id": 204,
        "title": "CoBERT: A Contextual BERT model for recommending employability profiles of information technology students in unstable developing countries",
        "authors": "Héritier Nsenge Mpia, Lucy Waruguru Mburu, Simon Nyaga Mwendia",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106728"
    },
    {
        "id": 205,
        "title": "A BERT-encoded ensembled CNN model for suicide risk identification in social media posts",
        "authors": "Joy Gorai, Dilip Kumar Shaw",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09642-w"
    },
    {
        "id": 206,
        "title": "Hybrid Approach to Explain BERT Model: Sentiment Analysis Case",
        "authors": "Aroua Hedhili, Islem Bouallagui",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012318400003636"
    },
    {
        "id": 207,
        "title": "German BERT Model for Legal Named Entity Recognition",
        "authors": "Harshil Darji, Jelena Mitrović, Michael Granitzer",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011749400003393"
    },
    {
        "id": 208,
        "title": "Detection of Hate Speech in Turkish Social Media Posts with BERT-Base Model",
        "authors": "Şengül Bayrak, Alper Karaca, Ferhat Toson, Aleyna Kocabey, Fatma Begüm ARSLANOĞLU",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10224040"
    },
    {
        "id": 209,
        "title": "The Application of the BERT Transformer Model for Phishing Email Classification",
        "authors": "Denish Omondi Otieno, Akbar Siami Namin, Keith S. Jones",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00198"
    },
    {
        "id": 210,
        "title": "Feature analysis of sentence vectors by an image-generation model using Sentence-BERT",
        "authors": "Masato Izumi, Kenya Jin'no",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/nolta.14.508"
    },
    {
        "id": 211,
        "title": "Identification of paraphrased text in research articles through improved embeddings and fine-tuned BERT model",
        "authors": "Abdur Razaq, Zahid Halim, Atta Ur Rahman, Kholla Sikandar",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18359-w"
    },
    {
        "id": 212,
        "title": "Heterogeneous data-based information retrieval using a fine-tuned pre-trained BERT language model",
        "authors": "Amjan Shaik, Surabhi Saxena, Manisha Gupta, Nikhat Parveen",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17868-4"
    },
    {
        "id": 213,
        "title": "Adversarial Sample Detection for BERT Model Based on Sample Sensitivity Characteristics",
        "authors": "Jing Liu, Yihao Wang, Yang Wu",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsa59317.2023.00014"
    },
    {
        "id": 214,
        "title": "Robust Analysis of IT Infrastructure's Log Data with BERT Language Model",
        "authors": "Deepali Arun Bhanage, Ambika Vishal Pawar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140675"
    },
    {
        "id": 215,
        "title": "Fine-Grained Sentiment Analysis with a Fine-Tuned BERT and an Improved Pre-Training BERT",
        "authors": "Jiaxi Li",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257673"
    },
    {
        "id": 216,
        "title": "Proposing sentiment analysis model based on BERT and XLNet for movie reviews",
        "authors": "Mian Muhammad Danyal, Sarwar Shah Khan, Muzammil Khan, Subhan Ullah, Faheem Mehmood, Ijaz Ali",
        "published": "2024-1-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18156-5"
    },
    {
        "id": 217,
        "title": "BERT-NAR-BERT: A Non-Autoregressive Pre-Trained Sequence-to-Sequence Model Leveraging BERT Checkpoints",
        "authors": "Mohammad Golam Sohrab, Masaki Asada, Matīss Rikters, Makoto Miwa",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3346952"
    },
    {
        "id": 218,
        "title": "Arabic Stock-News Sentiments and Economic Aspects using BERT Model",
        "authors": "Eman Alasmari, Mohamed Hamdy, Khaled H. Alyoubi, Fahd Saleh Alotaibi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140174"
    },
    {
        "id": 219,
        "title": "Abstractive Text Summarization Using BERT for Feature Extraction and Seq2Seq Model for Summary Generation",
        "authors": "Kania Galih Widowati, Nicholas Budiman, Kezia Foejiono, Kartika Purwandari",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmeralda60125.2023.10458190"
    },
    {
        "id": 220,
        "title": "Investigation of the structure of the latent variable space in Sentence-BERT sentence vectors using an image generation model",
        "authors": "Masato Izumi, Kenya Jin'no",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/nolta.15.376"
    },
    {
        "id": 221,
        "title": "Sentiment recognition and analysis method of official document text based on BERT–SVM model",
        "authors": "Shule Hao, Peng Zhang, Sen Liu, Yuhang Wang",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08226-4"
    },
    {
        "id": 222,
        "title": "Trajectory-BERT: Trajectory Estimation Based on BERT Trajectory Pre-Training Model and Particle Filter Algorithm",
        "authors": "You Wu, Hongyi Yu, Jianping Du, Chenglong Ge",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "In the realm of aviation, trajectory data play a crucial role in determining the target’s flight intentions and guaranteeing flight safety. However, the data collection process can be hindered by noise or signal interruptions, thus diminishing the precision of the data. This paper uses the bidirectional encoder representations from transformers (BERT) model to solve the problem by masking the high-precision automatic dependent survey broadcast (ADS-B) trajectory data and estimating the mask position value based on the front and rear trajectory points during BERT model training. Through this process, the model acquires knowledge of intricate motion patterns within the trajectory data and acquires the BERT pre-training Model. Afterwards, a refined particle filter algorithm is utilized to generate alternative trajectory sets for observation trajectory data that is prone to noise. Ultimately, the BERT trajectory pre-training model is supplied with the alternative trajectory set, and the optimal trajectory is determined by computing the maximum posterior probability. The results of the experiment show that the model has good performance and is stronger than traditional algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23229120"
    },
    {
        "id": 223,
        "title": "MNoR-BERT: multi-label classification of non-functional requirements using BERT",
        "authors": "Kamaljit Kaur, Parminder Kaur",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08833-1"
    },
    {
        "id": 224,
        "title": "CE-BERT: Concise and Efficient BERT-Based Model for Detecting Rumors on Twitter",
        "authors": "Rini Anggrainingsih, Ghulam Mubashar Hassan, Amitava Datta",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3299858"
    },
    {
        "id": 225,
        "title": "Naming entity recognition of citrus pests and diseases based on the BERT-BiLSTM-CRF model",
        "authors": "Yafei Liu, Siqi Wei, Haijun Huang, Qin Lai, Mengshan Li, Lixin Guan",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121103"
    },
    {
        "id": 226,
        "title": "BERT Model-based Natural Language to NoSQL Query Conversion using Deep Learning Approach",
        "authors": "Kazi Mojammel Hossen, Mohammed Nasir Uddin, Minhazul Arefin, Md Ashraf Uddin",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140293"
    },
    {
        "id": 227,
        "title": "CLES-BERT: Contrastive Learning-based BERT Model for Automated Essay Scoring",
        "authors": "Daegon Yu, Yongyeon Kim, Sangwoo Han, Byung-Won On",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14801/jkiit.2023.21.4.31"
    },
    {
        "id": 228,
        "title": "A false emotion opinion target extraction model with two stage BERT and background information fusion",
        "authors": "ZhiYang Hou, YaJun Du, QiZhi Li, XianYong Li, XiaoLiang Chen, HongMei Gao",
        "published": "2024-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123735"
    },
    {
        "id": 229,
        "title": "GeoTPE: A neural network model for geographical topic phrases extraction from literature based on BERT enhanced with relative position embedding",
        "authors": "Weirong Li, Kai Sun, Yunqiang Zhu, Fangyu Ding, Lei Hu, Xiaoliang Dai, Jia Song, Jie Yang, Lang Qian, Shu Wang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121077"
    },
    {
        "id": 230,
        "title": "A semantic-BERT model based on two Bi-LSTM-CNN for arabic question answering",
        "authors": "Abdullah Farhan Mahdi, Rabah N. Farhan, Salah Sleibi Al-Rawi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0203625"
    },
    {
        "id": 231,
        "title": "A lightweight IoT intrusion detection model based on improved BERT-of-Theseus",
        "authors": "Zhendong Wang, Jingfei Li, Shuxin Yang, Xiao Luo, Dahai Li, Soroosh Mahmoodi",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122045"
    },
    {
        "id": 232,
        "title": "Umami-BERT: An interpretable BERT-based model for umami peptides prediction",
        "authors": "Jingcheng Zhang, Wenjing Yan, Qingchuan Zhang, Zihan Li, Li Liang, Min Zuo, Yuyu Zhang",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.foodres.2023.113142"
    },
    {
        "id": 233,
        "title": "MedCT-BERT: Multimodal Mortality Prediction using Medical ConvTransformer-BERT Model",
        "authors": "Ke Zhang, Ke Niu, Yuhang Zhou, Wenjuan Tai, Guoqiang Lu",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00109"
    },
    {
        "id": 234,
        "title": "Feature Mining of News Communication Topic Elements Based on BERT Model",
        "authors": "Fei Yang Zheng",
        "published": "2023-5-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.54647/sociology841046"
    },
    {
        "id": 235,
        "title": "A Survey on BERT and Its Applications",
        "authors": "Sulaiman Aftan, Habib Shah",
        "published": "2023-1-26",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lt58159.2023.10092289"
    },
    {
        "id": 236,
        "title": "ColBERT: Using BERT sentence embedding in parallel neural networks for computational humor",
        "authors": "Issa Annamoradnejad, Gohar Zoghi",
        "published": "2024-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123685"
    },
    {
        "id": 237,
        "title": "BERT-Based Knowledge Distillation for Sentiment Analysis Model",
        "authors": "杨杰 孙",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/csa.2023.1310192"
    },
    {
        "id": 238,
        "title": "News Short Text Classification Based on Bert Model and Fusion Model",
        "authors": "Hongyang Cui, Chentao Wang, Yibo Yu",
        "published": "2023-2-28",
        "citations": 1,
        "abstract": "Text classification task is one of the most fundamental tasks in NLP, and the classification of short news text could be the basis for many other tasks. In this paper, we applied a fusion model combining Bert and TextRNN with some modified details to expect higher accuracy of text classification. We used the THUCNews as dataset which consists of two columns one for news text and the other for numbers. The original dataset was seperated into three parts: training set, validation set and test set. Besides, we used BERT model which contains two pre-training tasks and TextRNN model which refers to the use of RNN to solve text classification problems. We trained these two models in parallel, and then the optimal Bert and TextRNN models obtained through training and parameter tuning are added with a fully-connected layer to receive the final results by weighting the efficiency of Bert and TextRNN. The fusion model solves the problem of over-fitting and under-fitting of a single model, and helps to obtain a model with better generalization performance. The experimental results show the sharp change in loss and accuracy as well as the final accuracy of the BERT model. The precision, recall-rate and F1-score are also evaluated in this paper. The accuracy of fusion model of BERT and TextRNN is much better than single Bert model and has a gap to 1.76%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5482"
    },
    {
        "id": 239,
        "title": "medBERT.de: A comprehensive German BERT model for the medical domain",
        "authors": "Keno K. Bressem, Jens-Michalis Papaioannou, Paul Grundmann, Florian Borchert, Lisa C. Adams, Leonhard Liu, Felix Busch, Lina Xu, Jan P. Loyen, Stefan M. Niehues, Moritz Augustin, Lennart Grosser, Marcus R. Makowski, Hugo J.W.L. Aerts, Alexander Löser",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121598"
    },
    {
        "id": 240,
        "title": "ChatGPT and finetuned BERT: A comparative study for developing intelligent design support systems",
        "authors": "Yunjian Qiu, Yan Jin",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.iswa.2023.200308"
    },
    {
        "id": 241,
        "title": "Strategy to Develop a Domain-specific Pre-trained Language Model: Case of V-BERT, a Language Model for the Automotive Industry",
        "authors": "Younha Kim, Yunsu Jeon, Junwoo Kim, Namgyu Kim",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5626/ktcp.2023.29.5.228"
    },
    {
        "id": 242,
        "title": "Blind PV temperature model calibration",
        "authors": "Anastasios Kladas, Bert Herteleer, Jan Cappelle",
        "published": "2023",
        "citations": 0,
        "abstract": "The determination of module temperature in a photovoltaic (PV) system is a crucial factor in PV modelling and the assessment of system health status. However, the scarcity of on-site temperature measurements poses a challenge, and existing PV temperature models encounter difficulties in accurately estimating temperatures in systems characterized by unique structural or locational attributes. This paper introduces a novel approach that enables the calibration of PV temperature models without relying on direct temperature measurements. Referred to as blind calibration, this method eliminates the requirement for temperature measurements, thus offering a promising solution to the aforementioned challenges. The method is validated using three datasets, demonstrating accurate PV temperature (TPV) estimation with mean absolute errors below 2 °C. The findings highlight the suitability of the proposed approach for various PV system types, while acknowledging limitations regarding certain system configurations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/epjpv/2023021"
    },
    {
        "id": 243,
        "title": "A New Model for Fuel Transfer Leak Frequencies",
        "authors": "John Spouge, Bert Wolting",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3850/978-981-18-8071-1_p175-cd"
    },
    {
        "id": 244,
        "title": "Improving BERT model for requirements classification by bidirectional LSTM-CNN deep model",
        "authors": "Kamaljit Kaur, Parminder Kaur",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2023.108699"
    },
    {
        "id": 245,
        "title": "Sentiment Analysis of Text Based on BERT-BiLSTM-BiGRU-CNN Model",
        "authors": "昆 朱",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/mos.2024.131005"
    },
    {
        "id": 246,
        "title": "BERT-based B2B matching model for K-Startups and Overseas Buyers",
        "authors": "Jungsuk Choi, Nammee Moon",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5762/kais.2023.24.9.769"
    },
    {
        "id": 247,
        "title": "Covid based question criticality prediction with domain adaptive BERT embeddings",
        "authors": "Shiney Jeyaraj, Raghuveera T.",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.107913"
    },
    {
        "id": 248,
        "title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model",
        "authors": "Yongchang Cao, Liang He, Zhen Wu, Xinyu Dai",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191265"
    },
    {
        "id": 249,
        "title": "Greek Political Speech Classification Using BERT",
        "authors": "Kontilenia Maria Kotsifakou, Dionisios N. Sotiropoulos",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345868"
    },
    {
        "id": 250,
        "title": "BERT-siRNA: siRNA target prediction based on BERT pre-trained interpretable model",
        "authors": "Jiayu Xu, Nan Xu, Weixin Xie, Chengkui Zhao, Lei Yu, Weixing Feng",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.gene.2024.148330"
    },
    {
        "id": 251,
        "title": "Text Sentiment Analysis Model Based on BERT and Knowledge Graph",
        "authors": "斐瑜 刘",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/mos.2023.124382"
    },
    {
        "id": 252,
        "title": "Research of Chinese relation extraction based on BERT",
        "authors": "Zirui Song, Li Wan",
        "published": "2023-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca56706.2023.10075925"
    },
    {
        "id": 253,
        "title": "Tweet recommendation using Clustered Bert and Word2vec Models",
        "authors": "Surbhi Kakar, Deepali Dhaka, Monica Mehrotra",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smartnets58706.2023.10215867"
    },
    {
        "id": 254,
        "title": "Fusion of BERT embeddings and elongation-driven features",
        "authors": "Abderrahim Rafae, Mohammed Erritali, Mathieu Roche",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18786-9"
    },
    {
        "id": 255,
        "title": "An open intent detection model optimized for datasets based on the Bert large model",
        "authors": "Yichen Dong, Zhen Wang, Tianjun Wu",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Within current task-oriented dialogue systems, the focus of intent detection predominantly centers on closed domains. Nevertheless, in real-world usage scenarios, a substantial proportion of interactions fall into the open-domain category. User intentions frequently transcend predefined boundaries, giving rise to a multitude of out-of-domain intents, which pose a formidable challenge to existing models, ultimately leading to diminished recognition rates and accuracy. The demand for open intent detection models is increasing in today's society to address this issue effectively. This paper proposes a method to optimize datasets, thereby enhancing the training accuracy of open intent detection models. Specifically, this paper employs the Adaptive Decision Boundary Learning algorithm, which is currently popular in open intent detection. Leveraging this algorithm, this paper suggests using the K-means clustering algorithm to refine the intent labels within the dataset. This process helps identify and remove outliers in the dataset, making the distinction between known domain and open-domain intent labels more precise. Experimental results on two datasets, banking77 and stackoverflow, demonstrate the effectiveness of our approach in significantly improving model accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241381"
    },
    {
        "id": 256,
        "title": "Approximate Hybrid Binary-Unary Computing with Applications in BERT Language Model and Image Processing",
        "authors": "Alireza Khataei, Gaurav Singh, Kia Bazargan",
        "published": "2023-2-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543622.3573181"
    },
    {
        "id": 257,
        "title": "BERT-based coupling evaluation of biological strategies in bio-inspired design",
        "authors": "Feng Sun, He Xu, Yihan Meng, Zhimao Lu, Chengju Gong",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119725"
    },
    {
        "id": 258,
        "title": "POS-BERT: Point cloud one-stage BERT pre-training",
        "authors": "Kexue Fu, Peng Gao, Shaolei Liu, Linhao Qu, Longxiang Gao, Manning Wang",
        "published": "2024-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122563"
    },
    {
        "id": 259,
        "title": "Finding BERT Errors Using Activation Vectors",
        "authors": "Vedashree P. Bhandare, William B. Andreopoulos",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdataservice58306.2023.00010"
    },
    {
        "id": 260,
        "title": "Smart Contract Cloning Identification Using BERT Model Based Representation Learning",
        "authors": "Vishnu Kumar, Pisipati Pranav, M. Srinivas",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440822"
    },
    {
        "id": 261,
        "title": "An Aspect-Level Sentiment Analysis Approach Based on BERT and Attention Mechanism",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7753/ijcatr1206.1002"
    },
    {
        "id": 262,
        "title": "Automated and Interference-Free Inventory Solution Using Energy-Neutral BLE Tags",
        "authors": "Jona Cappelle, Bert Cox, Liesbet Van der Perre",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sas58821.2023.10254152"
    },
    {
        "id": 263,
        "title": "Improved Text Matching Model Based on BERT",
        "authors": "Qingyu Li, Yujun Zhang",
        "published": "2023-2-13",
        "citations": 0,
        "abstract": "Text matching is a basic and important task in natural language understanding, this paper proposes a new model BBMC for the problem of insufficient feature extraction ability of existing text matching models, which integrates BiLSTM and multi-scale CNN on the basis of BERT. First, the word embedding representation of the text is obtained by the BERT, and then the semantic features of the text are further extracted by the double-layer BiLSTM, followed by the multi-scale CNN model, the key local features are extracted, and finally the linear and SoftMax function are used to classify. Experimental results on the LCQMC dataset show that the BBMC has been improved to a certain extent compared with other methods, and the accuracy on the test set can be best achieved 88.01%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v2i3.5209"
    },
    {
        "id": 264,
        "title": "A Text Classification Model Based on BERT and Attention",
        "authors": "Binglin Zhu, Wei Pan",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cait59945.2023.10469363"
    },
    {
        "id": 265,
        "title": "Prediction of Author’s Profile basing on Fine-Tuning BERT model",
        "authors": "Bassem Bsir, Nabil Khoufi, Mounir Zrigui",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v48i1.4839"
    },
    {
        "id": 266,
        "title": "BERT-Based Sentiment Forensics Analysis for Intrusion Detection",
        "authors": "Shahrzad Sayyafzadeh, Hongmei Chi, Weifeng Xu, Kaushik Roy",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00235"
    },
    {
        "id": 267,
        "title": "Enhancing BERT-Based Chinese Address Recognition Model with Tag Revision Module",
        "authors": "Ruonan Zhao, Xiangwu Ding",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icise60366.2023.00060"
    },
    {
        "id": 268,
        "title": "Misinformation classification using LSTM and BERT model",
        "authors": "Aditya Harbola, Mahesh Manchanda, Deepti Negi",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icidca56705.2023.10100054"
    },
    {
        "id": 269,
        "title": "BAG: Text Classification Based on Attention Mechanism Combining BERT and GCN",
        "authors": "想 李",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/sea.2023.122023"
    },
    {
        "id": 270,
        "title": "BERT-Based Hybrid Deep Learning with Text Augmentation for Sentiment Analysis of Indonesian Hotel Reviews",
        "authors": "Maxwell Thomson, Hendri Murfi, Gianinna Ardaneswari",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012127400003541"
    },
    {
        "id": 271,
        "title": "BERT-Based Arabic Diacritization: A state-of-the-art approach for improving text accuracy and pronunciation",
        "authors": "Ruba Kharsa, Ashraf Elnagar, Sane Yagi",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123416"
    },
    {
        "id": 272,
        "title": "BERT for Sentiment Analysis on Rotten Tomatoes Reviews",
        "authors": "Aji Gautama Putrada, Nur Alamsyah, Mohamad Nurkamal Fauzan",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icodsa58501.2023.10276800"
    },
    {
        "id": 273,
        "title": "Retracted: BERT-Based Clinical Name Entity Reorganization Model for Health Diagnosis",
        "authors": "",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9836340"
    },
    {
        "id": 274,
        "title": "Construction and Experimental Evaluation of Document Causality Extraction Model Based on CGCN-BERT",
        "authors": "Guijiao He",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaisc58445.2023.10200466"
    },
    {
        "id": 275,
        "title": "Named Entity Recognition Method Based on BERT-whitening and Dynamic Fusion Model",
        "authors": "Meng Liang, Yao Shi",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnlp58431.2023.00041"
    },
    {
        "id": 276,
        "title": "Chinese Food Safety Entity Recognition Based on BERT Model",
        "authors": "Shiyong Xiong, Guozhi Bai",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456298"
    },
    {
        "id": 277,
        "title": "Creativity index calculation with question answering system using BERT model",
        "authors": "Abhinav Nandgaonkar, Sunil B Mane, Vaibhav Khatavkar",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205798"
    },
    {
        "id": 278,
        "title": "Contextual Embeddings-Based Web Page Categorization Using the Fine-Tune BERT Model",
        "authors": "Amit Kumar Nandanwar, Jaytrilok Choudhary",
        "published": "2023-2-2",
        "citations": 4,
        "abstract": "The World Wide Web has revolutionized the way we live, causing the number of web pages to increase exponentially. The web provides access to a tremendous amount of information, so it is difficult for internet users to locate accurate and useful information on the web. In order to categorize pages accurately based on the queries of users, methods of categorizing web pages need to be developed. The text content of web pages plays a significant role in the categorization of web pages. If a word’s position is altered within a sentence, causing a change in the interpretation of that sentence, this phenomenon is called polysemy. In web page categorization, the polysemy property causes ambiguity and is referred to as the polysemy problem. This paper proposes a fine-tuned model to solve the polysemy problem, using contextual embeddings created by the symmetry multi-head encoder layer of the Bidirectional Encoder Representations from Transformers (BERT). The effectiveness of the proposed model was evaluated by using the benchmark datasets for web page categorization, i.e., WebKB and DMOZ. Furthermore, the experiment series also fine-tuned the proposed model’s hyperparameters to achieve 96.00% and 84.00% F1-Scores, respectively, demonstrating the proposed model’s importance compared to baseline approaches based on machine learning and deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15020395"
    },
    {
        "id": 279,
        "title": "MIL-BERT: Military Domain Specialized Korean Pre-trained Language Model",
        "authors": "Hee-Soon Heo, Chang-Min Yoon, Young-Ha Ryu, Seok-hyun Yong, Dooyoung Kim",
        "published": "2023-6-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31818/jknst.2023.06.6.2.201"
    },
    {
        "id": 280,
        "title": "Sentiment Analysis with the Use of Transformers and BERT",
        "authors": "Elisavet Douka, Isidoros Perikos, Ioannis Hatzilygeroudis",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345952"
    },
    {
        "id": 281,
        "title": "Editorial: Physical neuromorphic computing and its industrial applications",
        "authors": "Toshiyuki Yamane, Akira Hirose, Bert Jan Offrein",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fninf.2023.1238168"
    },
    {
        "id": 282,
        "title": "A Comparison of LSTM and BERT model for sarcasm prediction",
        "authors": "Yizhong Ding",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Sarcasm prediction is a text analysis task that aims to identify sarcastic and non-sarcastic statements in text. Sarcasm is a figure of speech that uses opposite or contradictory language to express a certain meaning or idea. Sarcasm is usually cryptic, vague, and suggestive, which makes sarcasm prediction a challenging task. In sarcasm prediction projects, techniques of natural language processing are usually leveraged to analyze and classify the text. The main challenge of this task lies in the fact that sarcasm usually has multiple manifestations and needs to consider the contextual and semantic information of the text. The prediction of sarcasm holds significant application value in natural language processing, such as social media analysis, public opinion monitoring, sentiment analysis and so on. In this paper, by controlling variables, the influence of adding the long short-term memory (LSTM) layer and changing the grid structure of the model on the accuracy of prediction results is explored. Moreover, accuracy of the LSTM prediction performance is compared with that of the bidirectional encoder representations from Transformers (BERT) model. At the same time, this paper analyzed and discussed the phenomenon that adding the number of LSTM model layers could not obtain higher prediction accuracy, and the accuracy gap of prediction results between LSTM model and BERT model, and finally obtained relevant conclusions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/21/20231118"
    },
    {
        "id": 283,
        "title": "Transfer Learning for Sentiment Classification Using Bidirectional Encoder Representations from Transformers (BERT) Model",
        "authors": "Ali Areshey, Hassan Mathkour",
        "published": "2023-5-31",
        "citations": 2,
        "abstract": "Sentiment is currently one of the most emerging areas of research due to the large amount of web content coming from social networking websites. Sentiment analysis is a crucial process for recommending systems for most people. Generally, the purpose of sentiment analysis is to determine an author’s attitude toward a subject or the overall tone of a document. There is a huge collection of studies that make an effort to predict how useful online reviews will be and have produced conflicting results on the efficacy of different methodologies. Furthermore, many of the current solutions employ manual feature generation and conventional shallow learning methods, which restrict generalization. As a result, the goal of this research is to develop a general approach using transfer learning by applying the “BERT (Bidirectional Encoder Representations from Transformers)”-based model. The efficiency of BERT classification is then evaluated by comparing it with similar machine learning techniques. In the experimental evaluation, the proposed model demonstrated superior performance in terms of outstanding prediction and high accuracy compared to earlier research. Comparative tests conducted on positive and negative Yelp reviews reveal that fine-tuned BERT classification performs better than other approaches. In addition, it is observed that BERT classifiers using batch size and sequence length significantly affect classification performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23115232"
    },
    {
        "id": 284,
        "title": "An Algorithm Model of Power Grid Maintenance Ticket Based on Bert",
        "authors": "Guoqiang Zhang, Qun Wang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictech58362.2023.00104"
    },
    {
        "id": 285,
        "title": "Enhancing Natural Language Processing with LDA-BERT Integration: A Comprehensive Review of Methodologies, Applications, and Future Directions",
        "authors": "Ming Zheng Ong, Kathleen Swee Neo Tan, Chi Wee Tan",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdxa61007.2024.10470636"
    },
    {
        "id": 286,
        "title": "Sentiment analysis of imbalanced datasets using BERT and ensemble stacking for deep learning",
        "authors": "Nassera Habbat, Hicham Nouri, Houda Anoun, Larbi Hassouni",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106999"
    },
    {
        "id": 287,
        "title": "Fake news detection using dual BERT deep neural networks",
        "authors": "Mahmood Farokhian, Vahid Rafe, Hadi Veisi",
        "published": "2023-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17115-w"
    },
    {
        "id": 288,
        "title": "Offensive Hebrew Corpus and Detection using BERT",
        "authors": "Nagham Hamad, Mustafa Jarrar, Mohammad Khalilia, Nadim Nashif",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiccsa59173.2023.10479258"
    },
    {
        "id": 289,
        "title": "Hybrid model for extractive single document summarization: utilizing BERTopic and BERT model",
        "authors": "Maryanto Maryanto, Philips Philips, Abba Suganda Girsang",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "Extractive text summarization has been a popular research area for many years. The goal of this task is to generate a compact and coherent summary of a given document, preserving the most important information. However, current extractive summarization methods still face several challenges such as semantic drift, repetition, redundancy, and lack of coherence. A novel approach is presented in this paper to improve the performance of an extractive summarization model based on bidirectional encoder representations from transformers (BERT) by incorporating topic modeling using the BERTopic model. Our method first utilizes BERTopic to identify the dominant topics in a document and then employs a BERT-based deep neural network to extract the most salient sentences related to those topics. Our experiments on the cable news network (CNN)/daily mail dataset demonstrate that our proposed method outperforms state-of-the-art BERT-based extractive summarization models in terms of recall-oriented understudy for gisting evaluation (ROUGE) scores, which resulted in an increase of 32.53% of ROUGE-1, 47.55% of ROUGE-2, and 16.63% of ROUGE-L when compared to baseline BERT-based extractive summarization models. This paper contributes to the field of extractive text summarization, highlights the potential of topic modeling in improving summarization results, and provides a new direction for future research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp1723-1731"
    },
    {
        "id": 290,
        "title": "Software Subclassification Based on BERTopic-BERT-BiLSTM Model",
        "authors": "Wenjuan Bu, Hui Shu, Fei Kang, Qian Hu, Yuntian Zhao",
        "published": "2023-9-8",
        "citations": 1,
        "abstract": "With the continuous influx of application software onto the application software market, achieving accurate software recommendations for users in the huge software application market is urgent. To address this issue, each application software market currently provides its own classification tags. However, several problems still exist, such as the lack of objectivity, hierarchy, and standardization in these classifications, which in turn affects the accuracy of precise software recommendations. Accordingly, a customized BERTopic model is proposed to cluster the software description texts of the application software and the automatic tagging and updating of the application software tags are realized according to the clusters obtained by topic clustering and the extracted subject words. At the same time, a data enhancement method based on the c-TF-IDF algorithm is proposed to solve the problem of imbalance of datasets, and then the classification model based on the BERT-BiLSTM model is trained on the labeled datasets to classify the software in the dimension of the application function, so as to realize the accurate software recommendation for users. Based on the experimental verification of two datasets, 21 categories in the SourceForge dataset and 19 categories in the Chinese App Store dataset are subclassed by the clustering results of the customized BERTopic model, and the tags of 138 subclasses and 262 subclasses are formed, respectively. In addition, a complete tagged software description text dataset is constructed and the software tags are updated automatically. In the first stage of the classification experiment, the weighted average accuracy, recall rate, and F1 value can reach 0.92, 0.91, and 0.92, respectively. In the second stage, the weighted average accuracy, recall rate, and F1 value can all reach 0.96. After data enhancement, the weighted average F1 value of the classification model can be increased by up to two percentage points.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12183798"
    },
    {
        "id": 291,
        "title": "BERT-Based Model for Reading Comprehension Question Answering",
        "authors": "Abdelrahman A. Mosaed, Hanan Hindy, Mostafa Aref",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicis58388.2023.10391167"
    },
    {
        "id": 292,
        "title": "Predicting Disaster Tweets using Enhanced BERT Model",
        "authors": "Premkumar Duraisamy, M Duraisamy, M Periyanayaki, Yuvaraj Natarajan",
        "published": "2023-5-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciccs56967.2023.10142660"
    },
    {
        "id": 293,
        "title": "COVBERT: Enhancing Sentiment Analysis Accuracy in COVID-19 X Data through Customized BERT",
        "authors": "Vanshaj Gupta, Jaydeep Patel, Safa Shubbar, Kambiz Ghazinour",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "In a time when social media information is a valuable resource for gaining insights, the COVID-19 pandemic has released a flood of public sentiment, abundant with unstructured text data. This paper introduces CovBERT, a novel adaptation of the BERT model, specifically honed for the nuanced analysis of COVID-19-related discourse on X (formerly Twitter). CovBERT stands out by incorporating a bespoke vocabulary, meticulously curated from pandemic-centric tweets, resulting in a remarkable leap in sentiment analysis accuracy—from the baseline 72\\% to an impressive 78.64\\%. This paper not only presents a detailed comparison of CovBERT with the standard BERT model but also juxtaposes it against traditional machine learning approaches, showcasing its superior proficiency in decoding complex emotional undercurrents in social media data. Furthermore, the integration of geolocation analysis pipeline adds another layer of depth, offering a panoramic view of global sentiment trends.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.140212"
    },
    {
        "id": 294,
        "title": "LAD: Layer-Wise Adaptive Distillation for BERT Model Compression",
        "authors": "Ying-Jia Lin, Kuan-Yu Chen, Hung-Yu Kao",
        "published": "2023-1-28",
        "citations": 4,
        "abstract": "Recent advances with large-scale pre-trained language models (e.g., BERT) have brought significant potential to natural language processing. However, the large model size hinders their use in IoT and edge devices. Several studies have utilized task-specific knowledge distillation to compress the pre-trained language models. However, to reduce the number of layers in a large model, a sound strategy for distilling knowledge to a student model with fewer layers than the teacher model is lacking. In this work, we present Layer-wise Adaptive Distillation (LAD), a task-specific distillation framework that can be used to reduce the model size of BERT. We design an iterative aggregation mechanism with multiple gate blocks in LAD to adaptively distill layer-wise internal knowledge from the teacher model to the student model. The proposed method enables an effective knowledge transfer process for a student model, without skipping any teacher layers. The experimental results show that both the six-layer and four-layer LAD student models outperform previous task-specific distillation approaches during GLUE tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23031483"
    },
    {
        "id": 295,
        "title": "Deep Active Learning for Address Parsing Tasks with BERT",
        "authors": "Berkay Güler, Betül Aygün, Aydın Gerek, Alaeddin Selçuk Gürel",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223996"
    },
    {
        "id": 296,
        "title": "Malicious encrypted traffic detection based on Bert and one-dimensional CNN model",
        "authors": "Pengkai Kang",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icetci57876.2023.10176917"
    },
    {
        "id": 297,
        "title": "Social Media Analytics for Disaster Management using BERT Model",
        "authors": "Sherin R Varghese, Sujitha Juliet, J. Anitha",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icercs57948.2023.10434012"
    },
    {
        "id": 298,
        "title": "A Contextual Query Expansion Model using BERT Based Deep Neural Embeddings",
        "authors": "Deepak Vishwakarma, Suresh Kumar",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10111984"
    },
    {
        "id": 299,
        "title": "Quantification of Carbon Emission Technologies Based on Knowledge Graph Bert-BiLSTM-Attention-CRF Model",
        "authors": "Zicheng Zhao",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icedcs60513.2023.00014"
    },
    {
        "id": 300,
        "title": "BERT-CNN-BiLSTM: A Hybrid Deep Learning Model for Accurate Sentiment Analysis",
        "authors": "Yusheng He",
        "published": "2023-7-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpics58376.2023.10235335"
    }
]