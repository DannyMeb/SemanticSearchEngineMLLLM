[
    {
        "id": 1705,
        "title": "An Advanced BERT LayerSum Model for Sentiment Classification of COVID-19 Tweets",
        "authors": "Areeba Umair, Elio Masciari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012128900003541"
    },
    {
        "id": 1706,
        "title": "Bert Model for Position Estimation in Indoor Environments",
        "authors": "Qun Kong, Fuxiang Wang, Guiwen Zhang, Xue Wang, Zhuang Wang",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaica58456.2023.10405530"
    },
    {
        "id": 1707,
        "title": "A novel framework for aspect based sentiment analysis using a hybrid BERT (HybBERT) model",
        "authors": "Anushree Goud, Bindu Garg",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17647-1"
    },
    {
        "id": 1708,
        "title": "CoBERT: A Contextual BERT model for recommending employability profiles of information technology students in unstable developing countries",
        "authors": "Héritier Nsenge Mpia, Lucy Waruguru Mburu, Simon Nyaga Mwendia",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106728"
    },
    {
        "id": 1709,
        "title": "A BERT-encoded ensembled CNN model for suicide risk identification in social media posts",
        "authors": "Joy Gorai, Dilip Kumar Shaw",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09642-w"
    },
    {
        "id": 1710,
        "title": "Hybrid Approach to Explain BERT Model: Sentiment Analysis Case",
        "authors": "Aroua Hedhili, Islem Bouallagui",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012318400003636"
    },
    {
        "id": 1711,
        "title": "German BERT Model for Legal Named Entity Recognition",
        "authors": "Harshil Darji, Jelena Mitrović, Michael Granitzer",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011749400003393"
    },
    {
        "id": 1712,
        "title": "The Application of the BERT Transformer Model for Phishing Email Classification",
        "authors": "Denish Omondi Otieno, Akbar Siami Namin, Keith S. Jones",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00198"
    },
    {
        "id": 1713,
        "title": "Detection of Hate Speech in Turkish Social Media Posts with BERT-Base Model",
        "authors": "Şengül Bayrak, Alper Karaca, Ferhat Toson, Aleyna Kocabey, Fatma Begüm ARSLANOĞLU",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10224040"
    },
    {
        "id": 1714,
        "title": "Feature analysis of sentence vectors by an image-generation model using Sentence-BERT",
        "authors": "Masato Izumi, Kenya Jin'no",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/nolta.14.508"
    },
    {
        "id": 1715,
        "title": "Identification of paraphrased text in research articles through improved embeddings and fine-tuned BERT model",
        "authors": "Abdur Razaq, Zahid Halim, Atta Ur Rahman, Kholla Sikandar",
        "published": "2024-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18359-w"
    },
    {
        "id": 1716,
        "title": "Heterogeneous data-based information retrieval using a fine-tuned pre-trained BERT language model",
        "authors": "Amjan Shaik, Surabhi Saxena, Manisha Gupta, Nikhat Parveen",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17868-4"
    },
    {
        "id": 1717,
        "title": "Adversarial Sample Detection for BERT Model Based on Sample Sensitivity Characteristics",
        "authors": "Jing Liu, Yihao Wang, Yang Wu",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsa59317.2023.00014"
    },
    {
        "id": 1718,
        "title": "Robust Analysis of IT Infrastructure's Log Data with BERT Language Model",
        "authors": "Deepali Arun Bhanage, Ambika Vishal Pawar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140675"
    },
    {
        "id": 1719,
        "title": "Fine-Grained Sentiment Analysis with a Fine-Tuned BERT and an Improved Pre-Training BERT",
        "authors": "Jiaxi Li",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257673"
    },
    {
        "id": 1720,
        "title": "Proposing sentiment analysis model based on BERT and XLNet for movie reviews",
        "authors": "Mian Muhammad Danyal, Sarwar Shah Khan, Muzammil Khan, Subhan Ullah, Faheem Mehmood, Ijaz Ali",
        "published": "2024-1-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18156-5"
    },
    {
        "id": 1721,
        "title": "BERT-NAR-BERT: A Non-Autoregressive Pre-Trained Sequence-to-Sequence Model Leveraging BERT Checkpoints",
        "authors": "Mohammad Golam Sohrab, Masaki Asada, Matīss Rikters, Makoto Miwa",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3346952"
    },
    {
        "id": 1722,
        "title": "Arabic Stock-News Sentiments and Economic Aspects using BERT Model",
        "authors": "Eman Alasmari, Mohamed Hamdy, Khaled H. Alyoubi, Fahd Saleh Alotaibi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140174"
    },
    {
        "id": 1723,
        "title": "Sentiment recognition and analysis method of official document text based on BERT–SVM model",
        "authors": "Shule Hao, Peng Zhang, Sen Liu, Yuhang Wang",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08226-4"
    },
    {
        "id": 1724,
        "title": "Investigation of the structure of the latent variable space in Sentence-BERT sentence vectors using an image generation model",
        "authors": "Masato Izumi, Kenya Jin'no",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/nolta.15.376"
    },
    {
        "id": 1725,
        "title": "Abstractive Text Summarization Using BERT for Feature Extraction and Seq2Seq Model for Summary Generation",
        "authors": "Kania Galih Widowati, Nicholas Budiman, Kezia Foejiono, Kartika Purwandari",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmeralda60125.2023.10458190"
    },
    {
        "id": 1726,
        "title": "Trajectory-BERT: Trajectory Estimation Based on BERT Trajectory Pre-Training Model and Particle Filter Algorithm",
        "authors": "You Wu, Hongyi Yu, Jianping Du, Chenglong Ge",
        "published": "2023-11-11",
        "citations": 0,
        "abstract": "In the realm of aviation, trajectory data play a crucial role in determining the target’s flight intentions and guaranteeing flight safety. However, the data collection process can be hindered by noise or signal interruptions, thus diminishing the precision of the data. This paper uses the bidirectional encoder representations from transformers (BERT) model to solve the problem by masking the high-precision automatic dependent survey broadcast (ADS-B) trajectory data and estimating the mask position value based on the front and rear trajectory points during BERT model training. Through this process, the model acquires knowledge of intricate motion patterns within the trajectory data and acquires the BERT pre-training Model. Afterwards, a refined particle filter algorithm is utilized to generate alternative trajectory sets for observation trajectory data that is prone to noise. Ultimately, the BERT trajectory pre-training model is supplied with the alternative trajectory set, and the optimal trajectory is determined by computing the maximum posterior probability. The results of the experiment show that the model has good performance and is stronger than traditional algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23229120"
    },
    {
        "id": 1727,
        "title": "MNoR-BERT: multi-label classification of non-functional requirements using BERT",
        "authors": "Kamaljit Kaur, Parminder Kaur",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08833-1"
    },
    {
        "id": 1728,
        "title": "CE-BERT: Concise and Efficient BERT-Based Model for Detecting Rumors on Twitter",
        "authors": "Rini Anggrainingsih, Ghulam Mubashar Hassan, Amitava Datta",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3299858"
    },
    {
        "id": 1729,
        "title": "Naming entity recognition of citrus pests and diseases based on the BERT-BiLSTM-CRF model",
        "authors": "Yafei Liu, Siqi Wei, Haijun Huang, Qin Lai, Mengshan Li, Lixin Guan",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121103"
    },
    {
        "id": 1730,
        "title": "BERT Model-based Natural Language to NoSQL Query Conversion using Deep Learning Approach",
        "authors": "Kazi Mojammel Hossen, Mohammed Nasir Uddin, Minhazul Arefin, Md Ashraf Uddin",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140293"
    },
    {
        "id": 1731,
        "title": "CLES-BERT: Contrastive Learning-based BERT Model for Automated Essay Scoring",
        "authors": "Daegon Yu, Yongyeon Kim, Sangwoo Han, Byung-Won On",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14801/jkiit.2023.21.4.31"
    },
    {
        "id": 1732,
        "title": "A false emotion opinion target extraction model with two stage BERT and background information fusion",
        "authors": "ZhiYang Hou, YaJun Du, QiZhi Li, XianYong Li, XiaoLiang Chen, HongMei Gao",
        "published": "2024-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123735"
    },
    {
        "id": 1733,
        "title": "GeoTPE: A neural network model for geographical topic phrases extraction from literature based on BERT enhanced with relative position embedding",
        "authors": "Weirong Li, Kai Sun, Yunqiang Zhu, Fangyu Ding, Lei Hu, Xiaoliang Dai, Jia Song, Jie Yang, Lang Qian, Shu Wang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121077"
    },
    {
        "id": 1734,
        "title": "A semantic-BERT model based on two Bi-LSTM-CNN for arabic question answering",
        "authors": "Abdullah Farhan Mahdi, Rabah N. Farhan, Salah Sleibi Al-Rawi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0203625"
    },
    {
        "id": 1735,
        "title": "A lightweight IoT intrusion detection model based on improved BERT-of-Theseus",
        "authors": "Zhendong Wang, Jingfei Li, Shuxin Yang, Xiao Luo, Dahai Li, Soroosh Mahmoodi",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122045"
    },
    {
        "id": 1736,
        "title": "Umami-BERT: An interpretable BERT-based model for umami peptides prediction",
        "authors": "Jingcheng Zhang, Wenjing Yan, Qingchuan Zhang, Zihan Li, Li Liang, Min Zuo, Yuyu Zhang",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.foodres.2023.113142"
    },
    {
        "id": 1737,
        "title": "MedCT-BERT: Multimodal Mortality Prediction using Medical ConvTransformer-BERT Model",
        "authors": "Ke Zhang, Ke Niu, Yuhang Zhou, Wenjuan Tai, Guoqiang Lu",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictai59109.2023.00109"
    },
    {
        "id": 1738,
        "title": "Feature Mining of News Communication Topic Elements Based on BERT Model",
        "authors": "Fei Yang Zheng",
        "published": "2023-5-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.54647/sociology841046"
    },
    {
        "id": 1739,
        "title": "A Survey on BERT and Its Applications",
        "authors": "Sulaiman Aftan, Habib Shah",
        "published": "2023-1-26",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lt58159.2023.10092289"
    },
    {
        "id": 1740,
        "title": "ColBERT: Using BERT sentence embedding in parallel neural networks for computational humor",
        "authors": "Issa Annamoradnejad, Gohar Zoghi",
        "published": "2024-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123685"
    },
    {
        "id": 1741,
        "title": "BERT-Based Knowledge Distillation for Sentiment Analysis Model",
        "authors": "杨杰 孙",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/csa.2023.1310192"
    },
    {
        "id": 1742,
        "title": "News Short Text Classification Based on Bert Model and Fusion Model",
        "authors": "Hongyang Cui, Chentao Wang, Yibo Yu",
        "published": "2023-2-28",
        "citations": 1,
        "abstract": "Text classification task is one of the most fundamental tasks in NLP, and the classification of short news text could be the basis for many other tasks. In this paper, we applied a fusion model combining Bert and TextRNN with some modified details to expect higher accuracy of text classification. We used the THUCNews as dataset which consists of two columns one for news text and the other for numbers. The original dataset was seperated into three parts: training set, validation set and test set. Besides, we used BERT model which contains two pre-training tasks and TextRNN model which refers to the use of RNN to solve text classification problems. We trained these two models in parallel, and then the optimal Bert and TextRNN models obtained through training and parameter tuning are added with a fully-connected layer to receive the final results by weighting the efficiency of Bert and TextRNN. The fusion model solves the problem of over-fitting and under-fitting of a single model, and helps to obtain a model with better generalization performance. The experimental results show the sharp change in loss and accuracy as well as the final accuracy of the BERT model. The precision, recall-rate and F1-score are also evaluated in this paper. The accuracy of fusion model of BERT and TextRNN is much better than single Bert model and has a gap to 1.76%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5482"
    },
    {
        "id": 1743,
        "title": "medBERT.de: A comprehensive German BERT model for the medical domain",
        "authors": "Keno K. Bressem, Jens-Michalis Papaioannou, Paul Grundmann, Florian Borchert, Lisa C. Adams, Leonhard Liu, Felix Busch, Lina Xu, Jan P. Loyen, Stefan M. Niehues, Moritz Augustin, Lennart Grosser, Marcus R. Makowski, Hugo J.W.L. Aerts, Alexander Löser",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121598"
    },
    {
        "id": 1744,
        "title": "ChatGPT and finetuned BERT: A comparative study for developing intelligent design support systems",
        "authors": "Yunjian Qiu, Yan Jin",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.iswa.2023.200308"
    },
    {
        "id": 1745,
        "title": "Strategy to Develop a Domain-specific Pre-trained Language Model: Case of V-BERT, a Language Model for the Automotive Industry",
        "authors": "Younha Kim, Yunsu Jeon, Junwoo Kim, Namgyu Kim",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5626/ktcp.2023.29.5.228"
    },
    {
        "id": 1746,
        "title": "A New Model for Fuel Transfer Leak Frequencies",
        "authors": "John Spouge, Bert Wolting",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3850/978-981-18-8071-1_p175-cd"
    },
    {
        "id": 1747,
        "title": "Blind PV temperature model calibration",
        "authors": "Anastasios Kladas, Bert Herteleer, Jan Cappelle",
        "published": "2023",
        "citations": 0,
        "abstract": "The determination of module temperature in a photovoltaic (PV) system is a crucial factor in PV modelling and the assessment of system health status. However, the scarcity of on-site temperature measurements poses a challenge, and existing PV temperature models encounter difficulties in accurately estimating temperatures in systems characterized by unique structural or locational attributes. This paper introduces a novel approach that enables the calibration of PV temperature models without relying on direct temperature measurements. Referred to as blind calibration, this method eliminates the requirement for temperature measurements, thus offering a promising solution to the aforementioned challenges. The method is validated using three datasets, demonstrating accurate PV temperature (TPV) estimation with mean absolute errors below 2 °C. The findings highlight the suitability of the proposed approach for various PV system types, while acknowledging limitations regarding certain system configurations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/epjpv/2023021"
    },
    {
        "id": 1748,
        "title": "Improving BERT model for requirements classification by bidirectional LSTM-CNN deep model",
        "authors": "Kamaljit Kaur, Parminder Kaur",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2023.108699"
    },
    {
        "id": 1749,
        "title": "Sentiment Analysis of Text Based on BERT-BiLSTM-BiGRU-CNN Model",
        "authors": "昆 朱",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/mos.2024.131005"
    },
    {
        "id": 1750,
        "title": "BERT-based B2B matching model for K-Startups and Overseas Buyers",
        "authors": "Jungsuk Choi, Nammee Moon",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5762/kais.2023.24.9.769"
    },
    {
        "id": 1751,
        "title": "Covid based question criticality prediction with domain adaptive BERT embeddings",
        "authors": "Shiney Jeyaraj, Raghuveera T.",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.107913"
    },
    {
        "id": 1752,
        "title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model",
        "authors": "Yongchang Cao, Liang He, Zhen Wu, Xinyu Dai",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191265"
    },
    {
        "id": 1753,
        "title": "Greek Political Speech Classification Using BERT",
        "authors": "Kontilenia Maria Kotsifakou, Dionisios N. Sotiropoulos",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345868"
    },
    {
        "id": 1754,
        "title": "BERT-siRNA: siRNA target prediction based on BERT pre-trained interpretable model",
        "authors": "Jiayu Xu, Nan Xu, Weixin Xie, Chengkui Zhao, Lei Yu, Weixing Feng",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.gene.2024.148330"
    },
    {
        "id": 1755,
        "title": "Beyond the use of a novel Ensemble based Random Forest-BERT Model (Ens-RF-BERT) for the Sentiment Analysis of the hashtag COVID19 tweets",
        "authors": "Boutheina Jlifi, Chaima Abidi, Claude Duvallet",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13278-024-01240-x"
    },
    {
        "id": 1756,
        "title": "Text Sentiment Analysis Model Based on BERT and Knowledge Graph",
        "authors": "斐瑜 刘",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/mos.2023.124382"
    },
    {
        "id": 1757,
        "title": "Research of Chinese relation extraction based on BERT",
        "authors": "Zirui Song, Li Wan",
        "published": "2023-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca56706.2023.10075925"
    },
    {
        "id": 1758,
        "title": "Fusion of BERT embeddings and elongation-driven features",
        "authors": "Abderrahim Rafae, Mohammed Erritali, Mathieu Roche",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18786-9"
    },
    {
        "id": 1759,
        "title": "Tweet recommendation using Clustered Bert and Word2vec Models",
        "authors": "Surbhi Kakar, Deepali Dhaka, Monica Mehrotra",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smartnets58706.2023.10215867"
    },
    {
        "id": 1760,
        "title": "An open intent detection model optimized for datasets based on the Bert large model",
        "authors": "Yichen Dong, Zhen Wang, Tianjun Wu",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Within current task-oriented dialogue systems, the focus of intent detection predominantly centers on closed domains. Nevertheless, in real-world usage scenarios, a substantial proportion of interactions fall into the open-domain category. User intentions frequently transcend predefined boundaries, giving rise to a multitude of out-of-domain intents, which pose a formidable challenge to existing models, ultimately leading to diminished recognition rates and accuracy. The demand for open intent detection models is increasing in today's society to address this issue effectively. This paper proposes a method to optimize datasets, thereby enhancing the training accuracy of open intent detection models. Specifically, this paper employs the Adaptive Decision Boundary Learning algorithm, which is currently popular in open intent detection. Leveraging this algorithm, this paper suggests using the K-means clustering algorithm to refine the intent labels within the dataset. This process helps identify and remove outliers in the dataset, making the distinction between known domain and open-domain intent labels more precise. Experimental results on two datasets, banking77 and stackoverflow, demonstrate the effectiveness of our approach in significantly improving model accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241381"
    },
    {
        "id": 1761,
        "title": "Approximate Hybrid Binary-Unary Computing with Applications in BERT Language Model and Image Processing",
        "authors": "Alireza Khataei, Gaurav Singh, Kia Bazargan",
        "published": "2023-2-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543622.3573181"
    },
    {
        "id": 1762,
        "title": "BERT-based coupling evaluation of biological strategies in bio-inspired design",
        "authors": "Feng Sun, He Xu, Yihan Meng, Zhimao Lu, Chengju Gong",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119725"
    },
    {
        "id": 1763,
        "title": "POS-BERT: Point cloud one-stage BERT pre-training",
        "authors": "Kexue Fu, Peng Gao, Shaolei Liu, Linhao Qu, Longxiang Gao, Manning Wang",
        "published": "2024-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122563"
    },
    {
        "id": 1764,
        "title": "Finding BERT Errors Using Activation Vectors",
        "authors": "Vedashree P. Bhandare, William B. Andreopoulos",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdataservice58306.2023.00010"
    },
    {
        "id": 1765,
        "title": "Smart Contract Cloning Identification Using BERT Model Based Representation Learning",
        "authors": "Vishnu Kumar, Pisipati Pranav, M. Srinivas",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440822"
    },
    {
        "id": 1766,
        "title": "A Text Classification Model Based on BERT and Attention",
        "authors": "Binglin Zhu, Wei Pan",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cait59945.2023.10469363"
    },
    {
        "id": 1767,
        "title": "Prediction of Author’s Profile basing on Fine-Tuning BERT model",
        "authors": "Bassem Bsir, Nabil Khoufi, Mounir Zrigui",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v48i1.4839"
    },
    {
        "id": 1768,
        "title": "An Aspect-Level Sentiment Analysis Approach Based on BERT and Attention Mechanism",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7753/ijcatr1206.1002"
    },
    {
        "id": 1769,
        "title": "Automated and Interference-Free Inventory Solution Using Energy-Neutral BLE Tags",
        "authors": "Jona Cappelle, Bert Cox, Liesbet Van der Perre",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sas58821.2023.10254152"
    },
    {
        "id": 1770,
        "title": "Improved Text Matching Model Based on BERT",
        "authors": "Qingyu Li, Yujun Zhang",
        "published": "2023-2-13",
        "citations": 0,
        "abstract": "Text matching is a basic and important task in natural language understanding, this paper proposes a new model BBMC for the problem of insufficient feature extraction ability of existing text matching models, which integrates BiLSTM and multi-scale CNN on the basis of BERT. First, the word embedding representation of the text is obtained by the BERT, and then the semantic features of the text are further extracted by the double-layer BiLSTM, followed by the multi-scale CNN model, the key local features are extracted, and finally the linear and SoftMax function are used to classify. Experimental results on the LCQMC dataset show that the BBMC has been improved to a certain extent compared with other methods, and the accuracy on the test set can be best achieved 88.01%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v2i3.5209"
    },
    {
        "id": 1771,
        "title": "BERT-Based Sentiment Forensics Analysis for Intrusion Detection",
        "authors": "Shahrzad Sayyafzadeh, Hongmei Chi, Weifeng Xu, Kaushik Roy",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00235"
    },
    {
        "id": 1772,
        "title": "Enhancing BERT-Based Chinese Address Recognition Model with Tag Revision Module",
        "authors": "Ruonan Zhao, Xiangwu Ding",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icise60366.2023.00060"
    },
    {
        "id": 1773,
        "title": "Misinformation classification using LSTM and BERT model",
        "authors": "Aditya Harbola, Mahesh Manchanda, Deepti Negi",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icidca56705.2023.10100054"
    },
    {
        "id": 1774,
        "title": "BERT-Based Hybrid Deep Learning with Text Augmentation for Sentiment Analysis of Indonesian Hotel Reviews",
        "authors": "Maxwell Thomson, Hendri Murfi, Gianinna Ardaneswari",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012127400003541"
    },
    {
        "id": 1775,
        "title": "BAG: Text Classification Based on Attention Mechanism Combining BERT and GCN",
        "authors": "想 李",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/sea.2023.122023"
    },
    {
        "id": 1776,
        "title": "BERT-Based Arabic Diacritization: A state-of-the-art approach for improving text accuracy and pronunciation",
        "authors": "Ruba Kharsa, Ashraf Elnagar, Sane Yagi",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123416"
    },
    {
        "id": 1777,
        "title": "BERT for Sentiment Analysis on Rotten Tomatoes Reviews",
        "authors": "Aji Gautama Putrada, Nur Alamsyah, Mohamad Nurkamal Fauzan",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icodsa58501.2023.10276800"
    },
    {
        "id": 1778,
        "title": "Retracted: BERT-Based Clinical Name Entity Reorganization Model for Health Diagnosis",
        "authors": "",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9836340"
    },
    {
        "id": 1779,
        "title": "Construction and Experimental Evaluation of Document Causality Extraction Model Based on CGCN-BERT",
        "authors": "Guijiao He",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaisc58445.2023.10200466"
    },
    {
        "id": 1780,
        "title": "Named Entity Recognition Method Based on BERT-whitening and Dynamic Fusion Model",
        "authors": "Meng Liang, Yao Shi",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnlp58431.2023.00041"
    },
    {
        "id": 1781,
        "title": "Chinese Food Safety Entity Recognition Based on BERT Model",
        "authors": "Shiyong Xiong, Guozhi Bai",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456298"
    },
    {
        "id": 1782,
        "title": "Editorial: Physical neuromorphic computing and its industrial applications",
        "authors": "Toshiyuki Yamane, Akira Hirose, Bert Jan Offrein",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fninf.2023.1238168"
    },
    {
        "id": 1783,
        "title": "Creativity index calculation with question answering system using BERT model",
        "authors": "Abhinav Nandgaonkar, Sunil B Mane, Vaibhav Khatavkar",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205798"
    },
    {
        "id": 1784,
        "title": "Contextual Embeddings-Based Web Page Categorization Using the Fine-Tune BERT Model",
        "authors": "Amit Kumar Nandanwar, Jaytrilok Choudhary",
        "published": "2023-2-2",
        "citations": 4,
        "abstract": "The World Wide Web has revolutionized the way we live, causing the number of web pages to increase exponentially. The web provides access to a tremendous amount of information, so it is difficult for internet users to locate accurate and useful information on the web. In order to categorize pages accurately based on the queries of users, methods of categorizing web pages need to be developed. The text content of web pages plays a significant role in the categorization of web pages. If a word’s position is altered within a sentence, causing a change in the interpretation of that sentence, this phenomenon is called polysemy. In web page categorization, the polysemy property causes ambiguity and is referred to as the polysemy problem. This paper proposes a fine-tuned model to solve the polysemy problem, using contextual embeddings created by the symmetry multi-head encoder layer of the Bidirectional Encoder Representations from Transformers (BERT). The effectiveness of the proposed model was evaluated by using the benchmark datasets for web page categorization, i.e., WebKB and DMOZ. Furthermore, the experiment series also fine-tuned the proposed model’s hyperparameters to achieve 96.00% and 84.00% F1-Scores, respectively, demonstrating the proposed model’s importance compared to baseline approaches based on machine learning and deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15020395"
    },
    {
        "id": 1785,
        "title": "Sentiment Analysis with the Use of Transformers and BERT",
        "authors": "Elisavet Douka, Isidoros Perikos, Ioannis Hatzilygeroudis",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345952"
    },
    {
        "id": 1786,
        "title": "MIL-BERT: Military Domain Specialized Korean Pre-trained Language Model",
        "authors": "Hee-Soon Heo, Chang-Min Yoon, Young-Ha Ryu, Seok-hyun Yong, Dooyoung Kim",
        "published": "2023-6-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31818/jknst.2023.06.6.2.201"
    },
    {
        "id": 1787,
        "title": "A Comparison of LSTM and BERT model for sarcasm prediction",
        "authors": "Yizhong Ding",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Sarcasm prediction is a text analysis task that aims to identify sarcastic and non-sarcastic statements in text. Sarcasm is a figure of speech that uses opposite or contradictory language to express a certain meaning or idea. Sarcasm is usually cryptic, vague, and suggestive, which makes sarcasm prediction a challenging task. In sarcasm prediction projects, techniques of natural language processing are usually leveraged to analyze and classify the text. The main challenge of this task lies in the fact that sarcasm usually has multiple manifestations and needs to consider the contextual and semantic information of the text. The prediction of sarcasm holds significant application value in natural language processing, such as social media analysis, public opinion monitoring, sentiment analysis and so on. In this paper, by controlling variables, the influence of adding the long short-term memory (LSTM) layer and changing the grid structure of the model on the accuracy of prediction results is explored. Moreover, accuracy of the LSTM prediction performance is compared with that of the bidirectional encoder representations from Transformers (BERT) model. At the same time, this paper analyzed and discussed the phenomenon that adding the number of LSTM model layers could not obtain higher prediction accuracy, and the accuracy gap of prediction results between LSTM model and BERT model, and finally obtained relevant conclusions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/21/20231118"
    },
    {
        "id": 1788,
        "title": "Transfer Learning for Sentiment Classification Using Bidirectional Encoder Representations from Transformers (BERT) Model",
        "authors": "Ali Areshey, Hassan Mathkour",
        "published": "2023-5-31",
        "citations": 3,
        "abstract": "Sentiment is currently one of the most emerging areas of research due to the large amount of web content coming from social networking websites. Sentiment analysis is a crucial process for recommending systems for most people. Generally, the purpose of sentiment analysis is to determine an author’s attitude toward a subject or the overall tone of a document. There is a huge collection of studies that make an effort to predict how useful online reviews will be and have produced conflicting results on the efficacy of different methodologies. Furthermore, many of the current solutions employ manual feature generation and conventional shallow learning methods, which restrict generalization. As a result, the goal of this research is to develop a general approach using transfer learning by applying the “BERT (Bidirectional Encoder Representations from Transformers)”-based model. The efficiency of BERT classification is then evaluated by comparing it with similar machine learning techniques. In the experimental evaluation, the proposed model demonstrated superior performance in terms of outstanding prediction and high accuracy compared to earlier research. Comparative tests conducted on positive and negative Yelp reviews reveal that fine-tuned BERT classification performs better than other approaches. In addition, it is observed that BERT classifiers using batch size and sequence length significantly affect classification performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23115232"
    },
    {
        "id": 1789,
        "title": "An Algorithm Model of Power Grid Maintenance Ticket Based on Bert",
        "authors": "Guoqiang Zhang, Qun Wang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictech58362.2023.00104"
    },
    {
        "id": 1790,
        "title": "Enhancing Natural Language Processing with LDA-BERT Integration: A Comprehensive Review of Methodologies, Applications, and Future Directions",
        "authors": "Ming Zheng Ong, Kathleen Swee Neo Tan, Chi Wee Tan",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdxa61007.2024.10470636"
    },
    {
        "id": 1791,
        "title": "Sentiment analysis of imbalanced datasets using BERT and ensemble stacking for deep learning",
        "authors": "Nassera Habbat, Hicham Nouri, Houda Anoun, Larbi Hassouni",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106999"
    },
    {
        "id": 1792,
        "title": "Fake news detection using dual BERT deep neural networks",
        "authors": "Mahmood Farokhian, Vahid Rafe, Hadi Veisi",
        "published": "2023-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17115-w"
    },
    {
        "id": 1793,
        "title": "Offensive Hebrew Corpus and Detection using BERT",
        "authors": "Nagham Hamad, Mustafa Jarrar, Mohammad Khalilia, Nadim Nashif",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiccsa59173.2023.10479258"
    },
    {
        "id": 1794,
        "title": "Hybrid model for extractive single document summarization: utilizing BERTopic and BERT model",
        "authors": "Maryanto Maryanto, Philips Philips, Abba Suganda Girsang",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "Extractive text summarization has been a popular research area for many years. The goal of this task is to generate a compact and coherent summary of a given document, preserving the most important information. However, current extractive summarization methods still face several challenges such as semantic drift, repetition, redundancy, and lack of coherence. A novel approach is presented in this paper to improve the performance of an extractive summarization model based on bidirectional encoder representations from transformers (BERT) by incorporating topic modeling using the BERTopic model. Our method first utilizes BERTopic to identify the dominant topics in a document and then employs a BERT-based deep neural network to extract the most salient sentences related to those topics. Our experiments on the cable news network (CNN)/daily mail dataset demonstrate that our proposed method outperforms state-of-the-art BERT-based extractive summarization models in terms of recall-oriented understudy for gisting evaluation (ROUGE) scores, which resulted in an increase of 32.53% of ROUGE-1, 47.55% of ROUGE-2, and 16.63% of ROUGE-L when compared to baseline BERT-based extractive summarization models. This paper contributes to the field of extractive text summarization, highlights the potential of topic modeling in improving summarization results, and provides a new direction for future research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp1723-1731"
    },
    {
        "id": 1795,
        "title": "Software Subclassification Based on BERTopic-BERT-BiLSTM Model",
        "authors": "Wenjuan Bu, Hui Shu, Fei Kang, Qian Hu, Yuntian Zhao",
        "published": "2023-9-8",
        "citations": 1,
        "abstract": "With the continuous influx of application software onto the application software market, achieving accurate software recommendations for users in the huge software application market is urgent. To address this issue, each application software market currently provides its own classification tags. However, several problems still exist, such as the lack of objectivity, hierarchy, and standardization in these classifications, which in turn affects the accuracy of precise software recommendations. Accordingly, a customized BERTopic model is proposed to cluster the software description texts of the application software and the automatic tagging and updating of the application software tags are realized according to the clusters obtained by topic clustering and the extracted subject words. At the same time, a data enhancement method based on the c-TF-IDF algorithm is proposed to solve the problem of imbalance of datasets, and then the classification model based on the BERT-BiLSTM model is trained on the labeled datasets to classify the software in the dimension of the application function, so as to realize the accurate software recommendation for users. Based on the experimental verification of two datasets, 21 categories in the SourceForge dataset and 19 categories in the Chinese App Store dataset are subclassed by the clustering results of the customized BERTopic model, and the tags of 138 subclasses and 262 subclasses are formed, respectively. In addition, a complete tagged software description text dataset is constructed and the software tags are updated automatically. In the first stage of the classification experiment, the weighted average accuracy, recall rate, and F1 value can reach 0.92, 0.91, and 0.92, respectively. In the second stage, the weighted average accuracy, recall rate, and F1 value can all reach 0.96. After data enhancement, the weighted average F1 value of the classification model can be increased by up to two percentage points.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12183798"
    },
    {
        "id": 1796,
        "title": "BERT-Based Model for Reading Comprehension Question Answering",
        "authors": "Abdelrahman A. Mosaed, Hanan Hindy, Mostafa Aref",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicis58388.2023.10391167"
    },
    {
        "id": 1797,
        "title": "Predicting Disaster Tweets using Enhanced BERT Model",
        "authors": "Premkumar Duraisamy, M Duraisamy, M Periyanayaki, Yuvaraj Natarajan",
        "published": "2023-5-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciccs56967.2023.10142660"
    },
    {
        "id": 1798,
        "title": "LAD: Layer-Wise Adaptive Distillation for BERT Model Compression",
        "authors": "Ying-Jia Lin, Kuan-Yu Chen, Hung-Yu Kao",
        "published": "2023-1-28",
        "citations": 4,
        "abstract": "Recent advances with large-scale pre-trained language models (e.g., BERT) have brought significant potential to natural language processing. However, the large model size hinders their use in IoT and edge devices. Several studies have utilized task-specific knowledge distillation to compress the pre-trained language models. However, to reduce the number of layers in a large model, a sound strategy for distilling knowledge to a student model with fewer layers than the teacher model is lacking. In this work, we present Layer-wise Adaptive Distillation (LAD), a task-specific distillation framework that can be used to reduce the model size of BERT. We design an iterative aggregation mechanism with multiple gate blocks in LAD to adaptively distill layer-wise internal knowledge from the teacher model to the student model. The proposed method enables an effective knowledge transfer process for a student model, without skipping any teacher layers. The experimental results show that both the six-layer and four-layer LAD student models outperform previous task-specific distillation approaches during GLUE tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23031483"
    },
    {
        "id": 1799,
        "title": "Deep Active Learning for Address Parsing Tasks with BERT",
        "authors": "Berkay Güler, Betül Aygün, Aydın Gerek, Alaeddin Selçuk Gürel",
        "published": "2023-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223996"
    },
    {
        "id": 1800,
        "title": "A Contextual Query Expansion Model using BERT Based Deep Neural Embeddings",
        "authors": "Deepak Vishwakarma, Suresh Kumar",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10111984"
    },
    {
        "id": 1801,
        "title": "Quantification of Carbon Emission Technologies Based on Knowledge Graph Bert-BiLSTM-Attention-CRF Model",
        "authors": "Zicheng Zhao",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icedcs60513.2023.00014"
    },
    {
        "id": 1802,
        "title": "COVBERT: Enhancing Sentiment Analysis Accuracy in COVID-19 X Data through Customized BERT",
        "authors": "Vanshaj Gupta, Jaydeep Patel, Safa Shubbar, Kambiz Ghazinour",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "In a time when social media information is a valuable resource for gaining insights, the COVID-19 pandemic has released a flood of public sentiment, abundant with unstructured text data. This paper introduces CovBERT, a novel adaptation of the BERT model, specifically honed for the nuanced analysis of COVID-19-related discourse on X (formerly Twitter). CovBERT stands out by incorporating a bespoke vocabulary, meticulously curated from pandemic-centric tweets, resulting in a remarkable leap in sentiment analysis accuracy—from the baseline 72\\% to an impressive 78.64\\%. This paper not only presents a detailed comparison of CovBERT with the standard BERT model but also juxtaposes it against traditional machine learning approaches, showcasing its superior proficiency in decoding complex emotional undercurrents in social media data. Furthermore, the integration of geolocation analysis pipeline adds another layer of depth, offering a panoramic view of global sentiment trends.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.140212"
    },
    {
        "id": 1803,
        "title": "BERT-CNN-BiLSTM: A Hybrid Deep Learning Model for Accurate Sentiment Analysis",
        "authors": "Yusheng He",
        "published": "2023-7-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpics58376.2023.10235335"
    },
    {
        "id": 1804,
        "title": "An Experimental Study on Efficient Antenna OTA Test Method for Automotive Applications",
        "authors": "Zhichao Chen, Lukas Berkelmann, Carsten Monka-Ewe, Bert Jannsen",
        "published": "2023-9-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eumc58039.2023.10290223"
    },
    {
        "id": 1805,
        "title": "Malicious encrypted traffic detection based on Bert and one-dimensional CNN model",
        "authors": "Pengkai Kang",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icetci57876.2023.10176917"
    },
    {
        "id": 1806,
        "title": "Social Media Analytics for Disaster Management using BERT Model",
        "authors": "Sherin R Varghese, Sujitha Juliet, J. Anitha",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icercs57948.2023.10434012"
    },
    {
        "id": 1807,
        "title": "Sentiment analysis of Twitter user text based on the BERT model",
        "authors": "Chenyang Zhou",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "Deep Neural Networks (DNNs) utilizing Recurrent Neural Network (RNN) architectures have found extensive application in text sentiment analysis. A prevailing notion suggests that augmenting the model's capacity can significantly improve accuracy and overall model performance. Building upon this premise, this paper advocates the adoption of a larger BERT model for text sentiment analysis. Bidirectional Encoder Representations from Transformers (BERT) is a sophisticated pre-trained language comprehension model that leverages Transformers as feature extractors.  However, as the amount of model data increases, exceeding the memory limitations of a single GPU, algorithm optimization becomes crucial. Therefore, this paper employs two methods, namely data parallelism and GPipe parallelism, to accelerate and optimize the BERT model. Compared to a single GPU, training speed almost linearly increases with the addition of more GPUs. In addition, this research investigates the accuracy of the most advanced language model, chatgpt, by reannotating the dataset. During training, it was observed that the accuracy of the chatgpt-annotated dataset significantly declined in both RNN and BERT models. This indicates that chatgpt still exhibits some errors in sentiment text analysis.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/52/20241380"
    },
    {
        "id": 1808,
        "title": "AFS-BERT: Information entropy-based adaptive fusion sampling and Bert embedding model for link prediction",
        "authors": "Lei Zhang, Jiaxing Pan, Xiaoxuan Ma, Chengwei Yang",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": " Link prediction is an important problem in complex network analysis, which can discover missing or possible future edges in the network. In recent years, link prediction methods based on network representation learning have made progress. But there are two problems with these methods. One is that neighborhood-based node sampling methods cannot handle the situation between two nodes that do not have any common neighbors. The other is the Skip-Gram-based embedding model that represents nodes as static vectors, which cannot reflect the various meanings of nodes. To overcome these two limitations, this paper proposes a method called AFS-BERT (Information entropy based Adaptive Fusion Sampling and BERT embedding model). First, this method defines a centrality score based on adjacency information entropy, which reflects the global and local importance of nodes. Second, we propose a sampling method that adaptively fuses two different strategies using the centrality score. Finally, the BERT-based embedding model is used to realize the low-dimensional dynamic vector representation of nodes. Experimental result on six real-world network datasets shows that AFS-BERT has better performance. Compared with methods of the same type, AFS-BERT achieves upto 6.7% improvement. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0217979223502314"
    },
    {
        "id": 1809,
        "title": "EHR-BERT: A BERT-based model for effective anomaly detection in electronic health records",
        "authors": "Haoran Niu, Olufemi A. Omitaomu, Michael A. Langston, Mohammad Olama, Ozgur Ozmen, Hilda B. Klasky, Angela Laurio, Merry Ward, Jonathan Nebeker",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jbi.2024.104605"
    },
    {
        "id": 1810,
        "title": "Krishiq-BERT: A Few-Shot Setting BERT Model to Answer Agricultural-Related Questions in the Kannada Language",
        "authors": "Pratijnya Ajawan, Veena Desai, Shreya Kale, Sachingouda Patil",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40031-023-00952-6"
    },
    {
        "id": 1811,
        "title": "A Named Entity Recognition Model Based on BERT Model and Lexical Fusion in the Financial Regulation Field",
        "authors": "Xiaoguo Wang, Xiangbo Pan, Chao Chen, Jianwen Cui",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10455867"
    },
    {
        "id": 1812,
        "title": "EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain",
        "authors": "M Arif Rahman, Sarah Masud Preum, Ronald D. Williams, Homa Alemzadeh, John Stankovic",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3580252.3586978"
    },
    {
        "id": 1813,
        "title": "Supplementing domain knowledge to BERT with semi-structured information of documents",
        "authors": "Jing Chen, Zhihua Wei, Jiaqi Wang, Rui Wang, Chuanyang Gong, Hongyun Zhang, Duoqian Miao",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121054"
    },
    {
        "id": 1814,
        "title": "DetBERT: Enhancing Detection of Policy Violations for Voice Assistant Applications using BERT",
        "authors": "Rawan Baalous, Joud Alzahrani, Mariam Ali, Rana Asiri, Eman Nooli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140974"
    },
    {
        "id": 1815,
        "title": "Tail asymptotics for the delay in a Brownian fork-join queue",
        "authors": "Dennis Schol, Maria Vlasiou, Bert Zwart",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.spa.2023.06.013"
    },
    {
        "id": 1816,
        "title": "Resume Parsing Across Multiple Job Domains Using a BERT-Based NER Model",
        "authors": "Madhumita Srivastava, Paul Greaney",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aics60730.2023.10470917"
    },
    {
        "id": 1817,
        "title": "Enhanced Phishing URL Detection Using Leveraging BERT with Additional URL Feature Extraction",
        "authors": "K S Jishnu, B Arthi",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icirca57980.2023.10220647"
    },
    {
        "id": 1818,
        "title": "Effective Model for Improving Symptoms-Based Disease Prediction by BiMM - BERT Algorithm",
        "authors": "V. Jayasudha, N. Deepa, T Devi.",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccci56745.2023.10128318"
    },
    {
        "id": 1819,
        "title": "Design of a Biologically Inspired Active Visual Communication Strategy for Robotic Applications",
        "authors": "Bryan Cochran, Bert Bras",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "Abstract\nThis research considers the development of a biologically inspired active visual communication strategy for use within Multiple Robot Systems. Robotic systems currently communicate using radio frequency (RF) broadcasts, but they cannot transition from one communication modality to another unlike biological organisms. Given the extent in which robotic platforms employ machine vision systems, robotic systems could benefit from implementation of a visual communication modality. Humans commonly employ visual and auditory communication modalities and actively use a variety of different methods of visual communication, but none have been applied to robotic communication. Within biology however, there are numerous active visual communication strategies that may be better suited for robotic applications as compared to current human systems. Biological organisms employ a variety of different communication strategies to allow communication to occur in different environments, and some organisms are capable of transitioning from one communication strategy to another if a situation necessitates it. in a variety of different environments. The work detailed in this article investigates biological visual communication methods to be used for the development of an active robotic visual communication system suitable for use within a Multiple Robot System. This research in this article will highlight an investigation and the subsequent functional decomposition of these communication systems. The decomposition will be used to develop the foundation of the robotic visual communication system in later work.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/detc2023-117141"
    },
    {
        "id": 1820,
        "title": "Comparable GPU: Optimizing the BERT Model with AMX Feature",
        "authors": "Xiang Gao, Xiancheng Lin, Rongkai Liu",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccai57533.2023.10201262"
    },
    {
        "id": 1821,
        "title": "Unravelling the three lines model in cybersecurity: a systematic literature review",
        "authors": "Bert Valkenburg, Ivano Bongiovanni",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cose.2024.103708"
    },
    {
        "id": 1822,
        "title": "Graph-BERT and language model-based framework for protein–protein interaction identification",
        "authors": "Kanchan Jha, Sourav Karmakar, Sriparna Saha",
        "published": "2023-4-6",
        "citations": 9,
        "abstract": "AbstractIdentification of protein–protein interactions (PPI) is among the critical problems in the domain of bioinformatics. Previous studies have utilized different AI-based models for PPI classification with advances in artificial intelligence (AI) techniques. The input to these models is the features extracted from different sources of protein information, mainly sequence-derived features. In this work, we present an AI-based PPI identification model utilizing a PPI network and protein sequences. The PPI network is represented as a graph where each node is a protein pair, and an edge is defined between two nodes if there exists a common protein between these nodes. Each node in a graph has a feature vector. In this work, we have used the language model to extract feature vectors directly from protein sequences. The feature vectors for protein in pairs are concatenated and used as a node feature vector of a PPI network graph. Finally, we have used the Graph-BERT model to encode the PPI network graph with sequence-based features and learn the hidden representation of the feature vector for each node. The next step involves feeding the learned representations of nodes to the fully connected layer, the output of which is fed into the softmax layer to classify the protein interactions. To assess the efficacy of the proposed PPI model, we have performed experiments on several PPI datasets. The experimental results demonstrate that the proposed approach surpasses the existing PPI works and designed baselines in classifying PPI.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-31612-w"
    },
    {
        "id": 1823,
        "title": "Enhancing Sentiment Analysis Using MCNN-BRNN Model with BERT",
        "authors": "Sami Almlawi, Juan Fang, Jingjing Li",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiecc60864.2023.10456641"
    },
    {
        "id": 1824,
        "title": "Classification of Clothing Quality Dimension Based on Consumer Review Using BERT and RoBERTa",
        "authors": "Nadhif Ditertian Girawan, Andry Alamsyah",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icodsa58501.2023.10277164"
    },
    {
        "id": 1825,
        "title": "BERT: A Review of Applications in Sentiment Analysis",
        "authors": "Md Shohel Sayeed, Varsha Mohan, Kalaiarasi Sonai Muthu",
        "published": "2023-6-1",
        "citations": 1,
        "abstract": "E-commerce reviews are becoming more valued by both customers and companies. The high demand for sentiment analysis is driven by businesses relying on it as a crucial tool to improve product quality and make informed decisions in a fiercely competitive business environment. The purpose of this review paper is to explore and evaluate the applications of the BERT model, a Natural Language Processing (NLP) technique, in sentiment analysis across various fields. The model has been utilized in certain studies for various languages, restaurant businesses, agriculture, Automated Essay Scoring (AES), Twitter, and Google Play. The BERT model's fine-tuning steps involve using pre-trained BERT to perform various language understanding tasks. Text pre-processing is conducted to clean up the data and convert it to numbers before feeding it into BERT, which generates vectors for each input token. We found that BERT outperformed the norm on a range of general language understanding tasks, including sentiment analysis, paraphrase recognition, question-answering, and linguistic acceptability. The detection of neutral reviews and the presence of false reviews in the dataset are two problems that have an impact on the model's accuracy. Training is also slow because it is huge and there are many weights to update. Additional research could be conducted to improve the BERT model's accuracy by constructing a false review categorization model and providing more training to the model in recognizing neutral reviews. Doi: 10.28991/HIJ-2023-04-02-015 Full Text: PDF",
        "keywords": "",
        "link": "http://dx.doi.org/10.28991/hij-2023-04-02-015"
    },
    {
        "id": 1826,
        "title": "CASBERT: BERT-based retrieval for compositely annotated biosimulation model entities",
        "authors": "Yuda Munarko, Anand Rampadarath, David P. Nickerson",
        "published": "2023-2-14",
        "citations": 1,
        "abstract": "Maximising FAIRness of biosimulation models requires a comprehensive description of model entities such as reactions, variables, and components. The COmputational Modeling in BIology NEtwork (COMBINE) community encourages the use of Resource Description Framework with composite annotations that semantically involve ontologies to ensure completeness and accuracy. These annotations facilitate scientists to find models or detailed information to inform further reuse, such as model composition, reproduction, and curation. SPARQL has been recommended as a key standard to access semantic annotation with RDF, which helps get entities precisely. However, SPARQL is unsuitable for most repository users who explore biosimulation models freely without adequate knowledge of ontologies, RDF structure, and SPARQL syntax. We propose here a text-based information retrieval approach, CASBERT, that is easy to use and can present candidates of relevant entities from models across a repository’s contents. CASBERT adapts Bidirectional Encoder Representations from Transformers (BERT), where each composite annotation about an entity is converted into an entity embedding for subsequent storage in a list of entity embeddings. For entity lookup, a query is transformed to a query embedding and compared to the entity embeddings, and then the entities are displayed in order based on their similarity. The list structure makes it possible to implement CASBERT as an efficient search engine product, with inexpensive addition, modification, and insertion of entity embedding. To demonstrate and test CASBERT, we created a dataset for testing from the Physiome Model Repository and a static export of the BioModels database consisting of query-entities pairs. Measured using Mean Average Precision and Mean Reciprocal Rank, we found that our approach can perform better than the traditional bag-of-words method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fbinf.2023.1107467"
    },
    {
        "id": 1827,
        "title": "Improving Bert Model Accuracy for Uni-modal Aspect-Based Sentiment Analysis Task",
        "authors": "Amit Chauhan, Rajni Mohana",
        "published": "2023-9-10",
        "citations": 0,
        "abstract": "Techniques and methods for examining users’ feelings, emotions, and views in text or other media are known as ”sentiment analysis,” this phrase is used frequently. In many areas, including marketing and online social media, analysis of user and consumer opinions has always been essential to decision-making processes. The development of new methodologies that concentrate on analysing the sentiment associated with specific product characteristics, such as aspect-based sentiment analysis (ABSA), was prompted by the need for a deeper understanding of these opinions. Despite the growing interest in this field, some misunderstanding exists about ABSA’s core ideas. Even though sentiment, affect, emotion, and opinion refer to various ideas, they are frequently used synonymously. This ambiguity commonly causes user opinions to be analysed incorrectly. This work provides an overview of ABSA and the issue of overfitting. Following this analysis, we improved the model by enhancing the accuracy and F1 score of the existing model by fine-tuning the technique. Our model outperformed the others, achieving the best results for the restaurant dataset with an 85.02 accuracy and a 79.19 F1 score, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.12694/scpe.v24i3.2444"
    },
    {
        "id": 1828,
        "title": "BERT-Based Ensemble Model for Statute Law Retrieval and Legal Information Entailment",
        "authors": "Hsuan-Lei Shao, Yi-Chia Chen, Sieh-Chuen Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4318438"
    },
    {
        "id": 1829,
        "title": "Enhancing Depression Detection: Employing Autoencoders and Linguistic Feature Analysis with BERT and LSTM Model",
        "authors": "Neda Firoz, Olga G. Beresteneva, Sergey V. Aksyonov",
        "published": "2023-9-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rusautocon58002.2023.10272795"
    },
    {
        "id": 1830,
        "title": "Performance analysis of robustness of BERT model under attack",
        "authors": "Xi Chen",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "Abstract\nWith the aim of testing the robustness of machine learning models, this paper tests the performance of five classification models based on IMDB datasets. Furthermore, in this work, two types of sentence embeddings generated by word2vec and BERT are added with the noise of the normal distribution with different intensities. They are fed into the Support Vector Machine for testing. The experimental results show that the performance of the model slowly decreases as the noise intensity is increased. BERT-based sentiment embedding reduces less than Word2vec-based sentiment embedding. This paper believes that this experimental phenomenon can be used as a basis to support the robustness of BERT representation learning. At the same time, this paper also noted that the previous work experimented with perturbating the BERT model by replacing the words in the original text of the IMDB dataset, causing the BERT model performance to drop sharply. However, the experiment in this paper tested the robustness of the BERT model by embedding tampering, and the experimental results were stable. In this paper, it is assumed that word substitution and embedding interference are possible situations when operating the model. Therefore, for the two different experimental phenomena, this paper plans to design more detailed experiments in the next work to check the robustness of the model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2580/1/012022"
    },
    {
        "id": 1831,
        "title": "Short-Text Semantic Similarity Model of BERT-Based Siamese Network",
        "authors": "Haoyu Jiang",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3585967.3585994"
    },
    {
        "id": 1832,
        "title": "Named entity recognition for natural language understanding using BERT model",
        "authors": "Sandeep Kumar, Arun Solanki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0181535"
    },
    {
        "id": 1833,
        "title": "Hotel Reviews Classification and Review-based Recommendation Model Construction using BERT and RoBERTa",
        "authors": "Yudinda Gilang Pramudya, Andry Alamsyah",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoiact59844.2023.10455890"
    },
    {
        "id": 1834,
        "title": "Extracting Course Text Entity Relationships Based on Improved Bert BiGRU Ratt Model",
        "authors": "Jingdong Wang, Yongjia Guo",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cicn59264.2023.10402260"
    },
    {
        "id": 1835,
        "title": "Failure Identification Using Model-Implemented Fault Injection with Domain Knowledge-Guided Reinforcement Learning",
        "authors": "Mehrdad Moradi, Bert Van Acker, Joachim Denil",
        "published": "2023-2-14",
        "citations": 3,
        "abstract": "The safety assessment of cyber-physical systems (CPSs) requires tremendous effort, as the complexity of cyber-physical systems is increasing. A well-known approach for the safety assessment of CPSs is fault injection (FI). The goal of fault injection is to find a catastrophic fault that can cause the system to fail by injecting faults into it. These catastrophic faults are less likely to occur, and finding them requires tremendous labor and cost. In this study, we propose a reinforcement learning (RL)-based method to automatically configure faults in the system under test and to find catastrophic faults in the early stage of system development at the model level. The proposed method provides a guideline to utilize high-level domain knowledge about a system model for constructing the reinforcement learning agent and fault injection setup. In this study, we used the system (safety) specification to shape the reward function in the reinforcement learning agent. The reinforcement learning agent dynamically interacted with the model under test to identify catastrophic faults. We compared the proposed method with random-based fault injection in two case studies using MATLAB/Simulink. Our proposed method outperformed random-based fault injection in terms of the severity and number of faults found.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23042166"
    },
    {
        "id": 1836,
        "title": "Text Labels Classification Model based on BERT Algorithm",
        "authors": "Sai Wu, Zhiyin Huang, Hao Feng",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icbase59196.2023.10303262"
    },
    {
        "id": 1837,
        "title": "TMD-BERT: A Transformer-Based Model for Transportation Mode Detection",
        "authors": "Ifigenia Drosouli, Athanasios Voulodimos, Paris Mastorocostas, Georgios Miaoulis, Djamchid Ghazanfarpour",
        "published": "2023-1-24",
        "citations": 1,
        "abstract": "Aiming to differentiate various transportation modes and detect the means of transport an individual uses, is the focal point of transportation mode detection, one of the problems in the field of intelligent transport which receives the attention of researchers because of its interesting and useful applications. In this paper, we present TMD-BERT, a transformer-based model for transportation mode detection based on sensor data. The proposed transformer-based approach processes the entire sequence of data, understand the importance of each part of the input sequence and assigns weights accordingly, using attention mechanisms, to learn global dependencies in the sequence. The experimental evaluation shows the high performance of the model compared to the state of the art, demonstrating a prediction accuracy of 98.8%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12030581"
    },
    {
        "id": 1838,
        "title": "Intelligent Sentencing Assistance System based on BERT Model",
        "authors": "Yangli Zhu, Peimeng Luo",
        "published": "2023-10-12",
        "citations": 0,
        "abstract": "This paper takes sentencing prediction as the research object, uses deep learning model, and predicts sentencing recommendations combined with specific cases. BERT+DNN model is used to quantify the text of the case information, endow the sentence with certain meaning, and then send data to DNN network to train the classification model so that it can automatically identify the sentencing circumstances in the text of different cases, and put forward sentencing recommendations based on various circumstances.",
        "keywords": "",
        "link": "http://dx.doi.org/10.62051/9tkmjm47"
    },
    {
        "id": 1839,
        "title": "ASSBert: Active and semi-supervised bert for smart contract vulnerability detection",
        "authors": "Xiaobing Sun, Liangqiong Tu, Jiale Zhang, Jie Cai, Bin Li, Yu Wang",
        "published": "2023-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jisa.2023.103423"
    },
    {
        "id": 1840,
        "title": "LDA-Bert based public opinion subject mining analysis of emergencies",
        "authors": "Tiantian Liu, Xiaoyan Gu",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679263"
    },
    {
        "id": 1841,
        "title": "Employing the BERT model for sentiment analysis of online commentary",
        "authors": "Bowen Li, Xiaolu Liu, Ruijia Zhang",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "The objective of this research is to carry out a tone and semantic sentiment analysis of network comments on new media platforms by leveraging the BERT model. With the burgeoning popularity of social media, network comments, rich in emotional and tonal features, have emerged as a significant part of the online culture. Accurate interpretation and analysis of these comments' sentiment and semantic meanings are paramount to grasping online public opinion and user psychology. In this study, the BERT model, lauded for its bidirectional encoding and contextual understanding capabilities, is selected to scrutinize the sentiment and tone of network comments on new media platforms. Through a process of pre-training and fine-tuning, the sentiment attitudes and polarity of comments are accurately identified along with their conveyed tonal features, such as joy, anger, and sarcasm. Conducting an accurate tone and semantic sentiment analysis of network comments on new media platforms facilitates a profound understanding of user preferences and trends in public opinion. This can assist in optimizing content recommendations, enhancing user experiences, and increasing the operational effectiveness of new media platforms. The outcomes of this research will bear significant implications for studies and applications in online culture, offering invaluable references and guidance in related domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/32/20230218"
    },
    {
        "id": 1842,
        "title": "Multi-Task Learning Model Based on BERT and Knowledge Graph for Aspect-Based Sentiment Analysis",
        "authors": "Zhu He, Honglei Wang, Xiaoping Zhang",
        "published": "2023-2-1",
        "citations": 4,
        "abstract": "Aspect-based sentiment analysis (ABSA) aims to identify the sentiment of an aspect in a given sentence and thus can provide people with comprehensive information. However, many conventional methods need help to discover the linguistic knowledge implicit in sentences. Additionally, they are susceptible to unrelated words. To improve the performance of the model in the ABSA task, a multi-task sentiment analysis model based on Bidirectional Encoder Representation from Transformers (BERT) and a Knowledge Graph (SABKG) is proposed in this paper. Expressly, part-of-speech information is incorporated into the output representation of BERT, thereby obtaining textual semantic information through linguistic knowledge. It also enhances the textual representation to identify the aspect terms. Moreover, this paper constructs a knowledge graph of aspect and sentiment words. It uses a graph neural network to learn the embeddings in the triplet of “aspect word, sentiment polarity, sentiment word”. The constructed graph improves the contextual relationship between the text’s aspect and sentiment words. The experimental results on three open datasets show that the proposed model can achieve the most advanced performance compared with previous models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12030737"
    },
    {
        "id": 1843,
        "title": "Multimodal sentiment analysis with BERT-ResNet50",
        "authors": "Senchang Zhang, Yue He, Lei Li, Yaowen Dou",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679113"
    },
    {
        "id": 1844,
        "title": "EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network",
        "authors": "Hao Shen, Xiaolin Ju, Xiang Chen, Guang Yang",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00114"
    },
    {
        "id": 1845,
        "title": "Cycling speed variation: a multilevel model of characteristics of cyclists, trips and route tracking points",
        "authors": "Hong Yan, Kees Maat, Bert van Wee",
        "published": "2024-4-5",
        "citations": 0,
        "abstract": "AbstractSmooth cycling can improve the competitiveness of bicycles. Understanding cycling speed variation during a trip reveals the infrastructure or situations which promote or prevent smooth cycling. However, research on this topic is still limited. This study analyses speed variation based on data collected in the Netherlands, using GPS-based devices, continuously recording geographical positions and thus the variation in speeds during trips. Linking GPS data to spatial data sources adds features that vary during the trip. Multilevel mixed-effects models were estimated to test the influence of factors at cyclist, trip and tracking point levels. Results show that individuals who prefer a high speed have a higher average personal speed. Longer trips and trips made by conventional electric bicycles and sport bicycles have a higher average trip speed. Tracking point level variables explain intra-trip cycling speed variations. Light-medium precipitation and tailwind increase cycling speed, while both uphill and downhill cycling is relatively slow. Cycling in natural and industrial areas is relatively fast. Intersections, turns and their adjacent roads decrease cycling speed. The higher the speed, the stronger the influence of infrastructure on speed. Separate bicycle infrastructure, such as bike tracks, streets and lanes, increase speed. These findings are useful in the areas of cycling safety, mode choice models and bicycle accessibility analysis. Furthermore, these findings provide additional evidence for smooth cycling infrastructure construction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11116-024-10477-6"
    },
    {
        "id": 1846,
        "title": "Enhancing Arabic offensive language detection with BERT-BiGRU model",
        "authors": "Rajae Bensoltane, Taher Zaki",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "With the advent of Web 2.0, various platforms and tools have been developed to allow internet users to express their opinions and thoughts on diverse topics and occurrences. Nevertheless, certain users misuse these platforms by sharing hateful and offensive speeches, which has a negative impact on the mental health of internet society. Thus, the detection of offensive language has become an active area of research in the field of natural language processing. Rapidly detecting offensive language on the internet and preventing it from spreading is of great practical significance in reducing cyberbullying and self-harm behaviors. Despite the crucial importance of this task, limited work has been done in this field for nonEnglish languages such as Arabic. Therefore, in this paper, we aim to improve the results of Arabic offensive language detection without the need for laborious preprocessing or feature engineering work. To achieve this, we combine the bidirectional encoder representations from transformers (BERT) model model with a bidirectional gated recurrent unit (BiGRU) layer to further enhance the extracted context and semantic features. The experiments were conducted on the Arabic dataset provided by the SemEval 2020 Task 12. The evaluation results show the effectiveness of our model compared to the baseline and related work models by achieving a macro F1- score of 93.16%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/eei.v13i2.6530"
    },
    {
        "id": 1847,
        "title": "KBMQA: medical question and answering model based on Knowledge Graph and BERT",
        "authors": "Zhangkui Liu, Tao Wu",
        "published": "2023-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2685841"
    },
    {
        "id": 1848,
        "title": "Explaining BERT model decisions for near-duplicate news article detection based on named entity recognition",
        "authors": "Anne Stockem Novo, Fatih Gedikli",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc56153.2023.00054"
    },
    {
        "id": 1849,
        "title": "Automated vulnerability detection of blockchain smart contacts based on BERT artificial intelligent model",
        "authors": "Feng Yiting, Ma Zhaofeng, Duan Pengfei, Luo Shoushan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/jcc.ja.2023-0189"
    },
    {
        "id": 1850,
        "title": "Deep Learning-based Sentence Embeddings using BERT for Textual Entailment",
        "authors": "Mohammed Alsuhaibani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.01408108"
    },
    {
        "id": 1851,
        "title": "Comparison of the Performance of Chinese Text Classification Based on Bert Augmented Model",
        "authors": "P. Zhiying Hou, S. Junsheng Yu",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csrswtc60855.2023.10427146"
    },
    {
        "id": 1852,
        "title": "Implementing Quantization to Indonesian BERT Language Model",
        "authors": "Muhammad Ayyub Abdurrahman, Samuel Cahyawijaya, Genta Indra Winata, Ayu Purwarianti",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390159"
    },
    {
        "id": 1853,
        "title": "Knowledge Distillation Scheme for Named Entity Recognition Model Based on BERT",
        "authors": "Chengqiong Ye, Alexander A. Hernandez",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mlbdbi60823.2023.10482264"
    },
    {
        "id": 1854,
        "title": "Implications of Tokenizers in BERT Model for Low-Resource Indian Language",
        "authors": " N. Venkatesan,  N. Arulanand",
        "published": "2023-1-18",
        "citations": 0,
        "abstract": "For any deep learning language model, the initial tokens are prepared as a part of the text preparation process, Tokenization. Important de facto models like BERT and GPT de facto utilize WordPiece and Byte Pair Encoding (BPE) as approaches. Tokenization may have a distinct impact on models for low-resource languages, such as the south Indian Dravidian languages, where many words may be produced by adding prefixes and suffixes. In this paper, four tokenizers are compared at various granularity levels, i.e., their outputs range from the tiniest individual letters to words in their most basic form. Using the BERT pretraining process on the Tamil text, these tokenizers as well as the language models are trained. The model is then fine-tuned with numerous parameters adjusted for the improved performance for a subsequent job in Tamil text categorization. The custom-built tokenizer for Tamil text is created and trained with BPE, WordPiece Vocabulary, Unigram, and WordLevel mechanisms and the compared results are presented after the downstream task of Tamil text categorization is performed using the BERT algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36548/jscp.2022.4.005"
    },
    {
        "id": 1855,
        "title": "Advertisement Video Classification Methodology Based on Auditory Information Using the Korean BERT Model",
        "authors": "Yeon-Ji Park, Geun-Je Yang, Chae-Bong Sohn",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.9728/dcs.2024.25.1.121"
    },
    {
        "id": 1856,
        "title": "Text classification study of enterprise support policies based on Bert+DPCNN+BiGRU model",
        "authors": "Haibo Wang, Ningpeng Jiang, Ruzhi Xu, Shiyu Ma, Yang Li",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55092/pcs20240002"
    },
    {
        "id": 1857,
        "title": "BERT-5mC: an interpretable model for predicting 5-methylcytosine sites of DNA based on BERT",
        "authors": "Shuyu Wang, Yinbo Liu, Yufeng Liu, Yong Zhang, Xiaolei Zhu",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "DNA 5-methylcytosine (5mC) is widely present in multicellular eukaryotes, which plays important roles in various developmental and physiological processes and a wide range of human diseases. Thus, it is essential to accurately detect the 5mC sites. Although current sequencing technologies can map genome-wide 5mC sites, these experimental methods are both costly and time-consuming. To achieve a fast and accurate prediction of 5mC sites, we propose a new computational approach, BERT-5mC. First, we pre-trained a domain-specific BERT (bidirectional encoder representations from transformers) model by using human promoter sequences as language corpus. BERT is a deep two-way language representation model based on Transformer. Second, we fine-tuned the domain-specific BERT model based on the 5mC training dataset to build the model. The cross-validation results show that our model achieves an AUROC of 0.966 which is higher than other state-of-the-art methods such as iPromoter-5mC, 5mC_Pred, and BiLSTM-5mC. Furthermore, our model was evaluated on the independent test set, which shows that our model achieves an AUROC of 0.966 that is also higher than other state-of-the-art methods. Moreover, we analyzed the attention weights generated by BERT to identify a number of nucleotide distributions that are closely associated with 5mC modifications. To facilitate the use of our model, we built a webserver which can be freely accessed at: http://5mc-pred.zhulab.org.cn.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7717/peerj.16600"
    },
    {
        "id": 1858,
        "title": "Developing an Advanced Software Requirements Classification Model Using BERT: An Empirical Evaluation Study on Newly Generated Turkish Data",
        "authors": "Fatih Yucalar",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "Requirements Engineering (RE) is an important step in the whole software development lifecycle. The problem in RE is to determine the class of the software requirements as functional (FR) and non-functional (NFR). Proper and early identification of these requirements is vital for the entire development cycle. On the other hand, manual identification of these classes is a timewaster, and it needs to be automated. Methodically, machine learning (ML) approaches are applied to address this problem. In this study, twenty ML algorithms, such as Naïve Bayes, Rotation Forests, Convolutional Neural Networks, and transformers such as BERT, were used to predict FR and NFR. Any ML algorithm requires a dataset for training. For this goal, we generated a unique Turkish dataset having collected the requirements from real-world software projects with 4600 samples. The generated Turkish dataset was used to assess the performance of the three groups of ML algorithms in terms of F-score and related statistical metrics. In particular, out of 20 ML algorithms, BERTurk was found to be the most successful algorithm for discriminating FR and NFR in terms of a 95% F-score metric. From the FR and NFR identification problem point of view, transformer algorithms show significantly better performances.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132011127"
    },
    {
        "id": 1859,
        "title": "Named entity recognition in medical field based on BERT model",
        "authors": "bin Ma, lin Chen",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3011763"
    },
    {
        "id": 1860,
        "title": "Leveraging BERT-GRU Model for Drugs Interaction Relationship Extraction",
        "authors": "Hang Shen, Xun Zhu, Hongtao Deng",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscsic60498.2023.00081"
    },
    {
        "id": 1861,
        "title": "Validity Frame–enabled model-based engineering processes",
        "authors": "Bert Van Acker, Paul De Meulenaere, Hans Vangheluwe, Joachim Denil",
        "published": "2024-2",
        "citations": 0,
        "abstract": " Model-based systems engineering (MBSE) focuses on using models to support the design, optimization, simulation, and ultimately deployment of complex cyber-physical systems (CPSs). These models enable reasoning about and predicting the behavior of the (realized) real-world system in silico. The value of using such (predictive) model depends on its validity against its real-world counterpart. As such, the validity context of a model is critical to ensure correct model use. Reasoning on validity is only possible if the validity of the model was captured explicitly at design time. In previous work, the validity frame (VF) was presented as a way to explicitly capture a model’s validity; however, no guidance on the integration process within MBSE processes was given. Within this article, we present the creation and evolution of the model and its VFs to ensure model validity consistency and completeness. This evolution results in a set of interrelated models and VFs. By capturing these relations, we create a lightweight frame-enabled library of model variants. We show our contribution using an F1/10 vehicle simulation test bench. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/00375497231205035"
    },
    {
        "id": 1862,
        "title": "English to Kannada Translation Using BERT Model",
        "authors": "Sannidhi Shetty, Pratiksha Narasimha Nayak G, Pranamya Mady, Vaishnavi K Bhustali, Chetana Hegde",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nmitcon58196.2023.10276314"
    },
    {
        "id": 1863,
        "title": "ABMM: Arabic BERT-Mini Model for Hate-Speech Detection on Social Media",
        "authors": "Malik Almaliki, Abdulqader M. Almars, Ibrahim Gad, El-Sayed Atlam",
        "published": "2023-2-20",
        "citations": 13,
        "abstract": "Hate speech towards a group or an individual based on their perceived identity, such as ethnicity, religion, or nationality, is widely and rapidly spreading on social media platforms. This causes harmful impacts on users of these platforms and the quality of online shared content. Fortunately, researchers have developed different machine learning algorithms to automatically detect hate speech on social media platforms. However, most of these algorithms focus on the detection of hate speech that appears in English. There is a lack of studies on the detection of hate speech in Arabic due to the language’s complex nature. This paper aims to address this issue by proposing an effective approach for detecting Arabic hate speech on social media platforms, namely Twitter. Therefore, this paper introduces the Arabic BERT-Mini Model (ABMM) to identify hate speech on social media. More specifically, the bidirectional encoder representations from transformers (BERT) model was employed to analyze data collected from Twitter and classify the results into three categories: normal, abuse, and hate speech. In order to evaluate our model and state-of-the-art approaches, we conducted a series of experiments on Twitter data. In comparison with previous works on Arabic hate-speech detection, the ABMM model shows very promising results with an accuracy score of 0.986 compared to the other models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12041048"
    },
    {
        "id": 1864,
        "title": "BERT-Based Prediction Model of Management Sales Forecast Error Using Japanese Firms' Earnings Meeting Transcripts",
        "authors": "Siya Bao, Yiqun Jin",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccnc51664.2024.10454661"
    },
    {
        "id": 1865,
        "title": "Chinese grammatical error correction based on the BERT-BiLSTM-CRF model",
        "authors": "Fangfang Gu, Zhuo Wang",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2675154"
    },
    {
        "id": 1866,
        "title": "An analysis of BERT-based model for Berkshire stock performance prediction using Warren Buffet's letters",
        "authors": "Geyang Yu",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "The objective of this study is to discover and validate eective Bidirectional Encoder Representations from Transformers (BERT)-based models for stock market prediction of Berkshire Hathaway. The stock market is full of uncertainty and dynamism and its prediction has always been a critical challenge in the nancial domain. Therefore, accurate predictions of market trends are important for making investment decisions and risk management.  The primary approach involves sentiment analysis of reviews on market performance. This work selects Warren Buetts annual letters to investors and the year-by-year stock market performance of the Berkshire Hathway as the dataset. This work leverages three BERT-based models which are BERT-Gated Recurrent Units (BERT-GRU) model, BERT-Long short-term memory (BERT-LSTM) model, and BERT-Multi-Head Attention model to analyse the Buetts annual letters and predict the Berkshire Hathways stock price changes. After conducting experiments, it could be concluded that all three models have a certain degree of predictive capability, with the BERT-Multi-Head Attention model demonstrating the best predictive performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/52/20241232"
    },
    {
        "id": 1867,
        "title": "A flexible BERT model enabling width- and depth-dynamic inference",
        "authors": "Ting Hu, Christoph Meinel, Haojin Yang",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csl.2024.101646"
    },
    {
        "id": 1868,
        "title": "“LEVER” - Low-Cost Electric Vehicle Charging Robot: Development and Testing",
        "authors": "Bryan Cochran, Zulfiqar Zaidi, Bert Bras",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "Abstract\nThis article describes the design and development of an autonomous Electric Vehicle (EV) charging system. An autonomous EV charger can extend the effective range of autonomous EVs by enabling them to recharge without human intervention. The motivation behind the development of the system was to reduce costs compared to other autonomous EV chargers. As such, the system was developed using consumer grade IoT hardware instead of commercial robotic systems and controllers. The system employs a decentralized control architecture with Robot Operation System (ROS) as its backbone. This control architecture utilizes a Raspberry Pi single-board computer as the primary controller, a series of Arduino microcontrollers to interface with motion control, and a Nvidia Jetson vision processors to execute the necessary computation to allow the entire system to operate without the need of a dedicated computer or PLC. The control algorithm utilizes vision feedback and compliant mechanisms to compensate for the limited computational capabilities of the individual microcontrollers. The system was developed in a laboratory environment and once the system operated as intended, the system was tested on a production Plug-in Hybrid Electric Vehicle (PHEV). Experiments were conducted to improve the control algorithm accuracy and decrease operating time. Overall, the system demonstrates the feasibility of using low-cost IoT hardware to develop an autonomous EV charging system, reducing the barrier to entry for the widespread adoption of autonomous EVs. The system represents an important step towards the goal of achieving sustainable and efficient transportation and opens up new possibilities for the future of electric vehicles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/detc2023-117008"
    },
    {
        "id": 1869,
        "title": "Simple closed geodesics in dimensions $$\\ge 3$$",
        "authors": "Hans-Bert Rademacher",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractWe show that for a generic Riemannian or reversible Finsler metric on a compact differentiable manifold M of dimension at least three all closed geodesics are simple and do not intersect each other. Using results by Contreras (Ann Math 2(172):761–808, 2010; in: Proceedings of International Congress Mathematicians (ICM 2010) Hyderabad, India, pp 1729–1739, 2011) this shows that for a generic Riemannian metric on a compact and simply-connected manifold all closed geodesics are simple and the number N(t) of geometrically distinct closed geodesics of length $$\\le t$$\n\n≤\nt\n\n grows exponentially.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11784-023-01092-6"
    },
    {
        "id": 1870,
        "title": "Fine-tuning BERT for Persian Patent Classification: A Dataset and Model Exploration",
        "authors": "Mohammadreza Shaghouzi, Babak Teimourpour, Mohammad Ali Soltanshahi",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aisp61396.2024.10475235"
    },
    {
        "id": 1871,
        "title": "Emotion recognition in Hindi text using multilingual BERT transformer",
        "authors": "Tapesh Kumar, Mehul Mahrishi, Girish Sharma",
        "published": "2023-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-15150-1"
    },
    {
        "id": 1872,
        "title": "False Alarm Reduction Method for Weakness Static Analysis Using BERT Model",
        "authors": "Dinh Huong Nguyen, Aria Seo, Nnubia Pascal Nnamdi, Yunsik Son",
        "published": "2023-3-9",
        "citations": 2,
        "abstract": "In the era of the fourth Industrial Revolution, software has recently been applied in many fields. As the size and complexity of software increase, security attack problems continue to arise owing to potential software defects, resulting in significant social losses. To reduce software defects, a secure software development life cycle (SDLC) should be systematically developed and managed. In particular, a software weakness analyzer that uses a static analysis tool to check software weaknesses at the time of development is a very effective tool for solving software weaknesses. However, because numerous false alarms can be reported even when they are not real weaknesses, programmers and reviewers must review them, resulting in a decrease in the productivity of development. In this study, we present a system that uses the BERT model to determine the reliability of the weakness analysis results generated by the static analysis tool and to reduce false alarms by reclassifying the derived results into a decision tree model. Thus, it is possible to maintain the advantages of static analysis tools and increase productivity by reducing the cost of program development and the review process.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13063502"
    },
    {
        "id": 1873,
        "title": "Named entity recognition of military equipment based on BERT-BILSTM-CRF model",
        "authors": "Liping Yang, Qiqing Fang, Peng Zhang",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2668327"
    },
    {
        "id": 1874,
        "title": "A Natural Language Processing Model on BERT and YAKE Technique for Keyword Extraction on Sustainability Reports",
        "authors": "Akriti Gupta, Aman Chadha, Vijaishri Tewari",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3352742"
    },
    {
        "id": 1875,
        "title": "Deep Sentiment Analysis: Exploring Emotional Trends in Architectural Decoration Language Using the BERT Model",
        "authors": "Li Pengyu, Zhou Zhenkun, Zhu Yajing",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceace60673.2023.10441893"
    },
    {
        "id": 1876,
        "title": "A BERT-Span model for Chinese named entity recognition in rehabilitation medicine",
        "authors": "Jinhong Zhong, Zhanxiang Xuan, Kang Wang, Zhou Cheng",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "\nBackground\nDue to various factors such as the increasing aging of the population and the upgrading of people’s health consumption needs, the demand group for rehabilitation medical care is expanding. Currently, China’s rehabilitation medical care encounters several challenges, such as inadequate awareness and a scarcity of skilled professionals. Enhancing public awareness about rehabilitation and improving the quality of rehabilitation services are particularly crucial. Named entity recognition is an essential first step in information processing as it enables the automated extraction of rehabilitation medical entities. These entities play a crucial role in subsequent tasks, including information decision systems and the construction of medical knowledge graphs.\n\n\nMethods\nIn order to accomplish this objective, we construct the BERT-Span model to complete the Chinese rehabilitation medicine named entity recognition task. First, we collect rehabilitation information from multiple sources to build a corpus in the field of rehabilitation medicine, and fine-tune Bidirectional Encoder Representation from Transformers (BERT) with the rehabilitation medicine corpus. For the rehabilitation medicine corpus, we use BERT to extract the feature vectors of rehabilitation medicine entities in the text, and use the span model to complete the annotation of rehabilitation medicine entities.\n\n\nResult\nCompared to existing baseline models, our model achieved the highest F1 value for the named entity recognition task in the rehabilitation medicine corpus. The experimental results demonstrate that our method outperforms in recognizing both long medical entities and nested medical entities in rehabilitation medical texts.\n\n\nConclusion\nThe BERT-Span model can effectively identify and extract entity knowledge in the field of rehabilitation medicine in China, which supports the construction of the knowledge graph of rehabilitation medicine and the development of the decision-making system of rehabilitation medicine.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.7717/peerj-cs.1535"
    },
    {
        "id": 1877,
        "title": "Chinese Medical Short Text Matching Model Based on Fine-Tuning BERT-Attention-BiLSTM",
        "authors": "Xuesong Hu, Huajun Zhang, Youjun Sun",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icis57766.2023.10210224"
    },
    {
        "id": 1878,
        "title": "Automating Model Comparison in Factor Graphs",
        "authors": "Bart van Erp, Wouter W. L. Nuijten, Thijs van de Laar, Bert de Vries",
        "published": "2023-7-29",
        "citations": 0,
        "abstract": "Bayesian state and parameter estimation are automated effectively in a variety of probabilistic programming languages. The process of model comparison on the other hand, which still requires error-prone and time-consuming manual derivations, is often overlooked despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/e25081138"
    },
    {
        "id": 1879,
        "title": "Topic detection based on BERT and seed LDA clustering model",
        "authors": "Jing Wu, Bicheng Li, Qilong Liu",
        "published": "2023-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3594409.3594418"
    },
    {
        "id": 1880,
        "title": "Studi Empiris Model BERT dan DistilBERT Analisis Sentimen pada Pemilihan Presiden Indonesia",
        "authors": " Mahira Putri, Taufik Edy Sutanto, Suma Inna",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "Peningkatan jumlah pengguna media sosial di Indonesia sejak tahun 2014 menyebabkan data yang dihasilkan semakin besar dan kompleks, sehingga komputasi yang  diperlukan untuk mengolahnya juga semakin besar. Untuk melakukan komputasi pada data yang besar diperlukan model yang kompatibel, efektif, dan efisien. Penelitian ini adalah kajian numerik dari dua model terbaik Deep Learning saat paper ini ditulis, yaitu BERT dan DistilBERT pada kasus analisis sentimen menggunakan ratusan ribu tweet terkait pemilihan presiden Indonesia tahun 2014 dan 2019. Analisis yang dilakukan meliputi waktu eksekusi dan konsumsi memori. Pada model dengan nilai hyperparameter optimal, tercatat bahwa DistilBERT melakukan proses pelatihan dan prediksi 84% lebih cepat dengan penggunaan memori GPU 79% lebih efisien dengan nilai akurasi tidak terpaut jauh, yaitu 0.89 dan 0.85 untuk BERT dan DistilBERT. Hasil kajian ini dapat digunakan untuk memperkirakan besarnya sumberdaya komputasi atau biaya yang dibutuhkan ketika menggunakan model BERT atau DistilBERT pada data yang besar.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33022/ijcs.v12i5.3445"
    },
    {
        "id": 1881,
        "title": "OpExBERT: Opinion EXTRACTION and Classification of Reviews using BERT Model",
        "authors": "Ankur Ratmele, Ramesh Thakur, Archana Thakur",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14445/22315381/ijett-v71i11p222"
    },
    {
        "id": 1882,
        "title": "Relationship Extraction of Bushing Failure from Chinese Corpus Based on BERT-FC Model",
        "authors": "Gao Shuo-jie, Zhang Yu-fang, Yuan Zhi-kang",
        "published": "2023-12-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/apet59977.2023.10489084"
    },
    {
        "id": 1883,
        "title": "Retracted: Parameter Optimization of Educational Network Ecosystem Based on BERT Deep Learning Model",
        "authors": "",
        "published": "2023-8-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9790809"
    },
    {
        "id": 1884,
        "title": "MTBERT-Attention: An Explainable BERT Model based on Multi-Task Learning for Cognitive Text Classification",
        "authors": "Hanane Sebbaq, Nour-eddine El Faddouli",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.sciaf.2023.e01799"
    },
    {
        "id": 1885,
        "title": "Research on Text Sentiment Analysis of Movie Reviews Based on BERT Model",
        "authors": "Deqing Zhang, Cuoling Zhang",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3594315.3594365"
    },
    {
        "id": 1886,
        "title": "Text classification by CEFR levels using machine learning methods and BERT language model",
        "authors": "Nadezhda S. Lagutina, Ksenia V. Lagutina, Anastasya M. Brederman, Natalia N. Kasatkina",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "This paper presents a study of the problem of automatic classification of short coherent texts (essays) in English according to the levels of the international CEFR scale. Determining the level of text in natural language is an important component of assessing students knowledge, including checking open tasks in e-learning systems. To solve this problem, vector text models were considered based on stylometric numerical features of the character, word, sentence structure levels. The classification of the obtained vectors was carried out by standard machine learning classifiers. The article presents the results of the three most successful ones: Support Vector Classifier, Stochastic Gradient Descent Classifier, LogisticRegression. Precision, recall and F-score served as quality measures. Two open text corpora, CEFR Levelled English Texts and BEA-2019, were chosen for the experiments. The best classification results for six CEFR levels and sublevels from A1 to C2 were shown by the Support Vector Classifier with F-score 67 % for the CEFR Levelled English Texts. This approach was compared with the application of the BERT language model (six different variants). The best model, bert-base-cased, provided the F-score value of 69 %. The analysis of classification errors showed that most of them are between neighboring levels, which is quite understandable from the point of view of the domain. In addition, the quality of classification strongly depended on the text corpus, that demonstrated a significant difference in F-scores during application of the same text models for different corpora. In general, the obtained results showed the effectiveness of automatic text level detection and the possibility of its practical application.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18255/1818-1015-2023-3-202-213"
    },
    {
        "id": 1887,
        "title": "EnvText: A Chinese text mining tool for environmental domain with advanced BERT model",
        "authors": "Huaibin Bi, Bing Li, Yong Qiu, Miao Change",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.simpa.2023.100559"
    },
    {
        "id": 1888,
        "title": "Analyzing film and drama reviews: Distinguishing trolls from genuine audience feedback based on the BERT model",
        "authors": "Jiabei He",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "With the expanding influence of the Internet, an increasing number of individuals rely on viewer reviews to make informed decisions about whether to watch a movie or TV series. However, the prevalence of manipulated or \"navy\" reviews, employed by companies to boost their products' reputation, has created a significant challenge. While numerous studies have dissected film and drama reviews, a notable gap exists in discerning genuine audience feedback from deceptive ones. This article's research focus is on evaluating the model's capacity to effectively differentiate between authentic audience comments and navy reviews and delving into the complexities encountered when the model assesses comments, as well as highlighting the disparities between model-generated judgments and human assessments. This article first collects a large amount of different types of comment data, annotates these data, and then uses these data to train and fine tune the BERT model. Finally, the results are obtained and analyzed to determine the reasons. This article found that the accuracy rate of the model's judgment comments is around 71.08%, which is more accurate and stable. However, there are still some issues when judging comments with emojis and emoticons, and certain data is needed to support the judgment of comments for different movies or dramas. There are also certain issues with the dataset, as the data is manually annotated, and the annotation of the dataset itself may also be influenced by the annotator, which may lead to inaccurate judgments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/53/20241376"
    },
    {
        "id": 1889,
        "title": "Research on fusion model of BERT and CNN-BiLSTM for short text classification",
        "authors": "Chao Yang, Xiaotian Wang, Mengyu Li, Ji Li",
        "published": "2023-4-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135222"
    },
    {
        "id": 1890,
        "title": "NP-BERT: A Two-Staged BERT Based Nucleosome Positioning Prediction Architecture for Multiple Species",
        "authors": "Ahtisham Fazeel, Areeb Agha, Andreas Dengel, Sheraz Ahmed",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011679200003414"
    },
    {
        "id": 1891,
        "title": "BERT-enhanced sentiment analysis for personalized e-commerce recommendations",
        "authors": "Ikram Karabila, Nossayba Darraz, Anas EL-Ansari, Nabil Alami, Mostafa EL Mallahi",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17689-5"
    },
    {
        "id": 1892,
        "title": "Automated Poetry Scoring Using BERT with Multi-Scale Poetry Representation",
        "authors": "Kristin Schuster, Selin Ahipasaoglu, Mingzhi Gao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijista.2023.10056521"
    },
    {
        "id": 1893,
        "title": "Liquid Biopsy in Early-Stage Lung Cancer: Current and Future Clinical Applications",
        "authors": "Olivia Vandekerckhove, Kristof Cuppens, Karin Pat, Bert Du Pont, Guy Froyen, Brigitte Maes",
        "published": "2023-5-10",
        "citations": 2,
        "abstract": "Lung cancer remains the leading cause of cancer death worldwide, with the majority of cases diagnosed in an advanced stage. Early-stage disease non-small cell lung cancer (NSCLC) has a better outcome, nevertheless the 5-year survival rates drop from 60% for stage IIA to 36% for stage IIIA disease. Early detection and optimized perioperative systemic treatment are frontrunner strategies to reduce this burden. The rapid advancements in molecular diagnostics as well as the growing availability of targeted therapies call for the most efficient detection of actionable biomarkers. Liquid biopsies have already proven their added value in the management of advanced NSCLC but can also optimize patient care in early-stage NSCLC. In addition to having known diagnostic benefits of speed, accessibility, and enhanced biomarker detection compared to tissue biopsy, liquid biopsy could be implemented for screening, diagnostic, and prognostic purposes. Furthermore, liquid biopsy can optimize therapeutic management by overcoming the issue of tumor heterogeneity, monitoring tumor burden, and detecting minimal residual disease (MRD), i.e., the presence of tumor-specific ctDNA, post-operatively. The latter is strongly prognostic and is likely to become a guidance in the postsurgical management. In this review, we present the current evidence on the clinical utility of liquid biopsy in early-stage lung cancer, discuss a selection of key trials, and suggest future applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cancers15102702"
    },
    {
        "id": 1894,
        "title": "BERT‐TriF: An inductive short text classification model for power equipment defect records",
        "authors": "Zhenhao Ye, Bingyan Guo, Donglian Qi",
        "published": "2023-10",
        "citations": 1,
        "abstract": "AbstractThe descriptions of power equipment defect records are often characterized by colloquial short texts. Standardized classification of a large number of colloquial defect descriptions has laid a solid foundation for building a power equipment knowledge graph and improving the level of intelligence in the field of power inspection. Using deep learning and natural language processing technology, this paper proposes a text classification model for power equipment defect records named Bidirectional Encoder Representations from Transformers with Family Feature Fusion (BERT‐TriF). The model firstly leverages BERT to semantically represent the input text. To extract family history as well as text implicit information, we creatively propose a family feature fusion algorithm for training. An improved multi‐head attention mechanism is developed subsequently to enhance text semantic category features and strengthen the learning ability of the model. By comparing BERT‐TriF and baseline models such as TextCNN, TextRNN, and fastText on the specified and generic text dataset, the experimental results demonstrate that it has better performance, robustness, and universality for short text classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/eng2.12671"
    },
    {
        "id": 1895,
        "title": "Named Entities Based on the BERT-BILSTM-ACRF Model Recognition Research",
        "authors": "Jingdong Wang, Yongjia Guo",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3639233.3639347"
    },
    {
        "id": 1896,
        "title": "Exploring the interpretability of the BERT model for semantic similarity",
        "authors": "Diana Anahí Ledesma Roque, Olga Kolesnikova, Ricardo Menchaca Méndez",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "This study addresses the issue of semantic similarity in sentences using the BERT model through various aggregation techniques, such as max-pooling, mean-pooling, and an LSTM network applied to the output of the BERT model. Subsequently, the linguistic interpretability of the BERT-Base transformer model is analyzed through the unsupervised learning approach, specifically through dimensionality reduction using autoencoders and clustering algorithms, utilizing the representation of the classification token CLS. The results highlight that the CLS classification token achieves better abstractions than the proposed methods. In terms of interpretability, it is observed that sequence length is relevant in the early layers, with a gradual decrease across the layers. Additionally, attention to semantic similarity is concentrated in the intermediate and upper layers, especially in layers 6, 8, 9, and 10. All these findings were obtained by addressing the semantic similarity task using the STS-Benchmark dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/jifs-219359"
    },
    {
        "id": 1897,
        "title": "A Design of Automatic Reference Paper Collection System Using Selenium and Bert Model",
        "authors": "Inzali Naing, Nobuo Funabiki, Khaing Hsu Wai, Soe Thandar Aung",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315512"
    },
    {
        "id": 1898,
        "title": "Profiling fake news spreaders on Twitter using BERT and Naive Bayes model",
        "authors": "Yutong Sun, Hui Ning",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3011766"
    },
    {
        "id": 1899,
        "title": "Image Generation Based on Text Using BERT And GAN Model",
        "authors": "Mallaiahgari Rohith, L. Pallavi, Kogila Shirisha, Munukoti Sanjay, V. Sathya Priya",
        "published": "2023-4-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cictn57981.2023.10141495"
    },
    {
        "id": 1900,
        "title": "A three-tier BERT based transformer framework for detecting and classifying skin cancer with HSCGS algorithm",
        "authors": "Joseph George, Anne Koteswara Rao",
        "published": "2023-11-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17590-1"
    },
    {
        "id": 1901,
        "title": "Multi-Attribute BERT for Preferences Completion in Multi-Criteria Recommender System",
        "authors": "Rita Rismala, Nur Ulfa Maulidevi, Kridanto Surendro",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3587828.3587875"
    },
    {
        "id": 1902,
        "title": "Sentiment Analysis of Text Reviews Using Lexicon-Enhanced Bert Embedding (LeBERT) Model with Convolutional Neural Network",
        "authors": "James Mutinda, Waweru Mwangi, George Okeyo",
        "published": "2023-1-21",
        "citations": 26,
        "abstract": "Sentiment analysis has become an important area of research in natural language processing. This technique has a wide range of applications, such as comprehending user preferences in ecommerce feedback portals, politics, and in governance. However, accurate sentiment analysis requires robust text representation techniques that can convert words into precise vectors that represent the input text. There are two categories of text representation techniques: lexicon-based techniques and machine learning-based techniques. From research, both techniques have limitations. For instance, pre-trained word embeddings, such as Word2Vec, Glove, and bidirectional encoder representations from transformers (BERT), generate vectors by considering word distances, similarities, and occurrences ignoring other aspects such as word sentiment orientation. Aiming at such limitations, this paper presents a sentiment classification model (named LeBERT) combining sentiment lexicon, N-grams, BERT, and CNN. In the model, sentiment lexicon, N-grams, and BERT are used to vectorize words selected from a section of the input text. CNN is used as the deep neural network classifier for feature mapping and giving the output sentiment class. The proposed model is evaluated on three public datasets, namely, Amazon products’ reviews, Imbd movies’ reviews, and Yelp restaurants’ reviews datasets. Accuracy, precision, and F-measure are used as the model performance metrics. The experimental results indicate that the proposed LeBERT model outperforms the existing state-of-the-art models, with a F-measure score of 88.73% in binary sentiment classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13031445"
    },
    {
        "id": 1903,
        "title": "Task Sequence Filling for Multi-function Radar via BERT Model",
        "authors": "Changxin Wu, Xiangqin Hu, Hongyu Chen, Lidong Zhang, Xianxiang Yu, Guolong Cui",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsip57908.2023.10271052"
    },
    {
        "id": 1904,
        "title": "A semi-supervised short text sentiment classification method based on improved Bert model from unlabelled data",
        "authors": "Haochen Zou, Zitao Wang",
        "published": "2023-3-15",
        "citations": 4,
        "abstract": "AbstractShort text information has considerable commercial value and immeasurable social value. Natural language processing and short text sentiment analysis technology can organize and analyze short text information on the Internet. Natural language processing tasks such as sentiment classification have achieved satisfactory performance under a supervised learning framework. However, traditional supervised learning relies on large-scale and high-quality manual labels and obtaining high-quality label data costs a lot. Therefore, the strong dependence on label data hinders the application of the deep learning model to a large extent, which is the bottleneck of supervised learning. At the same time, short text datasets such as product reviews have an imbalance in the distribution of data samples. To solve the above problems, this paper proposes a method to predict label data according to semi-supervised learning mode and implements the MixMatchNL data enhancement method. Meanwhile, the Bert pre-training model is updated. The cross-entropy loss function in the model is improved to the Focal Loss function to alleviate the data imbalance in short text datasets. Experimental results based on public datasets indicate the proposed model has improved the accuracy of short text sentiment recognition compared with the previous update and other state-of-the-art models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40537-023-00710-x"
    }
]