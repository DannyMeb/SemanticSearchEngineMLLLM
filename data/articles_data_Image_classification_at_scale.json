[
    {
        "id": 27105,
        "title": "Multi-Scale Feature Aggregation Based Multiple Instance Learning for Pathological Image Classification",
        "authors": "Takeshi Yoshida, Kazuki Uehara, Hidenori Sakanashi, Hirokazu Nosato, Masahiro Murakawa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011615200003411"
    },
    {
        "id": 27106,
        "title": "Multi-scale Feature Modification Network for FewShot Image Classification",
        "authors": "Yuxiang Chen, Zongqing Lu",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424955"
    },
    {
        "id": 27107,
        "title": "Machine Learning. Correlational Convolution Method for Image Classification",
        "authors": "Valery Zenkov",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mlsd58227.2023.10303841"
    },
    {
        "id": 27108,
        "title": "An Efficient Convolutional Multi-Scale Vision Transformer for Image Classification",
        "authors": "Ji Zhang, Zhihao Chen, Yiyuan Ge, Mingxin Yu",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424909"
    },
    {
        "id": 27109,
        "title": "Large-Scale Insect Pest Image Classification",
        "authors": "Thanh-Nghi Doan",
        "published": "2023",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12720/jait.14.2.328-341"
    },
    {
        "id": 27110,
        "title": "MSFF: Multi-Scale feature fusion for fine-grained image classification",
        "authors": "",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060215"
    },
    {
        "id": 27111,
        "title": "A two‐stage substation equipment classification method based on dual‐scale attention",
        "authors": "Yiyang Yao, Xue Wang, Guoqing Zhou, Qing Wang",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "AbstractAccurate classification of substation equipment images remains challenging due to various factors such as unexpected illumination, viewing angles, scale variations, shadows, surface contaminants, and different elements sharing similar appearances. This paper presents a novel two‐stage substation equipment classification method based on dual‐scale attention. Leveraging the region proposal technique from Faster‐regions with CNN features (RCNN), the input images are initially decomposed into multiple scales to capture latent features. A dual‐scale attention module is introduced to enhance the precision of feature extraction. Furthermore, a two‐stage network is proposed to address the challenge of classifying closely similar substation equipment. A multi‐layer perceptron performs a coarse classification to categorize the equipment into broad categories. Then, a lightweight classifier is employed for fine‐grained subclassification, further distinguishing equipment within the same broad category. To mitigate the issue of limited training data, a specialized dataset is collected and annotated for the substation equipment classification. Experimental results demonstrate that the proposed method achieves remarkable accuracy, recall, and F1‐score surpassing 0.91, outperforming mainstream approaches in terms of recall and F1 scores. Ablation experiments further validate the significant contributions of both the dual‐scale attention and the two‐stage classification module in improving the overall performance of the classification network.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.13091"
    },
    {
        "id": 27112,
        "title": "CAME: Convolution and Attention Construct Multi-Scale Neural Network Efficiently for Medical Image Classification",
        "authors": "Jiuqiang Li",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394558"
    },
    {
        "id": 27113,
        "title": "Scale-Aware Graph Convolutional Network for Fine-Grained Image Classification",
        "authors": "Dongmei Chen, Wei Song",
        "published": "2023-6-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icis57766.2023.10311347"
    },
    {
        "id": 27114,
        "title": "Multi-Scale Spatial-Spectral Feature Extraction Based on Dilated Convolution for Hyperspectral Image Classification",
        "authors": "Zhen Ye, Jie Wang, Lin Bai",
        "published": "2023-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3582649.3582681"
    },
    {
        "id": 27115,
        "title": "Multi-Scale Recurrent Neural Networks for Medical Image Classification",
        "authors": "Parag Agarwal, M N Nachappa, Chandra Kant Gautam",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470694"
    },
    {
        "id": 27116,
        "title": "Nonparametric System for Automatic Classification of Large-Scale Statistical Data",
        "authors": "A. V. Lapko, V. A. Lapko, V. P. Tuboltsev",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1134/s1054661823030252"
    },
    {
        "id": 27117,
        "title": "A multi-scale channel interactive attention network for hyperspectral image classification",
        "authors": "Xianhao Zhang, Qinghua He, Miaomiao Liang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eiect60552.2023.10442514"
    },
    {
        "id": 27118,
        "title": "Multi-label image classification model based on multi-scale semantic attention and graph attention network",
        "authors": "Lu Jiang, JiHua Ye, ShunJie Xiao, Yi Zong, AiWen Jiang",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005199"
    },
    {
        "id": 27119,
        "title": "Superpixel-based multi-scale multi-instance learning for hyperspectral image classification",
        "authors": "Shiluo Huang, Zheng Liu, Wei Jin, Ying Mu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2024.110257"
    },
    {
        "id": 27120,
        "title": "Optimal Scale Determination for Object-Based Backscatter Image Analysis in Seafloor Substrate Classification Based on Classification Uncertainty",
        "authors": "Xiaodong Shang, Li Dong, Jianhu Zhao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lgrs.2024.3352086"
    },
    {
        "id": 27121,
        "title": "Multi-scale contrastive learning method for PolSAR image classification",
        "authors": "Wenqiang Hua, Chen Wang, Nan Sun, Lin Liu",
        "published": "2024-1-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/1.jrs.18.014502"
    },
    {
        "id": 27122,
        "title": "Gabor Filter-Based Multi-Scale Dense Network Hyperspectral Remote Sensing Image Classification Technique",
        "authors": "Chaozhu Zhang, Shengrong Zhu, Dan Xue, Song Sun",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3323595"
    },
    {
        "id": 27123,
        "title": "Multi-Scale Image Graph Representation: A Novel GNN Approach for Image Classification through Scale Importance Estimation",
        "authors": "João Pedro Oliveira Batisteli, Silvio Jamil F. Guimarãe, Zenilton K. G. Patrocínio",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ism59092.2023.00015"
    },
    {
        "id": 27124,
        "title": "Spectral Segmentation Multi-Scale Feature Extraction Residual Networks for Hyperspectral Image Classification",
        "authors": "Jiamei Wang, Jiansi Ren, Yinbin Peng, Meilin Shi",
        "published": "2023-8-28",
        "citations": 1,
        "abstract": "Hyperspectral image (HSI) classification is a vital task in hyperspectral image processing and applications. Convolutional neural networks (CNN) are becoming an effective approach for categorizing hyperspectral remote sensing images as deep learning technology advances. However, traditional CNN usually uses a fixed kernel size, which limits the model’s capacity to acquire new features and affects the classification accuracy. Based on this, we developed a spectral segmentation-based multi-scale spatial feature extraction residual network (MFERN) for hyperspectral image classification. MFERN divides the input data into many non-overlapping sub-bands by spectral bands, extracts features in parallel using the multi-scale spatial feature extraction module MSFE, and adds global branches on top of this to obtain global information of the full spectral band of the image. Finally, the extracted features are fused and sent into the classifier. Our MSFE module has multiple branches with increasing ranges of the receptive field (RF), enabling multi-scale spatial information extraction at both fine- and coarse-grained levels. On the Indian Pines (IP), Salinas (SA), and Pavia University (PU) HSI datasets, we conducted extensive experiments. The experimental results show that our model has the best performance and robustness, and our proposed MFERN significantly outperforms other models in terms of classification accuracy, even with a small amount of training data.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15174219"
    },
    {
        "id": 27125,
        "title": "Image Classification Method Based on Multi-Scale Convolutional Neural Network",
        "authors": "Shaobo Du, Jing Li",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": " Traditional convolutional neural networks (CNNs) typically use fixed scale convolutional kernels for feature extraction when processing image classification tasks, while ignoring the multi-scale information present in the image. To overcome this limitation, we propose an algorithm based on multi-scale CNNs, which capture features at different levels by introducing convolutional kernels of different scales into the convolutional layer. In this study, we first designed a multi-scale convolutional layer consisting of multiple convolutional kernels of different scales to extract multi-scale features of the image. To further enhance classification performance, we introduced a multi-scale feature fusion module that can effectively fuse features of different scales and classify them through a fully connected layer. Then we conducted extensive experiments on several commonly used image classification datasets. The experimental results show that this network can not only effectively identify and locate hyperspectral image targets in different scenarios, but also reduce missed detections and false positives during the detection process. The average accuracy of the improved model has been improved, and the recognition accuracy of some small markers affected by external factors such as occlusion and lighting has also been improved. In addition, by comparing the detection effect of a single image, the progressiveness and anti-leakage ability of the improved model are proved. The image classification method based on multi-scale CNNs has broad application prospects in image recognition and feature extraction, and can provide valuable reference and reference for research in related fields. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s021812662450186x"
    },
    {
        "id": 27126,
        "title": "Privacy-preserving Deep Learning for Grey Scale Image Classification with Pixel-based Encryption",
        "authors": "Jingyi Deng, San Hu, Wenjun Fan",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cyberc58899.2023.00049"
    },
    {
        "id": 27127,
        "title": "Texture image classification using scale invariant feature transform (SIFT) method",
        "authors": "Haider S. Kaduhm, Hameed M. Abduljabbar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0129552"
    },
    {
        "id": 27128,
        "title": "Deep learning based data augmentation for large-scale mineral image recognition and classification",
        "authors": "Yang Liu, Xueyi Wang, Zelin Zhang, Fang Deng",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mineng.2023.108411"
    },
    {
        "id": 27129,
        "title": "Multi-scale high and low feature fusion attention network for intestinal image classification",
        "authors": "Sheng Li, Beibei Zhu, Xinran Guo, Shufang Ye, Jietong Ye, Yongwei Zhuang, Xiongxiong He",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11760-023-02507-0"
    },
    {
        "id": 27130,
        "title": "Creating Deep Convolutional Neural Networks for Image Classification",
        "authors": "Nabeel Siddiqui",
        "published": "2023-3-23",
        "citations": 0,
        "abstract": "\n            This lesson provides a beginner-friendly introduction to convolutional neural networks (CNNs) for image classification. The tutorial provides a conceptual understanding of how neural networks work by using Google's Teachable Machine to train a model on paintings from the ArtUK database. This lesson also demonstrates how to use Javascript to embed the model in a live website.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.46430/phen0108"
    },
    {
        "id": 27131,
        "title": "Multi-scale Diffusion Features Fusion Network For Hyperspectral Image Classification",
        "authors": "Xunyan Tan, Zhuoyi Zhao, Xiang Xu, Si Li",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acie61839.2024.00018"
    },
    {
        "id": 27132,
        "title": "Efficient Multi-Scale Attention Residual Network for Breast Cancer Histopathology Image Classification",
        "authors": "Lu Cao, Ke Pan",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3603273.3637854"
    },
    {
        "id": 27133,
        "title": "A Lightweight Multi-Scale Quadratic Separation Convolution Module for CNN Image-Classification Tasks",
        "authors": "Yunyan Wang, Peng Chen",
        "published": "2023-11-30",
        "citations": 1,
        "abstract": "Currently, most convolutional networks use standard convolution for feature extraction to pursue accuracy. However, there is potential room for improvement in terms of the number of network parameters and model speed. Therefore, this paper proposes a lightweight multi-scale quadratic separable convolution module (Mqscm). First, the module uses a multi-branch topology to maintain the sparsity of the network architecture. Second, channel separation and spatial separation methods are used to separate the convolution kernels, reduce information redundancy within the network, and improve the utilization of hardware computing resources. In the end, the module uses a variety of convolution kernels to obtain information on different scales to ensure the performance of the network. The performance comparison on three image-classification datasets shows that, compared with standard convolution, the Mqscm module reduces computational effort by approximately 44.5% and the model training speed improves by a range of 14.93% to 35.41%, maintaining performance levels comparable to those of deep convolution. In addition, compared with ResNet-50, the pure convolution network MqscmNet reduces the parameters by about 59.5%, saves the training time by about 29.7%, and improves the accuracy by 0.59%. Experimental results show that the Mqscm module reduces the memory burden of the model, improves efficiency, and has good performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12234839"
    },
    {
        "id": 27134,
        "title": "Few-shot Image Classification Algorithm Based on Multi-scale Attention and Residual Network",
        "authors": "Qi Wang, Huazhong Jin, Meng Yan, Lin Li",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acctcs58815.2023.00122"
    },
    {
        "id": 27135,
        "title": "Single-scale convolution wavelet feature optimization classification model based on electrocardiogram coded image",
        "authors": "Jingjing Li, Qiang Wang",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2022.104202"
    },
    {
        "id": 27136,
        "title": "Hyperspectral Image Classification Using Multi-Scale Lightweight Transformer",
        "authors": "Quan Gu, Hongkang Luan, Kaixuan Huang, Yubao Sun",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "The distinctive feature of hyperspectral images (HSIs) is their large number of spectral bands, which allows us to identify categories of ground objects by capturing discrepancies in spectral information. Convolutional neural networks (CNN) with attention modules effectively improve the classification accuracy of HSI. However, CNNs are not successful in capturing long-range spectral–spatial dependence. In recent years, Vision Transformer (VIT) has received widespread attention due to its excellent performance in acquiring long-range features. However, it requires calculating the pairwise correlation between token embeddings and has the complexity of the square of the number of tokens, which leads to an increase in the computational complexity of the network. In order to cope with this issue, this paper proposes a multi-scale spectral–spatial attention network with frequency-domain lightweight Transformer (MSA-LWFormer) for HSI classification. This method synergistically integrates CNN, attention mechanisms, and Transformer into the spectral–spatial feature extraction module and frequency-domain fused classification module. Specifically, the spectral–spatial feature extraction module employs a multi-scale 2D-CNN with multi-scale spectral attention (MS-SA) to extract the shallow spectral–spatial features and capture the long-range spectral dependence. In addition, The frequency-domain fused classification module designs a frequency-domain lightweight Transformer that employs the Fast Fourier Transform (FFT) to convert features from the spatial domain to the frequency domain, effectively extracting global information and significantly reducing the time complexity of the network. Experiments on three classic hyperspectral datasets show that MSA-LWFormer has excellent performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13050949"
    },
    {
        "id": 27137,
        "title": "Hyperspectral Image Classification Via Multi-Scale Residual Attention Network",
        "authors": "Wen Xie, Qinzhe Wu, Wen Ren, Yuzhuo Zhang",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282167"
    },
    {
        "id": 27138,
        "title": "Image Edge Enhancement for Effective Image Classification",
        "authors": "Bu Tianhao, Michalis Lazarou, Tania Stathaki",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012364900003660"
    },
    {
        "id": 27139,
        "title": "Multi-scale contrastive learning with attention for histopathology image classification",
        "authors": "Jing Wei Tan, Khoa Tuan Nguyen, Kyoungbun Lee, Won-Ki Jeong",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2653423"
    },
    {
        "id": 27140,
        "title": "Network generating network for multi-scale image classification",
        "authors": "Hang Dong, Liping Xiao, Longjian Cong, Bin Zhou",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671561"
    },
    {
        "id": 27141,
        "title": "Pruning Multi-Scale Multi-Branch Network for Small-Sample Hyperspectral Image Classification",
        "authors": "Yu Bai, Meng Xu, Lili Zhang, Yuxuan Liu",
        "published": "2023-1-29",
        "citations": 2,
        "abstract": "In recent years, the use of deep learning models has developed rapidly in the field of hyperspectral image (HSI) classification. However, most network models cannot make full use of the rich spatial-spectral features in hyperspectral images, being disadvantaged by their complex models and low classification accuracy for small-sample data. To address these problems, we present a lightweight multi-scale multi-branch hybrid convolutional network for small-sample classification. The network contains two new modules, a pruning multi-scale multi-branch block (PMSMBB) and a 3D-PMSMBB, each of which contains a multi-branch part and a pruning part. Each branch of the multi-branch part contains a convolutional kernel of different scales. In the training phase, the multi-branch part can extract rich feature information through different perceptual fields using the asymmetric convolution feature, which can effectively improve the classification accuracy of the model. To make the model lighter, pruning is introduced in the master branch of each multi-branch module, and the pruning part can remove the insignificant parameters without affecting the learning of the multi-branch part, achieving a light weight model. In the testing phase, the multi-branch part and the pruning part are jointly transformed into one convolution, without adding any extra parameters to the network. The study method was tested on three datasets: Indian Pines (IP), Pavia University (PU), and Salinas (SA). Compared with other advanced classification models, this pruning multi-scale multi-branch hybrid convolutional network (PMSMBN) had significant advantages in HSI small-sample classification. For instance, in the SA dataset with multiple crops, only 1% of the samples were selected for training, and the proposed method achieved an overall accuracy of 99.70%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12030674"
    },
    {
        "id": 27142,
        "title": "Bird image classification based on improved ResNet-152 image classification model",
        "authors": "Huitong Song",
        "published": "2024-3-29",
        "citations": 0,
        "abstract": "Bird image classification is an important research direction in the field of computer vision, and its main purpose is to automatically identify bird images through computers to achieve bird classification, recognition and monitoring. Birds are an important part of the planet's biodiversity and have an important impact on the balance of ecosystems and human survival. ResNet-152 is a deep convolutional neural network model, which is the deepest model in the ResNet family, with a depth of 152 layers. The model mainly solves the problems of gradient vanishing and gradient explosion in deep neural networks through residual learning, and improves the accuracy and generalization ability of the network. The ResNet-152 model has achieved good results in the fields of image classification, object detection and semantic segmentation. For the bird classification dataset BIRDS 525 SPECIES-IMAGE CLASSIFICATION, the ResNet-152 model was used for classification, and excellent results were obtained. The classification accuracy reached 96.5%, the accuracy reached 98.0%, the recall rate was 94.6%, the f1 score was 94.7%, and the AUC reached 95.6%. These indicators indicate that the ResNet-152 model has high classification accuracy and generalization ability for bird classification datasets. At the same time, it also shows that the ResNet-152 model has a good application prospect in processing large-scale image classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/54/20241530"
    },
    {
        "id": 27143,
        "title": "Automated biomedical image classification using multi-scale dense dilated semi-supervised u-net with cnn architecture",
        "authors": "Anoop V, Bipin PR, Anoop BK",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-16659-1"
    },
    {
        "id": 27144,
        "title": "A Recurrent Attention Multi-Scale CNN–LSTM Network Based on Hyperspectral Image Classification",
        "authors": "Xinyue Zhang, Jing Zuo",
        "published": "2023-7-30",
        "citations": 0,
        "abstract": " Since hyperspectral images contain a variety of ground objects of different scales, long-distance ground objects can fully extract the global spatial information of the image. However, most existing methods struggle to capture multi-scale information and global features simultaneously. Therefore, we combine two algorithms, MCNN and LSTM, and propose the MCNN–LSTM algorithm. The MCNN–LSTM model first performs multiple convolution operations on the image, and the result of each pooling layer is subjected to a feature fusion of the fully connected layer. Then, the results of fully connected layers at multiple scales and an attention mechanism are fused to alleviate the information redundancy of the network. Next, the results obtained by the fully connected layer are fed into the LSTM neural network, which enables the global information of the image to be captured more efficiently. In addition, to make the model meet the expected standard, a layer of loop control module is added to the fully connected layer of the LSTM network to share the weight information of multiple pieces of training. Finally, multiple public datasets are adopted for testing. The experimental results demonstrate that the proposed MCNN–LSTM model effectively extracts multi-scale features and global information of hyperspectral images, thus achieving higher classification accuracy. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0218126623501967"
    },
    {
        "id": 27145,
        "title": "Active learning based on similarity level histogram and adaptive-scale sampling for very high resolution image classification",
        "authors": "Guangfei Li, Quanxue Gao, Ming Yang, Xinbo Gao",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.012"
    },
    {
        "id": 27146,
        "title": "Multi-Scale Hybrid Spectral Network for Feature Learning and Hyperspectral Image Classification",
        "authors": "Easala Ravi Kondal, Soubhagya Sankar Barpanda",
        "published": "2023-7-13",
        "citations": 0,
        "abstract": "Hyperspectral image (HSI) classification is an important concern in remote sensing, but it is complex since few numbers of labelled training samples and the high-dimensional space with many spectral bands. Hence, it is essential to develop a more efficient neural network architecture to improve performance in the HSI classification task. Deep learning models are contemporary techniques for pixel-based hyperspectral image (HSI) classification. Deep feature extraction from both spatial and spectral channels has led to high classification accuracy. Meanwhile, the effectiveness of these spatial-spectral methods relies on the spatial dimension of every patch, and there is no feasible method to determine the best spatial dimension to take into consideration. It makes better sense to retrieve spatial properties through examination at different neighborhood scales in spatial dimensions. In this context, this paper presents a multi-scale hybrid spectral convolutional neural network (MS-HybSN) model that uses three distinct multi-scale spectral-spatial patches to pull out properties in spectral and spatial domains. The presented deep learning framework uses three patches of different sizes in spatial dimension to find these possible features. The process of Hybrid convolution operation (3D-2D) is done on each selected patch and is repeated throughout the image. To assess the effectiveness of the presented model, three benchmark datasets that are openly accessible (Pavia University, Indian Pines, and Salinas) and new Indian datasets (Ahmedabad-1 and Ahmedabad-2) are being used in experimental studies. Empirically, it has been demonstrated that the presented model succeeds over the remaining state-of-the-art approaches in terms of classification performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i7s.7026"
    },
    {
        "id": 27147,
        "title": "Token labeling-guided multi-scale medical image classification",
        "authors": "Fangyuan Yan, Bin Yan, Wei Liang, Mingtao Pei",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.12.018"
    },
    {
        "id": 27148,
        "title": "Classification and Analysis of Pistachio Species Through Neural Embedding-Based Feature Extraction and Small-Scale Machine Learning Techniques",
        "authors": "S. Sathish Kumar, AN. Sigappi, G. Arun Sampaul Thomas, Y. Harold Robinson, S. P. Raja",
        "published": "2023-3-2",
        "citations": 1,
        "abstract": " Pistachios are a tremendous source of fiber, protein, antioxidants, healthy fats, and other minerals like thiamine and vitamin B6. They may help people lose weight, lower cholesterol, and blood sugar levels, lead to better gut, eye, and blood vessel health. The two main varieties farmed and exported in Turkey are kirmizi and siirt pistachios. Understanding how to detect the type of pistachio is essential as it plays an important role in trade. In this study, it is aimed to classify these two types of pistachios and analyze the performance of the various small-scale machine learning algorithms. 2148 sample images for these two kinds of pistachios were considered for this study which includes 1232 of Kirmizi type and 916 of Siirt type. In order to evaluate the model fairly, stratified random sampling is applied on the dataset. For feature extraction, we used deep neural network-based embeddings to acquire the vector representation of images. The classification of pistachio species is then performed using a variety of small-scale machine learning algorithms29,31 that have been trained using these feature vectors. As a result of this study, the success rate obtained from Logistic Regression through features extracted from the penultimate layer of Painters network is 97.20%. The performance of the models was evaluated through Class Accuracy, Precision, Recall, F1 Score, and values of Area under the curve (AUC). The outcomes show that the method suggested in this study may quickly and precisely identify different varieties of pistachios while also meeting agricultural production needs. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467824500323"
    },
    {
        "id": 27149,
        "title": "DNAT: Multi-scale Transformer with Dilated Neighborhood Attention for Image Classification",
        "authors": "Chenfeng Jiang, Quan Zhou, Zhiyi Mo, Jing Wang, Yawen Fan, Xiaofu Wu, Suofei Zhang, Bin Kang",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wcsp58612.2023.10404854"
    },
    {
        "id": 27150,
        "title": "Hyperspectral Image Classification Based on Dual-Scale Dense Network with Efficient Channel Attentional Feature Fusion",
        "authors": "Zhongyang Shi, Ming Chen, Zhigao Wu",
        "published": "2023-7-7",
        "citations": 1,
        "abstract": "Hyperspectral images (HSIs) have abundant spectral and spatial information, which shows bright prospects in the application industry of urban–rural. Thus, HSI classification has drawn much attention from researchers. However, the spectral and spatial information-extracting method is one of the research difficulties in HSI classification tasks. To meet this tough challenge, we propose an efficient channel attentional feature fusion dense network (CA-FFDN). Our network has two structures. In the feature extraction structure, we utilized a novel bottleneck based on separable convolution (SC-bottleneck) and efficient channel attention (ECA) to simultaneously fuse spatial–spectral features from different depths, which can make full use of the dual-scale shallow and deep spatial–spectral features of the HSI and also significantly reduce the parameters. In the feature enhancement structure, we used 3D convolution and average pooling to further integrate spatial–spectral features. Many experiments on Indian Pines (IP), University of Pavia (UP), and Kennedy Space Center (KSC) datasets demonstrated that our CA-FFDN outperformed the other five state-of-the-art networks, even with small training samples. Meanwhile, our CA-FFDN achieved classification accuracies of 99.51%, 99.91%, and 99.89%, respectively, in the case where the ratio of the IP, UP, and KSC datasets was 2:1:7, 1:1:8, and 2:1:7. It provided the best classification performance with the highest accuracy, fastest convergence, and slightest training and validation loss fluctuations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12132991"
    },
    {
        "id": 27151,
        "title": "A Lightweight Convolutional Neural Network with Hierarchical Multi-Scale Feature Fusion for Image Classification",
        "authors": "Adama Dembele, Ronald Waweru Mwangi, Ananda Omutokoh Kube",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4236/jcc.2024.122011"
    },
    {
        "id": 27152,
        "title": "Multi-scale attention-based few-shot hyperspectral images classification",
        "authors": "Lanwei Ding, Guo Cao, Ling Xu, Lindiao Deng, Hao Xu, Qikun Pan, Yanfeng Shang",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2680022"
    },
    {
        "id": 27153,
        "title": "A Hybrid-Scale Feature Enhancement Network for Hyperspectral Image Classification",
        "authors": "Dongxu Liu, Tao Shao, Guanglin Qi, Meihui Li, Jianlin Zhang",
        "published": "2023-12-20",
        "citations": 1,
        "abstract": "Due to their devastating ability to extract features, convolutional neural network (CNN)-based approaches have achieved tremendous success in hyperspectral image (HSI) classification. However, previous works have been dedicated to constructing deeper or wider deep learning networks to obtain exceptional classification performance, but as the layers get deeper, the gradient disappearance problem impedes the convergence stability of network models. Additionally, previous works usually focused on utilizing fixed-scale convolutional kernels or multiple available, receptive fields with varying scales to capture features, which leads to the underutilization of information and is vulnerable to feature learning. To remedy the above issues, we propose an innovative hybrid-scale feature enhancement network (HFENet) for HSI classification. Specifically, HFENet contains two key modules: a hybrid-scale feature extraction block (HFEB) and a shuffle attention enhancement block (SAEB). HFEB is designed to excavate spectral–spatial structure information of distinct scales, types, and branches, which can augment the multiplicity of spectral–spatial features while modeling the global long-range dependencies of spectral–spatial informative features. SAEB is devised to adaptively recalibrate spectral-wise and spatial-wise feature responses to generate the purified spectral–spatial information, which effectively filters redundant information and noisy pixels and is conducive to enhancing classification performance. Compared with several sophisticated baselines, a series of experiments conducted on three public hyperspectral datasets showed that the accuracies of OA, AA, and Kappa all exceed 99%, demonstrating that the presented HFENet achieves state-of-the-art performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs16010022"
    },
    {
        "id": 27154,
        "title": "Maize leaf disease image classification based on ResNet18 image classification model",
        "authors": "Haoyan Wu",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3026727"
    },
    {
        "id": 27155,
        "title": "M2S2-FNet: Multi-scale, Multi-stream feature network with Attention mechanism for classification of breast histopathological image",
        "authors": "Suvarna D. Pujari, Meenakshi M. Pawer, Swati P. Pawar",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17717-4"
    },
    {
        "id": 27156,
        "title": "Multi-Scale Feature Attention and Transformer for Hyperspectral Image Classification",
        "authors": "Zhe Meng, Qian Yan, Feng Zhao, Miaomiao Liang",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/whispers61460.2023.10431209"
    },
    {
        "id": 27157,
        "title": "3D atrous spatial pyramid pooling based multi-scale feature fusion network for hyperspectral image classification",
        "authors": "Tianxing Zhu, Qin Liu, Lixiang Zhang",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3010238"
    },
    {
        "id": 27158,
        "title": "Multitemporal hyperspectral satellite image analysis and classification using fast scale invariant feature transform and deep learning neural network classifier",
        "authors": "G. Vinuja, N. Bharatha Devi",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12145-023-00948-2"
    },
    {
        "id": 27159,
        "title": "Quantum Federated Learning for Image Classification",
        "authors": "Leo Sünkel, Philipp Altmann, Michael Kölle, Thomas Gabor",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012421200003636"
    },
    {
        "id": 27160,
        "title": "Uncertainty-driven ensembles of multi-scale deep architectures for image classification",
        "authors": "Juan E. Arco, Andrés Ortiz, Javier Ramírez, Francisco J. Martínez-Murcia, Yu-Dong Zhang, Juan M. Górriz",
        "published": "2023-1",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2022.08.010"
    },
    {
        "id": 27161,
        "title": "Flower image classification based on an improved lightweight neural network with multi-scale feature fusion and attention mechanism",
        "authors": "Zhigao Zeng, Cheng Huang, Wenqiu Zhu, Zhiqiang Wen, Xinpan Yuan",
        "published": "2023",
        "citations": 2,
        "abstract": "<abstract><p>In order to solve the problem that deep learning-based flower image classification methods lose more feature information in the early feature extraction process, and the model takes up more storage space, a new lightweight neural network model based on multi-scale feature fusion and attention mechanism is proposed in this paper. First, the AlexNet model is chosen as the basic framework. Second, a multi-scale feature fusion module (MFFM) is used to replace the shallow single-scale convolution. MFFM, which contains three depthwise separable convolution branches with different sizes, can fuse features with different scales and reduce the feature loss caused by single-scale convolution. Third, two layers of improved Inception module are first added to enhance the extraction of deep features, and a layer of hybrid attention module is added to strengthen the focus of the model on key information at a later stage. Finally, the flower image classification is completed using a combination of global average pooling and fully connected layers. The experimental results demonstrate that our lightweight model has fewer parameters, takes up less storage space and has higher classification accuracy than the baseline model, which helps to achieve more accurate flower image recognition on mobile devices.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/mbe.2023619"
    },
    {
        "id": 27162,
        "title": "Multi-scale receptive fields: Graph attention neural network for hyperspectral image classification",
        "authors": "Yao Ding, Zhili Zhang, Xiaofeng Zhao, Danfeng Hong, Wei Cai, Nengjun Yang, Bei Wang",
        "published": "2023-8",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119858"
    },
    {
        "id": 27163,
        "title": "Multi-scale hybrid three-dimensional-two-dimensional-attention boosted convolutional neural network for hyperspectral image classification",
        "authors": "Ximeng Fu, Gaoyu Wang, Chenyu Wang, Huanhuan Xu, Huiying Li",
        "published": "2023-6-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/1.jrs.17.026513"
    },
    {
        "id": 27164,
        "title": "Multi-Tree Genetic Programming for Learning Color and Multi-Scale Features in Image Classification",
        "authors": "Qinglan Fan, Ying Bi, Bing Xue, Mengjie Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tevc.2024.3384021"
    },
    {
        "id": 27165,
        "title": "Exploiting Patch Sizes and Resolutions for Multi-Scale Deep Learning in Mammogram Image Classification",
        "authors": "Gonzalo Iñaki Quintana, Zhijin Li, Laurence Vancamberg, Mathilde Mougeot, Agnès Desolneux, Serge Muller",
        "published": "2023-4-27",
        "citations": 3,
        "abstract": "Recent progress in deep learning (DL) has revived the interest on DL-based computer aided detection or diagnosis (CAD) systems for breast cancer screening. Patch-based approaches are one of the main state-of-the-art techniques for 2D mammogram image classification, but they are intrinsically limited by the choice of patch size, as there is no unique patch size that is adapted to all lesion sizes. In addition, the impact of input image resolution on performance is not yet fully understood. In this work, we study the impact of patch size and image resolution on the classifier performance for 2D mammograms. To leverage the advantages of different patch sizes and resolutions, a multi patch-size classifier and a multi-resolution classifier are proposed. These new architectures perform multi-scale classification by combining different patch sizes and input image resolutions. The AUC is increased by 3% on the public CBIS-DDSM dataset and by 5% on an internal dataset. Compared with a baseline single patch size and single resolution classifier, our multi-scale classifier reaches an AUC of 0.809 and 0.722 in each dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bioengineering10050534"
    },
    {
        "id": 27166,
        "title": "Multi-scale Superpixel based Hierarchical Attention model for brain CT classification",
        "authors": "Xiao Song, Xiaodan Zhang, Junzhong Ji, Ying Liu",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2023.103773"
    },
    {
        "id": 27167,
        "title": "Multiscale Context Features for Geological Image Classification",
        "authors": "Matheus Todescato, Luan Garcia, Dennis Balreira, Joel Carbonera",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011981100003467"
    },
    {
        "id": 27168,
        "title": "Halo-free image enhancement through multi-scale detail sharpening and single-scale contrast stretching",
        "authors": "Xiaojuan Deng, Yinghui Zhang, Xing Zhao, Hongwei Li",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.image.2023.116923"
    },
    {
        "id": 27169,
        "title": "CT image classification of COVID-19 based on VGG16 image classification algorithm",
        "authors": "Dan Wang, Qinyu Zhao, Guoyi Xuan, Haiyan Zhang",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "This paper introduces the background of the novel coronavirus outbreak and the role of CT scanning in diagnosis, as well as the importance of automated CT image analysis system. COVID-19, a highly contagious respiratory disease caused by the novel coronavirus, has caused millions of infections and hundreds of thousands of deaths worldwide since it broke out in Wuhan, China, in early 2020. CT scan is a commonly used diagnostic means to help doctors determine the extent and location of lung lesions, but due to the large number of COVID-19 patients, doctors have a large workload, so automated CT image analysis system is needed to assist doctors in rapid diagnosis. With the continuous training process, the accuracy of the training set of automated CT image analysis system is gradually stabilized at 90%. The accuracy of the test set exceeds 85%, and the prediction effect is good. The loss of training set and test set became smaller and the training accuracy gradually improved. According to the confusion matrix, 6 CT images that should have been COVID-19 were predicted to be non-COVID-19, 57 CT images that should have been non-COVID-19 were predicted to be COVID-19, and the remaining images were predicted correctly, and the prediction effect of the model was good, which could predict the COVID-19 images more accurately. The application of automated CT image analysis system can greatly reduce the work burden of doctors, improve the efficiency and accuracy of diagnosis, and provide strong technical support for the prevention, control and treatment of COVID-19. At the same time, the continuous optimization and improvement of the system will also provide more effective technical means for future epidemic prevention and control and medical diagnosis. However, there are still some problems and challenges in the application of automated CT image analysis system. For example, there may be bias in the models training data, resulting in inaccurate model predictions. At the same time, the case data in different regions are quite different, so targeted training and optimization are needed. Therefore, it is necessary to continuously improve and optimize the system to improve its prediction effect and reliability. In conclusion, the application of automated CT image analysis system provides strong technical support for the prevention, control and treatment of the novel coronavirus pneumonia, which can quickly and accurately diagnose lung lesions and provide better treatment and care for patients. With the continuous development and improvement of the technology, the application prospect of the system will be broader and make greater contributions to the cause of human health.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2753-8818/32/20240838"
    },
    {
        "id": 27170,
        "title": "Hyperspectral image classification using improved multi-scale block local binary pattern and bi-exponential edge-preserving smoother",
        "authors": "Xiaoqing Wan, Shuanghao Chen",
        "published": "2023-12-31",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/22797254.2023.2237654"
    },
    {
        "id": 27171,
        "title": "SS-TMNet: Spatial–Spectral Transformer Network with Multi-Scale Convolution for Hyperspectral Image Classification",
        "authors": "Xiaohui Huang, Yunfei Zhou, Xiaofei Yang, Xianhong Zhu, Ke Wang",
        "published": "2023-2-22",
        "citations": 5,
        "abstract": "Hyperspectral image (HSI) classification is a significant foundation for remote sensing image analysis, widely used in biology, aerospace, and other applications. Convolution neural networks (CNNs) and attention mechanisms have shown outstanding ability in HSI classification and have been widely studied in recent years. However, the existing CNN-based and attention mechanism-based methods cannot fully use spatial–spectral information, which is not conducive to further improving HSI classification accuracy. This paper proposes a new spatial–spectral Transformer network with multi-scale convolution (SS-TMNet), which can effectively extract local and global spatial–spectral information. SS-TMNet includes two key modules, i.e., multi-scale 3D convolution projection module (MSCP) and spatial–spectral attention module (SSAM). The MSCP uses multi-scale 3D convolutions with different depths to extract the fused spatial–spectral features. The spatial–spectral attention module includes three branches: height spatial attention, width spatial attention, and spectral attention, which can extract the fusion information of spatial and spectral features. The proposed SS-TMNet was tested on three widely used HSI datasets: Pavia University, IndianPines, and Houston2013. The experimental results show that the proposed SS-TMNet is superior to the existing methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15051206"
    },
    {
        "id": 27172,
        "title": "Gradient Descent Batch Clustering for Image Classification",
        "authors": "Jae-Sam Park",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "The batch clustering algorithm for classification application requires the initial parameters and also has a drifting phenomenon for the stochastic process. The initial parameters are critical for the clustering to con-verge to the partial optimum. The drifting phenomenon in original batch clustering still has space to be improved thus to speed up the convergence based on the initial parameters. This paper proposes an unsupervised clustering method by addressing these two issues. Firstly, the estimation method for the initial parameters has been given in preliminary with a hierarchical manner of principal component analysis (PCA). The nonlinear parameters have been estimated based on a mathematical connection between PCA and clusters membership. With initial parameters, the drifting issue is addressed by combing the gradient descent and the batch clustering on an auxiliary objective to refine the initial parameters. The efficiency of the clustering process is proved based on the relationship between two quadratic functions followed by a justification. In addition, the effectiveness of the proposed method has been validated with the statistical F measure in classification application. The validation results show that the efficiency of the proposed gradient descent batch clustering has been improved significantly with trade-off to the accuracy in comparison of the original algorithms under the mean squared error (MSE) criterion.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5566/ias.2905"
    },
    {
        "id": 27173,
        "title": "CryptonDL: Encrypted Image Classification Using Deep Learning Models",
        "authors": "Adham Helbawy, Mahmoud Bahaa, Alia El Bolock",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012088000003541"
    },
    {
        "id": 27174,
        "title": "Cross channel weight sharing for image classification",
        "authors": "Nazmul Shahadat, Anthony S. Maida",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.imavis.2023.104872"
    },
    {
        "id": 27175,
        "title": "Medi-CAT: Contrastive Adversarial Training for Medical Image Classification",
        "authors": "Pervaiz Khan, Andreas Dengel, Sheraz Ahmed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012396500003636"
    },
    {
        "id": 27176,
        "title": "Inter-Scale Sure-Let Image Restoration with Deep Unrolled Image Prior",
        "authors": "Jikai Li, Shogo Muramatsu",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222110"
    },
    {
        "id": 27177,
        "title": "SDFC dataset: a large-scale benchmark dataset for hyperspectral image classification",
        "authors": "Liwei Sun, Junjie Zhang, Jia Li, Yueming Wang, Dan Zeng",
        "published": "2023-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11082-022-04399-9"
    },
    {
        "id": 27178,
        "title": "Multi-modality multi-scale cardiovascular disease subtypes classification using Raman image and medical history",
        "authors": "Bo Yu, Hechang Chen, Chengyou Jia, Hongren Zhou, Lele Cong, Xiankai Li, Jianhui Zhuang, Xianling Cong",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119965"
    },
    {
        "id": 27179,
        "title": "PFC-UNIT: Unsupervised Image-to-Image Translation with Pre-Trained Fine-Grained Classification",
        "authors": "Yu-Ying Liang, Yuan-Gen Wang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222166"
    },
    {
        "id": 27180,
        "title": "Ceramic Microscope Image Classification Based on Multi-Scale Fusion Bottleneck Structure and Chunking Attention Mechanism",
        "authors": "Zhihuang Zhuang, Xing Xu, Xuewen Xia, Yuanxiang Li, Yinglong Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.01503112"
    },
    {
        "id": 27181,
        "title": "HiFuse: Hierarchical multi-scale feature fusion network for medical image classification",
        "authors": "Xiangzuo Huo, Gang Sun, Shengwei Tian, Yan Wang, Long Yu, Jun Long, Wendong Zhang, Aolun Li",
        "published": "2024-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.105534"
    },
    {
        "id": 27182,
        "title": "Research on cotton and flax fiber identification based on multi-scale features of the texture and Gaussian process classification",
        "authors": "Junjie Wei, Hai Bi, Hong Yao, Fangxin Chen",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3001453"
    },
    {
        "id": 27183,
        "title": "Large-scale image classification with multi-perspective deep transfer learning",
        "authors": "Bin Wu, Tao Zhang, Li Mao",
        "published": "2023",
        "citations": 0,
        "abstract": "Most research efforts on image classification so far have been focused on medium-scale datasets. In addition, there exist other problems, such as difficulty in feature extraction and small sample size. In order to address above difficulties, this paper proposes a multi-perspective convolutional neural network model, which contains channel attention module and spatial attention module. The proposed modules derive attention graphs from channel dimension and spatial dimension respectively, then the input features are selectively learned according to the importance of the features. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. In addition, we give the interpretability of the model at multiple scales. Quantitative and qualitative experimental results demonstrate that the accuracy of our proposed model can be improved by up to 3.8% and outperforms the state-of-the-art methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2298/csis220714015w"
    },
    {
        "id": 27184,
        "title": "MangNet: A Muti-Scale Contextual Information Driven Deep Learning Architecture designed to perform Sunderban Landcover classification",
        "authors": "S.K. Daud Hassan, Sripama Banerjee, Debanka Pal, Sheli Sinha Chaudhuri, Khalifa Djemal, Amir Ali Feiz",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipta59101.2023.10320081"
    },
    {
        "id": 27185,
        "title": "A scale and region-enhanced decoding network for nuclei classification in histology image",
        "authors": "Shuomin Xiao, Aiping Qu, Haiqin Zhong, Penghui He",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.104626"
    },
    {
        "id": 27186,
        "title": "Human-interpretable and deep features for image privacy classification",
        "authors": "Darya Baranouskaya, Andrea Cavallaro",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222833"
    },
    {
        "id": 27187,
        "title": "Privacy Preservation in Image Classification Using Seam Doppelganger",
        "authors": "Nishitha Prakash, James Pope",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012316100003660"
    },
    {
        "id": 27188,
        "title": "ImECGnet: Cardiovascular Disease Classification from Image-Based ECG Data Using a Multibranch Convolutional Neural Network",
        "authors": "Amir Ghahremani, Christoph Lofi",
        "published": "2023-3",
        "citations": 3,
        "abstract": "Reliable Cardiovascular Disease (CVD) classification performed by a smart system can assist medical doctors in recognizing heart illnesses in patients more efficiently and effectively. Electrocardiogram (ECG) signals are an important diagnostic tool as they are already available early in the patients’ health diagnosis process and contain valuable indicators for various CVDs. Most ECG processing methods represent ECG data as a time series, often as a matrix with each row containing the measurements of a sensor lead; and/or the transforms of such time series like wavelet power spectrums. While methods processing such time-series data have been shown to work well in benchmarks, they are still highly dependent on factors like input noise and sequence length, and cannot always correlate lead data from different sensors well. In this paper, we propose to represent ECG signals incorporating all lead data plotted as a single image, an approach not yet explored by literature. We will show that such an image representation combined with our newly proposed convolutional neural network specifically designed for CVD classification can overcome the aforementioned shortcomings. The proposed (Convolutional Neural Network) CNN is designed to extract features representing both the proportional relationships of different leads to each other and the characteristics of each lead separately. Empirical validation on the publicly available PTB, MIT-BIH, and St.-Petersburg benchmark databases shows that the proposed method outperforms time seriesbased state-of-the-art approaches, yielding classification accuracy of 97.91%, 99.62%, and 98.70%, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/joig.11.1.9-14"
    },
    {
        "id": 27189,
        "title": "Improvement of Satellite Image Classification Using Attention-Based Vision Transformer",
        "authors": "Nawel Slimani, Imen Jdey, Monji Kherallah",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012298400003636"
    },
    {
        "id": 27190,
        "title": "Metric-Based Few-Shot Learning for Pollen Grain Image Classification",
        "authors": "Philipp Viertel, Matthias König, Jan Rexilius",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011727900003411"
    },
    {
        "id": 27191,
        "title": "The model architecture search system for chromosome image classification",
        "authors": "Nurullah Calik",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11760-024-03084-6"
    },
    {
        "id": 27192,
        "title": "Colorectal Image Classification Using Randomized Neural Network Descriptors",
        "authors": "Jarbas Sá Junior, André Backes",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012507200003660"
    },
    {
        "id": 27193,
        "title": "Grid-Transformer for Few-Shot Hyperspectral Image Classification",
        "authors": "Ying Guo, Mingyi He, Bin Fan",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222023"
    },
    {
        "id": 27194,
        "title": "Boosting Image Classification Accuracy Leveraging Finer Grained Labels",
        "authors": "Lei Zhu",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424766"
    },
    {
        "id": 27195,
        "title": "Dual Transformer Encoder Model for Medical Image Classification",
        "authors": "Fangyuan Yan, Bin Yan, Mingtao Pei",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222303"
    },
    {
        "id": 27196,
        "title": "Research on Multi-Labels Image Classification Based on Self-Supervised Model",
        "authors": "Xuetian Xu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipcv57033.2023.00017"
    },
    {
        "id": 27197,
        "title": "Image recognition of garbage classification based on YOLOv8",
        "authors": "K. Ye, Y. Xue",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.3300"
    },
    {
        "id": 27198,
        "title": "A Deep Learning Model for Multiclass Image Classification Using Quantum CNN",
        "authors": "Roopa Golchha, Gyanendra K. Verma",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asip58895.2023.00024"
    },
    {
        "id": 27199,
        "title": "CLASSIFICATION OF INSTRUMENT SOUNDS WITH IMAGE CLASSIFICATION ALGORITHMS",
        "authors": "Remzi GÜRFİDAN",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Classification of audio files using CNN (Convolutional Neural Network) algorithm is an important application in the field of audio processing and artificial intelligence. This process aims to automatically classify audio files into different classes and can be used in speech recognition, emotional analysis, voice-based control systems and many other applications. The aim of this study is to perform spectrum transformation of instrumental sounds and classify them using image classification algorithms. The dataset contains a total of 1500 data from five different instruments. Audio files were processed, and signal and spectrogram images of each audio file were obtained. DenseNet121, ResNet and CNN algorithms were tested in experimental studies. The most successful results belong to the CNN algorithm with 99.34%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.46519/ij3dptdi.1330052"
    },
    {
        "id": 27200,
        "title": "Deep learning-based image classification of MRI brain image",
        "authors": "Xinzhe Xie",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "This article reviews the latest research on MRI brain image classification techniques based on deep learning. Firstly, MRI brain image classification and traditional machine learning methods applied to MRI image classification were briefly introduced. Then, a review was conducted on existing MRI brain image classification methods based on deep learning. The article reviews past and recent related research and provides a detailed introduction to the application of deep learning methods in MRI brain image classification. This includes some traditional machine learning methods and research achievements in deep neural networks (DNN), convolutional neural networks (CNN), and transfer learning. The most commonly used deep learning architecture for image classification is CNN. Research has shown that deep learning methods have high accuracy and performance in MRI brain image classification and can automatically extract image features for effective classification. Therefore, deep learning methods provide doctors with more comprehensive information, help them make more accurate diagnoses and formulate treatment plans, and have broad prospects for application in MRI brain image classification. In the future, the further development of deep learning technology will achieve better auxiliary effects on diagnosis, treatment, and scientific research related to the brain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2753-8818/27/20240670"
    },
    {
        "id": 27201,
        "title": "Expoliting Confidence-Based Model Fusion for Boosting Image Classification Accuracy",
        "authors": "Xinjian Jiang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424922"
    },
    {
        "id": 27202,
        "title": "Agrinet: A Hyperspectral Image Based Precise Crop Classification Model",
        "authors": "Aditi Palit, Himanshu Dolekar, Kalidas Yeturu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012378400003660"
    },
    {
        "id": 27203,
        "title": "Simple Self-Distillation Learning for Noisy Image Classification",
        "authors": "Tenta Sasaya, Takashi Watanabe, Takashi Ida, Toshiyuki Ono",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222352"
    },
    {
        "id": 27204,
        "title": "ResNet-101 Empowered Deep Learning for Breast Cancer Ultrasound Image Classification",
        "authors": "Agnesh Yadav, Maheshkumar Kolekar, Mukesh Zope",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012377800003657"
    },
    {
        "id": 27205,
        "title": "Adaptive Adversarial Samples Based Active Learning for Medical Image Classification",
        "authors": "Siteng Ma, Yu An, Jing Wang, Aonghus Lawlor, Ruihai Dong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011622100003411"
    },
    {
        "id": 27206,
        "title": "M-AResNet: a novel multi-scale attention residual network for melting curve image classification",
        "authors": "Pengxiang Su, Xuanjing Shen, Haipeng Chen, Di Gai, Yu Liu",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-14694-6"
    },
    {
        "id": 27207,
        "title": "Task‐oriented feature hallucination for few‐shot image classification",
        "authors": "Sining Wu, Xiang Gao, Xiaopeng Hu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "AbstractData hallucination generates additional training examples for novel classes to alleviate the data scarcity problem in few‐shot learning (FSL). Existing hallucination‐based FSL methods normally train a general embedding model first by applying information extracted from base classes that have abundant data. In those methods, hallucinators are then built upon the trained embedding model to generate data for novel classes. However, these hallucination methods usually rely on general‐purpose embeddings, limiting their ability to generate task‐oriented samples for novel classes. Recent studies have shown that task‐specific embedding models, which are adapted to novel tasks, can achieve better classification performance. To improve the performance of example hallucination for tasks, a task‐oriented embedding model is used in the proposed method to perform task‐oriented generation. After the initialization, the hallucinator is finetuned by applying a task‐oriented embedding model with the guidance of a teacher–student mechanism. The proposed task‐oriented hallucination method contains two steps. An initial embedding network and an initial hallucinator are trained with a base dataset in the first step. The second step contains a pseudo‐labelling process where the base dataset is pseudo‐labelled using support data of the few‐shot task and a task‐oriented fine‐tuning process where the embedding network and hallucinator are adjusted simultaneously. Both the embedding network and the hallucinator are updated with the support set and the pseudo‐labelled base dataset using knowledge distillation. The experiments are conducted on four popular few‐shot datasets. The results demonstrate that the proposed approach outperforms state‐of‐the‐art methods with 0.8% to 4.08% increases in classification accuracy for 5‐way 5‐shot tasks. It also achieves comparable accuracy to state‐of‐the‐art methods for 5‐way 1‐shot tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.12886"
    },
    {
        "id": 27208,
        "title": "Comparison of Supervised Classification Algorithms Using a Hyperspectral Image for Land Use/Land Cover Classification",
        "authors": "Sonia Sharma Banjade, Nitant Rai, Bipana Subedi",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/ecrs2023-16702"
    },
    {
        "id": 27209,
        "title": "Multi-Scale Residual Spectral–Spatial Attention Combined with Improved Transformer for Hyperspectral Image Classification",
        "authors": "Aili Wang, Kang Zhang, Haibin Wu, Yuji Iwahori, Haisong Chen",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "Aiming to solve the problems of different spectral bands and spatial pixels contributing differently to hyperspectral image (HSI) classification, and sparse connectivity restricting the convolutional neural network to a globally dependent capture, we propose a HSI classification model combined with multi-scale residual spectral–spatial attention and an improved transformer in this paper. First, in order to efficiently highlight discriminative spectral–spatial information, we propose a multi-scale residual spectral–spatial feature extraction module that preserves the multi-scale information in a two-layer cascade structure, and the spectral–spatial features are refined by residual spectral–spatial attention for the feature-learning stage. In addition, to further capture the sequential spectral relationships, we combine the advantages of Cross-Attention and Re-Attention to alleviate computational burden and attention collapse issues, and propose the Cross-Re-Attention mechanism to achieve an improved transformer, which can efficiently alleviate the heavy memory footprint and huge computational burden of the model. The experimental results show that the overall accuracy of the proposed model in this paper can reach 98.71%, 99.33%, and 99.72% for Indiana Pines, Kennedy Space Center, and XuZhou datasets, respectively. The proposed method was verified to have high accuracy and effectiveness compared to the state-of-the-art models, which shows that the concept of the hybrid architecture opens a new window for HSI classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13061061"
    },
    {
        "id": 27210,
        "title": "ENN: Hierarchical Image Classification Ensemble Neural Network for Large-Scale Automated Detection of Potential Design Infringement",
        "authors": "Chan Jae Lee, Seong Ho Jeong, Young Yoon",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "This paper presents a two-stage hierarchical neural network using image classification and object detection algorithms as key building blocks for a system that automatically detects a potential design right infringement. This neural network is trained to return the Top-N original design right records that highly resemble the input image of a counterfeit. This work proposes an ensemble neural network (ENN), an artificial neural network model that aims to deal with a large amount of counterfeit data and design right records that are frequently added and deleted. First, we performed image classification and objection detection learning per design right using acclaimed existing models with high accuracy. The distributed models form the backbone of the ENN and yield intermediate results aggregated at a master neural network. This master neural network is a deep residual network paired with a fully connected network. This ensemble layer is trained to determine the sub-models that return the best result for a given input image of a product. In the final stage, the ENN model multiplies the inferred similarity coefficients to the weighted input vectors produced by the individual sub-models to assess the similarity between the test input image and the existing product design rights to see any sign of violation. Given 84 design rights and the sample product images taken meticulously under various conditions, our ENN model achieved average Top-1 and Top-3 accuracies of 98.409% and 99.460%, respectively. Upon introducing new design rights data, a partial update of the inference model was performed an order of magnitude faster than the single model. The ENN maintained a high level of accuracy as it was scaled out to handle more design rights. Therefore, the ENN model is expected to offer practical help to the inspectors in the field, such as customs at the border that deal with a swarm of products.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132212166"
    },
    {
        "id": 27211,
        "title": "TransMCGC: a recast vision transformer for small-scale image classification tasks",
        "authors": "Jian-Wen Xiang, Min-Rong Chen, Pei-Shan Li, Hao-Li Zou, Shi-Da Li, Jun-Jie Huang",
        "published": "2023-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-022-08067-7"
    },
    {
        "id": 27212,
        "title": "A new hierarchical algorithm based on CapsGAN for imbalanced image classification",
        "authors": "Hamed Jabbari, Nooshin Bigdeli",
        "published": "2024-1",
        "citations": 0,
        "abstract": "AbstractImbalanced image datasets consist of image datasets where there is a significant disparity in the number of samples across different classes. With imbalanced image datasets, learning algorithms often tend to be biased toward the majority class samples. This leads to poor classification of minority class samples as their training is not properly conducted. It becomes more complicated when the number of samples in the minority class is very low. In this paper, a novel hierarchical algorithm is proposed for generating new data using Capsule Generative Adversarial Networks (CapsGAN) to address the class imbalance problem in imbalanced image datasets. Unlike common GAN models, the proposed method incorporates an auxiliary CapsNet to identify high‐value images in both minority and majority classes. This identification is based on the ability to detect complex relationships between low‐level and high‐level features present in capsule networks. Furthermore, the proposed CapsGAN model is conditioned to generate minority class samples based on feature vectors of last capsule layer to achieve a more balanced image dataset. For evaluating the performance of the proposed model, an image dataset called CICS was collected and introduced. Extensive experiments were also conducted using various online image datasets from different domains, with varying numbers of classes and data sizes. The experimental results demonstrated that the proposed model can generate high‐quality samples in cases where the image dataset or the number of minority class samples is relatively small. Furthermore, the proposed model has maintained an accuracy of over 80% in an imbalanced ratio of 1:60.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.12942"
    },
    {
        "id": 27213,
        "title": "Rotational Augmentation Techniques: A New Perspective on Ensemble Learning for Image Classification",
        "authors": "Unai Munoz Aseguinolaza, Basilio Sierra, Naiara Aginako",
        "published": "2023-5-27",
        "citations": 0,
        "abstract": "The popularity of data augmentation techniques in machine learning has increased in recent years, as they enable the creation of new samples from existing datasets. Rotational augmentation, in particular, has shown great promise by revolving images and utilising them as additional data points for training. The research in this study aimed to evaluate the effectiveness of rotational augmentation techniques and different voting systems in improving image classification accuracy. To accomplish this, several image datasets were evaluated using various augmentation methods, which were employed to generate testing sets. Subsequently, voting systems were used to determine the most reliable outcome for each original data. The findings of this study suggest that rotational augmentation techniques can significantly enhance the accuracy of classification models. Additionally, the selection of a voting scheme can considerably impact the model's performance. Overall, the study found that using an ensemble-based voting system produced more accurate results than simple voting.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130909"
    },
    {
        "id": 27214,
        "title": "Research on Printmaking Image Classification and Creation Based on Convolutional Neural Network",
        "authors": "Kai Pan, Hongyan Chi",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": " As an important form of expression in modern civilization art, printmaking has a rich variety of types and a prominent sense of artistic hierarchy. Therefore, printmaking is highly favored around the world due to its unique artistic characteristics. Classifying print types through image feature elements will improve people’s understanding of print creation. Convolutional neural networks (CNNs) have good application effects in the field of image classification, so CNN is used for printmaking analysis. Considering that the classification effect of the traditional convolutional neural image classification model is easily affected by the activation function, the T-ReLU activation function is introduced. By utilizing adjustable parameters to enhance the soft saturation characteristics of the model and avoid gradient vanishing, a T-ReLU convolutional model is constructed. A better convolutional image classification model is proposed based on the T-ReLU convolutional model, taking into account the issue of subpar multi-level feature fusion in deep convolutional image classification models. Utilize normalization to analyze visual input, an eleven-layer convolutional network with residual units in the convolutional layer, and cascading thinking to fuse convolutional network defects. The performance test results showed that in the data test of different styles of artificial prints, the GT-ReLU model can obtain the best image classification accuracy, and the image classification accuracy rate is 0.978. The GT-ReLU model maintains a classification accuracy above 94.4% in the multi-dataset test classification performance test, which is higher than that of other image classification models. For the use of visual processing technology in the field of classifying prints, the research content provides good reference value. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467825500196"
    },
    {
        "id": 27215,
        "title": "Dictionary-enabled efficient training of ConvNets for image classification",
        "authors": "Usman Haider, Muhammad Hanif, Ahmar Rashid, Syed Fawad Hussain",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.imavis.2023.104718"
    },
    {
        "id": 27216,
        "title": "Image Classification Model Selector",
        "authors": " N. Arulanand,  D. Kamalraj,  B. Krishna Teja",
        "published": "2023-1-25",
        "citations": 0,
        "abstract": "Image classification is a part of computer vision, in which the digital system categorizes the entire image. Deep Learning (DL) models are widely used for image classification. However, creating DL models is resource-intensive and time-consuming, and requires extensive knowledge in the DL domain. Google Teachable Machines (GTM) is a website that outputs a trained model given the dataset, however, GTM uses only the MobileNet model and does not balance the image dataset which affects the model’s accuracy. This paper proposes a tool that automates the steps in building and training an image classification model. Using this tool does not require any extensive knowledge in DL. The tool automates the image data pre-processing steps, model building, model training, and model testing to output the best model for the given image classification dataset based on the test accuracy. The tool is tested on two datasets (each balanced and unbalanced dataset): a custom construction dataset and a Minet dataset. Both datasets are also used to train models using the GTM website. Due to the automated pre-processing steps, the average increase in the accuracy is 14.55% in the construction dataset and 3.91% in the Minet dataset. Comparing to the GTM models, the tool produced model with 8.33% more accuracy on the construction dataset and model with 14.07% more accuracy on the Minet dataset. The models trained by the proposed tool have better accuracy compared to the models obtained using GTM. Thus, using the image classification model selector facilitates the creation of an effective image classification model for the target dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36548/jiip.2022.4.007"
    },
    {
        "id": 27217,
        "title": "Image classification of electronic components based on Vision Transformer",
        "authors": "Yuhui Chen, Xiahui Chen",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424884"
    },
    {
        "id": 27218,
        "title": "Skin Lesion Classification Based on Segmented Image",
        "authors": "Aleksandra Dzieniszewska, Piotr Garbat, Ryszard Piramidowicz",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipta59101.2023.10320004"
    },
    {
        "id": 27219,
        "title": "Enabling RAW Image Classification Using Existing RGB Classifiers",
        "authors": "Rasmus Munksø, Mathias Andersen, Lau Nørgaard, Andreas Møgelmose, Thomas Moeslund",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363200003660"
    },
    {
        "id": 27220,
        "title": "USE OF IMAGE FOR CLASSIFICATION AND IDENTIFICATION OF FOREST SEEDS QUALITY",
        "authors": "Karine Pereira Lívia, Lima Baute Júlia, Maria de Oliveira Pires Raquel",
        "published": "2023-8-29",
        "citations": 0,
        "abstract": "Seeds are essential inputs in agriculture and forestry, being the viability, crucial criteria for determining their quality. Image processing and analysis methods were explored to assess seed quality. The vigor of seeds is vital for the establishment of crops, being defi ned as their ability to germinate and grow in diff erent environments, often inhospitable. Conventional methods, such as germination tests, tetrazolium and the accelerated aging test, have their limitations to determine aspects of predictions and vigor, promoting the search for non-destructive and automated approaches. Image analysis, including near-infrared spectroscopy (NIR) and hyperspectral imaging (HSI), is an eff ective tool for evaluating various seed characteristics such as size, color and shape. Equipment such as GroundEye® allows for accurate and quick analysis of seed quality. Image analysis has expanded rapidly, off ering alternative options to ensure the determination of seed quality of forest and agricultural species, with the potential to revolutionize the selection of plant varieties and increase productivity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.53661/2763-686020230000008"
    },
    {
        "id": 27221,
        "title": "A Fusion Approach for Enhanced Remote Sensing Image Classification",
        "authors": "Vian Ahmed, Khaled Jouini, Amel Tuama, Ouajdi Korbaa",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012376600003660"
    },
    {
        "id": 27222,
        "title": "Robust Long-Tailed Image Classification via Adversarial Feature Re-Calibration",
        "authors": "Jinghao Zhang, Zhenhua Feng, Yaochu Jin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012432000003660"
    },
    {
        "id": 27223,
        "title": "Noise Robust Hyperspectral Image Classification With MNF-Based Edge Preserving Features",
        "authors": "Guangyi Chen, Adam Krzyzak, Shen-en Qian",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "Hyperspectral image (HSI) classification is an important topic in remote sensing. In this paper, we improve the principal component analysis (PCA)-based edge preserving features (EPFs) for HSI classification. We select to use minimum noise fraction (MNF) instead of PCA to reduce the dimensionality of the hyperspectral data cube to be classified. We keep all the rest steps from the PCA-based EPFs for HSI classification. Since MNF can preserve fine features of a HSI data cube better than PCA, our new method can outperform PCA-EPFs for HSI classification significantly. Experimental results show that our new method performs better than the PCA-based EPFs under such noisy environment as Gaussian white noise and shot noise. In addition, our MNF+EPFs outperform the PCA+EPFs even when no noise is added to the HSI data cubes for most testing cases, which is very desirable in remote sensing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5566/ias.2928"
    },
    {
        "id": 27224,
        "title": "PolSAR Image Classification Based-On Semi-Supervised Polarimetric Feature Selection",
        "authors": "Xiayuan Huang",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10223109"
    },
    {
        "id": 27225,
        "title": "Spatial-Spectral Neural Network for High Resolution Multispectral Image Classification",
        "authors": "Roozbeh Tanha, Hassan Ghassemian",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mvip62238.2024.10491153"
    },
    {
        "id": 27226,
        "title": "Deep Active Learning Based on Saliency-Guided Data Augmentation for Image Classification",
        "authors": "Ying Liu, Yuliang Pang, Weidong Zhang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222582"
    },
    {
        "id": 27227,
        "title": "A Research on Traditional Tangka Image Classification Based on Visual Features",
        "authors": "Si Qi Gao",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10165943"
    },
    {
        "id": 27228,
        "title": "Vehicle image datasets for image classification",
        "authors": "Narong Boonsirisumpun, Emmanuel Okafor, Olarik Surinta",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dib.2024.110133"
    },
    {
        "id": 27229,
        "title": "Scale-progressive Multi-patch Network for image dehazing",
        "authors": "Dan Zhang, Jingchun Zhou, Dehuan Zhang, Pengfei Qi",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.image.2023.117023"
    },
    {
        "id": 27230,
        "title": "Product Image Representation Learning on Large Scale Noisy Datasets",
        "authors": "Aniket Joshi, Nilotpal Das, Promod Yenigalla",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222339"
    },
    {
        "id": 27231,
        "title": "Cross‐modal knowledge learning with scene text for fine‐grained image classification",
        "authors": "Li Xiong, Yingchi Mao, Zicheng Wang, Bingbing Nie, Chang Li",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "AbstractScene text in natural images carries additional semantic information to aid in image classification. Existing methods lack full consideration of the deep understanding of the text and the visual text relationship, which results in the difficult to judge the semantic accuracy and the relevance of the visual text. This paper proposes image classification based on Cross modal Knowledge Learning of Scene Text (CKLST) method. CKLST consists of three stages: cross‐modal scene text recognition, text semantic enhancement, and visual‐text feature alignment. In the first stage, multi‐attention is used to extract features layer by layer, and a self‐mask‐based iterative correction strategy is utilized to improve the scene text recognition accuracy. In the second stage, knowledge features are extracted using external knowledge and are fused with text features to enhance text semantic information. In the third stage, CKLST realizes visual‐text feature alignment across attention mechanisms with a similarity matrix, thus the correlation between images and text can be captured to improve the accuracy of the image classification tasks. On Con‐Text dataset, Crowd Activity dataset, Drink Bottle dataset, and Synth Text dataset, CKLST can perform significantly better than other baselines on fine‐grained image classification, with improvements of 3.54%, 5.37%, 3.28%, and 2.81% over the best baseline in mAP, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.13039"
    },
    {
        "id": 27232,
        "title": "Image Classification Improvement: Text-to-Image AI for Synthetic Dataset Approach",
        "authors": "Olger Zambrano, Benaoumeur Senouci",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/seaa60479.2023.00020"
    },
    {
        "id": 27233,
        "title": "A Completed Multiply Threshold Encoding Pattern for Texture Classification",
        "authors": "Bin Li, Yibing Li, Q.M.Jonathan Wu",
        "published": "2023-10-22",
        "citations": 0,
        "abstract": "The binary pattern family has drawn wide attention for texture representation due to its promising performance and simple operation. However, most binary pattern methods focus on local neighborhoods but ignore center pixels. Even if some studies introduce the center based sub-pattern to provide complementary information, extant center based sub-patterns are much weaker than other local neighborhood based sub-patterns. This severe unbalance limits the classification performance of fusion features significantly. To alleviate this problem, this paper designs a multiply threshold center pattern (MTCP) to provide a more discriminative and complementary local texture representation with a compact form. First, a multiply thresholds encoding strategy is designed to encode the center pixel that generates three 1-bit binary patterns. Second, it adopts a compact multi-pattern encoding strategy to combine them into the 3-bit MTCP. Furthermore, this paper proposes a completed multiply threshold encoding pattern by fusing the MTCP, local sign pattern, and local magnitude pattern. Comprehensive experimental evaluations on three popular texture classification benchmarks confirm that the completed multiply threshold encoding pattern achieves superior texture classification performance. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.5566/ias.2824"
    },
    {
        "id": 27234,
        "title": "Trusted multi-scale classification framework for whole slide image",
        "authors": "Ming Feng, Kele Xu, Nanhui Wu, Weiquan Huang, Yan Bai, Yin Wang, Changjian Wang, Huaimin Wang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2023.105790"
    },
    {
        "id": 27235,
        "title": "Image Stitching Based on Multi-Scale Meshes",
        "authors": "Yixuan Li, Haotian Zhao, Qi Jia, Nan Pu",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222919"
    },
    {
        "id": 27236,
        "title": "Towards Transfer Learning for Large-Scale Image Classification Using Annealing-Based Quantum Boltzmann Machines",
        "authors": "Daniëlle Schuman, Leo Sünkel, Philipp Altmann, Jonas Stein, Christoph Roch, Thomas Gabor, Claudia Linnhoff-Popien",
        "published": "2023-9-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/qce57702.2023.10182"
    },
    {
        "id": 27237,
        "title": "ViT-Based Multi-Scale Classification Using Digital Signal Processing and Image Transformation",
        "authors": "Gyu-Il Kim, Kyungyong Chung",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3389808"
    },
    {
        "id": 27238,
        "title": "DM-CNN: Dynamic Multi-scale Convolutional Neural Network with uncertainty quantification for medical image classification",
        "authors": "Qi Han, Xin Qian, Hongxiang Xu, Kepeng Wu, Lun Meng, Zicheng Qiu, Tengfei Weng, Baoping Zhou, Xianqiang Gao",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2023.107758"
    },
    {
        "id": 27239,
        "title": "Bilaterally Normalized Scale-Consistent Sinkhorn Distance for Few-Shot Image Classification",
        "authors": "Yanbin Liu, Linchao Zhu, Xiaohan Wang, Makoto Yamada, Yi Yang",
        "published": "2024",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3262351"
    },
    {
        "id": 27240,
        "title": "Building typology classification using convolutional neural networks utilizing multiple ground-level image process for city-scale rapid seismic vulnerability assessment",
        "authors": "Hafidz R. Firmansyah, Prasanti Widyasih Sarli, Andru Putra Twinanda, Devin Santoso, Iswandi Imran",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107824"
    },
    {
        "id": 27241,
        "title": "Focal Modulation Based End-to-End Multi-Label Classification for Chest X-Ray Image Classification",
        "authors": "Şaban Öztürk, Tolga Çukur",
        "published": "2023-7-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/siu59756.2023.10223975"
    },
    {
        "id": 27242,
        "title": "TMNIO:Triplet merged network with involution operators for improved few‐shot image classification",
        "authors": "Qi Lulu, Xu Ranhui, Zhao Shaojie, Zheng Mingming, Yu Weiqin",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "AbstractFew‐shot learning enables machines to learn efficiently from limited labelled data. However, existing few‐shot learning methods may perform poorly when there is a lack of sufficient samples, and may encounter problems such as domain shift or overfitting when applied to new domains or tasks. To address the issues of poor fitting and insufficient generalization ability in new domains, a new method called triplet merged network with involution operators (TMNIO) is proposed. This method employs dual encoders that extract common and distinctive features from the prototype network, thereby enhancing the model's feature extraction capability. To further improve this ability, the traditional convolutional kernels are replaced with involution operators, which not only reduce the parameter count but also enlarge the receptive field to better extract local feature information. Additionally, this method employs a two‐stage training strategy, where triplet loss is used in the first stage to train the model and enhance its robustness and generalization ability. Extensive experiments on the miniImageNet, Omniglot, and Caltech‐UCSD Birds‐200 (CUB) datasets have shown that our proposed method achieved significant improvements in both training speed and accuracy, particularly on the miniImageNet dataset, where it achieved an outstanding 10% performance improvement.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.13055"
    },
    {
        "id": 27243,
        "title": "Deep Learning-Based Magnetic Resonance Image Segmentation and Classification for Alzheimer’s Disease Diagnosis",
        "authors": "T. Manochandar, P. Kumaraguru Diderot",
        "published": "2023-8-29",
        "citations": 1,
        "abstract": " Accurate and rapid detection of Alzheimer’s disease (AD) using magnetic resonance imaging (MRI) gained considerable attention among research workers because of an increased number of current researches being driven by deep learning (DL) methods that have accomplished outstanding outcomes in variety of domains involving medical image analysis. Especially, convolution neural network (CNN) is primarily applied for the analyses of image datasets according to the capability of handling massive unstructured datasets and automatically extracting significant features. Earlier detection is dominant to the success and development interferences, and neuroimaging characterizes the potential regions for earlier diagnosis of AD. The study presents and develops a novel Deep Learning-based Magnetic Resonance Image Segmentation and Classification for AD Diagnosis (DLMRISC-ADD) model. The presented DLMRISC-ADD model mainly focuses on the segmentation of MRI images to detect AD. To accomplish this, the presented DLMRISC-ADD model follows a two-stage process, namely, skull stripping and image segmentation. At the preliminary stage, the presented DLMRISC-ADD model employs U-Net-based skull stripping approach to remove skull regions from the input MRIs. Next, in the second stage, the DLMRISC-ADD model applies QuickNAT model for MRI image segmentation, which identifies distinct parts such as white matter, gray matter, hippocampus, amygdala, and ventricles. Moreover, densely connected network (DenseNet201) feature extractor with sparse autoencoder (SAE) classifier is used for AD detection process. A brief set of simulations is implemented on ADNI dataset to demonstrate the improved performance of the DLMRISC-ADD method, and the outcomes are examined extensively. The experimental results exhibit the effectual segmentation results of the DLMRISC-ADD technique. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467825500263"
    },
    {
        "id": 27244,
        "title": "DSHFNet: Dynamic Scale Hierarchical Fusion Network Based on Multiattention for Hyperspectral Image and LiDAR Data Classification",
        "authors": "Yining Feng, Liyang Song, Lu Wang, Xianghai Wang",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tgrs.2023.3311535"
    },
    {
        "id": 27245,
        "title": "Cybersecurity Intrusion Detection with Image Classification Model Using Hilbert Curve",
        "authors": "Punyawat Jaroensiripong, Karin Sumongkayothin, Prarinya Siritanawan, Kazunori Kotani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012306100003660"
    },
    {
        "id": 27246,
        "title": "Enhancing Wireless Capsule Endoscopic Image Classification using Mayfly Algorithm with Deep Learning Approach",
        "authors": "M. Amirthalingam, R. Ponnusamy",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115113636"
    },
    {
        "id": 27247,
        "title": "Fire image detection and classification analysis based on VGG16 image processing model",
        "authors": "Fengjun Hou",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "Fire image classification technology refers to the classification and recognition of fire images through computer vision technology in order to take timely countermeasures. With the development of computer vision technology, fire image classification technology has been widely studied and applied. Deep learning techniques have achieved great success in the field of image classification, with researchers classifying and identifying fire images by using deep learning models such as convolutional neural networks. This paper introduces the research background and application of fire image classification technology, and the experimental results of detection and classification analysis of fire image based on vgg16 image processing model. The model can classify and identify fire images well and achieve good prediction effect. The accuracy of training set and test set is stable at 99%, the accuracy and accuracy of the model reach 98%, and the recall rate and F1 score reach 99%. The application of fire image classification technology is of great significance. By using computer vision technology to classify and identify fire images, it can improve the accuracy and efficiency of fire monitoring and early warning system, timely detect and control fire, and reduce the loss caused by fire. At the same time, the continuous development of deep learning technology also provides a broader space for the research and application of fire image classification technology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/48/20241529"
    },
    {
        "id": 27248,
        "title": "An optimization framework for diabetic foot ulcer image classification",
        "authors": "A. Huong, K. G. Tay, X. Ngu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.3316"
    },
    {
        "id": 27249,
        "title": "Highresolution Remote Sensing Image Classification With Limited Training Data",
        "authors": "Mehdi Ariaei, Hassan Ghassemian, Maryam Imani",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mvip62238.2024.10491181"
    },
    {
        "id": 27250,
        "title": "Wavelet-Based Frequency-Dividing Interactive CNN for Image Classification",
        "authors": "Jidong Cao, Chu He, Jiahao Pan, Qingyi Zhang, Xi Chen",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222409"
    },
    {
        "id": 27251,
        "title": "Capsule attention module-based CapsNet for hyperspectral image classification",
        "authors": "Xinsheng Zhang, Zhaohui Wang",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2679994"
    },
    {
        "id": 27252,
        "title": "Empirical Mode Decomposition Based Morphological Profile For Hyperspectral Image Classification",
        "authors": "Kosar Amiri, Maryam Imani, Hassan Ghassemian",
        "published": "2023-2-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipria59240.2023.10147181"
    },
    {
        "id": 27253,
        "title": "Underwater Image Classification",
        "authors": "",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets47288"
    },
    {
        "id": 27254,
        "title": "Subdomain alignment based open-set domain adaptation image classification",
        "authors": "Kangkang Ji, Qingliang Zhang, Songhao Zhu",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2024.104047"
    },
    {
        "id": 27255,
        "title": "Fine_Denseiganet: Automatic Medical Image Classification in Chest CT Scan Using Hybrid Deep Learning Framework",
        "authors": "Hemlata P. Sahu, Ramgopal Kashyap",
        "published": "2023-7-22",
        "citations": 39,
        "abstract": " Medical image classification is one of the most significant tasks in computer-aided diagnosis. In the era of modern healthcare, the progress of digitalized medical images has led to a crucial role in analyzing medical image analysis. Recently, accurate disease recognition from medical Computed Tomography (CT) images remains a challenging scenario which is important in rendering effective treatment to patients. The infectious COVID-19 disease is highly contagious and leads to a rapid increase in infected individuals. Some drawbacks noticed with RT-PCR kits are high false negative rate (FNR) and a shortage in the number of test kits. Hence, a Chest CT scan is introduced instead of RT-PCR which plays an important role in diagnosing and screening COVID-19 infections. However, manual examination of CT scans performed by radiologists can be time-consuming, and a manual review of each individual CT image may not be feasible in emergencies. Therefore, there is a need to perform automated COVID-19 detection with the advances in AI-based models. This work presents effective and automatic Deep Learning (DL)-based COVID-19 detection using Chest CT images. Initially, the data is gathered and pre-processed through Spatial Weighted Bilateral Filter (SWBF) to eradicate unwanted distortions. The extraction of deep features is processed using Fine_Dense Convolutional Network (Fine_DenseNet). For classification, the Softmax layer of Fine_DenseNet is replaced using Improved Generative Adversarial Network_Artificial Hummingbird (IGAN_AHb) model in order to train the data on the labeled and unlabeled dataset. The loss in the network model is optimized using Artificial Hummingbird (AHb) optimization algorithm. Here, the proposed DL model (Fine_DenseIGANet) is used to perform automated multi-class classification of COVID-19 using CT scan images and attained a superior classification accuracy of 95.73% over other DL models. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467825500044"
    },
    {
        "id": 27256,
        "title": "Few-Shot Learning Based on Deep Learning for Image Classification",
        "authors": "Linglong Tan, Fengzhi Wu, Xianmeng Meng",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccisp59915.2023.10355802"
    },
    {
        "id": 27257,
        "title": "Deep hybrid manifold for image set classification",
        "authors": "Xianhua Zeng, Jueqiu Guo, Yifan Wei, Yang Zhuo",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.imavis.2024.104935"
    },
    {
        "id": 27258,
        "title": "Deep Learning in Image Classification: An Overview",
        "authors": "Kejian Xu, Jinlong Chen, Yi Ning, Wei Tang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10167276"
    },
    {
        "id": 27259,
        "title": "Disaster Monitoring of Satellite Image Processing Using Progressive Image Classification",
        "authors": "Romany F. Mansour, Eatedal Alabdulkreem",
        "published": "2023",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.023307"
    },
    {
        "id": 27260,
        "title": "Sequential Data Classification under Dynamic Emission",
        "authors": "L. Aslanyan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1134/s1054661824010048"
    },
    {
        "id": 27261,
        "title": "Hyperspectral Image Classification Using Multi-feature Fusion Residual Hypergraph Convolution Network",
        "authors": "Tianxing Zhu, Qin Liu, Lixiang Zhang",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsip57908.2023.10271030"
    },
    {
        "id": 27262,
        "title": "An efficient hyperspectral image classification method for limited training data",
        "authors": "Yitao Ren, Peiyang Jin, Yiyang Li, Keming Mao",
        "published": "2023-5",
        "citations": 0,
        "abstract": "AbstractHyperspectral image classification has gained great progress in recent years based on deep learning model and massive training data. However, it is expensive and unpractical to label hyperspectral image data and implement model in constrained environment. To address this problem, this paper proposes an effective ghost module based spectral network for hyperspectral image classification. First, Ghost3D module is adopted to reduce the size of model parameter dramatically by redundant feature maps generation with linear transformation. Then Ghost2D module with channel‐wise attention is used to explore informative spectral feature representation. For large field covering, the non‐local operation is utilized to promote self‐attention. Compared with the state‐of‐the‐art hyperspectral image classification methods, the proposed approach achieves superior performance on three hyperspectral image data sets with fewer sample labelling and less resource consumption.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.12749"
    },
    {
        "id": 27263,
        "title": "MRI Image-Based Automatic Segmentation and Classification of Brain Tumor and Swelling Using Novel Methodologies",
        "authors": "Kapil Mundada, Jayant Kulkarni",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": " In the medical image analysis field, brain tumors (BTs) classification is a complicated process. For effortlessly detecting the tumor devoid of any surgical interference, the radiologists are aided with automated along with computerized technology. Currently, in the field of medical image processing along with analysis, admirable progress has been made by deep learning (DL) methodologies. In medical fields, for resolving several issues, huge attention was paid to DL techniques. For automation of several performed by radiologists like (1) lesion detection, (2) segmentation, (3) classification, (4) monitoring, along with (5) also prediction of treatment response that is not achievable without software, DL might be wielded. Nevertheless, classifying BTs by utilizing magnetic resonance imaging (MRI) has various complications like the difficulty of brain structure along with the intertwining of tissues in it; additionally, the brain’s higher density nature also makes the BT Classification (BTC) process quite complex. Therefore, by utilizing novel systems, MRI-centric Automatic segmentation together with classifications of BT and swelling have been proposed to overcome the aforementioned issues. The proposed methodology underwent various operations to detect BTs effectively. Initially, by utilizing the Range-centric Otsu’s Thresholding (ROT) algorithm, the skull stripping (SS) is conducted. After that, by performing contrast enhancement (CE) along with noise removal, the skull-stripped images are pre-processed. Next, by employing the Rectilinear Watershed Segmentation (RWS) algorithm, the tumor or swelling areas are segmented. Afterward, to obtain the precise tumor or swelling region, the morphological operations are executed on the segmented areas; subsequently, the desired along with relevant features are extracted. Lastly, the features being extracted are inputted to the classifier termed Uniform Convolution neural network (UCNN). The tumor tissues along with the swelling tissues are classified precisely in the classification phase. Here, the openly accessible BT Image Segmentation Benchmark (BRATS) datasets are utilized. Then, the outcomes obtained are analogized with prevailing methodologies. The experiential outcomes displayed that the BTC is performed by the proposed model with a higher accuracy rate; thus, outshined the other prevailing models. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467824500517"
    },
    {
        "id": 27264,
        "title": "RRN: A differential private approach to preserve privacy in image classification",
        "authors": "Zhidong Shen, Ting Zhong, Hui Sun, Baiwen Qi",
        "published": "2023-5",
        "citations": 2,
        "abstract": "AbstractThe wide application of image classification has given rise to many intelligent systems, such as face recognition systems, which makes our life more convenient. However, the ensuing privacy leakage problem has become increasingly serious. The training of a deep neural network requires lots of data, which may contain sensitive information of users and may be exploited by data collectors. A perturbation algorithm named RRN is proposed for image data based on local differential privacy, which provides a rigorous privacy guarantee. Existing solutions have low accuracy due to the high sensitivity of an image; the authors' method combines the Randomized Response mechanism with the Laplace mechanism to solve this problem. Experiments were conducted on the MNIST and CIFAR‐10 datasets to show the effectiveness of the algorithm. Experimental results shows that the model is better than baseline models. The algorithm was also implemented on the commonly used model in deep learning, the VGG model, which can achieve 96.4% accuracy in the non‐private version on the CIFAR‐10 dataset. The accuracy of the differential private VGG model based on the RRN algorithm is 83% when , which is still excellent. The experimental results show that the RRN algorithm can both preserve privacy and data utility.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.12784"
    },
    {
        "id": 27265,
        "title": "A micro-hyperspectral image classification method of gallbladder cancer based on multi-scale fusion attention mechanism",
        "authors": "Gao Hongmin,  , Zhu Min, Cao Xueying, Li Chenming, Liu Qin, Xu Peipei",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.11834/jig.211201"
    },
    {
        "id": 27266,
        "title": "Classification of Red Blood Cells From a Geometric Morphometric Study",
        "authors": "Lluisa Gual-Vaya",
        "published": "2024-3-25",
        "citations": 0,
        "abstract": "Sickle cell disease causes the deformation of erythrocytes into sickle cells. The study of this process using digital images of peripheral blood smears can help specialists to quantify the number of deformed cells in order to gauge the severity of the illness. A new method for classifying red blood cells into three categories: healthy, sickle cell disease, and other deformations is proposed. This method does not require obtaining the contour of each cell but instead utilizes information obtained from a small number of points, obtained through appropriate geometric sampling and the use of stereological formulas. The parameters utilized for classification are the bending energy times length (E) and the circular shape factor (F). In normal cells, which exhibit an almost circular shape, these parameters typically have values close to (1,1). To assess the effectiveness of classification using the parameters (E,F), a synthetic curve dataset and a dataset of red blood cells are employed, applying various supervised and unsupervised classification methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5566/ias.2962"
    },
    {
        "id": 27267,
        "title": "Analysis of Effect of Image Augmentation with Image Enhancement on Fish Image Classification Using Convolutional Neural Network",
        "authors": "Daffa Muhamad Azhar, Nanik Suciati, Chastine Fatichah",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icts58770.2023.10330888"
    },
    {
        "id": 27268,
        "title": "Adaptive Image Anonymization in the Context of Image Classification with Neural Networks",
        "authors": "Nadiya Shvai, Arcadi Llanza Carmona, Amir Nakib",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00468"
    },
    {
        "id": 27269,
        "title": "Feature comparison residuals for foreign fibre classification model",
        "authors": "Wei Wei, Xue Zhou, Zhen Huang, Zhiwei Su",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "AbstractVarious types of foreign fibres may be mixed in the planting, transportation, and production processes of cotton, which not only cause equipment to be out of control, but also leads to a decrease in the quality of cotton textile products and economic losses. The machine vision based detection method for cotton foreign fibres is widely used. Based on existing related research, we construct a classification dataset for cotton foreign fibres in practical scenarios, named the CF2113‐10 dataset. The authors design a basic foreign fibre classification network called CottonNet that balances performance and efficiency. The classification accuracy on the validation set reached 94.2%. In order to enhance the high‐level feature extraction ability, this paper improves the feature fusion method of residual networks and proposes CottonNet‐Res, which improves the classification accuracy to 95.1%. Finally, a classification model based on feature difference fitting, CottonNet‐Fusion, is proposed to address the classification problem of foreign fibre images sampled in complex environments. The classification accuracy of foreign fibre images sampled in ordinary scenes has improved to 97.4%, while the images sampled in complex environments maintain an accuracy of 90.3%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/ipr2.13097"
    },
    {
        "id": 27270,
        "title": "Random image frequency aggregation dropout in image classification for deep convolutional neural networks",
        "authors": "Ju-Hyeon Nam, Sang-Chul Lee",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103684"
    },
    {
        "id": 27271,
        "title": "A Method for Robust and Explainable Image-Based Network Traffic Classification with Deep Learning",
        "authors": "Amine Hattak, Giacomo Iadarola, Fabio Martinelli, Francesco Mercaldo, Antonella Santone",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012083200003555"
    },
    {
        "id": 27272,
        "title": "Zero-shot image classification based on attribute word response",
        "authors": "Chenyu Wu, Jianxun Hong",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3004637"
    },
    {
        "id": 27273,
        "title": "Robust Image Classification with Grayscale Sequence: A VGG-ML Fusion Model for X-Ray Pneumonia Images",
        "authors": "Xueji Fang",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsip57908.2023.10270988"
    },
    {
        "id": 27274,
        "title": "Hyperspectral image classification based on spectral feature extraction",
        "authors": "Li Wang, Wei Wang",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2681034"
    },
    {
        "id": 27275,
        "title": "MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification",
        "authors": "Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni",
        "published": "2023-1-19",
        "citations": 98,
        "abstract": "AbstractWe introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28 × 28 (2D) or 28 × 28 × 28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 9,998 3D images in total, could support numerous research/educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D/3D neural networks and open-source/commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41597-022-01721-8"
    },
    {
        "id": 27276,
        "title": "Image Emotion Classification using deep learning",
        "authors": "",
        "published": "2023-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47750/jptcp.2023.30.14.040"
    },
    {
        "id": 27277,
        "title": "Image Classification and Recognition",
        "authors": "",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.46632/jdaai/2/2/9"
    },
    {
        "id": 27278,
        "title": "CSFINet: Cross-scale Feature Interaction For Medical Image Segmentation",
        "authors": "Yu Feng, Yuhao Zhan, Hao Zeng",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsip57908.2023.10270856"
    },
    {
        "id": 27279,
        "title": "Combining Metric Learning and Attention Heads for Accurate and Efficient Multilabel Image Classification",
        "authors": "Kirill Prokofiev, Vladislav Sovrasov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011603700003417"
    },
    {
        "id": 27280,
        "title": "How Quality Affects Deep Neural Networks in Fine-Grained Image Classification",
        "authors": "Joseph Smith, Zheming Zuo, Jonathan Stonehouse, Boguslaw Obara",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012359200003660"
    },
    {
        "id": 27281,
        "title": "Towards Human-Interpretable Prototypes for Visual Assessment of Image Classification Models",
        "authors": "Poulami Sinhamahapatra, Lena Heidemann, Maureen Monnet, Karsten Roscher",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011894900003417"
    },
    {
        "id": 27282,
        "title": "Boosting With Multiple Clustering Memberships For Hyperspectral Image Classification",
        "authors": "Giovanni Bellio, Randy Russell, Olcay Kursun",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/southeastcon51012.2023.10115209"
    },
    {
        "id": 27283,
        "title": "Multi-scale spectral-spatial dual-transformer network for hyperspectral image classification",
        "authors": "Zhaojie Pan, Sunjinyan Ding, Genyun Sun, Aizhu Zhang, Xiuping Jia, Hang Fu",
        "published": "2023-4-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/01431161.2023.2203340"
    },
    {
        "id": 27284,
        "title": "Achieving Medical Image Classification of Alzheimer’s Disease by the Implementation of Different Image Enhancement Methods Based on VGG",
        "authors": "Kewei Chen, Yijin Gong, Zhuodong Luo, Teoh Teik Toe",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsip57908.2023.10271034"
    },
    {
        "id": 27285,
        "title": "A multi-view-CNN framework for deep representation learning in image classification",
        "authors": "Emmanuel Pintelas, Ioannis E. Livieris, Sotiris Kotsiantis, Panagiotis Pintelas",
        "published": "2023-7",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103687"
    },
    {
        "id": 27286,
        "title": "ELSA: Expanded Latent Space Autoencoder for Image Feature Extraction and Classification",
        "authors": "Emerson Vilar de Oliveira, Dunfrey Aragão, Luiz Gonçalves",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012455300003660"
    },
    {
        "id": 27287,
        "title": "Rate-Distortion-Classification Model In Lossy Image Compression",
        "authors": "Yuefeng Zhang, Zhimeng Huang",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dcc55655.2023.00041"
    },
    {
        "id": 27288,
        "title": "Focused active learning for histopathological image classification",
        "authors": "Arne Schmidt, Pablo Morales-Álvarez, Lee AD Cooper, Lee A. Newberg, Andinet Enquobahrie, Rafael Molina, Aggelos K. Katsaggelos",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.media.2024.103162"
    },
    {
        "id": 27289,
        "title": "Diabetic Retinopathy (DR) Image Synthesis Using DCGAN and Classification of DR Using Transfer Learning Approaches",
        "authors": "Yerrarapu Sravani Devi, S. Phani Kumar",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": " Diabetic retinopathy (DR) refers to a diabetes complexity that immensely impacts the eyes. This is classified into 5 various stages of the severity in accordance with the international convention. Despite that, optimization of a grading model to have a robust generalizability needs a huge number of balanced training data that is very complicated to gather, especially for greater levels of severity. A vast amount of medical data is complex and has a very high-priced method which requires cooperation between the clinics and researchers. The issue is usually attempted to be figured out with the usage of the traditional methods of data augmentation by making certain changes to images of retina dataset for instance rotation, cropping, size and zooming. In this suggested paper, the latest methods or techniques of data augmentation is exhibited which is called as deep convolutional generative adversial network (DC-GAN) and variational auto encoders (VAE). This is a particular method which is responsible for the production of artificial medical images. In addition to this, to improve DR, we can also take the aid of the classification models which are resnet50, densenet201, InceptionV3 and VGG19 for the purpose of classification of the eye related diseases. The proposed method is depicted on the Asia Pacific Tele-Ophthalmology Society (APTOS)-Blindness dataset. First, the present-day online data augmentation techniques have been utilized, and the artificial images of retina are produced by the ease of DCGAN. Then, a method of classifying is used for both techniques. Ultimately, after the method training which is done by using the real & synthetic clinical images and the outcome exhibits which the proposed model determines every stage or phase of DR and achieve the accuracy of 98.66% with using of ResNet-50 which is contrary to the current existing techniques. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467823400090"
    },
    {
        "id": 27290,
        "title": "Robust Bayesian Vision Transformer for Image Analysis and Classification",
        "authors": "Fazlur Rahman Bin Karim, Dimah Dera",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wnyispw60588.2023.10349631"
    },
    {
        "id": 27291,
        "title": "Remote Sensing Image Classification Fusion Cross Convolution and Attention Mechanism",
        "authors": "Junji Wang, Jun Liu, Cheng Zeng, Bo Yang",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipec57296.2023.00020"
    },
    {
        "id": 27292,
        "title": "Residual Convolutional Neural Network With Autoencoder Based Attention For PolSAR Image Classification",
        "authors": "Maryam Imani",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mvip62238.2024.10491190"
    },
    {
        "id": 27293,
        "title": "Hierarchical Classification for Large-Scale Learning",
        "authors": "Boshi Wang, Adrian Barbu",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "Deep neural networks (DNNs) have drawn much attention due to their success in various vision tasks. Current DNNs are used on data with a relatively small number of classes (e.g., 1000 or less) and employ a fully connected layer for classification, which allocates one neuron for each class and thus, per-example, the classification scales as O(K) with the number of classes K. This approach is computationally intensive for many real-life applications where the number of classes is very large (e.g., tens of thousands of classes). To address this problem, our paper introduces a hierarchical approach for classification with a large number of classes that scales as O(K) and could be extended to O(logK) with a deeper hierarchy. The method, called Hierarchical PPCA, uses a self-supervised pretrained feature extractor to obtain meaningful features and trains Probabilistic PCA models on the extracted features for each class separately, making it easy to add classes without retraining the whole model. The Mahalanobis distance is used to obtain the classification result. To speed-up classification, the proposed Hierarchical PPCA framework clusters the image class models, represented as Gaussians, into a smaller number of super-classes using a modified k-means clustering algorithm. The classification speed increase is obtained by Hierarchical PPCA assigning a sample to a small number of the most likely super-classes and restricting the image classification to the image classes corresponding to these super-classes. The fact that the model is trained on each class separately makes it applicable to training on very large datasets such as the whole ImageNet with more than 10,000 classes. Experiments on three standard datasets (ImageNet-100, ImageNet-1k,and ImageNet-10k) indicate that the hierarchical classifier can achieve a superior accuracy with up to a 16-fold speed increase compared to a standard fully connected classifier.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12224646"
    },
    {
        "id": 27294,
        "title": "Global-guided weakly-supervised learning for multi-label image classification",
        "authors": "Yong Dai, Weiwei Song, Zhi Gao, Leyuan Fang",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2023.103823"
    },
    {
        "id": 27295,
        "title": "Masked Embedding Modeling With Rapid Domain Adjustment for Few-Shot Image Classification",
        "authors": "Reece Walsh, Islam Osman, Mohamed S. Shehata",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tip.2023.3306916"
    },
    {
        "id": 27296,
        "title": "Single-Stage Heavy-Tailed Food Classification",
        "authors": "Jiangpeng He, Fengqing Zhu",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222925"
    },
    {
        "id": 27297,
        "title": "Image Classification of Satellite Using VGG16 Model",
        "authors": " Ankita, Sonam Mittal",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdt61202.2024.10489685"
    },
    {
        "id": 27298,
        "title": "Analyzing Image Based Strategies for Android Malware Detection and Classification: An Empirical Exploration",
        "authors": "Chirag Jaju, Dhairya Agrawal, Rishi Poddar, Shubh Badjate, Sidharth Anand, Barsha Mitra, Soumyadeep Dey",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012139100003555"
    },
    {
        "id": 27299,
        "title": "Optimization and Learning Rate Influence on Breast Cancer Image Classification",
        "authors": "Gleidson Barbosa, Larissa Moreira, Pedro Moises de Sousa, Rodrigo Moreira, André Backes",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012507100003660"
    },
    {
        "id": 27300,
        "title": "Meta-optic accelerators for image processing and object classification",
        "authors": "Jason G. Valentine",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663483"
    },
    {
        "id": 27301,
        "title": "A Structure-Fusion Network for Medical Image Classification",
        "authors": "Fuli Wu, Wei Yuan, Pengyi Hao, Shuyuan Tian",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222263"
    },
    {
        "id": 27302,
        "title": "Amalgamation of Image Features for Medical Image Classification with Ensemble of Classifiers",
        "authors": "R Bhuvaneswari, K Ashwini, B Sai Kiran",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/c2i659362.2023.10431014"
    },
    {
        "id": 27303,
        "title": "Soccer line mark segmentation and classification with stochastic watershed transform",
        "authors": "Daniel Berjón, Carlos Cuevas, Narciso García",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.image.2023.117014"
    },
    {
        "id": 27304,
        "title": "Skin Cancer Disease Classification Using Tasnet V2 In Image Classification",
        "authors": "M.Sheerin Banu, V. Elanangai, Uddyalok Chakraborty, D. Sharmiladevi, Ramesh Babu P, P. Rajeswari",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdt61202.2024.10489262"
    }
]