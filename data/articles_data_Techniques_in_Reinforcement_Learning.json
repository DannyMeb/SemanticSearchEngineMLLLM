[
    {
        "id": 1901,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 1902,
        "title": "ENHANCING STEM EDUCATION USING MACHINE LEARNING AND REINFORCEMENT LEARNING TECHNIQUES FOR EDUCATIONAL SOFTWARE AND SERIOUS GAMES",
        "authors": "Yuexin Liu, Behbood Zoghi",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/edulearn.2023.1871"
    },
    {
        "id": 1903,
        "title": "Evaluation of Techniques for Sim2Real Reinforcement Learning",
        "authors": "Mahesh Ranaweera, Qusay Mahmoud",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) has demonstrated promising results in transferring learned policies from simulation to real-world environments. However, inconsistencies and discrepancies between the two environments cause a negative transfer. The phenomenon is commonly known as the “reality gap.” The reality gap prevents learned policies from generalizing to the physical environment. This paper aims to evaluate techniques to improve sim2real learning and bridge the reality gap using RL. For this research, a 3-DOF Stewart Platform was built virtually and physically. The goal of the platform was to guide and balance the marble towards the center of the Stewart platform. Custom API was created to induce noise, manipulate in-game physics, dynamics, and lighting conditions, and perform domain randomization to improve generalization. Two RL algorithms; Q-Learning and Actor-Critic were implemented to train the agent and to evaluate the performance in bridging the reality gap. This paper outlines the techniques utilized to create noise, domain randomization, perform training, results, and observations. Overall, the obtained results show the effectiveness of domain randomization and inducing noise during the agents' learning process. Additionally, the findings provide valuable insights into implementing sim2real RL algorithms to bridge the reality gap.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32473/flairs.36.133317"
    },
    {
        "id": 1904,
        "title": "An overview of reinforcement learning techniques",
        "authors": "Damjan Pecioski, Viktor Gavriloski, Simona Domazetovska, Anastasija Ignjatovska",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/meco58584.2023.10155066"
    },
    {
        "id": 1905,
        "title": "Network Intrusion Detection System Using Reinforcement Learning Techniques",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccpct58313.2023.10245608"
    },
    {
        "id": 1906,
        "title": "Research on path planning techniques based on deep reinforcement learning",
        "authors": "Tianyu Dong",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Path planning problem stands for a sort of problem that find a secure path from the departure position to the destination in the unique application area without conflicts with other objects under the premise of minimum time or distance cost. Path planning has many applications in real life. This paper summarizes and classifies the current main research results on path planning, which are classified according to conventional techniques, single-agent path planning techniques and multi-agent path planning techniques. Conventional techniques are classified into three different algorithms: Traditional techniques, Graphics techniques and Intelligent bionics techniques. For the single-agent path planning techniques, it is primarily broken down into two categories: value-based and strategy-based. For multi-agent techniques, according to the improvement techniques, it is split in three categories: expert demonstration type, improved communication type and task decomposition type. Based on above classification, the mechanisms, advantages, and disadvantages of the existing algorithms are summarized and compared, and the future work in this field is prospected.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/14/20230788"
    },
    {
        "id": 1907,
        "title": "An Overview of Machine Learning, Deep Learning, and Reinforcement Learning-Based Techniques in Quantitative Finance: Recent Progress and Challenges",
        "authors": "Santosh Kumar Sahu, Anil Mokhade, Neeraj Dhanraj Bokde",
        "published": "2023-2-2",
        "citations": 18,
        "abstract": "Forecasting the behavior of the stock market is a classic but difficult topic, one that has attracted the interest of both economists and computer scientists. Over the course of the last couple of decades, researchers have investigated linear models as well as models that are based on machine learning (ML), deep learning (DL), reinforcement learning (RL), and deep reinforcement learning (DRL) in order to create an accurate predictive model. Machine learning algorithms can now extract high-level financial market data patterns. Investors are using deep learning models to anticipate and evaluate stock and foreign exchange markets due to the advantage of artificial intelligence. Recent years have seen a proliferation of the deep reinforcement learning algorithm’s application in algorithmic trading. DRL agents, which combine price prediction and trading signal production, have been used to construct several completely automated trading systems or strategies. Our objective is to enable interested researchers to stay current and easily imitate earlier findings. In this paper, we have worked to explain the utility of Machine Learning, Deep Learning, Reinforcement Learning, and Deep Reinforcement Learning in Quantitative Finance (QF) and the Stock Market. We also outline potential future study paths in this area based on the overview that was presented before.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13031956"
    },
    {
        "id": 1908,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 1909,
        "title": "Self-Learning Robot Autonomous Navigation with Deep Reinforcement Learning Techniques",
        "authors": "Borja Pintos Gómez de las Heras, Rafael Martínez-Tomás, José Manuel Cuadra Troncoso",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Complex and high-computational-cost algorithms are usually the state-of-the-art solution for autonomous driving cases in which non-holonomic robots must be controlled in scenarios with spatial restrictions and interaction with dynamic obstacles while fulfilling at all times safety, comfort, and legal requirements. These highly complex software solutions must cover the high variability of use cases that might appear in traffic conditions, especially when involving scenarios with dynamic obstacles. Reinforcement learning algorithms are seen as a powerful tool in autonomous driving scenarios since the complexity of the algorithm is automatically learned by trial and error with the help of simple reward functions. This paper proposes a methodology to properly define simple reward functions and come up automatically with a complex and successful autonomous driving policy. The proposed methodology has no motion planning module so that the computational power can be limited like in the reactive robotic paradigm. Reactions are learned based on the maximization of the cumulative reward obtained during the learning process. Since the motion is based on the cumulative reward, the proposed algorithm is not bound to any embedded model of the robot and is not being affected by uncertainties of these models or estimators, making it possible to generate trajectories with the consideration of non-holonomic constrains. This paper explains the proposed methodology and discusses the setup of experiments and the results for the validation of the methodology in scenarios with dynamic obstacles. A comparison between the reinforcement learning algorithm and state-of-the-art approaches is also carried out to highlight how the methodology proposed outperforms state-of-the-art algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14010366"
    },
    {
        "id": 1910,
        "title": "Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques",
        "authors": "Emanuel Figetakis, Yahuza Bello, Ahmed Refaey, Lei Lei, Medhat Moussa",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437453"
    },
    {
        "id": 1911,
        "title": "Optimizing Forensic Investigation and Security Surveillance with Deep Reinforcement Learning Techniques",
        "authors": "T J Nandhini, K Thinakaran",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452551"
    },
    {
        "id": 1912,
        "title": "Learning team-based navigation: a review of deep reinforcement learning techniques for multi-agent pathfinding",
        "authors": "Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "AbstractMulti-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation indicators and providing comprehensive clarification on these indicators. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified indicators for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10462-023-10670-6"
    },
    {
        "id": 1913,
        "title": "Robot Arm Movement Control by Model-based Reinforcement Learning using Machine Learning Regression Techniques and Particle Swarm Optimization",
        "authors": "Meta Mueangprasert, Pisak Chermprayong, Kittipong Boonlong",
        "published": "2023-1-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ica-symp56348.2023.10044940"
    },
    {
        "id": 1914,
        "title": "Reinforcement Learning Techniques in Optimizing Energy Systems",
        "authors": "Stefan Stavrev, Dimitar Ginchev",
        "published": "2024-4-12",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) techniques have emerged as powerful tools for optimizing energy systems, offering the potential to enhance efficiency, reliability, and sustainability. This review paper provides a comprehensive examination of the applications of RL in the field of energy system optimization, spanning various domains such as energy management, grid control, and renewable energy integration. Beginning with an overview of RL fundamentals, the paper explores recent advancements in RL algorithms and their adaptation to address the unique challenges of energy system optimization. Case studies and real-world applications demonstrate the efficacy of RL-based approaches in improving energy efficiency, reducing costs, and mitigating environmental impacts. Furthermore, the paper discusses future directions and challenges, including scalability, interpretability, and integration with domain knowledge. By synthesizing the latest research findings and identifying key areas for further investigation, this paper aims to inform and inspire future research endeavors in the intersection of reinforcement learning and energy system optimization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13081459"
    },
    {
        "id": 1915,
        "title": "Advancing Relative Permeability and Capillary Pressure Estimation in Porous Media through Physics-Informed Machine Learning and Reinforcement Learning Techniques",
        "authors": "R. Kalule, H. A. Abderrahmane, S. Ahmed, A. M. Hassan, W. Alameri",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "Abstract\nRecent advances in machine learning have opened new possibilities for accurately solving and understanding complex physical phenomena by combining governing equations with data-driven models. Considering these advancements, this study aims to leverage the potential of a physics-informed machine learning, complemented by reinforcement learning, to estimate relative permeability and capillary pressure functions from unsteady-state core-flooding (waterflooding) data. The study covers the solution of an inverse problem using reinforcement learning, aiming to estimate LET model parameters governing the evolution of relative permeability to achieve the best fit with experimental data through a forward problem solution. In the forward problem, the estimated parameters are utilized to determine the water saturation and the trend of capillary pressure. The estimated curves portray the relationship between relative permeability values and saturation, demonstrating their asymptotic progression towards residual and maximum saturation points. Additionally, the estimated capillary pressure trend aligns with the existing literature, validating the accuracy of our approach. The study shows that the proposed approach offers a promising method for estimating petrophysical properties and provides valuable insights into fluid flow behaviour within a porous media.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2523/iptc-23572-ms"
    },
    {
        "id": 1916,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 1917,
        "title": "Development and Performance Analysis of an AI based Agent to Play Computer Games using Reinforcement Learning Techniques",
        "authors": "K.U. Akkshay, B. Sreevidya",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mysurucon59703.2023.10397011"
    },
    {
        "id": 1918,
        "title": "A Survey of the State-of-the-Art Reinforcement Learning-Based Techniques for Autonomous Vehicle Trajectory Prediction",
        "authors": "Vibha Bharilya, Neetesh Kumar",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370504"
    },
    {
        "id": 1919,
        "title": "Enhancing Data Engineering Efficiency with AI: Utilizing Retrieval-Augmented Generation, Reinforcement Learning from Human Feedback, and Fine-Tuning Techniques",
        "authors": "",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets50070"
    },
    {
        "id": 1920,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 1921,
        "title": "Optimizing Retaining Walls through Reinforcement Learning Approaches and Metaheuristic Techniques",
        "authors": "José Lemus-Romani, Diego Ossandón, Rocío Sepúlveda, Nicolás Carrasco-Astudillo, Victor Yepes, José García",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "The structural design of civil works is closely tied to empirical knowledge and the design professional’s experience. Based on this, adequate designs are generated in terms of strength, operability, and durability. However, such designs can be optimized to reduce conditions associated with the structure’s design and execution, such as costs, CO2 emissions, and related earthworks. In this study, a new discretization technique based on reinforcement learning and transfer functions is developed. The application of metaheuristic techniques to the retaining wall problem is examined, defining two objective functions: cost and CO2 emissions. An extensive comparison is made with various metaheuristics and brute force methods, where the results show that the S-shaped transfer functions consistently yield more robust outcomes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11092104"
    },
    {
        "id": 1922,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 1923,
        "title": "Advanced Seismic Magnitude Classification Through Convolutional and Reinforcement Learning Techniques",
        "authors": "Qiuyi Lin, Jin Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141118"
    },
    {
        "id": 1924,
        "title": "Reinforcement Learning Employing a Multi-Objective Reward Function for SDN Routing Optimization",
        "authors": "Animesh Giri, C Ujwal, H V Siri, Ashwina Rakish",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/easct59475.2023.10392368"
    },
    {
        "id": 1925,
        "title": "Reinforcement Learning Based Self-play and State Stacking Techniques for Noisy Air Combat Environment",
        "authors": "Ahmet S. Tasbas, Safa O. Sahin, Nazim Kemal Üre",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1077"
    },
    {
        "id": 1926,
        "title": "Blockchain-driven optimization of IoT in mobile edge computing environment with deep reinforcement learning and multi-criteria decision-making techniques",
        "authors": "Komeil Moghaddasi, Mohammad Masdari",
        "published": "2023-11-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10586-023-04195-4"
    },
    {
        "id": 1927,
        "title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
        "authors": "Zifan Xu, Bo Liu, Xuesu Xiao, Anirudh Nair, Peter Stone",
        "published": "2023-5-29",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160583"
    },
    {
        "id": 1928,
        "title": "Explainable Artificial Intelligence Techniques for the Analysis of Reinforcement Learning in Non-Linear Flight Regimes",
        "authors": "Gabriel de Haro Pizarroso, Erik-Jan Van Kampen",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2534"
    },
    {
        "id": 1929,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 1930,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 1931,
        "title": "Designing Traffic Management Strategies Using Reinforcement Learning Techniques",
        "authors": "Christine Taylor, Erik Vargo, Emily Bromberg, Tyler Manderfield",
        "published": "2023-10",
        "citations": 0,
        "abstract": " The future vision for traffic flow management is one that leverages advanced automation to assist human decision-makers in the identification of potential constraints and the development of resolution strategies. What makes this problem so challenging is the inherent uncertainty associated with forecasting these constraints, leaving human decision-makers reliant on experience to devise effective traffic management initiatives to mitigate demand in excess of resource capacity. This paper proposes to employ artificial intelligence-based methods to recommend traffic management initiatives under forecast uncertainty and to do so in a real-time planning context. The proposed algorithm consists of 1) a policy network that is generated offline using an Expert Iteration algorithm, 2) a statistical model that updates the likelihood of constraint futures based on observations, and 3) a Monte Carlo tree search algorithm that explores possible combinations of traffic management initiatives to identify the recommended actions for the current decision. The skill introduced by each of the algorithmic components is assessed for a case study focused on managing arrivals into the Atlanta Hartsfield–Jackson International Airport over 92 validation days. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/1.d0339"
    },
    {
        "id": 1932,
        "title": "Design Space Exploration of Approximate Computing Techniques with a Reinforcement Learning Approach",
        "authors": "Sepide Saeedi, Alessandro Savino, Stefano Di Carlo",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsn-w58399.2023.00047"
    },
    {
        "id": 1933,
        "title": "Reinforcement Learning-Based Task Scheduling Using DVFS Techniques in Mobile Devices",
        "authors": "Mohammadamin HajiKhodaverdian, Hamed Rastaghi, Milad Saadat, Hamed Shah-Mansouri",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pimrc56721.2023.10293955"
    },
    {
        "id": 1934,
        "title": "Design optimum PI controller by reinforcement learning for tank level in industrial process",
        "authors": "Anwer Abdulkareem Ali, Mofeed Turky Rashid",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0156702"
    },
    {
        "id": 1935,
        "title": "Join Order Selection with Deep Reinforcement Learning: Fundamentals, Techniques, and Challenges",
        "authors": "Zhengtong Yan, Valter Uotila, Jiaheng Lu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Join Order Selection (JOS) is a fundamental challenge in query optimization, as it significantly affects query performance. However, finding an optimal join order is an NP-hard problem due to the exponentially large search space. Despite the decades-long effort, traditional methods still suffer from limitations. Deep Reinforcement Learning (DRL) approaches have recently gained growing interest and shown superior performance over traditional methods. These DRL-based methods could leverage prior experience through the trial-and-error strategy to automatically explore the optimal join order. This tutorial will focus on recent DRL-based approaches for join order selection by providing a comprehensive overview of the various approaches. We will start by briefly introducing the core concepts of join ordering and the traditional methods for JOS. Next, we will provide some preliminary knowledge about DRL and then delve into DRL-based join order selection approaches by offering detailed information on those methods, analyzing their relationships, and summarizing their weaknesses and strengths. To help the audience gain a deeper understanding of DRL approaches for JOS, we will present two open-source demonstrations and compare their differences. Finally, we will identify research challenges and open problems to provide insights into future research directions. This tutorial will provide valuable guidance for developing more practical DRL approaches for JOS.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14778/3611540.3611576"
    },
    {
        "id": 1936,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 1937,
        "title": "Modern General Game Playing with Reinforcement Learning",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "In computational science and robotics, game playing and artificial intelligence (AI) are research areas that have long been studied. RL systems represent a major step towards autonomous systems that comprehend the visual world at an incredibly deep level and are poised to revolutionize the field of artificial intelligence (AI). Game Play is the testbench where environmental variables can be evaluated to generate adaptive, intelligent, or responsive behavior or simulate real-world scenarios. In this survey, we will explore general gameplay (GGP) with reinforcement learning and its various applications. This survey will cover central algorithms in deep RL, including the deep Qnetwork (DQN) and general mathematical and pragmatic approaches taken in the field. The objective of the current review is to examine the history, associations, and recent advances in the field of general game playing with reinforcement learning and its subfields. We conclude by describing several current areas of research within this field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.16"
    },
    {
        "id": 1938,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 1939,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 1940,
        "title": "Exploring Reinforcement Learning Techniques in the Realm of Mobile Robotics",
        "authors": "Saim Ahmed, Ahmad Taher Azar, Muhammad Zeeshan Sardar, Zeeshan Haider, Nashwa Ahmad Kamal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijaac.2024.10062261"
    },
    {
        "id": 1941,
        "title": "Classification of geogrid reinforcement in aggregate using machine learning techniques",
        "authors": "Samuel Olamide Aregbesola, Yong-Hoon Byun",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "AbstractThe present study proposes a novel ML methodology for differentiating between unstabilized aggregate specimens and those stabilized with triangular and rectangular aperture geogrids. This study utilizes the compiled experimental results obtained from stabilized and unstabilized specimens under repeated loading into a balanced, moderate-sized database. The efficacy of five ML models, including tree-ensemble and single-learning algorithms, in accurately identifying each specimen class was explored. Shapley’s additive explanation was used to understand the intricacies of the models and determine global feature importance ranking of the input variables. All the models could identify the unstabilized specimen with an accuracy of at least 0.9. The tree-ensemble models outperformed the single-learning models when all three classes (unstabilized specimens and specimens stabilized by triangular and rectangular aperture geogrids) were considered, with the light gradient boosting machine showing the best performance—an accuracy of 0.94 and an area under the curve score of 0.98. According to Shapley’s additive explanation, the resilient modulus and confining pressure were identified as the most important features across all models. Therefore, the proposed ML methodology may be effectively used to determine the type and presence of geogrid reinforcement in aggregates, based on a few aggregate material properties and performance under repeated loading.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40703-024-00206-4"
    },
    {
        "id": 1942,
        "title": "Comprehensive Review of Benefits from the Use of Sparse Updates Techniques in Reinforcement Learning: Experimental Simulations in Complex Action Space Environments",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciees58940.2023.10378830"
    },
    {
        "id": 1943,
        "title": "Using Deep Reinforcement Learning for Assessing the Consequences of Cyber Mitigation Techniques on Industrial Control Systems",
        "authors": "Terry Merz, Romarie Morales Rosado",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "This paper discusses an in-progress study involving the use of deep reinforcement learning (DRL) to mitigate the effects of an advanced cyber-attack against industrial control systems (ICS).  The research is a qualitative, exploratory study which emerged as a gap during the execution of two rapid prototyping studies.  During these studies, cyber defensive procedures, known as “Mitigation, were characterized as actions taken to minimize the impact of ongoing advanced cyber-attacks against an ICS while enabling primary operations to continue.  To execute Mitigation procedures, affected ICS components required rapid isolation and quarantining from “healthy” system segments. However today, with most attacks leveraging automation, mitigation also requires rapid decision-making capabilities operating at the speed of automation yet with human-like refinement.  The authors settled on the choice of DRL as a viable solution to this problem due to the algorithm’s designs which involves “intelligent” decisions based upon continuous learning achieved through a rewards system.  The primary theory of this study posits that processes informed by data sources relative to the execution path of an advanced cyber-attack as well as the consequences of deploying a particular Mitigation procedure evolve the system into an ever-improving defensive capability.  This study seeks to produce a defensive DLR based software agent trained by a DRL based offensive software agent that generates policy refinements based upon extrapolations from a corrupted network state as reported by an IDS and baseline data. Results include an estimation rule that would quantify impacts of various mitigation actions while protecting the operational critical path and isolating an in-progress attack.  This study is in a conceptual phase and development has not started.\r\nThis research questions for this study are:\r\nRQ1: Can this software agent categorize correctly an in-progress cyber-attack and extrapolate the potential ICS assets affected?\r\nRQ2: Can this software agent categorize novel cyber-attacks and extrapolate a probable attack vector while enumerating affected assets?\r\nRQ3: Can this software agent characterize how operations are affected by quarantine actions?\r\nRQ4: Can this software agent generate a set of ranked recommended courses of action by effectiveness, and least negative effects on the operational critical path?",
        "keywords": "",
        "link": "http://dx.doi.org/10.34190/iccws.18.1.1063"
    },
    {
        "id": 1944,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 1945,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 1946,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 1947,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 1948,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 1949,
        "title": "AIM-RL: A New Framework Supporting Reinforcement Learning Experiments",
        "authors": "Ionuţ-Cristian Pistol, Andrei Arusoaie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091100003538"
    },
    {
        "id": 1950,
        "title": "Combining Forecasting and Multi-Agent Reinforcement Learning Techniques on Power Grid Scheduling Task",
        "authors": "Cheng Yang, Jihai Zhang, Fangquan Lin, Li Wang, Wei Jiang, Hanwei Zhang",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eebda56825.2023.10090669"
    },
    {
        "id": 1951,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 1952,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 1953,
        "title": "A Survey of Sim-to-Real Transfer Techniques Applied to Reinforcement Learning for Bioinspired Robots",
        "authors": "Wei Zhu, Xian Guo, Dai Owaki, Kyo Kutsuzawa, Mitsuhiro Hayashibe",
        "published": "2023-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3112718"
    },
    {
        "id": 1954,
        "title": "Teaching Reinforcement Learning Agents via Reinforcement Learning",
        "authors": "Kun Yang, Chengshuai Shi, Cong Shen",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089695"
    },
    {
        "id": 1955,
        "title": "A Review of Techniques and Policies on Cybersecurity Using Artificial Intelligence and Reinforcement Learning Algorithms",
        "authors": "Neshat Elhami Fard, Rastko R. Selmic, Khashayar Khorasani",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mts.2023.3306540"
    },
    {
        "id": 1956,
        "title": "Molecule Builder: Environment for Testing Reinforcement Learning Agents",
        "authors": "Petr Hyner, Jan Hůla, Mikoláš Janota",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012257900003595"
    },
    {
        "id": 1957,
        "title": "Automating XSS Vulnerability Testing Using Reinforcement Learning",
        "authors": "Kento Hasegawa, Seira Hidano, Kazuhide Fukushima",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011653600003405"
    },
    {
        "id": 1958,
        "title": "Energy management investigation of multi-level grid power supplying micro-grid using reinforcement learning and improved PSO optimization",
        "authors": "Nabil Jalil Aklo, Mofeed Turky Rashid",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0154603"
    },
    {
        "id": 1959,
        "title": "Pre- and post-processing techniques for reinforcement-learning-based routing and spectrum assignment in elastic optical networks",
        "authors": "Takafumi Tanaka, Masayuki Shimoda",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Research on the routing and spectrum assignment (RSA) problem has long been conducted with the aim of efficiently utilizing the frequency resources of optical networks. Given the recent progress in machine learning (ML) technology, it has been reported that the application of ML in various areas of optical network design, operation, and management has the potential to bring about new innovations such as autonomous optical network operation and highly accurate estimation of network conditions. With regard to the RSA problem, it is expected that an algorithm that achieves better accommodation efficiency than conventional heuristic methods can be realized by applying reinforcement learning (RL), which well supports training in a simulation environment. In this paper, we introduce and evaluate three techniques devised to apply RL more effectively to elastic optical networks (EONs): two pre-processing techniques called link-axis positional encoding (LPE) and slot-axis positional encoding (SPE) and a post-processing technique named the assignable boundary slot mask. First, we build a simple model in which the state data of the optical network, including frequency slot utilization, are input to the neural network of the RSA agent in RL and show that this model has difficulty outperforming the conventional heuristic RSA algorithm. Next, we build an RSA agent model with the proposed techniques and simulate the accommodation of dynamic optical paths to quantitatively demonstrate that the blocking probability can be reduced by 17% compared to the conventional heuristic.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1364/jocn.503599"
    },
    {
        "id": 1960,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 1961,
        "title": "Intelligent Scheduling Based on Reinforcement Learning Approaches: Applying Advanced Q-Learning and State–Action–Reward–State–Action Reinforcement Learning Models for the Optimisation of Job Shop Scheduling Problems",
        "authors": "Atefeh Momenikorbekandi, Maysam Abbod",
        "published": "2023-11-23",
        "citations": 1,
        "abstract": "Flexible job shop scheduling problems (FJSPs) have attracted significant research interest because they can considerably increase production efficiency in terms of energy, cost and time; they are considered the main part of the manufacturing systems which frequently need to be resolved to manage the variations in production requirements. In this study, novel reinforcement learning (RL) models, including advanced Q-learning (QRL) and RL-based state–action–reward–state–action (SARSA) models, are proposed to enhance the scheduling performance of FJSPs, in order to reduce the total makespan. To more accurately depict the problem realities, two categories of simulated single-machine job shops and multi-machine job shops, as well as the scheduling of a furnace model, are used to compare the learning impact and performance of the novel RL models to other algorithms. FJSPs are challenging to resolve and are considered non-deterministic polynomial-time hardness (NP-hard) problems. Numerous algorithms have been used previously to solve FJSPs. However, because their key parameters cannot be effectively changed dynamically throughout the computation process, the effectiveness and quality of the solutions fail to meet production standards. Consequently, in this research, developed RL models are presented. The efficacy and benefits of the suggested SARSA method for solving FJSPs are shown by extensive computer testing and comparisons. As a result, this can be a competitive algorithm for FJSPs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12234752"
    },
    {
        "id": 1962,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 1963,
        "title": "Energy-Aware MPTCP Scheduling in Heterogeneous Wireless Networks Using Multi-Agent Deep Reinforcement Learning Techniques",
        "authors": "Zulfiqar Ali Arain, Xuesong Qiu, Changqiao Xu, Mu Wang, Mussadiq Abdul Rahim",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "This paper proposes an energy-efficient scheduling scheme for multi-path TCP (MPTCP) in heterogeneous wireless networks, aiming to minimize energy consumption while ensuring low latency and high throughput. Each MPTCP sub-flow is controlled by an agent that cooperates with other agents using the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. This approach enables the agents to learn decentralized policies through centralized training and decentralized execution. The scheduling problem is modeled as a multi-agent decision-making task. The proposed energy-efficient scheduling scheme, referred to as EE-MADDPG, demonstrates significant energy savings while maintaining lower latency and higher throughput compared to other state-of-the-art scheduling techniques. By adopting a multi-agent deep reinforcement learning approach, the agents can learn efficient scheduling policies that optimize various performance metrics in heterogeneous wireless networks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12214496"
    },
    {
        "id": 1964,
        "title": "Bird’s Eye View Map for End-to-end Autonomous Driving Using Reinforcement Learning",
        "authors": "Zhiqi Mao, Zhihang Song, Lihui Peng, Jianming Hu, Danya Yao, Yi Zhang",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ist59124.2023.10355730"
    },
    {
        "id": 1965,
        "title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012082200003541"
    },
    {
        "id": 1966,
        "title": "Microgrid based VANET monitoring and energy management in 5G networks by reinforcement deep learning techniques",
        "authors": "A. Selvakumar, S. Ramesh, T. Manikandan, G. Michael, U. Arul, R. Gnanajeyaraman",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2023.108933"
    },
    {
        "id": 1967,
        "title": "Investigating Suitable Combinations of Dynamic Models and Control Techniques for Offline Reinforcement Learning Based Navigation: Application of Universal Omni-Wheeled Robots",
        "authors": "Nalaka Amarasiri, Alan A. Barhorst, Raju Gottumukkala",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "Abstract\nOmnidirectional locomotion provides wheeled mobile robots (WMR) with better maneuverability and flexibility, which enhances their energy efficiency and dexterity. Universal omni-wheels are one of the best categories of wheels that can be used to develop a WMR (Amarasiri et al., 2022, “Robust Dynamic Modeling and Trajectory Tracking Controller of a Universal Omni-Wheeled Mobile Robot,” ASME Letters Dyn. Sys. Control., 2(4), p. 040902. 10.1115/1.4055690). We study dynamic modeling and controllers for mobile robots to train in a reinforcement learning (RL)-based navigation algorithm. RL tasks require copious amounts of learning iteration episodes, which makes training very time consuming. The choice of dynamic model and controller has a significant impact on training time. In this paper, we compare a traditional Kane’s equations model to a non-holonomic canonical momenta model (Barhorst, 2019, “Generalized Momenta in Constrained Non-Holonomic Systems—Another Perspective on the Canonical Equations of Motion,” Int. J. Non-Linear Mech., 113, pp. 128–145.). We implemented four controllers: proportional integral derivative, linear quadratic regulator with integral action, pole placement, and a full nonlinear sliding mode controller. We summarize the pros and cons of each of the modeling techniques, and control laws implemented. The outcomes of our analysis will improve RL training time for path generation in unstructured environments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4064517"
    },
    {
        "id": 1968,
        "title": "A simplified reinforcement: The Florida sleeve Ross procedure",
        "authors": "Stephen M. Spindel, Christopher M. Zumwalt, Jasmine Su, Autumn P. Stevenson",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.xjtc.2024.02.016"
    },
    {
        "id": 1969,
        "title": "Factory Simulation of Optimization Techniques Based on Deep Reinforcement Learning for Storage Devices",
        "authors": "Ju-Bin Lim, Jongpil Jeong",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "In this study, reinforcement learning (RL) was used in factory simulation to optimize storage devices for use in Industry 4.0 and digital twins. Industry 4.0 is increasing productivity and efficiency in manufacturing through automation, data exchange, and the integration of new technologies. Innovative technologies such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics are smartly automating manufacturing processes and integrating data with production systems to monitor and analyze production data in real time and optimize factory operations. A digital twin is a digital model of a physical product or process in the real world. It is built on data and real-time information collected through sensors and accurately simulates the behavior and performance of a real-world manufacturing floor. With a digital twin, one can leverage data at every stage of product design, development, manufacturing, and maintenance to predict, solve, and optimize problems. First, we defined an RL environment, modeled it, and validated its ability to simulate a real physical system. Subsequently, we introduced a method to calculate reward signals and apply them to the environment to ensure the alignment of the behavior of the RL agent with the task objective. Traditional approaches use simple reward functions to tune the behavior of reinforcement learning agents. These approaches issue rewards according to predefined rules and often use reward signals that are unrelated to the task goal. However, in this study, the reward signal calculation method was modified to consider the task goal and the characteristics of the physical system and calculate more realistic and meaningful rewards. This method reflects the complex interactions and constraints that occur during the optimization process of the storage device and generates more accurate episodes of reinforcement learning in agent behavior. Unlike the traditional simple reward function, this reflects the complexity and realism of the storage optimization task, making the reward more sophisticated and effective.The stocker simulation model was used to validate the effectiveness of RL. The model is a storage device that simulates logistics in a manufacturing production area. The results revealed that RL is a useful tool for automating and optimizing complex logistics systems, increasing the applicability of RL in logistics. We proposed a novel method for creating an agent through learning using the proximal policy optimization algorithm, and the agent was optimized by configuring various learning options. The application of reinforcement learning resulted in an effectiveness of 30–100%, and the methods can be expanded to other fields.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13179690"
    },
    {
        "id": 1970,
        "title": "Reinventing Astronomical Survey Scheduling with Reinforcement Learning: Unveiling the Potential of Self-Driving Telescope",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2246942"
    },
    {
        "id": 1971,
        "title": "Multi-Environment Training Against Reward Poisoning Attacks on Deep Reinforcement Learning",
        "authors": "Myria Bouhaddi, Kamel Adi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012139900003555"
    },
    {
        "id": 1972,
        "title": "A Recent Publications Survey on Reinforcement Learning for Selecting Parameters of Meta-Heuristic and Machine Learning Algorithms",
        "authors": "Maria Chernigovskaya, Andrey Kharitonov, Klaus Turowski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011954300003488"
    },
    {
        "id": 1973,
        "title": "Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control",
        "authors": "Rohit Bokade, Xiaoning Jin, Christopher Amato",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3275883"
    },
    {
        "id": 1974,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 1975,
        "title": "Multisite gaming streaming optimization over virtualized 5G environment using Deep Reinforcement Learning techniques",
        "authors": "Alberto del Rio, Javier Serrano, David Jimenez, Luis M. Contreras, Federico Alvarez",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2024.110334"
    },
    {
        "id": 1976,
        "title": "Research on Inertial Navigation Technology of Unmanned Aerial Vehicles with Integrated Reinforcement Learning Algorithm",
        "authors": "",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "With the continuous expansion of unmanned aerial vehicle (UAV) applications, traditional inertial navigation technology exhibits significant limitations in complex environments. In this study, we integrate improved reinforcement learning (RL) algorithms to enhance existing unmanned aerial vehicle inertial navigation technology and introduce a modulated mechanism (MM) for adjusting the state of the intelligent agent in an innovative manner [1,2]. Through interaction with the environment, the intelligent machine can learn more effective navigation strategies [3]. The ultimate goal is to provide a foundation for autonomous navigation of unmanned aerial vehicles during flight and improve navigation accuracy and robustness. We first define appropriate state representation and action space, and then design an adjustment mechanism based on the actions selected by the intelligent agent. The adjustment mechanism outputs the next state and reward value of the agent. Additionally, the adjustment mechanism calculates the error between the adjusted state and the unadjusted state. Furthermore, the intelligent agent stores the acquired experience samples containing states and reward values in a buffer and replays the experiences during each iteration to learn the dynamic characteristics of the environment. We name the improved algorithm as the DQM algorithm. Experimental results demonstrate that the intelligent agent using our proposed algorithm effectively reduces the accumulated errors of inertial navigation in dynamic environments. Although our research provides a basis for achieving autonomous navigation of unmanned aerial vehicles, there is still room for significant optimization. Further research can include testing unmanned aerial vehicles in simulated environments, testing unmanned aerial vehicles in realworld environments, optimizing the design of reward functions, improving the algorithm workflow to enhance convergence speed and performance, and enhancing the algorithm's generalization ability. It has been proven that by integrating reinforcement learning algorithms, unmanned aerial vehicles can achieve autonomous navigation, thereby improving navigation accuracy and robustness in dynamic and changing environments [4]. Therefore, this research plays an important role in promoting the development and application of unmanned aerial vehicle technology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.06"
    },
    {
        "id": 1977,
        "title": "Unlocking Autonomous Telescopes through Reinforcement Learning: An Offline Framework and Insights from a Case Study",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2217169"
    },
    {
        "id": 1978,
        "title": "Towards Self-Adaptive Resilient Swarms Using Multi-Agent Reinforcement Learning",
        "authors": "Rafael Pina, Varuna De Silva, Corentin Artaud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012462800003654"
    },
    {
        "id": 1979,
        "title": "PenGym: Pentesting Training Framework for Reinforcement Learning Agents",
        "authors": "Thanh Nguyen, Zhi Chen, Kento Hasegawa, Kazuhide Fukushima, Razvan Beuran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012367300003648"
    },
    {
        "id": 1980,
        "title": "Comprehensive Review of Benefits from the Use of Neuron Connection Pruning Techniques During the Training Process of Artificial Neural Networks in Reinforcement Learning: Experimental Simulations in Atari Games",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304968"
    },
    {
        "id": 1981,
        "title": "ADAPTIVE LEARNING THROUGH AI: REINFORCEMENT LEARNING IN TEACHING MULTIPLICATION TABLES",
        "authors": "Lara Drožđek, Igor Pesek",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/inted.2024.1186"
    },
    {
        "id": 1982,
        "title": "Welcome to the Jungle: A Conceptual Comparison of Reinforcement Learning Algorithms",
        "authors": "Kenneth Schröder, Alexander Kastius, Rainer Schlosser",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011626700003396"
    },
    {
        "id": 1983,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 1984,
        "title": "Safe Q-Learning Approaches for Human-in-Loop Reinforcement Learning",
        "authors": "Swathi Veerabathraswamy, Nirav Bhatt",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442899"
    },
    {
        "id": 1985,
        "title": "Quantum reinforcement learning",
        "authors": "Niels M. P. Neumann, Paolo B. U. L. de Heer, Frank Phillipson",
        "published": "2023-2-22",
        "citations": 1,
        "abstract": "AbstractIn this paper, we present implementations of an annealing-based and a gate-based quantum computing approach for finding the optimal policy to traverse a grid and compare them to a classical deep reinforcement learning approach. We extended these three approaches by allowing for stochastic actions instead of deterministic actions and by introducing a new learning technique called curriculum learning. With curriculum learning, we gradually increase the complexity of the environment and we find that it has a positive effect on the expected reward of a traversal. We see that the number of training steps needed for the two quantum approaches is lower than that needed for the classical approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11128-023-03867-9"
    },
    {
        "id": 1986,
        "title": "Dynamic Path Planning for Autonomous Vehicles Using Adaptive Reinforcement Learning",
        "authors": "Karim Wahdan, Nourhan Ehab, Yasmin Mansy, Amr El Mougy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363300003636"
    },
    {
        "id": 1987,
        "title": "Optimization of a Deep Reinforcement Learning Policy for Construction Manufacturing Control",
        "authors": "Ian Flood, Xiaoyan Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091400003546"
    },
    {
        "id": 1988,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 1989,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 1990,
        "title": "Networked Personalized Federated Learning Using Reinforcement Learning",
        "authors": "Francois Gauthier, Vinay Chakravarthi Gogineni, Stefan Werner",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279781"
    },
    {
        "id": 1991,
        "title": "Survey on Practical Reinforcement Learning : from Imitation Learning to Offline Reinforcement Learning",
        "authors": "Dongsu Lee, Chanin Eom, Sungwoo Choi, Sungkwan Kim, Minhae Kwon",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.11.1405"
    },
    {
        "id": 1992,
        "title": "Multi-Fidelity Reinforcement Learning with Control Variates",
        "authors": "Sami Khairy, Prasanna Balaprakash",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-181"
    },
    {
        "id": 1993,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 1994,
        "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
        "authors": "Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.977"
    },
    {
        "id": 1995,
        "title": "Quality Metrics for Reinforcement Learning for Edge Cloud and Internet-of-Things Systems",
        "authors": "Claus Pahl, Hamid Barzegar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194800003584"
    },
    {
        "id": 1996,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 1997,
        "title": "Subgoal Reachability in Goal Conditioned Hierarchical Reinforcement Learning",
        "authors": "Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Bartłomiej Sobieski, Tomasz Trzciński",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012326200003636"
    },
    {
        "id": 1998,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 1999,
        "title": "Prior-Information Enhanced Reinforcement Learning for Energy Management Systems",
        "authors": "Th´eo Zangato, Aomar Osmani, Pegah Alizadeh",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Amidst increasing energy demands and growing environmental concerns, the promotion of sustainable and energy-efficient practices has become imperative. This paper introduces a reinforcement learning-based technique for optimizing energy consumption and its associated costs, with a focus on energy management systems. A three-step approach for the efficient management of charging cycles in energy storage units within buildings is presented combining RL with prior knowledge. A unique strategy is adopted: clustering building load curves to discern typical energy consumption patterns, embedding domain knowledge into the learning algorithm to refine the agent’s action space and predicting of future observations to make real-time decisions. We showcase the effectiveness of our method using real-world data. It enables controlled exploration and efficient training of Energy Management System (EMS) agents. When compared to the benchmark, our model reduces energy costs by up to 15%, cutting down consumption during peak periods, and demonstrating adaptability across various building consumption profiles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.140207"
    },
    {
        "id": 2000,
        "title": "Learning Quadruped Locomotion Method Integrating Meta-Learning and Reinforcement Learning",
        "authors": "Xiafu Lv, Runming He, Chuangshi Wang, Yang Ou, Hong Wang, Zhenglin Chen",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450573"
    }
]