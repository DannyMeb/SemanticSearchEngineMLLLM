[
    {
        "id": 5105,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 5106,
        "title": "ENHANCING STEM EDUCATION USING MACHINE LEARNING AND REINFORCEMENT LEARNING TECHNIQUES FOR EDUCATIONAL SOFTWARE AND SERIOUS GAMES",
        "authors": "Yuexin Liu, Behbood Zoghi",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/edulearn.2023.1871"
    },
    {
        "id": 5107,
        "title": "Evaluation of Techniques for Sim2Real Reinforcement Learning",
        "authors": "Mahesh Ranaweera, Qusay Mahmoud",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) has demonstrated promising results in transferring learned policies from simulation to real-world environments. However, inconsistencies and discrepancies between the two environments cause a negative transfer. The phenomenon is commonly known as the “reality gap.” The reality gap prevents learned policies from generalizing to the physical environment. This paper aims to evaluate techniques to improve sim2real learning and bridge the reality gap using RL. For this research, a 3-DOF Stewart Platform was built virtually and physically. The goal of the platform was to guide and balance the marble towards the center of the Stewart platform. Custom API was created to induce noise, manipulate in-game physics, dynamics, and lighting conditions, and perform domain randomization to improve generalization. Two RL algorithms; Q-Learning and Actor-Critic were implemented to train the agent and to evaluate the performance in bridging the reality gap. This paper outlines the techniques utilized to create noise, domain randomization, perform training, results, and observations. Overall, the obtained results show the effectiveness of domain randomization and inducing noise during the agents' learning process. Additionally, the findings provide valuable insights into implementing sim2real RL algorithms to bridge the reality gap.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32473/flairs.36.133317"
    },
    {
        "id": 5108,
        "title": "An overview of reinforcement learning techniques",
        "authors": "Damjan Pecioski, Viktor Gavriloski, Simona Domazetovska, Anastasija Ignjatovska",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/meco58584.2023.10155066"
    },
    {
        "id": 5109,
        "title": "Network Intrusion Detection System Using Reinforcement Learning Techniques",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccpct58313.2023.10245608"
    },
    {
        "id": 5110,
        "title": "Research on path planning techniques based on deep reinforcement learning",
        "authors": "Tianyu Dong",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "Path planning problem stands for a sort of problem that find a secure path from the departure position to the destination in the unique application area without conflicts with other objects under the premise of minimum time or distance cost. Path planning has many applications in real life. This paper summarizes and classifies the current main research results on path planning, which are classified according to conventional techniques, single-agent path planning techniques and multi-agent path planning techniques. Conventional techniques are classified into three different algorithms: Traditional techniques, Graphics techniques and Intelligent bionics techniques. For the single-agent path planning techniques, it is primarily broken down into two categories: value-based and strategy-based. For multi-agent techniques, according to the improvement techniques, it is split in three categories: expert demonstration type, improved communication type and task decomposition type. Based on above classification, the mechanisms, advantages, and disadvantages of the existing algorithms are summarized and compared, and the future work in this field is prospected.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/14/20230788"
    },
    {
        "id": 5111,
        "title": "An Overview of Machine Learning, Deep Learning, and Reinforcement Learning-Based Techniques in Quantitative Finance: Recent Progress and Challenges",
        "authors": "Santosh Kumar Sahu, Anil Mokhade, Neeraj Dhanraj Bokde",
        "published": "2023-2-2",
        "citations": 19,
        "abstract": "Forecasting the behavior of the stock market is a classic but difficult topic, one that has attracted the interest of both economists and computer scientists. Over the course of the last couple of decades, researchers have investigated linear models as well as models that are based on machine learning (ML), deep learning (DL), reinforcement learning (RL), and deep reinforcement learning (DRL) in order to create an accurate predictive model. Machine learning algorithms can now extract high-level financial market data patterns. Investors are using deep learning models to anticipate and evaluate stock and foreign exchange markets due to the advantage of artificial intelligence. Recent years have seen a proliferation of the deep reinforcement learning algorithm’s application in algorithmic trading. DRL agents, which combine price prediction and trading signal production, have been used to construct several completely automated trading systems or strategies. Our objective is to enable interested researchers to stay current and easily imitate earlier findings. In this paper, we have worked to explain the utility of Machine Learning, Deep Learning, Reinforcement Learning, and Deep Reinforcement Learning in Quantitative Finance (QF) and the Stock Market. We also outline potential future study paths in this area based on the overview that was presented before.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13031956"
    },
    {
        "id": 5112,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 5113,
        "title": "Self-Learning Robot Autonomous Navigation with Deep Reinforcement Learning Techniques",
        "authors": "Borja Pintos Gómez de las Heras, Rafael Martínez-Tomás, José Manuel Cuadra Troncoso",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Complex and high-computational-cost algorithms are usually the state-of-the-art solution for autonomous driving cases in which non-holonomic robots must be controlled in scenarios with spatial restrictions and interaction with dynamic obstacles while fulfilling at all times safety, comfort, and legal requirements. These highly complex software solutions must cover the high variability of use cases that might appear in traffic conditions, especially when involving scenarios with dynamic obstacles. Reinforcement learning algorithms are seen as a powerful tool in autonomous driving scenarios since the complexity of the algorithm is automatically learned by trial and error with the help of simple reward functions. This paper proposes a methodology to properly define simple reward functions and come up automatically with a complex and successful autonomous driving policy. The proposed methodology has no motion planning module so that the computational power can be limited like in the reactive robotic paradigm. Reactions are learned based on the maximization of the cumulative reward obtained during the learning process. Since the motion is based on the cumulative reward, the proposed algorithm is not bound to any embedded model of the robot and is not being affected by uncertainties of these models or estimators, making it possible to generate trajectories with the consideration of non-holonomic constrains. This paper explains the proposed methodology and discusses the setup of experiments and the results for the validation of the methodology in scenarios with dynamic obstacles. A comparison between the reinforcement learning algorithm and state-of-the-art approaches is also carried out to highlight how the methodology proposed outperforms state-of-the-art algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14010366"
    },
    {
        "id": 5114,
        "title": "Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques",
        "authors": "Emanuel Figetakis, Yahuza Bello, Ahmed Refaey, Lei Lei, Medhat Moussa",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437453"
    },
    {
        "id": 5115,
        "title": "Optimizing Forensic Investigation and Security Surveillance with Deep Reinforcement Learning Techniques",
        "authors": "T J Nandhini, K Thinakaran",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452551"
    },
    {
        "id": 5116,
        "title": "Learning team-based navigation: a review of deep reinforcement learning techniques for multi-agent pathfinding",
        "authors": "Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "AbstractMulti-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation indicators and providing comprehensive clarification on these indicators. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified indicators for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10462-023-10670-6"
    },
    {
        "id": 5117,
        "title": "Robot Arm Movement Control by Model-based Reinforcement Learning using Machine Learning Regression Techniques and Particle Swarm Optimization",
        "authors": "Meta Mueangprasert, Pisak Chermprayong, Kittipong Boonlong",
        "published": "2023-1-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ica-symp56348.2023.10044940"
    },
    {
        "id": 5118,
        "title": "Reinforcement Learning Techniques in Optimizing Energy Systems",
        "authors": "Stefan Stavrev, Dimitar Ginchev",
        "published": "2024-4-12",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) techniques have emerged as powerful tools for optimizing energy systems, offering the potential to enhance efficiency, reliability, and sustainability. This review paper provides a comprehensive examination of the applications of RL in the field of energy system optimization, spanning various domains such as energy management, grid control, and renewable energy integration. Beginning with an overview of RL fundamentals, the paper explores recent advancements in RL algorithms and their adaptation to address the unique challenges of energy system optimization. Case studies and real-world applications demonstrate the efficacy of RL-based approaches in improving energy efficiency, reducing costs, and mitigating environmental impacts. Furthermore, the paper discusses future directions and challenges, including scalability, interpretability, and integration with domain knowledge. By synthesizing the latest research findings and identifying key areas for further investigation, this paper aims to inform and inspire future research endeavors in the intersection of reinforcement learning and energy system optimization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13081459"
    },
    {
        "id": 5119,
        "title": "Advancing Relative Permeability and Capillary Pressure Estimation in Porous Media through Physics-Informed Machine Learning and Reinforcement Learning Techniques",
        "authors": "R. Kalule, H. A. Abderrahmane, S. Ahmed, A. M. Hassan, W. Alameri",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "Abstract\nRecent advances in machine learning have opened new possibilities for accurately solving and understanding complex physical phenomena by combining governing equations with data-driven models. Considering these advancements, this study aims to leverage the potential of a physics-informed machine learning, complemented by reinforcement learning, to estimate relative permeability and capillary pressure functions from unsteady-state core-flooding (waterflooding) data. The study covers the solution of an inverse problem using reinforcement learning, aiming to estimate LET model parameters governing the evolution of relative permeability to achieve the best fit with experimental data through a forward problem solution. In the forward problem, the estimated parameters are utilized to determine the water saturation and the trend of capillary pressure. The estimated curves portray the relationship between relative permeability values and saturation, demonstrating their asymptotic progression towards residual and maximum saturation points. Additionally, the estimated capillary pressure trend aligns with the existing literature, validating the accuracy of our approach. The study shows that the proposed approach offers a promising method for estimating petrophysical properties and provides valuable insights into fluid flow behaviour within a porous media.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2523/iptc-23572-ms"
    },
    {
        "id": 5120,
        "title": "Development and Performance Analysis of an AI based Agent to Play Computer Games using Reinforcement Learning Techniques",
        "authors": "K.U. Akkshay, B. Sreevidya",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mysurucon59703.2023.10397011"
    },
    {
        "id": 5121,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 5122,
        "title": "A Survey of the State-of-the-Art Reinforcement Learning-Based Techniques for Autonomous Vehicle Trajectory Prediction",
        "authors": "Vibha Bharilya, Neetesh Kumar",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370504"
    },
    {
        "id": 5123,
        "title": "Enhancing Data Engineering Efficiency with AI: Utilizing Retrieval-Augmented Generation, Reinforcement Learning from Human Feedback, and Fine-Tuning Techniques",
        "authors": "",
        "published": "2024-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets50070"
    },
    {
        "id": 5124,
        "title": "Reinforcement Learning Employing a Multi-Objective Reward Function for SDN Routing Optimization",
        "authors": "Animesh Giri, C Ujwal, H V Siri, Ashwina Rakish",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/easct59475.2023.10392368"
    },
    {
        "id": 5125,
        "title": "Advanced Seismic Magnitude Classification Through Convolutional and Reinforcement Learning Techniques",
        "authors": "Qiuyi Lin, Jin Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0141118"
    },
    {
        "id": 5126,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 5127,
        "title": "Optimizing Retaining Walls through Reinforcement Learning Approaches and Metaheuristic Techniques",
        "authors": "José Lemus-Romani, Diego Ossandón, Rocío Sepúlveda, Nicolás Carrasco-Astudillo, Victor Yepes, José García",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "The structural design of civil works is closely tied to empirical knowledge and the design professional’s experience. Based on this, adequate designs are generated in terms of strength, operability, and durability. However, such designs can be optimized to reduce conditions associated with the structure’s design and execution, such as costs, CO2 emissions, and related earthworks. In this study, a new discretization technique based on reinforcement learning and transfer functions is developed. The application of metaheuristic techniques to the retaining wall problem is examined, defining two objective functions: cost and CO2 emissions. An extensive comparison is made with various metaheuristics and brute force methods, where the results show that the S-shaped transfer functions consistently yield more robust outcomes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11092104"
    },
    {
        "id": 5128,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 5129,
        "title": "Reinforcement Learning Based Self-play and State Stacking Techniques for Noisy Air Combat Environment",
        "authors": "Ahmet S. Tasbas, Safa O. Sahin, Nazim Kemal Üre",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1077"
    },
    {
        "id": 5130,
        "title": "Blockchain-driven optimization of IoT in mobile edge computing environment with deep reinforcement learning and multi-criteria decision-making techniques",
        "authors": "Komeil Moghaddasi, Mohammad Masdari",
        "published": "2023-11-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10586-023-04195-4"
    },
    {
        "id": 5131,
        "title": "Benchmarking Reinforcement Learning Techniques for Autonomous Navigation",
        "authors": "Zifan Xu, Bo Liu, Xuesu Xiao, Anirudh Nair, Peter Stone",
        "published": "2023-5-29",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160583"
    },
    {
        "id": 5132,
        "title": "Explainable Artificial Intelligence Techniques for the Analysis of Reinforcement Learning in Non-Linear Flight Regimes",
        "authors": "Gabriel de Haro Pizarroso, Erik-Jan Van Kampen",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2534"
    },
    {
        "id": 5133,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 5134,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 5135,
        "title": "Design optimum PI controller by reinforcement learning for tank level in industrial process",
        "authors": "Anwer Abdulkareem Ali, Mofeed Turky Rashid",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0156702"
    },
    {
        "id": 5136,
        "title": "Join Order Selection with Deep Reinforcement Learning: Fundamentals, Techniques, and Challenges",
        "authors": "Zhengtong Yan, Valter Uotila, Jiaheng Lu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Join Order Selection (JOS) is a fundamental challenge in query optimization, as it significantly affects query performance. However, finding an optimal join order is an NP-hard problem due to the exponentially large search space. Despite the decades-long effort, traditional methods still suffer from limitations. Deep Reinforcement Learning (DRL) approaches have recently gained growing interest and shown superior performance over traditional methods. These DRL-based methods could leverage prior experience through the trial-and-error strategy to automatically explore the optimal join order. This tutorial will focus on recent DRL-based approaches for join order selection by providing a comprehensive overview of the various approaches. We will start by briefly introducing the core concepts of join ordering and the traditional methods for JOS. Next, we will provide some preliminary knowledge about DRL and then delve into DRL-based join order selection approaches by offering detailed information on those methods, analyzing their relationships, and summarizing their weaknesses and strengths. To help the audience gain a deeper understanding of DRL approaches for JOS, we will present two open-source demonstrations and compare their differences. Finally, we will identify research challenges and open problems to provide insights into future research directions. This tutorial will provide valuable guidance for developing more practical DRL approaches for JOS.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14778/3611540.3611576"
    },
    {
        "id": 5137,
        "title": "Designing Traffic Management Strategies Using Reinforcement Learning Techniques",
        "authors": "Christine Taylor, Erik Vargo, Emily Bromberg, Tyler Manderfield",
        "published": "2023-10",
        "citations": 0,
        "abstract": " The future vision for traffic flow management is one that leverages advanced automation to assist human decision-makers in the identification of potential constraints and the development of resolution strategies. What makes this problem so challenging is the inherent uncertainty associated with forecasting these constraints, leaving human decision-makers reliant on experience to devise effective traffic management initiatives to mitigate demand in excess of resource capacity. This paper proposes to employ artificial intelligence-based methods to recommend traffic management initiatives under forecast uncertainty and to do so in a real-time planning context. The proposed algorithm consists of 1) a policy network that is generated offline using an Expert Iteration algorithm, 2) a statistical model that updates the likelihood of constraint futures based on observations, and 3) a Monte Carlo tree search algorithm that explores possible combinations of traffic management initiatives to identify the recommended actions for the current decision. The skill introduced by each of the algorithmic components is assessed for a case study focused on managing arrivals into the Atlanta Hartsfield–Jackson International Airport over 92 validation days. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/1.d0339"
    },
    {
        "id": 5138,
        "title": "Design Space Exploration of Approximate Computing Techniques with a Reinforcement Learning Approach",
        "authors": "Sepide Saeedi, Alessandro Savino, Stefano Di Carlo",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsn-w58399.2023.00047"
    },
    {
        "id": 5139,
        "title": "Reinforcement Learning-Based Task Scheduling Using DVFS Techniques in Mobile Devices",
        "authors": "Mohammadamin HajiKhodaverdian, Hamed Rastaghi, Milad Saadat, Hamed Shah-Mansouri",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pimrc56721.2023.10293955"
    },
    {
        "id": 5140,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 5141,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 5142,
        "title": "Edge device resource optimization (EDRO): evaluating the impact of reinforcement learning and deep reinforcement learning methods",
        "authors": "Rahul P, Hitaishi Roy, Sanglagna Khakhlary",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2024.0942"
    },
    {
        "id": 5143,
        "title": "Modern General Game Playing with Reinforcement Learning",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "In computational science and robotics, game playing and artificial intelligence (AI) are research areas that have long been studied. RL systems represent a major step towards autonomous systems that comprehend the visual world at an incredibly deep level and are poised to revolutionize the field of artificial intelligence (AI). Game Play is the testbench where environmental variables can be evaluated to generate adaptive, intelligent, or responsive behavior or simulate real-world scenarios. In this survey, we will explore general gameplay (GGP) with reinforcement learning and its various applications. This survey will cover central algorithms in deep RL, including the deep Qnetwork (DQN) and general mathematical and pragmatic approaches taken in the field. The objective of the current review is to examine the history, associations, and recent advances in the field of general game playing with reinforcement learning and its subfields. We conclude by describing several current areas of research within this field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.16"
    },
    {
        "id": 5144,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 5145,
        "title": "Comprehensive Review of Benefits from the Use of Sparse Updates Techniques in Reinforcement Learning: Experimental Simulations in Complex Action Space Environments",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciees58940.2023.10378830"
    },
    {
        "id": 5146,
        "title": "Classification of geogrid reinforcement in aggregate using machine learning techniques",
        "authors": "Samuel Olamide Aregbesola, Yong-Hoon Byun",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "AbstractThe present study proposes a novel ML methodology for differentiating between unstabilized aggregate specimens and those stabilized with triangular and rectangular aperture geogrids. This study utilizes the compiled experimental results obtained from stabilized and unstabilized specimens under repeated loading into a balanced, moderate-sized database. The efficacy of five ML models, including tree-ensemble and single-learning algorithms, in accurately identifying each specimen class was explored. Shapley’s additive explanation was used to understand the intricacies of the models and determine global feature importance ranking of the input variables. All the models could identify the unstabilized specimen with an accuracy of at least 0.9. The tree-ensemble models outperformed the single-learning models when all three classes (unstabilized specimens and specimens stabilized by triangular and rectangular aperture geogrids) were considered, with the light gradient boosting machine showing the best performance—an accuracy of 0.94 and an area under the curve score of 0.98. According to Shapley’s additive explanation, the resilient modulus and confining pressure were identified as the most important features across all models. Therefore, the proposed ML methodology may be effectively used to determine the type and presence of geogrid reinforcement in aggregates, based on a few aggregate material properties and performance under repeated loading.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40703-024-00206-4"
    },
    {
        "id": 5147,
        "title": "Exploring Reinforcement Learning Techniques in the Realm of Mobile Robotics",
        "authors": "Saim Ahmed, Ahmad Taher Azar, Muhammad Zeeshan Sardar, Zeeshan Haider, Nashwa Ahmad Kamal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijaac.2024.10062261"
    },
    {
        "id": 5148,
        "title": "Using Deep Reinforcement Learning for Assessing the Consequences of Cyber Mitigation Techniques on Industrial Control Systems",
        "authors": "Terry Merz, Romarie Morales Rosado",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "This paper discusses an in-progress study involving the use of deep reinforcement learning (DRL) to mitigate the effects of an advanced cyber-attack against industrial control systems (ICS).  The research is a qualitative, exploratory study which emerged as a gap during the execution of two rapid prototyping studies.  During these studies, cyber defensive procedures, known as “Mitigation, were characterized as actions taken to minimize the impact of ongoing advanced cyber-attacks against an ICS while enabling primary operations to continue.  To execute Mitigation procedures, affected ICS components required rapid isolation and quarantining from “healthy” system segments. However today, with most attacks leveraging automation, mitigation also requires rapid decision-making capabilities operating at the speed of automation yet with human-like refinement.  The authors settled on the choice of DRL as a viable solution to this problem due to the algorithm’s designs which involves “intelligent” decisions based upon continuous learning achieved through a rewards system.  The primary theory of this study posits that processes informed by data sources relative to the execution path of an advanced cyber-attack as well as the consequences of deploying a particular Mitigation procedure evolve the system into an ever-improving defensive capability.  This study seeks to produce a defensive DLR based software agent trained by a DRL based offensive software agent that generates policy refinements based upon extrapolations from a corrupted network state as reported by an IDS and baseline data. Results include an estimation rule that would quantify impacts of various mitigation actions while protecting the operational critical path and isolating an in-progress attack.  This study is in a conceptual phase and development has not started.\r\nThis research questions for this study are:\r\nRQ1: Can this software agent categorize correctly an in-progress cyber-attack and extrapolate the potential ICS assets affected?\r\nRQ2: Can this software agent categorize novel cyber-attacks and extrapolate a probable attack vector while enumerating affected assets?\r\nRQ3: Can this software agent characterize how operations are affected by quarantine actions?\r\nRQ4: Can this software agent generate a set of ranked recommended courses of action by effectiveness, and least negative effects on the operational critical path?",
        "keywords": "",
        "link": "http://dx.doi.org/10.34190/iccws.18.1.1063"
    },
    {
        "id": 5149,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 5150,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 5151,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 5152,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 5153,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 5154,
        "title": "A Survey of Sim-to-Real Transfer Techniques Applied to Reinforcement Learning for Bioinspired Robots",
        "authors": "Wei Zhu, Xian Guo, Dai Owaki, Kyo Kutsuzawa, Mitsuhiro Hayashibe",
        "published": "2023-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3112718"
    },
    {
        "id": 5155,
        "title": "AIM-RL: A New Framework Supporting Reinforcement Learning Experiments",
        "authors": "Ionuţ-Cristian Pistol, Andrei Arusoaie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091100003538"
    },
    {
        "id": 5156,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 5157,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 5158,
        "title": "Combining Forecasting and Multi-Agent Reinforcement Learning Techniques on Power Grid Scheduling Task",
        "authors": "Cheng Yang, Jihai Zhang, Fangquan Lin, Li Wang, Wei Jiang, Hanwei Zhang",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eebda56825.2023.10090669"
    },
    {
        "id": 5159,
        "title": "Teaching Reinforcement Learning Agents via Reinforcement Learning",
        "authors": "Kun Yang, Chengshuai Shi, Cong Shen",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089695"
    },
    {
        "id": 5160,
        "title": "A Review of Techniques and Policies on Cybersecurity Using Artificial Intelligence and Reinforcement Learning Algorithms",
        "authors": "Neshat Elhami Fard, Rastko R. Selmic, Khashayar Khorasani",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mts.2023.3306540"
    },
    {
        "id": 5161,
        "title": "Molecule Builder: Environment for Testing Reinforcement Learning Agents",
        "authors": "Petr Hyner, Jan Hůla, Mikoláš Janota",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012257900003595"
    },
    {
        "id": 5162,
        "title": "Automating XSS Vulnerability Testing Using Reinforcement Learning",
        "authors": "Kento Hasegawa, Seira Hidano, Kazuhide Fukushima",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011653600003405"
    },
    {
        "id": 5163,
        "title": "Energy management investigation of multi-level grid power supplying micro-grid using reinforcement learning and improved PSO optimization",
        "authors": "Nabil Jalil Aklo, Mofeed Turky Rashid",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0154603"
    },
    {
        "id": 5164,
        "title": "Pre- and post-processing techniques for reinforcement-learning-based routing and spectrum assignment in elastic optical networks",
        "authors": "Takafumi Tanaka, Masayuki Shimoda",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Research on the routing and spectrum assignment (RSA) problem has long been conducted with the aim of efficiently utilizing the frequency resources of optical networks. Given the recent progress in machine learning (ML) technology, it has been reported that the application of ML in various areas of optical network design, operation, and management has the potential to bring about new innovations such as autonomous optical network operation and highly accurate estimation of network conditions. With regard to the RSA problem, it is expected that an algorithm that achieves better accommodation efficiency than conventional heuristic methods can be realized by applying reinforcement learning (RL), which well supports training in a simulation environment. In this paper, we introduce and evaluate three techniques devised to apply RL more effectively to elastic optical networks (EONs): two pre-processing techniques called link-axis positional encoding (LPE) and slot-axis positional encoding (SPE) and a post-processing technique named the assignable boundary slot mask. First, we build a simple model in which the state data of the optical network, including frequency slot utilization, are input to the neural network of the RSA agent in RL and show that this model has difficulty outperforming the conventional heuristic RSA algorithm. Next, we build an RSA agent model with the proposed techniques and simulate the accommodation of dynamic optical paths to quantitatively demonstrate that the blocking probability can be reduced by 17% compared to the conventional heuristic.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1364/jocn.503599"
    },
    {
        "id": 5165,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 5166,
        "title": "Intelligent Scheduling Based on Reinforcement Learning Approaches: Applying Advanced Q-Learning and State–Action–Reward–State–Action Reinforcement Learning Models for the Optimisation of Job Shop Scheduling Problems",
        "authors": "Atefeh Momenikorbekandi, Maysam Abbod",
        "published": "2023-11-23",
        "citations": 1,
        "abstract": "Flexible job shop scheduling problems (FJSPs) have attracted significant research interest because they can considerably increase production efficiency in terms of energy, cost and time; they are considered the main part of the manufacturing systems which frequently need to be resolved to manage the variations in production requirements. In this study, novel reinforcement learning (RL) models, including advanced Q-learning (QRL) and RL-based state–action–reward–state–action (SARSA) models, are proposed to enhance the scheduling performance of FJSPs, in order to reduce the total makespan. To more accurately depict the problem realities, two categories of simulated single-machine job shops and multi-machine job shops, as well as the scheduling of a furnace model, are used to compare the learning impact and performance of the novel RL models to other algorithms. FJSPs are challenging to resolve and are considered non-deterministic polynomial-time hardness (NP-hard) problems. Numerous algorithms have been used previously to solve FJSPs. However, because their key parameters cannot be effectively changed dynamically throughout the computation process, the effectiveness and quality of the solutions fail to meet production standards. Consequently, in this research, developed RL models are presented. The efficacy and benefits of the suggested SARSA method for solving FJSPs are shown by extensive computer testing and comparisons. As a result, this can be a competitive algorithm for FJSPs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12234752"
    },
    {
        "id": 5167,
        "title": "Energy-Aware MPTCP Scheduling in Heterogeneous Wireless Networks Using Multi-Agent Deep Reinforcement Learning Techniques",
        "authors": "Zulfiqar Ali Arain, Xuesong Qiu, Changqiao Xu, Mu Wang, Mussadiq Abdul Rahim",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "This paper proposes an energy-efficient scheduling scheme for multi-path TCP (MPTCP) in heterogeneous wireless networks, aiming to minimize energy consumption while ensuring low latency and high throughput. Each MPTCP sub-flow is controlled by an agent that cooperates with other agents using the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. This approach enables the agents to learn decentralized policies through centralized training and decentralized execution. The scheduling problem is modeled as a multi-agent decision-making task. The proposed energy-efficient scheduling scheme, referred to as EE-MADDPG, demonstrates significant energy savings while maintaining lower latency and higher throughput compared to other state-of-the-art scheduling techniques. By adopting a multi-agent deep reinforcement learning approach, the agents can learn efficient scheduling policies that optimize various performance metrics in heterogeneous wireless networks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12214496"
    },
    {
        "id": 5168,
        "title": "Bird’s Eye View Map for End-to-end Autonomous Driving Using Reinforcement Learning",
        "authors": "Zhiqi Mao, Zhihang Song, Lihui Peng, Jianming Hu, Danya Yao, Yi Zhang",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ist59124.2023.10355730"
    },
    {
        "id": 5169,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 5170,
        "title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012082200003541"
    },
    {
        "id": 5171,
        "title": "Microgrid based VANET monitoring and energy management in 5G networks by reinforcement deep learning techniques",
        "authors": "A. Selvakumar, S. Ramesh, T. Manikandan, G. Michael, U. Arul, R. Gnanajeyaraman",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2023.108933"
    },
    {
        "id": 5172,
        "title": "Investigating Suitable Combinations of Dynamic Models and Control Techniques for Offline Reinforcement Learning Based Navigation: Application of Universal Omni-Wheeled Robots",
        "authors": "Nalaka Amarasiri, Alan A. Barhorst, Raju Gottumukkala",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "Abstract\nOmnidirectional locomotion provides wheeled mobile robots (WMR) with better maneuverability and flexibility, which enhances their energy efficiency and dexterity. Universal omni-wheels are one of the best categories of wheels that can be used to develop a WMR (Amarasiri et al., 2022, “Robust Dynamic Modeling and Trajectory Tracking Controller of a Universal Omni-Wheeled Mobile Robot,” ASME Letters Dyn. Sys. Control., 2(4), p. 040902. 10.1115/1.4055690). We study dynamic modeling and controllers for mobile robots to train in a reinforcement learning (RL)-based navigation algorithm. RL tasks require copious amounts of learning iteration episodes, which makes training very time consuming. The choice of dynamic model and controller has a significant impact on training time. In this paper, we compare a traditional Kane’s equations model to a non-holonomic canonical momenta model (Barhorst, 2019, “Generalized Momenta in Constrained Non-Holonomic Systems—Another Perspective on the Canonical Equations of Motion,” Int. J. Non-Linear Mech., 113, pp. 128–145.). We implemented four controllers: proportional integral derivative, linear quadratic regulator with integral action, pole placement, and a full nonlinear sliding mode controller. We summarize the pros and cons of each of the modeling techniques, and control laws implemented. The outcomes of our analysis will improve RL training time for path generation in unstructured environments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4064517"
    },
    {
        "id": 5173,
        "title": "A simplified reinforcement: The Florida sleeve Ross procedure",
        "authors": "Stephen M. Spindel, Christopher M. Zumwalt, Jasmine Su, Autumn P. Stevenson",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.xjtc.2024.02.016"
    },
    {
        "id": 5174,
        "title": "Factory Simulation of Optimization Techniques Based on Deep Reinforcement Learning for Storage Devices",
        "authors": "Ju-Bin Lim, Jongpil Jeong",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "In this study, reinforcement learning (RL) was used in factory simulation to optimize storage devices for use in Industry 4.0 and digital twins. Industry 4.0 is increasing productivity and efficiency in manufacturing through automation, data exchange, and the integration of new technologies. Innovative technologies such as the Internet of Things (IoT), artificial intelligence (AI), and big data analytics are smartly automating manufacturing processes and integrating data with production systems to monitor and analyze production data in real time and optimize factory operations. A digital twin is a digital model of a physical product or process in the real world. It is built on data and real-time information collected through sensors and accurately simulates the behavior and performance of a real-world manufacturing floor. With a digital twin, one can leverage data at every stage of product design, development, manufacturing, and maintenance to predict, solve, and optimize problems. First, we defined an RL environment, modeled it, and validated its ability to simulate a real physical system. Subsequently, we introduced a method to calculate reward signals and apply them to the environment to ensure the alignment of the behavior of the RL agent with the task objective. Traditional approaches use simple reward functions to tune the behavior of reinforcement learning agents. These approaches issue rewards according to predefined rules and often use reward signals that are unrelated to the task goal. However, in this study, the reward signal calculation method was modified to consider the task goal and the characteristics of the physical system and calculate more realistic and meaningful rewards. This method reflects the complex interactions and constraints that occur during the optimization process of the storage device and generates more accurate episodes of reinforcement learning in agent behavior. Unlike the traditional simple reward function, this reflects the complexity and realism of the storage optimization task, making the reward more sophisticated and effective.The stocker simulation model was used to validate the effectiveness of RL. The model is a storage device that simulates logistics in a manufacturing production area. The results revealed that RL is a useful tool for automating and optimizing complex logistics systems, increasing the applicability of RL in logistics. We proposed a novel method for creating an agent through learning using the proximal policy optimization algorithm, and the agent was optimized by configuring various learning options. The application of reinforcement learning resulted in an effectiveness of 30–100%, and the methods can be expanded to other fields.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13179690"
    },
    {
        "id": 5175,
        "title": "Reinventing Astronomical Survey Scheduling with Reinforcement Learning: Unveiling the Potential of Self-Driving Telescope",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2246942"
    },
    {
        "id": 5176,
        "title": "Multi-Environment Training Against Reward Poisoning Attacks on Deep Reinforcement Learning",
        "authors": "Myria Bouhaddi, Kamel Adi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012139900003555"
    },
    {
        "id": 5177,
        "title": "Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control",
        "authors": "Rohit Bokade, Xiaoning Jin, Christopher Amato",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3275883"
    },
    {
        "id": 5178,
        "title": "A Recent Publications Survey on Reinforcement Learning for Selecting Parameters of Meta-Heuristic and Machine Learning Algorithms",
        "authors": "Maria Chernigovskaya, Andrey Kharitonov, Klaus Turowski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011954300003488"
    },
    {
        "id": 5179,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 5180,
        "title": "Multisite gaming streaming optimization over virtualized 5G environment using Deep Reinforcement Learning techniques",
        "authors": "Alberto del Rio, Javier Serrano, David Jimenez, Luis M. Contreras, Federico Alvarez",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2024.110334"
    },
    {
        "id": 5181,
        "title": "Research on Inertial Navigation Technology of Unmanned Aerial Vehicles with Integrated Reinforcement Learning Algorithm",
        "authors": "",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "With the continuous expansion of unmanned aerial vehicle (UAV) applications, traditional inertial navigation technology exhibits significant limitations in complex environments. In this study, we integrate improved reinforcement learning (RL) algorithms to enhance existing unmanned aerial vehicle inertial navigation technology and introduce a modulated mechanism (MM) for adjusting the state of the intelligent agent in an innovative manner [1,2]. Through interaction with the environment, the intelligent machine can learn more effective navigation strategies [3]. The ultimate goal is to provide a foundation for autonomous navigation of unmanned aerial vehicles during flight and improve navigation accuracy and robustness. We first define appropriate state representation and action space, and then design an adjustment mechanism based on the actions selected by the intelligent agent. The adjustment mechanism outputs the next state and reward value of the agent. Additionally, the adjustment mechanism calculates the error between the adjusted state and the unadjusted state. Furthermore, the intelligent agent stores the acquired experience samples containing states and reward values in a buffer and replays the experiences during each iteration to learn the dynamic characteristics of the environment. We name the improved algorithm as the DQM algorithm. Experimental results demonstrate that the intelligent agent using our proposed algorithm effectively reduces the accumulated errors of inertial navigation in dynamic environments. Although our research provides a basis for achieving autonomous navigation of unmanned aerial vehicles, there is still room for significant optimization. Further research can include testing unmanned aerial vehicles in simulated environments, testing unmanned aerial vehicles in realworld environments, optimizing the design of reward functions, improving the algorithm workflow to enhance convergence speed and performance, and enhancing the algorithm's generalization ability. It has been proven that by integrating reinforcement learning algorithms, unmanned aerial vehicles can achieve autonomous navigation, thereby improving navigation accuracy and robustness in dynamic and changing environments [4]. Therefore, this research plays an important role in promoting the development and application of unmanned aerial vehicle technology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.06"
    },
    {
        "id": 5182,
        "title": "Comprehensive Review of Benefits from the Use of Neuron Connection Pruning Techniques During the Training Process of Artificial Neural Networks in Reinforcement Learning: Experimental Simulations in Atari Games",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304968"
    },
    {
        "id": 5183,
        "title": "Unlocking Autonomous Telescopes through Reinforcement Learning: An Offline Framework and Insights from a Case Study",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/2217169"
    },
    {
        "id": 5184,
        "title": "Towards Self-Adaptive Resilient Swarms Using Multi-Agent Reinforcement Learning",
        "authors": "Rafael Pina, Varuna De Silva, Corentin Artaud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012462800003654"
    },
    {
        "id": 5185,
        "title": "PenGym: Pentesting Training Framework for Reinforcement Learning Agents",
        "authors": "Thanh Nguyen, Zhi Chen, Kento Hasegawa, Kazuhide Fukushima, Razvan Beuran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012367300003648"
    },
    {
        "id": 5186,
        "title": "ADAPTIVE LEARNING THROUGH AI: REINFORCEMENT LEARNING IN TEACHING MULTIPLICATION TABLES",
        "authors": "Lara Drožđek, Igor Pesek",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/inted.2024.1186"
    },
    {
        "id": 5187,
        "title": "Optimization of a Deep Reinforcement Learning Policy for Construction Manufacturing Control",
        "authors": "Ian Flood, Xiaoyan Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091400003546"
    },
    {
        "id": 5188,
        "title": "Safe Q-Learning Approaches for Human-in-Loop Reinforcement Learning",
        "authors": "Swathi Veerabathraswamy, Nirav Bhatt",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442899"
    },
    {
        "id": 5189,
        "title": "Welcome to the Jungle: A Conceptual Comparison of Reinforcement Learning Algorithms",
        "authors": "Kenneth Schröder, Alexander Kastius, Rainer Schlosser",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011626700003396"
    },
    {
        "id": 5190,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 5191,
        "title": "Quantum reinforcement learning",
        "authors": "Niels M. P. Neumann, Paolo B. U. L. de Heer, Frank Phillipson",
        "published": "2023-2-22",
        "citations": 1,
        "abstract": "AbstractIn this paper, we present implementations of an annealing-based and a gate-based quantum computing approach for finding the optimal policy to traverse a grid and compare them to a classical deep reinforcement learning approach. We extended these three approaches by allowing for stochastic actions instead of deterministic actions and by introducing a new learning technique called curriculum learning. With curriculum learning, we gradually increase the complexity of the environment and we find that it has a positive effect on the expected reward of a traversal. We see that the number of training steps needed for the two quantum approaches is lower than that needed for the classical approach.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11128-023-03867-9"
    },
    {
        "id": 5192,
        "title": "Dynamic Path Planning for Autonomous Vehicles Using Adaptive Reinforcement Learning",
        "authors": "Karim Wahdan, Nourhan Ehab, Yasmin Mansy, Amr El Mougy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363300003636"
    },
    {
        "id": 5193,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 5194,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 5195,
        "title": "Networked Personalized Federated Learning Using Reinforcement Learning",
        "authors": "Francois Gauthier, Vinay Chakravarthi Gogineni, Stefan Werner",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279781"
    },
    {
        "id": 5196,
        "title": "Survey on Practical Reinforcement Learning : from Imitation Learning to Offline Reinforcement Learning",
        "authors": "Dongsu Lee, Chanin Eom, Sungwoo Choi, Sungkwan Kim, Minhae Kwon",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.11.1405"
    },
    {
        "id": 5197,
        "title": "Subgoal Reachability in Goal Conditioned Hierarchical Reinforcement Learning",
        "authors": "Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Bartłomiej Sobieski, Tomasz Trzciński",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012326200003636"
    },
    {
        "id": 5198,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 5199,
        "title": "Advancements in Deep Reinforcement Learning and Inverse Reinforcement Learning for Robotic Manipulation: Toward Trustworthy, Interpretable, and Explainable Artificial Intelligence",
        "authors": "Recep Ozalp, Aysegul Ucar, Cuneyt Guzelis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3385426"
    },
    {
        "id": 5200,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 5201,
        "title": "Multi-Fidelity Reinforcement Learning with Control Variates",
        "authors": "Sami Khairy, Prasanna Balaprakash",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-181"
    },
    {
        "id": 5202,
        "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
        "authors": "Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.977"
    },
    {
        "id": 5203,
        "title": "Quality Metrics for Reinforcement Learning for Edge Cloud and Internet-of-Things Systems",
        "authors": "Claus Pahl, Hamid Barzegar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194800003584"
    },
    {
        "id": 5204,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 5205,
        "title": "Learning Quadruped Locomotion Method Integrating Meta-Learning and Reinforcement Learning",
        "authors": "Xiafu Lv, Runming He, Chuangshi Wang, Yang Ou, Hong Wang, Zhenglin Chen",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450573"
    },
    {
        "id": 5206,
        "title": "Prior-Information Enhanced Reinforcement Learning for Energy Management Systems",
        "authors": "Th´eo Zangato, Aomar Osmani, Pegah Alizadeh",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Amidst increasing energy demands and growing environmental concerns, the promotion of sustainable and energy-efficient practices has become imperative. This paper introduces a reinforcement learning-based technique for optimizing energy consumption and its associated costs, with a focus on energy management systems. A three-step approach for the efficient management of charging cycles in energy storage units within buildings is presented combining RL with prior knowledge. A unique strategy is adopted: clustering building load curves to discern typical energy consumption patterns, embedding domain knowledge into the learning algorithm to refine the agent’s action space and predicting of future observations to make real-time decisions. We showcase the effectiveness of our method using real-world data. It enables controlled exploration and efficient training of Energy Management System (EMS) agents. When compared to the benchmark, our model reduces energy costs by up to 15%, cutting down consumption during peak periods, and demonstrating adaptability across various building consumption profiles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2024.140207"
    },
    {
        "id": 5207,
        "title": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",
        "authors": "Laura L. Pullum",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) has received significant interest in recent years, primarily because of the success of deep RL in solving many challenging tasks, such as playing chess, Go, and online computer games. However, with the increasing focus on RL, applications outside gaming and simulated environments require an understanding of the robustness, stability, and resilience of RL methods. To this end, we conducted a comprehensive literature review to characterize the available literature on these three behaviors as they pertain to RL. We classified the quantitative and theoretical approaches used to indicate or measure robustness, stability, and resilience behaviors. In addition, we identified the actions or events to which the quantitative approaches attempted to be stable, robust, or resilient. Finally, we provide a decision tree that is useful for selecting metrics to quantify behavior. We believe that this is the first comprehensive review of stability, robustness, and resilience, specifically geared toward RL.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130205"
    },
    {
        "id": 5208,
        "title": "Rapid Speed Change for Quadruped Robots via Deep Reinforcement Learning",
        "authors": "Seung Gyu Roh",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364428"
    },
    {
        "id": 5209,
        "title": "Longer duration intertrial intervals without visual stimuli have reinforcement value and increase the rate of reinforcement and punishment learning in computer-based discriminations in humans",
        "authors": "Xiaojin Ma, Blair Bracciano, Nicole Hoppas, Sydney Zimmerman, Charles L. Pickens",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.lmot.2022.101867"
    },
    {
        "id": 5210,
        "title": "Learning Policies for Neural Network Architecture Optimization Using Reinforcement Learning",
        "authors": "Raghav Vadhera, Manfred Huber",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Deep learning systems tend to be very sensitive to the specific network architecture both in terms of learning ability and performance of the learned solution. This, together with the difficulty of tuning neural network architectures leads to a need for automatic network optimization. Previous work largely optimizes a network for one specific problem using architecture search, requiring significant amounts of time training different architectures during optimization. To address this and to open up the potential for transfer across tasks, this paper presents a novel approach that uses Reinforcement Learning to learn a policy for network optimization in a derived architecture embedding space that incrementally optimizes the network for the given problem. By utilizing policy learning and an abstract problem embedding, this approach brings the promise of transfer of the policy across problems and thus the potential optimization of networks for new problems without the need for excessive additional training. For an initial evaluation of the base capabilities, experiments for a standard classification problem are performed in this paper, showing the ability of the approach to optimize the architecture for a specific problem within a given rang of fully connected networks, and indicating its potential for learning effective policies to automatically improve network architectures.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32473/flairs.36.133380"
    },
    {
        "id": 5211,
        "title": "Knowledge Distillation and Reward Shaping of Deep Reinforcement Learning",
        "authors": "Yue Mei",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424889"
    },
    {
        "id": 5212,
        "title": "Exploring the Synergy of Prompt Engineering and Reinforcement Learning for Enhanced Control and Responsiveness in Chat GPT",
        "authors": "",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "Conversational AI systems, such as Chat GPT, have exhibited remarkable performance in generating human-like responses. However, achieving consistent control and responsiveness remains a challenge. This research paper explores the combined effects of prompt engineering and reinforcement learning techniques in enhancing control and responsiveness in Chat GPT. Our experiments demonstrate significant improvements in the model’s performance across diverse domains and tasks. We discuss the implications of these findings for various real-world applications, such as customer support, virtual assistants, content generation, and education, and provide insights into future research directions and ethical considerations in the development of more reliable, controllable, and effective conversational AI systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.02"
    },
    {
        "id": 5213,
        "title": "Design of PID Controller using Reinforcement Learning",
        "authors": "Ashis De, Barun Mazumdar, Aritra Dhabal, Saikat Bhattacharjee, Aridip Maity, Sourav Bandopadhyay",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55248/gengpi.4.1123.113004"
    },
    {
        "id": 5214,
        "title": "DRL4HFC: Deep Reinforcement Learning for Container-Based Scheduling in Hybrid Fog/Cloud System",
        "authors": "Ameni Kallel, Molka Rekik, Mahdi Khemakhem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012356800003636"
    },
    {
        "id": 5215,
        "title": "Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments",
        "authors": "Georg Kruse, Theodora-Augustina Drăgan, Robert Wille, Jeanette Miriam Lorenz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012353100003636"
    },
    {
        "id": 5216,
        "title": "Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning",
        "authors": "Zhichao Chen, Yutaka Nakamura, Hiroshi Ishiguro",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3267385"
    },
    {
        "id": 5217,
        "title": "A deep actor critic reinforcement learning framework for learning to rank",
        "authors": "Vaibhav Padhye, Kailasam Lakshmanan",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126314"
    },
    {
        "id": 5218,
        "title": "Equivariant Reinforcement Learning for Quadrotor UAV",
        "authors": "Beomyeol Yu, Taeyoung Lee",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156379"
    },
    {
        "id": 5219,
        "title": "Environment Adversarial Reinforcement Learning",
        "authors": "John R. Cooper",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2405"
    },
    {
        "id": 5220,
        "title": "Rainfuzz: Reinforcement-Learning Driven Heat-Maps for Boosting Coverage-Guided Fuzzing",
        "authors": "Lorenzo Binosi, Luca Rullo, Mario Polino, Michele Carminati, Stefano Zanero",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011625300003411"
    },
    {
        "id": 5221,
        "title": "Reinforcement Learning for Radar Waveform Optimization",
        "authors": "Mario Coutino, Faruk Uysal",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149794"
    },
    {
        "id": 5222,
        "title": "A Study Toward Multi-Objective Multiagent Reinforcement Learning Considering Worst Case and Fairness Among Agents",
        "authors": "Toshihiro Matsui",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011687100003393"
    },
    {
        "id": 5223,
        "title": "Combining Deep Learning on Order Books with Reinforcement Learning for Profitable Trading",
        "authors": "Koti Jaddu, Paul Bilokon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4611708"
    },
    {
        "id": 5224,
        "title": "RL-Ripper:",
        "authors": "Upma Gandhi, Erfan Aghaeekiasaraee, Ismail S. K. Bustany, Payam Mousavi, Matthew E. Taylor, Laleh Behjat",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583781.3590312"
    },
    {
        "id": 5225,
        "title": "Multi-Domain Active Learning for Multi-Agent Reinforcement Learning",
        "authors": "Devarani Devi Ningombam",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icidea59866.2023.10295208"
    },
    {
        "id": 5226,
        "title": "Multi‐agent reinforcement learning for process control: Exploring the intersection between fields of reinforcement learning, control theory, and game theory",
        "authors": "Yue Yifei, Samavedham Lakshminarayanan",
        "published": "2023-11",
        "citations": 3,
        "abstract": "AbstractThe application of reinforcement learning (RL) in process control has garnered increasing research attention. However, much of the current literature is focused on training and deploying a single RL agent. The application of multi‐agent reinforcement learning (MARL) has not been fully explored in process control. This work aims to: (i) develop a unique RL agent configuration that is suitable in a MARL control system for multiloop control, (ii) demonstrate the efficacy of MARL systems in controlling multiloop process that even exhibit strong interactions, and (iii) conduct a comparative study of the performance of MARL systems trained with different game‐theoretic strategies. First, we propose a design of an RL agent configuration that combines the functionalities of a feedback controller and a decoupler in a control loop. Thereafter, we deploy two such agents to form a MARL system that learns how to control a two‐input, two‐output system that exhibits strong interactions. After training, the MARL system shows effective control performance on the process. With further simulations, we examine how the MARL control system performs with increasing levels of process interaction and when trained with reward function configurations based on different game‐theoretic strategies (i.e., pure cooperation and mixed strategies). The results show that the performance of the MARL system is weakly dependent on the reward function configuration for systems with weak to moderate loop interactions. The MARL system with mixed strategies appears to perform marginally better than MARL under pure cooperation in systems with very strong loop interactions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cjce.24878"
    },
    {
        "id": 5227,
        "title": "An Industrial Use-Case for Reinforcement Learning: Optimizing a Production Planning in Stochastic Conditions",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/ijml.2024.14.1.1152"
    },
    {
        "id": 5228,
        "title": "Optimizing Naval Movement Using Deep Reinforcement Learning",
        "authors": "Joseph Coble, Armon Barton, Chris Darken, Scotty Black",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00062"
    },
    {
        "id": 5229,
        "title": "Reinforcement Learning and Advanced Reinforcement Learning to Improve Autonomous Vehicle Planning",
        "authors": "Avinash. J. Agrawal, Rashmi R. Welekar, Namita Parati, Pravin R. Satav, Uma Patel Thakur, Archana V. Potnurwar",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "Planning for autonomous vehicles is a challenging process that involves navigating through dynamic and unpredictable surroundings while making judgments in real-time. Traditional planning methods sometimes rely on predetermined rules or customized heuristics, which could not generalize well to various driving conditions. In this article, we provide a unique framework to enhance autonomous vehicle planning by fusing conventional RL methods with cutting-edge reinforcement learning techniques. To handle many elements of planning issues, our system integrates cutting-edge algorithms including deep reinforcement learning, hierarchical reinforcement learning, and meta-learning. Our framework helps autonomous vehicles make decisions that are more reliable and effective by utilizing the advantages of these cutting-edge strategies.With the use of the RLTT technique, an autonomous vehicle can learn about the intentions and preferences of human drivers by inferring the underlying reward function from expert behaviour that has been seen. The autonomous car can make safer and more human-like decisions by learning from expert demonstrations about the fundamental goals and limitations of driving. Large-scale simulations and practical experiments can be carried out to gauge the effectiveness of the suggested approach. On the basis of parameters like safety, effectiveness, and human likeness, the autonomous vehicle planning system's performance can be assessed. The outcomes of these assessments can help to inform future developments and offer insightful information about the strengths and weaknesses of the strategy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i7s.7526"
    },
    {
        "id": 5230,
        "title": "Reward Design for Deep Reinforcement Learning Towards Imparting Commonsense Knowledge in Text-Based Scenario",
        "authors": "Ryota Kubo, Fumito Uwano, Manabu Ohta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012456900003636"
    },
    {
        "id": 5231,
        "title": "Improving Intrusion Detection Systems with Multi-Agent Deep Reinforcement Learning: Enhanced Centralized and Decentralized Approaches",
        "authors": "Amani Bacha, Farah Ktata, Faten Louati",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012124600003555"
    },
    {
        "id": 5232,
        "title": "Deep Reinforcement Learning",
        "authors": "Moez Krichen",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10306453"
    },
    {
        "id": 5233,
        "title": "Data Augmentation Through Expert-Guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning",
        "authors": "Giorgio Angelotti, Nicolas Drougard, Caroline Chanel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011633400003393"
    },
    {
        "id": 5234,
        "title": "Research on Image Recognition based on Reinforcement Learning",
        "authors": "Jiabin Luo, Rongzhen Luo",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166036"
    },
    {
        "id": 5235,
        "title": "Contrastive Learning Methods for Deep Reinforcement Learning",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3312383"
    },
    {
        "id": 5236,
        "title": "A reinforcement learning algorithm for scheduling parallel processors with identical speedup functions",
        "authors": "Farid Ziaei, Mohammad Ranjbar",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2023.100485"
    },
    {
        "id": 5237,
        "title": "Learning to Score: Tuning Cluster Schedulers through Reinforcement Learning",
        "authors": "Martin Asenov, Qiwen Deng, Gingfung Yeung, Adam Barker",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic2e59103.2023.00021"
    },
    {
        "id": 5238,
        "title": "Punisher: A Deep Reinforcement Learning Model Trained by Correcting Bad Actions",
        "authors": "Jianyi Yang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424763"
    },
    {
        "id": 5239,
        "title": "Deep Reinforcement Learning Guided Decision Tree Learning For Program Synthesis",
        "authors": "Mingrui Yang, Dalin Zhang",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/saner56733.2023.00112"
    },
    {
        "id": 5240,
        "title": "Exponential TD Learning: A Risk-Sensitive Actor-Critic Reinforcement Learning Algorithm",
        "authors": "Erfaun Noorani, Christos N. Mavridis, John S. Baras",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156626"
    },
    {
        "id": 5241,
        "title": "Learning and Adapting Behavior of Autonomous Vehicles through Inverse Reinforcement Learning",
        "authors": "Rainer Trauth, Marc Kaufeld, Maximilian Geisslinger, Johannes Betz",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186668"
    },
    {
        "id": 5242,
        "title": "On Selecting Optimal Hyperparameters for Reinforcement Learning Based Robotics Applications: A Practical Approach",
        "authors": "Ignacio Fidalgo, Guillermo Villate, Alberto Tellaeche, Juan Vázquez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012158200003543"
    },
    {
        "id": 5243,
        "title": "Deep Reinforcement Learning-based Quantization for Federated Learning",
        "authors": "Sihui Zheng, Yuhan Dong, Xiang Chen",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wcnc55385.2023.10118781"
    },
    {
        "id": 5244,
        "title": "Exploring the Potential of Reinforcement Learning for Applications in Hydrogen Energy Systems",
        "authors": "Yantang Li, Yang Miao",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10328000"
    },
    {
        "id": 5245,
        "title": "An Adaptive Agent Decision Model Based on Deep Reinforcement  Learning and Autonomous Learning",
        "authors": "",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33168/jliss.2023.0309"
    },
    {
        "id": 5246,
        "title": "Optimizing Multi Commodity Flow Problem Under Uncertainty: A Deep Reinforcement Learning Approach",
        "authors": "Bulent Soykan, Ghaith Rabadi",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00191"
    },
    {
        "id": 5247,
        "title": "Comparison of reinforcement learning techniques for controlling a CSTR process",
        "authors": "Eric Monteiro L. Luz, Wouter Caarls",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43153-023-00422-y"
    },
    {
        "id": 5248,
        "title": "Leveraging Empowerment to Model Tool Use in Reinforcement Learning",
        "authors": "Faizan Rasheed, Daniel Polani, Nicola Catenacci Volpi",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364342"
    },
    {
        "id": 5249,
        "title": "Learning Pessimism for Reinforcement Learning",
        "authors": "Edoardo Cetin, Oya Celiktutan",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25852"
    },
    {
        "id": 5250,
        "title": "A Personalized E-Learning System Using Reinforcement Learning Through Satellite",
        "authors": "Pooja Kiran, B K Swathi Prasad, Jayahar Sivasubramanian",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440852"
    },
    {
        "id": 5251,
        "title": "Research advanced in the integration of federated learning and reinforcement learning",
        "authors": "Chengan Li",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) and federated learning (FL) are two important machine learning paradigms. Reinforcement learning is concerned with enabling intelligence to learn optimal policies when interacting with an environment, while federated learning is concerned with collaboratively training models on distributed equipment while preserving data privacy. In recent years, the fusion and complementarity of reinforcement learning, and federated learning have attracted increasing research interest, providing new directions for the development of the machine learning community. Focusing on the integration of reinforcement learning and federated learning, this paper introduces in detail the latest technological developments in the integration of reinforcement learning and federated learning, and discusses the main challenges, existing methods and future directions of this intersection. Specifically, based on the introduction of classical reinforcement learning and federated learning. In addition, this document introduces cutting-edge results on the integration of reinforcement learning and joint learning and discusses the problems and future directions of the integration.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/40/20230641"
    },
    {
        "id": 5252,
        "title": "Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents<sup>*</sup>",
        "authors": "Foozhan Ataiefard, Hadi Hemmati",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00099"
    },
    {
        "id": 5253,
        "title": "Distributed Randomized Multiagent Policy Iteration in Reinforcement Learning",
        "authors": "Weipeng Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4489715"
    },
    {
        "id": 5254,
        "title": "Deep Reinforcement Learning-Based Multirestricted Dynamic-Request Transportation Framework",
        "authors": "Erdal Akin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3341471"
    },
    {
        "id": 5255,
        "title": "Comparing deep reinforcement learning architectures for autonomous racing",
        "authors": "Benjamin David Evans, Hendrik Willem Jordaan, Herman Arnold Engelbrecht",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2023.100496"
    },
    {
        "id": 5256,
        "title": "PARL: A Dialog System Framework with Prompts as Actions for Reinforcement Learning",
        "authors": "Tao Xiang, Yangzhe Li, Monika Wintergerst, Ana Pecini, Dominika Młynarczyk, Georg Groh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011725200003393"
    },
    {
        "id": 5257,
        "title": "Adaptive Action Supervision in Reinforcement Learning from Real-World Multi-Agent Demonstrations",
        "authors": "Keisuke Fujii, Kazushi Tsutsui, Atom Scott, Hiroshi Nakahara, Naoya Takeishi, Yoshinobu Kawahara",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012261100003636"
    },
    {
        "id": 5258,
        "title": "A Deep Reinforcement Learning-Based Approach in Porker Game",
        "authors": "Yan Kong Yan Kong, Yefeng Rui Yan Kong, Chih-Hsien Hsia Yefeng Rui",
        "published": "2023-4",
        "citations": 1,
        "abstract": "\n                        <p>Recent years have witnessed the big success deep reinforcement learning achieved in the domain of card and board games, such as Go, chess and Texas Hold&rsquo;em poker. However, Dou Di Zhu, a traditional Chinese card game, is still a challenging task for deep reinforcement learning methods due to the enormous action space and the sparse and delayed reward of each action from the environment. Basic reinforcement learning algorithms are more effective in the simple environments which have small action spaces and valuable and concrete reward functions, and unfortunately, are shown not be able to deal with Dou Di Zhu satisfactorily. This work introduces an approach named Two-steps Q-Network based on DQN to playing Dou Di Zhu, which compresses the huge action space through dividing it into two parts according to the rules of Dou Di Zhu and fills in the sparse rewards using inverse reinforcement learning (IRL) through abstracting the reward function from experts&rsquo; demonstrations. It is illustrated by the experiments that two-steps Q-network gains great advancements compared with DQN used in Dou Di Zhu.</p>\n<p>&nbsp;</p>\n                    ",
        "keywords": "",
        "link": "http://dx.doi.org/10.53106/199115992023043402004"
    },
    {
        "id": 5259,
        "title": "Optimization Sliding Mode Controller Using Reinforcement Learning",
        "authors": "Somporn Tiacharoen",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incit60207.2023.10413158"
    },
    {
        "id": 5260,
        "title": "Applying Reinforcement Learning to Option Pricing and Hedging",
        "authors": "Zoran Stoiljkovic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4546371"
    },
    {
        "id": 5261,
        "title": "Advanced Statistical Arbitrage with Reinforcement Learning",
        "authors": "Boming Ning, Kiseop Lee",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4739373"
    },
    {
        "id": 5262,
        "title": "Reinforcement learning influences memory specificity across development",
        "authors": "Kate Nussenbaum, Catherine Hartley",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1096-0"
    },
    {
        "id": 5263,
        "title": "Autonomous pricing using policy gradient reinforcement learning",
        "authors": "Kevin Michael Frick",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4527452"
    },
    {
        "id": 5264,
        "title": "Proposal of a Signal Control Method Using Deep Reinforcement Learning with Pedestrian Traffic Flow",
        "authors": "Akimasa Murata, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011665000003393"
    },
    {
        "id": 5265,
        "title": "Estimation on Human Motion Posture using Improved Deep Reinforcement Learning",
        "authors": "Wenjing Ma Wenjing Ma, Jianguang Zhao Wenjing Ma, Guangquan Zhu Jianguang Zhao",
        "published": "2023-8",
        "citations": 0,
        "abstract": "\n                        <p>Estimating human motion posture can provide important data for intelligent monitoring systems, human-computer interaction, motion capture, and other fields. However, the traditional human motion posture estimation algorithm is difficult to achieve the goal of fast estimation of human motion posture. To address the problems of traditional algorithms, in the paper, we propose an estimation algorithm for human motion posture using improved deep reinforcement learning. First, the double deep Q network is constructed to improve the deep reinforcement learning algorithm. The improved deep reinforcement learning algorithm is used to locate the human motion posture coordinates and improve the effectiveness of bone point calibration. Second, the human motion posture analysis generative adversarial networks are constructed to realize the automatic recognition and analysis of human motion posture. Finally, using the preset human motion posture label, combined with the undirected graph model of the human, the human motion posture estimation is completed, and the precise estimation algorithm of the human motion posture is realized. Experiments are performed based on MPII Human Pose data set and HiEve data set. The results show that the proposed algorithm has higher positioning accuracy of joint nodes. The recognition effect of bone joint points is better, and the average is about 1.45%. The average posture accuracy is up to 98.2%, and the average joint point similarity is high. Therefore, it is proved that the proposed method has high application value in human-computer interaction, human motion capture and other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "keywords": "",
        "link": "http://dx.doi.org/10.53106/199115992023083404008"
    },
    {
        "id": 5266,
        "title": "Learning to Take Cover with Navigation-Based Waypoints via Reinforcement Learning",
        "authors": "Timothy Aris, Volkan Ustun, Rajay Kumar",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "This paper presents a reinforcement learning model designed to learn how to take cover on geo-specific terrains, an essential behavior component for military training simulations. Training of the models is performed on the Rapid Integration and Development Environment (RIDE) leveraging the Unity ML-Agents framework. This work expands on previous work on raycast-based agents by increasing the number of enemies from one to three. We demonstrate an automated way of generating training and testing data within geo-specific terrains. We show that replacing the action space with a more abstracted, navmesh-based waypoint movement system can increase the generality and success rate of the models while providing similar results to our previous paper's results regarding retraining across terrains. We also comprehensively evaluate the differences between these and the previous models. Finally, we show that incorporating pixels into the model's input can increase performance at the cost of longer training times.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32473/flairs.36.133348"
    },
    {
        "id": 5267,
        "title": "An Improved Deep Learning Based Test Case Prioritization Using Deep Reinforcement Learning",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.64"
    },
    {
        "id": 5268,
        "title": "Inverse Reinforcement Learning with Learning and Leveraging Demonstrators’ Varying Expertise Levels",
        "authors": "Somtochukwu Oguchienti, Mahsa Ghasemi",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313475"
    },
    {
        "id": 5269,
        "title": "Learning to Draw Through A Multi-Stage Environment Model Based Reinforcement Learning",
        "authors": "Ji Qiu, Peng Lu, Xujun Peng",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222280"
    },
    {
        "id": 5270,
        "title": "Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments",
        "authors": "Guanlin Meng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424815"
    },
    {
        "id": 5271,
        "title": "Control of Composite Manufacturing Processes Through Deep Reinforcement Learning",
        "authors": "Simon Stieber, Leonard Heber, Christof Obertscheider, Wolfgang Reif",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00011"
    },
    {
        "id": 5272,
        "title": "Developments in Image Processing using Deep learning and Reinforcement learning",
        "authors": "Sarla More, Durgesh Mishra",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictbig59752.2023.10456257"
    },
    {
        "id": 5273,
        "title": "Curriculum learning for deep reinforcement learning in swarm robotic navigation task",
        "authors": "Alaa Iskandar, Béla Kovács",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "This study investigates the training of a swarm consisting of five E-puck robots using Deep reinforcement learning with curriculum learning in a 3D environment. The primary objective is to decompose the navigation task into a curriculum comprising progressively more challenging stages based on curriculum complexity metrics. These metrics encompass swarm size, collision avoidance complexity, and distances between targets and robots. The performance evaluation of the swarm includes key metrics such as success rate, collision rate, training efficiency, and generalization capabilities. To assess their effectiveness, a comparative analysis is conducted between curriculum learning and the proximal policy optimization algorithm. The results demonstrate that curriculum learning outperforms traditional one, yielding higher success rates, improved collision avoidance, and enhanced training efficiency. The trained swarm also exhibits robust generalization for novel scenarios.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35925/j.multi.2023.3.18"
    },
    {
        "id": 5274,
        "title": "Competitive-Cooperative Multi-Agent Reinforcement Learning for Auction-based Federated Learning",
        "authors": "Xiaoli Tang, Han Yu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Auction-based Federated Learning (AFL) enables open collaboration among self-interested data consumers and data owners. Existing AFL approaches cannot manage the mutual influence among multiple data consumers competing to enlist data owners. Moreover, they cannot support a single data owner to join multiple data consumers simultaneously. To bridge these gaps, we propose the Multi-Agent Reinforcement Learning for AFL (MARL-AFL) approach to steer data consumers to bid strategically\n\ntowards an equilibrium with desirable overall system characteristics. We design a temperature-based reward reassignment scheme to make tradeoffs between cooperation and competition among AFL data consumers. In this way, it can reach an equilibrium state that ensures individual data consumers can achieve good utility, while preserving system-level social welfare. To circumvent potential collusion behaviors among data consumers, we introduce a bar agent to set a personalized bidding\n\nlower bound for each data consumer. Extensive experiments on six commonly adopted benchmark datasets show that MARL-AFL is significantly more advantageous compared to six state-of-the-art approaches, outperforming the best by 12.2%, 1.9% and 3.4% in terms of social welfare, revenue and accuracy, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/474"
    },
    {
        "id": 5275,
        "title": "Learning key steps to attack deep reinforcement learning agents",
        "authors": "Chien-Min Yu, Ming-Hsin Chen, Hsuan-Tien Lin",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06318-9"
    },
    {
        "id": 5276,
        "title": "Time-Drift Aware RF Optimization with Machine Learning Techniques",
        "authors": "Ralitsa Sharankova, Kiyomi Seiya, Matilda Mwaniki",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/1983803"
    },
    {
        "id": 5277,
        "title": "A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data",
        "authors": "Samira Ghodratnama, Amin Behehsti, Mehrdad Zakershahrak",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icws60048.2023.00040"
    },
    {
        "id": 5278,
        "title": "Reinforcement Learning for Topic Models",
        "authors": "Jeremy Costello, Marek Reformat",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.265"
    },
    {
        "id": 5279,
        "title": "Sample-Efficient Goal-Conditioned Reinforcement Learning via Predictive Information Bottleneck for Goal Representation Learning",
        "authors": "Qiming Zou, Einoshin Suzuki",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161213"
    },
    {
        "id": 5280,
        "title": "A Study of Imbalanced Dataset Classification on KDD99 Datasets with Reinforcement Learning Mechanism",
        "authors": "Chih-Chen Pan, Yungho Leu",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327999"
    },
    {
        "id": 5281,
        "title": "Reinforcement and deep reinforcement learning-based solutions for machine maintenance planning, scheduling policies, and optimization",
        "authors": "Oluwaseyi Ogunfowora, Homayoun Najjaran",
        "published": "2023-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jmsy.2023.07.014"
    },
    {
        "id": 5282,
        "title": "Representation Learning and Reinforcement Learning for Dynamic Complex Motion Planning System",
        "authors": "Chengmin Zhou, Bingding Huang, Pasi Fränti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3247160"
    },
    {
        "id": 5283,
        "title": "Detection and Classification of Retinal Disorders using Deep Learning Techniques",
        "authors": "K. Sathish, B. Kirubagari, J. Jegan",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icotl59758.2023.10435203"
    },
    {
        "id": 5284,
        "title": "Sim-to-Real Reinforcement Learning Techniques for Double Inverted Pendulum Control with Recovery Property",
        "authors": "Taegun Lee, Doyoon Ju, Young-Sam Lee",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5370/kiee.2023.72.12.1705"
    },
    {
        "id": 5285,
        "title": "Shunting Route Search Based on Reinforcement Learning",
        "authors": "Wu Jiaru, Wang Hongwei, Ma Shuomei",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10452042"
    },
    {
        "id": 5286,
        "title": "Leveraging Business Data Analytics Using Machine Learning Techniques",
        "authors": "",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "This research explores the utilization of machine learning and deep learning methods for analyzing business data in the context of internal s. With the exponential growth of data in businesses, the extraction of insights and informed decisionmaking based on data has become crucial. Machine learning algorithms offer effective tools for identifying patterns and trends within large datasets. This article delves into various machine learning techniques, including supervised and unsupervised learning, reinforcement learning, and deep learning, and investigates their applications in business data analytics. Notably, machine and deep learning models such as Support Vector Machine (SVM) and Decision Tree (DT) prove highly valuable in analyzing complex datasets such as text and images. Moreover, the paper evaluates the challenges associated with implementing machine learning models, such as data preprocessing, model selection, and performance evaluation. Lastly, the paper concludes by discussing potential future research directions in the field of business data analytics, emphasizing the utilization of machine learning and deep learning techniques within the realm of internal.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/ann.07.01.05"
    },
    {
        "id": 5287,
        "title": "ROCKET LANDING USING REINFORCEMENT LEARNING",
        "authors": "",
        "published": "2023-12-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets47316"
    },
    {
        "id": 5288,
        "title": "Interactive Reinforcement Learning-Based Factory Layout Planning",
        "authors": "Jan Schneidewind, Stefan Galka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4469300"
    },
    {
        "id": 5289,
        "title": "Fast Simulation Method with Reinforcement Learning for Automated Optimization of Electronic Systems",
        "authors": "",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tensymp55890.2023.10223650"
    },
    {
        "id": 5290,
        "title": "Federated reinforcement learning for generalizable motion planning",
        "authors": "Zhenyuan Yuan, Siyuan Xu, Minghui Zhu",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156236"
    },
    {
        "id": 5291,
        "title": "Safe Exploration in Reinforcement Learning for Learning from Human Experts",
        "authors": "Jorge Ramirez, Wen Yu",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aibthings58340.2023.10292489"
    },
    {
        "id": 5292,
        "title": "Learning Stabilization Control of Quadrotor in Near-Ground Setting Using Reinforcement Learning",
        "authors": "Mantas Briliauskas",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "With the development of intelligent systems, the popularity of using micro aerial vehicles (MAV) increases significantly in the fields of rescue, photography, security, agriculture, and warfare. New modern solutions of machine learning like ChatGPT that are fine-tuned using reinforcement learning (RL) provides evidence of new trends in seeking general artificial intelligence. RL has already been proven to work as a flight controller for MAV performing better than Proportional Integral Derivative (PID)-based solutions. However, using negative Euclidean distance to the target point as the reward function is sufficient in obstacle-free spaces, e.g. in the air, but fails in special cases, e.g. when training near the ground. In this work, we address this issue by proposing a new reward function with early termination. It not only allows to successfully train Proximal Policy Optimization (PPO) algorithm to stabilize the quadrotor in the near-ground setting, but also achieves lower Euclidean distance error compared to the baseline setup.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5755/j01.itc.53.1.35135"
    },
    {
        "id": 5293,
        "title": "Automatic Voltage Control of Differential Power Grids Based on Transfer Learning and Deep Reinforcement Learning",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17775/cseejpes.2021.06320"
    },
    {
        "id": 5294,
        "title": "Tailored Learning Rates for Reinforcement Learning: A Visual Exploration and Guideline Formulation",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isas60782.2023.10391644"
    },
    {
        "id": 5295,
        "title": "Outcomes after the Ross procedure with pulmonary autograft reinforcement by reimplantation",
        "authors": "Lisa Guirgis, Sébastien Hascoet, Isabelle Van Aerschot, Jelena Radojevic, Mohamed Ly, Sarah Cohen, Emre Belli",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.xjtc.2022.11.016"
    },
    {
        "id": 5296,
        "title": "Reinforcement Learning Approach for a Cognitive Framework for Classification",
        "authors": "K. Barth, S. Brüggenwirth",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149571"
    },
    {
        "id": 5297,
        "title": "Online reinforcement learning of controller parameters adaptation law",
        "authors": "Khalid Alhazmi, S. Mani Sarathy",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156644"
    },
    {
        "id": 5298,
        "title": "Large Language Models Reasoning and Reinforcement Learning",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4656090"
    },
    {
        "id": 5299,
        "title": "Accelerate Training of Reinforcement Learning Agent by Utilization of Current and Previous Experience",
        "authors": "Chenxing Li, Yinlong Liu, Zhenshan Bing, Fabian Schreier, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011745600003393"
    },
    {
        "id": 5300,
        "title": "Reinforcement Learning for Supply Chain Attacks Against Frequency and Voltage Control",
        "authors": "Amr S. Mohamed, Sumin Lee, Deepa Kundur",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00058"
    },
    {
        "id": 5301,
        "title": "Learning at the Edge: Mobile Edge Computing and Reinforcement Learning for Enhanced Web Application Performance",
        "authors": "Komeil Moghaddasi, Shakiba Rajabi",
        "published": "2023-5-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icwr57742.2023.10138952"
    },
    {
        "id": 5302,
        "title": "Optimizing the microstructure in open-die forgings using reinforcement learning",
        "authors": "N. REINISCH",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "Abstract. The open-die forging process can produce large workpieces with excellent material properties that can be used for heavy-duty applications like turbine shafts. The mechanical properties result from the microstructure, which in turn directly results from the process route. Since in open-die forging processes commonly hundreds of unique forming operations are carried out, numerous process routes lead to the same final geometry but produce different microstructures. This is why the prior design of an optimal pass schedule is essential to ensure good mechanical properties of open-die forgings. In the past reinforcement learning (RL) was already used to design optimized pass schedules for open-die forging that achieve the desired geometry, utilize the available press force, and reduce the number of passes. Furthermore, the design of a single pass schedule only took a few seconds which creates opportunities for the use of RL in control systems e.g. for the microstructure. This is why in this publication an existing RL algorithm is extended so that microstructure can be included in the optimization. Within this process, a microstructure model is integrated into the RL algorithm and the reward function (defines the goal of the training process) was extended in two steps to also rate the achieved average grain size continuously dependent on the temperature of the workpiece. In addition, the RL implementation was changed to ensure the production of the desired final geometry leading to a decrease in the complexity of the optimization problem. Thus, both an improvement of the designed pass schedules and a significant reduction of the training time of the RL algorithm was achieved. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.21741/9781644902479-221"
    },
    {
        "id": 5303,
        "title": "Stocks and Options Portfolio Optimisation With Reinforcement Learning",
        "authors": "Daniel Alexandre Bloch",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4682061"
    },
    {
        "id": 5304,
        "title": "Confidence optimally modulates decision policy in reinforcement learning",
        "authors": "kobe desender, Tom Verguts",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1083-0"
    }
]