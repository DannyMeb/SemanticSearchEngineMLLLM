[
    {
        "id": 5101,
        "title": "Metaheuristic Optimization Algorithms",
        "authors": "Andre A. Keller",
        "published": "2019-3-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2174/9781681087054119010004"
    },
    {
        "id": 5102,
        "title": "Newton Algorithms",
        "authors": "Vasile Sima",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003067450-2"
    },
    {
        "id": 5103,
        "title": "Nature-Inspired Algorithms",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-3"
    },
    {
        "id": 5104,
        "title": "Matrix Adaptation Evolution Strategy with Multi-Objective Optimization for Multimodal Optimization",
        "authors": "Wei Li",
        "published": "2019-3-5",
        "citations": 3,
        "abstract": "The standard covariance matrix adaptation evolution strategy (CMA-ES) is highly effective at locating a single global optimum. However, it shows unsatisfactory performance for solving multimodal optimization problems (MMOPs). In this paper, an improved algorithm based on the MA-ES, which is called the matrix adaptation evolution strategy with multi-objective optimization algorithm, is proposed to solve multimodal optimization problems (MA-ESN-MO). Taking advantage of the multi-objective optimization in maintaining population diversity, MA-ESN-MO transforms an MMOP into a bi-objective optimization problem. The archive is employed to save better solutions for improving the convergence of the algorithm. Moreover, the peaks found by the algorithm can be maintained until the end of the run. Multiple subpopulations are used to explore and exploit in parallel to find multiple optimal solutions for the given problem. Experimental results on CEC2013 test problems show that the covariance matrix adaptation with Niching and the multi-objective optimization algorithm (CMA-NMO), CMA Niching with the Mahalanobis Metric and the multi-objective optimization algorithm (CMA-NMM-MO), and matrix adaptation evolution strategy Niching with the multi-objective optimization algorithm (MA-ESN-MO) have overall better performance compared with the covariance matrix adaptation evolution strategy (CMA-ES), matrix adaptation evolution strategy (MA-ES), CMA Niching (CMA-N), CMA-ES Niching with Mahalanobis Metric (CMA-NMM), and MA-ES-Niching (MA-ESN).",
        "link": "http://dx.doi.org/10.3390/a12030056"
    },
    {
        "id": 5105,
        "title": "Numerical Algorithms for Constrained Optimization",
        "authors": "Ossama Abdelkhalik",
        "published": "2021-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351119108-7"
    },
    {
        "id": 5106,
        "title": "Nature and Optimization Algorithms",
        "authors": "Rohollah Omidvar, Behrouz  Minaei Bidgoli",
        "published": "2021-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2174/9789811459597121010005"
    },
    {
        "id": 5107,
        "title": "Nature and Optimization Algorithms",
        "authors": "Rohollah Omidvar, Behrouz Minaei Bidgoli",
        "published": "2020-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2174/9789811459597120010005"
    },
    {
        "id": 5108,
        "title": "Genetic Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 61,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00013-5"
    },
    {
        "id": 5109,
        "title": "Structure-Preserving Algorithms",
        "authors": "Vasile Sima",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003067450-4"
    },
    {
        "id": 5110,
        "title": "Firefly Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00016-0"
    },
    {
        "id": 5111,
        "title": "Bat Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00018-4"
    },
    {
        "id": 5112,
        "title": "Particle Swarm Optimization",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-6"
    },
    {
        "id": 5113,
        "title": "Bee Colony Optimization",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-9"
    },
    {
        "id": 5114,
        "title": "Classical Optimization Methods",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-2"
    },
    {
        "id": 5115,
        "title": "Ant Colony Optimization",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-8"
    },
    {
        "id": 5116,
        "title": "Gray Wolf Optimization",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-15"
    },
    {
        "id": 5117,
        "title": "Optimization Functions",
        "authors": "Altaf Q. H. Badar",
        "published": "2021-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003206477-2"
    },
    {
        "id": 5118,
        "title": "Elephant Herding Optimization",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-16"
    },
    {
        "id": 5119,
        "title": "Optimization Problems and Algorithms",
        "authors": "Yujun Zheng, Xueqin Lu, Minxia Zhang, Shengyong Chen",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-2586-1_1"
    },
    {
        "id": 5120,
        "title": "Non-Stationary Stochastic Global Optimization Algorithms",
        "authors": "Jonatan Gomez, Andres Rivera",
        "published": "2022-9-29",
        "citations": 1,
        "abstract": "Studying the theoretical properties of optimization algorithms such as genetic algorithms and evolutionary strategies allows us to determine when they are suitable for solving a particular type of optimization problem. Such a study consists of three main steps. The first step is considering such algorithms as Stochastic Global Optimization Algorithms (SGoals ), i.e., iterative algorithm that applies stochastic operations to a set of candidate solutions. The second step is to define a formal characterization of the iterative process in terms of measure theory and define some of such stochastic operations as stationary Markov kernels (defined in terms of transition probabilities that do not change over time). The third step is to characterize non-stationary SGoals, i.e., SGoals having stochastic operations with transition probabilities that may change over time. In this paper, we develop the third step of this study. First, we generalize the sufficient conditions convergence from stationary to non-stationary Markov processes. Second, we introduce the necessary theory to define kernels for arithmetic operations between measurable functions. Third, we develop Markov kernels for some selection and recombination schemes. Finally, we formalize the simulated annealing algorithm and evolutionary strategies using the systematic formal approach.",
        "link": "http://dx.doi.org/10.3390/a15100362"
    },
    {
        "id": 5121,
        "title": "Flower Pollination Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00019-6"
    },
    {
        "id": 5122,
        "title": "Analysis of Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00010-x"
    },
    {
        "id": 5123,
        "title": "Introduction to Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00008-1"
    },
    {
        "id": 5124,
        "title": "Hybrid Biogeography-Based Optimization Algorithms",
        "authors": "Yujun Zheng, Xueqin Lu, Minxia Zhang, Shengyong Chen",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-2586-1_5"
    },
    {
        "id": 5125,
        "title": "Accelerated Algorithms for Nonconvex Optimization",
        "authors": "Zhouchen Lin, Huan Li, Cong Fang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-2910-8_4"
    },
    {
        "id": 5126,
        "title": "Multi-Guide Set-Based Particle Swarm Optimization for Multi-Objective Portfolio Optimization",
        "authors": "Kyle Erwin, Andries Engelbrecht",
        "published": "2023-1-17",
        "citations": 4,
        "abstract": "Portfolio optimization is a multi-objective optimization problem (MOOP) with risk and profit, or some form of the two, as competing objectives. Single-objective portfolio optimization requires a trade-off coefficient to be specified in order to balance the two objectives. Erwin and Engelbrecht proposed a set-based approach to single-objective portfolio optimization, namely, set-based particle swarm optimization (SBPSO). SBPSO selects a sub-set of assets that form a search space for a secondary optimization task to optimize the asset weights. The authors found that SBPSO was able to identify good solutions to portfolio optimization problems and noted the benefits of redefining the portfolio optimization problem as a set-based problem. This paper proposes the first multi-objective optimization (MOO) approach to SBPSO, and its performance is investigated for multi-objective portfolio optimization. Alongside this investigation, the performance of multi-guide particle swarm optimization (MGPSO) for multi-objective portfolio optimization is evaluated and the performance of SBPSO for portfolio optimization is compared against multi-objective algorithms. It is shown that SBPSO is as competitive as multi-objective algorithms, albeit with multiple runs. The proposed multi-objective SBPSO, i.e., multi-guide set-based particle swarm optimization (MGSBPSO), performs similarly to other multi-objective algorithms while obtaining a more diverse set of optimal solutions.",
        "link": "http://dx.doi.org/10.3390/a16020062"
    },
    {
        "id": 5127,
        "title": "Raven Roosting Optimization Algorithm",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071-18"
    },
    {
        "id": 5128,
        "title": "Particle Swarm Optimization",
        "authors": "Altaf Q. H. Badar",
        "published": "2021-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003206477-5"
    },
    {
        "id": 5129,
        "title": "Schur and Generalized Schur Algorithms",
        "authors": "Vasile Sima",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003067450-3"
    },
    {
        "id": 5130,
        "title": "Accelerated Algorithms for Unconstrained Convex Optimization",
        "authors": "Zhouchen Lin, Huan Li, Cong Fang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-2910-8_2"
    },
    {
        "id": 5131,
        "title": "Nature-Inspired Optimization Algorithms",
        "authors": "",
        "published": "2021",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/c2019-0-03762-4"
    },
    {
        "id": 5132,
        "title": "“Algorithms in Multi-Objective Optimization”: Foreword by the Guest Editor",
        "authors": "Massimiliano Caramia",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "Many real-world optimization problems typically involve multiple (conflicting) objectives [...]",
        "link": "http://dx.doi.org/10.3390/a15120476"
    },
    {
        "id": 5133,
        "title": "Accelerated Algorithms for Constrained Convex Optimization",
        "authors": "Zhouchen Lin, Huan Li, Cong Fang",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-2910-8_3"
    },
    {
        "id": 5134,
        "title": "Brain Storm Optimization Algorithms for Flexible Job Shop Scheduling Problem",
        "authors": "Xiuli Wu",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-15070-9_10"
    },
    {
        "id": 5135,
        "title": "Convex Optimization and Efficiency",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108699211.006"
    },
    {
        "id": 5136,
        "title": "Linear-Quadratic Optimization Problems",
        "authors": "Vasile Sima",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003067450-1"
    },
    {
        "id": 5137,
        "title": "Multiobjective optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00009-7"
    },
    {
        "id": 5138,
        "title": "Exploring Optimization Algorithms",
        "authors": "Vladislav Bukshtynov",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003275169-4"
    },
    {
        "id": 5139,
        "title": "Teaching Learning Based Optimization",
        "authors": "Altaf Q. H. Badar",
        "published": "2021-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003206477-9"
    },
    {
        "id": 5140,
        "title": "Unconstrained Optimization: Algorithms",
        "authors": "",
        "published": "2021-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108869027.006"
    },
    {
        "id": 5141,
        "title": "Test Function Benchmarks for Global Optimization",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00024-x"
    },
    {
        "id": 5142,
        "title": "Particle Swarm Optimization",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00015-9"
    },
    {
        "id": 5143,
        "title": "Multi-Objective Optimization",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00022-6"
    },
    {
        "id": 5144,
        "title": "Memetic Algorithms with Extremal Optimization",
        "authors": "Yong-Zai Lu, Yu-Wang Chen, Min-Rong Chen, Peng Chen, Guo-Qiang Chen",
        "published": "2018-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b19572-5"
    },
    {
        "id": 5145,
        "title": "Enhanced Hyper-Cube Framework Ant Colony Optimization for Combinatorial Optimization Problems",
        "authors": "Ali Ahmid, Thien-My Dao, Ngan Van Le",
        "published": "2021-9-29",
        "citations": 2,
        "abstract": "Solving of combinatorial optimization problems is a common practice in real-life engineering applications. Trusses, cranes, and composite laminated structures are some good examples that fall under this category of optimization problems. Those examples have a common feature of discrete design domain that turn them into a set of NP-hard optimization problems. Determining the right optimization algorithm for such problems is a precious point that tends to impact the overall cost of the design process. Furthermore, reinforcing the performance of a prospective optimization algorithm reduces the design cost. In the current study, a comprehensive assessment criterion has been developed to assess the performance of meta-heuristic (MH) solutions in the domain of structural design. Thereafter, the proposed criterion was employed to compare five different variants of Ant Colony Optimization (ACO). It was done by using a well-known structural optimization problem of laminate Stacking Sequence Design (SSD). The initial results of the comparison study reveal that the Hyper-Cube Framework (HCF) ACO variant outperforms the others. Consequently, an investigation of further improvement led to introducing an enhanced version of HCFACO (or EHCFACO). Eventually, the performance assessment of the EHCFACO variant showed that the average practical reliability became more than twice that of the standard ACO, and the normalized price decreased more to hold at 28.92 instead of 51.17.",
        "link": "http://dx.doi.org/10.3390/a14100286"
    },
    {
        "id": 5146,
        "title": "Copyright",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00003-2"
    },
    {
        "id": 5147,
        "title": "Optimization Algorithms - Examples",
        "authors": "",
        "published": "2018-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/intechopen.71370"
    },
    {
        "id": 5148,
        "title": "Nature-Inspired Optimization Algorithms",
        "authors": "A Vasuki",
        "published": "2020-5-31",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429289071"
    },
    {
        "id": 5149,
        "title": "Optimization algorithms",
        "authors": "Andrew Murphy, Rade Kovač",
        "published": "2019-7-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-69763"
    },
    {
        "id": 5150,
        "title": "Introduction to optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00001-2"
    },
    {
        "id": 5151,
        "title": "Online optimization algorithms",
        "authors": "Xiaobiao Huang",
        "published": "2019-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429434358-7"
    },
    {
        "id": 5152,
        "title": "Introduction to Other Optimization Techniques",
        "authors": "Altaf Q. H. Badar",
        "published": "2021-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003206477-10"
    },
    {
        "id": 5153,
        "title": "Bridging Continuous and Discrete Optimization",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108699211.003"
    },
    {
        "id": 5154,
        "title": "Nature-inspired optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00010-3"
    },
    {
        "id": 5155,
        "title": "Random Walks and Optimization",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00011-1"
    },
    {
        "id": 5156,
        "title": "Ellipsoid Method for Convex Optimization",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108699211.015"
    },
    {
        "id": 5157,
        "title": "Ant Colony Optimization with Warm-Up",
        "authors": "Mattia Neroni",
        "published": "2021-10-12",
        "citations": 6,
        "abstract": "The Ant Colony Optimization (ACO) is a probabilistic technique inspired by the behavior of ants for solving computational problems that may be reduced to finding the best path through a graph. Some species of ants deposit pheromone on the ground to mark some favorable paths that should be used by other members of the colony. Ant colony optimization implements a similar mechanism for solving optimization problems. In this paper a warm-up procedure for the ACO is proposed. During the warm-up, the pheromone matrix is initialized to provide an efficient new starting point for the algorithm, so that it can obtain the same (or better) results with fewer iterations. The warm-up is based exclusively on the graph, which, in most applications, is given and does not need to be recalculated every time before executing the algorithm. In this way, it can be made only once, and it speeds up the algorithm every time it is used from then on. The proposed solution is validated on a set of traveling salesman problem instances, and in the simulation of a real industrial application for the routing of pickers in a manual warehouse. During the validation, it is compared with other ACO adopting a pheromone initialization technique, and the results show that, in most cases, the adoption of the proposed warm-up allows the ACO to obtain the same or better results with fewer iterations.",
        "link": "http://dx.doi.org/10.3390/a14100295"
    },
    {
        "id": 5158,
        "title": "DoA Estimation Algorithms",
        "authors": "Tanuja Shendkar Dhope",
        "published": "2022-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003337607-4"
    },
    {
        "id": 5159,
        "title": "A Framework for Self-Tuning Algorithms",
        "authors": "Xin-She Yang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00020-2"
    },
    {
        "id": 5160,
        "title": "Other Important Optimization Algorithms",
        "authors": "Krishn Kumar Mishra",
        "published": "2022-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003313649-8"
    },
    {
        "id": 5161,
        "title": "Genetic Algorithms for Chemical Engineering Optimization Problems",
        "authors": "Thi Anh-Nga Nguyen, Tuan-Anh Nguyen",
        "published": "2022-10-12",
        "citations": 0,
        "abstract": "Chemical engineering processes are frequently composed of multiple complex phenomena. These systems can be represented by a set of several equations, which are referred to as mathematical model of the process. Optimization in chemical engineering utilizes specialized techniques to determine the values of the decision variables at which the performance of the process, measured as the objective function(s), is minimum or maximum. The profitability of the process improves remarkably as a result of this selection. This benefit has encouraged the broad application of optimization for important industrial challenges. However, many problems in chemical engineering processes are hard to find the optimum using gradient-based algorithms. For example, the cases when the objective functions of the processes are multimodal, discontinuous, or implicit. Genetic algorithms (GAs) are a kind of metaheuristic searching optimization methods, which are inspired by nature, the mechanics of natural evolution and genetics. Genetic algorithms have received significant attention due to their remarkable advantages over classical algorithms. Compared with traditional optimization approaches, GAs are straightforward, robust, capable of handling the non-differentiable, discontinuous, or multimodal problems. The purpose of this paper is to give several case studies using genetic algorithms in chemical engineering optimization problems.",
        "link": "http://dx.doi.org/10.5772/intechopen.104884"
    },
    {
        "id": 5162,
        "title": "Introduction to Genetic Algorithms in Search and Optimization",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "This chapter is all about introducing genetic algorithms in the search process, which are based on the theory of natural selection, genetics, and survival of fittest. By detailed understanding of the algorithm, one would be able to apply it in your respective field for optimization. At the end of this chapter, the reader will have acquired basic theory and working of these algorithms. Since genetic algorithms are used in diverse fields, the tone and language of this chapter is kept simple and casual for better understanding. Genetic algorithms in this chapter are applied through a hand calculation example. Genetic algorithms are basically mathematical calculations based on Darwin's theory of survival of fittest. This chapter gives a detailed understanding of the theory and working of genetic algorithms based on hand calculation examples. Comparison of genetic algorithms with other search procedures is also done. ",
        "link": "http://dx.doi.org/10.4018/978-1-7998-4105-0.ch003"
    },
    {
        "id": 5163,
        "title": "An Optimization Algorithm Inspired by the Phase Transition Phenomenon for Global Optimization Problems with Continuous Variables",
        "authors": "Zijian Cao, Lei Wang",
        "published": "2017-10-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/a10040119"
    },
    {
        "id": 5164,
        "title": "Biogeography-Based Optimization of the Portfolio Optimization Problem with Second Order Stochastic Dominance Constraints",
        "authors": "Tao Ye, Ziqiang Yang, Siling Feng",
        "published": "2017-8-25",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/a10030100"
    },
    {
        "id": 5165,
        "title": "Approximation Algorithms",
        "authors": "Bernhard Korte, Jens Vygen",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-56039-6_16"
    },
    {
        "id": 5166,
        "title": "Single-variable nonlinear optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00003-6"
    },
    {
        "id": 5167,
        "title": "Multivariable unconstrained nonlinear optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00004-8"
    },
    {
        "id": 5168,
        "title": "Global Optimization",
        "authors": "",
        "published": "2020-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316227268.023"
    },
    {
        "id": 5169,
        "title": "Multivariable constrained nonlinear optimization",
        "authors": "Sukanta Nayak",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821126-7.00005-x"
    },
    {
        "id": 5170,
        "title": "Brain Storm Optimization Algorithms: More Questions than Answers",
        "authors": "Shi Cheng, Hui Lu, Xiujuan Lei, Yuhui Shi",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-15070-9_1"
    },
    {
        "id": 5171,
        "title": "Brain Storm Optimization Algorithms for Solving Equations Systems",
        "authors": "Liviu Mafteiu-Scai, Emanuela Mafteiu, Roxana Mafteiu-Scai",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-15070-9_8"
    },
    {
        "id": 5172,
        "title": "Multiobjective Optimization",
        "authors": "",
        "published": "2020-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316227268.020"
    },
    {
        "id": 5173,
        "title": "Decentralized Multiobjective Optimization Algorithms",
        "authors": "Maude J. Blondin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-54621-2_797-1"
    },
    {
        "id": 5174,
        "title": "An Enhanced Lightning Attachment Procedure Optimization Algorithm",
        "authors": " Wang,  Jiang",
        "published": "2019-6-29",
        "citations": 6,
        "abstract": "To overcome the shortcomings of the lightning attachment procedure optimization (LAPO) algorithm, such as premature convergence and slow convergence speed, an enhanced lightning attachment procedure optimization (ELAPO) algorithm was proposed in this paper. In the downward leader movement, the idea of differential evolution was introduced to speed up population convergence; in the upward leader movement, by superimposing vectors pointing to the average individual, the individual updating mode was modified to change the direction of individual evolution, avoid falling into local optimum, and carry out a more fine local information search; in the performance enhancement stage, opposition-based learning (OBL) was used to replace the worst individuals, improve the convergence rate of population, and increase the global exploration capability. Finally, 16 typical benchmark functions in CEC2005 are used to carry out simulation experiments with LAPO algorithm, four improved algorithms, and ELAPO. Experimental results showed that ELAPO obtained the better convergence velocity and optimization accuracy.",
        "link": "http://dx.doi.org/10.3390/a12070134"
    },
    {
        "id": 5175,
        "title": "Optimization algorithms",
        "authors": "Xin-She Yang",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-817216-2.00010-7"
    },
    {
        "id": 5176,
        "title": "Gray Wolf Optimization Algorithm for Multi-Constraints Second-Order Stochastic Dominance Portfolio Optimization",
        "authors": "Yixuan Ren, Tao Ye, Mengxing Huang, Siling Feng",
        "published": "2018-5-15",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/a11050072"
    },
    {
        "id": 5177,
        "title": "Distributed Optimization in Machine Learning",
        "authors": "Gauri Joshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-19067-4_1"
    },
    {
        "id": 5178,
        "title": "Contents",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00004-4"
    },
    {
        "id": 5179,
        "title": "Evolutionary Algorithms in Engineering Design Optimization",
        "authors": "",
        "published": "2022-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/books978-3-0365-2715-4"
    },
    {
        "id": 5180,
        "title": "Index",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-821986-7.00026-3"
    },
    {
        "id": 5181,
        "title": "Optimization Algorithms - Classics and Last Advances [Working Title]",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/intechopen.111098"
    },
    {
        "id": 5182,
        "title": "Algorithms for Linear-Quadratic Optimization",
        "authors": "Vasile Sima",
        "published": "2021-12-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003067450"
    },
    {
        "id": 5183,
        "title": "Binary Horse Optimization Algorithm for Feature Selection",
        "authors": "Dorin Moldovan",
        "published": "2022-5-6",
        "citations": 6,
        "abstract": "The bio-inspired research field has evolved greatly in the last few years due to the large number of novel proposed algorithms and their applications. The sources of inspiration for these novel bio-inspired algorithms are various, ranging from the behavior of groups of animals to the properties of various plants. One problem is the lack of one bio-inspired algorithm which can produce the best global solution for all types of optimization problems. The presented solution considers the proposal of a novel approach for feature selection in classification problems, which is based on a binary version of a novel bio-inspired algorithm. The principal contributions of this article are: (1) the presentation of the main steps of the original Horse Optimization Algorithm (HOA), (2) the adaptation of the HOA to a binary version called the Binary Horse Optimization Algorithm (BHOA), (3) the application of the BHOA in feature selection using nine state-of-the-art datasets from the UCI machine learning repository and the classifiers Random Forest (RF), Support Vector Machines (SVM), Gradient Boosted Trees (GBT), Logistic Regression (LR), K-Nearest Neighbors (K-NN), and Naïve Bayes (NB), and (4) the comparison of the results with the ones obtained using the Binary Grey Wolf Optimizer (BGWO), Binary Particle Swarm Optimization (BPSO), and Binary Crow Search Algorithm (BCSA). The experiments show that the BHOA is effective and robust, as it returned the best mean accuracy value and the best accuracy value for four and seven datasets, respectively, compared to BGWO, BPSO, and BCSA, which returned the best mean accuracy value for four, two, and two datasets, respectively, and the best accuracy value for eight, seven, and five datasets, respectively.",
        "link": "http://dx.doi.org/10.3390/a15050156"
    },
    {
        "id": 5184,
        "title": "Evolutionary Optimization Algorithms",
        "authors": "Altaf Q. H. Badar",
        "published": "2021-8-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003206477"
    },
    {
        "id": 5185,
        "title": "New Optimization Algorithms and their Applications",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/c2020-0-02698-8"
    },
    {
        "id": 5186,
        "title": "Fundamentals of Optimization Techniques with Algorithms",
        "authors": "",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/c2019-1-02539-9"
    },
    {
        "id": 5187,
        "title": "“Multi-Objective and Multi-Level Optimization: Algorithms and Applications”: Foreword by the Guest Editor",
        "authors": "Massimiliano Caramia",
        "published": "2023-9-5",
        "citations": 1,
        "abstract": "Decision making in real-world applications frequently calls for taking into account multiple goals to come up with viable solutions [...]",
        "link": "http://dx.doi.org/10.3390/a16090425"
    },
    {
        "id": 5188,
        "title": "Linear Programming Algorithms",
        "authors": "Bernhard Korte, Jens Vygen",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-56039-6_4"
    },
    {
        "id": 5189,
        "title": "Optimization Algorithms for Detection of Social Interactions",
        "authors": "Vincenzo Cutello, Georgia Fargetta, Mario Pavone, Rocco A. Scollo",
        "published": "2020-6-11",
        "citations": 7,
        "abstract": "Community detection is one of the most challenging and interesting problems in many research areas. Being able to detect highly linked communities in a network can lead to many benefits, such as understanding relationships between entities or interactions between biological genes, for instance. Two different immunological algorithms have been designed for this problem, called Opt-IA and Hybrid-IA, respectively. The main difference between the two algorithms is the search strategy and related immunological operators developed: the first carries out a random search together with purely stochastic operators; the last one is instead based on a deterministic Local Search that tries to refine and improve the current solutions discovered. The robustness of Opt-IA and Hybrid-IA has been assessed on several real social networks. These same networks have also been considered for comparing both algorithms with other seven different metaheuristics and the well-known greedy optimization Louvain algorithm. The experimental analysis conducted proves that Opt-IA and Hybrid-IA are reliable optimization methods for community detection, outperforming all compared algorithms.",
        "link": "http://dx.doi.org/10.3390/a13060139"
    },
    {
        "id": 5190,
        "title": "The Porcupine Measure for Comparing the Performance of Multi-Objective Optimization Algorithms",
        "authors": "Christiaan Scheepers, Andries Engelbrecht",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "In spite of being introduced over twenty-five years ago, Fonseca and Fleming’s attainment surfaces have not been widely used. This article investigates some of the shortcomings that may have led to the lack of adoption of this performance measure. The quantitative measure based on attainment surfaces, introduced by Knowles and Corne, is analyzed. The analysis shows that the results obtained by the Knowles and Corne approach are influenced (biased) by the shape of the attainment surface. Improvements to the Knowles and Corne approach for bi-objective Pareto-optimal front (POF) comparisons are proposed. Furthermore, assuming M objective functions, an M-dimensional attainment-surface-based quantitative measure, named the porcupine measure, is proposed for comparing the performance of multi-objective optimization algorithms. A computationally optimized version of the porcupine measure is presented and empirically analyzed.",
        "link": "http://dx.doi.org/10.3390/a16060283"
    },
    {
        "id": 5191,
        "title": "Nature-Inspired Optimization Algorithms with Java",
        "authors": "Shashank Jain",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7401-9"
    },
    {
        "id": 5192,
        "title": "Second-order optimization algorithms",
        "authors": "",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009166164.007"
    },
    {
        "id": 5193,
        "title": "Optimization under Uncertainty",
        "authors": "",
        "published": "2020-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316227268.021"
    },
    {
        "id": 5194,
        "title": "First-order optimization algorithms",
        "authors": "",
        "published": "2023-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009166164.005"
    },
    {
        "id": 5195,
        "title": "Physics Inspired Optimization Algorithms",
        "authors": "George Lindfield, John Penny",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-803636-5.00008-6"
    },
    {
        "id": 5196,
        "title": "Introduction to Optimization",
        "authors": "",
        "published": "2020-11-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316227268.003"
    },
    {
        "id": 5197,
        "title": "Automatic hyperparameter tuning of topology optimization algorithms using surrogate optimization",
        "authors": "Dat Ha, Josephine Carstensen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper presents a new approach that automates the tuning process in topology optimization of parameters that are traditionally defined by the user. The new method draws inspiration from hyperparameter optimization in machine learning. A new design problem is formulated where the topology optimization hyperparameters are defined as design variables and the problem is solved by surrogate optimization. The new design problem is nested, such that a topology optimization problem is solved as an inner problem.To encourage the identification of high-performing solutions while limiting the computational resource requirements, the outer objective function is defined as the original objective combined with penalization for intermediate densities and deviations from the prescribed material consumption.The contribution is demonstrated on density-based topology optimization with various hyperparameters and objectives, including compliance minimization, compliant mechanism design, and buckling load factor maximization. Consistent performance is observed across all tested examples. For a simple two hyperparameter case, the new framework is shown to reduce amount of times a topology optimization algorithm is executed by 90\\% without notably sacrificing the objective compared to a rigorous manual grid search.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3944293/v1"
    },
    {
        "id": 5198,
        "title": "Evolutionary Algorithms and Difficult Optimization Problems",
        "authors": "Leonardo Azevedo Scardua",
        "published": "2021-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429298028-2"
    },
    {
        "id": 5199,
        "title": "Optimization I: Greedy Algorithms",
        "authors": "",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811263842_0011"
    },
    {
        "id": 5200,
        "title": "Stochastic Optimization",
        "authors": "",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009160865.008"
    }
]