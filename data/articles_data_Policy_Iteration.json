[
    {
        "id": 14771,
        "title": "Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we introduce dynamic programming, which is a cornerstone of model-based reinforcement learning. We demonstrate dynamic programming for policy iteration and value iteration, leading to the quality function and Q-learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.6fs4s9"
    },
    {
        "id": 14772,
        "title": "Template-based Analyses and Min-policy Iteration",
        "authors": "Pierre-Lo√Øc Garoche",
        "published": "2019-5-14",
        "citations": 0,
        "abstract": "This chapter considers other configurations aside from the direct synthesis of invariants as bound templates. A first case arises when the methods shown in the previous chapter only synthesizes the template but not the bound. A second appears when one wants to analyze a system with multiple templates. This chapter looks at bounds on each variable and considers the templates ùëù‚Äé(ùë•‚Äé) = ùë•¬≤‚Äéùëñ‚Äé for each variable ùë•‚Äéùëñ‚Äé in state characterization ùë•‚Äé ‚àà‚Äé Œ£‚Äé. The chapter thus proposes a policy iteration algorithm, based on sum-of-squares (SOS) optimization, to refine such template bounds. In practice, the chapter uses it by combining a Lyapunov-based template obtained using one of the previous methods with additional templates encoding bounds on some variables or property specific templates.",
        "link": "http://dx.doi.org/10.23943/princeton/9780691181301.003.0006"
    },
    {
        "id": 14773,
        "title": "Coding the Environment and MDP Solution",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_3"
    },
    {
        "id": 14774,
        "title": "Comparison of Value Iteration, Policy Iteration and Q-Learning for solving Decision-Making problems",
        "authors": "Mohand Hamadouche, Catherine Dezan, David Espes, Kalinka Branco",
        "published": "2021-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas51884.2021.9476691"
    },
    {
        "id": 14775,
        "title": "Approximate Dynamic Programming: Policy Iteration",
        "authors": "Ilya O. Ryzhov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-54621-2_802-1"
    },
    {
        "id": 14776,
        "title": "Distributed Randomized Multiagent Policy Iteration in Reinforcement Learning",
        "authors": "Weipeng Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4005620"
    },
    {
        "id": 14777,
        "title": "Distributed Randomized Multiagent Policy Iteration in Reinforcement Learning",
        "authors": "Weipeng Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4489715"
    },
    {
        "id": 14778,
        "title": "Quantum-Enhanced Policy Iteration on the Example of a Mountain Car",
        "authors": "Egor  E. Nuzhin, Dmitry Yudin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4582850"
    },
    {
        "id": 14779,
        "title": "Analytic Policy Function Iteration",
        "authors": "Zhao Han, Fei Tan, Jieran Wu",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3512320"
    },
    {
        "id": 14780,
        "title": "Template-based Analyses and Min-policy Iteration",
        "authors": "",
        "published": "2019-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv80cd4v.8"
    },
    {
        "id": 14781,
        "title": "Computing Stabilizing Linear Controllers via Policy Iteration",
        "authors": "Andrew Lamperski",
        "published": "2020-12-14",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9304202"
    },
    {
        "id": 14782,
        "title": "Filter based Explorized Policy Iteration Algorithm for On-Policy Approximate LQR",
        "authors": "Sumit Kumar Jha, Sayan Basu Roy, Shubhendu Bhasin",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci44817.2019.9002891"
    },
    {
        "id": 14783,
        "title": "Koopman-based Policy Iteration for Robust Optimal Control",
        "authors": "Alexander Krolicki, Sarang Sutavani, Umesh Vaidya",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867541"
    },
    {
        "id": 14784,
        "title": "Approximate policy iteration for online learning and continuous-action control",
        "authors": "",
        "published": "2017-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091-10"
    },
    {
        "id": 14785,
        "title": "Center strategies for universal geodetic transformations: modified iteration policy and two alternative models",
        "authors": "Wenxi Zhan, Xing Fang, Wenxian Zeng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Although centralized coordinates are applied in geodetic coordinate transformations implicitly or explicitly, the centering strategy has not been comprehensively investigated from the theoretical perspective. We rigorously model and extend the empirically used three center strategies based on different models:\n\nOriginal model: Based on the partition representations of the solution, we propose a modified iteration policy, which reduces the parameter number and improves numerical stability during iteration. Also, its simplified version is analyzed when the cofactor matrix has the Kronecker product structures. It can be regarded as the extension of the work of Teunissen, since we essentially follow the same idea of partitioning the transformation parameters and the translation parameters, but more general covariance matrix structures are investigated in our consideration.\nShifting model: With the partitioned solution forms, we prove the estimated transformation matrix and the residual vector are translational invariant. For iteration, with the classical iteration policy, the shifts should be chosen properly; with the modified iteration policy, there is no restriction since it is numerically equivalent to the original model. In addition, this model shows the feasibility of conducting the adjustment with the centralized coordinates and the original stochastic model.\nTranslation elimination model: By multiplying the transformation relation with a specific matrix from both sides, we formulate the translation elimination model with the coordinates centralized and the translation parameters eliminated. With this model reduction, the covariance matrix has also been transformed since the observation equations are comprised of coordinate combinations. In addition, Leick&#8217;s model reduction strategy is a special case of this model, which is conducted by subtracting one particular observation equation from the remaining equations.&#160;\n\nTest computations with different weight structures show the validity of these strategies.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-13902"
    },
    {
        "id": 14786,
        "title": "Improving modified policy iteration for probabilistic model checking",
        "authors": "Mohammadsadegh Mohagheghi, Jaber Karimpour",
        "published": "2022-3-24",
        "citations": 1,
        "abstract": "Value iteration, policy iteration and their modified versions are well-known algorithms for probabilistic model checking of Markov Decision Processes. One the challenge of these methods is that they are time-consuming in most cases. Several techniques have been proposed to improve the performance of iterative methods for probabilistic model checking. However, the running time of these techniques depends on the graphical structure of the model and in some cases their performance is worse than the performance of the standard methods. In this paper, we propose two new heuristics to accelerate the modified policy iteration method. We first define a criterion for the usefulness of the computations of each iteration of this method. The first contribution of our work is to develop and use a criterion to reduce the number of iterations in modified policy iteration. As the second contribution, we propose a new approach to identify useless updates in each iteration. This method reduces the running time of computations by avoiding useless updates of states. The proposed heuristics have been implemented in the PRISM model checker and applied on several standard case studies. We compare the running time of our heuristics with the running time of previous standard and improved methods. Experimental results show that our techniques yields a significant speed-up.",
        "link": "http://dx.doi.org/10.7494/csci.2022.23.1.4139"
    },
    {
        "id": 14787,
        "title": "ONLINE AND LIGHTWEIGHT KERNEL-BASED APPROXIMATE POLICY ITERATION FOR DYNAMIC P-NORM LINEAR ADAPTIVE FILTERING",
        "authors": "Yuki Akiyama, Minh Vu, Konstantinos Slavakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>¬†¬†This paper introduces a solution to the problem of selecting dynamically (online) the ``optimal'' p-norm to combat outliers in linear adaptive filtering without any knowledge on the probability density function of the outliers. The proposed online and data-driven framework is built on kernel-based reinforcement learning (KBRL). To this end, novel Bellman mappings on reproducing kernel Hilbert spaces (RKHSs) are introduced. These mappings do not require any knowledge on transition probabilities of Markov decision processes, and are nonexpansive with respect to the underlying Hilbertian norm. The fixed-point sets of the proposed Bellman mappings are utilized to build an approximate policy-iteration (API) framework for the problem at hand. To address the ``curse of dimensionality'' in RKHSs, random Fourier features are utilized to bound the computational complexity of the API. Numerical tests on synthetic data for several outlier scenarios demonstrate the superior performance of the proposed API framework over several non-RL and KBRL schemes.</p>\n<p><br></p>\n<p>----</p>\n<p>¬†¬© 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.¬†</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21376458"
    },
    {
        "id": 14788,
        "title": "Optimal Intermittent Feedback via Least Square Policy Iteration",
        "authors": "Domagoj Toliƒá, Sandra Hirche",
        "published": "2017-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315367934-10"
    },
    {
        "id": 14789,
        "title": "ONLINE AND LIGHTWEIGHT KERNEL-BASED APPROXIMATE POLICY ITERATION FOR DYNAMIC P-NORM LINEAR ADAPTIVE FILTERING",
        "authors": "Yuki Akiyama, Minh Vu, Konstantinos Slavakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>¬†¬†This paper introduces a solution to the problem of selecting dynamically (online) the ``optimal'' p-norm to combat outliers in linear adaptive filtering without any knowledge on the probability density function of the outliers. The proposed online and data-driven framework is built on kernel-based reinforcement learning (KBRL). To this end, novel Bellman mappings on reproducing kernel Hilbert spaces (RKHSs) are introduced. These mappings do not require any knowledge on transition probabilities of Markov decision processes, and are nonexpansive with respect to the underlying Hilbertian norm. The fixed-point sets of the proposed Bellman mappings are utilized to build an approximate policy-iteration (API) framework for the problem at hand. To address the ``curse of dimensionality'' in RKHSs, random Fourier features are utilized to bound the computational complexity of the API. Numerical tests on synthetic data for several outlier scenarios demonstrate the superior performance of the proposed API framework over several non-RL and KBRL schemes.</p>\n<p><br></p>\n<p>----</p>\n<p>¬†¬© 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.¬†</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21376458.v1"
    },
    {
        "id": 14790,
        "title": "6. Template-based Analyses and Min-policy Iteration",
        "authors": "",
        "published": "2019-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9780691189581-006"
    },
    {
        "id": 14791,
        "title": "Quasi-Newton Iteration in Deterministic Policy Gradient",
        "authors": "Arash Bahari Kordabad, Hossein Nejatbakhsh Esfahani, Wenqi Cai, Sebastien Gros",
        "published": "2022-6-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867217"
    },
    {
        "id": 14792,
        "title": "A policy iteration method for Mean Field Games",
        "authors": "Fabio Camilli",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2022.11.087"
    },
    {
        "id": 14793,
        "title": "Innovation through iteration: Policy feedback loops in China‚Äôs economic reform",
        "authors": "Wendy Leutert",
        "published": "2021-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.worlddev.2020.105173"
    },
    {
        "id": 14794,
        "title": "Stochastic Policy Iteration Methods",
        "authors": "Denis Belomestny, John Schoenmakers",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1057/978-1-137-03351-2_7"
    },
    {
        "id": 14795,
        "title": "Least Squares Policy Iteration with Instrumental Variables vs. Direct Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage",
        "authors": "Somayeh Moazeni, Warren R. Scott, Warren Powell",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3150467"
    },
    {
        "id": 14796,
        "title": "The Forgotten First Iteration of the ‚ÄòChinese Space Threat‚Äô to US National Security",
        "authors": "Cameron Hunter",
        "published": "2019-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.spacepol.2018.11.003"
    },
    {
        "id": 14797,
        "title": "On-Policy Data-Driven Linear Quadratic Regulator via Combined Policy Iteration and Recursive Least Squares",
        "authors": "Lorenzo Sforni, Guido Carnevale, Ivano Notarnicola, Giuseppe Notarstefano",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383604"
    },
    {
        "id": 14798,
        "title": "Incremental Generalized Policy Iteration for Adaptive Attitude Tracking Control of a Spacecraft",
        "authors": "Yifei Li, Erik-Jan van Kampen",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178221"
    },
    {
        "id": 14799,
        "title": "Explicitly Coordinated Policy Iteration",
        "authors": "Yujing Hu, Yingfeng Chen, Changjie Fan, Jianye Hao",
        "published": "2019-8",
        "citations": 1,
        "abstract": "Coordination on an optimal policy between independent learners in fully cooperative stochastic games is difficult due to problems such as relative overgeneralization and miscoordination. Most state-of-the-art algorithms apply fusion heuristics on agents' optimistic and average rewards, by which coordination between agents can be achieved implicitly. However, such implicit coordination faces practical issues such as tedious parameter-tuning in real world applications. The lack of an explicit coordination mechanism may also lead to a low likelihood of coordination in problems with multiple optimal policies. Based on the necessary conditions of an optimal policy, we propose the explicitly coordinated policy iteration (EXCEL) algorithm which always forces agents to coordinate by comparing the agents' separated optimistic and average value functions. We also propose three solutions for deep reinforcement learning extensions of EXCEL. Extensive experiments in matrix games (from 2-agent 2-action games to 5-agent 20-action games) and stochastic games (from 2-agent games to 5-agent games) show that EXCEL has better performance than the state-of-the-art algorithms (such as faster convergence and better coordination).",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/51"
    },
    {
        "id": 14800,
        "title": "Robust Control of An Inverted Pendulum System Based on Policy Iteration in Reinforcement Learning",
        "authors": "Xu Dengguo, Ma Yan, Huang Jiashun, Li Yahui",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is primarily focused on the robust control of an inverted pendulum system based \r\non the policy iteration in reinforcement learning. First, a mathematical model of the single inverted \r\npendulum system is established through a force analysis of the pendulum and trolley. Second,  \r\nbased on the theory of robust optimal control, the robust control of the uncertain linear inverted  \r\npendulum system is transformed into an optimal control problem with an appropriate performance  \r\nindex. Moreover, for the uncertain linear and nonlinear systems, two reinforcement-learning control  \r\nalgorithms are proposed using the policy iteration method. Finally, two numerical examples are  \r\nprovided to validate the reinforcement learning algorithms for the robust control of the inverted  \r\npendulum systems.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1100.v1"
    },
    {
        "id": 14801,
        "title": "A Data-Driven Policy Iteration Scheme based on Linear Programming",
        "authors": "Goran Banjac, John Lygeros",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029405"
    },
    {
        "id": 14802,
        "title": "Autonomous Soaring Policy Initialization Through Value Iteration",
        "authors": "Benjamin J. Rothaupt, Stefan Notter, Walter Fichter",
        "published": "2021-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-2012"
    },
    {
        "id": 14803,
        "title": "Robust Controller Design Dased on Policy Iteration",
        "authors": "Qiang Gao, Xuxi Yang, Yuehui Ji, Junjie Liu",
        "published": "2021-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icma52036.2021.9512653"
    },
    {
        "id": 14804,
        "title": "Policy Iteration Solution for Differential Games with Constrained Control Policies",
        "authors": "Mohammed I. Abouheaf, Magdi S. Mahmoud, Frank L. Lewis",
        "published": "2019-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8814441"
    },
    {
        "id": 14805,
        "title": "A Fixed-Point Policy-Iteration-Type Algorithm for Symmetric Nonzero-Sum Stochastic Impulse Control Games",
        "authors": "Diego Zabaljauregui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3530171"
    },
    {
        "id": 14806,
        "title": "Dynamic Task Scheduling Via Policy Iteration Scheduling Approach for Cloud Computing",
        "authors": "",
        "published": "2017-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2017.03.001"
    },
    {
        "id": 14807,
        "title": "Discrete-time generalized policy iteration ADP algorithm with approximation errors",
        "authors": "Qinglai Wei, Benkai Li, Ruizhuo Song",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8285276"
    },
    {
        "id": 14808,
        "title": "Analytic policy function iteration",
        "authors": "Zhao Han, Fei Tan, Jieran Wu",
        "published": "2022-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jet.2021.105395"
    },
    {
        "id": 14809,
        "title": "Iteration Trees",
        "authors": "",
        "published": "2017-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781316718315.006"
    },
    {
        "id": 14810,
        "title": "Memory-Efficient Filter Based Novel Policy Iteration Technique for Adaptive LQR",
        "authors": "Sumit Kumar Jha, Sayan Basu Roy, Shubhendu Bhasin",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431061"
    },
    {
        "id": 14811,
        "title": "Optimal tracking control for reconfigurable manipulators based on critic-only policy iteration algorithm",
        "authors": "Hongbing Xia, Bo Zhao, Yuanchun Li",
        "published": "2017-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2017.8027757"
    },
    {
        "id": 14812,
        "title": "Optimal Cooperative Control of Multiagent System Based on Generalized Policy Iteration",
        "authors": "Haixing Li, Yong Zhang, Ke Wang, Chaoxu Mu",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10239850"
    },
    {
        "id": 14813,
        "title": "Adaptive Optimal Control of UAV Formation Based on Policy Iteration",
        "authors": "Guangyan Xu, Shugang Zhang, Hao Liu",
        "published": "2022-8-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033911"
    },
    {
        "id": 14814,
        "title": "Adversarial Attacks on Computation of the Modified Policy Iteration Method",
        "authors": "Ali Yekkehkhany, Han Feng, Javad Lavaei",
        "published": "2021-12-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683559"
    },
    {
        "id": 14815,
        "title": "PID Parameter Tuning of Flight Atmospheric Parameter Test System Based on Policy Iteration",
        "authors": "Tao Xu, Hexuan Zhang",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326639"
    },
    {
        "id": 14816,
        "title": "Distributed Model-Free Policy Iteration for Networks of Homogeneous Systems",
        "authors": "Shahriar Talebi, Siavash Alemzadeh, Mehran Mesbahi",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683331"
    },
    {
        "id": 14817,
        "title": "Distributed randomized multiagent policy iteration in reinforcement learning",
        "authors": "Weipeng Zhang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.rico.2023.100255"
    },
    {
        "id": 14818,
        "title": "Least Square Policy Iteration for IBVS based Dynamic Target Tracking",
        "authors": "Raunak Srivastava, Rolif Lima, Kaushik Das, Arnab Maity",
        "published": "2019-6",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas.2019.8798242"
    },
    {
        "id": 14819,
        "title": "Policy iteration approach to average optimal control problems for boolean control networks",
        "authors": "Yuhu Wu, Ximing Sun, Wei Wang, Tielong Shen",
        "published": "2017-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2017.8028620"
    },
    {
        "id": 14820,
        "title": "Deep Conservative Policy Iteration",
        "authors": "Nino Vieillard, Olivier Pietquin, Matthieu Geist",
        "published": "2020-4-3",
        "citations": 3,
        "abstract": "Conservative Policy Iteration (CPI) is a founding algorithm of Approximate Dynamic Programming (ADP). Its core principle is to stabilize greediness through stochastic mixtures of consecutive policies. It comes with strong theoretical guarantees, and inspired approaches in deep Reinforcement Learning (RL). However, CPI itself has rarely been implemented, never with neural networks, and only experimented on toy problems. In this paper, we show how CPI can be practically combined with deep RL with discrete actions, in an off-policy manner. We also introduce adaptive mixture rates inspired by the theory. We experiment thoroughly the resulting algorithm on the simple Cartpole problem, and validate the proposed method on a representative subset of Atari games. Overall, this work suggests that revisiting classic ADP may lead to improved and more stable deep RL algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v34i04.6070"
    },
    {
        "id": 14821,
        "title": "GPU Parallelization of Policy Iteration RRT#",
        "authors": "R. Connor Lawson, Linda Wills, Panagiotis Tsiotras",
        "published": "2020-10-24",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros45743.2020.9341411"
    },
    {
        "id": 14822,
        "title": "Quantum reinforcement learning via policy iteration",
        "authors": "El Amine Cherrat, Iordanis Kerenidis, Anupam Prakash",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42484-023-00116-1"
    },
    {
        "id": 14823,
        "title": "Optimal Control for Continuous-time Nonlinear Systems based on a Linear-like Policy Iteration",
        "authors": "Adnan Tahirovic, Alessandro Astolfi",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029697"
    },
    {
        "id": 14824,
        "title": "Improving LMI controllers for discrete nonlinear systems using policy iteration",
        "authors": "Henry Diaz, Antonio Sala, Leopoldo Armesto",
        "published": "2017-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc.2017.8107140"
    },
    {
        "id": 14825,
        "title": "Optimizing Traffic Flow with Policy Iteration Method: A Study on 9-Intersection Network Management",
        "authors": "A. Merbah, J. Ben-Othman",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10182420"
    },
    {
        "id": 14826,
        "title": "Lambda Iteration and Genetic Algorithms Application to solve the Economic Load Dispatch Problem of Seven Nigerian Thermal Power Plants",
        "authors": "",
        "published": "2021-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7176/jetp/11-4-04"
    },
    {
        "id": 14827,
        "title": "Homotopic policy iteration-based learning design for unknown linear continuous-time systems",
        "authors": "Ci Chen, Frank L. Lewis, Bo Li",
        "published": "2022-4",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.automatica.2021.110153"
    },
    {
        "id": 14828,
        "title": "Adaptive Optimal Control for Large-Scale Systems based on Robust Policy Iteration",
        "authors": "Fuyu Zhao, Liang Zhao",
        "published": "2022-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033503"
    },
    {
        "id": 14829,
        "title": "Approximate Policy Iteration with Linear Action Models",
        "authors": "Hengshuai Yao, Csaba Szepesvari",
        "published": "2021-9-20",
        "citations": 0,
        "abstract": "\n      \n        In this paper we consider the problem of finding a good policy given some batch data.We propose a new approach, LAM-API, that first builds a so-called linear action model (LAM) from the data and then uses  the learned model and the collected data   in approximate policy iteration (API) to find a good policy.A natural choice for the policy evaluation step in this algorithm is to use least-squares temporal difference (LSTD) learning algorithm.Empirical results on three benchmark problems show that this particular instance of LAM-API performs competitively as compared with LSPI, both from the point of view of data and computational efficiency.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v26i1.8319"
    },
    {
        "id": 14830,
        "title": "A Distributed Policy Evaluation Scheme Over Consensus Iteration",
        "authors": "Changli Pu, Gang Chen, Yaoyao Zhou, Yiqing Huang, Jianghong Ren, Xin Lai",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240148"
    },
    {
        "id": 14831,
        "title": "Multiagent Reinforcement Learning: Rollout and Policy Iteration",
        "authors": "Dimitri Bertsekas",
        "published": "2021-2",
        "citations": 50,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jas.2021.1003814"
    },
    {
        "id": 14832,
        "title": "Privacy-Preserving Policy Iteration for Decentralized POMDPs",
        "authors": "Feng Wu, Shlomo Zilberstein, Xiaoping Chen",
        "published": "2018-4-26",
        "citations": 2,
        "abstract": "\n      \n        We propose the first privacy-preserving approach to address the privacy issues that arise in multi-agent planning problems modeled as a Dec-POMDP. Our solution is a distributed message-passing algorithm based on trials, where the agents' policies are optimized using the cross-entropy method. In our algorithm, the agents' private information is protected using a public-key homomorphic cryptosystem. We prove the correctness of our algorithm and analyze its complexity in terms of message passing and encryption/decryption operations. Furthermore, we analyze several privacy aspects of our algorithm and show that it can preserve the agent privacy of non-neighbors, model privacy, and decision privacy. Our experimental results on several common Dec-POMDP benchmark problems confirm the effectiveness of our approach.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.11584"
    },
    {
        "id": 14833,
        "title": "Model-free policy iteration approach to NCE-based strategy design for linear quadratic Gaussian games",
        "authors": "Zhenhui Xu, Tielong Shen, Minyi Huang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.automatica.2023.111162"
    },
    {
        "id": 14834,
        "title": "Dual policy iteration-reinforcement learning to optimize the detection quality of passive remote sensing device",
        "authors": "Rui Guo, Zhonghao Fu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sigpro.2023.109002"
    },
    {
        "id": 14835,
        "title": "Neuro-Control for Continuous-Time Stochastic Nonlinear Systems via Online Policy Iteration Algorithm",
        "authors": "Tianmin Zhou, Jiaxu Hou, Handong Li, Zengru Di, Bo Zhao",
        "published": "2020-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc49329.2020.9164777"
    },
    {
        "id": 14836,
        "title": "Online Policy Iteration-Based Tracking Control of Four Wheeled Omni-Directional Robots",
        "authors": "Arash Sheikhlar, Ahmad Fakharian",
        "published": "2018-8-1",
        "citations": 3,
        "abstract": "In this paper, online policy iteration reinforcement learning (RL) algorithm is proposed for motion control of four wheeled omni-directional robots. The algorithm solves the linear quadratic tracking (LQT) problem in an online manner using real-time measurement data of the robot. This property enables the tracking controller to compensate the alterations of dynamics of the robot's model and environment. The online policy iteration based tracking method is employed as low level controller. On the other side, a proportional derivative (PD) scheme is performed as supervisory planning system (high level controller). In this study, the followed paths of online and offline policy iteration algorithms are compared in a rectangular trajectory in the presence of slippage drawback and motor heat. Simulation and implementation results of the methods demonstrate the effectiveness of the online algorithm compared to offline one in reducing the command trajectory tracking error and robot's path deviations. Besides, the proposed online controller shows a considerable ability in learning appropriate control policy on different types of surfaces. The novelty of this paper is proposition of a simple-structure learning based adaptive optimal scheme that tracks the desired path, optimizes the energy consumption, and solves the uncertainty problem in omni-directional wheeled robots.",
        "link": "http://dx.doi.org/10.1115/1.4039287"
    },
    {
        "id": 14837,
        "title": "Convergence and Iteration Complexity of Policy Gradient Method for Infinite-horizon Reinforcement Learning",
        "authors": "Kaiqing Zhang, Alec Koppel, Hao Zhu, Tamer Basar",
        "published": "2019-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9030265"
    },
    {
        "id": 14838,
        "title": "Approximate policy iteration for dynamic resource-constrained project scheduling",
        "authors": "Mahshid Salemi Parizi, Yasin Gocgun, Archis Ghate",
        "published": "2017-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.orl.2017.06.002"
    },
    {
        "id": 14839,
        "title": "Policy iteration based robust co-design for nonlinear control systems with state constraints",
        "authors": "Quan-Yong Fan, Guang-Hong Yang",
        "published": "2018-10",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2018.08.006"
    },
    {
        "id": 14840,
        "title": "A Policy Iteration Approach for Flock Motion Control",
        "authors": "Shuzheng Qu, Mohammed Abouheaf, Wail Gueaieb, Davide Spinello",
        "published": "2021-10-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rose52750.2021.9611776"
    },
    {
        "id": 14841,
        "title": "Conservative and Greedy Approaches to Classification-Based Policy Iteration",
        "authors": "Mohammad Ghavamzadeh, Alessandro Lazaric",
        "published": "2021-9-20",
        "citations": 0,
        "abstract": "\n      \n        The existing classification-based policy iteration (CBPI) algorithms can be divided into two categories: direct policy iteration (DPI) methods that directly assign the output of the classifier (the approximate greedy policy w.r.t.~the current policy) to the next policy, and conservative policy iteration (CPI) methods in which the new policy is a mixture distribution of the current policy and the output of the classifier. The conservative policy update gives CPI a desirable feature, namely the guarantee that the policies generated by this algorithm improve at each iteration. We provide a detailed algorithmic and theoretical comparison of these two classes of CBPI algorithms. Our results reveal that in order to achieve the same level of accuracy, CPI requires more iterations, and thus, more samples than the DPI algorithm. Furthermore, CPI may converge to suboptimal policies whose performance is not better than DPI's.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v26i1.8304"
    },
    {
        "id": 14842,
        "title": "Policy iteration-based indirect adaptive optimal control for completely unknown continuous-time LTI systems",
        "authors": "Sumit Kumar Jha, Sayan Basu Roy, Shubhendu Bhasin",
        "published": "2017-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8285349"
    },
    {
        "id": 14843,
        "title": "An Improved Minimax-Q Algorithm Based on Generalized Policy Iteration to Solve a Chaser-Invader Game",
        "authors": "Minsong Liu, Yuanheng Zhu, Dongbin Zhao",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207446"
    },
    {
        "id": 14844,
        "title": "Lower Bounds for Policy Iteration on Multi-action MDPs",
        "authors": "Kumar Ashutosh, Sarthak Consul, Bhishma Dedhia, Parthasarathi Khirwadkar, Sahil Shah, Shivaram Kalyanakrishnan",
        "published": "2020-12-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9303956"
    },
    {
        "id": 14845,
        "title": "Center strategies for universal transformations: modified iteration policy and two alternative models",
        "authors": "Yu Hu, Xing Fang, Hansj√∂rg Kutterer",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10291-023-01419-3"
    },
    {
        "id": 14846,
        "title": "The Main Iteration Lemma",
        "authors": "Philip Isett",
        "published": "2017-10-19",
        "citations": 0,
        "abstract": "This chapter properly formalizes the Main Lemma, first by discussing the frequency energy levels for the Euler-Reynolds equations. Here the bounds are all consistent with the symmetries of the Euler equations, and the scaling symmetry is reflected by dimensional analysis. The chapter proceeds by making assumptions that are consistent with the Galilean invariance of the Euler equations and the Euler-Reynolds equations. If (v, p, R) solve the Euler-Reynolds equations, then a new solution to Euler-Reynolds with the same frequency energy levels can be obtained. The chapter also states the Main Lemma, taking into account dimensional analysis, energy regularity, and Onsager's conjecture. Finally, it introduces the main theorem (Theorem 10.1), which states that there exists a nonzero solution to the Euler equations with compact support in time.",
        "link": "http://dx.doi.org/10.23943/princeton/9780691174822.003.0010"
    },
    {
        "id": 14847,
        "title": "Multi-Objective Optimization Control for Discrete-Time Nonlinear Systems: A Policy Iteration Approach",
        "authors": "Chenggang Zhou, Man Li, Zhijian Cheng",
        "published": "2020-11-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccss52145.2020.9336762"
    },
    {
        "id": 14848,
        "title": "Least squares approximate policy iteration for learning bid prices in choice-based revenue management",
        "authors": "Sebastian Koch",
        "published": "2017-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cor.2016.07.015"
    },
    {
        "id": 14849,
        "title": "Safety-critical Policy Iteration Algorithm for Control under Model Uncertainty",
        "authors": "Navid Moshtaghi Yazdani, Reihaneh Kardehi Moghaddam, Mohammad Hasan Olyaei",
        "published": "2022-4-11",
        "citations": 0,
        "abstract": "Safety is an important aim in designing safe-critical systems. To design such systems, many policy iterative algorithms are introduced to find safe optimal controllers. Due to the fact that in most practical systems, finding accurate information from the system is rather impossible, a new online training method is presented in this paper to perform an iterative reinforcement learning based algorithm using real data instead of identifying system dynamics. Also, in this paper the impact of model uncertainty is examined on control Lyapunov functions (CLF) and control barrier functions (CBF) dynamic limitations. The Sum of Square program is used to iteratively find an optimal safe control solution. The simulation results which are applied on a quarter car model show the efficiency of the proposed method in the fields of optimality and robustness.",
        "link": "http://dx.doi.org/10.30564/aia.v4i1.4361"
    },
    {
        "id": 14850,
        "title": "A policy iteration method for mean field games",
        "authors": "Simone Cacace, Fabio Camilli, Alessandro Goffi",
        "published": "2021",
        "citations": 10,
        "abstract": "The policy iteration method is a classical algorithm for solving optimal control problems. In this paper, we introduce a policy iteration method for Mean Field Games systems, and we study the convergence of this procedure to a solution of the problem. We also introduce suitable discretizations to numerically solve both stationary and evolutive problems. We show the convergence of the policy iteration method for the discrete problem and we study the performance of the proposed algorithm on some examples in dimension one and two.",
        "link": "http://dx.doi.org/10.1051/cocv/2021081"
    },
    {
        "id": 14851,
        "title": "8 Optimal Intermittent Feedback via Least Square Policy Iteration",
        "authors": "",
        "published": "2017-4-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315367934-11"
    },
    {
        "id": 14852,
        "title": "The Convergence of Iteration Sequences",
        "authors": "Ioannis K. Argyros",
        "published": "2021-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003128915-2"
    },
    {
        "id": 14853,
        "title": "Computational Complexity of Asynchronous Policy Iteration for Two-Player Zero-Sum Markov Games",
        "authors": "Chenyu Xu, Sihai Zhang, Zhengdao Wang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447319"
    },
    {
        "id": 14854,
        "title": "Policy iteration vs Q-Sarsa approach optimization for embedded system communications with energy harvesting",
        "authors": "Mohammed Assaouy, Ouadoudi Zytoune, Driss Aboutajdine",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/atsip.2017.8075557"
    },
    {
        "id": 14855,
        "title": "Robust Policy Iteration of Uncertain Interconnected Systems With Imperfect Data",
        "authors": "Omar Qasem, Weinan Gao",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tase.2023.3276369"
    },
    {
        "id": 14856,
        "title": "The Image as Iteration",
        "authors": "Peter Sealy",
        "published": "2020-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429402166-8"
    },
    {
        "id": 14857,
        "title": "Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning",
        "authors": "Shangtong Zhang, Bo Liu, Shimon Whiteson",
        "published": "2021-5-18",
        "citations": 8,
        "abstract": "We present a mean-variance policy iteration (MVPI) framework for risk-averse control in a discounted infinite horizon MDP optimizing the variance of a per-step reward random variable.\nMVPI enjoys great flexibility in that\nany policy evaluation method and risk-neutral control method can be dropped in for risk-averse control off the shelf,\nin both on- and off-policy settings.\nThis flexibility reduces the gap between risk-neutral control and risk-averse control and is achieved by working on a novel augmented MDP directly.\nWe propose risk-averse TD3 as an example instantiating MVPI,\nwhich outperforms vanilla TD3 and many previous risk-averse control methods in challenging Mujoco robot simulation tasks under a risk-aware performance metric.\nThis risk-averse TD3 is the first to introduce deterministic policies and off-policy learning into risk-averse reinforcement learning,\nboth of which are key to the performance boost we show in Mujoco domains.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i12.17302"
    },
    {
        "id": 14858,
        "title": "AssistMe: Using policy iteration to improve shared control of a non-holonomic vehicle",
        "authors": "Catalin Stefan Teodorescu, Tom Carlson",
        "published": "2022-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945423"
    },
    {
        "id": 14859,
        "title": "On finite-control-set MPC for switched-mode power converters: Improved tracking cost function and fast policy iteration solver",
        "authors": "Duo Xu, Mircea Lazar",
        "published": "2022-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta49430.2022.9965987"
    },
    {
        "id": 14860,
        "title": "RATE OF CONVERGENCE OF S-ITERATION, SP ITERATION AND KS-ITERATION FOR CONTINUOUS FUNCTIONS ON CLOSED INTERVAL",
        "authors": "Kritsana Sokhuma, Naknimit Akkasriworn",
        "published": "2017-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17654/ms102020409"
    },
    {
        "id": 14861,
        "title": "The convergence of iteration sequences",
        "authors": "Ioannis K. Argyros, Ferenc Szidarovszky",
        "published": "2018-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780203719169-2"
    },
    {
        "id": 14862,
        "title": "Offline Policy Iteration Based Reinforcement Learning Controller for Online Robotic Knee Prosthesis Parameter Tuning",
        "authors": "Minhan Li, Xiang Gao, Yue Wen, Jennie Si, He Helen Huang",
        "published": "2019-5",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794212"
    },
    {
        "id": 14863,
        "title": "Policy Iteration Adaptive Dynamic Programming Based Control for Hypersonic Flight Vehicles",
        "authors": "Yuanhang Zhang, Jingbo Fu, Chen Liang, Chenliang Wang, Jianzhong Qiao",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450781"
    },
    {
        "id": 14864,
        "title": "Online Speech Enhancement by Retraining of LSTM Using SURE Loss and Policy Iteration",
        "authors": "Sriharsha Koundinya, Abhijit Karmakar",
        "published": "2021-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11063-021-10535-5"
    },
    {
        "id": 14865,
        "title": "Policy iteration and coupled Riccati solutions for dynamic graphical games",
        "authors": "Mohammed I. Abouheaf, Magdi Sadek Mahmoud",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijdsss.2017.10008987"
    },
    {
        "id": 14866,
        "title": "Policy iteration and coupled Riccati solutions for dynamic graphical games",
        "authors": "Mohammed I. Abouheaf, Magdi Sadek Mahmoud",
        "published": "2017",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijdsss.2017.088058"
    },
    {
        "id": 14867,
        "title": "Composite robust control of uncertain nonlinear systems with unmatched disturbances using policy iteration",
        "authors": "Quan-Yong Fan, Hongru Jiang, Xuekui Song, Bin Xu",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.isatra.2023.03.028"
    },
    {
        "id": 14868,
        "title": "A policy iteration method for improving robot assembly trajectory efficiency",
        "authors": "Qi ZHANG, Zongwu XIE, Baoshi CAO, Yang LIU",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cja.2022.07.014"
    },
    {
        "id": 14869,
        "title": "Policy iteration for Hamilton‚ÄìJacobi‚ÄìBellman equations with control constraints",
        "authors": "Sudeep Kundu, Karl Kunisch",
        "published": "2021-4-24",
        "citations": 2,
        "abstract": "AbstractPolicy iteration is a widely used technique to solve the Hamilton Jacobi Bellman (HJB) equation, which arises from nonlinear optimal feedback control theory. Its convergence analysis has attracted much attention in the unconstrained case. Here we analyze the case with control constraints both for the HJB equations which arise in deterministic and in stochastic control cases. The linear equations in each iteration step are solved by an implicit upwind scheme. Numerical examples are conducted to solve the HJB equation with control constraints and comparisons are shown with the unconstrained cases.",
        "link": "http://dx.doi.org/10.1007/s10589-021-00278-3"
    },
    {
        "id": 14870,
        "title": "Shelah-Stupp‚Äôs Iteration and Muchnik‚Äôs Iteration",
        "authors": "Didier Caucal, Teodor Knapik",
        "published": "2018-3-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/fi-2018-1667"
    }
]