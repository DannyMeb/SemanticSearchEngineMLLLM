[
    {
        "id": 32705,
        "title": "Deep Deterministic Policy Gradient for Nested Parallel Negotiation",
        "authors": "Ryota Arakawa, Katsuhide Fujita",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wi-iat59888.2023.00032"
    },
    {
        "id": 32706,
        "title": "Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient",
        "authors": "Ming Zhan, Jingjing Fan, Jianying Guo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3305453"
    },
    {
        "id": 32707,
        "title": "Local Path Planning with Turnabouts for Mobile Robot by Deep Deterministic Policy Gradient",
        "authors": "Tomoaki Nakamura, Masato Kobayashi, Naoki Motoi",
        "published": "2023-3-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icm54990.2023.10101921"
    },
    {
        "id": 32708,
        "title": "Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge",
        "authors": "Bolun Zhang, Nguyen Van Huynh",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436824"
    },
    {
        "id": 32709,
        "title": "Multi-AUV Charging Navigation Trajectory Planning Based on Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Jiaming Yu, Hao Sun, Qinglin Sun",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240061"
    },
    {
        "id": 32710,
        "title": "Deep Deterministic Policy Gradient (DDPG) Agent-Based Sliding Mode Control for Quadrotor Attitudes",
        "authors": "Wenjun Hu, Yueneng Yang, Zhiyang Liu",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "A novel reinforcement deep learning deterministic policy gradient agent-based sliding mode control (DDPG-SMC) approach is proposed to suppress the chattering phenomenon in attitude control for quadrotors, in the presence of external disturbances. First, the attitude dynamics model of the quadrotor under study is derived, and the attitude control problem is described using formulas. Second, a sliding mode controller, including its sliding mode surface and reaching law, is chosen for the nonlinear dynamic system. The stability of the designed SMC system is validated through the Lyapunov stability theorem. Third, a reinforcement learning (RL) agent based on deep deterministic policy gradient (DDPG) is trained to adaptively adjust the switching control gain. During the training process, the input signals for the agent are the actual and desired attitude angles, while the output action is the time-varying control gain. Finally, the trained agent mentioned above is utilized in the SMC as a parameter regulator to facilitate the adaptive adjustment of the switching control gain associated with the reaching law. The simulation results validate the robustness and effectiveness of the proposed DDPG-SMC method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/drones8030095"
    },
    {
        "id": 32711,
        "title": "Development of a Deep Deterministic Policy Gradient (DDPG) Algorithm for Suturing Task Automation",
        "authors": "Antonella Imperato, Marco Caianiello, Fanny Ficuciello",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406967"
    },
    {
        "id": 32712,
        "title": "Power Control in Device-to-Device Communications using Deep Deterministic Policy Gradient Method",
        "authors": "Ranjeet Kumar, Saikat Majumder",
        "published": "2023-3-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112096"
    },
    {
        "id": 32713,
        "title": "Enhanced Fingerprint Image Compression using Deep Deterministic Policy Gradient",
        "authors": "Abdelhak Ouanane, Mohamed Riad Yagoubi, Amina Serir, Nacereddine Djelal",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceeat60471.2023.10425859"
    },
    {
        "id": 32714,
        "title": "Smart Noise Jamming Power Adjustment Using Exploratory Deep Deterministic Policy Gradient",
        "authors": "Yujie Zhang, Weibo Huo, Cui Zhang, Jifang Pei, Yin Zhang, Yulin Huang",
        "published": "2023-5-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149662"
    },
    {
        "id": 32715,
        "title": "Deep Deterministic Policy Gradient for Throughput Maximization in Energy Harvesting NOMA-Cognitive Radio Network",
        "authors": "Lav Garg, Saikat Majumder, Sumit Chakravarty",
        "published": "2023-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iconat57137.2023.10080443"
    },
    {
        "id": 32716,
        "title": "Integrating Multi-Agent Deep Deterministic Policy Gradient and Go-Explore for Enhanced Reward Optimization",
        "authors": "Muchen Liu",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "The field of Multi-Agent Reinforcement Learning (MARL) continues to advance with the development of new and effective methods. This research is centered on two prominent approaches within this field: Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Go-Explore. The study explores the synergistic potential of combining these two methodologies to enhance rewards for individual agents as well as for agent groups. In the course of this research, MADDPG is introduced into the experimental environment, providing agents with both actor networks (policy networks) and critic networks (Q networks) to implement the actor-critic model. Additionally, each individual agent is equipped with a Go-Explore network, empowering them to conduct deeper explorations of the environment and accumulate rewards at an accelerated rate, often resulting in higher overall rewards. This novel approach emphasizes achieving a balance between individual and collaborative rewards, offering a promising avenue for optimizing multi-agent systems. The results of this study demonstrate that the combined method exhibits notable advantages in certain scenarios. Specifically, it showcases a higher rate of reward accumulation and improved overall performance. This research contributes to the MARL domain by highlighting the potential of combining MADDPG and Go-Explore to enhance the efficiency and effectiveness of multi-agent systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/znrt8d63"
    },
    {
        "id": 32717,
        "title": "Dynamic Prioritization and Adaptive Scheduling Using Deep Deterministic Policy Gradient for Deploying Microservice-Based VNFs",
        "authors": "Swarna B. Chetty, Hamed Ahmadi, Avishek Nag",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278718"
    },
    {
        "id": 32718,
        "title": "Hybrid Energy Storage Control Method For DC Microgrid Based On Deep Deterministic Policy Gradient",
        "authors": "Shengji Tan, Min Ding, Zili Tao, Danyun Li, Zhijian Fang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450954"
    },
    {
        "id": 32719,
        "title": "Research on Automatic Lane Changing Method for Electric Vehicles Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060104"
    },
    {
        "id": 32720,
        "title": "A Multi-Agent Deep Deterministic Policy Gradient Method for Multi-Zone HVAC Control",
        "authors": "Xuebo Liu, Yingying Wu, Bo Liu, Hongyu Wu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pesgm52003.2023.10252541"
    },
    {
        "id": 32721,
        "title": "Power Allocation in Cell-Free mmWave Massive MIMO: Using Deep Deterministic Policy Gradient",
        "authors": "Yu Zhao, Fengming Zhang, Yangjun Gao, Chaoqi Fu",
        "published": "2023-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wccct56755.2023.10052585"
    },
    {
        "id": 32722,
        "title": "Deep Deterministic Policy Gradient based Dynamic Virtual Network Embedding Algorithm",
        "authors": "Yue Zong, Han Xu, Zhaoyang Zhang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640912.3640982"
    },
    {
        "id": 32723,
        "title": "Deep reinforcement learning for PMSG wind turbine control via twin delayed deep deterministic policy gradient (TD3)",
        "authors": "Darkhan Zholtayev, Matteo Rubagotti, Ton Duc Do",
        "published": "2024-4-7",
        "citations": 0,
        "abstract": "AbstractThis article proposes the use of a deep reinforcement learning method—and precisely a variant of the deep deterministic policy gradient (DDPG) method known as twin delayed DDPG, or TD3—for maximum power point tracking in wind energy conversion systems that use permanent magnet synchronous generators (PMSGs). An overview of the TD3 algorithm is provided, together with a detailed description of its implementation and training for the considered application. Simulation results are provided, also including a comparison with a model‐based control method based on feedback linearization and linear‐quadratic regulation. The proposed TD3‐based controller achieves a satisfactory control performance and is more robust to PMSG parameter variations as compared to the presented model‐based method. To the best of the authors' knowledge, this article presents for the first time an approach for generating both speed and current control loops using DRL for wind energy conversion systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/oca.3129"
    },
    {
        "id": 32724,
        "title": "Attitude Control of Rotary Steering Drilling Stabilized Platform Based on Improved Deep Deterministic Policy Gradient",
        "authors": "Aiqing Huo, Kun Zhang, Shuhan Zhang",
        "published": "2024-2-14",
        "citations": 2,
        "abstract": "Summary\nThe rotary steerable drilling system is an advanced drilling technology, with stabilized platform toolface attitude control being a critical component. Due to a multitude of downhole interference factors, coupled with nonlinearities and uncertainties, challenges arise in model establishment and attitude control. Furthermore, considering that stabilized platform toolface attitude determines the drilling direction of the entire drill bit, the effectiveness of toolface attitude control will directly impact the precision and success of drilling tool guidance. In this paper, a mathematical model and a friction model of the stabilized platform are established, and an improved deep deterministic policy gradient (I_DDPG) attitude control method is proposed to address the friction nonlinearity problem existing in the rotary steering drilling stabilized platform. A prioritized experience replay based on temporal difference (TD) error and policy gradient is introduced to improve sample usage, and high similarity samples are pruned to prevent overfitting. Furthermore, SumTree structure is adopted to sort samples for reducing computational effort, and a double critic network is used to alleviate the overestimated value. Numerical simulation results illustrate that the stabilized platform attitude control system based on I_DDPG can achieve high control accuracy with both strong anti-interference capability and good robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/217992-pa"
    },
    {
        "id": 32725,
        "title": "Simulated Annealing-Deep Deterministic Policy Gradient Algorithm For Quadrotor Attitude Control",
        "authors": "Taha Yacine Trad, Kheireddine Choutri, Mohand Lagha, Raouf Fareh, Maamar Bettayeb",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aset56582.2023.10180752"
    },
    {
        "id": 32726,
        "title": "Optimization of an MPC Based Motion Cueing Algorithm with Deep Deterministic Policy Gradient",
        "authors": "Yi Liang, Dongsu Wu, Rongjun Fu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccasit58768.2023.10351597"
    },
    {
        "id": 32727,
        "title": "Comparative Study for Deep Deterministic Policy Gradient and Soft Actor Critic Using an Inverted Pendulum System",
        "authors": "Aditya Shelke, Devang Vyas, Abhishek Srivastava",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370289"
    },
    {
        "id": 32728,
        "title": "Retracted: An Automatic Driving Control Method Based on Deep Deterministic Policy Gradient",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9896258"
    },
    {
        "id": 32729,
        "title": "A Control Method of Robotic Arm Based on Improved Deep Deterministic Policy Gradient",
        "authors": "Yanpeng Shao, Haibo Zhou, Shuaishuai Zhao, Xiaoyan Fan, Jiayi Jiang",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icma57826.2023.10215662"
    },
    {
        "id": 32730,
        "title": "Power flow rebalancing in optimal scheduling of smart distribution systems based on Deep Deterministic Policy Gradient",
        "authors": "Xinyu Ai, Junfeng Cai, Jingrui Zhang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135884"
    },
    {
        "id": 32731,
        "title": "A Task-Oriented Hybrid Routing Approach based on Deep Deterministic Policy Gradient",
        "authors": "Zongxuan Sha, Ru Huo, Chuang Sun, Shuo Wang, Tao Huang",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comcom.2023.07.040"
    },
    {
        "id": 32732,
        "title": "Design of missile longitudinal controller based on deep deterministic policy gradient learning algorithm",
        "authors": "Wanchao Zhang, Guangshan Chen, Hao Ni, TingShuai Tong, YiWen Zhou",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291719"
    },
    {
        "id": 32733,
        "title": "UAV Coverage Path Planning with Quantum-based Recurrent Deep Deterministic Policy Gradient",
        "authors": " Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2023.3347219"
    },
    {
        "id": 32734,
        "title": "Trust Metric-Based Anomaly Detection via Deep Deterministic Policy Gradient Reinforcement Learning Framework",
        "authors": "Shruthi N, Siddesh G K",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "Addressing real-time network security issues is paramount due to the rapidly expanding IoT jargon. The erratic rise in usage of inadequately secured IoT- based sensory devices like wearables of mobile users, autonomous vehicles, smartphones and appliances by a larger user community is fuelling the need for a trustable, super-performant security framework. An efficient anomaly detection system would aim to address the anomaly detection problem by devising a competent attack detection model. This paper delves into the Deep Deterministic Policy Gradient (DDPG) approach, a promising Reinforcement Learning platform to combat noisy sensor samples which are instigated by alarming network attacks. The authors propose an enhanced DDPG approach based on trust metrics and belief networks, referred to as Deep Deterministic Policy Gradient Belief Network (DDPG-BN). This deep-learning-based approach is projected as an algorithm to provide “Deep-Defense” to the plethora of network attacks. Confidence interval is chosen as the trust metric to decide on the termination of sensor sample collection. Once an enlisted attack is detected, the collection of samples from the particular sensor will automatically cease. The evaluations and results of the experiments highlight a better detection accuracy of 98.37% compared to its counterpart conventional DDPG implementation of 97.46%. The paper also covers the work based on a contemporary Deep Reinforcement Learning (DRL) algorithm, the Actor Critic (AC). The proposed deep learning binary classification model is validated using the NSL-KDD dataset and the performance is compared to a few deep learning implementations as well.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijcnc.2023.15601"
    },
    {
        "id": 32735,
        "title": "A Switching Strategy for Run-to-Run Control Using Deep Deterministic Policy Gradient Algorithm and Integral Controller",
        "authors": "Zhu Ma, Tianhong Pan",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450404"
    },
    {
        "id": 32736,
        "title": "A Dual-Critic Deep Deterministic Policy Gradient Approach for Task Offloading in Edge-Fog-Cloud Environment",
        "authors": "Moshira A. Ebrahim, Gamal A. Ebrahim, Hoda K. Mohamed",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icca59364.2023.10401692"
    },
    {
        "id": 32737,
        "title": "Deep Deterministic Policy Gradient With Compatible Critic Network",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023-8",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3117790"
    },
    {
        "id": 32738,
        "title": "Twin delayed deep deterministic policy gradient-based intelligent computation offloading for IoT",
        "authors": "Siguang Chen, Bei Tang, Kun Wang",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dcan.2022.06.008"
    },
    {
        "id": 32739,
        "title": "Swap Softmax Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Chaohu Liu, Yunbo Zhao",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isas59543.2023.10164333"
    },
    {
        "id": 32740,
        "title": "Optimization control of the double‐capacity water tank‐level system using the deep deterministic policy gradient algorithm",
        "authors": "Likun Ye, Pei Jiang",
        "published": "2023-11",
        "citations": 0,
        "abstract": "AbstractProcess control systems are subject to external factors such as changes in working conditions and perturbation interference, which can significantly affect the system's stability and overall performance. The application and promotion of intelligent control algorithms with self‐learning, self‐optimization, and self‐adaption characteristics have thus become a challenging yet meaningful research topic. In this article, we propose a novel approach that incorporates the deep deterministic policy gradient (DDPG) algorithm into the control of double‐capacity water tanklevel system. Specifically, we introduce a fully connected layer on the observer side of the critic network to enhance its expression capability and processing efficiency, allowing for the extraction of important features for water‐level control. Additionally, we optimize the node parameters of the neural network and use the RELU activation function to ensure the network's ability to continuously observe and learn from the external water tank environment while avoiding the issue of vanishing gradients. We enhance the system's feedback regulation ability by adding the PID controller output to the observer input based on the liquid level deviation and height. This integration with the DDPG control method effectively leverages the benefits of both, resulting in improved robustness and adaptability of the system. Experimental results show that our proposed model outperforms traditional control methods in terms of convergence, tracking, anti‐disturbance and robustness performances, highlighting its effectiveness in improving the stability and precision of double‐capacity water tank systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/eng2.12668"
    },
    {
        "id": 32741,
        "title": "5G communication resource allocation strategy for mobile edge computing based on deep deterministic policy gradient",
        "authors": "Jun He",
        "published": "2023-3",
        "citations": 2,
        "abstract": "AbstractDistributed base station deployment, limited server resources and dynamically changing end users in mobile edge networks make the design of computing offloading schemes extremely challenging. Considering the advantages of deep reinforcement learning (DRL) in dealing with dynamic complex problems, this paper designs an optimal computing offloading and resource allocation strategy. Firstly, the authors consider a multi‐user mobile edge network scenario consisting of Macro‐cell Base Station (MBS), Small‐cell Base Station (SBS) and multiple terminal devices, the communication overhead and calculation overhead generated are formulated and described in detail. Besides, combined with the deterministic delay of tasks, the optimization objective of this paper is clarified to comprehensively consider system energy consumption. Then, a learning algorithm based on Deep Deterministic Policy Gradient (DDPG) is proposed to minimize system energy consumption. Finally, simulation experiments show that the authors’ proposed DDPG algorithm can effectively optimize the target value, and the total system energy consumption is only 15.6 J, which is better than other compared algorithms. It is also proved that the proposed algorithm has excellent communication resource allocation ability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/tje2.12250"
    },
    {
        "id": 32742,
        "title": "Multi-PET Cooperative Autonomous Navigation Based on Multi-agent Deep Deterministic Policy Gradient",
        "authors": "Yichuan Huang, Yuhui Song, Zeyang Liu, Zhanhua Pan, Jisong Zhu, Zhaoxia Jing",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135975"
    },
    {
        "id": 32743,
        "title": "Integrated Multi-Energy Scheduling Strategy for Smart Community based on Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Mengfei Wen, Yujie Wang, Jie Chen, Ren Zhu",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/segre58867.2023.00055"
    },
    {
        "id": 32744,
        "title": "An enhanced deep deterministic policy gradient algorithm for intelligent control of robotic arms",
        "authors": "Ruyi Dong, Junjie Du, Yanan Liu, Ali Asghar Heidari, Huiling Chen",
        "published": "2023-1-23",
        "citations": 3,
        "abstract": "Aiming at the poor robustness and adaptability of traditional control methods for different situations, the deep deterministic policy gradient (DDPG) algorithm is improved by designing a hybrid function that includes different rewards superimposed on each other. In addition, the experience replay mechanism of DDPG is also improved by combining priority sampling and uniform sampling to accelerate the DDPG’s convergence. Finally, it is verified in the simulation environment that the improved DDPG algorithm can achieve accurate control of the robot arm motion. The experimental results show that the improved DDPG algorithm can converge in a shorter time, and the average success rate in the robotic arm end-reaching task is as high as 91.27%. Compared with the original DDPG algorithm, it has more robust environmental adaptability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fninf.2023.1096053"
    },
    {
        "id": 32745,
        "title": "Deep deterministic policy gradient with constraints for gait optimisation of biped robots",
        "authors": "Xingyang Liu, Haina Rong, Ferrante Neri, Peng Yue, Gexiang Zhang",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "In this paper, we propose a novel Reinforcement Learning (RL) algorithm for robotic motion control, that is, a constrained Deep Deterministic Policy Gradient (DDPG) deviation learning strategy to assist biped robots in walking safely and accurately. The previous research on this topic highlighted the limitations in the controller’s ability to accurately track foot placement on discrete terrains and the lack of consideration for safety concerns. In this study, we address these challenges by focusing on ensuring the overall system’s safety. To begin with, we tackle the inverse kinematics problem by introducing constraints to the damping least squares method. This enhancement not only addresses singularity issues but also guarantees safe ranges for joint angles, thus ensuring the stability and reliability of the system. Based on this, we propose the adoption of the constrained DDPG method to correct controller deviations. In constrained DDPG, we incorporate a constraint layer into the Actor network, incorporating joint deviations as state inputs. By conducting offline training within the range of safe angles, it serves as a deviation corrector. Lastly, we validate the effectiveness of our proposed approach by conducting dynamic simulations using the CRANE biped robot. Through comprehensive assessments, including singularity analysis, constraint effectiveness evaluation, and walking experiments on discrete terrains, we demonstrate the superiority and practicality of our approach in enhancing walking performance while ensuring safety. Overall, our research contributes to the advancement of biped robot locomotion by addressing gait optimisation from multiple perspectives, including singularity handling, safety constraints, and deviation learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/ica-230724"
    },
    {
        "id": 32746,
        "title": "Reinforcement Learning Approach with Deep Deterministic Policy Gradient DDPG-Controlled Virtual Synchronous Generator for an Islanded Microgrid",
        "authors": "Mohamed A. Afifi, Mostafa I. Marei, Ahmed M.I. Mohamad",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mepcon58725.2023.10462333"
    },
    {
        "id": 32747,
        "title": "A path following controller for deep-sea mining vehicles considering slip control and random resistance based on improved deep deterministic policy gradient",
        "authors": "Qihang Chen, Jianmin Yang, Jinghang Mao, Zhixuan Liang, Changyu Lu, Pengfei Sun",
        "published": "2023-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.114069"
    },
    {
        "id": 32748,
        "title": "Research on Wargame Decision-Making Method Based on Multi-Agent Deep Deterministic Policy Gradient",
        "authors": "Sheng Yu, Wei Zhu, Yong Wang",
        "published": "2023-4-4",
        "citations": 0,
        "abstract": "Wargames are essential simulators for various war scenarios. However, the increasing pace of warfare has rendered traditional wargame decision-making methods inadequate. To address this challenge, wargame-assisted decision-making methods that leverage artificial intelligence techniques, notably reinforcement learning, have emerged as a promising solution. The current wargame environment is beset by a large decision space and sparse rewards, presenting obstacles to optimizing decision-making methods. To overcome these hurdles, a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) based wargame decision-making method is presented. The Partially Observable Markov Decision Process (POMDP), joint action-value function, and the Gumbel-Softmax estimator are applied to optimize MADDPG in order to adapt to the wargame environment. Furthermore, a wargame decision-making method based on the improved MADDPG algorithm is proposed. Using supervised learning in the proposed approach, the training efficiency is improved and the space for manipulation before the reinforcement learning phase is reduced. In addition, a policy gradient estimator is incorporated to reduce the action space and to obtain the global optimal solution. Furthermore, an additional reward function is designed to address the sparse reward problem. The experimental results demonstrate that our proposed wargame decision-making method outperforms the pre-optimization algorithm and other algorithms based on the AC framework in the wargame environment. Our approach offers a promising solution to the challenging problem of decision-making in wargame scenarios, particularly given the increasing speed and complexity of modern warfare.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13074569"
    },
    {
        "id": 32749,
        "title": "Optimization of Robotic Arm Grasping through Fractional-Order Deep Deterministic Policy Gradient Algorithm",
        "authors": "Hui Geng, Qi Hu, Zhe Wang",
        "published": "2023-11-1",
        "citations": 1,
        "abstract": "Abstract\nWith the rapid development of robotics technology, robotic arm grasping has gained significant attention in the fields of automation and artificial intelligence. In this study, we propose a fractional-order deep deterministic policy gradient (DDPG) algorithm for optimizing robotic arm grasping tasks. Traditional machine learning algorithms face challenges in handling continuous action spaces, while the DDPG algorithm effectively addresses this issue. In this research, we first review the background and challenges of robotic arm grasping and provide an overview of the application of traditional reinforcement learning algorithms in grasping tasks. Subsequently, we introduce the principles and fundamental ideas of the DDPG algorithm in detail, discussing its potential for optimizing robotic arm grasping. To further enhance the performance of robotic arm grasping, we propose an improved approach based on fractional-order control. Fractional-order control exhibits unique advantages in environmental dynamics modeling and grasp posture optimization, enhancing the robustness and adaptability of robotic arm grasping. Through a series of experiments, we validate the effectiveness and superiority of the fractional-order DDPG algorithm in robotic arm grasping tasks. Our algorithm achieves significant improvements in grasping success rate and stability compared to traditional methods. The experimental results demonstrate that the fractional-order DDPG algorithm is better equipped to handle control challenges in continuous action spaces and optimize the performance of robotic arm grasping tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2637/1/012006"
    },
    {
        "id": 32750,
        "title": "Energy Management for Hybrid Energy Storage System in Electric Based on Deep Deterministic Policy Gradient",
        "authors": "Shuai Xia, Chun Wang",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": "In this paper, an intelligent control system design scheme based on deep deterministic policy gradient (DDPG) algorithm is proposed for the complex continuous action space problem in the hybrid energy storage system of electric vehicles. Firstly, the basic principle and internal logic of DDPG algorithm are introduced, including key elements such as Actor-Critic architecture, experience playback, target network, reward signal, policy gradient and value function update. Then, how to apply the DDPG algorithm to the industrial control system is described in detail. The Actor network learns the optimal strategy, the Critic network evaluates the value of the state-action pair, and uses the experience playback and the target network to improve the system stability and performance. Finally, the effect of the intelligent control system based on DDPG algorithm in complex environment is verified by simulation experiments. The results show that the system can effectively optimize the control strategy, improve the response speed and stability of the system, and has a good engineering application prospect.",
        "keywords": "",
        "link": "http://dx.doi.org/10.62051/ijcsit.v2n1.22"
    },
    {
        "id": 32751,
        "title": "A Deep Deterministic Policy Gradient Algorithm Based Controller with Adjustable Learning Rate for DC-AC Inverters",
        "authors": "Jian Ye, Sen Mei, Huanyu Guo, Di Zhao, Xinan Zhang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/peas58692.2023.10395502"
    },
    {
        "id": 32752,
        "title": "Enhancing Crane Handling Safety: A Deep Deterministic Policy Gradient Approach to Collision-Free Path Planning",
        "authors": "Rafaela Iovanovichi Machado, Matheus Dos Santos Machado, Silvia Silva Da Costa Botelho",
        "published": "2023-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indin51400.2023.10218087"
    },
    {
        "id": 32753,
        "title": "Deep deterministic policy gradient reinforcement learning for collision-free navigation of mobile robots in unknown environments",
        "authors": "Taner Yılmaz, Ömür Aydoğmus",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5505/fujece.2023.85047"
    },
    {
        "id": 32754,
        "title": "Deep deterministic policy gradient based multi-UAV control for moving convoy tracking",
        "authors": "Armaan Garg, Shashi Shekhar Jha",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107099"
    },
    {
        "id": 32755,
        "title": "WSEE Optimization of Cell-Free mMIMO Uplink Using Deep Deterministic Policy Gradient",
        "authors": "Abhinav Kumar, Venkatesh Tentu, Dheeraj Naidu Amudala, Rohit Budhiraja",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcomm.2022.3209867"
    },
    {
        "id": 32756,
        "title": "Deep-Deterministic Policy Gradient Based Multi-Resource Allocation in Edge-Cloud System: A Distributed Approach",
        "authors": "Arslan Qadeer, Myung Jong Lee",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3249153"
    },
    {
        "id": 32757,
        "title": "Hyperspectral image classification based on deep deterministic policy gradient",
        "authors": "Jian Zhou, Qianqian Cheng, Yu Su, Yuhe Qiu, Hu He, Jiawei Huang, Shuijie Wang",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2668126"
    },
    {
        "id": 32758,
        "title": "Exploration- and Exploitation-Driven Deep Deterministic Policy Gradient for Active SLAM in Unknown Indoor Environments",
        "authors": "Shengmin Zhao, Seung-Hoon Hwang",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "This study proposes a solution for Active Simultaneous Localization and Mapping (Active SLAM) of robots in unknown indoor environments using a combination of Deep Deterministic Policy Gradient (DDPG) path planning and the Cartographer algorithm. To enhance the convergence speed of the DDPG network and minimize collisions with obstacles, we devised a unique reward function that integrates exploration and exploitation strategies. The exploration strategy allows the robot to achieve the shortest running time and movement trajectory, enabling efficient traversal of unmapped environments. Moreover, the exploitation strategy introduces active closed loops to enhance map accuracy. We conducted experiments using the simulation platform Gazebo to validate our proposed model. The experimental results demonstrate that our model surpasses other Active SLAM methods in exploring and mapping unknown environments, achieving significant grid completeness of 98.7%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13050999"
    },
    {
        "id": 32759,
        "title": "Wide-angle monostatic radar cross section enhanced metasurface based on deep deterministic policy gradient",
        "authors": "Yilin Jiang, Chengyue Yan, Yuxuan Pan, Jinxin Li, Yuxi Tian",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/elex.20.20230315"
    },
    {
        "id": 32760,
        "title": "Twin-Delayed Deep Deterministic Policy Gradient for altitude control of a flying-wing aircraft with an uncertain aerodynamic model",
        "authors": "Willem Völker, Yifei Li, Erik-Jan Van Kampen",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2678"
    },
    {
        "id": 32761,
        "title": "Cooperative motion planning and control of a group of autonomous underwater vehicles using twin-delayed deep deterministic policy gradient",
        "authors": "Behnaz Hadi, Alireza Khosravi, Pouria Sarhadi",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apor.2024.103977"
    },
    {
        "id": 32762,
        "title": "D2PG: Deep Deterministic Policy Gradient-Based Vehicular Edge Caching Scheme for Digital Twin-Based Vehicular Networks",
        "authors": "Chauhan Harshvardhan Singh, Babbar Himanshi, Rani Shalli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23940/ijpe.23.05.p7.350358"
    },
    {
        "id": 32763,
        "title": "Current-constraint speed regulation for PMSM based on port-controlled Hamiltonian realization and deep deterministic policy gradient",
        "authors": "Min Wang, Yanhong Liu, Qi Wang, Patrick Wheeler",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1587/elex.20.20230516"
    },
    {
        "id": 32764,
        "title": "Perception Enhanced Deep Deterministic Policy Gradient for Autonomous Driving in Complex Scenarios",
        "authors": "Lyuchao Liao, Hankun Xiao, Pengqi Xing, Zhenhua Gan, Youpeng He, Jiajun Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmes.2024.047452"
    },
    {
        "id": 32765,
        "title": "DEEP DETERMINISTIC POLICY GRADIENT AND GRAPH CONVOLUTIONAL NETWORKS FOR TOPOLOGY OPTIMIZATION OF BRACED STEEL FRAMES",
        "authors": "Chi-tathon KUPWIWAT, Yuichi IWAGOE, Kazuki HAYASHI, Makoto OHSAKI",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3130/aijjse.69b.0_129"
    },
    {
        "id": 32766,
        "title": "Multi-Agent Collaborative Target Search Based on the Multi-Agent Deep Deterministic Policy Gradient with Emotional Intrinsic Motivation",
        "authors": "Xiaoping Zhang, Yuanpeng Zheng, Li Wang, Arsen Abdulali, Fumiya Iida",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "Multi-agent collaborative target search is one of the main challenges in the multi-agent field, and deep reinforcement learning (DRL) is a good way to learn such a task. However, DRL always faces the problem of sparse reward, which to some extent reduces its efficiency in task learning. Introducing intrinsic motivation has proved to be a useful way to make the sparse reward in DRL. So, based on the multi-agent deep deterministic policy gradient (MADDPG) structure, a new MADDPG algorithm with the emotional intrinsic motivation name MADDPG-E is proposed in this paper for the multi-agent collaborative target search. In MADDPG-E, a new emotional intrinsic motivation module with three emotions, joy, sadness, and fear, is designed. The three emotions are defined by corresponding psychological knowledge to the multi-agent embodied situations in an environment. An emotional steady-state variable function H is then designed to help judge the goodness of the emotions. Based on H, an emotion-based intrinsic reward function is finally proposed. With the designed emotional intrinsic motivation module, the multi-agent system always tries to make itself joy, which means it always learns to search the target. To show the effectiveness of the proposed MADDPG-E algorithm, two kinds of simulation experiments with a determined initial position and random initial position, respectively, are carried out, and comparisons are performed with MADDPG as well as MADDPG-ICM (MADDPG with an intrinsic curiosity module). The results show that with the designed emotional intrinsic motivation module, MADDPG-E has a higher learning speed and better learning stability, and the advantage is more obvious when facing complex situations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132111951"
    },
    {
        "id": 32767,
        "title": "DDRCN: Deep Deterministic Policy Gradient Recommendation Framework Fused with Deep Cross Networks",
        "authors": "Tianhan Gao, Shen Gao, Jun Xu, Qihui Zhao",
        "published": "2023-2-16",
        "citations": 1,
        "abstract": "As an essential branch of artificial intelligence, recommendation systems have gradually penetrated people’s daily lives. It is the active recommendation of goods or services of potential interest to users based on their preferences. Many recommendation methods have been proposed in both industry and academia. However, there are some limitations of previous recommendation methods: (1) Most of them do not consider the cross-correlation between data. (2) Many treat the recommendation process as a one-time act and do not consider the continuity of the recommendation system. To overcome these limitations, we propose a recommendation framework based on deep reinforcement learning techniques, known as DDRCN: a deep deterministic policy gradient recommendation framework incorporating deep cross networks. We use a Deep network and a Cross network to fit the cross relationships between the data, to obtain a representation of the user interaction data. The Actor-Critic network is designed to simulate the continuous interaction behavior of users through a greedy strategy. A deep deterministic policy gradient network is also used to train the recommendation model. Finally, we conduct experiments with two publicly available datasets and find that our proposed recommendation framework outperforms the baseline approach in the recall and ranking phases of recommendations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13042555"
    },
    {
        "id": 32768,
        "title": "Twin delayed deep deterministic policy gradient based energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles considering predicted terrain information",
        "authors": "Fazhan Tao, Zhigao Fu, Huixian Gong, Baofeng Ji, Yao Zhou",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.129173"
    },
    {
        "id": 32769,
        "title": "Optimal Scheduling of Wind-Photovoltaic-Energy Storage System Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Qingquan Ye, Xuguang Wu, Liyuan Chen, Xingda Wen, Qidai Lin, Yizhi Shi, Hongru Liu",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135686"
    },
    {
        "id": 32770,
        "title": "Combining multi-agent deep deterministic policy gradient and rerouting technique to improve traffic network performance under mixed traffic conditions",
        "authors": "Hung Tuan Trinh, Sang-Hoon Bae, Duy Quang Tran",
        "published": "2024-3-22",
        "citations": 0,
        "abstract": " In the future, mixed traffic flow will include two types of vehicles: connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs). CAVs emerge as new solutions to disrupt the traditional transportation system. This new solution shares real-time data with each other and the roadside units (RSU) for network management. Reinforcement learning (RL) is a promising approach for traffic signal management in complex urban areas by leveraging information gathered from CAVs. In particular, coordinating signal management at many intersections is a critical challenge in multi-agent reinforcement learning (MARL). According to this vision, we propose an approach that combines an actor–critic network–based multi-agent deep deterministic policy gradient (MADDPG) model and a rerouting technique (RT) to increase traffic performance in vehicular networks. This algorithm overcomes the inherent non-stationary of Q-learning and the high variance of policy gradient (PG) algorithms. Based on centralized learning with decentralized execution, the MADDPG model employs one actor and one critic for each agent. The actor network uses local information to execute actions, while the critic network is trained with extra information, including the states and actions of other agents. Through a centralized learning process, agents can coordinate with each other, diminishing the influence of an unstable environment. Unlike previous studies, we not only manage traffic light systems but also consider the effect of platooning vehicles on increasing throughput. Experimental results show that our model outperforms other models in terms of traffic performance in different scenarios. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/00375497241237831"
    },
    {
        "id": 32771,
        "title": "Twin Delayed Deep Deterministic Policy Gradient (TD3) Based Virtual Inertia Control for Inverter-Interfacing DGs in Microgrids",
        "authors": "Osarodion Emmanuel Egbomwan, Shichao Liu, Hicham Chaoui",
        "published": "2023-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jsyst.2022.3222262"
    },
    {
        "id": 32772,
        "title": "Joint Optimization of Trajectory and Task Offloading Based on Deep Deterministic Policy Gradient in UAV-Assisted MEC",
        "authors": "Qisheng Tan, Jielin Fu",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icct59356.2023.10419420"
    },
    {
        "id": 32773,
        "title": "Integration of design and control for renewable energy systems with an application to anaerobic digestion: A deep deterministic policy gradient framework",
        "authors": "Tannia A. Mendiola-Rodriguez, Luis A. Ricardez-Sandoval",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.127212"
    },
    {
        "id": 32774,
        "title": "Optimizing <scp>UAV</scp> computation offloading via <scp>MEC</scp> with deep deterministic policy gradient",
        "authors": "Ahmed Bashir Abbasi, Muhammad Usman Hadi",
        "published": "2024-1",
        "citations": 0,
        "abstract": "AbstractMobile edge computing (MEC) seems to be highly efficient to process the generated data from IoT devices by providing computational resources locating in close range to network edge. MEC can be promising in reduction of latency and consumption of energy from data transmissions from offloading computational tasks from IoT devices to nearby edge servers. In the context of the growing IoT ecosystem, there is an increasing need for efficient data processing and communication strategies. There is a demand of bridging the gap in current research with novel optimization algorithms tailored for UAV‐assisted MEC systems, shedding light on the necessity of efficient computation offloading in meeting the demands of the IoT era. In this article, a computation offloading optimization algorithm is proposed which is based on deep deterministic policy gradient for realistic Aurelia X6 Pro unmanned aerial vehicle (UAV)‐assisted MEC systems. The proposed algorithm optimizes the offloading decision for UAVs by taking task characteristics and the communication environment into consideration. To demonstrate the effectiveness of the proposed algorithm, comprehensive simulations were conducted, and the results indicate substantial improvements in MEC systems' competency. Our research not only showcases the feasibility of deep deterministic policy gradient in UAV‐assisted MEC systems but also highlights the importance of developing efficient computation offloading strategies for the evolving landscape of IoT and edge computing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/ett.4874"
    },
    {
        "id": 32775,
        "title": "Reward adaptive wind power tracking control based on deep deterministic policy gradient",
        "authors": "Peng Chen, Dezhi Han",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2023.121519"
    },
    {
        "id": 32776,
        "title": "Efficient Massive-Device Orchestration Through Reinforcement Learning With Boosted Deep Deterministic Policy Gradient",
        "authors": "Haowei Shi, Jiadao Zou, Qingxue Zhang",
        "published": "2024-2-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2023.3301795"
    },
    {
        "id": 32777,
        "title": "Novel task decomposed multi-agent twin delayed deep deterministic policy gradient algorithm for multi-UAV autonomous path planning",
        "authors": "Yatong Zhou, Xiaoran Kong, Kuo-Ping Lin, Liangyu Liu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2024.111462"
    },
    {
        "id": 32778,
        "title": "Deep Deterministic Policy Gradient and Active Disturbance Rejection Controller based coordinated control for gearshift manipulator of driving robot",
        "authors": "Gang Chen, Zhifeng Chen, Liangmo Wang, Weigong Zhang",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105586"
    },
    {
        "id": 32779,
        "title": "Hardware-in-the-loop Testing of a Deep Deterministic Policy Gradient Algorithm as a Microgrid Secondary Controller",
        "authors": "Pedro I. N. Barbalho, Denis V. Coury, Vinicius A. Lacerda, Ricardo A. S. Fernandes",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isgteurope56780.2023.10407700"
    },
    {
        "id": 32780,
        "title": "A perspective on the use of deep deterministic policy gradient reinforcement learning for retention time modeling in reversed-phase liquid chromatography",
        "authors": "Alexander Kensert, Gert Desmet, Deirdre Cabooter",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chroma.2023.464570"
    },
    {
        "id": 32781,
        "title": "Adaptive Cruise Control Using Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Himanshu Kumar Bishen, K.V. Shihabudheen, P.P. Muhammed Shanir",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icepe57949.2023.10201488"
    },
    {
        "id": 32782,
        "title": "Enhancing Longitudinal Velocity Control With Attention Mechanism-Based Deep Deterministic Policy Gradient (DDPG) for Safety and Comfort",
        "authors": "Fahmida Islam, John E. Ball, Christopher T. Goodin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3368435"
    },
    {
        "id": 32783,
        "title": "Visual interpretation of deep deterministic policy gradient models for energy consumption prediction",
        "authors": "Huixue Wang, Yunzhe Wang, You Lu, Qiming Fu, Jianping Chen",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jobe.2023.107847"
    },
    {
        "id": 32784,
        "title": "Hierarchical path planner combining probabilistic roadmap and deep deterministic policy gradient for unmanned ground vehicles with non-holonomic constraints",
        "authors": "Jie Fan, Xudong Zhang, Kun Zheng, Yuan Zou, Nana Zhou",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2024.106821"
    },
    {
        "id": 32785,
        "title": "Bidirectional Obstacle Avoidance Enhancement‐Deep Deterministic Policy Gradient: A Novel Algorithm for Mobile‐Robot Path Planning in Unknown Dynamic Environments",
        "authors": "Junxiao Xue, Shiwen Zhang, Yafei Lu, Xiaoran Yan, Yuanxun Zheng",
        "published": "2024-2-6",
        "citations": 0,
        "abstract": "Real‐time path planning in unknown dynamic environments is a significant challenge for mobile robots. Many researchers have attempted to solve this problem by introducing deep reinforcement learning, which trains agents through interaction with their environments. A method called BOAE‐DDPG, which combines the novel bidirectional obstacle avoidance enhancement (BOAE) mechanism with the deep deterministic policy gradient (DDPG) algorithm, is proposed to enhance the learning ability of obstacle avoidance. Inspired by the analysis of the reaction advantage in dynamic psychology, the BOAE mechanism focuses on obstacle‐avoidance reactions from the state and action. The cross‐attention mechanism is incorporated to enhance the attention to valuable obstacle‐avoidance information. Meanwhile, the obstacle‐avoidance behavioral advantage is separately estimated using the modified dueling network. Based on the learning goals of the mobile robot, new assistive reward factors are incorporated into the reward function to promote learning and convergence. The proposed method is validated through several experiments conducted using the simulation platform Gazebo. The results show that the proposed method is suitable for path planning tasks in unknown environments and has an excellent obstacle‐avoidance learning capability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/aisy.202300444"
    },
    {
        "id": 32786,
        "title": "Price-taker Bidding and Pricing Strategy Using Deep Deterministic Policy Gradient Algorithm with Transformer Neural Networks",
        "authors": "Jiao Shu, Ningkai Tang, Wenteng Kuang, Tianyu Chen, Jixiang Lu, Wei Wang",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acpee56931.2023.10135772"
    },
    {
        "id": 32787,
        "title": "Modeling Interactions of Autonomous/Manual Vehicles and Pedestrians with a Multi-Agent Deep Deterministic Policy Gradient",
        "authors": "Weichao Hu, Hongzhang Mu, Yanyan Chen, Yixin Liu, Xiaosong Li",
        "published": "2023-4-3",
        "citations": 0,
        "abstract": "This article focuses on the development of a stable pedestrian crash avoidance mitigation system for autonomous vehicles (AVs). Previous works have only used simple AV–pedestrian models, which do not reflect the actual interaction and risk status of intelligent intersections with manual vehicles. The paper presents a model that simulates the interaction between automatic driving vehicles and pedestrians on unsignalized crosswalks using the multi-agent deep deterministic policy gradient (MADDPG) algorithm. The MADDPG algorithm optimizes the PCAM strategy through the continuous interaction of multiple independent agents and effectively captures the inherent uncertainty in continuous learning and human behavior. Experimental results show that the MADDPG model can fully mitigate collisions in different scenarios and outperforms the DDPG and DRL algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su15076156"
    },
    {
        "id": 32788,
        "title": "A novel data-driven energy management strategy for fuel cell hybrid electric bus based on improved twin delayed deep deterministic policy gradient algorithm",
        "authors": "Ruchen Huang, Hongwen He",
        "published": "2024-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ijhydene.2023.04.335"
    },
    {
        "id": 32789,
        "title": "Adaptive Optimization Design of Building Energy System for Smart Elderly Care Community Based on Deep Deterministic Policy Gradient",
        "authors": "Chunmei Liu, Zhe Xue",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "In smart elderly care communities, optimizing the design of building energy systems is crucial for improving the quality of life and health of the elderly. This study pioneers an innovative adaptive optimization design methodology for building energy systems by harnessing the cutting-edge capabilities of deep reinforcement learning. This avant-garde method initially involves modeling a myriad of energy equipment embedded within the energy ecosystem of smart elderly care community buildings, thereby extracting their energy computation formulae. In a groundbreaking progression, this study ingeniously employs the actor–critic (AC) algorithm to refine the deep deterministic policy gradient (DDPG) algorithm. The enhanced DDPG algorithm is then adeptly wielded to perform adaptive optimization of the operational states within the energy system of a smart retirement community building, signifying a trailblazing approach in this realm. Simulation experiments indicate that the proposed method has better stability and convergence compared to traditional deep Q-learning algorithms. When the environmental interaction coefficient and learning ratio is 4, the improved DDPG algorithm under the AC framework can converge after 60 iterations. The stable reward value in the convergence state is −996. When the scheduling cycle of the energy system is between 0:00 and 8:00, the photovoltaic output of the system optimized by the DDPG algorithm is 0. The wind power output fluctuates within 50 kW. This study realizes efficient operation, energy saving, and emission reduction in building energy systems in smart elderly care communities and provides new ideas and methods for research in this field. It also provides an important reference for the design and operation of building energy systems in smart elderly care communities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/pr11072155"
    },
    {
        "id": 32790,
        "title": "A Method for Task Offloading based on Multi-agent Deep Deterministic Policy Gradient(MADDPG) and Minimum Cost Maximum Flow(MCMF) in Internet of Vehicles(IoVs)",
        "authors": "Zhiping Ouyang, Liang Li, Bo Wang",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640771.3641824"
    },
    {
        "id": 32791,
        "title": "A Novel Approach to Light Detection and Ranging Sensor Placement for\n                    Autonomous Driving Vehicles Using Deep Deterministic Policy Gradient\n                    Algorithm",
        "authors": "Felix Berens, Jordan Ambs, Stefan Elser, Markus Reischl",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "<div>This article presents a novel approach to optimize the placement of light\n                    detection and ranging (LiDAR) sensors in autonomous driving vehicles using\n                    machine learning. As autonomous driving technology advances, LiDAR sensors play\n                    a crucial role in providing accurate collision data for environmental\n                    perception. The proposed method employs the deep deterministic policy gradient\n                    (DDPG) algorithm, which takes the vehicle’s surface geometry as input and\n                    generates optimized 3D sensor positions with predicted high visibility. Through\n                    extensive experiments on various vehicle shapes and a rectangular cuboid, the\n                    effectiveness and adaptability of the proposed method are demonstrated.\n                    Importantly, the trained network can efficiently evaluate new vehicle shapes\n                    without the need for re-optimization, representing a significant improvement\n                    over classical methods such as genetic algorithms. By leveraging machine\n                    learning techniques, this research streamlines the sensor placement optimization\n                    process, enhancing the perception capabilities of autonomous driving vehicles.\n                    The optimized sensor configurations obtained from the DDPG algorithm lead to\n                    safer and more reliable autonomous driving systems, contributing to the\n                    advancement and widespread adoption of autonomous driving technology.</div>",
        "keywords": "",
        "link": "http://dx.doi.org/10.4271/12-07-03-0019"
    },
    {
        "id": 32792,
        "title": "Optimal demand response based dynamic pricing strategy via Multi-Agent Federated Twin Delayed Deep Deterministic policy gradient algorithm",
        "authors": "Haining Ma, Huifeng Zhang, Ding Tian, Dong Yue, Gerhard P. Hancke",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108012"
    },
    {
        "id": 32793,
        "title": "Fault‐resilient control of parallel PV inverters using multi‐agent twin‐delayed deep deterministic policy gradient approach",
        "authors": "Azra Malik, Ahteshamul Haque, V. S. Bharath Kurukuru, Saad Mekhilef",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "SummaryGrid‐tied photovoltaic system (GTPS) are widely favored due to their inherent benefits. The interface of parallel power electronic converters in these systems is rapidly increasing. The inverter in GTPS has a very important role in power conversion, transfer, and control. However, their challenging working conditions contribute to susceptibility to power switching device failures. Traditional fault diagnosis and tolerant approaches exhibit limitations in control efficiency, model dependency, and slow recovery from fault. This study proposes a multi‐agent twin delayed deep deterministic policy gradient (MATD3PG) configuration for intelligent parallel inverter control, fault diagnosis, and fault‐tolerant operation in GTPS structure. By leveraging the multi‐agent reinforcement learning (RL) framework, an optimal control of the parallel inverter can be achieved, encompassing fault‐tolerant operation using MATLAB Simulink/PLECS. The major advantage the given technique offers is that it carries out optimum fault‐tolerant operation without causing the system derating. Experimental findings demonstrate that the proposed fault‐tolerant model based on RL traditional methods is able to ensure continuous supply to the connected loads even during fault events. Further, the transition time from fault occurrence to recovery is found to be 6 ms, which is quite less compared with the fault‐tolerant techniques presented in literature. Through real‐time fault diagnosis‐based results, the proposed approach ensures precise tracking of reference currents, quicker response times, uninterrupted supply, and smooth transition to the post‐fault operation mode.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/cta.3911"
    },
    {
        "id": 32794,
        "title": "Intelligent Array Antenna in Complex Environment Based on Deep Deterministic Policy Gradient Algorithm",
        "authors": "Tong Wang, Jinshan Deng, Kaiqi Cao, Hongwei Gao, Cheng Jin",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/apcap59480.2023.10469688"
    },
    {
        "id": 32795,
        "title": "Autonomous handover parameter optimisation for 5G cellular networks using deep deterministic policy gradient",
        "authors": "Chiew Foong Kwong, Chenhao Shi, Qianyu Liu, Sen Yang, David Chieng, Pushpendu Kar",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122871"
    },
    {
        "id": 32796,
        "title": "Multi-Objective Optimization of Vehicle-Following Control for\n                    Connected Electric Vehicles Based on Deep Deterministic Policy\n                    Gradient",
        "authors": "Yulin Zhang, Yue Wu, Wei He, Yang Gao, Hui Peng, Heng Li",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "<div>Eco-driving plays an increasingly important role in intelligent transportation\n                    systems, where the vehicle-following economy and safety are receiving increasing\n                    attention in recent years. In this context, this article proposes a novel deep\n                    deterministic policy gradient (DDPG)-based driving control strategy for\n                    connected electric vehicles (CEVs) under vehicle-following scenarios. Three\n                    original contributions make this article distinctive from existing studies.\n                    First, a multi-objective optimization problem including driving safety,\n                    passenger comfort, and the driving economy for the following vehicle is\n                    established, in which the battery capacity degradation cost is first considered\n                    in the vehicle-following problem. Second, a DDPG-based driving control strategy\n                    is proposed where a penalty is introduced into the multi-objective optimization\n                    reward function to accelerate the convergence process. Third, the coupling\n                    relationship of the three objectives is carefully studied. Different weighting\n                    factors are tested and analyzed to balance the three objectives. Detailed\n                    discussion and comparison under different driving cycles validate the\n                    superiority of the proposed method, e.g., a 16–31% reduction of battery capacity\n                    degradation cost with better safety and comfort, compared with existing\n                    vehicle-following strategies. This work makes a potential contribution to the\n                    artificial intelligence application of intelligent transportation systems.</div>",
        "keywords": "",
        "link": "http://dx.doi.org/10.4271/14-13-01-0005"
    },
    {
        "id": 32797,
        "title": "Lithium-Plating Suppressed and Deep Deterministic Policy Gradient-Based Energy Management Strategy",
        "authors": "Dongyang Zhang, Shen Li, Zhongwei Deng, Xiaolin Tang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tte.2023.3288034"
    },
    {
        "id": 32798,
        "title": "Active control of flexible rotors using deep reinforcement learning with application of multi-actor-critic deep deterministic policy gradient",
        "authors": "Maheed H. Ahmed, Abdullah AboHussien, Aly El-Shafei, Ahmed M. Darwish, Ahmed H. Abdel-Gawad",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106593"
    },
    {
        "id": 32799,
        "title": "Optimal antisynchronization control for unknown multiagent systems with deep deterministic policy gradient approach",
        "authors": "Cuijuan Zhang, Lianghao Ji, Shasha Yang, Huaqing Li",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2022.12.008"
    },
    {
        "id": 32800,
        "title": "Multi-fidelity optimization of a quiet propeller based on deep deterministic policy gradient and transfer learning",
        "authors": "Xin Geng, Peiqing Liu, Tianxiang Hu, Qiulin Qu, Jiahua Dai, Changhao Lyu, Yunsong Ge, Rinie A.D. Akkermans",
        "published": "2023-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ast.2023.108288"
    },
    {
        "id": 32801,
        "title": "Twin-Delayed Deep Deterministic Policy Gradient Algorithm to Control a Boost Converter in a DC Microgrid",
        "authors": "Rifqi Firmansyah Muktiadji, Makbul A. M. Ramli, Ahmad H. Milyani",
        "published": "2024-1-20",
        "citations": 0,
        "abstract": "A stable output voltage of a boost converter is vital for the appropriate functioning of connected devices and loads in a DC microgrid. Variations in load demands and source uncertainties can damage equipment and disrupt operations. In this study, a modified twin-delayed deep deterministic policy gradient (TD3) algorithm is proposed to regulate the output voltage of a boost converter in a DC microgrid. TD3 optimizes PI controller gains, which ensure system stability by employing a non-negative, fully connected layer. To achieve optimal gains, multi-deep reinforcement learning agents are trained. The agents utilize the error signal to obtain the desired output voltage. Furthermore, a new reward function used in the TD3 algorithm is introduced. The proposed controller is tested under load variations and input voltage uncertainties. Simulation and experimental results demonstrate that TD3 outperforms PSO, GA, and the conventional PI. TD3 exhibits less steady-state error, reduced overshoots, fast response times, fast recovery times, and a small voltage deviation. These findings confirm TD3’s superiority and its potential application in DC microgrid voltage control. It can be used by engineers and researchers to design DC microgrids.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13020433"
    },
    {
        "id": 32802,
        "title": "Power Allocation Based on Multi-Agent Deep Deterministic Policy Gradient for Underwater Acoustic Communication Networks",
        "authors": "Xuan Geng, Xinyu Hui",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "This paper proposes a reinforcement learning-based power allocation for underwater acoustic communication networks (UACNs). The objective function is formulated as maximizing channel capacity under constraints of maximum power and minimum channel capacity. To solve this problem, a multi-agent deep deterministic policy gradient (MADDPG) approach is introduced, where each transmitter node is considered as an agent. Given the definition of a Markov decision process (MDP) model for this problem, the agents learn to collaboratively maximize the channel capacity by deep deterministic policy gradient (DDPG) learning. Specifically, the power allocation of each agent is obtained by a centralized training and distributed execution (CTDE) method. Simulation results show the sum rate achieved by the proposed algorithm approximates that of the fractional programming (FP) algorithm and improves by at least 5% compared with the DQN (deep Q-learning network) -based power allocation algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13020295"
    },
    {
        "id": 32803,
        "title": "A Hybrid Optimization Algorithm for Efficient Virtual Machine Migration and Task Scheduling Using a Cloud-Based Adaptive Multi-Agent Deep Deterministic Policy Gradient Technique",
        "authors": "Et al. Gurpreet Singh Panesar",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "This To achieve optimal system performance in the quickly developing field of cloud computing, efficient resource management—which includes accurate job scheduling and optimized Virtual Machine (VM) migration—is essential. The Adaptive Multi-Agent System with Deep Deterministic Policy Gradient (AMS-DDPG) Algorithm is used in this study to propose a cutting-edge hybrid optimization algorithm for effective virtual machine migration and task scheduling. An sophisticated combination of the War Strategy Optimization (WSO) and Rat Swarm Optimizer (RSO) algorithms, the Iterative Concept of War and Rat Swarm (ICWRS) algorithm is the foundation of this technique. Notably, ICWRS optimizes the system with an amazing 93% accuracy, especially for load balancing, job scheduling, and virtual machine migration. The VM migration and task scheduling flexibility and efficiency are greatly improved by the AMS-DDPG technology, which uses a powerful combination of deterministic policy gradient and deep reinforcement learning. By assuring the best possible resource allocation, the Adaptive Multi-Agent System method enhances decision-making even more. Performance in cloud-based virtualized systems is significantly enhanced by our hybrid method, which combines deep learning and multi-agent coordination. Extensive tests that include a detailed comparison with conventional techniques verify the effectiveness of the suggested strategy. As a consequence, our hybrid optimization approach is successful. The findings show significant improvements in system efficiency, shorter job completion times, and optimum resource utilization. Cloud-based systems have unrealized potential for synergistic optimization, as shown by the integration of ICWRS inside the AMS-DDPG framework. Enabling a high-performing and sustainable cloud computing infrastructure that can adapt to the changing needs of modern computing paradigms is made possible by this strategic resource allocation, which is attained via careful computational utilization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10.8570"
    },
    {
        "id": 32804,
        "title": "Twin delayed deep deterministic policy gradient for free-electron laser online optimization",
        "authors": "M Cai, Z H Zhu, K Q Zhang, C Feng, L J Tu, D Gu, Z T Zhao",
        "published": "2023-1-1",
        "citations": 1,
        "abstract": "Abstract\nX-ray free-electron lasers (FEL) have contributed to many frontier applications of nanoscale science which benefit from its extraordinary properties. During FEL commissioning, the beam status optimization especially orbits correction is particularly significant for FEL amplification. For example, the deviation between beam orbit and the magnetic center of undulator can affect the interaction between the electron beam and the FEL pulse. Usually, FEL commissioning requires a lot of effort for multi-dimensional parameters optimization in a time-varying system. Therefore, advanced algorithms are needed to facilitate the commissioning procedure. In this paper, we propose an online method to optimize the FEL power and transverse coherence by using a twin delayed deep deterministic policy gradient (TD3) algorithm. The algorithm exhibits more stable learning convergence and improves learning performance because the overestimation bias of policy gradient methods is suppressed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2420/1/012027"
    },
    {
        "id": 32805,
        "title": "Three-Dimensional Path Planning of UAVs in a Complex Dynamic Environment Based on Environment Exploration Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Danyang Zhang, Xiongwei Li, Guoquan Ren, Jiangyi Yao, Kaiyan Chen, Xi Li",
        "published": "2023-7-5",
        "citations": 2,
        "abstract": "Unmanned Aerial Vehicle (UAV) path planning research refers to the UAV automatically planning an optimal path to the destination under the corresponding environment, while avoiding collision with obstacles in this process. In order to solve the problem of 3D path planning of UAV in a dynamic environment, a heuristic dynamic reward function is designed to guide the UAV. We propose the Environment Exploration Twin Delayed Deep Deterministic Policy Gradient (EE-TD3) algorithm, which combines the symmetrical 3D environment exploration coding mechanism on the basis of TD3 algorithm. The EE-TD3 algorithm model can effectively avoid collisions, improve the training efficiency, and achieve faster convergence speed. Finally, the performance of the EE-TD3 algorithm and other deep reinforcement learning algorithms was tested in the simulation environment. The results show that the EE-TD3 algorithm is better than other algorithms in solving the 3D path planning problem of UAV.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15071371"
    },
    {
        "id": 32806,
        "title": "Decentralized multi-agent control of a three-tank hybrid system based on twin delayed deep deterministic policy gradient reinforcement learning algorithm",
        "authors": "N. Rajasekhar, T. K. Radhakrishnan, N. Samsudeen",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40435-023-01227-0"
    },
    {
        "id": 32807,
        "title": "Adversarial Sample Generation using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) and Classification for IEEE 802.11 using the Deep Deterministic Policy Gradient (DDPG)",
        "authors": "D. Sudaroli Vijayakumar, Sannasi Ganapathy",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "One of today's most promising developments is wireless networking, as it enables people across the globe to stay connected. As the wireless networks' transmission medium is open, there are potential issues in safeguarding the privacy of the information. Though several security protocols exist in the literature for the preservation of information, most cases fail with a simple spoof attack. So, intrusion detection systems are vital in wireless networks as they help in the identification of harmful traffic. One of the challenges that exist in wireless intrusion detection systems (WIDS) is finding a balance between accuracy and false alarm rate. The purpose of this study is to provide a practical classification scheme for newer forms of attack. The AWID dataset is used in the experiment, which proposes a feature selection strategy using a combination of Elastic Net and recursive feature elimination. The best feature subset is obtained with 22 features, and a deep deterministic policy gradient learning algorithm is then used to classify attacks based on those features. Samples are generated using the Euclidean Jacobian-based Saliency Map Attack (EJSMA) to evaluate classification outcomes using adversarial samples. The meta-analysis reveals improved results in terms of feature production (22 features), classification accuracy (98.75% for testing samples and 85.24% for adversarial samples), and false alarm rates (0.35%). ",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i8.7946"
    },
    {
        "id": 32808,
        "title": "Deep Deterministic Policy Gradient Reinforcement Learning Based Adaptive PID Load Frequency Control of an AC Micro-Grid",
        "authors": "Kamran Sabahi, Mohsin Jamil, Yaser Shokri-Kalandaragh, Mehdi Tavan, Yogendra Arya",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icjece.2024.3353670"
    },
    {
        "id": 32809,
        "title": "Combining Q-learning and Deterministic Policy Gradient for Learning-Based MPC",
        "authors": "Katrine Seel, Sébastien Gros, Jan Tommy Gravdahl",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383562"
    },
    {
        "id": 32810,
        "title": "A Painless Deterministic Policy Gradient Method for Learning-based MPC",
        "authors": "Akhil S Anand, Dirk Reinhardt, Shambhuraj Sawant, Jan Tommy Gravdahl, Sebastien Gros",
        "published": "2023-6-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178119"
    },
    {
        "id": 32811,
        "title": "Robotic-Arm-Based Force Control by Deep Deterministic Policy Gradient in Neurosurgical Practice",
        "authors": "Ibai Inziarte-Hidalgo, Erik Gorospe, Ekaitz Zulueta, Jose Manuel Lopez-Guede, Unai Fernandez-Gamiz, Saioa Etxebarria",
        "published": "2023-9-30",
        "citations": 0,
        "abstract": "This research continues the previous work “Robotic-Arm-Based Force Control in Neurosurgical Practice”. In that study, authors acquired an optimal control arm speed shape for neurological surgery which minimized a cost function that uses an adaptive scheme to determine the brain tissue force. At the end, the authors proposed the use of reinforcement learning, more specifically Deep Deterministic Policy Gradient (DDPG), to create an agent that could obtain the optimal solution through self-training. In this article, that proposal is carried out by creating an environment, agent (actor and critic), and reward function, that obtain a solution for our problem. However, we have drawn conclusions for potential future enhancements. Additionally, we analyzed the results and identified mistakes that can be improved upon in the future, such as exploring the use of varying desired distances of retraction to enhance training.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11194133"
    },
    {
        "id": 32812,
        "title": "Robotic Arm Trajectory Planning Method Using Deep Deterministic Policy Gradient With Hierarchical Memory Structure",
        "authors": "Di Zhao, Zhenyu Ding, Wenjie Li, Sen Zhao, Yuhong Du",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3340684"
    },
    {
        "id": 32813,
        "title": "Deep deterministic policy gradient and graph attention network for geometry optimization of latticed shells",
        "authors": "Chi-tathon Kupwiwat, Kazuki Hayashi, Makoto Ohsaki",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-04565-w"
    },
    {
        "id": 32814,
        "title": "The effects investigation of data-driven fitting cycle and deep deterministic policy gradient algorithm on energy management strategy of dual-motor electric bus",
        "authors": "Kaixuan Zhang, Jiageng Ruan, Tongyang Li, Hanghang Cui, Changcheng Wu",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.126760"
    },
    {
        "id": 32815,
        "title": "Decision Model of Ship Intelligent Collision Avoidance Based on Automatic Information System Data and Generic Adversary Imitation Learning-Deep Deterministic Policy Gradient",
        "authors": "Jiao Liu, Guoyou Shi, Kaige Zhu, Jiahui Shi, Yuchuang Wang",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583788.3583790"
    },
    {
        "id": 32816,
        "title": "Modified deep deterministic policy gradient based on active disturbance rejection control for hypersonic vehicles",
        "authors": "Li Xu, Ji Yuehui, Song Yu, Liu Junjie, Gao Qiang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09302-5"
    },
    {
        "id": 32817,
        "title": "A model predictive control trajectory tracking lateral controller for autonomous vehicles combined with deep deterministic policy gradient",
        "authors": "Zhaokang Xie, Xiaoci Huang, Suyun Luo, Ruoping Zhang, Fang Ma",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": " To solve the problem of trajectory tracking lateral control in autonomous driving technology, a model predictive control (MPC) controller trajectory tracking lateral control method combined with a deep deterministic policy gradient algorithm (DDPG) is proposed in this paper. This method inputs the real-time state of the vehicle into DDPG to achieve real-time automatic optimization of the prediction time domain and control time domain parameters of the MPC controller, and then affects the specific performance of the MPC controller in trajectory tracking lateral control. Specifically, the state space, action space, and reward function of DDPG are defined, and the automatic driving trajectory tracking lateral controller is designed in combination with the vehicle dynamics model. To reduce the exploration space of DDPG and improve the training efficiency of the entire model, the technique of advantage-disadvantage experience separation and extraction is introduced. Finally, the proposed method was trained and verified in various scenarios, and compared with two other lateral control methods for autonomous driving. The results showed that the learning and training time of the trajectory tracking lateral control method based on DDPG-MPC was shorter than that of the DDPG-based method, and the evaluation indicators in the trajectory tracking control process were better than those of the DDPG-based method and original MPC-based method. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/01423312231197854"
    },
    {
        "id": 32818,
        "title": "Autonomous Driving of Mobile Robots in Dynamic Environments Based on Deep Deterministic Policy Gradient: Reward Shaping and Hindsight Experience Replay",
        "authors": "Minjae Park, Chaneun Park, Nam Kyu Kwon",
        "published": "2024-1-13",
        "citations": 0,
        "abstract": "In this paper, we propose a reinforcement learning-based end-to-end learning method for the autonomous driving of a mobile robot in a dynamic environment with obstacles. Applying two additional techniques for reinforcement learning simultaneously helps the mobile robot in finding an optimal policy to reach the destination without collisions. First, the multifunctional reward-shaping technique guides the agent toward the goal by utilizing information about the destination and obstacles. Next, employing the hindsight experience replay technique to address the experience imbalance caused by the sparse reward problem assists the agent in finding the optimal policy. We validated the proposed technique in both simulation and real-world environments. To assess the effectiveness of the proposed method, we compared experiments for five different cases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/biomimetics9010051"
    },
    {
        "id": 32819,
        "title": "Novel deep deterministic policy gradient technique for automated micro-grid energy management in rural and islanded areas",
        "authors": "Lilia Tightiz, L. Minh Dang, Joon Yoo",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aej.2023.09.066"
    },
    {
        "id": 32820,
        "title": "Virtual Synchronous Generator Control Using Twin Delayed Deep Deterministic Policy Gradient Method",
        "authors": "Oroghene Oboreh-Snapps, Buxin She, Shah Fahad, Haotian Chen, Jonathan Kimball, Fangxing Li, Hantao Cui, Rui Bo",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tec.2023.3309955"
    },
    {
        "id": 32821,
        "title": "Deep Deterministic Policy Gradient Virtual Coupling control for the coordination and manoeuvring of heterogeneous uncertain nonlinear High-Speed Trains",
        "authors": "Giacomo Basile, Dario Giuseppe Lui, Alberto Petrillo, Stefania Santini",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2024.108120"
    },
    {
        "id": 32822,
        "title": "Twin-delayed deep deterministic policy gradient algorithm for the energy management of microgrids",
        "authors": "David Domínguez-Barbero, Javier García-González, Miguel Á. Sanz-Bobi",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106693"
    },
    {
        "id": 32823,
        "title": "Real-Time Energy Scheduling Applying the Twin Delayed Deep Deterministic Policy Gradient and Data Clustering",
        "authors": "Ioannis Zenginis, John Vardakas, Nikolaos E. Koltsaklis, Christos Verikoukis",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jsyst.2023.3326978"
    },
    {
        "id": 32824,
        "title": "Path Planning Method for Manipulators Based on Improved Twin Delayed Deep Deterministic Policy Gradient and RRT*",
        "authors": "Ronggui Cai, Xiao Li",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "This paper proposes a path planning framework that combines the experience replay mechanism from deep reinforcement learning (DRL) and rapidly exploring random tree star (RRT*), employing the DRL-RRT* as the path planning method for the manipulator. The iteration of the RRT* is conducted independently in path planning, resulting in a tortuous path and making it challenging to find an optimal path. The setting of reward functions in policy learning based on DRL is very complex and has poor universality, making it difficult to complete the task in complex path planning. Aiming at the insufficient exploration of the current deterministic policy gradient DRL algorithm twin delayed deep deterministic policy gradient (TD3), a stochastic policy was combined with TD3, and the performance was verified on the simulation platform. Furthermore, the improved TD3 was integrated with RRT* for performance analysis in two-dimensional (2D) and three-dimensional (3D) path planning environments. Finally, a six-degree-of-freedom manipulator was used to conduct simulation and experimental research on the manipulator.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14072765"
    },
    {
        "id": 32825,
        "title": "Path Planning of Unmanned Aerial Vehicle in Complex Environments Based on State-Detection Twin Delayed Deep Deterministic Policy Gradient",
        "authors": "Danyang Zhang, Zhaolong Xuan, Yang Zhang, Jiangyi Yao, Xi Li, Xiongwei Li",
        "published": "2023-1-13",
        "citations": 2,
        "abstract": "This paper investigates the path planning problem of an unmanned aerial vehicle (UAV) for completing a raid mission through ultra-low altitude flight in complex environments. The UAV needs to avoid radar detection areas, low-altitude static obstacles, and low-altitude dynamic obstacles during the flight process. Due to the uncertainty of low-altitude dynamic obstacle movement, this can slow down the convergence of existing algorithm models and also reduce the mission success rate of UAVs. In order to solve this problem, this paper designs a state detection method to encode the environmental state of the UAV’s direction of travel and compress the environmental state space. In considering the continuity of the state space and action space, the SD-TD3 algorithm is proposed in combination with the double-delayed deep deterministic policy gradient algorithm (TD3), which can accelerate the training convergence speed and improve the obstacle avoidance capability of the algorithm model. Further, to address the sparse reward problem of traditional reinforcement learning, a heuristic dynamic reward function is designed to give real-time rewards and guide the UAV to complete the task. The simulation results show that the training results of the SD-TD3 algorithm converge faster than the TD3 algorithm, and the actual results of the converged model are better.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/machines11010108"
    },
    {
        "id": 32826,
        "title": "A theoretical demonstration for reinforcement learning of PI control dynamics for optimal speed control of DC motors by using Twin Delay Deep Deterministic Policy Gradient Algorithm",
        "authors": "Sevilay Tufenkci, Baris Baykant Alagoz, Gurkan Kavuran, Celaleddin Yeroglu, Norbert Herencsar, Shibendu Mahata",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.119192"
    },
    {
        "id": 32827,
        "title": "Multilayer Deep Deterministic Policy Gradient for Static Safety and Stability Analysis of Novel Power Systems",
        "authors": "Yun Long, Youfei Lu, Hongwei Zhao, Renbo Wu, Tao Bao, Jun Liu",
        "published": "2023-4-21",
        "citations": 2,
        "abstract": "More and more renewable energy sources are integrated into novel power systems. The randomness and fluctuation of such renewable energy sources bring challenges to the static stability and safety analysis of novel power systems. In this work, a multilayer deep deterministic policy gradient is proposed to address the fluctuation of renewable energy sources. The proposed method is stacked with multilayer deep reinforcement learning methods that can be continuously updated online. The proposed multilayer deep deterministic policy gradient is compared with other deep learning algorithms. The feasibility, effectiveness, and superiority of the proposed method are verified by numerical simulations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/4295384"
    },
    {
        "id": 32828,
        "title": "Fractional-Order Control Method Based on Twin-Delayed Deep Deterministic Policy Gradient Algorithm",
        "authors": "Guangxin Jiao, Zhengcai An, Shuyi Shao, Dong Sun",
        "published": "2024-2-6",
        "citations": 0,
        "abstract": "In this paper, a fractional-order control method based on the twin-delayed deep deterministic policy gradient (TD3) algorithm in reinforcement learning is proposed. A fractional-order disturbance observer is designed to estimate the disturbances, and the radial basis function network is selected to approximate system uncertainties in the system. Then, a fractional-order sliding-mode controller is constructed to control the system, and the parameters of the controller are tuned using the TD3 algorithm, which can optimize the control effect. The results show that the fractional-order control method based on the TD3 algorithm can not only improve the closed-loop system performance under different operating conditions but also enhance the signal tracking capability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fractalfract8020099"
    },
    {
        "id": 32829,
        "title": "Enhanced Deep Deterministic Policy Gradient Algorithm Using Grey Wolf Optimizer for Continuous Control Tasks",
        "authors": "Ebrahim Hamid Hasan Sumiea, Said Jadid Abdulkadir, Mohammed Gamal Ragab, Safwan Mahmood Al-Selwi, Suliamn Mohamed Fati, Alawi AlQushaibi, Hitham Alhussian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3341507"
    },
    {
        "id": 32830,
        "title": "Satellite Edge Computing With Collaborative Computation Offloading: An Intelligent Deep Deterministic Policy Gradient Approach",
        "authors": "Hangyu Zhang, Rongke Liu, Aryan Kaushik, Xiangqiang Gao",
        "published": "2023-5-15",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2022.3233383"
    },
    {
        "id": 32831,
        "title": "Multi‐dimensional resource management with deep deterministic policy gradient for digital twin‐enabled Industrial Internet of Things in 6 generation",
        "authors": "Yue Hu, Ning Cao, Hao Lu, Yunzhe Jiang, Yinqiu Liu, Xiaoming He",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractIn the era of sixth generation mobile networks (6G), industrial big data is rapidly generated due to the increasing data‐driven applications in the Industrial Internet of Things (IIoT). Effectively processing such data, for example, knowledge learning, on resource‐limited IIoT devices becomes a challenge. To this end, we introduce a cloud‐edge‐end collaboration architecture, in which computing, communication, and storage resources are flexibly coordinated to alleviate the issue of resource constraints. To achieve better performance in hyper‐connected experience, real‐time communication, and sustainable computing, we construct a novel architecture combining digital twin (DT)‐IIoT with edge networks. In addition, considering the energy consumption and delay issues in distributed learning, we propose a deep reinforcement learning‐based method called deep deterministic policy gradient with double actors and double critics (D4PG) to manage the multi‐dimensional resources, that is, CPU cycles, DT models, and communication bandwidths, enhancing the exploration ability and improving the inaccurate value estimation of agents in continuous action spaces. In addition, we introduce a synchronization threshold for distributed learning framework to avoid the synchronization latency caused by stragglers. Extensive experimental results prove that the proposed architecture can efficiently conduct knowledge learning, and the intelligent scheme can also improve system efficiency by managing multi‐dimensional resources.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/ett.4962"
    },
    {
        "id": 32832,
        "title": "Water management scheme based on prioritized deep deterministic policy gradient for proton exchange membrane fuel cells",
        "authors": "De Xiang, Yijun Cheng, Qingxian Li, Qiong Wang, Liangjiang Liu",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "AbstractTo effectively tackle the intricate and dynamic challenges encountered in proton exchange membrane fuel cells (PEMFCs), this paper introduces a model-free reinforcement learning approach to address its water management issue. Recognizing the limitations of conventional reinforcement learning methods such as Q-learning in handling the continuous actions and nonlinearity inherent in PEMFCs water management, we propose a prioritized deep deterministic policy gradient (DDPG) method. This method, rooted in the Actor-Critic framework, leverages double neural networks and prioritized experience replay to enable adaptive water management and balance. Additionally, we establish a PEMFCs water management platform and implement the prioritized DDPG method using \"Tianshou\", a modularized Python library for deep reinforcement learning. Through experimentation, the effectiveness of our proposed method is verified. This study contributes to advancing the understanding and management of water dynamics in PEMFCs, offering a promising avenue for enhancing their performance and reliability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42452-024-05789-2"
    },
    {
        "id": 32833,
        "title": "Local 2-D Path Planning of Unmanned Underwater Vehicles in Continuous Action Space Based on the Twin-Delayed Deep Deterministic Policy Gradient",
        "authors": "Zhenzhong Chu, Yu Wang, Daqi Zhu",
        "published": "2024-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2023.3348827"
    },
    {
        "id": 32834,
        "title": "Delayed Deep Deterministic Policy Gradient-Based Energy Management Strategy for Overall Energy Consumption Optimization of Dual Motor Electrified Powertrain",
        "authors": "Jiageng Ruan, Changcheng Wu, Hanghang Cui, Weihan Li, Dirk Uwe Sauer",
        "published": "2023-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2023.3265073"
    },
    {
        "id": 32835,
        "title": "Optimization of reward shaping function based on genetic algorithm applied to a cross validated deep deterministic policy gradient in a powered landing guidance problem",
        "authors": "Larasmoyo Nugroho, Rika Andiarti, Rini Akmeliawati, Ali Türker Kutay, Diva Kartika Larasati, Sastra Kusuma Wijaya",
        "published": "2023-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2022.105798"
    },
    {
        "id": 32836,
        "title": "Multi-Agent Recurrent Deterministic Policy Gradient with Inter-Agent Communication",
        "authors": "Joohyun Cho, Mingxi Liu, Yi Zhou, Rong-Rong Chen",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ieeeconf59524.2023.10477063"
    },
    {
        "id": 32837,
        "title": "Deep Deterministic Policy Gradient Algorithm Based Reinforcement Learning Controller for Single-Inductor Multiple-Output DC–DC Converter",
        "authors": "Jian Ye, Huanyu Guo, Benfei Wang, Xinan Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpel.2024.3350181"
    },
    {
        "id": 32838,
        "title": "Hierarchical Rewarding Deep Deterministic Policy Gradient Strategy for Energy Management of Hybrid Electric Vehicles",
        "authors": "Jinhai Wang, Changqing Du, Fuwu Yan, Quan Zhou, Hongming Xu",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tte.2023.3263927"
    },
    {
        "id": 32839,
        "title": "Optimization of Profile Control and Oil Displacement Scheme Parameters Based on Deep Deterministic Policy Gradient",
        "authors": "Chaodong Tan, Chunqiu Wang, Jinjie Tian, HuiZhao Niu, Qi Wei, Xiongying Zhang",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acsomega.3c02003"
    },
    {
        "id": 32840,
        "title": "Incentive-based demand response under incomplete information based on the deep deterministic policy gradient",
        "authors": "Siyu Ma, Hui Liu, Ni Wang, Lidong Huang, Hui Hwang Goh",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2023.121838"
    },
    {
        "id": 32841,
        "title": "The application of machine learning based energy management strategy in multi-mode plug-in hybrid electric vehicle, part I: Twin Delayed Deep Deterministic Policy Gradient algorithm design for hybrid mode",
        "authors": "Changcheng Wu, Jiageng Ruan, Hanghang Cui, Bin Zhang, Tongyang Li, Kaixuan Zhang",
        "published": "2023-1",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2022.125084"
    },
    {
        "id": 32842,
        "title": "Advanced deep deterministic policy gradient based energy management strategy design for dual-motor four-wheel-drive electric vehicle",
        "authors": "Hanghang Cui, Jiageng Ruan, Changcheng Wu, Kaixuan Zhang, Tongyang Li",
        "published": "2023-1",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mechmachtheory.2022.105119"
    },
    {
        "id": 32843,
        "title": "Cooperative control of velocity and heading for unmanned surface vessel based on twin delayed deep deterministic policy gradient with an integral compensator",
        "authors": "Yibai Wang, Shulong Zhao, Qingling Wang",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.115943"
    },
    {
        "id": 32844,
        "title": "Value activation for bias alleviation: Generalized-activated deep double deterministic policy gradients",
        "authors": "Jiafei Lyu, Yu Yang, Jiangpeng Yan, Xiu Li",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.10.085"
    },
    {
        "id": 32845,
        "title": "Intelligent Trajectory Tracking Linear Active Disturbance Rejection Control of a Powered Parafoil Based on Twin Delayed Deep Deterministic Policy Gradient Algorithm Optimization",
        "authors": "Yuemin Zheng, Zelin Fei, Jin Tao, Qinglin Sun, Hao Sun, Zengqiang Chen, Mingwei Sun",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "Powered parafoils, known for their impressive load-bearing capacity and extended endurance, have garnered significant interest. However, the parafoil system is a highly complex nonlinear system. It primarily relies on the steering gear to change flight direction and utilizes a thrust motor for climbing. However, achieving precise trajectory tracking control presents a challenge due to the interdependence of direction and altitude control. Furthermore, underactuation and wind disturbances bring additional difficulties for trajectory tracking control. Consequently, realizing trajectory tracking control for powered parafoils holds immense significance. In this paper, we propose a trajectory tracking method based on Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm-optimized Linear Active Disturbance Rejection Control (LADRC). Our method addresses the underactuation issue by incorporating a guiding law while utilizing two LADRC methods to achieve decoupling and compensate for disturbances. Moreover, we employ the TD3 algorithm to dynamically adjust controller parameters, thus enhancing the controller performance. The simulation results demonstrate the effectiveness of our proposed method as a trajectory tracking control approach. Additionally, since the control process is not reliant on system-specific models, our method can also provide guidance for trajectory tracking control in other aircraft.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132312555"
    },
    {
        "id": 32846,
        "title": "Adaptive Weighted Combination Approach for Wind Power Forecast Based on Deep Deterministic Policy Gradient Method",
        "authors": "Menglin Li, Ming Yang, Yixiao Yu, Mohammad Shahidehpour, Fushuan Wen",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpwrs.2023.3294839"
    },
    {
        "id": 32847,
        "title": "UAV's air combat decision-making based on deep deterministic policy gradient and prediction",
        "authors": "Yongfeng LI, Yongxi LYU, Jingping SHI, Weihua LI",
        "published": "2023-2",
        "citations": 0,
        "abstract": "To solve the enemy uncertain manipulation problem during a UAV's autonomous air combat maneuver decision-making, this paper proposes an autonomous air combat maneuver decision-making method that combines target maneuver command prediction with the deep deterministic policy algorithm. The situation data of both sides of air combat are effectively fused and processed, the UAV's six-degree-of-freedom model and maneuver library are built. In air combat, the target generates its corresponding maneuver library instructions through the deep Q network algorithm; at the same time, the UAV on our side gives the target maneuver prediction results through the probabilistic neural network. A deep deterministic policy gradient reinforcement learning method that considers both the situation information of two aircraft and the prediction results of enemy aircraft is proposed, so that the UAV can choose the appropriate maneuver decision according to the current air combat situation. The simulation results show that the method can effectively use the air combat situation information and target maneuver prediction information so that it can improve the effectiveness of the reinforcement learning method for UAV's autonomous air combat decision-making on the premise of ensuring convergence.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/jnwpu/20234110056"
    },
    {
        "id": 32848,
        "title": "Adaptive Active Disturbance Rejection Load Frequency Control for Power System with Renewable Energies Using the Lyapunov Reward-Based Twin Delayed Deep Deterministic Policy Gradient Algorithm",
        "authors": "Yuemin Zheng, Jin Tao, Qinglin Sun, Hao Sun, Zengqiang Chen, Mingwei Sun",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "The substitution of renewable energy sources (RESs) for conventional fossil fuels in electricity generation is essential in addressing environmental pollution and resource depletion. However, the integration of RESs in the load frequency control (LFC) of power systems can have a negative impact on frequency deviation response, resulting in a decline in power quality. Moreover, load disturbances can also affect the stability of frequency deviation. Hence, this paper presents an LFC method that utilizes the Lyapunov reward-based twin delayed deep deterministic policy gradient (LTD3) algorithm to optimize the linear active disturbance rejection control (LADRC). With the advantages of being model-free and mitigating unknown disturbances, LADRC can regulate load disturbances and renewable energy deviations. Additionally, the LTD3 algorithm, based on the Lyapunov reward function, is employed to optimize controller parameters in real-time, resulting in enhanced control performance. Finally, the LADRC-LTD3 is evaluated using a power system containing two areas, comprising thermal, hydro, and gas power plants in each area, as well as RESs such as a noise-based wind turbine and photovoltaic (PV) system. A comparative analysis is conducted between the performance of the proposed controller and other control techniques, such as integral controller (IC), fractional-order proportional integral derivative (FOPID) controller, I-TD, ID-T, and TD3-optimized LADRC. The results indicate that the proposed method effectively addresses the LFC problem.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/su151914452"
    },
    {
        "id": 32849,
        "title": "Attitude Control of Stabilized Platform Based on Deep Deterministic Policy Gradient with Disturbance Observer",
        "authors": "Aiqing Huo, Xue Jiang, Shuhan Zhang",
        "published": "2023-11-3",
        "citations": 1,
        "abstract": "A rotary steerable drilling system is an advanced drilling technology, with stabilized platform tool face attitude control being a critical component. Due to a multitude of downhole interference factors, coupled with nonlinearities and uncertainties, challenges arise in model establishment and attitude control. Furthermore, considering that stabilized platform tool face attitude determines the drilling direction of the entire drill bit, the effectiveness of tool face attitude control and nonlinear disturbances, such as friction interference, will directly impact the precision and success of drilling tool guidance. In this study, a mathematical model and a friction model of the stabilized platform are established, and a Disturbance-Observer-Based Deep Deterministic Policy Gradient (DDPG_DOB) control algorithm is proposed to address the friction nonlinearity problem existing in the rotary steering drilling stabilized platform. The numerical simulation results illustrate that the stabilized platform attitude control system based on DDPG_DOB can effectively suppress friction interference, improve non-linear hysteresis, and demonstrate strong anti-interference capability and good robustness.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132112022"
    },
    {
        "id": 32850,
        "title": "SFNAS-DDPG: A Biomass-Based Energy Hub Dynamic Scheduling Approach via Connecting Supervised Federated Neural Architecture Search and Deep Deterministic Policy Gradient",
        "authors": "Amirhossein Dolatabadi, Hussein Abdeltawab, Yasser Abdel-Rady I. Mohamed",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3352032"
    },
    {
        "id": 32851,
        "title": "Research on control strategy of dual active full bridge converter based on deep deterministic strategy gradient",
        "authors": "Fulu Yan, Mian Hua, Zhi Xun, Dongdong Cao",
        "published": "2023-12-1",
        "citations": 1,
        "abstract": "In order to cope with the need for fast and adaptive voltage regulation in the face of large-scale disturbance in future smart grid EV grid-connected converters, a control strategy based on deep reinforcement learning is proposed in this paper. For electric vehicle grid-connected dual active bridge (DAB) DC–DC converter, a deep deterministic strategy gradient (DDPG) reinforcement learning intelligent controller based on an actor-critic architecture is designed. Through real-time online learning and training by collecting a large amount of system data, the intelligent controller can automatically adjust the power parameters of the DAB converter to ensure that the DC converter has strict stability in the face of various system disturbances. Simulation results show that the control strategy is effective. This research provides an innovative and feasible solution, which has important practical application value to the voltage regulation of electric vehicle grid-connected converters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0180348"
    },
    {
        "id": 32852,
        "title": "Memory-Enhanced Twin Delayed Deep Deterministic Policy Gradient (ME-TD3)-Based Unmanned Combat Aerial Vehicle Trajectory Planning for Avoiding Radar Detection Threats in Dynamic and Unknown Environments",
        "authors": "Jiantao Li, Tianxian Zhang, Kai Liu",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "Unmanned combat aerial vehicle (UCAV) trajectory planning to avoid radar detection threats is a complicated optimization problem that has been widely studied. The rapid changes in Radar Cross Sections (RCSs), the unknown cruise trajectory of airborne radar, and the uncertain distribution of radars exacerbate the complexity of this problem. In this paper, we propose a novel UCAV trajectory planning method based on deep reinforcement learning (DRL) technology to overcome the adverse impacts caused by the dynamics and randomness of environments. A predictive control model is constructed to describe the dynamic characteristics of the UCAV trajectory planning problem in detail. To improve the UCAV’s predictive ability, we propose a memory-enhanced twin delayed deep deterministic policy gradient (ME-TD3) algorithm that uses an attention mechanism to effectively extract environmental patterns from historical information. The simulation results show that the proposed method can successfully train UCAVs to carry out trajectory planning tasks in dynamic and unknown environments. Furthermore, the ME-TD3 algorithm outperforms other classical DRL algorithms in UCAV trajectory planning, exhibiting superior performance and adaptability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15235494"
    },
    {
        "id": 32853,
        "title": "Twin Delayed Deterministic Policy Gradient Based Cooperative Platoon Longitudinal Control Strategy",
        "authors": "Fa Tao Zhou, Yong Fu Li, Long Wang Huang, Xin Huang",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3616901.3616947"
    },
    {
        "id": 32854,
        "title": "Policy ensemble gradient for continuous control problems in deep reinforcement learning",
        "authors": "Guoqiang Liu, Gang Chen, Victoria Huang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126381"
    },
    {
        "id": 32855,
        "title": "Determination of annual rainfall in north-east India using deterministic, geospatial, and machine learning techniques",
        "authors": "Shivam Agarwal, Disha Mukherjee, Nilotpal Debbarma",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nAnalysis of extreme annual rainfall in the six north-east Indian states of Assam, Meghalaya, Nagaland, Manipur, Mizoram, and Tripura using the deterministic interpolation technique of inverse distance weighting (IDW), the geospatial interpolation technique of Ordinary Kriging (OK) and the machine learning prediction technique of generalised additive model (GAM). GAM is used only for prediction and hence the results are then subsequently interpolated by OK to create the rainfall maps. The datasets considered for this study are a training dataset of 171 points which consisted of satellite rainfall and a testing dataset with ground rain gauge data of 33 points which was used for validation of the former. A combined dataset of training + testing was also interpolated and mapped to compare for visual accuracy of each technique. It was seen that OK was a superior and a much more realistic interpolation technique than IDW, since it took the altitude of each site into consideration along with latitude and longitude, unlike IDW, which only interpolated over the x–y plane and didn't rely on altitude. When the predictions of the training dataset through GAM were mapped using OK, it showed almost parallel contours, which is undesirable for natural phenomenon like rain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2166/wp.2023.078"
    },
    {
        "id": 32856,
        "title": "Sim-real joint experimental verification for an unmanned surface vehicle formation strategy based on multi-agent deterministic policy gradient and line of sight guidance",
        "authors": "Yan Li, Xiaowen Li, Xiangwei Wei, Hao Wang",
        "published": "2023-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.113661"
    },
    {
        "id": 32857,
        "title": "Optimal control of nonlinear system based on deterministic policy gradient with eligibility traces",
        "authors": "Jun Rao, Jingcheng Wang, Jiahui Xu, Shangwei Zhao",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11071-023-08909-6"
    },
    {
        "id": 32858,
        "title": "AUV Collision Avoidance Planning Method Based on Deep Deterministic Policy Gradient",
        "authors": "Jianya Yuan, Mengxue Han, Hongjian Wang, Bo Zhong, Wei Gao, Dan Yu",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "Collision avoidance planning has always been a hot and important issue in the field of unmanned aircraft research. In this article, we describe an online collision avoidance planning algorithm for autonomous underwater vehicle (AUV) autonomous navigation, which relies on its own active sonar sensor to detect obstacles. The improved particle swarm optimization (I-PSO) algorithm is used to complete the path planning of the AUV under the known environment, and we use it as a benchmark to improve the fitness function and inertia weight of the algorithm. Traditional path-planning algorithms rely on accurate environment maps, where re-adapting the generated path can be highly demanding in terms of computational cost. We propose a deep reinforcement learning (DRL) algorithm based on collision avoidance tasks. The algorithm discussed in this paper takes into account the relative position of the target point and the rate of heading change from the previous timestep. Its reward function considers the target point, running time and turning angle at the same time. Compared with the LSTM structure, the Gated Recurrent Unit (GRU) network has fewer parameters, which helps to save training time. A series of simulation results show that the proposed deep deterministic policy gradient (DDPG) algorithm can obtain excellent results in simple and complex environments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/jmse11122258"
    },
    {
        "id": 32859,
        "title": "A Novel Integral Reinforcement Learning-Based Control Method Assisted by Twin Delayed Deep Deterministic Policy Gradient for Solid Oxide Fuel Cell in DC Microgrid",
        "authors": "Yulin Liu, Tianhao Qie, Yang Yu, Yuxuan Wang, Tat Kei Chau, Xinan Zhang, Ujjal Manandhar, Sinan Li, Herbert H. C. Iu, Tyrone Fernando",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tste.2022.3224179"
    },
    {
        "id": 32860,
        "title": "A novel deep deterministic policy gradient model applied to intelligent transportation system security problems in 5G and 6G network scenarios",
        "authors": "David Augusto Ribeiro, Dick Carrillo Melgarejo, Muhammad Saadi, Renata Lopes Rosa, Demóstenes Zegarra Rodríguez",
        "published": "2023-2",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.phycom.2022.101938"
    },
    {
        "id": 32861,
        "title": "A twin delayed deep deterministic policy gradient-based energy management strategy for a battery-ultracapacitor electric vehicle considering driving condition recognition with learning vector quantization neural network",
        "authors": "Rui Liu, Chun Wang, Aihua Tang, Yongzhi Zhang, Quanqing Yu",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.est.2023.108147"
    },
    {
        "id": 32862,
        "title": "Computational Offloading in Vehicular Edge Computing using Multiple Agents based Deterministic Policy Gradient Algorithm and Generative Adversarial Networks (DPG-GAN)",
        "authors": "PriyaPonnusamy P, Shanthi N, Shabariram C. P",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijahuc.2023.10057819"
    },
    {
        "id": 32863,
        "title": "Computational offloading in vehicular edge computing using multiple agent-based deterministic policy gradient algorithm and generative adversarial networks",
        "authors": "C.P. Shabariram, N. Shanthi, P. Priya Ponnuswamy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijahuc.2023.135105"
    },
    {
        "id": 32864,
        "title": "Parameter-Free Reduction of the Estimation Bias in Deep Reinforcement Learning for Deterministic Policy Gradients",
        "authors": "Baturay Saglam, Furkan Burak Mutlu, Dogan Can Cicek, Suleyman Serdar Kozat",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "AbstractApproximation of the value functions in value-based deep reinforcement learning induces overestimation bias, resulting in suboptimal policies. We show that when the reinforcement signals received by the agents have a high variance, deep actor-critic approaches that overcome the overestimation bias lead to a substantial underestimation bias. We first address the detrimental issues in the existing approaches that aim to overcome such underestimation error. Then, through extensive statistical analysis, we introduce a novel, parameter-free Deep Q-learning variant to reduce this underestimation bias in deterministic policy gradients. By sampling the weights of a linear combination of two approximate critics from a highly shrunk estimation bias interval, our Q-value update rule is not affected by the variance of the rewards received by the agents throughout learning. We test the performance of the introduced improvement on a set of MuJoCo and Box2D continuous control tasks and demonstrate that it outperforms the existing approaches and improves the baseline actor-critic algorithm in most of the environments tested.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11461-y"
    },
    {
        "id": 32865,
        "title": "A dosing strategy model of deep deterministic policy gradient algorithm for sepsis patients",
        "authors": "Tianlai Lin, Xinjue Zhang, Jianbing Gong, Rundong Tan, Weiming Li, Lijun Wang, Yingxia Pan, Xiang Xu, Junhui Gao",
        "published": "2023-5-4",
        "citations": 0,
        "abstract": "Abstract\nBackground\nA growing body of research suggests that the use of computerized decision support systems can better guide disease treatment and reduce the use of social and medical resources. Artificial intelligence (AI) technology is increasingly being used in medical decision-making systems to obtain optimal dosing combinations and improve the survival rate of sepsis patients. To meet the real-world requirements of medical applications and make the training model more robust, we replaced the core algorithm applied in an AI-based medical decision support system developed by research teams at the Massachusetts Institute of Technology (MIT) and IMPERIAL College London (ICL) with the deep deterministic policy gradient (DDPG) algorithm. The main objective of this study was to develop an AI-based medical decision-making system that makes decisions closer to those of professional human clinicians and effectively reduces the mortality rate of sepsis patients.\n\nMethods\nWe used the same public intensive care unit (ICU) dataset applied by the research teams at MIT and ICL, i.e., the Multiparameter Intelligent Monitoring in Intensive Care III (MIMIC-III) dataset, which contains information on the hospitalizations of 38,600 adult sepsis patients over the age of 15. We applied the DDPG algorithm as a strategy-based reinforcement learning approach to construct an AI-based medical decision-making system and analyzed the model results within a two-dimensional space to obtain the optimal dosing combination decision for sepsis patients.\n\nResults\nThe results show that when the clinician administered the exact same dose as that recommended by the AI model, the mortality of the patients reached the lowest rate at 11.59%. At the same time, according to the database, the baseline mortality rate of the patients was calculated as 15.7%. This indicates that the patient mortality rate when difference between the doses administered by clinicians and those determined by the AI model was zero was approximately 4.2% lower than the baseline patient mortality rate found in the dataset. The results also illustrate that when a clinician administered a different dose than that recommended by the AI model, the patient mortality rate increased, and the greater the difference in dose, the higher the patient mortality rate. Furthermore, compared with the medical decision-making system based on the Deep-Q Learning Network (DQN) algorithm developed by the research teams at MIT and ICL, the optimal dosing combination recommended by our model is closer to that given by professional clinicians. Specifically, the number of patient samples administered by clinicians with the exact same dose recommended by our AI model increased by 142.3% compared with the model based on the DQN algorithm, with a reduction in the patient mortality rate of 2.58%.\n\nConclusions\nThe treatment plan generated by our medical decision-making system based on the DDPG algorithm is closer to that of a professional human clinician with a lower mortality rate in hospitalized sepsis patients, which can better help human clinicians deal with complex conditional changes in sepsis patients in an ICU. Our proposed AI-based medical decision-making system has the potential to provide the best reference dosing combinations for additional drugs.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12911-023-02175-7"
    },
    {
        "id": 32866,
        "title": "A Novel Deep Deterministic Policy Gradient Assisted Learning-Based Control Algorithm for Three-Phase DC/AC Inverter With an RL Load",
        "authors": "Chaoqun Xiang, Xinan Zhang, Tianhao Qie, Tat Kei Chau, Jian Ye, Yang Yu, Herbert Ho Ching Iu, Tyrone Fernando",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jestpe.2022.3174638"
    },
    {
        "id": 32867,
        "title": "Multiple DC Modulation Coordination Strategies Based on Transient Energy Function and Deep Deterministic Policy Gradient Algorithm",
        "authors": "Chao Xing, Mingqun Liu, Junzhen Peng, Xueke Wang, Yuhong Wang, Zongsheng Zheng, Shilin Gao, Jianquan Liao",
        "published": "2024-3-16",
        "citations": 0,
        "abstract": "Ultra-low-frequency oscillation (ULFO) is a new problem of frequency stability in asynchronous network back-end power networks. The negative damping provided by hydropower units is the direct cause of ultra-low-frequency oscillation, and DC modulation is an effective means of suppressing system frequency oscillation. Based on the transient energy function and the limitation of DC modulation, this paper analyzes the conditions of suppressing system oscillation by DC modulation from the perspective of energy. The weight of unit energy participation is defined, and the calculation formula of transient oscillation energy in a certain oscillation mode is derived. DC modulation sensitivity is then used to create a DC modulation sequence table based on the Prony identification approach. Ultimately, the DC involved in the modulation is identified using the DC modulation sequence table and the transient oscillation energy, and the Deep Deterministic Policy Gradient (DDPG) algorithm is used to optimize the chosen DC modulation parameters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14062519"
    },
    {
        "id": 32868,
        "title": "The application of machine learning-based energy management strategy in a multi-mode plug-in hybrid electric vehicle, part II: Deep deterministic policy gradient algorithm design for electric mode",
        "authors": "Jiageng Ruan, Changcheng Wu, Zhaowen Liang, Kai Liu, Bin Li, Weihan Li, Tongyang Li",
        "published": "2023-4",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.energy.2023.126792"
    },
    {
        "id": 32869,
        "title": "Enhancing Brain Tumor Detection with Gradient-Weighted Class Activation Mapping and Deep Learning Techniques",
        "authors": "Kornprom Pikulkaew",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jcsse58229.2023.10202020"
    },
    {
        "id": 32870,
        "title": "A Three-Stage Optimal Operation Strategy of Interconnected Microgrids With Rule-Based Deep Deterministic Policy Gradient Algorithm",
        "authors": "Huifeng Zhang, Dong Yue, Chunxia Dou, Gerhard P. Hancke",
        "published": "2024-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3185211"
    },
    {
        "id": 32871,
        "title": "DTN-Assisted Dynamic Cooperative Slicing for Delay-Sensitive Service in MEC-Enabled IoT via Deep Deterministic Policy Gradient With Variable Action",
        "authors": "Li Li, Lun Tang, Qinghai Liu, Yaqing Wang, Xiaoqiang He, Qianbin Chen",
        "published": "2023-6-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2023.3241925"
    },
    {
        "id": 32872,
        "title": "TD3LVSL: A lane-level variable speed limit approach based on twin delayed deep deterministic policy gradient in a connected automated vehicle environment",
        "authors": "Wenqi Lu, Ziwei Yi, Yuanli Gu, Yikang Rui, Bin Ran",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.trc.2023.104221"
    },
    {
        "id": 32873,
        "title": "Deep-Deterministic-Policy-Gradient-Based Task Offloading With Optimized <i>K</i>-Means in Edge-Computing-Enabled IoMT Cyber-Physical Systems",
        "authors": "Chenyi Yang, Xiaolong Xu, Muhammad Bilal, Yiping Wen, Tao Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jsyst.2023.3311454"
    },
    {
        "id": 32874,
        "title": "An improved analysis of cryptocurrencies for business trading deterministic with deep learning techniques",
        "authors": "Iskandar Muda, Jaymin Arvind Shah,  Jarudin, Gioia Arnone, Mohd Aarif, I. Sravan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0178455"
    },
    {
        "id": 32875,
        "title": "Optimizing Advantage Actor-Critic with Policy Gradient and Deep Q-learning to Maximize Profit in Forex Trading Prediction",
        "authors": "Abdillah Baradja, Rahmat Gernowo, Adi Wibowo",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ice-smartech59237.2023.10461955"
    },
    {
        "id": 32876,
        "title": "Gradient Clipping in Deep Learning: A Dynamical Systems Perspective",
        "authors": "Arunselvan Ramaswamy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678000003411"
    },
    {
        "id": 32877,
        "title": "An efficient and robust gradient reinforcement learning: Deep comparative policy",
        "authors": "Jiaguo Wang, Wenheng Li, Chao Lei, Meng Yang, Yang Pei",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "Recently, actor-critic architectures such as deep deterministic policy gradient (DDPG) are able to understand higher-level concepts for searching rich reward, and generate complex actions in continuous action space, and widely used in practical applications. However, when action space is limited and has dynamic hard margins, training DDPG can be problematic and inefficiency. Since real-world actuators always have margins and interferences, after initialization, the actor network is likely to be stuck at a local optimal point on action space margin: actor gradient orients to the outside of action space but actuators stop at the margin. If the hard margins are complex, dynamic and unknown to the DDPG agent, it is unable to use penalty functions to recover from local optimum. If we enlarge the random process for local exploration, the training could be in potential risk of failure. Therefore, simply relying on gradient of critic network to train the actor network is not a robust method in real environment. To solve this problem, in this paper we modify DDPG to deep comparative policy (DCP). Rather than leveraging critic-to-actor gradient, the core training process of DCP is regulated by a T-fold compare among random proposed adjacent actions. The performance of DDPG, DCP and related algorithms are tested and compared in two experiments. Our results show that, DCP is effective, efficient and qualified to perform all tasks that DDPG can perform. More importantly, DCP is less likely to be influenced by the action space margins, DCP can provide more safety in avoiding training failure and local optimum, and gain more robustness in applications with dynamic hard margins in the action space. Another advantage is that, complex penalty for margin touching detection is not required, the reward function can always be brief and short.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/jifs-233747"
    },
    {
        "id": 32878,
        "title": "Expert System-Based Multiagent Deep Deterministic Policy Gradient for Swarm Robot Decision Making",
        "authors": "Zhen Wang, Xiaoyue Jin, Tao Zhang, Jiahao Li, Dengxiu Yu, Kang Hao Cheong, C. L. Philip Chen",
        "published": "2024-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcyb.2022.3228578"
    },
    {
        "id": 32879,
        "title": "Deep Deterministic Policy Gradient-Based Intelligent Task Offloading for Vehicular Computing With Priority Experience Playback",
        "authors": "Yongan Guo, Derui Ma, Hao She, Guan Gui, Chau Yuen, Hikmet Sari, Fumiyuki Adachi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvt.2024.3378919"
    },
    {
        "id": 32880,
        "title": "A note on the convergence of deterministic gradient sampling in nonsmooth optimization",
        "authors": "Bennet Gebken",
        "published": "2024-2-6",
        "citations": 0,
        "abstract": "AbstractApproximation of subdifferentials is one of the main tasks when computing descent directions for nonsmooth optimization problems. In this article, we propose a bisection method for weakly lower semismooth functions which is able to compute new subgradients that improve a given approximation in case a direction with insufficient descent was computed. Combined with a recently proposed deterministic gradient sampling approach, this yields a deterministic and provably convergent way to approximate subdifferentials for computing descent directions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10589-024-00552-0"
    },
    {
        "id": 32881,
        "title": "Alternated Greedy-Step Deterministic Policy Gradient",
        "authors": "Xuesong Wang, Jiazhi Zhang, Yang Gu, Longyang Huang, Kun Yu, Yuhu Cheng",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcds.2023.3242274"
    },
    {
        "id": 32882,
        "title": "Three-dimensional cooperative inversion of airborne magnetic and gravity gradient data using deep-learning techniques",
        "authors": "Yanyan Hu, Xiaolong Wei, Xuqing Wu, Jiajia Sun, Yueqin Huang, Jiefu Chen",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": " Using multiple geophysical methods has become a prevailing approach in numerous geophysical applications to investigate subsurface structures and parameters. These multimethod-based exploration strategies have the potential to greatly diminish uncertainties and ambiguities encountered during geophysical data analysis and interpretation. One of the applications is the cooperative inversion of airborne magnetic and gravity gradient data for the interpretation of data obtained in mineral, oil and gas, and geothermal explorations. In this paper, a unified cooperative inversion framework is designed by combining the standard separate inversions with a deep neural network (DNN), which serves as the link between different types of data. A well-trained DNN takes the separately inverted susceptibility and density models as the inputs and provides improved models that will be used as the initial models of deterministic inversions. A two-round iteration strategy is adopted to guarantee the reasonability of the recovered models and overall efficiency of the inversion. In addition, this deep-learning (DL)-based framework demonstrates excellent generalization abilities when tested on models that are entirely distinct from the training data sets. The framework can easily incorporate multiphysics without necessitating any structural changes to the network. Synthetic experiments validate that our DL-based method outperforms conventional separate inversions and cross-gradient-based joint inversion in view of the accuracy of the recovered models and inversion efficiency. Successful application to field data further verifies the effectiveness of our DL-based method. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1190/geo2023-0225.1"
    },
    {
        "id": 32883,
        "title": "Deep learning for gradient flows using the Brezis–Ekeland principle",
        "authors": "Laura Carini, Max Jensen, Robert Nürnberg",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5817/am2023-3-249"
    },
    {
        "id": 32884,
        "title": "Guided deterministic policy optimization with gradient-free policy parameters information",
        "authors": "Chun Shen, Sheng Zhu, Shuai Han, Xiaoyu Gong, Shuai Lü",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120693"
    },
    {
        "id": 32885,
        "title": "Improved Deep Deterministic Policy Gradient for Dynamic Obstacle Avoidance of Mobile Robot",
        "authors": "Xiaoshan Gao, Liang Yan, Zhijun Li, Gang Wang, I-Ming Chen",
        "published": "2023-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2022.3230666"
    },
    {
        "id": 32886,
        "title": "A novel deep policy gradient action quantization for trusted collaborative computation in intelligent vehicle networks",
        "authors": "Miaojiang Chen, Meng Yi, Mingfeng Huang, Guosheng Huang, Yingying Ren, Anfeng Liu",
        "published": "2023-7",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119743"
    },
    {
        "id": 32887,
        "title": "Forecasting the mineral resource rent through the inclusion of economy, environment and energy: Advanced machine learning and deep learning techniques",
        "authors": "Suleman Sarwar, Ghazala Aziz, Rida Waheed, Lucía Morales",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.resourpol.2024.104729"
    },
    {
        "id": 32888,
        "title": "The Reinforce Policy Gradient Algorithm Revisited",
        "authors": "Shalabh Bhatnagar",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442123"
    },
    {
        "id": 32889,
        "title": "Comparative Analysis of Deep Learning Techniques",
        "authors": "Rutika Pawar",
        "published": "2024-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr24127165416"
    },
    {
        "id": 32890,
        "title": "Deep Reinforcement Learning for On-Demand Intelligent Routing in Deterministic Networks",
        "authors": "Ying Liu, Jianhui Yin, Weiting Zhang, Shanghan Xie",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436769"
    },
    {
        "id": 32891,
        "title": "A Multi-Policy Deep Reinforcement Learning Approach for Multi-Objective Joint Routing and Scheduling in Deterministic Networks",
        "authors": "Sijin Yang, Lei Zhuang, Jianhui Zhang, Julong Lan, Bingkui Li",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jiot.2024.3358403"
    },
    {
        "id": 32892,
        "title": "Research on the Deep Deterministic Policy Algorithm Based on the First-Order Inverted Pendulum",
        "authors": "Hailin Hu, Yuhui Chen, Tao Wang, Fu Feng, Weijin Chen",
        "published": "2023-6-27",
        "citations": 1,
        "abstract": "With the mature development of artificial intelligence technology, the application of intelligent control algorithms in control systems has become a trend to meet the high-performance requirements of modern society. This paper proposes a deep deterministic policy gradient (DDPG) controller design method based on deep reinforcement learning to improve system control performance. Firstly, the optimal control policy of the DDPG algorithm is derived from the Markov decision process and the Actor–Critic algorithm. Secondly, in order to avoid local optima in traditional control systems, the capacity and the settlement method of the DDPG experience pool are adjusted to absorb positive experience to accelerate convergence and to complete efficient training. In response, and to solve the overestimation of the Q value in DDPG, the overall structure of the Critic network is changed to shorten the convergence period of DDPG at low learning rates. Finally, a first-order inverted pendulum control system was constructed in a simulation environment to verify the control effectiveness of PID, DDPG, and improved DDPG. The simulation results reveal that the improved DDPG controller has a faster response to disturbances, smaller displacement, and angular displacement of the first-order inverted pendulum. The simulation further proves that the improved DDPG algorithm has better stability and convergence and stronger anti-interference ability and stability recovery. This control method provides a certain reference for the application of reinforcement learning in traditional control systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13137594"
    },
    {
        "id": 32893,
        "title": "Intelligent Fault Quantitative Identification via the Improved Deep Deterministic Policy Gradient (DDPG) Algorithm Accompanied With Imbalanced Sample",
        "authors": "Qianwen Cui, Liangyu Zhu, Huanqin Feng, Shuilong He, Jinglong Chen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tim.2023.3250284"
    },
    {
        "id": 32894,
        "title": "A Robust Deterministic-Based Automatic Vessel Centerline Extraction Algorithm in 3-D Binary Volumes",
        "authors": "Hassan Saeed, Andrzej Skalski",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ist59124.2023.10355715"
    },
    {
        "id": 32895,
        "title": "Deep Learning Techniques in Brain Cancer Detection",
        "authors": "Madhav Agrawal, Arham Jain",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231029151256"
    },
    {
        "id": 32896,
        "title": "Multi-Modal Fusion Techniques in Deep Learning",
        "authors": "Radhika Shetty D S",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr23905100554"
    },
    {
        "id": 32897,
        "title": "Improving generalization capability of deep learning-based nuclei instance segmentation by non-deterministic train time and deterministic test time stain normalization",
        "authors": "Amirreza Mahbod, Georg Dorffner, Isabella Ellinger, Ramona Woitek, Sepideh Hatamikia",
        "published": "2024-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csbj.2023.12.042"
    },
    {
        "id": 32898,
        "title": "Non-Deterministic Factors Affect Competition Between Thermophilic Autotrophs from Deep-Sea Hydrothermal Vents",
        "authors": "Briana Kubik, James Holden",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "Hydrothermal vents provide windows into the rocky subseafloor on Earth and serve as terrestrial analog sites for extraterrestrial environments. By studying patterns of community assembly in hydrothermal vents and using geochemical models, we can better understand how the deep-sea biosphere contributes to local and global biogeochemical cycling and gather valuable information about how similar communities may arise on Earth and beyond Earth. One prevailing thought is that vent microbial community assembly is driven by deterministic factors such as the thermodynamic favorability of redox reactions. We hypothesized that subsurface microbial communities may also be significantly influenced by other factors, such as differential cell yields, varying optimal growth temperatures, and stochasticity.\nAt Axial Seamount in the Pacific Ocean, H2-consuming methanogens of the genera Methanocaldococcus (Topt 82°C) and Methanothermococcus (Topt 65°C) and H2-consuming sulfur reducers of the genus Desulfurobacterium (Topt 72°C) are the most abundant autotrophs that grow optimally at or above 65°C (Fortunato et al. 2017). At one low-temperature hydrothermal vent site, Marker 113, methanogens are the predominant thermophilic autotrophs while at another site, Marker 33, thermophilic autotrophic sulfur reducers predominate. There is no apparent geochemical or thermodynamic explanation for the differences in community composition. In this study, we performed a series of co-culture competition experiments using Methanocaldococcus jannaschii, Methanothermococcus thermolithotrophicus, and Desulfurobacterium thermolithotrophum HR11 as representative methanogens and sulfur reducers common to hydrothermal vents to explain the variations in community composition between thermophilic autotrophs.\nM. jannaschii increases its cell yield (cells produced per mole of CH4 produced) when grown on very low H2 concentrations as part of a growth rate-growth yield tradeoff (Topçuoğlu et al. 2019). This increase in cell yield could provide methanogens with a competitive growth advantage over H2-consuming sulfur reducers, who otherwise catalyze a more thermodynamically favorable growth reaction. Competition co-culture experiments were conducted between M. jannaschii and D. thermolithotrophum at 72°C and between M. thermolithotrophicus and D. thermolithotrophum at 65°C, both at 1:1 ratios and initial aqueous H2 concentrations of 1.2 mM (high H2) and 85 μM (low H2) to determine the effects of temperature and H2 availability on autotroph competition. For both methanogens, the growth rate, maximum cell concentration, and total CH4 produced decreased when they were grown in co-culture, at low H2, or both relative to monocultures grown with high H2. The methanogen cell yields generally increased in co-culture and at low H2. At both experimental temperatures, the growth rate of D. thermolithotrophum remained unchanged in co-culture and at low H2 relative to monocultures but the maximum cell concentration decreased in co-culture relative to monocultures at both H2 concentrations. However, at low H2, both in mono- and co-culture, there was no detectable H2S produced by the sulfur reducer suggesting a significant shift in growth yield. At both temperatures and H2 concentrations, the sulfur reducer reached higher cell concentrations than the methanogens.\nStochasticity or vent fluid chemistry could lead to early colonization of a vent by methanogens followed by niche exclusion of autotrophic sulfur reducers due to a numerical advantage of the methanogens. Therefore, competitive co-culture experiments were run as before at high H2 with varying initial methanogen:sulfur reducer ratios. At 72°C, D. thermolithotrophum reached the same maximum cell concentration and produced the same amount of H2S in monoculture and co-culture even when the methanogens initially outnumbered the sulfur reducer up to 10,000-fold. M. jannaschii reached a lower maximum cell concentration and produced less CH4 in all co-cultures relative to growth in monoculture. At 65°C, D. thermolithotrophum reached the same maximum cell concentrations and produced the same amount of H2S in monoculture and co-culture when the methanogens initially outnumbered the sulfur reducers up to 100-fold. However, when the methanogens initially outnumbered the sulfur reducers 1,000-fold, M. thermolithotrophicus grew as well as in monoculture and the maximum cell concentration and amount of H2S produced by D. thermolithotrophum was significantly lower than in monoculture and the other co-culture conditions.\nIn conclusion, both methanogens and sulfur reducers shift their redox reactions away from CH4 and H2S production, respectively, and towards biomass production when H2 is limiting. This should be accounted for in thermodynamic predictive models. Furthermore, a combination of growth temperatures lower than the optimum of sulfur reducers and high initial methanogen cell concentrations relative to sulfur reducers can lead to a long-term predominance of methanogens over autotrophic sulfur reducers in vent environments through niche exclusion.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3897/aca.6.e108248"
    },
    {
        "id": 32899,
        "title": "Deterministic processes drive turnover-dominated beta diversity of breeding birds along the central Himalayan elevation gradient",
        "authors": "Zhifeng Ding, Jianchao Liang, Le Yang, Cong Wei, Huijian Hu, Xingfeng Si",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.avrs.2024.100170"
    },
    {
        "id": 32900,
        "title": "Evaluation of Deep Learning Techniques for Entity Matching",
        "authors": "Paulo Lima, Douglas Santana, Wellington Martins, Leonardo Ribeiro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011996200003467"
    },
    {
        "id": 32901,
        "title": "Effective credit assignment deep policy gradient multi-agent reinforcement learning for vehicle dispatch",
        "authors": "Xiaohui Huang, Xiong Zhang, Jiahao Ling, Xuebo Cheng",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-04689-z"
    },
    {
        "id": 32902,
        "title": "Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model",
        "authors": "Yingkai Sha, Ryan A. Sobash, David John Gagne",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Abstract\nAn ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1–24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overconfident but produces meaningful ensemble spreads that can distinguish good and bad forecasts. The quality of CGAN outputs is also evaluated. Results show that the CGAN outputs behave similarly to a numerical ensemble; they preserved the inter-variable correlations and the contribution of influential predictors as in the original HRRR forecasts. This work provides a novel approach to post-process CAM output using neural networks that can be applied to severe weather prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1175/aies-d-23-0094.1"
    },
    {
        "id": 32903,
        "title": "Autonomous pricing using policy gradient reinforcement learning",
        "authors": "Kevin Michael Frick",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4527452"
    },
    {
        "id": 32904,
        "title": "New radiological techniques for planning of deep brain stimulation",
        "authors": "Cihan Isler, Gulcin Bas",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jdbs.2023.12.004"
    }
]