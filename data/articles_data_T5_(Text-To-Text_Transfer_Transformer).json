[
    {
        "id": 9384,
        "title": "Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\"",
        "authors": "Qurrota A’yuna Itsnaini, Mardhiya Hayaty, Andriyan Dwi Putra, Nidal A.M Jabari",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "Automatic Text Summarization (ATS) is one of the utilizations of technological sophistication in terms of text processing assisting humans in producing a summary or key points of a document in large quantities. We use Indonesian language as objects because there are few resources in NLP research using Indonesian language. This paper utilized PLTMs (Pre-Trained Language Models) from the transformer architecture, namely T5 (Text-to-Text Transfer Transformer) which has been completed previously with a larger dataset. Evaluation in this study was measured through comparison of the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) calculation results between the reference summary and the model summary. The experiments with the pre-trained t5-base model with fine tuning parameters of 220M for the Indonesian news dataset yielded relatively high ROUGE values, namely ROUGE-1 = 0.68, ROUGE-2 = 0.61, and ROUGE-L = 0.65. The evaluation value worked well, but the resulting model has not achieved satisfactory results because in terms of abstraction, the model did not work optimally. We also found several errors in the reference summary in the dataset used.",
        "link": "http://dx.doi.org/10.33096/ilkom.v15i1.1532.124-131"
    },
    {
        "id": 9385,
        "title": "Ensemble-NQG-T5: Ensemble Neural Question Generation Model Based on Text-to-Text Transfer Transformer",
        "authors": "Myeong-Ha Hwang, Jikang Shin, Hojin Seo, Jeong-Seon Im, Hee Cho, Chun-Kwon Lee",
        "published": "2023-1-9",
        "citations": 6,
        "abstract": "Deep learning chatbot research and development is exploding recently to offer customers in numerous industries personalized services. However, human resources are used to create a learning dataset for a deep learning chatbot. In order to augment this, the idea of neural question generation (NQG) has evolved, although it has restrictions on how questions can be expressed in different ways and has a finite capacity for question generation. In this paper, we propose an ensemble-type NQG model based on the text-to-text transfer transformer (T5). Through the proposed model, the number of generated questions for each single NQG model can be greatly increased by considering the mutual similarity and the quality of the questions using the soft-voting method. For the training of the soft-voting algorithm, the evaluation score and mutual similarity score weights based on the context and the question–answer (QA) dataset are used as the threshold weight. Performance comparison results with existing T5-based NQG models using the SQuAD 2.0 dataset demonstrate the effectiveness of the proposed method for QG. The implementation of the proposed ensemble model is anticipated to span diverse industrial fields, including interactive chatbots, robotic process automation (RPA), and Internet of Things (IoT) services in the future.",
        "link": "http://dx.doi.org/10.3390/app13020903"
    },
    {
        "id": 9386,
        "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
        "authors": "Markéta Řezáčková, Jan Švec, Daniel Tihelka",
        "published": "2021-8-30",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-546"
    },
    {
        "id": 9387,
        "title": "Turkish abstractive text document summarization using text to text transfer transformer",
        "authors": "Betul Ay, Fatih Ertam, Guven Fidan, Galip Aydin",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aej.2023.01.008"
    },
    {
        "id": 9388,
        "title": "Abstractive summarization of COVID-19 with transfer text-to-text transformer",
        "authors": "Zhaopu Teng",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "As a classic problem of Natural Language Processing, summarization provides convenience for studies, research, and daily life. The performance of generation summarization by Natural Language Processing techniques has attracted considerable attention. Meanwhile, COVID-19, a global explosion event, has led to the emergence of a large number of articles and research. The wide variety of articles makes it a perfect realization object for summarization generation tasks. This paper designed and implemented experiments by fine tuning T5 model to get an abstract summarization of COVID-19 literatures. A comparison of performance was shown to prove the reliability of the model.",
        "link": "http://dx.doi.org/10.54254/2755-2721/2/20220520"
    },
    {
        "id": 9389,
        "title": "Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer",
        "authors": "Kim Cheng Sheang, Horacio Saggion",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.inlg-1.38"
    },
    {
        "id": 9390,
        "title": "Enhance Text-to-Text Transfer Transformer with Generated Questions for Thai Question Answering",
        "authors": "Puri Phakmongkol, Peerapon Vateekul",
        "published": "2021-11-1",
        "citations": 4,
        "abstract": "Question Answering (QA) is a natural language processing task that enables the machine to understand a given context and answer a given question. There are several QA research trials containing high resources of the English language. However, Thai is one of the languages that have low availability of labeled corpora in QA studies. According to previous studies, while the English QA models could achieve more than 90% of F1 scores, Thai QA models could obtain only 70% in our baseline. In this study, we aim to improve the performance of Thai QA models by generating more question-answer pairs with Multilingual Text-to-Text Transfer Transformer (mT5) along with data preprocessing methods for Thai. With this method, the question-answer pairs can synthesize more than 100 thousand pairs from provided Thai Wikipedia articles. Utilizing our synthesized data, many fine-tuning strategies were investigated to achieve the highest model performance. Furthermore, we have presented that the syllable-level F1 is a more suitable evaluation measure than Exact Match (EM) and the word-level F1 for Thai QA corpora. The experiment was conducted on two Thai QA corpora: Thai Wiki QA and iApp Wiki QA. The results show that our augmented model is the winner on both datasets compared to other modern transformer models: Roberta and mT5.",
        "link": "http://dx.doi.org/10.3390/app112110267"
    },
    {
        "id": 9391,
        "title": "Text-to-Text Transfer Transformer Phrasing Model Using Enriched Text Input",
        "authors": "Markéta Řezáčková, Jindřich Matoušek",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-16270-1_32"
    },
    {
        "id": 9392,
        "title": "HaT5: Hate Language Identification using Text-to-Text Transfer Transformer",
        "authors": "Sana Sabah Sabry, Tosin Adewumi, Nosheen Abid, Gyorgy Kovacs, Foteini Liwicki, Marcus Liwicki",
        "published": "2022-7-18",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892696"
    },
    {
        "id": 9393,
        "title": "Transformer Text Classification Model for Arabic Dialects that Utilizes Inductive Transfer",
        "authors": "Laith H. Baniata, Sangwoo Kang",
        "published": "No Date",
        "citations": 3,
        "abstract": "In the realm of the five-category classification endeavor, there has been limited exploration into applied techniques for classifying Arabic text. These methods have primarily leaned on singletask learning, incorporating manually crafted features that lack robust sentence representations. Recently, the Transformer paradigm has emerged as a highly promising alternative. However, when these models are trained using single-task learning, they often face challenges in achieving outstanding performance and generating robust latent feature representations, especially when dealing with small datasets. This issue is particularly pronounced in the context of the Arabic dialect, which has a scarcity of available resources. Given these constraints, this study introduces an innovative approach to dissecting sentiment in Arabic text. This approach combines Inductive Transfer (INT) with the Transformer paradigm to augment the adaptability of the model and refine the representation of sentences. By employing self-attention SE-A and feed-forward sub-layers as a shared Transformer encoder for both the five-category and three-category Arabic text classification tasks, this proposed model adeptly discerns sentiment in Arabic dialect sentences. The empirical findings underscore the commendable performance of the proposed model, as demonstrated in assessments of the Hotel Arabic-Reviews Dataset, the Book Reviews Arabic Dataset, and the LARB dataset. ",
        "link": "http://dx.doi.org/10.20944/preprints202311.0818.v1"
    },
    {
        "id": 9394,
        "title": "English grammar multiple-choice question generation using Text-to-Text Transfer Transformer",
        "authors": "Peerawat Chomphooyod, Atiwong Suchato, Nuengwong Tuaycharoen, Proadpran Punyabukkana",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.caeai.2023.100158"
    },
    {
        "id": 9395,
        "title": "End-to-End generation of Multiple-Choice questions using Text-to-Text transfer Transformer models",
        "authors": "Ricardo Rodriguez-Torrealba, Eva Garcia-Lopez, Antonio Garcia-Cabot",
        "published": "2022-12",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.118258"
    },
    {
        "id": 9396,
        "title": "Advancing Abstractive Bangla Text Summarization: A Deep Learning Approach Using Seq2seq Encoder- Decoder Model and T5 Transformer",
        "authors": "S M Tasnimul Hasan, Md Ashfaqur Rahman, Md Mahamudul Hasan, Mohammad Rakibul Hasan, Md Moinul Hoque",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sti59863.2023.10464712"
    },
    {
        "id": 9397,
        "title": "Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks",
        "authors": "Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, Gabriele Bavota",
        "published": "2021-5",
        "citations": 85,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icse43902.2021.00041"
    },
    {
        "id": 9398,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 9399,
        "title": "Mask Transformer: Unpaired Text Style Transfer Based on Masked Language",
        "authors": "Chunhua Wu, Xiaolong Chen, Xingbiao Li",
        "published": "2020-9-6",
        "citations": 1,
        "abstract": "Currently, most text style transfer methods encode the text into a style-independent latent representation and decode it into new sentences with the target style. Due to the limitation of the latent representation, previous works can hardly get satisfactory target style sentence especially in terms of semantic remaining of the original sentence. We propose a “Mask and Generation” structure, which can obtain an explicit representation of the content of original sentence and generate the target sentence with a transformer. This explicit representation is a masked text that masks the words with the strong style attribute in the sentence. Therefore, it can preserve most of the semantic meaning of the original sentence. In addition, as it is the input of the generator, it also simplified this process compared to the current work who generate the target sentence from scratch. As the explicit representation is readable and the model has better interpretability, we can clearly know which words changed and why the words changed. We evaluate our model on two review datasets with quantitative, qualitative, and human evaluations. The experimental results show that our model generally outperform other methods in terms of transfer accuracy and content preservation.",
        "link": "http://dx.doi.org/10.3390/app10186196"
    },
    {
        "id": 9400,
        "title": "Text Data Labelling using Transformer based Sentence Embeddings and Text Similarity for Text Classification",
        "authors": "Amiya Amitabh Chakrabarty",
        "published": "2022-4-30",
        "citations": 0,
        "abstract": "This paper demonstrates that a lot of time, cost, and complexities can be saved and avoided that would otherwise be used to label the text data for classification purposes. The AI world realizes the importance of labelled data and its use for various NLP applications.  Here, we have labelled and categorized close to 6,000 unlabelled samples into five distinct classes. This labelled dataset was further used for multi-class text classification.  Data labelling task using transformer-based sentence embeddings and applying cosine-based text similarity threshold saved close to 20-30 days of human efforts and multiple human validations with 98.4% of classes correctly labelled as per business validation. Text classification results obtained using this AI labelled data fetched accuracy score and F1 score of 90%.",
        "link": "http://dx.doi.org/10.5121/ijnlc.2022.11201"
    },
    {
        "id": 9401,
        "title": "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models",
        "authors": "Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, Yinfei Yang",
        "published": "2022",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-acl.146"
    },
    {
        "id": 9402,
        "title": "Masked transformer through knowledge distillation for unsupervised text style transfer",
        "authors": "Arthur Scalercio, Aline Paes",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "Abstract\nText style transfer (TST) aims at automatically changing a text’s stylistic features, such as formality, sentiment, authorial style, humor, and complexity, while still trying to preserve its content. Although the scientific community has investigated TST since the 1980s, it has recently regained attention by adopting deep unsupervised strategies to address the challenge of training without parallel data. In this manuscript, we investigate how relying on sequence-to-sequence pretraining models affects the performance of TST when the pretraining step leverages pairs of paraphrase data. Furthermore, we propose a new technique to enhance the sequence-to-sequence model by distilling knowledge from masked language models. We evaluate our proposals on three unsupervised style transfer tasks with widely used benchmarks: author imitation, formality transfer, and polarity swap. The evaluation relies on quantitative and qualitative analyses and comparisons with the results of state-of-the-art models. For the author imitation and the formality transfer task, we show that using the proposed techniques improves all measured metrics and leads to state-of-the-art (SOTA) results in content preservation and an overall score in the author imitation domain. In the formality transfer domain, we paired with the SOTA method in the style control metric. Regarding the polarity swap domain, we show that the knowledge distillation component improves all measured metrics. The paraphrase pretraining increases content preservation at the expense of harming style control. Based on the results reached in these domains, we also discuss in the manuscript if the tasks we address have the same nature and should be equally treated as TST tasks.",
        "link": "http://dx.doi.org/10.1017/s1351324923000323"
    },
    {
        "id": 9403,
        "title": "Text Mining Drug-Protein Interactions using an Ensemble of BERT, Sentence BERT and T5 models",
        "authors": "Xin Sui, Wanjing Wang, Jinfeng Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn this work, we trained an ensemble model for predicting drug-protein interactions within a sentence based on only its semantics. Our ensembled model was built using three separate models: 1) a classification model using a fine-tuned BERT model; 2) a fine-tuned sentence BERT model that embeds every sentence into a vector; and 3) another classification model using a fine-tuned T5 model. In all models, we further improved performance using data augmentation. For model 2, we predicted the label of a sentence using k-nearest neighbors with its embedded vector. We also explored ways to ensemble these 3 models: a) we used the majority vote method to ensemble these 3 models; and b) based on the HDBSCAN clustering algorithm, we trained another ensemble model using features from all the models to make decisions. Our best model achieved an F-1 score of 0.753 on the BioCreative VII Track 1 test dataset.",
        "link": "http://dx.doi.org/10.1101/2021.10.26.465944"
    },
    {
        "id": 9404,
        "title": "Chatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language transformer ensemble for text classification",
        "authors": "Jordan J. Bird, Anikó Ekárt, Diego R. Faria",
        "published": "2023-4",
        "citations": 17,
        "abstract": "AbstractIn this work we present the Chatbot Interaction with Artificial Intelligence (CI-AI) framework as an approach to the training of a transformer based chatbot-like architecture for task classification with a focus on natural human interaction with a machine as opposed to interfaces, code, or formal commands. The intelligent system augments human-sourced data via artificial paraphrasing in order to generate a large set of training data for further classical, attention, and language transformation-based learning approaches for Natural Language Processing (NLP). Human beings are asked to paraphrase commands and questions for task identification for further execution of algorithms as skills. The commands and questions are split into training and validation sets. A total of 483 responses were recorded. Secondly, the training set is paraphrased by the T5 model in order to augment it with further data. Seven state-of-the-art transformer-based text classification algorithms (BERT, DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are benchmarked for both sets after fine-tuning on the training data for two epochs. We find that all models are improved when training data is augmented by the T5 model, with an average increase of classification accuracy by 4.01%. The best result was the RoBERTa model trained on T5 augmented data which achieved 98.96% classification accuracy. Finally, we found that an ensemble of the five best-performing transformer models via Logistic Regression of output label predictions led to an accuracy of 99.59% on the dataset of human responses. A highly-performing model allows the intelligent system to interpret human commands at the social-interaction level through a chatbot-like interface (e.g. “Robot, can we have a conversation?”) and allows for better accessibility to AI by non-technical users.",
        "link": "http://dx.doi.org/10.1007/s12652-021-03439-8"
    },
    {
        "id": 9405,
        "title": "Medical Reports Summarization Using Text-To-Text Transformer",
        "authors": "Abdulkader Helwan, Danielle Azar, Dilber Uzun Ozsahin",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aset56582.2023.10180671"
    },
    {
        "id": 9406,
        "title": "Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation",
        "authors": "Hirotaka Tokuyama, Sakriani Sakti, Katsuhito Sudoh, Satoshi Nakamura",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1020"
    },
    {
        "id": 9407,
        "title": "Application of Noise Filter Mechanism for T5-Based Text-to-SQL Generation",
        "authors": "M.R. Aadhil Rushdy, Uthayasanker Thayasivam",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mercon60487.2023.10355492"
    },
    {
        "id": 9408,
        "title": "MSR43 Extracting Severity Markers from Unstructured Clinical Data of Congestive Heart Failure Patients Using a Pretrained Text-to-Text Transfer Transformer Model",
        "authors": "V Kumar, L Rasouliyan, AG Althoff, S Long, C Zema, MB Rao",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jval.2022.04.1250"
    },
    {
        "id": 9409,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v2"
    },
    {
        "id": 9410,
        "title": "Development of a Text Classification Framework using Transformer-based Embeddings",
        "authors": "Sumona Yeasmin, Nazia Afrin, Kashfia Saif, Mohammad Huq",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011268000003269"
    },
    {
        "id": 9411,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v1"
    },
    {
        "id": 9412,
        "title": "Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation",
        "authors": "Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang",
        "published": "2019",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1601"
    },
    {
        "id": 9413,
        "title": "Automatic FAQ Generation Using Text-to-Text Transformer Model",
        "authors": "Santosh Vasisht, Varun Tirthani, Akhil Eppa, Punit Koujalgi, Ramamoorthy Srinath",
        "published": "2022-5-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet54531.2022.9823967"
    },
    {
        "id": 9414,
        "title": "Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer",
        "authors": "Johanes Effendi, Sakriani Sakti, Satoshi Nakamura",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-970"
    },
    {
        "id": 9415,
        "title": "T5G2P: Multilingual Grapheme-to-Phoneme Conversion with Text-to-Text Transfer Transformer",
        "authors": "Markéta Řezáčková, Adam Frémund, Jan Švec, Jindřich Matoušek",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47665-5_27"
    },
    {
        "id": 9416,
        "title": "Transformer Text Classification Model for Arabic Dialects That Utilizes Inductive Transfer",
        "authors": "Laith H. Baniata, Sangwoo Kang",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "In the realm of the five-category classification endeavor, there has been limited exploration of applied techniques for classifying Arabic text. These methods have primarily leaned on single-task learning, incorporating manually crafted features that lack robust sentence representations. Recently, the Transformer paradigm has emerged as a highly promising alternative. However, when these models are trained using single-task learning, they often face challenges in achieving outstanding performance and generating robust latent feature representations, especially when dealing with small datasets. This issue is particularly pronounced in the context of the Arabic dialect, which has a scarcity of available resources. Given these constraints, this study introduces an innovative approach to dissecting sentiment in Arabic text. This approach combines Inductive Transfer (INT) with the Transformer paradigm to augment the adaptability of the model and refine the representation of sentences. By employing self-attention SE-A and feed-forward sub-layers as a shared Transformer encoder for both the five-category and three-category Arabic text classification tasks, this proposed model adeptly discerns sentiment in Arabic dialect sentences. The empirical findings underscore the commendable performance of the proposed model, as demonstrated in assessments of the Hotel Arabic-Reviews Dataset, the Book Reviews Arabic Dataset, and the LARB dataset.",
        "link": "http://dx.doi.org/10.3390/math11244960"
    },
    {
        "id": 9417,
        "title": "mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences",
        "authors": "David Uthus, Santiago Ontanon, Joshua Ainslie, Mandy Guo",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.628"
    },
    {
        "id": 9418,
        "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
        "authors": "Zhiming Mao, Huimin Wang, Yiming Du, Kam-Fai Wong",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.100"
    },
    {
        "id": 9419,
        "title": "Learning Embeddings from Free-text Triage Notes using Pretrained Transformer Models",
        "authors": "Émilien Arnaud, Mahmoud Elbattah, Maxime Gignon, Gilles Dequen",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011012800003123"
    },
    {
        "id": 9420,
        "title": "EdiT5: Semi-Autoregressive Text Editing with T5 Warm-Start",
        "authors": "Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.156"
    },
    {
        "id": 9421,
        "title": "Comparative analysis of T5 model for abstractive text summarization on different datasets",
        "authors": "Tawmo T, Mrinmoi Borah, Pankaj Dadure, Partha Pakray",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4096413"
    },
    {
        "id": 9422,
        "title": "Speech-and-Text Transformer: Exploiting Unpaired Text for End-to-End Speech Recognition",
        "authors": "Qinyi Wang, Xinyuan Zhou, Haizhou Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/116.00000001"
    },
    {
        "id": 9423,
        "title": "BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature",
        "authors": "Giacomo Frisoni, Miki Mizutani, Gianluca Moro, Lorenzo Valgimigli",
        "published": "2022",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.390"
    },
    {
        "id": 9424,
        "title": "Text-to-Text Pre-Training with Paraphrasing for Improving Transformer-Based Image Captioning",
        "authors": "Ryo Masumura, Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro Tanaka, Shota Orihashi",
        "published": "2023-9-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289992"
    },
    {
        "id": 9425,
        "title": "Assessing the Effectiveness of Multilingual Transformer-based Text Embeddings for Named Entity Recognition in Portuguese",
        "authors": "Diego Santos, Frederico Dutra, Fernando Parreiras, Wladmir Brandão",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010443204730483"
    },
    {
        "id": 9426,
        "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
        "authors": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang",
        "published": "2022",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-naacl.55"
    },
    {
        "id": 9427,
        "title": "(Psycho-)Linguistic Features Meet Transformer Models for Improved Explainable and Controllable Text Simplification",
        "authors": "Yu Qiao, Xiaofei Li, Daniel Wiechmann, Elma Kerz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.tsar-1.12"
    },
    {
        "id": 9428,
        "title": "WATS-SMS: A T5-based French Wikipedia Abstractive Text Summarizer for SMS",
        "authors": "Jean Louis Ebongue Kedieng Fendji, Désiré Manuel Taira, Marcellin Atemkeng, Adam Musa Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "Text summarization remains a challenging task in the Natural Language Processing field despite the plethora of applications in enterprises and daily life. One of the common use cases is the summarization of web pages which has the potential to provide an overview of web pages to devices with limited features. In fact, despite the increasing penetration rate of mobile devices in rural areas, the bulk of those devices offer limited features in addition to the fact that these areas are covered with limited connectivity such as the GSM network. Summarizing web pages into SMS becomes, therefore, an important task to provide information to limited devices. This work introduces WATS-SMS, a T5-based French Wikipedia Abstractive Text Summarizer for SMS. It is built through a transfer learning approach. The T5 English pre-trained model is used to generate a French text summarization model by retraining the model on 25,000 Wikipedia pages then compared with different approaches in the literature. The objective is twofold: (1) to check the assumption made in the literature that abstractive models provide better results compared to extractive ones; and (2) to evaluate the performance of our model compared to other existing abstractive models. A score based on ROUGE metrics gave us a value of 52% for articles with length up to 500 characters against 34.2% for transformer-ED and 12.7% for seq-2seq-attention; and a value of 77% for articles with larger size against 37% for transformers-DMCA. Moreover, an architecture including a software SMS-gateway has been developed to allow owners of mobile devices with limited features to send requests and to receive summaries through the GSM network.",
        "link": "http://dx.doi.org/10.20944/preprints202108.0311.v1"
    },
    {
        "id": 9429,
        "title": "T5 language models for text simplification",
        "authors": "Д.Д. Васильев, D.D. Vasiliev,, А.В. Пятаева, A.V. Pyataeva,",
        "published": "2023-6-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15827/0236-235x.142.228-236"
    },
    {
        "id": 9430,
        "title": "Transforming the Generative Pretrained Transformer Into Augmented Business Text Writer",
        "authors": "Faisal Khalil, Prof. Dr. Gordon Pipa",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study uses transformers architecture of Artificial neural networks to generate artificial business text for a given topic or theme. The implication of the study is to augment the business report writing, and general business writings process with help of Generative pretrained transformers (GPT) networks. Main focus of study is to provide practical use case for GPTs models with help of big data. Our study model has 355 million model parameters and trained for three months on GPU enable devices using 2.3 billion text tokens(is available as open-source data now). Text tokens are collected with help of rigorous preprocessing, which includes; shortlisting of Subreddits of Fortune 500 companies and industries, listed on US-based social news aggregation online portal called \"Reddit\". After shortlisting, millions of submission of users during the five years, are parsed to collect the URLs out of it. 1.8 million working URLs are scrutinized. Business text is parsed, cleaned, and converted into word embeddings out of URLs. The result shows that both models; conditional interactive and random sampling, generate text paragraphs that are grammatically accurate and stick to the given topic.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1170589/v1"
    },
    {
        "id": 9431,
        "title": "Technical Notes on the Newell Lyon Text",
        "authors": "",
        "published": "2021-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2307/j.ctv1rnpj52.24"
    },
    {
        "id": 9432,
        "title": "ArTST: Arabic Text and Speech Transformer",
        "authors": "Hawau Toyin, Amirbek Djanibekov, Ajinkya Kulkarni, Hanan Aldarmaki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.5"
    },
    {
        "id": 9433,
        "title": "Diffusion Transformer for Adaptive Text-to-Speech",
        "authors": "Haolin Chen, Philip N. Garner",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/ssw.2023-25"
    },
    {
        "id": 9434,
        "title": "TEXT SUMMARIZATION BASED ON TOPICRANK METHOD AND TEXT-TO-TEXT TRANSFORMER NEURAL NETWORK",
        "authors": "I.S. Sukhaniuk, K.R. Potapova, M.V. Nalyvaichuk, L.B. Vovk",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32782/2663-5941/2023.6/22"
    },
    {
        "id": 9435,
        "title": "Math Word Problem Solver Based on Text-to-Text Transformer Model",
        "authors": "Chuanzhi Yang, Runze Huang, Xinguo Yu, Rao Peng",
        "published": "2021-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tale52509.2021.9678686"
    },
    {
        "id": 9436,
        "title": "Abstractive Text Summarization Based on Long-Short Transformer",
        "authors": "Shibo Ji, Bo Yang",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiiot58121.2023.10174260"
    },
    {
        "id": 9437,
        "title": "Renierite (ren)",
        "authors": "Ricardo Castroviejo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12654-3_106"
    },
    {
        "id": 9438,
        "title": "Keyword Extraction from Short Texts with a Text-to-Text Transfer Transformer",
        "authors": "Piotr Pęzik, Agnieszka Mikołajczyk, Adam Wawrzyński, Bartłomiej Nitoń, Maciej Ogrodniczuk",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-8234-7_41"
    },
    {
        "id": 9439,
        "title": "Controlled text style transfer via noise enhancement of deep learning transformer",
        "authors": "Xingxin Zhang, Shuhao Shi, Zhigang Guo, Gang Chen, Han Wei, Yongwang Tang, Liuyang Yu",
        "published": "2022-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2639492"
    },
    {
        "id": 9440,
        "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model",
        "authors": "Kuntal Kumar Pal, Chitta Baral",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.265"
    },
    {
        "id": 9441,
        "title": "IndT5: A Text-to-Text Transformer for 10 Indigenous Languages",
        "authors": "El Moatez Billah Nagoudi, Wei-Rui Chen, Muhammad Abdul-Mageed, Hasan Cavusoglu",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.americasnlp-1.30"
    },
    {
        "id": 9442,
        "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
        "authors": "Wen-Chin Huang, Tomoki Hayashi, Yi-Chiao Wu, Hirokazu Kameoka, Tomoki Toda",
        "published": "2020-10-25",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1066"
    },
    {
        "id": 9443,
        "title": "Rare top quark production at the LHC: $\\text{t}\\bar{\\text{t}}\\text{Z}$, $\\text{t}\\bar{\\text{t}}\\text{W}$, $\\text{t}\\bar{\\text{t}}\\gamma$, $\\text{tZq}$, $\\text{t}\\gamma\\text{q}$, and $\\text{t}\\bar{\\text{t}}\\text{t}\\bar{\\text{t}}$",
        "authors": "Joscha Knolle,  ",
        "published": "2019-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22323/1.352.0157"
    },
    {
        "id": 9444,
        "title": "Comparing Transformer and RNN Models in BCIs for Handwritten Text Decoding via Neural Signals",
        "authors": "Aashna Hari",
        "published": "No Date",
        "citations": 0,
        "abstract": "The purpose of this study is to ​​explore the use of a custom Transformer model in brain-computer interfaces (BCIs) that translate the neural activity present when an individual with limited verbal and fine-motor skills attempts to handwrite. As found in previous studies, Transformers have performed better than recurrent neural networks (RNNs) in translation tasks which are similar to its usage here in decoding neural signals into intended handwritten text. Due to the known benefits of a Transformer, the hypothesis was that the Transformer would show promise in the context of a BCI through the recorded metrics. The neural signals of a tetraplegic individual, when they attempted to handwrite, were provided by existing research. Four trials were conducted using data with or without augmentation which the model used when training to separately minimize training loss and validation loss. When comparing the results of a BCI with the implementation of a Transformer model with the original RNN BCI (the original data source), the Transformer model performed less favorably across all four trials. Although the results do not indicate that the Transformer model currently outperforms an RNN BCI, it is important to note that further testing of the model's capabilities (such as training it with a larger and more preferable dataset and/or for longer, comparing training times between the RNN and Transformer, and/or seeing how the Transformer is improved with an offline autocorrect feature) is necessary before determining whether Transformers can enhance communication in this manner.",
        "link": "http://dx.doi.org/10.20944/preprints202312.0674.v1"
    },
    {
        "id": 9445,
        "title": "Proposed Quantum Text Teleportation Protocol (QTTP) for Secure Text Transfer by using Quantum Teleportation and Huffman Coding",
        "authors": "Mekala Karthik, Jitesh Lalwani, Babita Jajodia",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this work, the authors present Quantum Text Teleportation Protocol (QTTP) that uses Quantum Teleportation (QT) technique and Huffman Coding for secure text transfers. The QTTP enables the teleportation of quantum states of text (for example, email) in a secure manner, while simultaneously encrypting and decrypting them using Huffman Coding since data can only be retrieved or decoded if the prefix codes are known. The Huffman Coding approach offers the benefit of compressing the entire text, resulting in faster transmission of large amounts of information. For proof of concept, the authors experimentally evaluated both of the proposed QTTPs (Standard QTTP and QTTP with Huffman Coding) using Quantum Information Science Kit (QISKit), a quantum computing platform and simulated on IBM QASM Simulator and on IBM real quantum hardware.",
        "link": "http://dx.doi.org/10.31219/osf.io/4svxf"
    },
    {
        "id": 9446,
        "title": "Text simplification as a controlled text style transfer task",
        "authors": "Maria Tikhonova,  , Alena Fenogenova,  ",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "The task of text simplification is to reduce the complexity of the given piece of text while preserving its original meaning to improve readability and understanding. In this paper, we consider the simplification task as a subfield of the general text style transfer problem and apply methods of controllable text style to rewrite texts in a simpler manner preserving their meaning. Namely, we use a paraphrase model guided by another style-conditional language model. In our work, we perform a series of experiments and compare this approach with the standard fine-tuning of an autoregressive model.",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-507-516"
    },
    {
        "id": 9447,
        "title": "TT2INet: Text to Photo-realistic Image Synthesis with Transformer as Text Encoder",
        "authors": "Jianwei Zhu, Zhixin Li, Huifang Ma",
        "published": "2021-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534074"
    },
    {
        "id": 9448,
        "title": "Improving Text Classifiers Through Controlled Text Generation Using Transformer Wasserstein Autoencoder",
        "authors": "C. Harikrishnan, N. M. Dhanya",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-5529-6_8"
    },
    {
        "id": 9449,
        "title": "Text Summarization using Transformer Model",
        "authors": "Jaishree Ranganathan, Gloria Abuka",
        "published": "2022-11-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/snams58071.2022.10062698"
    },
    {
        "id": 9450,
        "title": "A transformer-based generative adversarial learning to detect sarcasm from Bengali text with correct classification of confusing text",
        "authors": "Sanzana Karim Lora, Ishrat Jahan, Rahad Hussain, Rifat Shahriyar, A.B.M. Alim Al Islam",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2023.e22531"
    },
    {
        "id": 9451,
        "title": "MGCoT: Multi-Grained Contextual Transformer for Table-Based Text Generation",
        "authors": "Xianjie Mo, Yang Xiang, Youcheng Pan, Yongshuai Hou, Ping Luo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4401864"
    },
    {
        "id": 9452,
        "title": "Transformer Network for video to text translation",
        "authors": "Mubashira N, Ajay James",
        "published": "2020-12-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/picc51425.2020.9362374"
    },
    {
        "id": 9453,
        "title": "Text Simplification Using Transformer and BERT",
        "authors": "Sarah Alissa, Mike Wald",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/cmc.2023.033647"
    },
    {
        "id": 9454,
        "title": "Tamil Text Error Correction with Multi-lingual T5 Model",
        "authors": "Vaan Amuthu Elango, Peeta Basa Pati",
        "published": "2023-5-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vitecon58111.2023.10157315"
    },
    {
        "id": 9455,
        "title": "A Method of Generating Chinese News Text Summaries Based on the T5-PEGASUS-PGN Model",
        "authors": "一平 曹",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2024.143053"
    },
    {
        "id": 9456,
        "title": "Luminescence and energy transfer of Sm3+-codoped $${{\\text{Y}}_{2.9}}{\\text{A}}{{\\text{l}}_{4.25}}{\\text{G}}{{\\text{a}}_{0.75}}{{\\text{O}}_{12}}:{\\text{Ce}}_{{0.1}}^{{3+}}$$ phosphor",
        "authors": "Lipeng Jiang, Xiyan Zhang, Chengli Wang, He Tang, Shiqi Zhu, Qingxi Li, Xiaoyun Mi, Liping Lu",
        "published": "2017-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10854-017-7843-0"
    },
    {
        "id": 9457,
        "title": "The effectiveness of T5, GPT-2, and BERT on text-to-image generation task",
        "authors": "Mourad Bahani, Aziza El Ouaazizi, Khalil Maalmi",
        "published": "2023-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.08.001"
    },
    {
        "id": 9458,
        "title": "Emo-Tts:Parallel Transformer-based Text-to-Speech Model with Emotional Awareness",
        "authors": "Mohamed Osman",
        "published": "2022-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icci54321.2022.9756092"
    },
    {
        "id": 9459,
        "title": "Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "Asmaa ELSAID, AMMAR MOHAMMED, Lamiaa Fattouh, MOHAMMED SAKRE",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2856782/v1"
    },
    {
        "id": 9460,
        "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
        "authors": "Sahar Abdelnabi, Mario Fritz",
        "published": "2021-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sp40001.2021.00083"
    },
    {
        "id": 9461,
        "title": "WITHDRAWN: A Transformer based approach using LSTM and Paraphrase reference to Translate English Text into Hindi",
        "authors": "Surbhi Sharma, Nisheeth Joshi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs many translation systems and applications, such as textual translation, speech systems, etc. use this approach very well with some constraints of grammatical accuracy and completeness, Machine translation is very old concept and work as an intermediary to perform cross-language communication in this age of the internet. Next, using SMT, these statements are translated into the target language without altering their original meaning Statistical Machine translation (SMT). In this paper, an English to Hindi Machine Translation system model was developed by employing the paraphrasing idea under the NMT tree (Neural Machine Translation). The metric scores produced by paraphrasing are closely like human utterances. In the proposed work, we develop a model that uses paraphrased references to convert plain English text into Hindi text. We will evaluate the translation's quality based on its sufficiency, fluency, and correspondence with human-predicted translation to determine how well this system replaces human expressions.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230686/v1"
    },
    {
        "id": 9462,
        "title": "Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR in Transfer Learning",
        "authors": "Zhiping Zeng, Van Tung Pham, Haihua Xu, Yerbolat Khassanov, Eng Siong Chng, Chongjia Ni, Bin Ma",
        "published": "2021-1-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscslp49672.2021.9362086"
    },
    {
        "id": 9463,
        "title": "Scene Text Image Super-Resolution Via Content Perceptual Loss and Criss-Cross Transformer Blocks",
        "authors": "Rui Qin, Yu-wing Tai, Bin Wang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4464486"
    },
    {
        "id": 9464,
        "title": "WITHDRAWN: A Transformer based approach using LSTM and Paraphrase reference to Translate English Text into Hindi",
        "authors": "Surbhi Sharma, Nisheeth Joshi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe full text of this preprint has been withdrawn, as it was submitted in error. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3230686/v2"
    },
    {
        "id": 9465,
        "title": "Confli-T5: An AutoPrompt Pipeline for Conflict Related Text Augmentation",
        "authors": "Erick Skorupa Parolin, Yibo Hu, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito D'Orazio",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020509"
    },
    {
        "id": 9466,
        "title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation",
        "authors": "Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh",
        "published": "2022",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.naacl-srw.18"
    },
    {
        "id": 9467,
        "title": "Text Detection of Transformer Based on Deep Learning Algorithm",
        "authors": "",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17559/tv-20211027110610"
    },
    {
        "id": 9468,
        "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs",
        "authors": "Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Xia Song, Furu Wei",
        "published": "2021",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.emnlp-main.125"
    },
    {
        "id": 9469,
        "title": "Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio",
        "authors": "Manuel Giollo, Deniz Gunceler, Yulan Liu, Daniel Willett",
        "published": "2021-8-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-198"
    },
    {
        "id": 9470,
        "title": "WATS-SMS: A T5-Based French Wikipedia Abstractive Text Summarizer for SMS",
        "authors": "Jean Louis Ebongue Kedieng Fendji, Désiré Manuel Taira, Marcellin Atemkeng, Adam Musa Ali",
        "published": "2021-9-18",
        "citations": 2,
        "abstract": "Text summarization remains a challenging task in the natural language processing field despite the plethora of applications in enterprises and daily life. One of the common use cases is the summarization of web pages which has the potential to provide an overview of web pages to devices with limited features. In fact, despite the increasing penetration rate of mobile devices in rural areas, the bulk of those devices offer limited features in addition to the fact that these areas are covered with limited connectivity such as the GSM network. Summarizing web pages into SMS becomes, therefore, an important task to provide information to limited devices. This work introduces WATS-SMS, a T5-based French Wikipedia Abstractive Text Summarizer for SMS. It is built through a transfer learning approach. The T5 English pre-trained model is used to generate a French text summarization model by retraining the model on 25,000 Wikipedia pages then compared with different approaches in the literature. The objective is twofold: (1) to check the assumption made in the literature that abstractive models provide better results compared to extractive ones; and (2) to evaluate the performance of our model compared to other existing abstractive models. A score based on ROUGE metrics gave us a value of 52% for articles with length up to 500 characters against 34.2% for transformer-ED and 12.7% for seq-2seq-attention; and a value of 77% for articles with larger size against 37% for transformers-DMCA. Moreover, an architecture including a software SMS-gateway has been developed to allow owners of mobile devices with limited features to send requests and to receive summaries through the GSM network.",
        "link": "http://dx.doi.org/10.3390/fi13090238"
    },
    {
        "id": 9471,
        "title": "Display-Semantic Transformer for Scene Text Recognition",
        "authors": "Xinqi Yang, Wushour Silamu, Miaomiao Xu, Yanbing Li",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "Linguistic knowledge helps a lot in scene text recognition by providing semantic information to refine the character sequence. The visual model only focuses on the visual texture of characters without actively learning linguistic information, which leads to poor model recognition rates in some noisy (distorted and blurry, etc.) images. In order to address the aforementioned issues, this study builds upon the most recent findings of the Vision Transformer, and our approach (called Display-Semantic Transformer, or DST for short) constructs a masked language model and a semantic visual interaction module. The model can mine deep semantic information from images to assist scene text recognition and improve the robustness of the model. The semantic visual interaction module can better realize the interaction between semantic information and visual features. In this way, the visual features can be enhanced by the semantic information so that the model can achieve a better recognition effect. The experimental results show that our model improves the average recognition accuracy on six benchmark test sets by nearly 2% compared to the baseline. Our model retains the benefits of having a small number of parameters and allows for fast inference speed. Additionally, it attains a more optimal balance between accuracy and speed.",
        "link": "http://dx.doi.org/10.3390/s23198159"
    },
    {
        "id": 9472,
        "title": "Investigation of text data augmentation for transformer training via translation technique",
        "authors": "Dominykas Šeputis",
        "published": "2021-5-14",
        "citations": 0,
        "abstract": "Data augmentation can improve model’s final accuracy by introducing new data samples to the dataset. In this paper, text data augmentation using translation technique is investigated. Synthetic translations, generated by Opus-MT model are compared to the unique foreign data samples in terms of an impact to the trans- former network-based models’ performance. The experimental results showed that multilingual models like DistilBERT in some cases benefit from the introduction of the addition artificially created data samples presented in a foreign language.",
        "link": "http://dx.doi.org/10.15388/lmitt.2021.11"
    },
    {
        "id": 9473,
        "title": "Chinese text sentiment analysis based on transformer model",
        "authors": "Huang Jing, Cai Yang",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwecai55315.2022.00043"
    },
    {
        "id": 9474,
        "title": "Transformer Based Language Identification for Malayalam-English Code-Mixed Text",
        "authors": "S. Thara, Prabaharan Poornachandran",
        "published": "2021",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3104106"
    },
    {
        "id": 9475,
        "title": "Text Graph Transformer for Document Classification",
        "authors": "Haopeng Zhang, Jiawei Zhang",
        "published": "2020",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.emnlp-main.668"
    },
    {
        "id": 9476,
        "title": "TransText: Improving scene text detection via transformer",
        "authors": "Jiajun Zhu, Guodong Wang",
        "published": "2022-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.dsp.2022.103698"
    },
    {
        "id": 9477,
        "title": "A Hybrid Arabic text summarization Approach based on Seq-to-seq and Transformer",
        "authors": "asmaa Elsaid, ammar mohamed, lamiaa Fattouh, mohamed sakre",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nText summarization is essential in natural language processing as the data volume increases quickly. Therefore, the user needs to summarize that data into a meaningful text in a short time. There are two common methods of text summarization: extractive and abstractive. There are many efforts to summarize Latin texts. However, summarizing Arabic texts is challenging for many reasons, including the language’s complexity, structure, and morphology. Also, there is a need for benchmark data sources and a gold standard Arabic evaluation metrics summary. Thus, the contribution of this paper is multi-fold: First, the paper proposes a hybrid approach consisting of a Modified Sequence-To-Sequence (MSTS) model and a transformer-based model. The seq-to-seq-based model is modified by adding multi-layer encoders and a one-layer decoder to its structure. The output of the MSTS model is the extractive summarization. To generate the abstractive summarization, the extractive summarization is manipulated by a transformer-based model. Second, it introduces a new Arabic benchmark dataset, called the HASD, which includes 43k articles with their extractive and abstractive summaries. Third, this work modifies the well-known extractive EASC benchmarks by adding to each text its abstractive summarization. Finally, this paper proposes a new measure called the Arabic-rouge measure for the abstractive summary depending on structure and similarity between words. The proposed method is tested using the proposed HASD and Modified EASC benchmarks and evaluated using Rouge, Bleu, and Arabic Rouge. The experimental results show satisfactory results compared to state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2672691/v1"
    },
    {
        "id": 9478,
        "title": "Federated Learning with Dynamic Transformer for Text to Speech",
        "authors": "Zhenhou Hong, Jianzong Wang, Xiaoyang Qu, Jie Liu, Chendong Zhao, Jing Xiao",
        "published": "2021-8-30",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-2039"
    },
    {
        "id": 9479,
        "title": "Extremely Low Resource Text simplification with Pre-trained Transformer Language Model",
        "authors": "Takumi Maruyama, Kazuhide Yamamoto",
        "published": "2019-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037650"
    },
    {
        "id": 9480,
        "title": "Improving Text Classification with Transformer",
        "authors": "Gokhan Soyalp, Artun Alar, Kaan Ozkanli, Beytullah Yildiz",
        "published": "2021-9-15",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk52708.2021.9558906"
    },
    {
        "id": 9481,
        "title": "Novel Approach Based on Transformer with Attention Mechanism for Text Summarization",
        "authors": "Mostafa Gamal, Hesham F. A. Hamed, Mustafa Abdul Salam, Sara Sweidan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4609431"
    },
    {
        "id": 9482,
        "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence",
        "authors": "Kelvin Lo, Yuan Jin, Weicong Tan, Ming Liu, Lan Du, Wray Buntine",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-emnlp.283"
    },
    {
        "id": 9483,
        "title": "Making Text",
        "authors": "Emma Walters",
        "published": "2017-2-14",
        "citations": 0,
        "abstract": "This chapter explores the act of making text. It looks at the author's own biography. Understanding one's contribution to both the textual and cultural milieu in which one situates themself provides concrete meaning to their existence in the space and time in which they are historically rooted. Of equal importance are questions on the very nature of how one's textual and cultural footprint impacts on the wider society they are in the process of creating. Making texts is a mode of reinforcing this underlying ideology, a strategy of facilitating, locating, constructing, and ultimately communicating or 'making' one's sense of self and place.",
        "link": "http://dx.doi.org/10.3828/liverpool/9781911325031.003.0006"
    }
]