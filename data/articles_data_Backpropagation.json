[
    {
        "id": 4501,
        "title": "Lernen mit dem Backpropagation-Algorithmus",
        "authors": "Martin Werner",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-22185-0_13"
    },
    {
        "id": 4502,
        "title": "Backpropagation and F-adjoint",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>This paper presents a concise mathematical framework for investigating both feed-forward and backward process, during the training to learn model weights, of an artificial neural network (ANN). Inspired from the idea of the two-step rule for backpropagation, we define a notion of F-adjoint which is aimed at a better description of the backpropagation algorithm. In particular, by introducing the notions of F-propagation and F-adjoint through a deep neural network architecture, the backpropagation associated to a cost/loss function is proven to be completely characterized by the F-adjoint of the corresponding F-propagation relatively to the partial derivative, with respect to the inputs, of the cost function.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22650193"
    },
    {
        "id": 4503,
        "title": "Backpropagation and F-adjoint",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a concise mathematical framework for investigating both feed-forward and backward process, during the training to learn model weights, of an artificial neural network (ANN). Inspired from the idea of the two-step rule for backpropagation, we define a notion of F-adjoint which is aimed at a better description of the backpropagation algorithm. In particular, by introducing the notions of F-propagation and F-adjoint through a deep neural network architecture, the backpropagation associated to a cost/loss function is proven to be completely characterized by the F-adjoint of the corresponding F-propagation relatively to the partial derivative, with respect to the inputs, of the cost function.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22650193.v1"
    },
    {
        "id": 4504,
        "title": "Design and Control of a Vertically Moving Base Inverted Pendulum using NARMA-L2 with Resilient backpropagation and Levenberg Marquardt backpropagation Training Algorithm",
        "authors": "Mustefa Jibril, Messay Tadese, Reta Degefa",
        "published": "No Date",
        "citations": 1,
        "abstract": "In this paper, a vertically moving base inverted pendulum control analysis has been done using Matlab/Simulink Toolbox. Because the vertically moving base inverted pendulum system is nonlinear and highly unstable, a feedback control system is used to make the system controlled and stable. A nonlinear autoregressive moving average L2 controller which is a family of Neural Network controller is used with Resilient backpropagation and Levenberg Marquardt backpropagation Training Algorithm to improve the stability of the pendulum. Comparison of the vertically moving base inverted pendulum using NARMA-L2 with Resilient backpropagation and Levenberg Marquardt backpropagation Training Algorithm for tracking a desired angular position of the system using a step and random input signals and a promising results have been obtained succesfully.",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.ppvnr50.v1"
    },
    {
        "id": 4505,
        "title": "Backpropagation (machine learning)",
        "authors": "Frank Gaillard, Matt Adams",
        "published": "2017-10-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-56164"
    },
    {
        "id": 4506,
        "title": "Acceleration of Backpropagation Training with Selective Momentum Term",
        "authors": "Diego Carvalho, Areolino Neto",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007272004430450"
    },
    {
        "id": 4507,
        "title": "Backpropagation Neural Network as Earthquake Early Warning\nTool using a new Elementary Modified Levenberg–Marquardt\nAlgorithm to minimise Backpropagation Errors",
        "authors": "Jyh-Woei Lin, Chun-Tang Chao, Juing-Shian Chiou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract. A new Elementary Modified Levenberg–Marquardt Algorithm (M-LMA) was used to minimise backpropagation errors in training a backpropagation neural network (BPNN) to predict the records related to the Chi-Chi earthquake from four seismic stations, Station-TAP003, Station-TAP005, Station-TCU084 and Station-TCU078, with the learning rates of 0.3, 0.05, 0.2 and 0.28, respectively. For these four recording stations, the M-LMA has been shown to produce smaller predicted errors compared to LMA. A sudden predicted error could be an indicator for Early Earthquake Warning (EEW), which indicated the initiation of strong motion due to large earthquakes. a trade-off decision-making process with BPNN (TDPB), using two alarms, adjusted the threshold of the magnitude of predicted error without a mistaken alarm. This approach was not necessary to consider the problems of characterising the wave phases and pre-processing, but did not require complex hardware; an existing seismic monitoring network-covered researched area was already sufficient for these purposes.\n                        ",
        "link": "http://dx.doi.org/10.5194/gi-2018-13"
    },
    {
        "id": 4508,
        "title": "FASFA: A Novel Next-Generation Backpropagation Optimizer",
        "authors": "Philip Naveen",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper introduces the fast adaptive stochastic function accelerator (FASFA) for gradient-based optimization of stochastic objective functions. It works based on Nesterov-enhanced first and second momentum estimates. The method is simple and effective during implementation because it has intuitive/familiar hyperparameterization. The training dynamics can be progressive or conservative depending on the decay rate sum. It works well with a low learning rate and mini batch size. Experiments and statistics showed convincing evidence that FASFA could be an ideal candidate for optimizing stochastic objective functions, particularly those generated by multilayer perceptrons with convolution and dropout layers. In addition, the convergence properties and regret bound provide results aligning with the online convex optimization framework. In a first of its kind, FASFA addresses the growing need for diverse optimizers by pro-viding next-generation training dynamics for artificial intelligence algorithms. Future experiments could modify FASFA based on the infinity norm. FASFA: A Novel Next-Generation Backpropagation Optimizer</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20427852.v1"
    },
    {
        "id": 4509,
        "title": "FASFA: A Novel Next-Generation Backpropagation Optimizer",
        "authors": "Philip Naveen",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper introduces the fast adaptive stochastic function accelerator (FASFA) for gradient-based optimization of stochastic objective functions. It works based on Nesterov-enhanced first and second momentum estimates. The method is simple and effective during implementation because it has intuitive/familiar hyperparameterization. The training dynamics can be progressive or conservative depending on the decay rate sum. It works well with a low learning rate and mini batch size. Experiments and statistics showed convincing evidence that FASFA could be an ideal candidate for optimizing stochastic objective functions, particularly those generated by multilayer perceptrons with convolution and dropout layers. In addition, the convergence properties and regret bound provide results aligning with the online convex optimization framework. In a first of its kind, FASFA addresses the growing need for diverse optimizers by pro-viding next-generation training dynamics for artificial intelligence algorithms. Future experiments could modify FASFA based on the infinity norm. FASFA: A Novel Next-Generation Backpropagation Optimizer</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20427852"
    },
    {
        "id": 4510,
        "title": "Forward and Backpropagation",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.004"
    },
    {
        "id": 4511,
        "title": "Prediksi Rentet Waktu Penjualan Barang Menggunakan Algoritma Backpropagation",
        "authors": " Budi Mulyono,  Nursalim",
        "published": "2023-2-13",
        "citations": 0,
        "abstract": "Data transaksi penjualan merupakan informasi yang sangat penting untuk menentukan strategi pemasaran dan juga untuk menentukan ketersediaan barang. Ketersediaan barang yang dijual merupakan bagian dari pelayanan pelanggan, kurangnya ketersediaan barang akan mengganggu proses pelayanan dan penjualan serta akan mengurangi keuntungan. Dalam menentukan ketersediaan barang sangat penting untuk mengetahui berapa jumlah barang yang akan terjual pada periode berikutnya. Untuk mengetahui angka penjualan periode selanjutnya perlu dilakukan prediksi berdasarkan data transaksi yang terjadi. Penelitian ini menggunakan data time series 10 kumpulan data barang berdasarkan jumlah penjualan tertinggi dan dilakukan dengan menggunakan analisis Neural Network dengan struktur algoritma backpropagation, dimana Neural Network sering digunakan untuk memprediksi sedangkan algoritma backpropagation memiliki akurasi yang tinggi dalam prediksi. Hasil Aplikasi Neural Network dengan Algoritma Backpropagation Arsitektur Indomie Goreng Spesial MN (28-18-1) dan MSE 0.002772, Arsitektur Gula Lokal 1 Kg (23-18-i) dan MSE 65.503470, Arsitektur Telur Ayam Ras (19-25 -1) dan MSE 1.952032, arsitektur Teh Kotak Ultra 200 Extra 50 (21-21-i) dan MSE 0.672921, arsitektur Indomie Sedap Soto WF (22-18-1) dan MSE 0.173779, arsitektur Air Mineral Vitro 240 Ml (25- 19-1) dan UMK 10.626992, Arsitektur Rokok Sampoerna 16 (21-18-1) dan UMK 0.375329, Arsitektur Indomie Sedap Goreng WF (2 7-21-1) dan UMK 5.228250, Arsitektur SKM Bendera Kaleng Putih 385 Kg (16- 18-1) dan UMK 5.178826, Arsitektur Indomie Kari Ayam (17-19-1) dan UMK 7.01215.",
        "link": "http://dx.doi.org/10.56338/jks.v6i2.3300"
    },
    {
        "id": 4512,
        "title": "Deep Networks and Backpropagation",
        "authors": "",
        "published": "2021-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108955652.007"
    },
    {
        "id": 4513,
        "title": "Backpropagation and F-adjoint",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper presents a concise mathematical framework for investigating both feed-forward and backward process, during the training to learn model weights, of an artificial neural network (ANN). Inspired from the idea of the two-step rule for backpropagation, we define a notion of F_adjoint which is aimed at a better description of the backpropagation algorithm. In particular, by introducing the notions of F-propagation and F-adjoint through a deep neural network architecture, the backpropagation associated to a cost/loss function is proven to be completely characterized by the F-adjoint of the corresponding F-propagation relatively to the partial derivative, with respect to the inputs, of the cost function.",
        "link": "http://dx.doi.org/10.20944/preprints202304.1046.v1"
    },
    {
        "id": 4514,
        "title": "A Two-Step Rule for Backpropagation",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a simplified computational rule for the back-propagation formulas for artificial neural networks. In this work, we provide a generic two-step rule for the back-propagation algorithm in matrix notation. Moreover, this rule incorporates both the forward and backward phases of the computations involved in the learning process. Specifically, this recursively computing rule permits the propagation of the changes to all synaptic weights in the network, layer by layer, efficiently. In particular, we use this rule to compute both the up and down partial derivatives of the cost function of all the connections feeding into the output layer. ",
        "link": "http://dx.doi.org/10.20944/preprints202303.0001.v3"
    },
    {
        "id": 4515,
        "title": "A Two-Step Rule for Backpropagation",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a simplified computational rule for the back-propagation formulas for artificial neural networks. In this work, we provide a generic two-step rule for the back-propagation algorithm in matrix notation. Moreover, this rule incorporates both the forward and backward phases of the computations involved in the learning process. Specifically, this recursively computing rule permits the propagation of the changes to all synaptic weights in the network, layer by layer, efficiently. In particular, we use this rule to compute both the up and down partial derivatives of the cost function of all the connections feeding into the output layer. ",
        "link": "http://dx.doi.org/10.20944/preprints202303.0001.v2"
    },
    {
        "id": 4516,
        "title": "Symmetric Encoding-decoding Framework without Backpropagation",
        "authors": "Pengyuan Zhai",
        "published": "No Date",
        "citations": 0,
        "abstract": "We propose a forward-only multi-layer encoding-decoding framework based on the principle of Maximal Coding Rate Reduction (MCR$^2$), an information-theoretic metric that measures a statistical distance between two sets of feature vectors up to the second moment. The encoder directly transforms data vectors themselves via gradient ascent to maximize the MCR$^2$ distance between different classes in the feature space, resulting in class-wise mutually orthogonal subspace representations. The decoder follows a process symmetric to the encoder, and transforms the subspace feature vectors via gradient descent to minimize the MCR$^2$ distance between the reconstructed data and the original data. We show that the encoder transforms data to linear discriminative representations without breaking the higher-order manifolds, and the decoder reconstructs the data with high fidelity.",
        "link": "http://dx.doi.org/10.20944/preprints202211.0537.v2"
    },
    {
        "id": 4517,
        "title": "A Two-Step Rule for Backpropagation",
        "authors": "Ahmed Boughammoura",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a simplified computational rule for the back-propagation formulas for artificial neural networks. In this work, we provide a generic two-step rule for the back-propagation algorithm in matrix notation. Moreover, this rule incorporates both the forward and backward phases of the computations involved in the learning process. Specifically, this recursively computing rule permits the propagation of the changes to all synaptic weights in the network, layer by layer, efficiently. In particular, we use this rule to compute both the up and down partial derivatives of the cost function of all the connections feeding into the output layer. ",
        "link": "http://dx.doi.org/10.20944/preprints202303.0001.v1"
    },
    {
        "id": 4518,
        "title": "Sistem Pengenalan Nomor Pelat Kendaraan Menggunakan Jaringan Syaraf Tiruan Backpropagation",
        "authors": "Afifatul Mukaroh, Priyo Sidik Sasongko",
        "published": "2020-11-17",
        "citations": 0,
        "abstract": "Dalam pengawasan lalu lintas, pengenalan nomor pelat kendaraan menjadi penting untuk dilakukan. Hal ini dikarenakan pengenalan nomor pelat kendaraan memiliki banyak tujuan seperti identifikasi kendaraan curian, identifikasi kendaraan yang melanggar tata tertib, manajemen perparkiran, pengecekan keluar masuk kendaraan, dan lain sebagainya. Sayangnya jumlah kendaraan yang semakin tinggi membuat hal ini tidak bisa lagi dilakukan secara manual. Maka dari itu dibutuhkan sebuah machine vision yang dapat mengenali nomor pelat kendaraan dengan cepat dan akurasi yang tinggi. Pada penelitian ini dikembangkan sistem pengenalan nomor pelat kendaraan menggunakan jaringan syaraf tiruan backpropagation (JST BP). Sistem ini mencari model terbaik JST BP yang mampu melakukan pengenalan karakter-karakter pada pelat dengan waktu tercepat namun dengan akurasi yang tetap tinggi. Dari model terbaik yang didapatkan, sistem pengenalan nomor pelat kendaraan ini memiliki persentase keberhasilan sebesar  99.80% pada data pelat yang digunakan sebagai pembentuk model dan sebesar 96.10% pada data verifikasi.",
        "link": "http://dx.doi.org/10.14710/jmasif.11.2.34870"
    },
    {
        "id": 4519,
        "title": "Multilayer nets and backpropagation",
        "authors": "",
        "published": "2018-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315273570-11"
    },
    {
        "id": 4520,
        "title": "Symmetric Encoding-decoding Framework without Backpropagation",
        "authors": "Pengyuan Zhai",
        "published": "No Date",
        "citations": 0,
        "abstract": "We propose a forward-only multi-layer encoding-decoding framework based on the principle of Maximal Coding Rate Reduction (MCR$^2$), an information-theoretic metric that measures a statistical distance between two sets of feature vectors up to the second moment. The encoder directly transforms data vectors themselves via gradient ascent to maximize the MCR$^2$ distance between different classes in the feature space, resulting in class-wise mutually orthogonal subspace representations. The decoder follows a process symmetric to the encoder, and transforms the subspace feature vectors via gradient descent to minimize the MCR$^2$ distance between the reconstructed data and the original data. We show that the encoder transforms data to linear discriminative representations without breaking the higher-order manifolds, and the decoder reconstructs the data with high fidelity.",
        "link": "http://dx.doi.org/10.20944/preprints202211.0537.v1"
    },
    {
        "id": 4521,
        "title": "Comparison of Optimization Using Hybrid Genetic Agorithm-Backpropagation and Hybrid Particle Swarm Optimization-Backpropagation for Tide Level Forecasting",
        "authors": "N Nikentari, H Kurniawan",
        "published": "2019-11-1",
        "citations": 0,
        "abstract": "Abstract\nEvolutionary computation or evolutionary algorithm has been used in many areas. For the last ten years, evolutionary computation became a powerful method to solve problems in the real world. Forecasting is a well-known method to determine the direction of the future for better results. Hybridization between the Genetic Algorithm and Particle Swarm Optimization to Backpropagation Neural Network are applied to forecast tide level data. The experiments based on a comparison of these two algorithms prove that Particle Swarm Optimization with Backpropagation Neural Network is exceeding Genetic Algorithm in measuring tide level forecasting.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1376/1/012028"
    },
    {
        "id": 4522,
        "title": "Backpropagation",
        "authors": "Paul Munro",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_51"
    },
    {
        "id": 4523,
        "title": "Backpropagation Neural Network Levenberg-Marquardt Method in Predicting Lung Cancer in Smokers",
        "authors": "Muhammad Iqbal, Muhammad Rafai, . Solikhun",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012448200003848"
    },
    {
        "id": 4524,
        "title": "Levenberg Marquardt Backpropagation Algorithm in Predicting Potential Mortality in Heart Failure",
        "authors": "Verdi Yasin, Selli Oktaviani, Muryan Awaludin, Ifan Junaedi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012444100003848"
    },
    {
        "id": 4525,
        "title": "Backpropagation-Based Recollection of Memories: Biological Plausibility and Computational Efficiency",
        "authors": "Zied Ben Houidi",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSince the advent of the neuron doctrine more than a century ago, information processing in the brain is widely believed to follow the forward pre to post-synaptic neurons direction. Challenging this view, we introduce thebackpropagation-based recollectionhypothesis as follows:Cue-based memory recollection occurs when backpropagated Action Potentials (APs), originating in sparse neurons that uniquely activate in response to a specific trace being recalled (e.g. image of a cat), travel backwards. The resulting transient backpropagating currents follow the available open backward and lateral pathways, guided by synaptic weights or couplings. In doing so, they stimulate the same neurons that fired during the very first perception and subsequent encoding, effectively allowing a “replay” of the experience (e.g., recalling the image of the cat).This process is pervasive, seen in tasks like cue-based attention, imagination, future episodic thinking, modality-specific language understanding, and naming.After detailing our hypothesis, we challenge it against a thorough literature review, finding compelling evidence supporting our claims. We further found that gap junctions could be a plausible medium for such currents, and that cholinergic modulation, which is known to favour backpropagated APs and is crucial for memory, is a reasonable candidate trigger for the entire process. We then leverage computer simulations to demonstrate the computational efficiency of the backpropagation-based recollection principle in (i) reconstructing an image, backwards, starting from its forward-pass sparse activations and (ii) successfully naming an object with a comparable high accuracy as a state of the art machine learning classifier. Given the converging evidence and the hypothesis’s critical role in cognition, this paradigm shift warrants broader attention: it opens the way, among others, to novel interpretations of language acquisition and understanding, the interplay between memory encoding and retrieval, as well as reconciling the apparently opposed views between sparse coding and distributed representations, crucial for developing a theory of consciousness and the mind.Significance StatementTry to mentally picture the image of a cat. In this process, the word “cat” acted as a cue, and the fragile and non-persistent retrieved mental image is a recollected memory. Similar cue-based generative activities are ubiquitous in our lives, yet the underlying neural mechanisms are still a mystery. Neuroimaging and optogenetic-based studies suggest that cue-based recollection of memories involve the reactivation of the same neural ensembles which were active during perception (encoding). However, the exact neural mechanisms that mediate such reactivation remain unknown. We elaborate a novel hypothesis explaining how this can be implemented at single neurons: we hypothesize that the very same neural pathways used for perception are used backwards for recall, thus creating similar impressions during retrieval.",
        "link": "http://dx.doi.org/10.1101/2024.02.05.578854"
    },
    {
        "id": 4526,
        "title": "Pengenalan Digit 0 Sampai 9 Menggunakan Ekstraksi Ciri MFCC dan Jaringan Syaraf Tiruan Backpropagation",
        "authors": "Sitti Amalia",
        "published": "2017-1-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21063/jte.2017.3133601"
    },
    {
        "id": 4527,
        "title": "Analisa Kelayakan Pemberian Kredit Mobil Dengan Menggunakan Neural Network Backpropagation",
        "authors": "Amrin Amrin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Problems are often encountered in the provision of credit is to determine lendingdecisions to someone, while other issues are not all credit payments can run well.Among the causes are errors of judgment in making credit decisions. In this studywill be used back propagation neural network method to analyze the feasibility ofproviding car loans. From the test results to measure the performance of themethod is to use testing methods Confusion Matrix and ROC curve, it is knownthat the method ofback propagation neural network has a value of89% accuracyand AUC value of 0.831. This shows that the model produced, including theclassification is quite good because it has the AUC values between 0.8-0.9.",
        "link": "http://dx.doi.org/10.31227/osf.io/vczfp"
    },
    {
        "id": 4528,
        "title": "Backpropagation artificial neural network learning algorithm process impact based on hyperparameters",
        "authors": "Oleksandr Bilokon, Ivan Denkov",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The problem of computing machine passing the maze is one of theoretical computer science key tasks. This task was partially considered by the classics of computer science, e.g. A. Turing, C. Shannon, L. Budach and Z. Pawlak. Labyrinth problem solving includes fundamental knowledges in the main three branches, namely  environment (labyrinth) knowledge, computing machines features knowledge and behavior of computing machines in labyrinth knowledge. These three components are built on the fundamental basis of the theoretical computer science. Nowadays the labyrinth passing task is complemented by intellectuality as an additional criterion. That is, computing machines must acquire intelligent functions, and this task gets a new component and a new statement, which is briefly formulated as the search for a way out of the labyrinth by intelligent computing machines. The main concept of this article is to develop artificial neural networks based intelligent functions for calculating machine to pass the maze. Besides this visual aspect is emphasized, i.e. the computational machine includes a maze viewing function. Authors accentuate fundamental principles of artificial neural networks technologies building  and engage backpropagation algorithm, which is used in a artificial neural network learning process. The article discusses maze, dataset construction, artificial neural network training, maze recognition computational experiment and analysis of the hyperparameters effect on artificial neural network training.</p>\n<p>The aim of the article is to build mathematical model based on backpropagation algorithm and to identify how hyperparameters affect the learning process of artificial neural networks. Thus the research is based on the methods of computational experiment and step-by-step detailing to implement the algorithm. The results of the research include theoretical basis of computing machines intellectualization for the maze and computing machine ability to recognize the maze. These results may be useful to theorists for a detailed description of the process and to practitioners for ability to test this algorithm and approach while solving a problem of maze passing by intelligent agents.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24282511.v1"
    },
    {
        "id": 4529,
        "title": "Simple image deconvolution based on reverse image convolution and backpropagation algorithm",
        "authors": "Loc Nguyen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeconvolution task is not important in convolutional neural network (CNN) because it is not imperative to recover convoluted image when convolutional layer is important to extract features. However, the deconvolution task is useful in some cases of inspecting and reflecting a convolutional filter as well as trying to improve a generated image when information loss is not serious with regard to trade-off of information loss and specific features such as edge detection and sharpening. This research proposes a duplicated and reverse process of recovering a filtered image. Firstly, source layer and target layer are reversed in accordance with traditional image convolution so as to train the convolutional filter. Secondly, the trained filter is reversed again to derive a deconvolutional operator for recovering the filtered image. The reverse process is associated with backpropagation algorithm which is most popular in learning neural network. Experimental results show that the proposed technique in this research is better to learn the filters that focus on discovering pixel differences. Therefore, the main contribution of this research is to inspect convolutional filters from data.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3247106/v1"
    },
    {
        "id": 4530,
        "title": "Improved Webpage Classification Technology Based on Feedforward Backpropagation Neural Network",
        "authors": "Ruihui Mu, Xiaoqin Zeng",
        "published": "2018-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7546/crabs.2018.09.11"
    },
    {
        "id": 4531,
        "title": "Backpropagation artificial neural network learning algorithm process impact based on hyperparameters",
        "authors": "Oleksandr Bilokon, Ivan Denkov",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The problem of computing machine passing the maze is one of theoretical computer science key tasks. This task was partially considered by the classics of computer science, e.g. A. Turing, C. Shannon, L. Budach and Z. Pawlak. Labyrinth problem solving includes fundamental knowledges in the main three branches, namely  environment (labyrinth) knowledge, computing machines features knowledge and behavior of computing machines in labyrinth knowledge. These three components are built on the fundamental basis of the theoretical computer science. Nowadays the labyrinth passing task is complemented by intellectuality as an additional criterion. That is, computing machines must acquire intelligent functions, and this task gets a new component and a new statement, which is briefly formulated as the search for a way out of the labyrinth by intelligent computing machines. The main concept of this article is to develop artificial neural networks based intelligent functions for calculating machine to pass the maze. Besides this visual aspect is emphasized, i.e. the computational machine includes a maze viewing function. Authors accentuate fundamental principles of artificial neural networks technologies building  and engage backpropagation algorithm, which is used in a artificial neural network learning process. The article discusses maze, dataset construction, artificial neural network training, maze recognition computational experiment and analysis of the hyperparameters effect on artificial neural network training.</p>\n<p>The aim of the article is to build mathematical model based on backpropagation algorithm and to identify how hyperparameters affect the learning process of artificial neural networks. Thus the research is based on the methods of computational experiment and step-by-step detailing to implement the algorithm. The results of the research include theoretical basis of computing machines intellectualization for the maze and computing machine ability to recognize the maze. These results may be useful to theorists for a detailed description of the process and to practitioners for ability to test this algorithm and approach while solving a problem of maze passing by intelligent agents.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24282511"
    },
    {
        "id": 4532,
        "title": "Penerapan Artificial Neural Network Terhadap Identifikasi Wajah Menggunakan Metode Backpropagation",
        "authors": "Mimin Hendriani,  Rais, Lilies Handayani",
        "published": "2019-12-30",
        "citations": 0,
        "abstract": "Backpropagation is one of the supervised training methods that causes an error in the output produced. Backpropagation neural networks will be carried out in 3 stages, namely feedforward from input training patterns, backpropagation from errors related to adjustment of weights. Updating the weight is done when the training results obtained have not been converged. The value of the goal error (MSE) is 0.0070579 which is achieved at epochs to 99994 from the provisions of 100000 iterations. Based on the plot regression, the training data resulted in a correlation coefficient value of up to 0.55321. The correlation coefficient value is concluded that the greater the R value produced, the better the level of accuracy in face identification carried out in this study",
        "link": "http://dx.doi.org/10.22487/25411969.2019.v8.i3.14599"
    },
    {
        "id": 4533,
        "title": "On improving Cheng’s backpropagation for fully connectedcascade networks",
        "authors": "Eiji Mizutani, Naoyuki Kubota, Tam Truong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this journal, Cheng has proposed a backpropagation (BP) procedure calledBPFCC for deep L-stage fully connected cascaded (FCC) neural network learning in comparisonwith a neuron-by-neuron (NBN) algorithm of Wilamowski and Yu; yet, Cheng apparentlyneglected two major points of the NBN algorithm: (1) the merit of NBN is emphasizedon the multiple q-output (q > 1) case; and (2) NBN is described for a q-outputFCC different in structure from Cheng’s FCC type. In nonlinear regression (for minimizingthe sum of squared residuals), NBN employs forward passes only on the q-output FCC toevaluate the Gauss-Newton (approximate Hessian) matrix ∇rT∇r, the cross product of theJacobian matrix ∇r of the residual vector r. Notably, both BPFCC and NBN are designedto reduce the cost for evaluating (q rows per datum of) the Jacobian matrix ∇r rather thanthe dominant cost for forming the cross product ∇rT∇r by rank updates. The purpose ofthis paper is to present a new BP procedure that exploits the special FCC network structureof Cheng’s type for reducing the dominant cost (with no rows of ∇r explicitly), evaluatingthe Gauss-Newton matrix efficiently in a block arrow matrix form when q > 1.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1872159/v1"
    },
    {
        "id": 4534,
        "title": "Backpropagation neural network as earthquake early warning tool using a new modified elementary Levenberg–Marquardt Algorithm to minimise backpropagation errors",
        "authors": "Jyh-Woei Lin, Chun-Tang Chao, Juing-Shian Chiou",
        "published": "2018-8-16",
        "citations": 2,
        "abstract": "Abstract. A new modified elementary Levenberg–Marquardt Algorithm (M-LMA) was used to minimise\nbackpropagation errors in training a backpropagation neural network (BPNN) to\npredict the records related to the Chi-Chi earthquake from four seismic\nstations: Station-TAP003, Station-TAP005, Station-TCU084, and Station-TCU078\nbelonging to the Free Field Strong Earthquake Observation Network, with the\nlearning rates of 0.3, 0.05, 0.2, and 0.28, respectively. For these four\nrecording stations, the M-LMA has been shown to produce smaller predicted\nerrors compared to the Levenberg–Marquardt Algorithm (LMA). A sudden predicted\nerror could be an indicator for Early Earthquake Warning (EEW), which\nindicated the initiation of strong motion due to large earthquakes. A\ntrade-Off decision-making process with BPNN (TDPB), using two alarms,\nadjusted the threshold of the magnitude of predicted error without a mistaken\nalarm. With this approach, it is unnecessary to consider the problems of\ncharacterising the wave phases and pre-processing, and does not require\ncomplex hardware; an existing seismic monitoring network-covered research\narea was already sufficient for these purposes.\n                    ",
        "link": "http://dx.doi.org/10.5194/gi-7-235-2018"
    },
    {
        "id": 4535,
        "title": "The Shallow Gibbs Network, Double Backpropagation and Differential Machine learning",
        "authors": "Alejandro Murua, Nonvikan Karl-Augustt ALAHASSA",
        "published": "No Date",
        "citations": 0,
        "abstract": "We have built a Shallow Gibbs Network model as a Random Gibbs Network Forest to reach the performance of the Multilayer feedforward Neural Network in a few numbers of parameters, and fewer backpropagation iterations. To make it happens, we propose a novel optimization framework for our Bayesian Shallow Network, called the {Double Backpropagation Scheme} (DBS) that can also fit perfectly the data with appropriate learning rate, and which is convergent and universally applicable to any Bayesian neural network problem. The contribution of this model is broad. First, it integrates all the advantages of the Potts Model, which is a very rich random partitions model, that we have also modified to propose its Complete Shrinkage version using agglomerative clustering techniques. The model takes also an advantage of Gibbs Fields for its weights precision matrix structure, mainly through Markov Random Fields, and even has five (5) variants structures at the end: the Full-Gibbs, the Sparse-Gibbs, the Between layer Sparse Gibbs which is the B-Sparse Gibbs in a short, the Compound Symmetry Gibbs (CS-Gibbs in short), and the Sparse Compound Symmetry Gibbs (Sparse-CS-Gibbs) model. The Full-Gibbs is mainly to remind fully-connected models, and the other structures are useful to show how the model can be reduced in terms of complexity with sparsity and parsimony. All those models have been experimented with the Mulan project multivariate regression dataset, and the results arouse interest in those structures, in a sense that different structures help to reach different results in terms of Mean Squared Error (MSE) and Relative Root Mean Squared Error (RRMSE). For the Shallow Gibbs Network model, we have found the perfect learning framework : it is the $(l_1, \\boldsymbol{\\zeta}, \\epsilon_{dbs})-\\textbf{DBS}$ configuration, which is a combination of the \\emph{Universal Approximation Theorem}, and the DBS optimization, coupled with the (\\emph{dist})-Nearest Neighbor-(h)-Taylor Series-Perfect Multivariate Interpolation (\\emph{dist}-NN-(h)-TS-PMI) model [which in turn is a combination of the research of the Nearest Neighborhood for a good Train-Test association, the Taylor Approximation Theorem, and finally the Multivariate Interpolation Method]. It indicates that, with an appropriate number $l_1$ of neurons on the hidden layer, an optimal number $\\zeta$ of DBS updates, an optimal DBS learnnig rate $\\epsilon_{dbs}$, an optimal distance \\emph{dist}$_{opt}$ in the research of the nearest neighbor in the training dataset for each test data $x_i^{\\mbox{test}}$, an optimal order $h_{opt}$ of the Taylor approximation for the Perfect Multivariate Interpolation (\\emph{dist}-NN-(h)-TS-PMI) model once the {\\bfseries DBS} has overfitted the training dataset, the train and the test error converge to zero (0).",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.pps25dj.v2"
    },
    {
        "id": 4536,
        "title": "9 Backpropagation",
        "authors": "",
        "published": "2023-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783111025551-009"
    },
    {
        "id": 4537,
        "title": "Backpropagation for Parametric STL",
        "authors": "Karen Leung, Nikos Arechiga, Marco Pavone",
        "published": "2019-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2019.8814167"
    },
    {
        "id": 4538,
        "title": "Backpropagation",
        "authors": "Robert H. Chen, Chelsea Chen",
        "published": "2022-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003214892-21"
    },
    {
        "id": 4539,
        "title": "Effect of Hyperparameters on Backpropagation",
        "authors": "Aaditree Jaisswal, Anjali Naik",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/punecon52575.2021.9686489"
    },
    {
        "id": 4540,
        "title": "Identifikasi Penyakit Diabetes Millitus Menggunakan Jaringan Syaraf Tiruan Dengan Metode Perambatan-Balik (Backpropagation)",
        "authors": "Sutedi Sutedi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Diabetes Melitus (DM) is dangerous disease that affect many of the variouslayer of work society. This disease is not easy to accurately recognized by thegeneral society. So we need to develop a system that can identify accurately. Systemis built using neural networks with backpropagation methods and the functionactivation sigmoid. Neural network architecture using 8 input layer, 2 output layerand 5 hidden layer. The results show that this methods succesfully clasifies datadiabetics and non diabetics with near 100% accuracy rate.",
        "link": "http://dx.doi.org/10.31224/osf.io/bgs42"
    },
    {
        "id": 4541,
        "title": "Bayesian Bidirectional Backpropagation Learning",
        "authors": "Olaoluwa Adigun, Bart Kosko",
        "published": "2021-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533873"
    },
    {
        "id": 4542,
        "title": "Peramalan Tingkat Inflasi Indonesia  Menggunakan Neural  Network Backpropagation Berbasis Metode Time Series",
        "authors": "Amrin Amrin",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this study will be used back propagation neural network method to predict themonthly inflation rate in Indonesia. In the results of the data analysis is concludedthat the  performance of  back  propagation  neural network   that  formed by thetraining data and validated by  testing data generates prediction accuracy rate  isvery good with a mean square error (MSE) is 0.0171. By using a moving averageto forecast the independent variables obtained the rate of inflation in the month ofJuly  2014  is  0.514,  by using  exponential  smoothing  to  forecast  the independentvariables obtained by  the rate of inflation  in  the month of July 2014 is 0.45, andby using  seasonal method  to  forecast  the independent variables obtained by  therate of inflation in the month of July 2014 is 0.93.",
        "link": "http://dx.doi.org/10.31227/osf.io/7hsp2"
    },
    {
        "id": 4543,
        "title": "Noise-boosted recurrent backpropagation",
        "authors": "Olaoluwa Adigun, Bart Kosko",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126438"
    },
    {
        "id": 4544,
        "title": "The Shallow Gibbs Network, Double Backpropagation and Differential Machine learning",
        "authors": "Nonvikan Karl-Augustt ALAHASSA, alejandro Murua",
        "published": "No Date",
        "citations": 0,
        "abstract": "We have built a Shallow Gibbs Network model as a Random Gibbs Network Forest to reach the performance of the Multilayer feedforward Neural Network in a few numbers of parameters, and fewer backpropagation iterations. To make it happens, we propose a novel optimization framework for our Bayesian Shallow Network, called the {Double Backpropagation Scheme} (DBS) that can also fit perfectly the data with appropriate learning rate, and which is convergent and universally applicable to any Bayesian neural network problem. The contribution of this model is broad. First, it integrates all the advantages of the Potts Model, which is a very rich random partitions model, that we have also modified to propose its Complete Shrinkage version using agglomerative clustering techniques. The model takes also an advantage of Gibbs Fields for its weights precision matrix structure, mainly through Markov Random Fields, and even has five (5) variants structures at the end: the Full-Gibbs, the Sparse-Gibbs, the Between layer Sparse Gibbs which is the B-Sparse Gibbs in a short, the Compound Symmetry Gibbs (CS-Gibbs in short), and the Sparse Compound Symmetry Gibbs (Sparse-CS-Gibbs) model. The Full-Gibbs is mainly to remind fully-connected models, and the other structures are useful to show how the model can be reduced in terms of complexity with sparsity and parsimony. All those models have been experimented with the Mulan project multivariate regression dataset, and the results arouse interest in those structures, in a sense that different structures help to reach different results in terms of Mean Squared Error (MSE) and Relative Root Mean Squared Error (RRMSE). For the Shallow Gibbs Network model, we have found the perfect learning framework : it is the $(l_1, \\boldsymbol{\\zeta}, \\epsilon_{dbs})-\\textbf{DBS}$ configuration, which is a combination of the \\emph{Universal Approximation Theorem}, and the DBS optimization, coupled with the (\\emph{dist})-Nearest Neighbor-(h)-Taylor Series-Perfect Multivariate Interpolation (\\emph{dist}-NN-(h)-TS-PMI) model [which in turn is a combination of the research of the Nearest Neighborhood for a good Train-Test association, the Taylor Approximation Theorem, and finally the Multivariate Interpolation Method]. It indicates that, with an appropriate number $l_1$ of neurons on the hidden layer, an optimal number $\\zeta$ of DBS updates, an optimal DBS learnnig rate $\\epsilon_{dbs}$, an optimal distance \\emph{dist}$_{opt}$ in the research of the nearest neighbor in the training dataset for each test data $x_i^{\\mbox{test}}$, an optimal order $h_{opt}$ of the Taylor approximation for the Perfect Multivariate Interpolation (\\emph{dist}-NN-(h)-TS-PMI) model once the {\\bfseries DBS} has overfitted the training dataset, the train and the test error converge to zero (0). \n   ",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.pps25dj.v1"
    },
    {
        "id": 4545,
        "title": "IDENTIFICATION OF INDONESIAN PHONEMES IN VIDEOS USING A NEURAL NETWORK BACKPROPAGATION",
        "authors": " Aripin, Nadila Ekasiti Kamarukmi, Adelia Nadyalishandi Nugroho, Abdul Ghufron, Kristianus Lajang",
        "published": "2022-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3568231.3568241"
    },
    {
        "id": 4546,
        "title": "Backpropagation",
        "authors": "Christopher M. Bishop, Hugh Bishop",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45468-4_8"
    },
    {
        "id": 4547,
        "title": "Backpropagation-Based Recollection of Memories: Biological Plausibility and Computational Efficiency",
        "authors": "Zied Ben Houidi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Since the advent of the neuron doctrine more than a century ago, information processing in the brain is widely believed to follow the forward pre to post-synaptic neurons direction. Challenging this view, we introduce the backpropagation-based recollection hypothesis as follows: Cue-based memory recollection occurs when backpropagated Action Potentials (APs), originating in sparse neurons that uniquely activate in response to a specific trace being recalled (e.g. image of a cat), travel backwards. The resulting transient backpropagating currents follow the available open backward and lateral pathways, guided by synaptic weights or couplings. In doing so, they stimulate the same neurons that fired during the very first perception and subsequent encoding, effectively allowing a ''replay'' of the experience (e.g., recalling the image of the cat). This process is pervasive, seen in tasks like cue-based attention, imagination, future episodic thinking, modality-specific language understanding, and naming.After detailing our hypothesis, we challenge it against a thorough literature review, finding compelling evidence supporting our claims. We further found that gap junctions could be a plausible medium for such currents, and that cholinergic modulation, which is known to favor backpropagated APs and is crucial for memory, is a reasonable candidate trigger for the entire process. We then leverage computer simulations to demonstrate the computational efficiency of the backpropagation-based recollection principle in (i) reconstructing an image, backwards, starting from its forward-pass sparse activations and (ii) successfully naming an object with a comparable high accuracy as a state-of-the-art machine learning classifier. Given the substantial converging evidence and the hypothesis's critical role in cognition, we believe this paradigm shift warrants broader attention.",
        "link": "http://dx.doi.org/10.31219/osf.io/8rmhf"
    },
    {
        "id": 4548,
        "title": "Replay as a Basis for Backpropagation Through Time in the Brain",
        "authors": "Huzi Cheng, Joshua Brown",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4398262"
    },
    {
        "id": 4549,
        "title": "A Comparison of Quaternion Neural Network Backpropagation Algorithms",
        "authors": "Jeremiah Bill, Bruce  A. Cox, Lance  E. Champagne",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4348795"
    },
    {
        "id": 4550,
        "title": "The Backpropagation Algorithm",
        "authors": "Charu Aggarwal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29642-0_2"
    },
    {
        "id": 4551,
        "title": "COMBINATION OF SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE (SMOTE) AND BACKPROPAGATION NEURAL NETWORK TO CONTRACEPTIVE IUD PREDICTION",
        "authors": "Mustaqim Mustaqim, Budi Warsito, Bayu Surarso",
        "published": "2020-6-26",
        "citations": 0,
        "abstract": "Data imbalance occurs when the amount of data in a class is more than other data. The majority class is more data, while the minority class is fewer. Imbalance class will decrease the performance of the classification algorithm. Data on IUD contraceptive use is imbalanced data. National IUD failure in 2018 was 959 or 3.5% from 27.400 users. Synthetic minority oversampling technique (SMOTE) is used to balance data on IUD failure. Balanced data is then predicted with neural networks. The system is for predicting someone when using IUD whether they have a pregnancy or not. This study uses 250 data with 235 major data (not pregnant) and 15 minor data (pregnant). From 250 data divided into two parts, 225 training and 25 testing data. Minority class on training data will be duplicated to 1524%, so that the amount of minority data become balanced with  the majority data. The results of predictive with an accuracy rate of  99.9% at 1000 epoch.",
        "link": "http://dx.doi.org/10.14710/medstat.13.1.36-46"
    },
    {
        "id": 4552,
        "title": "Equivalence of Equilibrium Propagation and Recurrent Backpropagation",
        "authors": "Benjamin Scellier, Yoshua Bengio",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1100-0"
    },
    {
        "id": 4553,
        "title": "PREDIKSI PERILAKU POLA JUMLAH MAHASISWA MENGGUNAKAN JARINGAN SYARAF TIRUAN DENGAN METODE BACKPROPAGATION",
        "authors": "Yustria Handika Siregar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Penelitian ini bertujuan untuk memprediksi perilaku pola mahasiswa sehingga dapat memprediksi berdasarkan jumlah mahasiswa. Untuk mencapai output yang optimal maka penelitian ini menggunakan Jaringan Syaraf Tiruan dengan metode Backpropagation. Studi kasus yang dilakukan di Fakultas Teknik Universitas Asahan. Data yang digunakan adalah data jumlah mahasiswa pada tahun ajaran 2011 s/d 2013 sebagai data training dan data tahun ajaran 2014 s/d 2016 sebagai data testing. Selanjutnya, data dianalisa dengan beberapa pola arsitektur jaringan dan pola yang terbaik akan dipilih untuk diimplementasikan ke program Matlab R2010. Hasil sistem menunjukkan korelasi antara jumlah mahasiswa yang terjadi.",
        "link": "http://dx.doi.org/10.31227/osf.io/m34fg"
    },
    {
        "id": 4554,
        "title": "Perancangan Sistem Pendukung Keputusan Untuk Prediksi Penerima Beasiswa Menggunakan Metode Neural Network Backpropagation",
        "authors": "Ade Pujianto, Kusrini Kusrini, Andi Sunyoto",
        "published": "2018-5-2",
        "citations": 2,
        "abstract": "Seleksi di Amikom masih mengalami kendala pada proses pengambilan keputusan, banyaknya data menyebabkan pengambil keputusan membutuhkan tools yang dapat membantu dalam menentukan penerima beasiswa, salah satu metode yang sering digunakan adalah artificial neural network (ANN). Metode ini meniru jaringan pemodelan saraf otak manusia berupa neuron-neuron untuk menyelesaikan suatu permasalahan. Salah satu penerapan neural network adalah untuk melakukan prediksi atau peramalan terhadap suatu peristiwa tertentu serta dianggap mampu menyelesaikan masalah yang komplek seperti penalaran otak manusia. Untuk menyelesaiakn masalah yang komplek neural network memerlukan banyak neuron atau yang biasa disebut layer (lapis). Salah satu metode neural network multi lapis adalah backpropagation yang mampu mengoptimalisasi bobot pada neuron dan menyelesaikan masalah yang komplek. Hasil dari penelitian ini adalah sebuah perancangan sistem prediksi dengan menggunakan metode neural network backpropagation untuk melakukan peramalan terhadap mahasiswa yang mendaftar beasiswa. hasil akhir penelitian ini adalah nilai akurasi sebesar 90% dan nilai error terkecil sebesar 0,000101 pada epoch ke 329 dengan jumlah 3000 data dengan pembagian data training 2.250 dan 750 data testing serta konfigurasi learning rate sebesar 0,2 dan momentum 0,2. Kata kunci: Artificial Neural netwok, Backpropagarion, Prediksi, beasiswa, Pengambilan Keputusan. AbstractSelection in Amikom is still constrained in the decision-making process, the number of data causing decision makers need tools that can assist in determining scholarship recipients, one of the most commonly used method is artificial neural network (ANN). This method mimics the neural network modeling of the human brain in the form of neurons to solve a problem. One application of neural network is to make predictions or forecasting of a particular event and is considered capable of solving complex problems such as human brain reasoning. To solve the problem the complex neural network requires many neurons or so-called layers. One method of multi layer neural network is backpropagation that is able to optimize the weight of neurons and solve complex problems. The result of this research is a prediction system design using neural network backpropagation method to forecast the students who apply for scholarship. the final result of this research is the accuracy value of 90% and the smallest error value of 0.000101 on epoch to 329 with the amount of 3000 data with sharing training 2,250 and 750 data testing and learning rate configuration of 0.2 and momentum 0.2.Keywords: Artificial Neural Netwok, Backpropagarion, Prediction, Scholarship, Decision Making.",
        "link": "http://dx.doi.org/10.25126/jtiik.201852631"
    },
    {
        "id": 4555,
        "title": "ANALISA KOMPARASI NEURAL NETWORK BACKPROPAGATION DAN MULTIPLE LINEAR REGRESSION UNTUK PERAMALAN TINGKAT INFLASI",
        "authors": "Amrin Amrin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Tingkat inflasi tidak dapat dianggap remeh dalam sistem perekonomian suatu negara dan pelaku bisnis pada umumnya. Jika inflasi dapat diramalkan dengan akurasi yang tinggi, tentunya dapat dijadikan dasar pengambilan kebijakan pemerintah dalam mengantisipasi aktivitas ekonomi di masa depan. Pada penelitian ini akan digunakan metode prediksi neural network backpropagation dan multiple linear regression untuk memprediksi tingkat inflasi bulanan di indonesia, selanjutnya membandingkan manakah yang terbaik dari kedua metode tersebut. Data inflasi yang digunakan bersumber dari Badan Pusat Statistik dari tahun 2006-2015, dimana 80% sebagai data training dan 20% sebagai data testing. Dari hasil analisis data yang dilakukan disimpulkan bahwa Performa model multiple linear regression lebih baik dibandingkan dengan metode neural network backpropagation dengan nilai mean absolute deviation (MAD) sebesar 0.0380, mean square error (MSE) sebesar 0.0023, dan nilai Root Mean Square Error (RMSE) sebesar 0.0481",
        "link": "http://dx.doi.org/10.31227/osf.io/wp58d"
    },
    {
        "id": 4556,
        "title": "Pole Optimization of IIR Filters Using Backpropagation",
        "authors": "Kristóf Horváth, Balázs Bank",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3311/minisy2022-011"
    },
    {
        "id": 4557,
        "title": "Replay as a basis for backpropagation through time in the brain",
        "authors": "Huzi Cheng, Joshua W. Brown",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractHow episodic memories are formed in the brain is an outstanding puzzle for the neuroscience community. The brain areas that are critical for episodic learning (e.g., the hippocampus) are characterized by recurrent connectivity and generate frequent offline replay events. The function of the replay events is a subject of active debate. Recurrent connectivity, computational simulations show, enables sequence learning when combined with a suitable learning algorithm such asBackpropagation through time(BPTT). BPTT, however, is not biologically plausible. We describe here, for the first time, a biologically plausible variant of BPTT in a reversible recurrent neural network, R2N2, that critically leverages offline-replay to support episodic learning. The model uses forwards and backwards offline replay to transfer information between two recurrent neural networks, acacheand aconsolidator,that perform rapid one-shot learning and statistical learning, respectively. Un-like replay in standard BPTT, this architecture requires no artificial external memory store. This architecture and approach outperform existing solutions and account for the functional significance to hippocampal replay events. We demonstrate the R2N2 network properties using benchmark tests from computer science and simulate the rodent delayed alternation T-maze task.",
        "link": "http://dx.doi.org/10.1101/2023.02.23.529770"
    },
    {
        "id": 4558,
        "title": "A chaotic spiking backpropagation approach to brain learning",
        "authors": "Guanrong Chen",
        "published": "2024-3-4",
        "citations": 0,
        "abstract": "A highlight of the chaotic spiking backpropagation (CSBP) method, which is a powerful tool for directly training spiking neural networks and helps to understand the learning mechanisms of human brain.",
        "link": "http://dx.doi.org/10.1093/nsr/nwae070"
    },
    {
        "id": 4559,
        "title": "Delta Rule and Backpropagation",
        "authors": "Leonardo Vanneschi, Mauro Castelli",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-809633-8.20340-3"
    },
    {
        "id": 4560,
        "title": "Switching of Interfaces and Self-optimization of weights using Backpropagation ANN in WHN Enviornment",
        "authors": "Monika Rani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn today’s scenario, mobile communication is facing a healthy competition due to different networks, interfaces, channels, and many more available in wireless heterogeneous environment. The problem arises when customers/users get the availability of many interfaces at the same time. At that time users need an intelligent or smart mechanism to connect them to the best services according to their requirements/preferences. Interface management manages available interfaces and connects the user with the best. In this paper, Interface management with Artificial Neural Network (ANN) allows the smart use of different radio accesses/interfaces. The selection is made with different parameters of different networks. This paper proposed a backpropagation neural network that is used for the switching in between different networks-3G, WLAN, 4G and 5G. The different parameters of a network are used as the selection parameters with assigning proper weights. Weights are initialized with fuzzy AHP and optimized with Back Propagation Neural Network (BPNN). The target value and the actual value is compared and their difference used as the adjusting value for the weights to get the optimum value. The backpropagation is used to train the network. The comparison among the projected algorithm and the existing algorithm shows the valuablity of the new method and the best connectivity of the network.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2626450/v1"
    },
    {
        "id": 4561,
        "title": "From Backpropagation to Neurocontrol ⋆",
        "authors": "Paul J. Werbos",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315218427-2"
    },
    {
        "id": 4562,
        "title": "The underlying mechanisms of alignment in error backpropagation through arbitrary weights",
        "authors": "Alireza Rahmansetayesh, Ali Ghazizadeh, Farokh Marvasti",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractUnderstanding the mechanisms by which plasticity in millions of synapses in the brain is orchestrated to achieve behavioral and cognitive goals is a fundamental quest in neuroscience. In this regard, insights from learning methods in artificial neural networks (ANNs) and in particular supervised learning using backpropagation (BP) seem inspiring. However, the implementation of BP requires exact matching between forward and backward weights, which is unrealistic given the known connectivity pattern in the brain (known as “weight transport problem”). Notably, it has been shown that under certain conditions, errorBackPropagationThroughArbitraryWeights (BP-TAW) can lead to a partial alignment between forward and backward weights (weight alignment or WA). This learning algorithm, which is also known as feedback alignment (FA), can result in surprisingly good degrees of accuracy in simple classification tasks. However, the underlying mechanisms and mathematical basis of WA are not thoroughly understood. In this work, we show that the occurrence of WA is governed by statistical properties of the output and error signals of neurons, such as autocorrelation and cross-correlation, and can happen even in the absence of learning or reduction of the loss function. Moreover, we show that WA can be improved significantly by limiting the norm of input weights to neurons and that such a weight normalization (WN) method can improve the classification accuracy of BP-TAW. The findings presented can be used to further improve the performance of BP-TAW and open new ways for exploring possible learning mechanisms in biological neural networks without exact matching between forward and backward weights.",
        "link": "http://dx.doi.org/10.1101/2021.06.12.447639"
    },
    {
        "id": 4563,
        "title": "Performance Analysis of Backpropagation Artificial Neural Networks with Various Activation Functions and Network Sizes",
        "authors": "Hamed Hosseinzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper conducts a comprehensive performance analysis of Back Propagation Artificial Neural Networks (BP-ANNs) utilizing various activation functions. Activation functions play a crucial role in shaping neural networks' behavior and learning capabilities. Through systematic evaluation across diverse network sizes (numbers of hidden layers and neurons), this study assesses the impact of commonly employed activation functions—such as Sigmoidalm, Tanh, Cloglog, Aranda, and others—on the convergence speed and accuracy of BP-ANNs. The findings provide empirical insights essential for optimizing neural network artificial intelligence architectures tailored to specific applications and datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4141485/v1"
    },
    {
        "id": 4564,
        "title": "Pengaruh Normalisasi Data Terhadap Performa Hasil Klasifikasi Algoritma Backpropagation",
        "authors": "Inggih Permana, Febi Nur Salisah Salisah",
        "published": "2022-3-31",
        "citations": 1,
        "abstract": "Keberhasilan Algoritma Backpropagation (BP) tergantung pada kualitas data. Sehingga, normalisasi data merupakan proses yang penting. Akan tetapi, beberapa penelitian juga ada yang tidak menggunakan normalisasi data. Oleh sebab itu, penelitian ini mengukur pengaruh normalisasi data terhadap performa hasil klasifikasi Algoritma Backpropagation. Agar diketahui apakah normalisasi benar-benar bisa meningkatkan performa hasil klasifikasi pada Algoritma BP. Penelitian ini menggunakan tiga metode normalisasi data, yaitu: MinMax Normalization; MaxAbs Normalization; dan Z-Score Normalization. Berdasarkan hasil percobaan didapat bahwa jika data yang digunakan terdapat perbedaan rentang nilai antar atribut yang tidak berbeda jauh, maka BP tanpa normalisasi data bisa menjadi pilihan terbaik. Akan tetapi jika pada data terdapat atribut yang memiliki perbedaan rentang nilai yang jauh dari atribut lainnya, maka menggunakan normalisasi data bisa menjadi pilihan terbaik. Berdasarkan hasil percobaan juga didapat bahwa Z-Score Normalization merupakan metode normalisasi terbaik.",
        "link": "http://dx.doi.org/10.57152/ijirse.v2i1.311"
    },
    {
        "id": 4565,
        "title": "Backpropagation Neural Tree",
        "authors": "Varun Ojha, Giuseppe Nicosia",
        "published": "2022-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.02.003"
    },
    {
        "id": 4566,
        "title": "Unsupervised Image Segmentation by Backpropagation",
        "authors": "Asako Kanezaki",
        "published": "2018-4",
        "citations": 119,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2018.8462533"
    },
    {
        "id": 4567,
        "title": "Human Emotion Identification Using F eed Forward Neural Network with Backpropagation and Bayesian Regularized Backpropagation Algorithm",
        "authors": "R. Sofia*,  , Dr.D. Sivakumar,  ",
        "published": "2019-8-30",
        "citations": 0,
        "abstract": "In this work, initially the human face will be detected. Then the facial features will be extracted and classified into different expressions. Here two types of algorithm viz. used in Feed Forward neural network(FFNN), ie., Backpropagation(BP) Algorithm and Bayesian Regularization Algorithm. After evaluating Bayesian regularized Backpropagation Algorithm (BR) is found to be better suited for automatic facial expression recognition than Backpropagation algorithm(BP), and the performance is evaluate using various metrics.",
        "link": "http://dx.doi.org/10.35940/ijeat.f8352.088619"
    },
    {
        "id": 4568,
        "title": "Semi-Supervised Learning Combining Backpropagation and STDP: STDP Enhances Learning by Backpropagation with a Small Amount of Labeled Data in a Spiking Neural Network",
        "authors": "Kotaro Furuya, Jun Ohkubo",
        "published": "2021-7-15",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7566/jpsj.90.074802"
    },
    {
        "id": 4569,
        "title": "Stock market prediction using multivariate neural network backpropagation",
        "authors": "Tendra Kristian, Farida Titik Kristanti",
        "published": "2020-2-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367814557-54"
    },
    {
        "id": 4570,
        "title": "Comparative evaluation of backpropagation neural network and genetic algorithm-backpropagation neural network models for PM2.5 concentration prediction based on aerosol optical depth, meteorological factors, and air pollutants",
        "authors": "Jilin Gu, Shuang Liang, Qiao Song, Yuwei Li, Yiwei Wang, Shumin Guo",
        "published": "2023-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/1.jrs.18.012006"
    },
    {
        "id": 4571,
        "title": "Volterra Series Digital Backpropagation Accounting for PMD",
        "authors": "Cristian B. Czegledi, Ronen Dar",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecoc.2017.8346173"
    },
    {
        "id": 4572,
        "title": "Multi-Task Modular Backpropagation For Dynamic Time Series Prediction",
        "authors": "Rohitash Chandra",
        "published": "2018-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489740"
    },
    {
        "id": 4573,
        "title": "Combining Backpropagation with Equilibrium Propagation to improve an Actor-Critic RL framework",
        "authors": "Yoshimasa Kubo, Eric Chalmers, Artur Luczak",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackpropagation has been used to train neural networks for many years, allowing them to solve a wide variety of tasks like image classification, speech recognition, and reinforcement learning tasks. But the biological plausibility of backpropagation as a mechanism of neural learning has been questioned. Equilibrium Propagation (EP) has been proposed as a more biologically plausible alternative and achieves comparable accuracy on the CIFAR-10 image classification task. This study proposes the first EP-based reinforcement learning architecture: an actor-critic architecture with the actor network trained by EP. We show that this model can solve the basic control tasks often used as benchmarks for BP-based models. Interestingly, our trained model demonstrates more consistent high-reward behavior than a comparable model trained exclusively by backpropagation.",
        "link": "http://dx.doi.org/10.1101/2022.06.21.496871"
    },
    {
        "id": 4574,
        "title": "Spiking Neural Networks Using Backpropagation",
        "authors": "Tehreem Syed, Vijay Kakani, Xuenan Cui, Hakil Kim",
        "published": "2021-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp52854.2021.9550994"
    },
    {
        "id": 4575,
        "title": "Combination of Fuzzy C-Means, Xie-Beni Index, and Backpropagation Neural Network for Better Forecasting Result",
        "authors": "Muttabik Fathul Lathief, Indah Soesanti, Adhistya Erna Permanasari",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009858200720077"
    },
    {
        "id": 4576,
        "title": "Comparison of Backpropagation and Kalman Filter-based Training for Neural Networks",
        "authors": "Laurin Luttmann, Paolo Mercorelli",
        "published": "No Date",
        "citations": 0,
        "abstract": "This work describes and compares the backpropagation algorithm with the Extended Kalman filter, a second-order training method which can be applied to the problem of learning neural network parameters and is known to converge in only a few iterations. The algorithms are compared with respect to their effectiveness and speed of convergence using simulated data for both, a regression and a classification task.",
        "link": "http://dx.doi.org/10.20944/preprints202104.0523.v1"
    },
    {
        "id": 4577,
        "title": "Backpropagation in hyperbolic chaos via adjoint shadowing",
        "authors": "Angxiu Ni",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "Abstract\nTo generalise the backpropagation method to both discrete-time and continuous-time hyperbolic chaos, we introduce the adjoint shadowing operator \n\n\n\nS\n\n\n\n acting on covector fields. We show that \n\n\n\nS\n\n\n\n can be equivalently defined as:\n\n\n\n\n\n\n\nS\n\n\n\n is the adjoint of the linear shadowing operator S;\n\n\n\n\n\n\n\nS\n\n\n\n is given by a ‘split then propagate’ expansion formula;\n\n\n\n\n\n\n\nS\n\n(\nω\n)\n\n\n is the only bounded inhomogeneous adjoint solution of ω.\n\n\n\nBy (a), \n\n\n\nS\n\n\n\n adjointly expresses the shadowing contribution, a significant part of the linear response, where the linear response is the derivative of the long-time statistics with respect to system parameters. By (b), \n\n\n\nS\n\n\n\n also expresses the other part of the linear response, the unstable contribution. By (c), \n\n\n\nS\n\n\n\n can be efficiently computed by the nonintrusive shadowing algorithm in Ni and Talnikar (2019 J. Comput. Phys.\n395 690–709), which is similar to the conventional backpropagation algorithm. For continuous-time cases, we additionally show that the linear response admits a well-defined decomposition into shadowing and unstable contributions.",
        "link": "http://dx.doi.org/10.1088/1361-6544/ad1aed"
    },
    {
        "id": 4578,
        "title": "Error Backpropagation Neural Network for Classifying Diabetic Diseases based on the Grading Method of Circular Region",
        "authors": "Murad Obaid Abed, Osama Qasim Jumah Al-Thahab, Mohammad Qasim Shakir",
        "published": "2021-10-30",
        "citations": 1,
        "abstract": "Diabetic retinopathy grading is an important issue after detecting lesions of retina to estimate their risk and to take a suitable decision for treatment. Here, the grading of diabetic retinopathy is examined by consistent medical approaches to build a computer model for grading in automated way, which improve the efficiency of diabetic screening services. After the grading of diabetic retinopathy, Error Backpropagation Neural Network Learning Rule is used to give suggestions to a doctor for suitable treatment for the patient. Here, sixteen different cases are trained, and it takes about 8.368 seconds with 20820 iterations. The Neural network diagnosis four-treatment cases and they are urgent, moderate, mild and normal. It is also found from the results that Neural Network is very fast algorithm to give these decisions. In addition, the program that is used for carrying out processes is MATLAB Program version 2015, the computer is HP core i7.",
        "link": "http://dx.doi.org/10.14704/web/v18si05/web18260"
    },
    {
        "id": 4579,
        "title": "On Optimizing the Number of Filters in Cnns with and Without Backpropagation",
        "authors": "Muhammad Manzar Maqbool, Clive Cheong Took",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627332"
    },
    {
        "id": 4580,
        "title": "IDENTIFICATION OF NITROGEN STATUS IN Brassica juncea L.USING COLOR MOMENT GLCM AND BACKPROPAGATION NEURAL NETWORK",
        "authors": "Satya Kumara",
        "published": "No Date",
        "citations": 1,
        "abstract": "Vegetables cultivation using hydroponic is becoming popular now days because of its irrigation and fertilizer efficiency. One type of vegetable which can be cultivated using hydroponic is green mustard (Brassica juncea L.) tosakan variety. This vegetable is harvested in the vegetative phase, approximately aged of 30 days after planting. In addition, during the vegetative phase, this plant requires more nitrogen for growth of vegetative organs. The lack of nitrogen will lead to slow growth and the leaves turn yellow. In this study, non-destructive technology was developed to identify nitrogen status through the image of green mustard leaf by using digital image processing and artificial neural network. The image processing method used was the color moment for color feature extraction, gray level co-occurrence matrix (GLCM) for texture feature extraction and back propagation neural network to identify nitrogen status from the image of leaf. The input image data resulted from acquisition process was RGB color image which was converted to HSV. Prior to the color and texture feature extraction and texture, acquisition image was segmented and cropped to get the leaf image only. Next Step was to conduct training using back propagation neural network with two hidden layer combinations, 20,000 iteration epoch. Accuracy of the test results using those methods was 97.82%. The result indicates those three methods is reliable to identify nitrogen status in the leaf of green mustard.",
        "link": "http://dx.doi.org/10.31219/osf.io/cmg2b"
    },
    {
        "id": 4581,
        "title": "Neural networks and backpropagation",
        "authors": "Adamantios Zaras, Nikolaos Passalis, Anastasios Tefas",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-385787-1.00007-5"
    },
    {
        "id": 4582,
        "title": "Backpropagation Neural Network",
        "authors": "",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781394217519.ch5"
    },
    {
        "id": 4583,
        "title": "Speckle backpropagation for compensation of nonlinear effects in few-mode optical fibers",
        "authors": "Pavel S. Anisimov, Evgeny D. Tsyplakov, Viacheslav V. Zemlyakov, Jiexing Gao",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/col202321.030601"
    },
    {
        "id": 4584,
        "title": "Backpropagation of Spirit: Hegelian Recollection and Human-A.I. Abductive Communities",
        "authors": "Rocco Gangle",
        "published": "2022-3-26",
        "citations": 1,
        "abstract": "This article examines types of abductive inference in Hegelian philosophy and machine learning from a formal comparative perspective and argues that Robert Brandom’s recent reconstruction of the logic of recollection in Hegel’s Phenomenology of Spirit may be fruitful for anticipating modes of collaborative abductive inference in human/A.I. interactions. Firstly, the argument consists of showing how Brandom’s reading of Hegelian recollection may be understood as a specific type of abductive inference, one in which the past interpretive failures and errors of a community are explained hypothetically by way of the construction of a narrative that rehabilitates those very errors as means for the ongoing successful development of the community, as in Brandom’s privileged jurisprudential example of Anglo-American case law. Next, this Hegelian abductive dynamic is contrasted with the error-reducing backpropagation algorithms characterizing many current versions of machine learning, which can be understood to perform abductions in a certain sense for various problems but not (yet) in the full self-constituting communitarian mode of creative recollection canvassed by Brandom. Finally, it is shown how the two modes of “error correction” may possibly coordinate successfully on certain types of abductive inference problems that are neither fully recollective in the Hegelian sense nor algorithmically optimizable.",
        "link": "http://dx.doi.org/10.3390/philosophies7020036"
    },
    {
        "id": 4585,
        "title": "Quantum Circuit for Regression Learning with Backpropagation",
        "authors": "Natt Luangsirapornchai, Prabhas Chongstitvatana",
        "published": "2021-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsec53205.2021.9684610"
    },
    {
        "id": 4586,
        "title": "Hidden Priors for Bayesian Bidirectional Backpropagation",
        "authors": "Olaoluwa Adigun, Bart Kosko",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394213"
    },
    {
        "id": 4587,
        "title": "Impact of Digital Backpropagation in ultrawideband transmission systems",
        "authors": "Esteban Paz, Gabriel Saavedra",
        "published": "2022",
        "citations": 0,
        "abstract": "Ultrawideband transmission can provide increased capacity in single-mode fiber links. Stimulated Raman scattering emerges as a critical effect to manage in such systems. Here we discuss the impact of digital backpropagation in ultrawideband systems.",
        "link": "http://dx.doi.org/10.1364/laop.2022.th1c.3"
    },
    {
        "id": 4588,
        "title": "Digital Backpropagation Based on Binary Logarithmic Step Size Distribution for Fiber Nonlinearity Compensation",
        "authors": "Albert Dede, Emmanuel Akowuah, Shyqyri Haxha",
        "published": "No Date",
        "citations": 0,
        "abstract": "Capacity crunch has become critical in recent years as commercial\ncommunication systems approach their theoretical data rate limits. This\nwork presents a low-complexity digital backpropagation (DBP)\nimplementation approach based on step size distribution that uses a\nbinary logarithmic step size method to achieve high data rate optical\ntransmission. The proposed scheme shows performance improvements (∆Q) of\n2.36 dB, 1.19 dB, and 0.71 dB over linear compensation, constant step\nsize DBP, and logarithmic step size DBP techniques in a 2400 km 112\nGbit/s DP-16QAM system, respectively. At 13 dBm, a high performance (Q)\nof 10.9 dB (BER = 2.25×10-4) is achieved, above the 3.80×10-3\nhard-decision forward error correction (HD-FEC) limit, using the\nproposed scheme. Also, the allowable transmission distance is extended\nby 960 km at the HD-FEC limit over the linear compensation technique.\nThe optimization achieves a 38% savings in the number of DBP\ncalculation steps compared to the constant step size DBP, which\nconsiderably reduces the computational cost since a few steps are\nrequired for effective nonlinearity compensation.",
        "link": "http://dx.doi.org/10.22541/au.168493332.22824639/v1"
    },
    {
        "id": 4589,
        "title": "KLASIFIKASI PEMINJAMAN BUKU MENGGUNAKAN  NEURAL NETWORK BACKPROPAGATION",
        "authors": "Norhikmah Norhikmah, Rumini Rumini",
        "published": "2020-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32520/stmsi.v9i1.562"
    },
    {
        "id": 4590,
        "title": "The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware",
        "authors": "Alpha Renner, Forrest Sheldon, Anatoly Zlotnik, Louis Tao, Andrew Sornborger",
        "published": "No Date",
        "citations": 11,
        "abstract": "Abstract\nThe capabilities of natural neural systems have inspired new generations of machine learning algorithms as well as neuromorphic very large-scale integrated (VLSI) circuits capable of fast, low-power information processing. However, it has been argued that most modern machine learning algorithms are not neurophysiologically plausible. In particular, the workhorse of modern deep learning, the backpropagation algorithm, has proven difficult to translate to neuromorphic hardware. In this study, we present a neuromorphic, spiking backpropagation algorithm based on synfire-gated dynamical information coordination and processing, implemented on Intel's Loihi neuromorphic research processor. We demonstrate a proof-of-principle three-layer circuit that learns to classify digits from the MNIST dataset. To our knowledge, this is the first work to show a Spiking Neural Network (SNN) implementation of the backpropagation algorithm that is fully on-chip, without a computer in the loop.  It is competitive in accuracy with off-chip trained SNNs and achieves an energy-delay product suitable for edge computing. This implementation shows a path for using in-memory, massively parallel neuromorphic processors for low-power, low-latency implementation of modern deep learning applications.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-701752/v1"
    },
    {
        "id": 4591,
        "title": "ReSprop: Reuse Sparsified Backpropagation",
        "authors": "Negar Goli, Tor M. Aamodt",
        "published": "2020-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr42600.2020.00162"
    },
    {
        "id": 4592,
        "title": "HESSPROP: Mitigating Memristive DNN Weight Mapping Errors with Hessian Backpropagation",
        "authors": "Jack Cai, Muhammad Ahsan Kaleem, Amirali Amirsoleimani, Roman Genov",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.167727732.20256861/v1"
    },
    {
        "id": 4593,
        "title": "Prediksi Kunjungan Wisata Kota Payakumbuh Menggunakan Metode Jaringan Syaraf Tiruan Backpropagation",
        "authors": "Nurul Aulya",
        "published": "2022-9-6",
        "citations": 0,
        "abstract": "Tourism is a whole related elements which consist of tourists, tourist destinations, travel, industry and so on which are tourism activities and abundant natural wealth. The tourism sector is a very important service-based sector. Tourism is the fastest growing, vibrant and strong economic sector development, it also contributes to Gross Domestic Product (GDP), job creation, social and economic development. Artificial Neural Networks are computer programs that can imitate thought processes and knowledge to solve a specific problem. One of which is applied by the Artificial Neural Network to predict tourist visits. By using the Backpropagation method, it will be known the prediction of the number of tourist visits. The Backpropagation method is very useful for Artificial Neural Networks predicting the number of tourist visits the following year. The data processed in this study were 12 data sourced from the tourism section of the Payakumbuh City Youth and Sports Tourism Office. Furthermore, the data is processed using Matlab software. The stages of backpropagation are initialization, activation, training and iteration. The calculation of the network pattern used and the accuracy level of the expected error is continued. The result of testing this method is that it can predict tourist visits. So the level of accuracy is 95%. The prediction process has been carried out to predict tourist visits to the city of Payakumbuh. With the level of accuracy obtained is met, it can be used to help the Payakumbuh City Tourism Office increase the number of tourist visits in the future and further improve tourism management.",
        "link": "http://dx.doi.org/10.37034/infeb.v4i4.157"
    },
    {
        "id": 4594,
        "title": "Using the Backpropagation Algorithm to Distinguish Arabic Alphabet",
        "authors": "",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21271/zjhs.28.1.6"
    },
    {
        "id": 4595,
        "title": "Prediction of active compounds from SMILES codes using backpropagation\n          algorithm",
        "authors": "Dian Eka Ratnawati,  Marjono, Syaiful Anam",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.5062773"
    },
    {
        "id": 4596,
        "title": "Heart Disease Detection using Iridology with Principal Component Analysis (PCA) and Backpropagation Neural Network",
        "authors": "Leonardus Sandy Ade Putra, R. Rizal Isnanto, Aris Triwiyatno, Vincentius Abdi Gunawan",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009009402510258"
    },
    {
        "id": 4597,
        "title": "Two Dimensional Estimation of Speed Flow Relationships with Backpropagation Neural Networks",
        "authors": "Matti Pursula",
        "published": "2019-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429445286-13"
    },
    {
        "id": 4598,
        "title": "Peramalan Jumlah Permintaan Produksi Menggunakan Jaringan Saraf Tiruan Algoritma Backpropagation",
        "authors": "Muhammad Thoriq",
        "published": "2022-2-16",
        "citations": 0,
        "abstract": "Artificial Neural Network (ANN) technique has developed rapidly in the field of estimation. ANN can predict based on data on events and related factors that existed in the past. ANN has advantages in parallel computing in classifying patterns. ANN is also capable of self-regulating the data to be processed without requiring an explicit function specification. The advantage of using ANN is the elimination of complex analytical and numerical iterative computations. The ANN method that is often used in prediction case studies is the Backpropagation Algorithm. This algorithm has the ability to solve problems in the real world by building trained methods that show good performance on large data scales and are able to overcome complex pattern recognition. This study aims to predict the demand for salt optimally using the ANN Method with the Backprogation Algorithm at PT. Kurnia Garam Prosperous Padang City. This forecasting is needed because of the high cost of production with the large number of requests that occur to be more effective. Proper forecasting will be able to optimize production so that it can reduce the required production costs. The data processed is salt production data from 2016 to 2018 at PT. Kurnia Garam Prosperous. The momentum results obtained are 3-9-1 for dividing the data into 2, namely 24 training data and 12 test data. The optimal prediction result is 0.98946, so this research is very helpful in forecasting optimal and efficient production costs.",
        "link": "http://dx.doi.org/10.37034/jidt.v4i1.178"
    },
    {
        "id": 4599,
        "title": "K-Means – Resilient Backpropagation Neural Network in Predicting Poverty Levels",
        "authors": "Bobby Poerwanto",
        "published": "2023-5-17",
        "citations": 0,
        "abstract": "In solving economic problems, the government has implemented several development policies. However, this policy is considered to be too centered on big cities. So, through this research it is hoped that it can provide an overview related to regional groups that fall into the poorer category so that the government can also provide accelerated development policies that are oriented towards improving the economy of residents in the area. This study aims to determine the results of classifying district/city poverty levels in Indonesia as a basis for classification for predictions and to classify district/city poverty levels based on influencing factors. The method used in this study is K-Means Clustering using the poverty depth index and poverty severity index variables, then proceed with using the Backpropagation Neural Network (BNN) algorithm using the GRDP, per capita expenditure, human development index, and mean years of schooling. The results obtained using the K-Means algorithm are that there are 42 districts/cities that belong to cluster 1 where this region has a poverty index depth and severity index value that is higher than the 472 districts/cities in cluster 2. Furthermore, the cluster results are used as response variables for classification with BNN. The accuracy of the model obtained is very high, which is equal to 98.06, so the model is very feasible to be used as a poverty rate prediction model based on the variables used.",
        "link": "http://dx.doi.org/10.30812/varian.v6i2.2756"
    },
    {
        "id": 4600,
        "title": "Stochastic Backpropagation through Fourier Transforms",
        "authors": "Amine Echraibi, Joachim Flocon-Cholet, Stephane Gosselin, Sandrine Vaton",
        "published": "2021-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco54536.2021.9616294"
    }
]