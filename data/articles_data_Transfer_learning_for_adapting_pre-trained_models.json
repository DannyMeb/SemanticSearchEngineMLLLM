[
    {
        "id": 18705,
        "title": "Representation Transfer Learning via Multiple Pre-trained models for Linear Regression",
        "authors": "Navjot Singh, Suhas Diggavi",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isit54713.2023.10207013"
    },
    {
        "id": 18706,
        "title": "Deep Learning for Violence Detection in Surveillance: The Role of Transfer Learning and Pre-Trained Models",
        "authors": "Anum Ilyas, Surayya Obaid, Narmeen Zakaria Bawany",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acit58888.2023.10453685"
    },
    {
        "id": 18707,
        "title": "Transfer Learning using Very Deep Pre-Trained Models for Food Image Classification",
        "authors": "Pranjal Kumar Singh, Seba Susan",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307479"
    },
    {
        "id": 18708,
        "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
        "authors": "Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Tomohiro Tanaka, Takatomo Kano, Atsunori Ogawa, Marc Delcroix",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1307"
    },
    {
        "id": 18709,
        "title": "Classifying Dog Emotions Using Transfer Learning and Pre-trained Models",
        "authors": "Rishabh Chauhan, Harsh Manchanda, Neha Tyagi",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/confluence60223.2024.10463254"
    },
    {
        "id": 18710,
        "title": "Classification of Regional Food Using Pre-Trained Transfer Learning Models",
        "authors": "Jeet Gadhiya, Anjali Khatik, Shruti Kodinariya, Dipak Ramoliya",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceca58529.2023.10395249"
    },
    {
        "id": 18711,
        "title": "Towards Inadequately Pre-trained Models in Transfer Learning",
        "authors": "Andong Deng, Xingjian Li, Di Hu, Tianyang Wang, Haoyi Xiong, Cheng-Zhong Xu",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01777"
    },
    {
        "id": 18712,
        "title": "Exploring the Transfer Learning: A Comparative Study of Pre-trained Models and Fine-tuning Techniques on image dataset",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56452/6-3-664"
    },
    {
        "id": 18713,
        "title": "Efficient MLOps Pipeline for Transfer Learning and Reuse of Pre-Trained ML Models",
        "authors": "Vimal Kumar, Debjani Ghosh, Shivom Srivastava",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ants59832.2023.10468932"
    },
    {
        "id": 18714,
        "title": "A Novel Transfer Learning Approach for Eye Tumour Detection using Pre-Trained CNN Models",
        "authors": "G Prabaharan, Vani Rajasekar, Velliangiri Sarveshwaran",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/stcr59085.2023.10396907"
    },
    {
        "id": 18715,
        "title": "Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning",
        "authors": "Lifeng Han, Gleb Erofeev, Irina Sorokina, Serge Gladkoff, Goran Nenadic",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.5"
    },
    {
        "id": 18716,
        "title": "Deep Transfer Learning for Tomato Leaf Diseases Detection and Classification using Pre-trained Models",
        "authors": "Ashish Mishra, Ashutosh Mishra, A. K Tewari, Jayesh Gangrade",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc60394.2023.10441215"
    },
    {
        "id": 18717,
        "title": "A Natural Language Processing Model Based on Pre-trained Deep Learning Models",
        "authors": "Yitong Niu",
        "published": "2023-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.57237/j.cst.2022.01.007"
    },
    {
        "id": 18718,
        "title": "PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models",
        "authors": "Tiantian Feng, Shrikanth Narayanan",
        "published": "2023-9-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acii59096.2023.10388152"
    },
    {
        "id": 18719,
        "title": "A novel application of deep transfer learning with audio pre-trained models in pump audio fault detection",
        "authors": "Ali Akbar Taghizadeh Anvar, Hossein Mohammadi",
        "published": "2023-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compind.2023.103872"
    },
    {
        "id": 18720,
        "title": "EmoAsst: emotion recognition assistant via text-guided transfer learning on pre-trained visual and acoustic models",
        "authors": "Minxiao Wang, Ning Yang",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "Children diagnosed with Autism Spectrum Disorder (ASD) often struggle to grasp social conventions and promptly recognize others' emotions. Recent advancements in the application of deep learning (DL) to emotion recognition are solidifying the role of AI-powered assistive technology in supporting autistic children. However, the cost of collecting and annotating large-scale high-quality human emotion data and the phenomenon of unbalanced performance on different modalities of data challenge DL-based emotion recognition. In response to these challenges, this paper explores transfer learning, wherein large pre-trained models like Contrastive Language-Image Pre-training (CLIP) and wav2vec 2.0 are fine-tuned to improve audio- and video-based emotion recognition with text- based guidance. In this work, we propose the EmoAsst framework, which includes a visual fusion module and emotion prompt fine-tuning for CLIP, in addition to leveraging CLIP's text encoder and supervised contrastive learning for audio-based emotion recognition on the wav2vec 2.0 model. In addition, a joint few-shot emotion classifier enhances the accuracy and offers great adaptability for real-world applications. The evaluation results on the MELD dataset highlight the outstanding performance of our methods, surpassing the majority of existing video and audio-based approaches. Notably, our research demonstrates the promising potential of the proposed text-based guidance techniques for improving video and audio-based Emotion Recognition and Classification (ERC).",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fcomp.2024.1304687"
    },
    {
        "id": 18721,
        "title": "Selecting the optimal transfer learning model for precise breast cancer diagnosis utilizing pre-trained deep learning models and histopathology images",
        "authors": "Aswathy Ravikumar, Harini Sriraman, B. Saleena, B. Prakash",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12553-023-00772-0"
    },
    {
        "id": 18722,
        "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
        "authors": "Wangchunshu Zhou, Ronan Le Bras, Yejin Choi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.368"
    },
    {
        "id": 18723,
        "title": "Fire Detection using Transfer Learning and Pre-Trained Model",
        "authors": "Prachi Pednekar, Abheet Srivastava, Anil Jadhav",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205560"
    },
    {
        "id": 18724,
        "title": "Classification of Rice Leaf Diseases Using CNN-Based Pre-Trained Models and Transfer Learning",
        "authors": "Marjan Mavaddat, Marjan Naderan, Seyyed Enayatallah Alavi",
        "published": "2023-2-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipria59240.2023.10147178"
    },
    {
        "id": 18725,
        "title": "An Analysis of Pre-trained Models Versus Custom Deep Learning Models for Forest Fire Detection",
        "authors": "Shouthiri Partheepan, Farzad Sanati, Jahan Hassan",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itnac59571.2023.10368557"
    },
    {
        "id": 18726,
        "title": "Classification of Hazelnut Species with Pre-Trained Deep Learning Models",
        "authors": "Selçuk Harmancı, Yavuz Ünal, Barış Ateş",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "A form of shelled nut in the Betulaceae family is the hazelnut. The majority of it is grown in Türkiye internationally. It grows in the provinces of Türkiye's Black Sea region, which is a significant global production hub. Hazelnuts can be eaten in a variety of ways and are a great source of protein, fat, fiber, vitamins, and minerals. There are numerous applications for hazelnuts in the food business. This study uses pre-trained networks to categorize eight of the most popular hazelnut kinds farmed in Türkiye. In this study, locally named hazelnut varieties grown in Türkiye were examined. An automated computer vision system was used to capture the images of the different hazelnut kinds. Our dataset includes a total of 2722 images, consisting of 155 palaz, 340 yagli, 399 deve disi, 236 tombul, 399 damat, 354 cakildak, 437 kara findik, and 402 sivri hazelnuts. Using transfer learning, the DenseNet121 and InceptionV3 models of convolutional neural networks were employed to categorize these images. The dataset was split into training and testing portions, respectively. With InceptionV3 and DenseNet121, respectively, the research revealed classification accuracy of 96.99% and 96.18%.\n\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.58190/imiens.2023.14"
    },
    {
        "id": 18727,
        "title": "Efficient packaging defect detection: leveraging pre-trained vision models through transfer learning",
        "authors": "Wiwi Prastiwinarti, Mera Kartika Delimayanti, Hendra Kurniawan, Yoga Putra Pratama, Hanin Wendho, Rizky Adi",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "The inspection of packaging defects is a crucial aspect of maintaining the quality of industrial production, especially in the case of boxed products. This study introduces a novel approach for detecting physical defects in product packaging boxes by integrating image processing with deep learning, specifically transfer learning with two images as an input. The proposed method utilizes both top view and side view images of the packaging to determine its condition, a significant departure from the conventional single image input. Our approach incorporates 16 pre-trained model variants from EfficientNetV2, MobileNetV3, and ResNetV2 for transfer learning as feature extractors. The experimental findings demonstrate that the best model that leverages EfficientNetV2 variant achieves 100% accuracy and F1 score in terms of classification performance. However, the most optimal model in terms of classification performance and inference speed was the one that leveraged ResNetV2 variant. This model scored 95% accuracy and 95.24% F1 score, with an inference speed of 91 ms per prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijeecs.v34.i3.pp2096-2106"
    },
    {
        "id": 18728,
        "title": "Image Classification by Transfer Learning using Pre-Trained CNN Models",
        "authors": "Jaya H. Dewan, Rik Das, Sudeep D. Thepade, Harsh Jadhav, Nishant Narsale, Ajinkya Mhasawade, Sinu Nambiar",
        "published": "2023-4-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/raeeucci57140.2023.10134069"
    },
    {
        "id": 18729,
        "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
        "authors": "Minglun Han, Feilong Chen, Jing Shi, Shuang Xu, Bo Xu",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-423"
    },
    {
        "id": 18730,
        "title": "Efficient Data Learning for Open Information Extraction with Pre-trained Language Models",
        "authors": "Zhiyuan Fan, Shizhu He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.869"
    },
    {
        "id": 18731,
        "title": "Performance Evaluation of Pre-Trained Deep Learning Models for Bird Species Identification",
        "authors": "Kerolos Hany, Youssef Ayman, Ayman Atia",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10255070"
    },
    {
        "id": 18732,
        "title": "Pre-trained Deep Learning Models for UAV-based Weed Recognition",
        "authors": "Faiza Mekhalfa, Fouad Yacef, Mahmoud Belhocine",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/spa59660.2023.10274449"
    },
    {
        "id": 18733,
        "title": "Transfer Learning with Pre-trained CNNs for MRI Brain Tumor Multi-Classification: A Comparative Study of VGG16, VGG19, and Inception Models",
        "authors": "Abd Aziz Siti Nurfarahin, Dziyauddin Rudzidatul Akmam, Mohd Noor Norliza",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nbec58134.2023.10352589"
    },
    {
        "id": 18734,
        "title": "Gender Bias in Pre-Trained Deep Learning Models",
        "authors": "Ahsan Ul Islam, ABM Rezbaul Islam",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csde59766.2023.10487724"
    },
    {
        "id": 18735,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 18736,
        "title": "Comparison of transfer-learning for lightweight pre-trained model on image classification",
        "authors": "Wenhan Zheng",
        "published": "2024-3-28",
        "citations": 0,
        "abstract": "This paper presents a comparative study of the performance of three convolutional neural network (CNN) architectures - EffcientNet-B0, ResNet-50, and AlexNet - for a given image classification task. The study provides a comprehensive investigation of the training process, hardware configurations, training time, and individual model performance. The investigation also assesses the models suitability for different applications. The findings can help both researchers and practitioners select the most suitable model for their specific needs and applications. The paper provides an analysis of each CNN architecture and discusses their strength and weaknesses. The results demonstrate that EffcientNet-B0 achieves the highest accuracy, but its training performance is not optimal. ResNet-50, on the other hand, exhibits high accuracy with efficient training using transfer learning. Finally, ALEXNET provides a baseline for comparison with traditional CNN designs. The paper also highlights the trade-offs involved in selecting a CNN architecture and highlights their relative advantages and disadvantages. The reader is provided with insights into which CNN architecture is most suitable for specific applications based on their requirements.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/53/20241244"
    },
    {
        "id": 18737,
        "title": "A Transfer Learning-based Pre-trained VGG16 Model for Skin Disease Classification",
        "authors": "Gurpreet Singh, Kalpna Guleria, Shagun Sharma",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mysurucon59703.2023.10396942"
    },
    {
        "id": 18738,
        "title": "Transfer Learning-Based Flower Image Classification: Leveraging The Pre-Trained Alexnet Model",
        "authors": "Ruitong Xiao, Rui Wang",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isceic59030.2023.10271209"
    },
    {
        "id": 18739,
        "title": "Pre-Trained Lightweight Deep Learning Models for Surgical Instrument Detection: Performance Evaluation for Edge Inference",
        "authors": "Md Sabbir Ahmed, Stefano Giordano",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437676"
    },
    {
        "id": 18740,
        "title": "Pre-trained deep learning models for brain MRI image classification",
        "authors": "Srigiri Krishnapriya, Yepuganti Karuna",
        "published": "2023-4-20",
        "citations": 17,
        "abstract": "Brain tumors are serious conditions caused by uncontrolled and abnormal cell division. Tumors can have devastating implications if not accurately and promptly detected. Magnetic resonance imaging (MRI) is one of the methods frequently used to detect brain tumors owing to its excellent resolution. In the past few decades, substantial research has been conducted in the field of classifying brain images, ranging from traditional methods to deep-learning techniques such as convolutional neural networks (CNN). To accomplish classification, machine-learning methods require manually created features. In contrast, CNN achieves classification by extracting visual features from unprocessed images. The size of the training dataset had a significant impact on the features that CNN extracts. The CNN tends to overfit when its size is small. Deep CNNs (DCNN) with transfer learning have therefore been developed. The aim of this work was to investigate the brain MR image categorization potential of pre-trained DCNN VGG-19, VGG-16, ResNet50, and Inception V3 models using data augmentation and transfer learning techniques. Validation of the test set utilizing accuracy, recall, Precision, and F1 score showed that the pre-trained VGG-19 model with transfer learning exhibited the best performance. In addition, these methods offer an end-to-end classification of raw images without the need for manual attribute extraction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnhum.2023.1150120"
    },
    {
        "id": 18741,
        "title": "Enhanced Skin Cancer Classification using Pre-Trained CNN Models and Transfer Learning: A Clinical Decision Support System for Dermatologists",
        "authors": "Sally Ibrahim, Khalid Amin, Mina Ibrahim",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21608/ijci.2023.236366.1141"
    },
    {
        "id": 18742,
        "title": "Skin Cancer Classification Using Transfer Learning-Based Pre-Trained VGG 16 Model",
        "authors": "Archana Saini, Kalpna Guleria, Shagun Sharma",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccis60361.2023.10425341"
    },
    {
        "id": 18743,
        "title": "Unveiling the Power of Pre - Trained Language Models in NLP Applications",
        "authors": "Shrinath Pai",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231115202502"
    },
    {
        "id": 18744,
        "title": "Do Pre-trained Models Benefit Equally in Continual Learning?",
        "authors": "Kuan-Ying Lee, Yuanyi Zhong, Yu-Xiong Wang",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00642"
    },
    {
        "id": 18745,
        "title": "Enhancement of Pre-Trained Deep Learning Models to Improve Brain Tumor",
        "authors": "Zaka Ullah, Ayman Odeh, Ihtisham Khattak, Muath Al Hasan",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31449/inf.v47i6.4645"
    },
    {
        "id": 18746,
        "title": "Prediction of Diseases in Potato Plant using Pre-trained and Traditional Machine Learning Models",
        "authors": "Swati Laxmi Sahu, Renta Chintala Bhargavi",
        "published": "2023-5-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170149"
    },
    {
        "id": 18747,
        "title": "PLM-AS: Pre-trained Language Models Augmented with Scanpaths for Sentiment Classification",
        "authors": "Duo Yang, Nora Hollenstein",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "Recent research demonstrated that deep neural networks could generate meaningful feature representations from both eye-tracking data and sentences without designing handcrafted features, which achieved competitive performance across cognitive NLP tasks, such as sentiment classification over gaze datasets, but the previous works mainly encode the text and gaze data separately without considering the interaction between these two modalities or applying large-scaled pre-trained models. To address these challenges, we introduce PLM-AS, a novel framework to take full advantage of textual and eye-tracking features by sequence modeling in a highly interactive way for multimodal fusion. It is also the first attempt to combine large-scaled pre-trained models with eye-tracking features in the cognitive reading task. We show that PLM-AS captures cognitive signals from eye-tracking data and shows improved performance on sentiment classification within and across three datasets of different domains.",
        "keywords": "",
        "link": "http://dx.doi.org/10.7557/18.6797"
    },
    {
        "id": 18748,
        "title": "Transfer-DDG: Prediction of protein-protein binding affinity changes with mutations based on large pre-trained model transfer learning",
        "authors": "Yuxiang Wang, Xiumin Shi, Han Zhou",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/oncon60463.2023.10430682"
    },
    {
        "id": 18749,
        "title": "Pre-Trained Deep Learning Models for Detecting Strikeouts in Kannada Handwritten Documents",
        "authors": " Bhargav, Shivayogi B. Ullagaddi",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "In the digital document analysis, handwritten character recognition is a challenging area.  Various methods are proposed in the literature to identify the strike-outs in various languages. Kannada handwritten classification is a crucial machine vision problem due to its various practical applications in the development of recognition systems. Identifying of strikeout string on the given Kannada statement is an important challenge to develop robust recognition system. In the present study, pre-trained models such as DenseNet121, EficientNetB0, MobileNet, InceptionResNetV2 are used to recognize strikeout Kannada words.  Experimental analysis indicates that EficientNetB0 performed better for strikeout Kannada words and InceptionResNetV2 results in higher performance for non-strikeout Kannada words.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/tjjpt.v44.i3.556"
    },
    {
        "id": 18750,
        "title": "ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning",
        "authors": "Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, Yang Liu",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icse48619.2023.00207"
    },
    {
        "id": 18751,
        "title": "Adaptive Textual Label Noise Learning based on Pre-trained Models",
        "authors": "Shaohuan Cheng, Wenyu Chen, Fu Mingsheng, Xuanting Xie, Hong Qu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.209"
    },
    {
        "id": 18752,
        "title": "Collaborative Learning across Heterogeneous Systems with Pre-Trained Models",
        "authors": "Trong Nghia Hoang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The increasingly decentralized and private nature of data in our digital society has  motivated the development of personalized, collaborative intelligent systems that enable knowledge aggregation across multiple data owners while accommodating for their data privacy and system constraints. However, collaborative learning has only been investigated in simple and limited settings: isolated task scenarios where learning begins from scratch and does not build on prior expertise; learned model is represented in task-specific forms which are not generalizable to unseen, emerging scenarios; and more often, a universal model representation is assumed across collaborators, ignoring their local compute constraints or input representations. This restricts its practicality in continual learning scenarios with limited task data, which demand continuous adaptation and knowledge transfer across different information silos, tasks, and learning models, as well as the utilization of prior solution expertises. To overcome these limitations, my research has been focused on developing effective and scalable resource-aware collaborative learning frameworks across heterogeneous systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30284"
    },
    {
        "id": 18753,
        "title": "A Weighted Ensemble Approach with Multiple Pre-trained Deep Learning Models for Classification of Stroke",
        "authors": "Rusul Ali Jabbar Alhatemi, Serkan Savaş",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "Stroke ranks as one of the deadliest diseases globally, emphasizing the crucial need for early diagnosis. This study aims to create a two-stage classification system for stroke and non-stroke images to support early clinical detection. Deep learning, a cornerstone of diagnosis, detection, and prompt treatment, is the primary methodology. Transfer learning adapts successful deep learning architectures for diverse problems, and ensemble learning combines multiple classifiers for enhanced results. These two techniques are applied to classify stroke using a dataset of stroke and normal images. In the initial stage, six pre-trained models are fine-tuned, with DenseNet, Xception, and EfficientNetB2 emerging as the top performers, achieving validation accuracies of 98.4%, 98.4%, and 98%, respectively. These models serve as base learners within an ensemble framework. A weighted average ensemble method combines them, resulting in a remarkable 99.84% accuracy on a reserved test dataset. This approach exhibits promise for stroke detection, a life-threatening condition, while also demonstrating the effectiveness of ensemble techniques in enhancing model performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47852/bonviewmedin32021963"
    },
    {
        "id": 18754,
        "title": "Transfer Learning and Tuning of Deep Pre-trained Architecture for Face Recognition",
        "authors": " Shem L. Gonzales",
        "published": "2023-7-30",
        "citations": 0,
        "abstract": "Automatic Image Identification is one of the interests of software developers with the application of machine and deep learning methods. With the incorporation of Transfer Learning and Tuning in pre-trained architecture, a substantial increase in the model’s performance is evident. This paper performs face recognition using an image identification and recognition approach. Feature extraction was performed using ResNet50 pre-trained architecture with Support Vector Machine as a classifier. Initial evaluation was made to generate a precision of 62.50%, recall of 65.55%, and f1-score of 63.99%. With this poor performance of ResNet50, the hyperparameters were tuned using transfer learning and tuning. After several times of manual experiments, a significant increase in precision is 93.75%, recall is 94.36%, and f1-score is 94.05%. Based on the remarkable yield of 35.25% for accuracy, 38.79% for recall, and an f1-score of 30.06%, it is advisable to apply the model for image identification and recognition",
        "keywords": "",
        "link": "http://dx.doi.org/10.48175/ijarsct-12196"
    },
    {
        "id": 18755,
        "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
        "authors": "Derek Chen, Kun Qian, Zhou Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.115"
    },
    {
        "id": 18756,
        "title": "Detection of Monkeypox Among Different Pox Diseases with Different Pre-Trained Deep Learning Models",
        "authors": "Muhammed ÇELİK, Özkan İNİK",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "Monkeypox is a viral disease that has recently rapidly spread. Experts have trouble diagnosing the disease because it is similar to other smallpox diseases. For this reason, researchers are working on artificial intelligence-based computer vision systems for the diagnosis of monkeypox to make it easier for experts, but a professional dataset has not yet been created. Instead, studies have been carried out on datasets obtained by collecting informal images from the Internet.  The accuracy of state-of-the-art deep learning models on these datasets is unknown. Therefore, in this study, monkeypox disease was detected in cowpox, smallpox, and chickenpox diseases using the pre-trained deep learning models VGG-19, VGG-16, MobileNet V2, GoogLeNet, and EfficientNet-B0.  In experimental studies on the original and augmented datasets, MobileNet V2 achieved the highest classification accuracy of 99.25% on the augmented dataset. In contrast, the VGG-19 model achieved the highest classification accuracy with 78.82% of the original data. Considering these results, the shallow model yielded better results for the datasets with fewer images. When the amount of data increased, the success of deep networks was better because the weights of the deep models were updated at the desired level.",
        "keywords": "",
        "link": "http://dx.doi.org/10.21597/jist.1206453"
    },
    {
        "id": 18757,
        "title": "Class-Incremental Learning Based on Big Dataset Pre-Trained Models",
        "authors": "Bin Wen, Qiuyu Zhu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3287771"
    },
    {
        "id": 18758,
        "title": "Neural machine translation of clinical text: an empirical investigation into multilingual pre-trained language models and transfer-learning",
        "authors": "Lifeng Han, Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Betty Galiano, Goran Nenadic",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "Clinical text and documents contain very rich information and knowledge in healthcare, and their processing using state-of-the-art language technology becomes very important for building intelligent systems for supporting healthcare and social good. This processing includes creating language understanding models and translating resources into other natural languages to share domain-specific cross-lingual knowledge. In this work, we conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three sub-tasks including (1) clinical case (CC), (2) clinical terminology (CT), and (3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) outperformed the other two extra-large language models by a large margin in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself, which deserves more exploitation for clinical knowledge transformation, e.g. to investigate into more languages. These research findings can shed some light on domain-specific machine translation development, especially in clinical and healthcare fields. Further research projects can be carried out based on our work to improve healthcare text analytics and knowledge transformation. Our data is openly available for research purposes at: https://github.com/HECTA-UoM/ClinicalNMT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fdgth.2024.1211564"
    },
    {
        "id": 18759,
        "title": "Parameter-Efficient Transfer Learning of Pre-Trained Transformer Models for Speaker Verification Using Adapters",
        "authors": "Junyi Peng, Themos Stafylakis, Rongzhi Gu, Oldřich Plchot, Ladislav Mošner, Lukáš Burget, Jan Černocký",
        "published": "2023-6-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10094795"
    },
    {
        "id": 18760,
        "title": "Unified Prompt Learning Makes Pre-Trained Language Models Better Few-Shot Learners",
        "authors": "Feihu Jin, Jinliang Lu, Jiajun Zhang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095738"
    },
    {
        "id": 18761,
        "title": "Disaster Image Classification Using Pre-trained Transformer and Contrastive Learning Models",
        "authors": "Soudabeh Taghian Dinani, Doina Caragea",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302517"
    },
    {
        "id": 18762,
        "title": "Analyzing Pre-trained and Fine-tuned Language Models",
        "authors": "Marius Mosbach",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bigpicture-1.10"
    },
    {
        "id": 18763,
        "title": "Transfer learning of pre-trained CNNs on digital transaction fraud detection",
        "authors": "Chandana Gouri Tekkali, Karthika Natarajan",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "This article proposes an artificial intelligence-empowered and efficient detection approach for customers with Severe Failure in Digital Transactions (SFDT) through a deep transfer network learning approach from discretized fraud data. Presently, the Real-time global payment system is suffered primarily by fraudsters based on customer behavior. For the identification of fraud, scientists used many techniques. However, identifying and tracking the customers infected by the fraud takes a significant amount of time. The proposed study employs pre-trained convolution neural network-based (CNN) architectures to find SFDT. CNN is pre-trained on the various network architectures using fraud data. This article contributed to pre-trained networks with newly developed versions ResNet152, DenseNet201, InceptionNetV4, and EfficientNetB7 by integrating the loss function to minimize the error. We run numerous experiments on large data set of credit payment transactions which are public in nature, to determine the high rate of SFDT with our model by comparing accuracy with other fraud detection methods and also proved best in evaluating minimum loss cost.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/kes-230067"
    },
    {
        "id": 18764,
        "title": "Reducing Bias in Pre-Trained Models by Tuning While Penalizing Change",
        "authors": "Niklas Penzel, Gideon Stein, Joachim Denzler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012345800003660"
    },
    {
        "id": 18765,
        "title": "Integration of pre-trained protein language models into geometric deep learning networks",
        "authors": "Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu, Stan Z. Li",
        "published": "2023-8-25",
        "citations": 8,
        "abstract": "AbstractGeometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several preceding studies consider combining these different protein modalities to promote the representation power of geometric neural networks but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines. Strong evidence indicates that the incorporation of protein language models’ knowledge enhances geometric networks’ capacity by a significant margin and can be generalized to complex tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s42003-023-05133-1"
    },
    {
        "id": 18766,
        "title": "Transfer-Learning of a LiDAR Detection Algorithm Pre-Trained on Autonomous Driving Data for Roadside Infrastructure Application",
        "authors": "Daniel Lengerer, Mathias Pechinger, Carsten Markgraf",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422207"
    },
    {
        "id": 18767,
        "title": "Smart transfer learning from pre-trained networks: a case study for infrared classification",
        "authors": "Bethany L. Allik, Nathan Schomer, Cory I. Miller",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663908"
    },
    {
        "id": 18768,
        "title": "WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive Learning",
        "authors": "Ghadeer Mobasher, Wolfgang Müller, Olga Krebs, Michael Gertz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.40"
    },
    {
        "id": 18769,
        "title": "Predicting and Classifying Diabetic Retinopathy (DR) Using 5-Class Label Based on Pre-Trained Deep Learning Models",
        "authors": "S. Karthika, M. Durgadevi",
        "published": "2023-2-27",
        "citations": 0,
        "abstract": "Diabetic Retinopathy (DR) is a condition in which damage to the eyes occurs as a result of diabetes mellitus. It is the most frequent diabetes-related eye condition. It can also cause full blindness and vision loss. With effective eye treatment, the majority of new occurrences of diabetic retinopathy can be reduced. Early detection helps to avoid total vision loss. However, detecting it early can be difficult because it may not present symptoms in the early stages. The wide selection of fundus imaging makes classification challenging, mainly in Proliferative_DR, which includes the formation of new vessels in retina and bleeding. Pre-trained deep learning model is used on the publicly accessible retinal fundus image dataset on kaggle in this paper (APTOS 2019 Blindness Detection). Pre-processing and augmentation procedures are used to increase the accuracy of the models that have been pre-trained. The training accuracy of 8-Layer Convolutional Neural Network (CNN) and MobileNetV2 obtained is 83.07% and 85.21%. Testing accuracy achieved 71.93% using CNN & MobileNetV2 is 83.42%. The most often employed measures, such as the F1 Score, precision, and recall is used to ignore class level of label disagreement, which aids in diagnosing all phases of diabetic retinopathy. The results using a confusion matrix is analyzed, which is useful for categorising different stages of diabetic retinopathy according to severity. It also takes into account the degree of mismatch between the actual and anticipated labels.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4028/p-5z45nx"
    },
    {
        "id": 18770,
        "title": "Exploratory Architectures Analysis of Various Pre-trained Image Classification Models for Deep Learning",
        "authors": "S. Deepa, J. Loveline Zeema, S. Gokila",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12720/jait.15.1.66-78"
    },
    {
        "id": 18771,
        "title": "Olive Leaf Disease Detection via Wavelet Transform and Feature Fusion of Pre-Trained Deep Learning Models",
        "authors": "Mahmood A. Mahmood, Khalaf Alsalem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2024.047604"
    },
    {
        "id": 18772,
        "title": "Elevating Brain Tumor Classification Accuracy with Pre-Trained CNNs and Transfer Learning from MRI Data",
        "authors": "Neeru Saxena, Ajeet Singh, Sanjay Pratap Singh Chauhan",
        "published": "2024-2-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic2pct60090.2024.10486319"
    },
    {
        "id": 18773,
        "title": "Indonesian Hate Speech and Abusive Tweets Classification with Deep Learning Pre-trained Language Models",
        "authors": "Bagus Tri Yulianto Darmawan, Bassamtiano Renaufalgi Irnawan, Yoshimi Suzuki",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic2ie60547.2023.10331354"
    },
    {
        "id": 18774,
        "title": "A pre-trained model selection for transfer learning of remaining useful life prediction of grinding wheel",
        "authors": "Seung-Ho Park, Kyoung-Su Park",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10845-023-02154-9"
    },
    {
        "id": 18775,
        "title": "Unlocking the Potential of Transfer Learning for Leaf Disease Detection: A Comprehensive Study with CNN and Pre-Trained Architectures",
        "authors": "Pravitha P S, V Kumar",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccins58907.2023.10450014"
    },
    {
        "id": 18776,
        "title": "Supervised Contrastive Learning as Multi-Objective Optimization for Fine-Tuning Large Pre-Trained Language Models",
        "authors": "Youness Moukafih, Mounir Ghogho, Kamel Smaili",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095108"
    },
    {
        "id": 18777,
        "title": "Big dermatological data service for precise and immediate diagnosis by utilizing pre-trained learning models",
        "authors": "Mohammed Elbes, Shadi AlZu’bi, Tarek Kanan, Ala Mughaid, Samia Abushanab",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10586-024-04331-8"
    },
    {
        "id": 18778,
        "title": "Multilingual Fairness Analysis and Research based on Pre-Trained Models",
        "authors": "",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.53469/jissr.2023.10(12).07"
    },
    {
        "id": 18779,
        "title": "On the N-gram Approximation of Pre-trained Language Models",
        "authors": "Aravind Krishnan, Jesujoba O. Alabi, Dietrich Klakow",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2182"
    },
    {
        "id": 18780,
        "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
        "authors": "Jungwoo Heo, Chan-yeong Lim, Ju-ho Kim, Hyun-seo Shin, Ha-Jin Yu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-605"
    },
    {
        "id": 18781,
        "title": "Classification of Lung Cancer in Segmented CT Images Using Pre-Trained Deep Learning Models",
        "authors": "P. Deepa, M. Arulselvi, S. Meenakshi Sundaram",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Many Diagnosis systems have been designed and used for diagnosing different types of cancer. Identification of carcinoma at an earlier stage is more important, and it is made possible due to the use of processing of medical images and deep learning techniques. Lung cancer is seen to develop often to be increased, and Computed Tomography (CT) scan images were utilized in the investigation to locate and classify lung cancer and also to determine the severity of cancer. This work is aimed at employing pre-trained deep neural networks for lung cancer classification. A Gaussian-based approach is used to segment CT scan images. This work exploits a transfer learning-based classification method for the chest CT images acquired from Cancer Image Archive and available in the Kaggle platform. The dataset includes lung CT images from the Cancer Image Archive for classifying lung cancer types. Pre-trained models such as VGG, RESNET, and INCEPTION were used to classify segmented chest CT images, and their performance was evaluated using different optimization algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37391/ijeer.120122"
    },
    {
        "id": 18782,
        "title": "Deep Interactive Volume Exploration Through Pre-Trained 3D CNN and Active Learning",
        "authors": "Marwa Salhi, Riadh Ksantini, Belhassen Zouari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011638500003417"
    },
    {
        "id": 18783,
        "title": "Beyond Simple Text Style Transfer: Unveiling Compound Text Style Transfer with Prompt-Based Pre-Trained Language Models",
        "authors": "Shuai Ju, Chenxu Wang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447801"
    },
    {
        "id": 18784,
        "title": "Detection of driver distraction in the Australian naturalistic driving study videos using pre-trained models and transfer learning",
        "authors": "Mohammed Elhenawy, Mahmoud Masoud, Narelle Haworth, Kristie Young, Andry Rakotonirainy, Raphael Grzebieta, Ann Williamson",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.trf.2023.06.016"
    },
    {
        "id": 18785,
        "title": "GPT-Based Models Meet Simulation: How to Efficiently use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": "Philippe J. Giabbanelli",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408017"
    },
    {
        "id": 18786,
        "title": "Enhancing coffee bean classification: a comparative analysis of pre-trained deep learning models",
        "authors": "Esraa Hassan",
        "published": "2024-4-1",
        "citations": 0,
        "abstract": "AbstractCoffee bean production can encounter challenges due to fluctuations in global coffee prices, impacting the economic stability of some countries that heavily depend on coffee production. The primary objective is to evaluate how effectively various pre-trained models can predict coffee types using advanced deep learning techniques. The selection of an optimal pre-trained model is crucial, given the growing popularity of specialty coffee and the necessity for precise classification. We conducted a comprehensive comparison of several pre-trained models, including AlexNet, LeNet, HRNet, Google Net, Mobile V2 Net, ResNet (50), VGG, Efficient, Darknet, and DenseNet, utilizing a coffee-type dataset. By leveraging transfer learning and fine-tuning, we assess the generalization capabilities of the models for the coffee classification task. Our findings emphasize the substantial impact of the pre-trained model choice on the model's performance, with certain models demonstrating higher accuracy and faster convergence than conventional alternatives. This study offers a thorough evaluation of pre-trained architectural models regarding their effectiveness in coffee classification. Through the evaluation of result metrics, including sensitivity (1.0000), specificity (0.9917), precision (0.9924), negative predictive value (1.0000), accuracy (1.0000), and F1 score (0.9962), our analysis provides nuanced insights into the intricate landscape of pre-trained models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09623-z"
    },
    {
        "id": 18787,
        "title": "An Ensemble Voting Method of Pre-Trained Deep Learning Models for Orchid Recognition",
        "authors": "Chia-Ho Ou, Yi-Nuo Hu, Dong-Jie Jiang, Po-Yen Liao",
        "published": "2023-4-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/syscon53073.2023.10131263"
    },
    {
        "id": 18788,
        "title": "Constraint Learning to Define Trust Regions in Optimization over Pre-Trained Predictive Models",
        "authors": "Chenbo Shi, Mohsen Emadikhiav, Leonardo Lozano, David Bergman",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": " There is a recent proliferation of research on the integration of machine learning and optimization. One expansive area within this research stream is optimization over pre-trained predictive models, which proposes the use of pre-trained predictive models as surrogates for uncertain or highly complex objective functions. In this setting, features of the predictive models become decision variables in the optimization problem. Despite a recent surge in publications in this area, only a few papers note the importance of incorporating trust-region considerations in this decision-making pipeline, that is, enforcing solutions to be similar to the data used to train the predictive models. Without such constraints, the evaluation of the predictive model at solutions obtained from optimization cannot be trusted and the practicality of the solutions may be unreasonable. In this paper, we provide an overview of the approaches appearing in the literature to construct a trust region and propose three alternative approaches. Our numerical evaluation highlights that trust-region constraints learned through our newly proposed approaches compare favorably with previously suggested approaches, both in terms of solution quality and computational time.  History: Accepted by Pascal Van Hentenryck, Area Editor for Computational Modeling: Methods & Analysis.  Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2022.0312 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2022.0312 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ . ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/ijoc.2022.0312"
    },
    {
        "id": 18789,
        "title": "White Blood Cell Classification Using Pre-Trained Deep Neural Networks and Transfer Learning",
        "authors": "Rojalin Biswal, Pradeep Kumar Mallick, Amiya Ranjan Panda, Gyoo Soo Chae, Annapurna Mishra",
        "published": "2023-9-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccpis59145.2023.10291642"
    },
    {
        "id": 18790,
        "title": "Adapting transfer learning models to dataset through pruning and Avg-TopK pooling",
        "authors": "Cuneyt OZDEMIR",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractThis study focuses on efficiently adapting transfer learning models to address the challenges of creating customized deep learning models for specific datasets. Designing a model from scratch can be time-consuming and complex due to factors like model complexity, size, and dataset structure. To overcome these obstacles, a novel approach is proposed using transfer learning models. The proposed method involves identifying relevant layers in transfer learning models and removing unnecessary ones using a layer-based variance pruning technique. This results in the creation of new models with improved computational efficiency and classification performance. By streamlining the models through layer-based variance pruning, the study achieves enhanced accuracy and faster computation. Experiments were conducted using the COVID-19 dataset and well-known transfer learning models, including InceptionV3, ResNet50V2, DenseNet201, VGG16, and Xception to validate the approach. Among these models, the variance-based layer pruning technique was applied to InceptionV3 and DenseNet201, yielding the best results. When these pruned models were combined with the new pooling layer, Avg-TopK, the proposed method achieved an outstanding image classification accuracy of 99.3%. Comparisons with previous models and literature studies indicate that the proposed approach outperforms existing methods, showcasing state-of-the-art performance. This high-performance approach provides great potential for diagnosing COVID-19 and monitoring disease progression, especially on hardware-limited devices. By leveraging transfer learning models, pruning, and efficient pooling techniques, the study presents a promising strategy for tackling challenges in custom model design, leading to exceptional results in such as image classification and segmentation tasks. The proposed methodology holds the potential to yield exceptional outcomes across a spectrum of tasks, encompassing disciplines such as image classification and segmentation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09484-6"
    },
    {
        "id": 18791,
        "title": "Bidirectional English-Marathi Translation using Pretrained Models: A Comparative Study of Different Pre-Trained Models",
        "authors": "Lekhraj Saini, Deepti Vidhyarthi",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425770"
    },
    {
        "id": 18792,
        "title": "COVID-19 diagnosis: A comprehensive review of pre-trained deep learning models based on feature extraction algorithm",
        "authors": "Rahul Gowtham Poola, Lahari Pl, Siva Sankar Y",
        "published": "2023-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rineng.2023.101020"
    },
    {
        "id": 18793,
        "title": "Exploring One-Shot Semi-supervised Federated Learning with Pre-trained Diffusion Models",
        "authors": "Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Recently, semi-supervised federated learning (semi-FL) has been proposed to handle the commonly seen real-world scenarios with labeled data on the server and unlabeled data on the clients. However, existing methods face several challenges such as communication costs, data heterogeneity, and training pressure on client devices. To address these challenges, we introduce the powerful diffusion models (DM) into semi-FL and propose FedDISC, a Federated Diffusion-Inspired Semi-supervised Co-training method. Specifically, we first extract prototypes of the labeled server data and use these prototypes to predict pseudo-labels of the client data. For each category, we compute the cluster centroids and domain-specific representations to signify the semantic and stylistic information of their distributions. After adding noise, these representations are sent back to the server, which uses the pre-trained DM to generate synthetic datasets complying with the client distributions and train a global model on it. With the assistance of vast knowledge within DM, the synthetic datasets have comparable quality and diversity to the client images, subsequently enabling the training of global models that achieve performance equivalent to or even surpassing the ceiling of supervised centralized training. FedDISC works within one communication round, does not require any local training, and involves very minimal information uploading, greatly enhancing its practicality. Extensive experiments on three large-scale datasets demonstrate that FedDISC effectively addresses the semi-FL problem on non-IID clients and outperforms the compared SOTA methods. Sufficient visualization experiments also illustrate that the synthetic dataset generated by FedDISC exhibits comparable diversity and quality to the original client dataset, with a neglectable possibility of leaking privacy-sensitive information of the clients.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29568"
    },
    {
        "id": 18794,
        "title": "Interpreting Art by Leveraging Pre-Trained Models",
        "authors": "Niklas Penzel, Joachim Denzler",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10216010"
    },
    {
        "id": 18795,
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": "Liwen Hou, David Smith",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.230"
    },
    {
        "id": 18796,
        "title": "Palmprint Recognition Using Pre-Trained Convolutional Neural Networks",
        "authors": "Nader Ebrahimpour, Faruk Baturalp Günay",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.36287/setsci.6.1.018"
    },
    {
        "id": 18797,
        "title": "Exploring Robust Overfitting for Pre-trained Language Models",
        "authors": "Bin Zhu, Yanghui Rao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.340"
    },
    {
        "id": 18798,
        "title": "Modularized Zero-shot VQA with Pre-trained Models",
        "authors": "Rui Cao, Jing Jiang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.5"
    },
    {
        "id": 18799,
        "title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models",
        "authors": "Qingyi Liu, Jinghui Qin, Wenxuan Ye, Hao Mou, Yuxuan He, Keze Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Recently, arbitrary text style transfer (TST) has made significant progress with the paradigm of prompt learning. In this paradigm, researchers often design or search for a fixed prompt for any input. However, existing evidence shows that large language models (LLMs) are prompt-sensitive and it is sub-optimal to apply the same prompt to any input for downstream TST tasks. Besides, the prompts obtained by searching are often unreadable and unexplainable to humans. To address these issues, we propose an Adaptive Prompt Routing (APR) framework to adaptively route prompts from a human-readable prompt set for various input texts and given styles. Specifically, we first construct a candidate prompt set of diverse and human-readable prompts for the target style. This set consists of several seed prompts and their variants paraphrased by an LLM. Subsequently, we train a prompt routing model to select the optimal prompts efficiently according to inputs. The adaptively selected prompt can guide the LLMs to perform a precise style transfer for each input sentence while maintaining readability for humans. Extensive experiments on 4 public TST benchmarks over 3 popular LLMs (with parameter sizes ranging from 1.5B to 175B) demonstrate that our APR achieves superior style transfer performances, compared to the state-of-the-art prompt-based and fine-tuning methods. The source code is available at https://github.com/DwyaneLQY/APR",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i17.29832"
    },
    {
        "id": 18800,
        "title": "Breast Cancer Detection on Histopathology Images Using Pre-trained Computer Vision Models",
        "authors": "Daffa Farras Fauzan, Rahmat Fauzi, Oktariani Nurul Pratiwi, José Manuel Ferreira Machado",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icadeis58666.2023.10270900"
    },
    {
        "id": 18801,
        "title": "$$\\cal{Y}$$-Tuning: an efficient tuning paradigm for large-scale pre-trained models via label representation learning",
        "authors": "Yitao Liu, Chenxin An, Xipeng Qiu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-3131-8"
    },
    {
        "id": 18802,
        "title": "Prompt learning for metonymy resolution: Enhancing performance with internal prior knowledge of pre-trained language models",
        "authors": "Biao Zhao, Weiqiang Jin, Yu Zhang, Subin Huang, Guang Yang",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110928"
    },
    {
        "id": 18803,
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models",
        "authors": "Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, Zenglin Xu",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.632"
    },
    {
        "id": 18804,
        "title": "DeBERTa-BiLSTM: A multi-label classification model of Arabic medical questions using pre-trained models and deep learning",
        "authors": "Bushra Salem Al-Smadi",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2024.107921"
    },
    {
        "id": 18805,
        "title": "A Framework for Dress Code Monitoring System using Transfer Learning from Pre-Trained YOLOv4 Model",
        "authors": "Divyangini Agarwal, Prashant Gupta, Naived George Eapen",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icetet-sip58143.2023.10151460"
    },
    {
        "id": 18806,
        "title": "Comparative Analysis of Deep Learning Pre- Trained Models and Machine Learning Classifiers for Accurate Cervical Cancer Diagnosis using Colposcopy Images",
        "authors": "Madhura Malhari Kalbhor, Swati Vijay Shinde",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccubea58933.2023.10392024"
    },
    {
        "id": 18807,
        "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
        "authors": "Joshua Jansen van Vüren, Thomas Niesler",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1841"
    },
    {
        "id": 18808,
        "title": "TOCOL: Improving Contextual Representation of Pre-trained Language Models via Token-Level Contrastive Learning",
        "authors": "Keheng Wang, Chuantao Yin, Rumei Li, Sirui Wang, Yunsen Xian, Wenge Rong, Zhang Xiong",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302506"
    },
    {
        "id": 18809,
        "title": "Exploiting Prompt Learning with Pre-Trained Language Models for Alzheimer’s Disease Detection",
        "authors": "Yi Wang, Jiajun Deng, Tianzi Wang, Bo Zheng, Shoukang Hu, Xunying Liu, Helen Meng",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095993"
    },
    {
        "id": 18810,
        "title": "Efficient utilization of pre-trained models: A review of sentiment analysis via prompt learning",
        "authors": "Kun Bu, Yuanchao Liu, Xiaolong Ju",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.111148"
    },
    {
        "id": 18811,
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": "Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-910"
    },
    {
        "id": 18812,
        "title": "Efficiently Robustify Pre-Trained Models",
        "authors": "Nishant Jain, Harkirat Behl, Yogesh Singh Rawat, Vibhav Vineet",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00507"
    },
    {
        "id": 18813,
        "title": "Emotion Classification using Generative Pre-trained Embedding and Machine Learning",
        "authors": "Geeta Pattun, Pradeep Kumar",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmlant59547.2023.10372980"
    },
    {
        "id": 18814,
        "title": "Performance Evaluation of Pre-Trained Convolutional Neural Network and Transfer Learning for Classification of Spices and Herbal Medicines in Madura",
        "authors": "Muhlis Tahir, Abdul Azis Jakfar, Dian Budi Elnursa, Rio Meisya Resnanda",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icosnikom60230.2023.10364518"
    },
    {
        "id": 18815,
        "title": "Quantum Transfer Learning Using the Large-Scale Unsupervised Pre-Trained Model Wavlm-Large for Synthetic Speech Detection",
        "authors": "Ruoyu Wang, Jun Du, Tian Gao",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096141"
    },
    {
        "id": 18816,
        "title": "Lung Cancer Detection and Classification using Transfer Learning with Pre-trained VGG19 Convolutional Neural Networks",
        "authors": "Surajit Das, Lavanya G, Sangem Jaya Prakash, Niladri Sekhar Dey, Jeshwanth Panuganti, Rokati Poojitha",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icefeet59656.2023.10452195"
    },
    {
        "id": 18817,
        "title": "Artificial intelligence in cystoscopic bladder cancer classification based on transfer learning with a pre-trained convolutional neural network without natural images",
        "authors": "Ryuunosuke Kounosu, Wonjik Kim, Atsushi Ikeda, Hirokazu Nosato, Yuu Nakajima",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005605"
    },
    {
        "id": 18818,
        "title": "Comparison of Pre-trained vs Custom-trained Word Embedding Models for Word Sense Disambiguation",
        "authors": "Muhammad Farhat Ullah, Ali Saeed, Naveed Hussain",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "\n\nThe prime objective of word sense disambiguation (WSD) is to develop such machines that can automatically recognize the actual meaning (sense) of ambiguous words in a sentence. WSD can improve various NLP and HCI challenges. Researchers explored a wide variety of methods to resolve this issue of sense ambiguity. However, majorly, their focus was on English and some other well-reputed languages. Urdu with more than 300 million users and a large amount of electronic text available on the web is still unexplored. In recent years, for a variety of Natural Language Processing tasks, word embedding methods have proven extremely successful. This study evaluates, compares, and applies a variety of word embedding approaches to Urdu Word embedding (both Lexical Sample and All-Words), including pre-trained (Word2Vec, Glove, and FastText) as well as custom-trained (Word2Vec, Glove, and FastText trained on the Ur-Mono corpus). Two benchmark corpora are used for the evaluation in this study: (1) the UAW-WSD-18 corpus and (2) the ULS-WSD-18 corpus. For Urdu All-Words WSD tasks, top results have been achieved (Accuracy=60.07 and F1=0.45) using pre-trained FastText. For the Lexical Sample, WSD has been achieved (Accuracy=70.93 and F1=0.60) using custom-trained GloVe word embedding method.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.14201/adcaij.31084"
    },
    {
        "id": 18819,
        "title": "Dissecting Adversarial Attacks: A Comparative Analysis of Adversarial Perturbation Effects on Pre-Trained Deep Learning Models",
        "authors": "Rekha Kumari, Tushar Bhatia, Peeyush Kumar Singh, Kanishk Vikram Singh",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "It is well known that the majority of neural networks widely employed today are extremely susceptible to adversarial perturbations which causes the misclassification of the output. This, in turn, can cause severe security concerns. In this paper, we meticulously evaluate the robustness of prominent pre-trained deep learning models against images that are modified with the Fast Gradient Sign Method (FGSM) attack. For this purpose, we have selected the following models: InceptionV3, InceptionResNetV2, ResNet152V2, Xception, DenseNet121, and MobileNetV2. All these models are pre-trained on ImageNet, and hence, we use our custom 10- animals test dataset to produce clean as well as misclassified output. Rather than focusing solely on prediction accuracy, our study uniquely quantifies the perturbation required to alter output labels, shedding light on the models' susceptibility to misclassification. The outcomes underscore varying vulnerabilities among the models to FGSM attacks, providing nuanced insights crucial for fortifying neural networks against adversarial threats.  Key Words: Adversarial Perturbations, Deep Learning, ImageNet, FGSM Attack, Neural Networks, Pre-trained Models",
        "keywords": "",
        "link": "http://dx.doi.org/10.55041/ijsrem27337"
    },
    {
        "id": 18820,
        "title": "Optimizing Pre-Trained Models of Deep Learning for Identification of Plant Disease",
        "authors": "S. Prakadeswaran, A. Bazilla Banu, R. Senthil Kumar, S. Gokul Raj",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "The Plant diseases should be identified early to prevent the economic loss of farmers and ensure the availability of food for humans. The plant disease identification can be automated by using the Artificial Intelligence techniques. Researchers have proposed many deep learning methods for identifying plant diseases. Deep learning models use an increased number of parameters, it requires higher computational power, training a deep learning model from start requires more time. In this article we utilized transfer learning along with fine tuning for identification of plant diseases. Cassava plant disease dataset was utilised for training. and evaluate the suggested model. The performance accuracy achieved by Resnet50 is 73.12 % and fine-tuned Resnet50 is 80.84 %. The fine-tuned model achieves greater accuracy with a lesser amount of parameters\r\nImpact Statement–Artificial Intelligence is evolving all around the world. The AI techniques are used to automate the process of plant disease identification. Traditional methods are not accurate and time consuming. To help the farmers in diagnosing plant disease and stop economic loss to them, we employ deep learning models to do the work. The pretrained models predict the plant diseases, further we fine-tune them in order to get high accuracy. Early identification of the diseases accurately will avoid loss and improve productivity of the crops.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10s.7647"
    },
    {
        "id": 18821,
        "title": "Source Code Plagiarism Detection with Pre-Trained Model Embeddings and Automated Machine Learning",
        "authors": "Fahad Ebrahim,  , Mike Joy,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_034"
    },
    {
        "id": 18822,
        "title": "Target Speech Extraction with Pre-Trained Self-Supervised Learning Models",
        "authors": "Junyi Peng, Marc Delcroix, Tsubasa Ochiai, Oldřich Plchot, Shoko Araki, Jan Černocký",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448315"
    },
    {
        "id": 18823,
        "title": "Probabilistic end-to-end irradiance forecasting through pre-trained deep learning models using all-sky-images",
        "authors": "Samer Chaaraoui, Sebastian Houben, Stefanie Meilinger",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "Abstract. This work proposes a novel approach for probabilistic end-to-end all-sky imager-based nowcasting with horizons of up to 30 min using an ImageNet pre-trained deep neural network. The method involves a two-stage approach. First, a backbone model is trained to estimate the irradiance from all-sky imager (ASI) images. The model is then extended and retrained on image and parameter sequences for forecasting. An open access data set is used for training and evaluation. We investigated the impact of simultaneously considering global horizontal (GHI), direct normal (DNI), and diffuse horizontal irradiance (DHI) on training time and forecast performance as well as the effect of adding parameters describing the irradiance variability proposed in the literature. The backbone model estimates current GHI with an RMSE and MAE of 58.06 and 29.33 W m−2, respectively. When extended for forecasting, the model achieves an overall positive skill score reaching 18.6 % compared to a smart persistence forecast. Minor modifications to the deterministic backbone and forecasting models enables the architecture to output an asymmetrical probability distribution and reduces training time while leading to similar errors for the backbone models. Investigating the impact of variability parameters shows that they reduce training time but have no significant impact on the GHI forecasting performance for both deterministic and probabilistic forecasting while simultaneously forecasting GHI, DNI, and DHI reduces the forecast performance.\n                    ",
        "keywords": "",
        "link": "http://dx.doi.org/10.5194/asr-20-129-2024"
    },
    {
        "id": 18824,
        "title": "Learnable Sparsity Structured Pruning for Acoustic Pre-trained Models",
        "authors": "Siyuan Wang, Haoyu Wang, Jian Li, Wei-Qiang Zhang",
        "published": "2023-7-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3614008.3614020"
    },
    {
        "id": 18825,
        "title": "Combining State-of-the-Art Pre-Trained Deep Learning Models: A Noble Approach for Skin Cancer Detection Using Max Voting Ensemble",
        "authors": "Md. Mamun Hossain, Md. Moazzem Hossain, Most. Binoee Arefin, Fahima Akhtar, John Blake",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Skin cancer poses a significant healthcare challenge, requiring precise and prompt diagnosis for effective treatment. While recent advances in deep learning have dramatically improved medical image analysis, including skin cancer classification, ensemble methods offer a pathway for further enhancing diagnostic accuracy. This study introduces a cutting-edge approach employing the Max Voting Ensemble Technique for robust skin cancer classification on ISIC 2018: Task 1-2 dataset. We incorporate a range of cutting-edge, pre-trained deep neural networks, including MobileNetV2, AlexNet, VGG16, ResNet50, DenseNet201, DenseNet121, InceptionV3, ResNet50V2, InceptionResNetV2, and Xception. These models have been extensively trained on skin cancer datasets, achieving individual accuracies ranging from 77.20% to 91.90%. Our method leverages the synergistic capabilities of these models by combining their complementary features to elevate classification performance further. In our approach, input images undergo preprocessing for model compatibility. The ensemble integrates the pre-trained models with their architectures and weights preserved. For each skin lesion image under examination, every model produces a prediction. These are subsequently aggregated using the max voting ensemble technique to yield the final classification, with the majority-voted class serving as the conclusive prediction. Through comprehensive testing on a diverse dataset, our ensemble outperformed individual models, attaining an accuracy of 93.18% and an AUC score of 0.9320, thus demonstrating superior diagnostic reliability and accuracy. We evaluated the effectiveness of our proposed method on the HAM10000 dataset to ensure its generalizability. Our ensemble method delivers a robust, reliable, and effective tool for the classification of skin cancer. By utilizing the power of advanced deep neural networks, we aim to assist healthcare professionals in achieving timely and accurate diagnoses, ultimately reducing mortality rates and enhancing patient outcomes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/diagnostics14010089"
    },
    {
        "id": 18826,
        "title": "A Comparative Study of Different Pre-Trained Deep Learning Models and Custom CNN for Pancreatic Tumor Detection",
        "authors": "Muhammed Zavalsız, Sleiman Alhajj, Kashfia Sailunaz, Tansel Ozyer, Reda Alhajj",
        "published": "2023",
        "citations": 2,
        "abstract": "Artificial Intelligence and its sub-branches like Machine Learning (ML) and Deep Learning (DL) applications have the potential to have positive effects that can directly affect human life. Medical imaging is briefly making the internal structure of the human body visible with various methods. With deep learning models, cancer detection, which is one of the most lethal diseases in the world, can be made possible with high accuracy. Pancreatic Tumor detection, which is one of the cancer types with the highest fatality rate, is one of the main targets of this project, together with the data set of Computed Tomography images, which is one of the medical imaging techniques and has an effective structure in Pancreatic Cancer imaging. In the field of image classification, which is a computer vision task, the transfer learning technique, which has gained popularity in recent years, has been applied quite frequently. Using pre-trained models were previously trained on a fairly large dataset and using them on medical images is common nowadays. The main objective of this article is to use this method, which is very popular in the medical imaging field, in the detection of PDAC, one of the deadliest types of pancreatic cancer, and to investigate how it per- forms compared to the custom model created and trained from scratch. The pre-trained models which are used in this project are VGG-16 and ResNet, which are popular Convolutional Neutral Network models, for Pancreatic Tumor Detection task. With the use of these models, early diagnosis of pancreatic cancer, which progresses insidiously and therefore does not spread to neighboring tissues and organs when the treatment process is started, may be possible. Due to the abundance of medical images reviewed by medical professionals, which is one of the main causes for heavy workload of healthcare systems, this application can assist radiologists and other specialists in Pancreatic Tumor detection by providing faster and more accurate method",
        "keywords": "",
        "link": "http://dx.doi.org/10.34028/iajit/20/3a/9"
    },
    {
        "id": 18827,
        "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
        "authors": "Shotaro Ishihara",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.23"
    },
    {
        "id": 18828,
        "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
        "authors": "Siyu Ren, Kenny Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.573"
    },
    {
        "id": 18829,
        "title": "The severity level classification of Fusarium wilt of chickpea by pre-trained deep learning models",
        "authors": "Tolga Hayit, Ali Endes, Fatma Hayit",
        "published": "2023-10-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42161-023-01520-z"
    },
    {
        "id": 18830,
        "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost",
        "authors": "Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, Rob Voigt, Jie Yang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly ``translate'' pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x less pre-training efforts. Aiming at the Leave No One Behind Principle (LNOB), our approach manages to reduce inequalities between languages and energy consumption greatly. We make our codes and models publicly available at https://github.com/qcznlp/GreenPLMs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/698"
    },
    {
        "id": 18831,
        "title": "Adapting the pre-trained convolutional neural networks to improve the anomaly detection and classification in mammographic images",
        "authors": "Abeer Saber, Abdelazim G. Hussien, Wael A. Awad, Amena Mahmoud, Alaa Allakany",
        "published": "2023-9-9",
        "citations": 1,
        "abstract": "AbstractMortality from breast cancer (BC) is among the top causes of cancer death in women. BC can be effectively treated when diagnosed early, improving the likelihood that a patient will survive. BC masses and calcification clusters must be identified by mammography in order to prevent disease effects and commence therapy at an early stage. A mammography misinterpretation may result in an unnecessary biopsy of the false-positive results, lowering the patient’s odds of survival. This study intends to improve breast mass detection and identification in order to provide better therapy and reduce mortality risk. A new deep-learning (DL) model based on a combination of transfer-learning (TL) and long short-term memory (LSTM) is proposed in this study to adequately facilitate the automatic detection and diagnosis of the BC suspicious region using the 80–20 method. Since DL designs are modelled to be problem-specific, TL applies the knowledge gained during the solution of one problem to another relevant problem. In the presented model, the learning features from the pre-trained networks such as the squeezeNet and DenseNet are extracted and transferred with the features that have been extracted from the INbreast dataset. To measure the proposed model performance, we selected accuracy, sensitivity, specificity, precision, and area under the ROC curve (AUC) as our metrics of choice. The classification of mammographic data using the suggested model yielded overall accuracy, sensitivity, specificity, precision, and AUC values of 99.236%, 98.8%, 99.1%, 96%, and 0.998, respectively, demonstrating the model’s efficacy in detecting breast tumors.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-41633-0"
    },
    {
        "id": 18832,
        "title": "Enhancement of virtual data quality using pre-trained Bayesian transfer learning under inaccurate and insufficient measurement data",
        "authors": "Jaewook Lee, Jinha Heo, Jongsoo Lee",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aei.2023.102241"
    },
    {
        "id": 18833,
        "title": "Improving Braille–Chinese translation with jointly trained and pre-trained language models",
        "authors": "Tianyuan Huang, Wei Su, Lei Liu, Chuan Cai, Hailong Yu, Yongna Yuan",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.displa.2024.102660"
    },
    {
        "id": 18834,
        "title": "Leaf Image Classification Based on Pre-trained Convolutional Neural Network Models",
        "authors": "Yunus CAMGÖZLÜ, Yakup KUTLU",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "It is important to identify a high-performance model that can classify all leaves and even differentiate according to regional variations of the same leaf type. In this study, a leaf classification model was created using 5 different datasets with different number of images and compared with models. For this purpose, 4 different pre-trained models called VGG16, InceptionV3, MobileNet and DenseNet are used. In addition, a new model was proposed and model training was carried out using these datasets . Using the all models, inputs are transformed into feature vectors by parameter transfer method and used for classification with the nearest neighbor algorithm and support vector machine. The performance of the classifications were compared with similar studies in the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28978/nesciences.1405175"
    },
    {
        "id": 18835,
        "title": "Revolutionizing Translation with AI: Unravelling Neural Machine Translation and Generative Pre-Trained Large Language Models",
        "authors": "Sai Cheong Siu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499768"
    },
    {
        "id": 18836,
        "title": "Ensemble fusion model for improved lung abnormality classification: Leveraging pre-trained models",
        "authors": "Suresh Kumar Samarla, Maragathavalli P",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mex.2024.102640"
    },
    {
        "id": 18837,
        "title": "Enhancing Pre-Trained Transfer Learning Performance Using Automated Brain Tumor  Classification",
        "authors": "Sivakumar Karuppan, Kalpana G, Sika K, Abhirami J S",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4108/eai.23-11-2023.2343173"
    },
    {
        "id": 18838,
        "title": "Analyzing Out-of-Domain Generalization Performance of Pre-Trained Segmentation Models",
        "authors": "Johnson Zhong",
        "published": "2023-2-16",
        "citations": 0,
        "abstract": "Artists illustrate objects to various degrees of complexity. As the amount of detail or the similarity to reality of a depiction decreases, the object tends to be reduced to its simplest, most relevant higher-level features (Harrison, 1981). One of the reasons Deep Neural Networks (DNN) may fail to identify objects in an image is that models are unable to recognize the order of importance of features such as shape, depth, or color within an image, which means even the most minute distortions of pixels within an image that would be imperceptible to humans would greatly impact the performance of the object detection models (Eykholt et al., 2018). However, by training DNN on artworks where the most prominent features defining specific objects are emphasized, perhaps a model can be made to be more resilient against small-scale changes in an image. In this paper, the correlation between the level of similarity to reality of images and artworks of an object and the accuracy of object detection models is investigated to test the ability of object detection models in identifying the most salient features of a particular object. The results of this report can help outline the efficacy of models only trained on real images in identifying increasingly abstract artworks that have simplified an object to its most prominent features. The experiment shows that the accuracies of models decrease as the images or illustrations provided become more abstract or simplified, which suggests the higher level features that identify a particular object are different in object detection models and humans. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.5539/nct.v8n1p1"
    },
    {
        "id": 18839,
        "title": "Multi-task Learning Approach Based on Pre-trained Language Models Using Temporal Relations",
        "authors": "Chae-Gyun Lim, Kyo-Joong Oh, Ho-Jin Choi",
        "published": "2023-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5626/jok.2023.50.1.25"
    },
    {
        "id": 18840,
        "title": "Fuzzy Recurrence Plots for Shallow Learning-Based Blockage Detection in a Centrifugal Pump Using Pre-Trained Image Recognition Models",
        "authors": "Nagendra Singh Ranawat, Jatin Prakash, Ankur Miglani, Pavan Kumar Kankar",
        "published": "2023-10-1",
        "citations": 1,
        "abstract": "Abstract\nRags, dusts, foreign particles, etc., are the primary cause of blockage in the centrifugal pump and deteriorate the performance. This study elaborates an experimental and data-driven methodology to identify suction, discharge, and simultaneous occurrence of both blockages. The discharge pressure signals are acquired and denoised using CEEMD. The fuzzy recurrence plots obtained from denoised signals are attempted to classify using three pre-trained models: Xception, GoogleNet, and Inception. None of these models are trained on such images; thus, features are extracted from different pooling layers which include shallow features too. The features extracted from different layers are fed to four shallow learning classifiers: Quadratic SVM, Weighted k-nearest network, Narrow Neural network, and subspace discriminant classifier. The study finds that subspace discriminant achieves the highest accuracy of 97.8% when trained using features from second pooling of Xception model. Furthermore, this proposed methodology is implemented at other blockage conditions of the pump. The subspace discriminant analysis outperforms the other selected shallow classifier with an accuracy of 93% for the features extracted from the first pooling layer of the Xception model. Therefore, this study demonstrates an efficient method to identify pump blockage using pre-trained and shallow classifiers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1115/1.4062425"
    },
    {
        "id": 18841,
        "title": "Detection of Alzheimer's Disease Stages Using Pre-Trained Deep Learning Approaches",
        "authors": "Shruti Pallawi, Dushyant Kumar Singh",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccmla58983.2023.10346730"
    },
    {
        "id": 18842,
        "title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
        "authors": "Nikitha Karkera, Sathwik Acharya, Sucheendra K. Palaniappan",
        "published": "2023-7-19",
        "citations": 3,
        "abstract": "Abstract\nBackground\nThe growing recognition of the microbiome’s impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature.\n\nResults\nIn this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over $$>0.8$$\n\n>\n0.8\n\n compared to the previous best of  0.74.\n\nConclusion\nOverall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12859-023-05411-z"
    },
    {
        "id": 18843,
        "title": "CLOP-hERG: The Contrastive Learning Optimized Pre-Trained Model for Representation Learning in Predicting Drug-Induced hERG Channel Blockers",
        "authors": "Shida He, Xiucai Ye, Tetsuya Sakurai",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "During drug development, ensuring that drug molecules do not block the hERG (human Ether-à-go-go-Related Gene) channel is critical. If this channel is blocked, it can cause many cardiovascular-related diseases. However, traditional experimental detection methods are expensive and time-consuming. In this work, we proposed a novel deep learning framework CLOP-hERG, that combines contrastive learning with the RoBERTa pre-trained model to predict whether drug molecules will block the hERG channel. We employed data augmentation techniques on molecular structures to ensure that our model can capture the multifaceted information of the molecules. Besides, we used a contrastive learning strategy to enable the model to learn meaningful molecular features from large unlabeled datasets. The RoBERTa pre-trained model played a pivotal role in this process, giving our model with a robust representational learning capability. The model, obtained through contrastive learning, was further fine-tuned to achieve high-precision prediction of hERG blockers. Through a series of experiments, we demonstrated the effectiveness of CLOP-hERG. This work provides a novel and effective strategy for predicting hERG blockers and provides some insights for other similar pharmaceutical tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47852/bonviewmedin42022049"
    },
    {
        "id": 18844,
        "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
        "authors": "Sweta Agrawal, Marine Carpuat",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.790"
    },
    {
        "id": 18845,
        "title": "Generative Pre-trained Transformer (GPT) Models for Irony Detection and Classification",
        "authors": "Mustafa Ulvi Aytekin, O. Ayhan Erdem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391005"
    },
    {
        "id": 18846,
        "title": "Pre-Trained Language Models and Their Applications",
        "authors": "Haifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, Yu Sun",
        "published": "2023-6",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eng.2022.04.024"
    },
    {
        "id": 18847,
        "title": "Token-level Identification of Multiword Expressions using Pre-trained Multilingual Language Models",
        "authors": "Raghuraman Swaminathan, Paul Cook",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.mwe-1.1"
    },
    {
        "id": 18848,
        "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
        "authors": "Yangyi Chen, Xingyao Wang, Heng Ji",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.624"
    },
    {
        "id": 18849,
        "title": "Predicting Defective and Good Tyre Quality Status with Pre-Trained CNN Models",
        "authors": "Yavuz Selim Taspinar",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mosicom59118.2023.10458796"
    },
    {
        "id": 18850,
        "title": "Adapting Pre-Trained Self-Supervised Learning Model for Speech Recognition with Light-Weight Adapters",
        "authors": "Xianghu Yue, Xiaoxue Gao, Xinyuan Qian, Haizhou Li",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "Self-supervised learning (SSL) is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks. However, effectively incorporating a pre-trained SSL model into an automatic speech recognition (ASR) system remains challenging. In this paper, we propose a network architecture with light-weight adapters to adapt a pre-trained SSL model for an end-to-end (E2E) ASR. An adapter is introduced in each SSL network layer and trained on the downstream ASR task, while the parameters of the pre-trained SSL network layers remain unchanged. By carrying over all pre-trained parameters, we avoid the catastrophic forgetting problem. At the same time, we allow the network to quickly adapt to ASR task with light-weight adapters. The experiments using LibriSpeech and Wall Street Journal (WSJ) datasets show that (1) the proposed adapter-based fine-tuning consistently outperforms full-fledged training in low-resource scenarios, with up to 17.5%/12.2% relative word error rate (WER) reduction on the 10 min LibriSpeech split; (2) the adapter-based adaptation also shows competitive performance in high-resource scenarios, which further validates the effectiveness of the adapters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13010190"
    },
    {
        "id": 18851,
        "title": "DrugFinder: Druggable Protein Identification Model Based on Pre-Trained Models and Evolutionary Information",
        "authors": "Mu Zhang, Fengqiang Wan, Taigang Liu",
        "published": "2023-5-25",
        "citations": 1,
        "abstract": "The identification of druggable proteins has always been the core of drug development. Traditional structure-based identification methods are time-consuming and costly. As a result, more and more researchers have shifted their attention to sequence-based methods for identifying druggable proteins. We propose a sequence-based druggable protein identification model called DrugFinder. The model extracts the features from the embedding output of the pre-trained protein model Prot_T5_Xl_Uniref50 (T5) and the evolutionary information of the position-specific scoring matrix (PSSM). Afterwards, to remove redundant features and improve model performance, we used the random forest (RF) method to select features, and the selected features were trained and tested on multiple different machine learning classifiers, including support vector machines (SVM), RF, naive Bayes (NB), extreme gradient boosting (XGB), and k-nearest neighbors (KNN). Among these classifiers, the XGB model achieved the best results. DrugFinder reached an accuracy of 94.98%, sensitivity of 96.33% and specificity of 96.83% on the independent test set, which is much better than the results from existing identification methods. Our model also performed well on another additional test set related to tumors, achieving an accuracy of 88.71% and precision of 93.72%. This further demonstrates the strong generalization capability of the model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16060263"
    },
    {
        "id": 18852,
        "title": "Study for Performance of Un-Pretrained and Pre-trained Models based on CNN",
        "authors": "Bingsen Wang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": "In recent years, as the accuracy of deep learning algorithms in image classification tasks exceeds that of the human brain, Artificial Intelligence (AI) auxiliary diagnosis systems have attracted more and more attention. In this paper, some commonly used Convolutional Neural Network (CNN) models e.g. MobileNet, VGG and ResNet are trained and compared on the cancer detection dataset, and it is found that the pre-trained models based on the idea of the transfer learning perform better than the newly trained models in terms of training speed and model performance. Thus, it can be seen that the transfer learning method has great potential in the field of cancer diagnosis. This study provides some experimental support and suggestions on how to further improve the property of the transfer learning method in the field of cancer diagnosis. Meantime, the performance of VGG19 can be proved to be better compared to other models (i.e., MobileNet and ResNet).",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6486"
    },
    {
        "id": 18853,
        "title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation",
        "authors": "Chunliu Wang, Huiyuan Lai, Malvina Nissim, Johan Bos",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.345"
    },
    {
        "id": 18854,
        "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
        "authors": "Santosh Kesiraju, Marek Sarvaš, Tomáš Pavlíček, Cécile Macaire, Alejandro Ciuba",
        "published": "2023-8-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2506"
    },
    {
        "id": 18855,
        "title": "Automatic Text Summarization Based on Pre-trained Models",
        "authors": "Alaa Ahmed Al-Banna, Abeer K. Al-Mashhadany",
        "published": "2023-7-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiccit57614.2023.10218006"
    },
    {
        "id": 18856,
        "title": "On Calibration of Pre-trained Code Models",
        "authors": "Zhenhao Zhou, Chaofeng Sha, Xin Peng",
        "published": "2024-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3597503.3639126"
    },
    {
        "id": 18857,
        "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
        "authors": "Basel Mousi, Nadir Durrani, Fahim Dalvi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.196"
    },
    {
        "id": 18858,
        "title": "Math Function Recognition with Fine-Tuning Pre-Trained Models",
        "authors": "Fatimah Alshamari, Abdou Youssef",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "A Mathematical Function Recognition (MFR) is an important research direction for efficient downstream math tasks such as information retrieval, knowledge extraction, and question answering. The aim of this task is to identify and classify mathematical function into a predefined set of function. However, the lack of annotated data is the bottleneck in the development of an MFR automated model. We begin this paper by describing our approach to creating a labelled dataset for MFR. Then, to identify five categories of mathematical functions, we fine-tuned a set of common pre-trained models: BERT base-cased, BERT baseuncased, DistilBERT-cased, and DistilBERT-uncased. As a result, our contributions in this paper include: (1) an annotated MFR dataset that future researchers can use; and (2) SOTA results obtained by finetuning pre-trained models for the MFR task. Our experiments demonstrate that the proposed approach achieved a high-quality recognition, with an F1 score of 96.80% on a held-out test set provided by DistilBERT-cased model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijci.2023.120204"
    },
    {
        "id": 18859,
        "title": "Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models",
        "authors": "Byounggyu Lew, Donghyun Son, Buru Chang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00478"
    },
    {
        "id": 18860,
        "title": "Jack and Masters of all Trades: One-Pass Learning Sets of Model Sets From Large Pre-Trained Models",
        "authors": "Han Xiang Choong, Yew-Soon Ong, Abhishek Gupta, Caishun Chen, Ray Lim",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mci.2023.3277769"
    },
    {
        "id": 18861,
        "title": "Pre-trained language models in Spanish for health insurance coverage",
        "authors": "Claudio Aracena, Nicolás Rodríguez, Victor Rocco, Jocelyn Dunstan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.46"
    },
    {
        "id": 18862,
        "title": "Hybridization of Deep Learning Pre-Trained Models with Machine Learning Classifiers and Fuzzy Min–Max Neural Network for Cervical Cancer Diagnosis",
        "authors": "Madhura Kalbhor, Swati Shinde, Daniela Elena Popescu, D. Jude Hemanth",
        "published": "2023-4-6",
        "citations": 12,
        "abstract": "Medical image analysis and classification is an important application of computer vision wherein disease prediction based on an input image is provided to assist healthcare professionals. There are many deep learning architectures that accept the different medical image modalities and provide the decisions about the diagnosis of various cancers, including breast cancer, cervical cancer, etc. The Pap-smear test is the commonly used diagnostic procedure for early identification of cervical cancer, but it has a high rate of false-positive results due to human error. Therefore, computer-aided diagnostic systems based on deep learning need to be further researched to classify the pap-smear images accurately. A fuzzy min–max neural network is a neuro fuzzy architecture that has many advantages, such as training with a minimum number of passes, handling overlapping class classification, supporting online training and adaptation, etc. This paper has proposed a novel hybrid technique that combines the deep learning architectures with machine learning classifiers and fuzzy min–max neural network for feature extraction and Pap-smear image classification, respectively. The deep learning pretrained models used are Alexnet, ResNet-18, ResNet-50, and GoogleNet. Benchmark datasets used for the experimentation are Herlev and Sipakmed. The highest classification accuracy of 95.33% is obtained using Resnet-50 fine-tuned architecture followed by Alexnet on Sipakmed dataset. In addition to the improved accuracies, the proposed model has utilized the advantages of fuzzy min–max neural network classifiers mentioned in the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/diagnostics13071363"
    },
    {
        "id": 18863,
        "title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning",
        "authors": "Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01044"
    },
    {
        "id": 18864,
        "title": "Performance Enhancement in Pre-Trained Deep Learning Models for Monkeypox Skin Lesions Identification Using Feature Selection Algorithms",
        "authors": "Kien Trang, An Hoang Nguyen, Nguyen Gia Minh Thao, Bao Quoc Vuong, Long Ton-That",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/sice59929.2023.10354166"
    },
    {
        "id": 18865,
        "title": "A Novel Shape Retrieval Method for 3D Mechanical Components Based on Object Projection, Pre-Trained Deep Learning Models and Autoencoder",
        "authors": "S. Bickel, B. Schleich, S. Wartzack",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cad.2022.103417"
    },
    {
        "id": 18866,
        "title": "A Novel Virtual Cosmetics Recommender System Based On Pre-Trained Computer Vision Models",
        "authors": "Samia A. Abu-Shanab, Shadi AlZu'bi, Amjad Zraiqat",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icit58056.2023.10225835"
    },
    {
        "id": 18867,
        "title": "Arabic Extractive Summarization Using Pre-Trained Models",
        "authors": "Yasmin Einieh, Amal AlMansour, Amani Jamal",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "Automatic Text Summarization (ATS) is a crucial area of study in Natural Language Processing (NLP) due to the vast amount of online information available. Extractive summarization, which involves selecting important sentences from the original document without altering their wording, is one approach to generating summaries. While many methods for Arabic text summarization exist, deep learning applications are still in their early stages, and there is a shortage of available datasets. Unlike English, there have been fewer experiments conducted on Arabic language summarization due to its unique characteristics. This study aims to fill this gap by experimenting with several models for summarizing Arabic text, including QARiB, AraELECTRA, and AraBERT-base models, all trained using the KALIMA dataset. The AraBERT model performed exceptionally well, achieving high scores of 0.44, 0.26, and 0.44 on the ROUGE-1, ROUGE-2, and ROUGE-L measures, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4197/comp.12-1.6"
    },
    {
        "id": 18868,
        "title": "Leveraging pre-trained language models for code generation",
        "authors": "Ahmed Soliman, Samir Shaheen, Mayada Hadhoud",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "AbstractCode assistance refers to the utilization of various tools, techniques, and models to help developers in the process of software development. As coding tasks become increasingly complex, code assistant plays a pivotal role in enhancing developer productivity, reducing errors, and facilitating a more efficient coding workflow. This assistance can manifest in various forms, including code autocompletion, error detection and correction, code generation, documentation support, and context-aware suggestions. Language models have emerged as integral components of code assistance, offering developers the capability to receive intelligent suggestions, generate code snippets, and enhance overall coding proficiency. In this paper, we propose new hybrid models for code generation by leveraging pre-trained language models BERT, RoBERTa, ELECTRA, and LUKE with the Marian Causal Language Model. Selecting these models based on their strong performance in various natural language processing tasks. We evaluate the performance of these models on two datasets CoNaLa and DJANGO and compare them to existing state-of-the-art models. We aim to investigate the potential of pre-trained transformer language models to revolutionize code generation, offering improved precision and efficiency in navigating complex coding scenarios. Additionally, conducting error analysis and refining the generated code. Our results show that these models, when combined with the Marian Decoder, significantly improve code generation accuracy and efficiency. Notably, the RoBERTaMarian model achieved a maximum BLEU score of 35.74 and an exact match accuracy of 13.8% on CoNaLa, while LUKE-Marian attained a BLEU score of 89.34 and an exact match accuracy of 78.50% on DJANGO. Implementation of this work is available at https://github.com/AhmedSSoliman/Leveraging-Pretrained-Language-Models-for-Code-Generation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s40747-024-01373-8"
    },
    {
        "id": 18869,
        "title": "Comparison of large-scale pre-trained models based ViT, swin transformer and ConvNeXt",
        "authors": "Jiapeng Yu",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671201"
    },
    {
        "id": 18870,
        "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models",
        "authors": "Mingyang Song, Yi Feng, Liping Jing",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.161"
    },
    {
        "id": 18871,
        "title": "Text clustering based on pre-trained models and autoencoders",
        "authors": "Qiang Xu, Hao Gu, ShengWei Ji",
        "published": "2024-1-5",
        "citations": 1,
        "abstract": "Text clustering is the task of grouping text data based on similarity, and it holds particular importance in the medical field. sIn healthcare, medical data clustering is a highly active and effective research area. It not only provides strong support for making correct medical decisions from medical datasets but also aids in patient record management and medical information retrieval. With the development of the healthcare industry, a large amount of medical data is being generated, and traditional medical data clustering faces significant challenges. Many existing text clustering algorithms are primarily based on the bag-of-words model, which has issues such as high dimensionality, sparsity, and the neglect of word positions and context. Pre-trained models are a deep learning-based approach that treats text as a sequence to accurately capture word positions and context information. Moreover, compared to traditional K-means and fuzzy C-means clustering models, deep learning-based clustering algorithms are better at handling high-dimensional, complex, and nonlinear data. In particular, clustering algorithms based on autoencoders can learn data representations and clustering information, effectively reducing noise interference and errors during the clustering process. This paper combines pre-trained language models with deep embedding clustering models. Experimental results demonstrate that our model performs exceptionally well on four public datasets, outperforming most existing text clustering algorithms, and can be applied to medical data clustering.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fncom.2023.1334436"
    },
    {
        "id": 18872,
        "title": "Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging",
        "authors": "Rachel M. Harrison, Anton Dereventsov, Anton Bibin",
        "published": "2023-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdmw60847.2023.00195"
    },
    {
        "id": 18873,
        "title": "Self-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models",
        "authors": "Christopher Simic, Tobias Bocklet",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448047"
    },
    {
        "id": 18874,
        "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models",
        "authors": "Raymond Li, Gabriel Murray, Giuseppe Carenini",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.634"
    },
    {
        "id": 18875,
        "title": "A Systematic Survey of Chemical Pre-trained Models",
        "authors": "Jun Xia, Yanqiao Zhu, Yuanqi Du, Stan Z. Li",
        "published": "2023-8",
        "citations": 6,
        "abstract": "Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Chemical Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder architectures, pre-training strategies, and applications. We also highlight the challenges and promising avenues for future research, providing a useful resource for both machine learning and scientific communities.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/760"
    },
    {
        "id": 18876,
        "title": "Domain-specific language models pre-trained on construction management systems corpora",
        "authors": "Yunshun Zhong, Sebastian D. Goodfellow",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.autcon.2024.105316"
    },
    {
        "id": 18877,
        "title": "Evaluation of Pre-Trained CNN Models for Cardiovascular Disease Classification: A Benchmark Study",
        "authors": "",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18576/isl/120755"
    },
    {
        "id": 18878,
        "title": "Exploring Arabic Pre-Trained Language Models for Arabic Abstractive Text Summarization",
        "authors": "Dhuha Alqahtani, Maha Al-Yahya",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/snams60348.2023.10375464"
    },
    {
        "id": 18879,
        "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation",
        "authors": "Tianyu Yang, Thy Tran, Iryna Gurevych",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.313"
    },
    {
        "id": 18880,
        "title": "A Geophysics Library of Trained Machine Learning Models",
        "authors": "A. Huck, P. De Groot, H. Refayee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202332013"
    },
    {
        "id": 18881,
        "title": "OPTIMIZING ULTRASOUND IMAGE CLASSIFICATION THROUGH TRANSFER LEARNING: FINE-TUNING STRATEGIES AND CLASSIFIER IMPACT ON PRE-TRAINED INNER-LAYERS",
        "authors": "Mohamed Bal-Ghaoui, My Hachem El Yousfi Alaoui, Abdelilah Jilbab, Abdennaser Bourouhou",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "Transfer Learning (TL) is a popular deep learning technique used in medical image analysis, especially when data is limited. It leverages pre-trained knowledge from State-Of-The-Art (SOTA) models and applies it to specific applications through Fine-Tuning (FT). However, fine-tuning large models can be time-consuming, and determining which layers to use can be challenging. This study explores different fine-tuning strategies for five SOTA models (VGG16, VGG19, ResNet50, ResNet101, and InceptionV3) pre-trained on ImageNet. It also investigates the impact of the classifier by using a linear SVM for classification. The experiments are performed on four open-access ultrasound datasets related to breast cancer, thyroid nodules cancer, and salivary glands cancer. Results are evaluated using a five-fold stratified cross-validation technique, and metrics like accuracy, precision, and recall are computed. The findings show that fine-tuning 15% of the last layers in ResNet50 and InceptionV3 achieves good results. Using SVM for classification further improves overall performance by 6% for the two best-performing models. This research provides insights into fine-tuning strategies and the importance of the classifier in transfer learning for ultrasound image classification.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35784/iapgos.4464"
    },
    {
        "id": 18882,
        "title": "DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit Recognition",
        "authors": "Farsana Salim, Faisal Saeed, Shadi Basurra, Sultan Noman Qasem, Tawfik Al-Hadhrami",
        "published": "2023-7-19",
        "citations": 8,
        "abstract": "With the dramatic increase of the global population and with food insecurity increasing, it has become a major concern for both individuals and governments to fulfill the need for foods such as vegetables and fruits. Moreover, the desire for the consumption of healthy food, including fruit, has increased the need for applications in the field of agriculture that help to achieve better methods for fruit sorting and fruit disease prediction and classification. Automated fruit recognition is a potential solution to reduce the time and labor required to identify different fruits in situations such as retail stores during checkout, fruit processing centers during sorting, and orchards during harvest. Automating these processes reduces the need for human intervention, making them cheaper, faster, and immune to human error and biases. Past research in the field has focused mainly on the size, shape, and color features of fruits or employed convolutional neural networks (CNNs) for their classification. This study investigates the effectiveness of pre-trained deep learning models for fruit classification using two distinct datasets: Fruits-360 and the Fruit Recognition dataset. Four pre-trained models, DenseNet-201, Xception, MobileNetV3-Small, and ResNet-50, were chosen for the experiments based on their architecture and features. The results show that all models achieved almost 99% accuracy or higher with Fruits-360. With the Fruit Recognition dataset, DenseNet-201 and Xception achieved accuracies of around 98%. The good results exhibited by DenseNet-201 and Xception on both the datasets are remarkable, with DenseNet-201 attaining accuracies of 99.87% and 98.94%, and Xception attaining 99.13% and 97.73% accuracy, respectively, on Fruits-360 and the Fruit Recognition dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12143132"
    },
    {
        "id": 18883,
        "title": "Enhancing smart contract security: Leveraging pre‐trained language models for advanced vulnerability detection",
        "authors": "Fei He, Fei Li, Peili Liang",
        "published": "2024-3-29",
        "citations": 0,
        "abstract": "AbstractThe burgeoning interest in decentralized applications (Dapps), spurred by advancements in blockchain technology, underscores the critical role of smart contracts. However, many Dapp users, often without deep knowledge of smart contracts, face financial risks due to hidden vulnerabilities. Traditional methods for detecting these vulnerabilities, including manual inspections and automated static analysis, are plagued by issues such as high rates of false positives and overlooked security flaws. To combat this, the article introduces an innovative approach using the bidirectional encoder representations from transformers (BERT)‐ATT‐BiLSTM model for identifying potential weaknesses in smart contracts. This method leverages the BERT pre‐trained model to discern semantic features from contract opcodes, which are then refined using a Bidirectional Long Short‐Term Memory Network (BiLSTM) and augmented by an attention mechanism that prioritizes critical features. The goal is to improve the model's generalization ability and enhance detection accuracy. Experiments on various publicly available smart contract datasets confirm the model's superior performance, outperforming previous methods in key metrics like accuracy, F1‐score, and recall. This research not only offers a powerful tool to bolster smart contract security, mitigating financial risks for average users, but also serves as a valuable reference for advancements in natural language processing and deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/blc2.12072"
    },
    {
        "id": 18884,
        "title": "TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models",
        "authors": "Sucheng Ren, Fangyun Wei, Zheng Zhang, Han Hu",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00359"
    },
    {
        "id": 18885,
        "title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network",
        "authors": "Rongxiang Weng, Wen Sen Cheng, Min Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.291"
    },
    {
        "id": 18886,
        "title": "Attenuate Class Imbalance Problem for Pneumonia Diagnosis Using Ensemble Parallel Stacked Pre-Trained Models",
        "authors": "Aswathy Ravikumar, Harini Sriraman",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.035848"
    },
    {
        "id": 18887,
        "title": "Two-stage Thai Misspelling Correction based on Pre-trained Language Models",
        "authors": "Idhibhat Pankam, Peerat Limkonchotiwat, Ekapol Chuangsuwanich",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jcsse58229.2023.10202006"
    },
    {
        "id": 18888,
        "title": "Pre-Trained Deep Learning-Based Approaches for Eye Disease Detection",
        "authors": "Pankaj Kumar, Sudhir Bhandari, Vishal Dutt",
        "published": "2023-8-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccpct58313.2023.10245175"
    },
    {
        "id": 18889,
        "title": "Adapting physiologically-based pharmacokinetic models for machine learning applications",
        "authors": "Sohaib Habiballah, Brad Reisfeld",
        "published": "2023-9-11",
        "citations": 0,
        "abstract": "AbstractBoth machine learning and physiologically-based pharmacokinetic models are becoming essential components of the drug development process. Integrating the predictive capabilities of physiologically-based pharmacokinetic (PBPK) models within machine learning (ML) pipelines could offer significant benefits in improving the accuracy and scope of drug screening and evaluation procedures. Here, we describe the development and testing of a self-contained machine learning module capable of faithfully recapitulating summary pharmacokinetic (PK) parameters produced by a full PBPK model, given a set of input drug-specific and regimen-specific information. Because of its widespread use in characterizing the disposition of orally administered drugs, the PBPK model chosen to demonstrate the methodology was an open-source implementation of a state-of-the-art compartmental and transit model called . The model was tested for drug formulations spanning a large range of solubility and absorption characteristics, and was evaluated for concordance against predictions of  and relevant experimental data. In general, the values predicted by the ML models were within 20% of those of the PBPK model across the range of drug and formulation properties. However, summary PK parameter predictions from both the ML model and full PBPK model were occasionally poor with respect to those derived from experiments, suggesting deficiencies in the underlying PBPK model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-42165-3"
    },
    {
        "id": 18890,
        "title": "Önceden eğitilmiş derin öğrenme modelleri kullanılarak damlacık yoluyla bulaşan salgın hastalıkları önlemek için robotik tabanlı maske tespiti",
        "authors": "Ali ÜNLÜTÜRK",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "Aralık 2019’da Çin’in Wuhan şehrinde ortaya çıkan ve tüm dünyada hızla yayılan Koronavirüs hastalığı solunum yolu sonucu oluşan küçük damlacıklar ile bulaşarak sağlıklı insanları enfekte etmiştir. Tıp uzmanları Koronavirüs hastalığına karşı en etkili mücadelenin temas halindeki kişilerin maske takması gerekliliğini belirtmişlerdir. Buna rağmen bazı kişiler maske takma zorunluluğunu ihlal etmişlerdir. Bu çalışmada maske takma zorunluluğunu ihlal eden kişilerin otomatik olarak tespit edilebilmesi için önceden eğitilmiş olan NaNetMobile, MobileNetV3Small, ResNet50, DenseNet121 ve EfficientNetV2B0 gibi derin sinir ağı modellerinin maske tanıma performansları değerlendirilmiştir. Bu değerlendirme sonucunda en başarılı DenseNet121 modeli ele edilmiştir. Bu model 6- Serbestlik Derecesine (6-DOF) sahip robotik bir sisteminin üzerinde yer alan kameradan elde edilen görüntü ile doğrulanmıştır. Kameradan alınan insane ait yüz görüntüleri yüksek kare hızlarında NVIDIA tarafından geliştirilen Jetson Xavier geliştirme kartı kullanılarak işlenmiştir. Sonuç olarak, bu çalışma toplu alanlarda maske denetimi gerçekleştiren görevlilere yardımcı olacak ve Koronavirüs benzeri çıkabilecek yeni salgınların yayılımı önemli ölçüde azaltacaktır.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28948/ngumuh.1291781"
    },
    {
        "id": 18891,
        "title": "Prompting and Fine-tuning Pre-trained Generative Language Models",
        "authors": "Johny Moreira, Altigran da Silva, Luciano Barbosa",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "There has been an explosion of available pre-trained and fine-tuned Generative Language Models (LM). They vary in the number of parameters, architecture, training strategy, and training set size. Aligned with it, alternative strategies exist to exploit these models, such as Fine-tuning and Prompt Engineering. However, many questions may arise throughout this process: Which model to apply for a given task? Which strategies to use? Will Prompt Engineering solve all tasks? What are the computational and financial costs involved? This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5753/sbbd_estendido.2023.25636"
    },
    {
        "id": 18892,
        "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
        "authors": "Abir Betka, Zeyd Ferhat, Riyadh Barka, Selma Boutiba, Zineddine Kahhoul, Tiar Lakhdar, Ahmed Abdelali, Habiba Dahmani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.33"
    },
    {
        "id": 18893,
        "title": "Photo-based Carbohydrates Counting using Pre-trained Transformer Models",
        "authors": "Ivan Contreras, Marti Guso, Aleix Beneyto, Josep Vehi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.445"
    },
    {
        "id": 18894,
        "title": "The Presence of Bias Based on Pre-trained Language Models",
        "authors": "Rui Liu, Yiqi Wu, Yufei Xing",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "One of the most active fields of machine learning research is natural language processing (NLP). Although existing linguistic machine learning models excel numerically on a variety of linguistic comprehension tasks, they frequently lack implicit bias reduction optimization. To ensure that deep learning models can avoid the traps of implicit bias and that robots can make fair judgments, bias in NLP must be addressed adequately. The ramifications of permitting biased models to reach the actual world are serious. Thus must address this issue as quickly as feasible. This paper conducted data validation-bias experiments on several real datasets to verify the presence of gender bias in the pre-trained models, then proposed a word vector balancing algorithm to modify the actual vector representation of words by biasing the models and verifying the effectiveness of the debiasing method through debiasing experiments to mitigate the gender bias of the models while ensuring data accuracy, thereby improving the fairness of the classification results. Furthermore, this paper provides more accurate information for the future development and use of deep learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/8/20230250"
    },
    {
        "id": 18895,
        "title": "On Checking Robustness on Named Entity Recognition with Pre-trained Transformers Models",
        "authors": "Aitor García-Pablos, Justina Mandravickaitė, Egidija Veršinskienė",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22364/bjmc.2023.11.4.05"
    },
    {
        "id": 18896,
        "title": "Aspects of creating a corporate question-and-answer system using generative pre-trained language models",
        "authors": "Aleksei Golikov, Dmitrii Akimov, Maksim Romanovskii, Sergei Trashchenkov",
        "published": "2023-12",
        "citations": 0,
        "abstract": "\n The article describes various ways to use generative pre-trained language models to build a corporate question-and-answer system. A significant limitation of the current generative pre-trained language models is the limit on the number of input tokens, which does not allow them to work \"out of the box\" with a large number of documents or with a large document. To overcome this limitation, the paper considers the indexing of documents with subsequent search query and response generation based on two of the most popular open source solutions at the moment – the Haystack and LlamaIndex frameworks. It has been shown that using the open source Haystack framework with the best settings allows you to get more accurate answers when building a corporate question-and-answer system compared to the open source LlamaIndex framework, however, requires the use of an average of several more tokens.    The article used a comparative analysis to evaluate the effectiveness of using generative pre-trained language models in corporate question-and-answer systems using the Haystack and Llamaindex frameworks. The evaluation of the obtained results was carried out using the EM (exact match) metric. The main conclusions of the conducted research on the creation of question-answer systems using generative pre-trained language models are: 1. Using hierarchical indexing is currently extremely expensive in terms of the number of tokens used (about 160,000 tokens for hierarchical indexing versus 30,000 tokens on average for sequential indexing), since the response is generated by sequentially processing parent and child nodes. 2. Processing information using the Haystack framework with the best settings allows you to get somewhat more accurate answers than using the LlamaIndex framework (0.7 vs. 0.67 with the best settings). 3. Using the Haystack framework is more invariant with respect to the accuracy of responses in terms of the number of tokens in the chunk. 4. On average, using the Haystack framework is more expensive in terms of the number of tokens (about 4 times) than the LlamaIndex framework. 5. The \"create and refine\" and \"tree summarize\" response generation modes for the LlamaIndex framework are approximately the same in terms of the accuracy of the responses received, however, more tokens are required for the \"tree summarize\" mode.\n\t",
        "keywords": "",
        "link": "http://dx.doi.org/10.25136/2409-8698.2023.12.69353"
    },
    {
        "id": 18897,
        "title": "Afrikaans Literary Genre Recognition using Embeddings and Pre-Trained Multilingual Language Models",
        "authors": "Eduan Kotzé, Burgert A. Senekal",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467838"
    },
    {
        "id": 18898,
        "title": "Comparing The Fine-Tuning and Performance of Whisper Pre-Trained Models for Turkish Speech Recognition Task",
        "authors": "Saadin Oyucu",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304891"
    },
    {
        "id": 18899,
        "title": "The Impact of Training Methods on the Development of Pre-trained Language Models",
        "authors": "Diego Uribe, Enrique Cuan, Elisa Urquizo",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.13053/cys-28-1-4718"
    },
    {
        "id": 18900,
        "title": "Improved Image Quality Assessment by Utilizing Pre-Trained Architecture Features with Unified Learning Mechanism",
        "authors": "Jihyoung Ryu",
        "published": "2023-2-19",
        "citations": 1,
        "abstract": "The purpose of the no-reference image quality assessment (NR-IQA) is to measure perceived image quality based on subjective judgments; however, due to the lack of a clean reference image, this is a complicated and unresolved challenge. Massive new IQA datasets have facilitated the creation of deep learning-based image quality measurements. We present a unique model to handle the NR-IQA challenge in this research by employing a hybrid strategy that leverages from pre-trained CNN model and the unified learning mechanism that extracts both local and non-local characteristics from the input patch. The deep analysis of the proposed framework shows that the model uses features and a mechanism that improves the monotonicity relationship between objective and subjective ratings. The intermediary goal was mapped to a quality score using a regression architecture. To extract various feature maps, a deep architecture with an adaptive receptive field was used. Analyses of this biggest NR-IQA benchmark datasets demonstrate that the suggested technique outperforms current state-of-the-art NR-IQA measures.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13042682"
    },
    {
        "id": 18901,
        "title": "Data Redaction from Pre-trained GANs",
        "authors": "Zhifeng Kong, Kamalika Chaudhuri",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/satml54575.2023.00048"
    },
    {
        "id": 18902,
        "title": "Goal exploration augmentation via pre-trained skills for sparse-reward long-horizon goal-conditioned reinforcement learning",
        "authors": "Lisheng Wu, Ke Chen",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "AbstractReinforcement learning often struggles to accomplish a sparse-reward long-horizon task in a complex environment. Goal-conditioned reinforcement learning (GCRL) has been employed to tackle this difficult problem via a curriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is essential for the agent to ultimately find the pathway to the desired goal. How to explore novel sub-goals efficiently is one of the most challenging issues in GCRL. Several goal exploration methods have been proposed to address this issue but still struggle to find the desired goals efficiently. In this paper, we propose a novel learning objective by optimizing the entropy of both achieved and new goals to be explored for more efficient goal exploration in sub-goal selection based GCRL. To optimize this objective, we first explore and exploit the frequently occurring goal-transition patterns mined in the environments similar to the current task to compose skills via skill learning. Then, the pre-trained skills are applied in goal exploration with theoretical justification. Evaluation on a variety of spare-reward long-horizon benchmark tasks suggests that incorporating our method into several state-of-the-art GCRL baselines significantly boosts their exploration efficiency while improving or maintaining their performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06503-w"
    },
    {
        "id": 18903,
        "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
        "authors": "Xuanjie Fang, Sijie Cheng, Yang Liu, Wei Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.461"
    },
    {
        "id": 18904,
        "title": "Dataset Construction and Opinion Holder Detection Using Pre-trained Models",
        "authors": "Al- Mahmud, Kazutaka Shimada",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52731/ijskm.v7.i2.779"
    }
]