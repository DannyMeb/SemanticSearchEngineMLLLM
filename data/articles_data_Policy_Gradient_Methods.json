[
    {
        "id": 18871,
        "title": "Policy Gradient Methods",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-0716-1006-0_300497"
    },
    {
        "id": 18872,
        "title": "Policy Gradient Methods",
        "authors": "Ali H. Sayed",
        "published": "2022-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009218245.024"
    },
    {
        "id": 18873,
        "title": "Policy Gradient Methods",
        "authors": "Jan Peters, J. Andrew Bagnell",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_646"
    },
    {
        "id": 18874,
        "title": "Policy Gradient Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_9"
    },
    {
        "id": 18875,
        "title": "Advanced Policy Gradient Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_11"
    },
    {
        "id": 18876,
        "title": "Local Analysis of Entropy-Regularized Stochastic Soft-Max Policy Gradient Methods",
        "authors": "Yuhao Ding, Junzi Zhang, Javad Lavaei",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178123"
    },
    {
        "id": 18877,
        "title": "On the Convergence of Natural Policy Gradient and Mirror Descent-Like Policy Methods for Average-Reward MDPs",
        "authors": "Yashaswini Murthy, R. Srikant",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383691"
    },
    {
        "id": 18878,
        "title": "Global Optimality Guarantees for Policy Gradient Methods",
        "authors": "Jalaj Bhandari, Daniel Russo",
        "published": "2024-1-5",
        "citations": 3,
        "abstract": " Policy gradient methods, which have powered a lot of recent success in reinforcement learning, search for an optimal policy in a parameterized policy class by performing stochastic gradient descent on the cumulative expected cost-to-go under some initial state distribution. Although widely used, these methods lack theoretical guarantees as the optimization objective is typically nonconvex even for simple control problems, and hence are understood to only converge to a stationary point. In “Global Optimality Guarantees for Policy Gradient Methods,” J. Bhandari and D. Russo identify structural properties of the underlying MDP that guarantee that despite nonconvexity, the optimization objective has no suboptimal stationary points, ensuring asymptotic convergence of policy gradient methods to globally optimal policies. Under stronger conditions, authors show the policy gradient objective to satisfy a Polyak-lojasiewicz (gradient dominance) condition that yields fast convergence rates. In addition, when some of the said conditions are relaxed, authors provide bounds on the suboptimality gap of any stationary point. The results rely on a key connection with policy iteration, a classic dynamic programming algorithm which solves a single period optimization problem at every step. The authors show how structure in the single period optimization problems solved by policy iteration translate into nice properties of the multiperiod policy gradient objective, making it amenable for first-order methods to find globally optimal solutions. The authors also instantiate their framework for several classical control problems including tabular and linear MDPs, linear quadratic control, optimal stopping, and finite-horizon inventor control problems. ",
        "link": "http://dx.doi.org/10.1287/opre.2021.0014"
    },
    {
        "id": 18879,
        "title": "Similarities Between Policy Gradient Methods (PGM) in Reinforcement Learning (RL) and Supervised Learning (SL)",
        "authors": "Eric Benhamou",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3391216"
    },
    {
        "id": 18880,
        "title": "Benchmarking the Natural Gradient in Policy Gradient Methods and Evolution Strategies",
        "authors": "Kay Hansel, Janosch Moos, Cedric Derstroff",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_7"
    },
    {
        "id": 18881,
        "title": "Policy gradient methods with Gaussian process modelling acceleration",
        "authors": "Dong Li, Dongbin Zhao, Qichao Zhang, Chaomin Luo",
        "published": "2017-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966065"
    },
    {
        "id": 18882,
        "title": "Conjugate Gradient Type Methods",
        "authors": "Martin Hanke",
        "published": "2017-11-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315140193-1"
    },
    {
        "id": 18883,
        "title": "Learning General Policies with Policy Gradient Methods",
        "authors": "Simon Ståhlberg, Blai Bonet, Hector Geffner",
        "published": "2023-9",
        "citations": 0,
        "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
        "link": "http://dx.doi.org/10.24963/kr.2023/63"
    },
    {
        "id": 18884,
        "title": "Matrix Low-Rank Approximation for Policy Gradient Methods",
        "authors": "Sergio Rozada, Antonio G. Marques",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10094802"
    },
    {
        "id": 18885,
        "title": "Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a Finite Horizon",
        "authors": "Ben M. Hambly, Renyuan Xu, Huining Yang",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3734179"
    },
    {
        "id": 18886,
        "title": "Approximation Benefits of Policy Gradient Methods with Aggregated States",
        "authors": "Daniel Russo",
        "published": "2023-11",
        "citations": 1,
        "abstract": " Folklore suggests that policy gradient can be more robust to misspecification than its relative, approximate policy iteration. This paper studies the case of state-aggregated representations, in which the state space is partitioned and either the policy or value function approximation is held constant over partitions. This paper shows a policy gradient method converges to a policy whose regret per period is bounded by ϵ, the largest difference between two elements of the state-action value function belonging to a common partition. With the same representation, both approximate policy iteration and approximate value iteration can produce policies whose per-period regret scales as [Formula: see text], where γ is a discount factor. Faced with inherent approximation error, methods that locally optimize the true decision objective can be far more robust.  This paper was accepted by Hamid Nazerzadeh, data science.  Supplemental Material: Data are available at https://doi.org/10.1287/mnsc.2023.4788 . ",
        "link": "http://dx.doi.org/10.1287/mnsc.2023.4788"
    },
    {
        "id": 18887,
        "title": "Fisher Information Approximations in Policy Gradient Methods",
        "authors": "Markus Semmler",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_6"
    },
    {
        "id": 18888,
        "title": "Policy gradient methods for discrete time linear quadratic regulator with random parameters",
        "authors": "Deyue Li",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "This paper studies an infinite horizon optimal control problem for discrete-time linear system and quadratic criteria, both with random parameters which are independent and identically distributed with respect to time. In this general setting, we apply the policy gradient method, a reinforcement learning technique, to search for the optimal control without requiring knowledge of statistical information of the parameters. We investigate the sub-Gaussianity of the state process and establish global linear convergence guarantee for this approach based on assumptions that are weaker and easier to verify compared to existing results. Numerical experiments are presented to illustrate our result.",
        "link": "http://dx.doi.org/10.1051/cocv/2024014"
    },
    {
        "id": 18889,
        "title": "Bayesian sequential optimal experimental design for nonlinear models using policy gradient reinforcement learning",
        "authors": "Wanggang Shen, Xun Huan",
        "published": "2023-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cma.2023.116304"
    },
    {
        "id": 18890,
        "title": "Deterministic Policy Gradient and the DDPG",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_13"
    },
    {
        "id": 18891,
        "title": "How are policy gradient methods affected by the limits of control?",
        "authors": "Ingvar Ziemann, Anastasios Tsiamis, Henrik Sandberg, Nikolai Matni",
        "published": "2022-12-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992612"
    },
    {
        "id": 18892,
        "title": "Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games",
        "authors": "Ben M. Hambly, Renyuan Xu, Huining Yang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3894471"
    },
    {
        "id": 18893,
        "title": "Policy Gradient Learning Methods for Stochastic Control with Exit Time and Applications to Share Repurchase Pricing",
        "authors": "Mohamed Hamdouche, Pierre Henry-Labordere, Huyen Pham",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358537"
    },
    {
        "id": 18894,
        "title": "Independent Natural Policy Gradient Methods for Potential Games: Finite-time Global Convergence with Entropy Regularization",
        "authors": "Shicong Cen, Fan Chen, Yuejie Chi",
        "published": "2022-12-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9993175"
    },
    {
        "id": 18895,
        "title": "Leveraging Class Abstraction for Commonsense Reinforcement Learning via Residual Policy Gradient Methods",
        "authors": "Niklas Hopner, Ilaria Tiddi, Herke van Hoof",
        "published": "2022-7",
        "citations": 0,
        "abstract": "Enabling reinforcement learning (RL) agents to leverage a knowledge base while learning from experience promises to advance RL in knowledge intensive domains. However, it has proven difficult to leverage knowledge that is not manually tailored to the environment. We propose to use the subclass relationships present in open-source knowledge graphs to abstract away from specific objects. We develop a residual policy gradient method that is able to integrate knowledge across different abstraction levels in the class hierarchy. Our method results in improved sample efficiency and generalisation to unseen objects in commonsense games, but we also investigate failure modes, such as excessive noise in the extracted class knowledge or environments with little class structure.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/423"
    },
    {
        "id": 18896,
        "title": "Unconstrained Optimization Methods: Conjugate Gradient Methods and Trust-Region Methods",
        "authors": "Snezana S. Djordjevic",
        "published": "2019-9-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/intechopen.84374"
    },
    {
        "id": 18897,
        "title": "Standard Conjugate Gradient Methods",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_4"
    },
    {
        "id": 18898,
        "title": "Other Conjugate Gradient Methods",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_11"
    },
    {
        "id": 18899,
        "title": "Three-Term Conjugate Gradient Methods",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_9"
    },
    {
        "id": 18900,
        "title": "Reward Only Training of Encoder-Decoder Digit Recognition Systems Based on Policy Gradient Methods",
        "authors": "Yilong Peng, Hayato Shibata, Takahiro Shinozaki",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/apsipa.2018.8659527"
    },
    {
        "id": 18901,
        "title": "Conjugate Gradient Methods Memoryless BFGS Preconditioned",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_8"
    },
    {
        "id": 18902,
        "title": "Hybrid and Parameterized Conjugate Gradient Methods",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_6"
    },
    {
        "id": 18903,
        "title": "General Convergence Results for Nonlinear Conjugate Gradient Methods",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_3"
    },
    {
        "id": 18904,
        "title": "Diverse Exploration via Conjugate Policies for Policy Gradient Methods",
        "authors": "Andrew Cohen, Xingye Qiao, Lei Yu, Elliot Way, Xiangrong Tong",
        "published": "2019-7-17",
        "citations": 1,
        "abstract": "We address the challenge of effective exploration while maintaining good performance in policy gradient methods. As a solution, we propose diverse exploration (DE) via conjugate policies. DE learns and deploys a set of conjugate policies which can be conveniently generated as a byproduct of conjugate gradient descent. We provide both theoretical and empirical results showing the effectiveness of DE at achieving exploration, improving policy performance, and the advantage of DE over exploration by random policy perturbations.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33013404"
    },
    {
        "id": 18905,
        "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings",
        "authors": "Matthew S. Zhang, Murat A Erdogdu, Animesh Garg",
        "published": "2022-6-28",
        "citations": 1,
        "abstract": "Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with L2 integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20891"
    },
    {
        "id": 18906,
        "title": "Conjugate Gradient Methods as Modifications of the Standard Schemes",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_7"
    },
    {
        "id": 18907,
        "title": "Gradient Methods Using Momentum",
        "authors": "",
        "published": "2022-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009004282.005"
    },
    {
        "id": 18908,
        "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization",
        "authors": "Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, Yuejie Chi",
        "published": "2022-7",
        "citations": 17,
        "abstract": " Preconditioning and Regularization Enable Faster Reinforcement Learning  Natural policy gradient (NPG) methods, in conjunction with entropy regularization to encourage exploration, are among the most popular policy optimization algorithms in contemporary reinforcement learning. Despite the empirical success, the theoretical underpinnings for NPG methods remain severely limited. In “Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization”, Cen, Cheng, Chen, Wei, and Chi develop nonasymptotic convergence guarantees for entropy-regularized NPG methods under softmax parameterization, focusing on tabular discounted Markov decision processes. Assuming access to exact policy evaluation, the authors demonstrate that the algorithm converges linearly at an astonishing rate that is independent of the dimension of the state-action space. Moreover, the algorithm is provably stable vis-à-vis inexactness of policy evaluation. Accommodating a wide range of learning rates, this convergence result highlights the role of preconditioning and regularization in enabling fast convergence. ",
        "link": "http://dx.doi.org/10.1287/opre.2021.2151"
    },
    {
        "id": 18909,
        "title": "Continuous Parameter Control in Genetic Algorithms using Policy Gradient Reinforcement Learning",
        "authors": "Alejandro de Miguel Gomez, Farshad Toosi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010643500003063"
    },
    {
        "id": 18910,
        "title": "An ML Agent using the Policy Gradient Method to win a SoccerTwos Game",
        "authors": "Victor Pugliese",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011108400003179"
    },
    {
        "id": 18911,
        "title": "Theory for Gradient Smoothing Methods",
        "authors": "",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811280016_0002"
    },
    {
        "id": 18912,
        "title": "Reinforcement Learning in Linear Quadratic Deep Structured Teams: Global Convergence of Policy Gradient Methods",
        "authors": "Vida Fathi, Jalal Arabneydi, Amir G. Aghdam",
        "published": "2020-12-14",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9304234"
    },
    {
        "id": 18913,
        "title": "Chapter 5. Conjugate Gradient Methods",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1051/978-2-7598-3175-3.c006"
    },
    {
        "id": 18914,
        "title": "Conjugate Gradient Type Methods for Ill-Posed Problems",
        "authors": "Martin Hanke",
        "published": "2017-11-22",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315140193"
    },
    {
        "id": 18915,
        "title": "Globally Convergent Policy Gradient Methods for Linear Quadratic Control of Partially Observed Systems",
        "authors": "Feiran Zhao, Xingyun Fu, Keyou You",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.208"
    },
    {
        "id": 18916,
        "title": "Time Critic Policy Gradient Methods for Traffic Signal Control in Complex and Congested Scenarios",
        "authors": "Stefano Giovanni Rizzo, Giovanna Vantini, Sanjay Chawla",
        "published": "2019-7-25",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3292500.3330988"
    },
    {
        "id": 18917,
        "title": "Theoretical Guarantees of Fictitious Discount Algorithms for Episodic Reinforcement Learning and Global Convergence of Policy Gradient Methods",
        "authors": "Xin Guo, Anran Hu, Junzi Zhang",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "When designing algorithms for finite-time-horizon episodic reinforcement learning problems, a common approach is to introduce a fictitious discount factor and use stationary policies for approximations. Empirically, it has been shown that the fictitious discount factor helps reduce variance, and stationary policies serve to save the per-iteration computational cost. Theoretically, however, there is no existing work on convergence analysis for algorithms with this fictitious discount recipe. This paper takes the first step towards analyzing these algorithms. It focuses on two vanilla policy gradient (VPG) variants: the first being a widely used variant with discounted advantage estimations (DAE), the second with an additional fictitious discount factor in the score functions of the policy gradient estimators. Non-asymptotic convergence guarantees are established for both algorithms, and the additional discount factor is shown to reduce the bias introduced in DAE and thus improve the algorithm convergence asymptotically. A key ingredient of our analysis is to connect three settings of Markov decision processes (MDPs): the finite-time-horizon, the average reward and the discounted settings. To our best knowledge, this is the first theoretical guarantee on fictitious discount algorithms for the episodic reinforcement learning of finite-time-horizon MDPs, which also leads to the (first) global convergence of policy gradient methods for finite-time-horizon episodic reinforcement learning.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i6.20633"
    },
    {
        "id": 18918,
        "title": "Convergence of Policy Gradient Methods for Finite-Horizon Exploratory Linear-Quadratic Control Problems",
        "authors": "Michael Giegrich, Christoph Reisinger, Yufei Zhang",
        "published": "2024-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1137/22m1533517"
    },
    {
        "id": 18919,
        "title": "Methods of French policy studies",
        "authors": "Claire Dupuy, Philippe Zittoun",
        "published": "2018-1-3",
        "citations": 0,
        "abstract": "The chapter discusses the various analytical methods used in French policy studies. It begins by mapping out the range of methods used in French academic policy studies. Such work mainly deals with policy-making process and their analysis, with an emphasis on the role of elite, expert, and institutional constraints. Issues and concepts from (political) sociology mainly frame these works. Policy analysis borrowed its methods from (political) sociology rather than developed a specific set of methodological approaches. Drawing on sociological approaches, policy analysis in France features a preference for qualitative over quantitative methods. Also, empirical studies prevail, and over time, small-n comparative research frameworks were introduced on a more systematic basis. The chapter also develops an analysis of the most popular methods used in the past years among practitionners, such as socio-economic appraisal in transport or housing sectors, indicators in environmental and economic sectors, and argumentative methods in public institutional debates.",
        "link": "http://dx.doi.org/10.1332/policypress/9781447324218.003.0005"
    },
    {
        "id": 18920,
        "title": "The Policy Gradient Method in Breakout",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-19"
    },
    {
        "id": 18921,
        "title": "The Reinforce Policy Gradient Algorithm Revisited",
        "authors": "Shalabh Bhatnagar",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442123"
    },
    {
        "id": 18922,
        "title": "Multi-agent Policy Gradient Algorithms for Cyber-physical Systems with Lossy Communication",
        "authors": "Adrian Redder, Arunselvan Ramaswamy, Holger Karl",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010845400003116"
    },
    {
        "id": 18923,
        "title": "A State-Space Perspective on the Expedited Gradient Methods: Nadam, RAdam, and Rescaled Gradient Flow",
        "authors": "Kushal Chakrabarti, Nikhil Chopra",
        "published": "2022-12-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc56513.2022.10093397"
    },
    {
        "id": 18924,
        "title": "Chapter 12: Dual-Based Proximal Gradient Methods",
        "authors": "",
        "published": "2017-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1137/1.9781611974997.ch12"
    },
    {
        "id": 18925,
        "title": "Policy gradient methods for free-electron laser and terahertz source optimization and stabilization at the FERMI free-electron laser at Elettra",
        "authors": "F. H. O’Shea, N. Bruchon, G. Gaio",
        "published": "2020-12-21",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1103/physrevaccelbeams.23.122802"
    },
    {
        "id": 18926,
        "title": "Policy Gradient Learning Methods for Stochastic Control with Exit Time and Applications to Share Repurchase Pricing",
        "authors": "Mohamed Hamdouche, Pierre Henry-Labordere, Huyên Pham",
        "published": "2022-11-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/1350486x.2023.2239850"
    },
    {
        "id": 18927,
        "title": "Reinforced Variable Selection via Natural Policy Gradient",
        "authors": "Yuan Le, Fan Zhou, Yang Bai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nVariable selection identifies the best subset of covariates when building the prediction model, among all possible subsets. In this paper, we propose a novel reinforced variable selection method, called “Actor-Critic-Predictor”. The actor takes an action to choose variables and the predictor evaluates the action based on a well-designed reward function, where the reward baseline is learned by the critic. We model the variable selection process as a multi-armed bandit and update the subset of variables being selected using a natural policy gradient algorithm. We provide an analytical framework on how different errors impact the performance of our method theoretically. Large amounts of experiments on both synthetic and real datasets show that the proposed framework is easy-implemented and outperforms classical variable selection methods in a wide range of scenarios.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3932698/v1"
    },
    {
        "id": 18928,
        "title": "Gradient Methods with Dynamic Inexact Oracles",
        "authors": "Shuo Han",
        "published": "2021-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483370"
    },
    {
        "id": 18929,
        "title": "Adaptive Gradient Methods",
        "authors": "",
        "published": "2022-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009218146.018"
    },
    {
        "id": 18930,
        "title": "Context-Sensitive Policy Methods",
        "authors": "",
        "published": "2017-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315093192-44"
    },
    {
        "id": 18931,
        "title": "Linear Conjugate Gradient Algorithm",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_2"
    },
    {
        "id": 18932,
        "title": "Limited memory gradient methods for unconstrained optimization",
        "authors": "Giulia Ferrandi, Michiel E. Hochstenbach",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe limited memory steepest descent method (LMSD, Fletcher, 2012) for unconstrained optimization problems stores a few past gradients to compute multiple stepsizes at once. We review this method and propose new variants. For strictly convex quadratic objective functions, we study the numerical behavior of different techniques to compute new stepsizes. In particular, we introduce a method to improve the use of harmonic Ritz values. We also show the existence of a secant condition associated with LMSD, where the approximating Hessian is projected onto a low-dimensional space. In the general nonlinear case, we propose two new alternatives to Fletcher's method: first, the addition of symmetry constraints to the secant condition valid for the quadratic case; second, a perturbation of the last differences between consecutive gradients, to satisfy multiple secant equations simultaneously. We show that Fletcher's method can also be interpreted from this viewpoint.\nAMS Classification: 65K05 , 90C20 , 90C30 , 65F15 , 65F10",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3648563/v1"
    },
    {
        "id": 18933,
        "title": "Strain-gradient vs damage-gradient regularizations of softening damage models",
        "authors": "Duc Trung Le, Jean-Jacques Marigo, Corrado Maurini, Stefano Vidoli",
        "published": "2018-10",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cma.2018.06.013"
    },
    {
        "id": 18934,
        "title": "Multidimension Application Introduction and the Gradient",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.861opt_ch7"
    },
    {
        "id": 18935,
        "title": "Quantitative Methods for Policy Analysis",
        "authors": "",
        "published": "2017-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315093192-36"
    },
    {
        "id": 18936,
        "title": "Gradient Operator, Methods of Fluid Mechanics, and Electrodynamics",
        "authors": "",
        "published": "2019-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108635639.013"
    },
    {
        "id": 18937,
        "title": "Gradient Methods for Nonlinear Inversion",
        "authors": "",
        "published": "2020-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108903035.020"
    },
    {
        "id": 18938,
        "title": "Convergence of Policy Gradient Methods for Nash Equilibria in General-sum Stochastic Games",
        "authors": "Yan Chen, Tao Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.1494"
    },
    {
        "id": 18939,
        "title": "Advanced Quantum Policy Approximation in Policy Gradient Reinforcement Learning",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_6"
    },
    {
        "id": 18940,
        "title": "A Deterministic Policy Gradient Based Load Control Policy in Direct Current Distribution Networks",
        "authors": "",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2019.06.149"
    },
    {
        "id": 18941,
        "title": "Acceleration of Conjugate Gradient Algorithms",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_5"
    },
    {
        "id": 18942,
        "title": "Actor-only Deterministic Policy Gradient via Zeroth-order Gradient Oracles in Action Space",
        "authors": "Harshat Kumar, Dionysios S. Kalogerias, George J. Pappas, Alejandro Ribeiro",
        "published": "2021-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isit45174.2021.9518023"
    },
    {
        "id": 18943,
        "title": "Policy-Based Reinforcement Learning Approaches",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_10"
    },
    {
        "id": 18944,
        "title": "Methods for Applying Policy Feedback Theory",
        "authors": "Mallory SoRelle, Jamila Michener",
        "published": "2022-3-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003269083-4"
    },
    {
        "id": 18945,
        "title": "Qualitative-Interpretive Methods in Policy Research",
        "authors": "",
        "published": "2017-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315093192-41"
    },
    {
        "id": 18946,
        "title": "Spectral Gradient Methods",
        "authors": "Luigi Grippo, Marco Sciandrone",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-26790-1_25"
    },
    {
        "id": 18947,
        "title": "Risk-Sensitive Reinforcement Learning via Policy Gradient Search",
        "authors": "Prashanth L. A., Michael C. Fu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/9781638280279"
    },
    {
        "id": 18948,
        "title": "Policy Gradient Algorithms",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-14"
    },
    {
        "id": 18949,
        "title": "Improving Deep Deterministic Policy Gradient with Compact Experience Replay",
        "authors": "Daniel Neves, Lucila Ishitani, Zenilton Patrocínio",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nExperience Replay (ER) improves data efficiency in Deep Reinforcement Learning by allowing the agent to revisit past experiences that could contribute to the current policy learning. A recent method called COMPact Experience Replay (COMPER) seeks to improve ER by reducing the required number of experiences for agent training regarding the total accumulated rewards in the long run. This method can approximate good policies on Atari 2600 games on the Arcade Learning Environment (ALE) from a considerably smaller number of frame observations to achieve similar results obtained by DQN-based methods in the literature, which generally demand millions of video frames. Therefore, we conduct a detailed analysis of its components to verify and understand how they operate and how they could improve data efficiency on the Deep Deterministic Policy Gradient (DDPG) algorithm applied to physical control problems in Mujoco's environments. We present an extension of DDPG, named COMPER-DDPG, which uses components from COMPER and achieves better results than the original method in almost all environments used in the experiments with only 50,000 iterations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3991723/v1"
    },
    {
        "id": 18950,
        "title": "Coordinate Gradient Descent Methods",
        "authors": "Ion Necoara",
        "published": "2017-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315155678-15"
    },
    {
        "id": 18951,
        "title": "Elementary Gradient-Based Optimizers: CSLS and ISD",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.861opt_ch8"
    },
    {
        "id": 18952,
        "title": "Density Gradient Ultracentrifugation Technique",
        "authors": "Qian Zhang, Xiong Sun",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-5190-6_3"
    },
    {
        "id": 18953,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784.v1"
    },
    {
        "id": 18954,
        "title": "Training neural networks with policy gradient",
        "authors": "Sourabh Bose, Manfred Huber",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966360"
    },
    {
        "id": 18955,
        "title": "A deep reinforcement learning model based on deterministic policy gradient for collective neural crest cell migration",
        "authors": "George Lykotrafitis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5f5f8e69aa777f8ba5bd6177"
    },
    {
        "id": 18956,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784.v2"
    },
    {
        "id": 18957,
        "title": "Preconditioning of the Nonlinear Conjugate Gradient Algorithms",
        "authors": "Neculai Andrei",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-42950-8_10"
    },
    {
        "id": 18958,
        "title": "Methods of Foreign Policy Analysis",
        "authors": "Falk Ostermann, Patrick A. Mello",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003139850-2"
    },
    {
        "id": 18959,
        "title": "UAV Coverage Path Planning with Quantum-based Deep Deterministic Policy Gradient",
        "authors": "Silvirianti Silvirianti, Bhaskara Narottama, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study proposes quantum-based deep deterministic policy gradient (Q-DDPG) and quantum-based <em>recurrent </em>DDPG (Q-RDDPG) schemes for time-series optimization in UAV communications. Herein, Q-DDPG based actor-critic reinforcement learning is utilized to optimize action selections in a large state and continuous action space. In this scheme, quantum models are exploited to reduce computational complexity and training loss. As a particular case, Q-DDPG and Q-RDDPG are employed for trajectory optimization and dynamic resource allocation in UAV communications. Quantum circuits of the Q-DDPG schemes are described to showcase their implementation in noisy intermediate-scale quantum (NISQ) computers. The results demonstrate that Q-DDPG and Q-RDDPG schemes achieved higher rewards with lower training losses compared to classical DDPG.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21973784"
    },
    {
        "id": 18960,
        "title": "Optimizing UAV Computation Offloading via MEC with Deep Deterministic Policy Gradient",
        "authors": "Muhammad Usman Hadi, Ahmed Bashir Abbasi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Mobile edge computing (MEC) seems to be highly efficient to process the\ngenerated data from IoT devices by providing computational resources\nlocating in close range to network edge. MEC can be promising in\nreduction of latency and consumption of energy from data transmissions\nfrom offloading computational tasks from IoT devices to nearby edge\nservers. In this paper, a computation offloading optimization algorithm\nis proposed which is based on deep deterministic policy gradient for\nrealistic Aurelia X6 Pro unmanned aerial vehicle (UAV)-assisted MEC\nsystems. The proposed algorithm optimizes the offloading decision for\nUAVs by taking task characteristics and the communication environment\ninto consideration. The simulation yields outcomes indicating that the\nsuggested algorithm can considerably enhance the competency of MEC\nsystems.",
        "link": "http://dx.doi.org/10.22541/au.168414484.46468157/v1"
    },
    {
        "id": 18961,
        "title": "Gradient boosting machines",
        "authors": "Brandon M. Greenwell",
        "published": "2022-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003089032-8"
    },
    {
        "id": 18962,
        "title": "Combining Natural Gradient with Hessian Free Methods for Sequence Training",
        "authors": "Adnan Haider, Philip Woodland",
        "published": "2018-9-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2335"
    },
    {
        "id": 18963,
        "title": "Gravity-magnetic cross-gradient joint inversion by the cyclic gradient method",
        "authors": "Cong Sun, Yanfei Wang",
        "published": "2020-9-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/10556788.2020.1786565"
    },
    {
        "id": 18964,
        "title": "5 Stochastic gradient methods",
        "authors": "",
        "published": "2022-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783110783186-005"
    },
    {
        "id": 18965,
        "title": "Gradient descent-type methods",
        "authors": "Quoc Tran-Dinh, Marten van Dijk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-44-319037-7.00008-9"
    },
    {
        "id": 18966,
        "title": "Accelerated gradient methods with absolute and relative noise in the gradient",
        "authors": "Artem Vasin, Alexander Gasnikov, Pavel Dvurechensky, Vladimir Spokoiny",
        "published": "2023-11-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/10556788.2023.2212503"
    },
    {
        "id": 18967,
        "title": "Dai-Kou type conjugate gradient methods with a line search only using gradient",
        "authors": "Yuanyuan Huang, Changhe Liu",
        "published": "2017-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1186/s13660-017-1341-z"
    },
    {
        "id": 18968,
        "title": "Optimisation: Steepest Ascent, Steepest Descent and Gradient Methods",
        "authors": "Richard G. Brereton",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-409547-2.14835-8"
    },
    {
        "id": 18969,
        "title": "Particle Sedimentation Behaviors in a Density Gradient",
        "authors": "Pengsong Li",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-5190-6_4"
    },
    {
        "id": 18970,
        "title": "Detection of Epilepsy Through EEG Signals Using the DWT and Extreme Gradient Boosting Methods",
        "authors": "Erlina Agustin, Ade Eviyanti",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21070/ups.258"
    }
]