[
    {
        "id": 501,
        "title": "Comparing the Prediction of Numeric Patterns on Form C1 Using the K-Nearest Neighbors (K-NN) Method and a Combination of K-Nearest Neighbors (K-NN) with Connected Component Labeling (CCL)",
        "authors": "Uci Suriani, Tri Basuki Kurniawan",
        "published": "2023-12-3",
        "citations": 0,
        "abstract": "Indonesia's elections serve as a cornerstone of its democratic system, with the active participation of its citizens being of paramount importance. To bolster transparency and civic engagement during these elections, the SITUNG system (Election Result Information System) is employed for the tabulation of election results. However, the current tabulation process remains manual, potentially leading to data entry errors and a reduced accuracy of election outcomes. This research endeavor seeks to enhance the efficiency and accuracy of election result tabulation by employing the K-Nearest Neighbors (K-NN) method for recognizing numeric patterns on Form C1, both independently and in combination with Connected Component Labeling (CCL). The K-NN method demonstrates a commendable 60.0% accuracy in recognizing numeric patterns from the original Form C1 data. However, when combined with CCL, the accuracy drops to 51.2%. This research makes a significant contribution by simplifying the tabulation process and improving the accuracy of election results in Indonesia through the application of the K-NN method. The technology is anticipated to fortify democracy by promoting a more transparent and participatory electoral process for the citizens.",
        "link": "http://dx.doi.org/10.51519/journalisi.v5i4.592"
    },
    {
        "id": 502,
        "title": "197K-Nearest Neighbors (K-NN)",
        "authors": "Yasha Hasija, Rajkumar Chakraborty",
        "published": "2021-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003090113-10-10"
    },
    {
        "id": 503,
        "title": "Fuzzy k-NN classification with weights modified by most informative neighbors of nearest neighbors",
        "authors": "Nilgun Guler Bayazit, Ulug Bayazit",
        "published": "2019-6-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/jifs-18974"
    },
    {
        "id": 504,
        "title": "NS-k-NN: Neutrosophic Set-Based k-Nearest Neighbors Classifier",
        "authors": "Yaman Akbulut, Abdulkadir Sengur, Yanhui Guo, Florentin Smarandache",
        "published": "2017-9-2",
        "citations": 54,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/sym9090179"
    },
    {
        "id": 505,
        "title": "Klasifikasi Bumbu Dapur Indonesia Menggunakan Metode K-Nearest Neighbors (K-NN)",
        "authors": "Suastika Yulia Riska, Lia Farokhah",
        "published": "2021-6-30",
        "citations": 0,
        "abstract": "Bumbu adalah salah satu elemen yang sangat penting dalam sebuah masakan. Bumbu dapur atau rempah Indonesia memiliki jenis ragam yang sangat banyak. Kesalahan dalam pemilihan bumbu sangat berpengaruh terhadap rasa masakan. Image processing merupakan salah satu cabang ilmu dalam bidang teknologi yang dapat dimanfaatkan untuk mengenali objek citra yang ditangkap oleh kamera. Penelitian ini akan mengklasifikasikan jenis bumbu dapur yang hampir mirip yaitu jahe,lengkuas, kunyit dan kencur. Metode klasifikasi yang dipakai adalah K-Nearest Neighbors(K-NN). Pada penelitian ini kita menguji cara split data training dan data testing yaitu 66,7%: 33,33%, 75%:25% dan 90%:10%. Pembagian data training dan data testing menggunakan 90%:10% memiliki rata rata akurasi yang paling besar dibandingkan cara pembagian yang lain. Pemilihan K=3 atau K=5 memiliki rata rata akurasi yang hampir sama pada semua cara split data training dan data testing yaitu 64,66%: 65%. Pada K=1 memiliki akurasi yang cukup tinggi dibandingkan K sebelumnya yaitu 73%.",
        "link": "http://dx.doi.org/10.32664/smatika.v11i01.568"
    },
    {
        "id": 506,
        "title": "SelB-k-NN: A Mini-Batch K-Nearest Neighbors Algorithm on AI Processors",
        "authors": "Yifeng Tang, Cho-Li Wang",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipdps54959.2023.00088"
    },
    {
        "id": 507,
        "title": "Integrated Effect of Nearest Neighbors and Distance Measures in k-NN Algorithm",
        "authors": "Rashmi Agrawal",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-10-6620-7_74"
    },
    {
        "id": 508,
        "title": "Lung Cancer Stage Classification Utilizing k-Nearest Neighbors (k-NN) and Convolutional Neural Networks (CNN)",
        "authors": "Venna Venkata Reddy, Pokuri Venkata Pavan Kumar, Koneru Suvarna Vani",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425683"
    },
    {
        "id": 509,
        "title": "K<sup>2</sup>NN: Self-Supervised Learning with Hierarchical Nearest Neighbors for Remote Sensing",
        "authors": "Jianlong Yuan, Yuanhong Xu, Zhibin Wang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096425"
    },
    {
        "id": 510,
        "title": "IMPLEMENTASI DATA MINING UNTUK MENENTUKAN KELAYAKAN PEMBERIAN KREDIT DENGAN MENGGUNAKAN ALGORITMA  K-NEAREST NEIGHBORS (K-NN)",
        "authors": "Tupan Tri Muryono, Irwansyah Irwansyah",
        "published": "2020-6-11",
        "citations": 0,
        "abstract": "The banking world in terms of lending to customers is routine activities that are at high risk. In its execution, the problematic credit or bad credit is often due to the lack of careful credit analysis in the process of granting credit, as well as from poor customers. The purpose of this study is to implement data mining to assist in conducting credit analysis process in order to produce the right information whether the customer who will apply for the credit is worthy or not to be able to see the potential payment by the customer. The attributes used in this study consist of 11 attributes i.e. marital status, number of liabilities, age, last education, occupation, monthly income, home ownership, warranties, loan amount, length of loan and description as a result attribute. The methods of data collection used are observation, interviews, and documentation.  The method used in this study is K-Nearest Neighbor (K-NN). From the results of evaluation and validation using the K-5 fold that has been done using the RapidMiner tools obtained the highest accuracy results from the K-Nearest Neighbor (K-NN) method of 93.33% in the 5th test.",
        "link": "http://dx.doi.org/10.37365/jti.v6i1.78"
    },
    {
        "id": 511,
        "title": "IMPLEMENTASI DATA MINING UNTUK MENENTUKAN KELAYAKAN PEMBERIAN KREDIT DENGAN MENGGUNAKAN ALGORITMA  K-NEAREST NEIGHBORS (K-NN)",
        "authors": "Tupan Tri Muryono, Irwansyah Irwansyah",
        "published": "2020-6-11",
        "citations": 0,
        "abstract": "The banking world in terms of lending to customers is routine activities that are at high risk. In its execution, the problematic credit or bad credit is often due to the lack of careful credit analysis in the process of granting credit, as well as from poor customers. The purpose of this study is to implement data mining to assist in conducting credit analysis process in order to produce the right information whether the customer who will apply for the credit is worthy or not to be able to see the potential payment by the customer. The attributes used in this study consist of 11 attributes i.e. marital status, number of liabilities, age, last education, occupation, monthly income, home ownership, warranties, loan amount, length of loan and description as a result attribute. The methods of data collection used are observation, interviews, and documentation.  The method used in this study is K-Nearest Neighbor (K-NN). From the results of evaluation and validation using the K-5 fold that has been done using the RapidMiner tools obtained the highest accuracy results from the K-Nearest Neighbor (K-NN) method of 93.33% in the 5th test.",
        "link": "http://dx.doi.org/10.37365/it.v6i1.78"
    },
    {
        "id": 512,
        "title": "Application of the K-Nearest Neighbors (K-NN) Algorithm for Classification of Heart Failure",
        "authors": "Ryan Yunus, Uli Ulfa, Melinna Dwi Safitri",
        "published": "2021-5-10",
        "citations": 4,
        "abstract": "Heart failure is a type of disease that has the largest number of patients in the world. Based on information from the data center, there were 229,696 people with heart failure in 2013. Lack of public knowledge about what indications of a person having heart failure make the main cause not handled properly by heart failure patients. In this study, data classification was carried out using KNN algorithm because it has a simple calculation and has a fast time. This study only uses 12 attributes, while the previous study compared 6 algorithms with 13 attributes from 299 data. The highest algorithm with 94.31% accuracy by Random Forest while KNN had an accuracy rate of 86.95% with the same data. In this study, the accuracy of the sample data was compared between 20 data and 299 total data. Both of them have different accuracy. 20 sample data has an accuracy rate of 89.29% while 299 data has an accuracy rate of 96.66%.",
        "link": "http://dx.doi.org/10.33633/jais.v6i1.4513"
    },
    {
        "id": 513,
        "title": "Analisis Metode K-Nearest Neighbors (K-NN) Dan Naive Bayes Dalam Memprediksi Kelulusan Mahasiswa",
        "authors": "Kartarina Kartarina, Ni Ketut Sriwinarti, Ni luh Putu Juniarti",
        "published": "2021-8-14",
        "citations": 0,
        "abstract": "In this research the author aims to apply the K-NN and Naive Bayes algorithms for predicting student graduation rates at Sekolah Tinggi Pariwisata (STP) Mataram, The comparison of these two methods was carried out because based on several previous studies it was found that K-NN and Naive Bayes are well-known classification methods with a good level of accuracy. But which one has a better accuracy rate than the two algorithms, that's what researchers are trying to do. The output of this application is in the form of information on the prediction of student graduation, whether to graduate on time or not on time. The selection of STP as the research location was carried out because of the imbalance between the entry and exit of students who had completed their studies. Students who enter have a large number, but students who graduate on time according to the provisions are far very small, resulting in accumulation of the high number of students in each period of graduation, so it takes the initial predictions to quickly overcome these problems. Based on the results of designing, implementing, testing, and testing the Student Graduation Prediction Application program using the K-NN and Naive Bayes Methods with the Cross Validation method, the result is an accuracy for the K-NN method of 96.18% and for the Naive Bayes method an accuracy of 91.94% with using the RapideMiner accuracy test. So based on the results of the two tests between the K-NN and Naive Bayes methods which produce the highest accuracy, namely the K-NN method with an accuracy of 96.18%. So it can be concluded that the K-NN method is more feasible to use to predict student graduation",
        "link": "http://dx.doi.org/10.35746/jtim.v3i2.159"
    },
    {
        "id": 514,
        "title": "Perbandingan Algoritma k-Nearest Neighbors (k-NN) dan Support Vector Machines (SVM) untuk Klasifikasi Pengenalan Citra Wajah",
        "authors": "Parasian DP Silitonga, Romanus Damanik",
        "published": "2021-7-31",
        "citations": 0,
        "abstract": "Abstract- The study of face recognition is one of the areas of computer vision that requires significant research at the moment. Numerous researchers have conducted studies on facial image recognition using a variety of techniques or methods to achieve the highest level of accuracy possible when recognizing a person's face from existing images. However, recognizing the image of a human face is not easy for a computer. As a result, several approaches were taken to resolve this issue. This study compares two (two) machine learning algorithms for facial image recognition to determine which algorithm has the highest level of accuracy, precision, recall, and AUC. The comparison is carried out in the following steps: image acquisition, preprocessing, feature extraction, face classification, training, and testing. Based on the stages and experiments conducted on public image datasets, it is concluded that the SVM algorithm, on average, has a higher level of accuracy, precision, and recall than the k-NN algorithm when the dataset proportion is 90:10. While the k-NN algorithm has the highest similarity in terms of accuracy, precision, and recall at 80%: 20% and 70%: 30% of 99.20. However, for the highest AUC percentage level, the k-NN algorithm outperforms SVM at a dataset proportion of 80%: 20% at 100%.",
        "link": "http://dx.doi.org/10.36054/jict-ikmi.v20i1.354"
    },
    {
        "id": 515,
        "title": "PL-k NN: A Parameterless Nearest Neighbors Classifier",
        "authors": "Danilo Samuel Jodas, Leandro Aparecido Passos, Ahsan Adeel, João Paulo Papa",
        "published": "2022-6-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwssip55020.2022.9854445"
    },
    {
        "id": 516,
        "title": "Sensor virtual para classificação de emissões de so2 baseado em k-NN (k-Nearest Neighbors)",
        "authors": "Paolla Marlene Caetano da Cunha, Gustavo Matheus de Almeida",
        "published": "2021-8-22",
        "citations": 0,
        "abstract": "As indústrias em geral buscam cada vez mais, além do aumento de produção com qualidade, redução de custos e operações mais seguras, uma produção mais limpa. De outro lado, o uso de modelos obtidos diretamente a partir de dados históricos sobre as operações se fortaleceu a partir da geração massiva de dados pelos processos industriais. Outro fator que contribuiu para esse avanço foi a aumento da complexidade das operações com o tempo. O objetivo deste trabalho foi construir um sensor virtual para a classificação de seis categorias de emissões de SO2 em uma caldeira de recuperação química de uma fábrica de celulose kraft no Brasil. Baseou-se esse sensor no algoritmo k-vizinhos mais próximos (k-NN; k-Nearest Neighbors), que é uma técnica de aprendizado supervisionado. Testaram-se três abordagens para a obtenção dos modelos k-NN: univariável (usada como referência), a partir de subconjuntos pré-selecionados de preditores, e a partir de comitês de modelos k-NN. O melhor desempenho ocorreu para um comitê de vinte e cinco modelos k-NN, com oito preditores, k (número de vizinhos mais próximos) igual a 2 e distância de Manhattan. Foi possível aumentar a acurácia de classificação das classes minoritárias (4 a 6), sem perda de desempenho sobre as classes majoritárias (1 a 3), como usualmente ocorre em problemas de classificação com dados desbalanceados. Para a classe 6 por exemplo, com o menor número de observações, a precisão passou de 0.794 para 0.906, o recall, de 0.640 para 0.715, a F1-score, de 0.693 para 0.791, e a g-mean de 0.797 para 0.843, sem perda de generalização para as demais classes, em comparação com a abordagem de subconjuntos de preditores. Um sensor virtual pode ser útil como uma fonte de informação redundante sobre variáveis-chave de processo, ou para antecipar resultados de análises de laboratório, o que demanda um tempo razoável em geral, entre outras aplicações.",
        "link": "http://dx.doi.org/10.6008/cbpc2179-6858.2021.009.0022"
    },
    {
        "id": 517,
        "title": "Comparison K-Nearest Neighbors (K-NN) and Artificial Neural Network (ANN) in Real Time Entrants Recognition",
        "authors": "Christin Panjaitan, Aldo Silaban, Mikhael Napitupulu, Joni Welman Simatupang",
        "published": "2018-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isriti.2018.8864366"
    },
    {
        "id": 518,
        "title": "Increasing Generalizability: Naïve Bayes Vs K-Nearest Neighbors",
        "authors": "",
        "published": "2022-5-30",
        "citations": 0,
        "abstract": "Marketing research is often criticized for lacking generalizability and inability to reproduce results. The problem lies in using models to fit data, rather than determining the predictive power of models in conditions of uncertainty. For instance, how does the predictive power of a model change when customer dynamics change? The current study suggests that marketing researchers can supplement existing research methods with non-probabilistic prediction methods, such as the kNN algorithm-based model. Unlike probabilistic models that rely on past outcomes to predict future events – and lose predictive power when newer events are observed - non-probabilistic models better capture uncertainty. In the current study, the predictive power of the kNN algorithmbased model and the Naïve Bayes model is compared using data from two real markets. The kNN algorithm-based model provides more accurate predictions, showing the utility of combining the kNN algorithm-based model with existing marketing research to improve the predictability and generalizability of models. Implications for research and future research are discussed.",
        "link": "http://dx.doi.org/10.33140/jrar.03.02.07"
    },
    {
        "id": 519,
        "title": "Food Detection Using Histogram of Oriented Gradient (HOG) as Feature Extraction and K-Nearest Neighbors (K-NN) as Classifier",
        "authors": "Diah Rahmadani",
        "published": "2020-9-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.30534/ijatcse/2020/3191.52020"
    },
    {
        "id": 520,
        "title": "Comparison of Colorectal Cancer Classification between K-Nearest Neighbors (K-NN) and Neural Network",
        "authors": "F Zhafarina, Z Rustam, Y Amalia, I Wirasati",
        "published": "2021-3-1",
        "citations": 0,
        "abstract": "Abstract\nMachine learning is one of the technologies used in medicine. Machine learning can help detect various kinds of problems in the medical field and enables a process to be faster and more efficient. Cancer is one of the most dangerous diseases in the world. Machine learning is widely used in bioinformatics and particularly in cancer diagnosis. One of the most popular methods is K-nearest neighbors (K-NN) and Neural Network. There are supervised learning methods. Using K-NN, the quality of the results depends largely on the distance and the value of the parameter “k” which represents the number of the nearest neighbors. This research is explains the classification of colorectal cancer by using K-NN with different k values and Neural Network Classification. Our work will be performed on the Colorectal Cancer dataset obtained by the Al-Islam Hospital, Bandung, Indonesia and it consists of benign cases 163 and malignant cases 47 samples. Thus, the final result indicates better performance for K-nearest neighbors’ accuracy is 0.786 in K-parameter equal to 7, 9, 11 has the same accuracy with 60% data training and Neural Network reached 0.904 with 90% of data training.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1821/1/012014"
    },
    {
        "id": 521,
        "title": "Nearest Neighbors",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_100845"
    },
    {
        "id": 522,
        "title": "All-Nearest-Neighbors",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_100054"
    },
    {
        "id": 523,
        "title": "Least Absolute Shrinkage and Selection Operator (LASSO) and k-Nearest Neighbors (k-NN) Algorithm Analysis Based on Feature Selection for Diamond Price Prediction",
        "authors": "Shafilah Ahmad Fitriani, Yuli Astuti, Irma Rofni Wulandari",
        "published": "2022-1-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismode53584.2022.9742936"
    },
    {
        "id": 524,
        "title": "KNN-DK: A Modified K-NN Classifier with Dynamic k Nearest Neighbors",
        "authors": "Nazrul Hoque, Dhruba K. Bhattacharyya, Jugal K. Kalita",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6919-1_2"
    },
    {
        "id": 525,
        "title": "Use of nearest neighbors (k-NN) algorithm in tool condition identification in the case of drilling in melamine faced particleboard",
        "authors": "Albina Jegorowa, Jarostaw Górski, Jarostaw Kurek, Michat Kruk",
        "published": "2020",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4067/s0718-221x2020005000205"
    },
    {
        "id": 526,
        "title": "All-k-Nearest Neighbors",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_100052"
    },
    {
        "id": 527,
        "title": "Explaining Inaccurate Predictions of Models through k-Nearest Neighbors",
        "authors": "Zeki Bilgin, Murat Gunestas",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010257902280236"
    },
    {
        "id": 528,
        "title": "KLASIFIKASI DATA MINING UNTUK MENENTUKAN KUALITAS UDARA  DI PROVINSI DKI JAKARTA MENGGUNAKAN ALGORITMA  K-NEAREST NEIGHBORS (K-NN)",
        "authors": "Ade Davy Wiranata, Soleman Soleman, Irwansyah Irwansyah, I Ketut Sudaryana, Rizal Rizal",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "Air plays an important role in maintaining the life of living things on earth. Metabolic processes that occur in the bodies of living things cannot take place without oxygen from the air. The air pollution problem in DKI Jakarta is very serious and can cause health problems such as irritation of the respiratory tract, respiratory diseases, and long-term health problems such as cardiovascular disease and lung cancer. Air pollution can also affect environmental quality, reduce visibility, and damage ecosystems. The purpose of this study is to determine the accuracy of classifying air quality in DKI Jakarta province. The data mining method that the author uses is the K-Nearest Neighbors (K-NN) algorithm. From the results of the evaluation process of the K-Nearest Neighbors (K-NN) algorithm using the K-5 fold that has been carried out using the RapidMiner tool, the results of K-2 fold accuracy of 73.97%, K-3 fold accuracy of 72.60%, K-4 fold accuracy of 72.60%, and K-5 fold accuracy of 75.35%.",
        "link": "http://dx.doi.org/10.37365/jti.v9i1.164"
    },
    {
        "id": 529,
        "title": "Perbandingan Algoritma K-Nearest Neighbors (K-NN) dan Random forest terhadap Penyakit Gagal Jantung",
        "authors": "Fredilio Fredilio, Julfikar Rahmad, Stiven Hamonangan Sinurat, Daniel Ryan Hamonangan Sitompul, Dennis Jusuf Ziegel, Evta Indra",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Penelitian ini bertujuan untuk membandingkan akurasi algoritma K-Nearest Neighbor (K-NN) dan Random Forest dalam mengklasifikasikan penyebab penyakit gagal jantung. Penyakit ini menjadi salah satu penyebab utama kematian di seluruh dunia dan kasusnya terus meningkat di Indonesia. Oleh karena itu, penanganan dan klasifikasi dini terhadap penyebab gagal jantung sangat diperlukan untuk mencegah penyakit tersebut. Penelitian ini diharapkan dapat memberikan informasi tentang metode terbaik untuk mengklasifikasikan penyebab penyakit gagal jantung serta memberikan manfaat bagi tenaga medis dan masyarakat umum dalam menjaga kesehatan jantung mereka.",
        "link": "http://dx.doi.org/10.37012/jtik.v9i1.1432"
    },
    {
        "id": 530,
        "title": "Classification of rice plant nitrogen nutrient status using k-nearest neighbors (k-NN) with light intensity data",
        "authors": "Muliady Muliady, Lim Tien Sze, Koo Voon Chet, Suhadra Patra",
        "published": "2021-4-1",
        "citations": 2,
        "abstract": "<span>Crop management including the efficient use of nitrogen (N) fertilizer is important to ensure crop productivity. Human error in judging the leaf greenness when using the leaf color chart (LCC) to estimate the rice plant N nutrient status has encouraged numerous researchers to implement a machine-learning algorithm but experienced some issues in calibration and lighting. The datasets are created at 6.00-7.00AM (consistent lighting) and including light intensity, so each dataset contains RGB value and light intensity as inputs, and LCC value as a target. A system consists of a smartphone with an application that prevents user from taking an image if the light intensity is not in 2000-3500 lux, and a computer for preprocessing and classification purposes were developed. The preprocessing included cropping, splitting the rice leaf images, and calculating the average RGB values. A k-NN classifier is implemented and by using a cross-validation method is found k=5 gives the best accuracy of 97,22%. The in-site test of the system also works with an accuracy of 96.40%. </span>",
        "link": "http://dx.doi.org/10.11591/ijeecs.v22.i1.pp179-186"
    },
    {
        "id": 531,
        "title": "Reverse-k-Nearest-Neighbors aNN",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-17885-1_101110"
    },
    {
        "id": 532,
        "title": "Nearest Neighbors Search Algorithm for High Dimensional Data",
        "authors": "Vasanthi G",
        "published": "2020-7-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5373/jardcs/v12sp8/20202636"
    },
    {
        "id": 533,
        "title": "Classification of Mango Fruit Quality Based on Texture Characteristics of GLCM (Gray Level Co-Occurrence Matrices) with Algorithm K-NN (K-Nearest Neighbors)",
        "authors": "Wahyu Wijaya Widiyanto, Eko Purwanto, Kusrini Kusrini",
        "published": "2019-4-30",
        "citations": 0,
        "abstract": "Proses klasifikasi kualitas mutu buah mangga dengan cara konvensional menggunakan mata manusia memiliki kelemahan di antaranya membutuhkan tenaga lebih banyak untuk memilah, anggapan mutu kualitas buah mangga antar manusia yang berbeda, tingkat konsistensi manusia dalam menilai kualitas mutu buah mangga yang tidak menjamin valid karena manusia dapat mengalami kelelahan. Penelitian ini bertujuan untuk klasifikasi kualitas mutu buah mangga ke dalam tiga kelas mutu yaitu kelas Super, A, dan B dengan computer vision dan algoritma k-Nearest Neighbor. Hasil pengujian menggunakan jumlah k tetangga 9 menunjukan tingkat akurasi sebesar 88,88%.Kata-kata kunci— Klasifikasi, GLCM, K-Nearest Neighbour, Mangga",
        "link": "http://dx.doi.org/10.30595/techno.v20i1.3816"
    },
    {
        "id": 534,
        "title": "Residual K-Nearest Neighbors Label Distribution Learning",
        "authors": "Jing Wang, Xin Geng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4332113"
    },
    {
        "id": 535,
        "title": "K-Nearest Neighbors",
        "authors": "Poornachandra Sarang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45633-6_7"
    },
    {
        "id": 536,
        "title": "Nearest Neighbors",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-23161-2_300829"
    },
    {
        "id": 537,
        "title": "Increasing Generalizability: Naïve Bayes Vs K-Nearest Neighbors",
        "authors": "Fahad Mansoor Pasha",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nMarketing research is often criticized for lacking generalizability and inability to reproduce results. The problem lies in using models to fit data, rather than determining the predictive power of models in conditions of uncertainty. For instance, how does the predictive power of a model change when customer dynamics change? The current study suggests that marketing researchers can supplement existing research methods with non-probabilistic prediction methods, such as the kNN algorithm-based model. Unlike probabilistic models that rely on past outcomes to predict future events – and lose predictive power when newer events are observed - non-probabilistic models better capture uncertainty. In the current study, the predictive power of the kNN algorithm-based model and the Naïve Bayes model is compared using data from two real markets. The kNN algorithm-based model provides more accurate predictions, showing the utility of combining the kNN algorithm-based model with existing marketing research to improve the predictability and generalizability of models. Implications for research and future research are discussed.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1578985/v1"
    },
    {
        "id": 538,
        "title": "ASPECT-BASED SENTIMENT ANALYSIS TERHADAP ULASAN APLIKASI FLIP MENGGUNAKAN PEMBOBOTAN TERM FREQUENCY-INVERSE DOCUMENT FREQUENCY (TF-IDF) DENGAN METODE KLASIFIKASI K-NEAREST NEIGHBORS (K-NN)",
        "authors": "Ferda Ayu Dwi Putri Febrianti, Faqih Hamami, Riska Yanu Fa’rifah",
        "published": "2023-9-10",
        "citations": 0,
        "abstract": "The rapid growth of online transactions in Indonesia has increased the demand for efficient  interbank transfer solutions. However, the costs associated with such transactions have become a significant obstacle. Flip, a company with a vision to become a global leader in customer satisfaction-driven services, offers a solution to this challenge. This study proposes an aspect-based sentiment analysis method using the K-Nearest Neighbors (K-NN) algorithm to analyze user sentiment on key aspects, namely speed, security, and the cost of using the Flip application. The results of this research provide valuable information that can be used as a basis to provide insights, suggestions and recommendations to businesses, so they can create better solutions and promote optimal user experience. The research results show that the K-NN model has the ability to predict user psychology well in all aspects, with a significant level of accuracy, specifically speed (73.04%), security (86, 05%) and costs (80.11%). In addition, this study also compares two model validation methods: simple  data splitting method and K-Fold cross-validation. Although the simple data splitting method has a higher average accuracy, K-fold cross-validation is considered superior as it provides a more accurate and reliable estimate of the overall performance of the model. Sentiment analysis results  show that Flip app users tend to give negative feedback on speed and security, while they give positive feedback on cost. Therefore, the main recommendation is that the company PT Fliptech Lentera Inspirasi Pertiwi improves the speed and security aspects to increase user satisfaction with the Flip application. Therefore, this customer-centric service will continue to prioritize user satisfaction as its primary goal.",
        "link": "http://dx.doi.org/10.35870/jimik.v4i3.429"
    },
    {
        "id": 539,
        "title": "Perbandingan Rapid Centroid Estimation (RCE) — K Nearest Neighbor (K-NN) Dengan K Means — K Nearest Neighbor (K-NN)",
        "authors": "Khairul Umam Syaliman, M. Zulfahmi, Aldi Abdillah Nababan",
        "published": "2017-9-3",
        "citations": 0,
        "abstract": "Teknik Clustering terbukti dapat meningkatkan akurasi dalam melakukan klasifikasi, terutama pada algoritma K-Nearest Neighbor (K-NN). Setiap data dari setiap kelas akan membentuk K cluster yang kemudian nilai centroid akhir dari setiap cluster pada setiap kelas data tersebut akan dijadikan data acuan untuk melakukan proses klasifikasi menggunakan algoritma K-NN. Namun kendala dari banyaknya teknik clustering adalah biaya komputasi yang mahal, Rapid Centroid Estimation (RCE) dan K-Means termasuk kedalam teknik clustering dengan biaya komputasi yang murah. Untuk melihat manakah dari kedua algoritma ini (RCE dan K-Means) yang lebih baik memberikan peningkatan akurasi pada algoritma K-NN maka, pada penelitian ini akan mencoba untuk membandingkan kedua algoritma tersebut. Hasil dari penelitian ini adalah gabungan RCE—K-NN memberikan hasil akurasi yang lebih baik dari K-Means—K-NN pada data set iris dan wine. Namun dalam perubahan nilai akurasi RCE—K-NN lebih stabil hanya pada data set iris. Sedangkan pada data set wine, K-Means—K-NN terlihat mendapati perubahan akurasi yang lebih stabil dibandingkan RCE—K-NN.",
        "link": "http://dx.doi.org/10.30743/infotekjar.v2i1.166"
    },
    {
        "id": 540,
        "title": "A Comparative Analysis of N-Nearest Neighbors (N3) and Binned Nearest Neighbors (BNN) Algorithms for Indoor Localization",
        "authors": "Serpil Ustebay, M. Ali Aydin, Ahmet Sertbas, Tulin Atmaca",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-59767-6_7"
    },
    {
        "id": 541,
        "title": "Weighted k-Nearest Neighbors Feature Selection (WkNN-FS)",
        "authors": "Peter Drotár",
        "published": "2019-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31988/scitrends.48207"
    },
    {
        "id": 542,
        "title": "Things You Might Not Know about the k-Nearest Neighbors Algorithm",
        "authors": "Aleksandra Karpus, Marta Raczyńska, Adam Przybylek",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008365005390547"
    },
    {
        "id": 543,
        "title": "Unsupervised Word Segmentation using K Nearest Neighbors",
        "authors": "Tzeviya Fuchs, Yedid Hoshen, Yossi Keshet",
        "published": "2022-9-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-11474"
    },
    {
        "id": 544,
        "title": "K-nearest neighbors in R",
        "authors": "Nima Rezaei, Parnian Jabbari",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-822400-7.00006-3"
    },
    {
        "id": 545,
        "title": "K-Nearest Neighbors",
        "authors": "Chuck Easttom",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003230588-16"
    },
    {
        "id": 546,
        "title": "Voice Conversion With Just Nearest Neighbors",
        "authors": "Matthew Baas, Benjamin van Niekerk, Herman Kamper",
        "published": "2023-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-419"
    },
    {
        "id": 547,
        "title": "K-Nearest Neighbors",
        "authors": "Brad Boehmke, Brandon Greenwell",
        "published": "2019-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367816377-8"
    },
    {
        "id": 548,
        "title": "Explainable Outlier Detection Using Feature Ranking for k-Nearest Neighbors, Gaussian Mixture Model and Autoencoders",
        "authors": "Lucas Krenmayr, Markus Goldstein",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011631900003411"
    },
    {
        "id": 549,
        "title": "The Performance Analysis of K-Nearest Neighbors (K-NN) Algorithm for Motor Imagery Classification Based on EEG Signal",
        "authors": "Nurul E’zzati Md Isa, Amiza Amir, Mohd Zaizu Ilyas, Mohammad Shahrazel Razalli",
        "published": "2017",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1051/matecconf/201714001024"
    },
    {
        "id": 550,
        "title": "Nearest Neighbors of Multivariate Runs",
        "authors": "Yong Kong",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4614-8414-1_63-1"
    },
    {
        "id": 551,
        "title": "The Polypharmacology Browser PPB2: Target Prediction Combining Nearest Neighbors with Machine Learning",
        "authors": "Mahendra Awale, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>Here we report PPB2 as a target prediction tool assigning targets to a query molecule based on ChEMBL data. PPB2 computes ligand similarities using molecular fingerprints encoding composition (MQN), molecular shape and pharmacophores (Xfp), and substructures (ECfp4), and features an unprecedented combination of nearest neighbor (NN) searches and Naïve Bayes (NB) machine learning, together with simple NN searches, NB and Deep Neural Network (DNN) machine learning models as further options. Although NN(ECfp4) gives the best results in terms of recall in a 10-fold cross-validation study, combining NN searches with NB machine learning provides superior precision statistics, as well as better results in a case study predicting off-targets of a recently reported TRPV6 calcium channel inhibitor, illustrating the value of this combined approach. PPB2 is available to assess possible off-targets of small molecule drug-like compounds by public access at ppb2.gdb.tools.</div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.6895646"
    },
    {
        "id": 552,
        "title": "k-Nearest Neighbors",
        "authors": "Amin Zollanvari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-33342-2_5"
    },
    {
        "id": 553,
        "title": "Can Reverse Nearest Neighbors Perceive Unknowns?",
        "authors": "Payel Sadhukhan",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2963471"
    },
    {
        "id": 554,
        "title": "A Novel Nearest Neighbors Algorithm Based on Power Muirhead Mean",
        "authors": "Kourosh Shahnazari, Seyed Moein Ayyoubzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nK-Nearest Neighbors algorithm is one of the most used classifiers in terms of simplicity and performance. Although, when a dataset has many outliers or when it is small or unbalanced, KNN doesn't work well. This paper aims to propose a novel classifier, based on K-Nearest Neighbors which calculates the local means of every class using the Power Muirhead Mean operator to overcome alluded issues. We called our new algorithm Power Muirhead Mean K-Nearest Neighbors (PMM-KNN). Eventually, we used five well-known datasets to assess PMM-KNN performance. The research results demonstrate that the PMM-KNN has outperformed three state-of-the-art classification methods in all experiments.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2432411/v1"
    },
    {
        "id": 555,
        "title": "Applying k-Nearest Neighbors to Increase the Utility of k-Anonymity",
        "authors": "Abdulrahman Almohaimeed, Srikanth Gampa",
        "published": "2019-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon42311.2019.9020525"
    },
    {
        "id": 556,
        "title": "A Probabilistic Nearest Neighbors Based Locality Preserving Projections",
        "authors": "Alexandre Levada",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4267754"
    },
    {
        "id": 557,
        "title": "Cosine Approximate Nearest Neighbors",
        "authors": "David C. Anastasiu",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-19287-7_6"
    },
    {
        "id": 558,
        "title": "Incorporating Connectivity in k-Nearest Neighbors Regression",
        "authors": "Mohamed A. Mahfouz",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217502"
    },
    {
        "id": 559,
        "title": "Classification of Public Opinion on Social Media Twitter concerning the Education in Indonesia Using the K-Nearest Neighbors  (K-NN) Algorithm and K-Fold Cross Validation",
        "authors": " Intan Monica Hanmastiana,  Budi Warsito,  Rita Rahmawati,  Hasbi Yasin,  Puspita Kartikasari",
        "published": "2022-1-30",
        "citations": 0,
        "abstract": "Developing country is a country that has perspective and idea which reflect its awareness of the importance of advancing the education sector. Assessment of the quality of education in Indonesia from the perspective of the community gets different responses. Therefore, it makes people respond differently. The community response is often found on social media, one of which is Twitter. Twitter is one of the application service that is popular due to its uses to interact and communicate with people in daily life. The sentiment analysis on Twitter can be a choice to see the community’s responses to the condition of education in Indonesia. The responses are classified into positive sentiments and negative sentiments using the K-Nearest Neighbors (K-NN) algorithm with a 10-fold cross validation model evaluation. K-NN has several advantages, they are fast training, simple, easy to learn, resistance toward training data which has noise, and effective if the training data is large. In this study, the sentiment classification uses Cosine Similarity distance measurement and four k value parameters which are 3, 5, 7, and 9. Data labelling is done manually and done by scoring sentiment. Visualization of positive and negative sentiments use Word Cloud. The test results show that public sentiment about education tends to be positive on Twitter and the parameter k = 7 obtained the highest accuracy value in data labelling that was done manually and done by scoring sentiment. In labelling data manually, it obtained an accuracy of 76.93% whereas, in labelling the data with scoring sentiment, it obtained an accuracy of 77.87%. Sentiment analysis is made using the RStudio programming language as the support software.",
        "link": "http://dx.doi.org/10.29313/statistika.v21i2.297"
    },
    {
        "id": 560,
        "title": "Nine-Point Nearest Neighbors Finite Difference Method",
        "authors": "Charles White",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceaa52647.2021.9539539"
    },
    {
        "id": 561,
        "title": "The Polypharmacology Browser PPB2: Target Prediction Combining Nearest Neighbors with Machine Learning",
        "authors": "Mahendra Awale, Jean-Louis Reymond",
        "published": "No Date",
        "citations": 1,
        "abstract": "Here we report PPB2 as a target prediction tool assigning targets to a query molecule based on ChEMBL data. PPB2 computes ligand similarities using molecular fingerprints encoding composition (MQN), molecular shape and pharmacophores (Xfp), and substructures (ECfp4), and features an unprecedented combination of nearest neighbor (NN) searches and Naïve Bayes (NB) machine learning, together with simple NN searches, NB and Deep Neural Network (DNN) machine learning models as further options. Although NN(ECfp4) gives the best results in terms of recall in a 10-fold cross-validation study, combining NN searches with NB machine learning provides superior precision statistics, as well as better results in a case study predicting off-targets of a recently reported TRPV6 calcium channel inhibitor, illustrating the value of this combined approach. PPB2 is available to assess possible off-targets of small molecule drug-like compounds by public access at ppb2.gdb.tools.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.6895646.v1"
    },
    {
        "id": 562,
        "title": "Enhancing Diabetic Retinopathy Detection Using CNNs with Dimensionality Reduction Techniques and K-Nearest Neighbors Ensembles",
        "authors": "Chaymaa Lahmar, Ali Idri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012191900003598"
    },
    {
        "id": 563,
        "title": "Double Ensemble kNN: Towards an Enhanced k-Nearest-Neighbors",
        "authors": "Dhruv Roongta",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4286996"
    },
    {
        "id": 564,
        "title": "k‐Nearest Neighbors",
        "authors": "",
        "published": "2020-4-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119591542.ch6"
    },
    {
        "id": 565,
        "title": "Regression I: K-nearest neighbors",
        "authors": "Tiffany Timbers, Trevor Campbell, Melissa Lee",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003080978-7"
    },
    {
        "id": 566,
        "title": "Nearest Neighbors between Moving Points in a 2d Lattice",
        "authors": "Victor Gorelik",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4214132"
    },
    {
        "id": 567,
        "title": "Nearest Neighbors",
        "authors": "Jan Žižka, František Dařena, Arnošt Svoboda",
        "published": "2019-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429469275-6"
    },
    {
        "id": 568,
        "title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning",
        "authors": "Robin Algayres, Adel Nabli, Benoît Sagot, Emmanuel Dupoux",
        "published": "2022-9-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-226"
    },
    {
        "id": 569,
        "title": "Hierarchical Belief K-Nearest Neighbors for Human Activity Recognition",
        "authors": "Yilin Dong, Yong Zhou",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240988"
    },
    {
        "id": 570,
        "title": "On the Robustness of Deep K-Nearest Neighbors",
        "authors": "Chawin Sitawarin, David Wagner",
        "published": "2019-5",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spw.2019.00014"
    },
    {
        "id": 571,
        "title": "Hjorth features and k-nearest neighbors algorithm for visual imagery classification",
        "authors": "Fabio Ricardo Llorella Costa, Gustavo Patow",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nVisual imagery is an interesting paradigm for use in Brain-Computer Interface systems. Through visual imagery we can extend the potential of BCI systems beyond motor imagery or evoked potentials. In this work we have studied the possibility of classifying different visual imagery shapes in the time domain using EEG signals, with the Hjorth parameters and k-nearest neighbors classifier 69% accuracy has been obtained with a Cohen's kappa value of 0.64 in the classification of seven geometric shapes, obtaining results superior to other related works.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-711349/v1"
    },
    {
        "id": 572,
        "title": "k Nearest Neighbors",
        "authors": "Frank Acito",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45630-5_10"
    },
    {
        "id": 573,
        "title": "Decision letter: Automated analysis of long-term grooming behavior in Drosophila using a k-nearest neighbors classifier",
        "authors": "",
        "published": "2018-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.34497.041"
    },
    {
        "id": 574,
        "title": "Perbandingan Algoritma Naïve Bayes dan K-Nearest Neighbors untuk Klasifikasi Metabolik Sindrom",
        "authors": "Fitriana Sholekhah, Adinda Dwi Putri, Rahmaddeni Rahmaddeni, Luasiana Efrizoni",
        "published": "2024-2-24",
        "citations": 0,
        "abstract": "Kondisi medis yang dikenal sebagai sindrom metabolik berpotensi meningkatkan kemungkinan penyakit jantung koroner, stroke, serangan jantung dan diabetes tipe 2. Sindrom metabolik juga dapat menyebabkan gula darah tinggi, kadar kolesterol rendah, obesitas secara bersamaan dan kelebihan lemak di daerah pinggang. Jika kombinasi dari ketiga kondisi ini terjadi maka dapat dikatakan penyakit ini  sebagai sindrom metabolik. Selain itu, sindrom metabolik juga dikaitkan dengan resistensi insulin, artinya dimana sel-sel tubuh tidak merespon baik terhadap efek insulin yang menyebabkan kadar gula darah tinggi karena gula tidak terserap ke dalam sel dengan baik. Sindrom metabolik tumbuh seiring meningkatnya obesitas di Asia, dengan perkiraan prevalensi yang terus naik. Ini berpotensi meningkatkan kasus penyakit kardiovaskular dan risiko kematian. Oleh karena itu, perlu dikembangkan model untuk mendiagnosis sindrom metabolik. Penelitian ini bertujuan untuk membandingkan kinerja algoritma klasifikasi utama, yaitu Naïve Bayes (NB) dan K-Nearest Neighbors (KNN) dalam mendeteksi sindrom metabolik. Hasil dari penelitian ini menunjukkan bahwa penggunaan algoritma Naïve Bayes menghasilkan akurasi sebesar 79%, sedangkan akurasi tertinggi dari algoritma K-Nearest Neighbors (KNN) adalah 82%. Kesimpulannya, dari hasil penelitian ini menunjukkan bahwa algoritma K-NN dengan pembagian data 50:50 lebih efektif dalam memprediksi dan mengklasifikasikan sindrom metabolik.",
        "link": "http://dx.doi.org/10.57152/malcom.v4i2.1249"
    },
    {
        "id": 575,
        "title": "AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding",
        "authors": "Artem Babenko Yandex, Victor Lempitsky",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.523"
    },
    {
        "id": 576,
        "title": "Anne: Adaptive Nearest Neighbors and Eigenvector-Based Sample Selection for Robust Learning with Noisy Labels",
        "authors": "Filipe Cordeiro, Gustavo Carneiro",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683589"
    },
    {
        "id": 577,
        "title": "Robustness Certification of k-Nearest Neighbors",
        "authors": "Nicolò Fassina, Francesco Ranzato, Marco Zanella",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00020"
    },
    {
        "id": 578,
        "title": "Sequential change-point detection based on nearest neighbors",
        "authors": "Hao Chen",
        "published": "2019-6-1",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1214/18-aos1718"
    },
    {
        "id": 579,
        "title": "Parallel Implementation of K-Nearest-Neighbors for Face Recognition",
        "authors": "Filip Despotovski, Marjan Gusev, Vladimir Zdraveski",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/telfor.2018.8611971"
    },
    {
        "id": 580,
        "title": "Effective Weighted k-Nearest Neighbors for Dynamic Data Streams",
        "authors": "Maroua Bahri",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020652"
    },
    {
        "id": 581,
        "title": "Lattice fluid with attractive interaction between nearest neighbors and repulsive interaction between next-next-nearest neighbors on simple cubic lattice",
        "authors": "Yaroslav G. Groda,  , Vyacheslav S. Vikhrenko, Dung di Caprio,  ,  ",
        "published": "2019-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33581/2520-2243-2019-2-84-95"
    },
    {
        "id": 582,
        "title": "Selective Nearest Neighbors Clustering",
        "authors": "Souhardya Sengupta, Swagatam Das",
        "published": "2022-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2021.10.005"
    },
    {
        "id": 583,
        "title": "Phase Diagram and the Ground State of the Decorated Ising Model on a Triangular Lattice with Ferromagnetic Interaction between the First Nearest Neighbors and Antiferromagnetic Interaction between the Next Nearest Neighbors",
        "authors": "V. A. Mutailamov, A. K. Murtazaev",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1134/s1063776122120068"
    },
    {
        "id": 584,
        "title": "6 K-Nearest Neighbors (KNN) – Theory",
        "authors": "",
        "published": "2022-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781501505737-006"
    },
    {
        "id": 585,
        "title": "7 K-Nearest Neighbors (KNN) – Practice",
        "authors": "",
        "published": "2022-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781501505737-007"
    },
    {
        "id": 586,
        "title": "Solar power prediction with data source weighted nearest neighbors",
        "authors": "Zheng Wang, Irena Koprinska",
        "published": "2017-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966018"
    },
    {
        "id": 587,
        "title": "K-Nearest Neighbors Classifier for Field Bit Error Rate Data",
        "authors": "Stephanie Allogba, Christine Tremblay",
        "published": "2018-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acp.2018.8596133"
    },
    {
        "id": 588,
        "title": "Text classification for subjective scoring using K-nearest neighbors",
        "authors": "Kittakorn Sriwanna",
        "published": "2018-2",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdamt.2018.8376511"
    },
    {
        "id": 589,
        "title": "Intelligent System to Classify Peanuts Varieties Using K-Nearest Neighbors (K-NN) and Support Vector Machine (SVM)",
        "authors": "V. G. Narendra, K. Govardhan Hegde",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-0108-1_33"
    },
    {
        "id": 590,
        "title": "5 K-Nearest Neighbors (KNN) – Concept",
        "authors": "",
        "published": "2022-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781501505737-005"
    },
    {
        "id": 591,
        "title": "A k-Nearest Neighbors approach for COCOMO calibration",
        "authors": "Phu Le, Vu Nguyen",
        "published": "2017-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nafosted.2017.8108067"
    },
    {
        "id": 592,
        "title": "Interpretable Locally Adaptive Nearest Neighbors",
        "authors": "Jan Philip Göpfert, Heiko Wersing, Barbara Hammer",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.05.105"
    },
    {
        "id": 593,
        "title": "Supervised Learning—Classification Using K‐Nearest Neighbors (KNN)",
        "authors": "",
        "published": "2019-4-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119557500.ch9"
    },
    {
        "id": 594,
        "title": "Approximate k-nearest neighbors graph for single-cell Hi-C dimensional reduction with MinHash",
        "authors": "Joachim Wolff, Rolf Backofen, Björn Grüning",
        "published": "No Date",
        "citations": 1,
        "abstract": "Single-cell Hi-C interaction matrices are high dimensional and very sparse. To cluster thousands of single-cell Hi-C interaction matrices they are flattened and compiled into one matrix. This matrix can, depending on the resolution, have a few millions or even billions of features and any computation with it is therefore memory demanding. A common approach to reduce the number of features is to compute a nearest neighbors graph. However, the exact euclidean distance computation is in O(n2) and therefore we present an implementation of an approximate nearest neighbors method based on local sensitive hashing running in O(n). The presented method is able to process a 10kb single-cell Hi-C data set with 2500 cells and needs 53 GB of memory while the exact k-nearest neighbors approach is not computable with 1 TB of memory.",
        "link": "http://dx.doi.org/10.1101/2020.03.05.978569"
    },
    {
        "id": 595,
        "title": "K-Nearest Neighbors Hashing",
        "authors": "Xiangyu He, Peisong Wang, Jian Cheng",
        "published": "2019-6",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00295"
    },
    {
        "id": 596,
        "title": "Identification of Rice Leaf Disease based on Rice Leaf Image Features using the k-Nearest Neighbour (k-NN) Technique",
        "authors": "K. Adiyarta, C. Zonyfar, T. Fatimah",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008931101600165"
    },
    {
        "id": 597,
        "title": "Uniform in number of neighbors consistency and weak convergence of $ k $NN empirical conditional processes and $ k $NN conditional $ U $-processes involving functional mixing data",
        "authors": "Salim Bouzebda, Amel Nezzal",
        "published": "2024",
        "citations": 2,
        "abstract": "<abstract><p>$ U $-statistics represent a fundamental class of statistics arising from modeling quantities of interest defined by multi-subject responses.  \t\t$ U $-statistics generalize the empirical mean of a random variable $ X $ to sums over every $ m $-tuple of distinct observations of $ X $. Stute [182] introduced a class of so-called conditional $ U $-statistics, which may be viewed as a generalization of the Nadaraya-Watson estimates of a regression function. Stute proved their strong pointwise consistency to:  \t\t$ r^{(m)}(\\varphi, \\mathbf{t}): = \\mathbb{E}[\\varphi(Y_{1}, \\ldots, Y_{m})|(X_{1}, \\ldots, X_{m}) = \\mathbf{t}], \\; \\mbox{for}\\; \\mathbf{ t}\\in \\mathcal{X}^{m}. $ In this paper, we are mainly interested in the study of the $ k $NN conditional $ U $-processes in a functional mixing data framework. More precisely, we investigate the weak convergence of the conditional empirical process indexed by a suitable class of functions and of the $ k $NN conditional $ U $-processes when the explicative variable is functional. We treat the uniform central limit theorem in both cases when the class of functions is bounded or unbounded satisfying some moment conditions. The second main contribution of this study is the establishment of a sharp almost complete Uniform consistency in the Number of Neighbors of the constructed estimator. Such a result allows the number of neighbors to vary within a complete range for which the estimator is consistent. Consequently, it represents an interesting guideline in practice to select the optimal bandwidth in nonparametric functional data analysis. These results are proved under some standard structural conditions on the Vapnik-Chervonenkis classes of functions and some mild conditions on the model. The theoretical results established in this paper are (or will be) key tools for further functional data analysis developments. Potential applications include the set indexed conditional <italic>U</italic>-statistics, Kendall rank correlation coefficient, the discrimination problems and the time series prediction from a continuous set of past values.</p></abstract>",
        "link": "http://dx.doi.org/10.3934/math.2024218"
    },
    {
        "id": 598,
        "title": "Modernizing k‐nearest neighbors",
        "authors": "Robin Elizabeth Yancey, Bochao Xin, Norm Matloff",
        "published": "2021-12",
        "citations": 1,
        "abstract": "The k‐nearest neighbors (k‐NN) method is one of the oldest statistical/machine learning techniques. It is included in virtually every major package, such as caret, parsnip, mlr3 and scikit‐learn. Yet those packages do not go beyond the basics. With today's high‐speed computation capability, k‐NN can be made much more powerful. Here, we present directions in which that can be done.",
        "link": "http://dx.doi.org/10.1002/sta4.335"
    },
    {
        "id": 599,
        "title": "Klasifikasi Penyakit Dengue Menggunakan Algoritma K-Nearest Neighbors Berbasis Flask",
        "authors": "Rozaq Rozaq",
        "published": "2022-8-1",
        "citations": 0,
        "abstract": "Penanggulangan penyakit dengue merupakan salah satu elemen penting dalam memastikan masyarakat hidup sehat dan mendapatkan penanganan kesehatan yang baik. Penanggulangan penyakit dengue yang terlambat dapat mengakibatkan kematian. Berdasarkan permasalahan tersebut, maka diperlukan suatu sistem klasifikasi yang dapat mendiagnosis penyakit demam berdarah berdasarkan pemeriksaan kesehatan  pasien. Tujuan dari penelitian ini adalah untuk menerapkan algoritma k-nearest neighbor pada pembangunan sistem klasifikasi dengue. Algoritma K-Nearest Neighbors bekerja dengan menetapkan kelas dari suatu objek berdasarkan kesamaan atribut dari data yang sebelumnya dijadikan model. Data model yang dibangun menggunakan data kasus dengue yang terjadi di kota madiun dengan total data sebanyak 276 baris dengan kolom data sebanyak 7 kolom. Data model tersebut nantinya akan di pickle menggunakan library yang ada pada python. Setelah di pickle kemudian data di split menjadi data training sebanyak 70% dan data testing sebanyak 30% yang nantinya akan digunakan sebagai bahan untuk melakukan prediksi data. Metode pengembangan sistem untuk penelitian ini menggunakan metode extreme programming yang terdiri dari empat tahap yaitu: (1) perencanaan, (2) perancangan, (3) pengkodean, dan (4) pengujian. Pengembangan sistem dilakukan pada aplikasi berbasis web dengan menggunakan framework Flask menggunakan bahasa pemrograman Python, dengan akurasi rata-rata 0,72, akurasi  rata-rata  0,83, nilai recall rata-rata  0,59, dan nilai rata-rata f-1 0,64. Itu telah dibuat atau sebesar 64%.",
        "link": "http://dx.doi.org/10.33395/remik.v6i3.11501"
    },
    {
        "id": 600,
        "title": "Categorizing document by fuzzy C-Means and K-nearest neighbors approach",
        "authors": "Novita Priandini, Badrus Zaman, Endah Purwanti",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/1.4994415"
    }
]