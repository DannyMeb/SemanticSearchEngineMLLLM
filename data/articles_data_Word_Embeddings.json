[
    {
        "id": 1701,
        "title": "Word Embeddings are Word Story Embeddings (and That's Fine)",
        "authors": "Katrin Erk, Gabriella Chronis",
        "published": "2022-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003205388-9"
    },
    {
        "id": 1702,
        "title": "Understanding and Creating Word Embeddings",
        "authors": "Avery Blankenship, Sarah Connell, Quinn Dombrowski",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "\n            Word embeddings allow you to analyze the usage of different terms in a corpus of texts by capturing information about their contextual usage. Through a primarily theoretical lens, this lesson will teach you how to prepare a corpus and train a word embedding model. You will explore how word vectors work, how to interpret them, and how to answer humanities research questions using them.\n          ",
        "link": "http://dx.doi.org/10.46430/phen0116"
    },
    {
        "id": 1703,
        "title": "Clustering and Visualising Documents using Word Embeddings",
        "authors": "Jonathan Reades, Jennie Williams",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "\n            This lesson uses word embeddings and clustering algorithms in Python to identify groups of similar documents in a corpus of approximately 9,000 academic abstracts. It will teach you the basics of dimensionality reduction for extracting structure from a large corpus and how to evaluate your results.\n          ",
        "link": "http://dx.doi.org/10.46430/phen0111"
    },
    {
        "id": 1704,
        "title": "Adjusting Word Embeddings by Deep Neural Networks",
        "authors": "Xiaoyang Gao, Ryutaro Ichise",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006120003980406"
    },
    {
        "id": 1705,
        "title": "Des Graphes aux Word Embeddings: comment mathématiques et visualisations s’immiscent dans les études littéraires",
        "authors": "Anne Baillot, David Lassner",
        "published": "2022-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/germanica.19002"
    },
    {
        "id": 1706,
        "title": "Multi-Sense Embeddings per Word",
        "authors": "Masashi Sugiyama",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recently, word embeddings have been used in many natural language processing problems successfully and how to train a robust and accurate word embedding system efficiently is a popular research area. Since many, if not all, words have more than one sense, it is necessary to learn vectors for all senses of word separately. Therefore, in this project, we have explored two multi-sense word embedding models, including Multi-Sense Skip-gram (MSSG) model and Non-parametric Multi-sense Skip Gram model (NP-MSSG). Furthermore, we propose an extension of the Multi-Sense Skip-gram model called Incremental Multi-Sense Skip-gram (IMSSG) model which could learn the vectors of all senses per word incrementally. We evaluate all the systems on word similarity task and show that IMSSG is better than the other models.",
        "link": "http://dx.doi.org/10.31219/osf.io/udfhn"
    },
    {
        "id": 1707,
        "title": "Disambiguating Clinical Abbreviations using Pre-trained Word Embeddings",
        "authors": "Areej Jaber, Paloma Martínez",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010256105010508"
    },
    {
        "id": 1708,
        "title": "Unsupervised Learning of Cross-Lingual Word Embeddings",
        "authors": "Anders Søgaard, Ivan Vulić, Sebastian Ruder, Manaal Faruqui",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-02171-8_9"
    },
    {
        "id": 1709,
        "title": "NMT Multi-Sense Embeddings per Word",
        "authors": "William Jin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Recently, word embeddings have been used in many natural language processing problems successfully and how to train a robust and accurate word embedding system efficiently is a popular research area. Since many, if not all, words have more than one sense, it is necessary to learn vectors for all senses of word separately. Therefore, in this project, we have explored two multi-sense word embedding models, including Multi-Sense Skip-gram (MSSG) model and Non-parametric Multi-sense Skip Gram model (NP-MSSG). Furthermore, we propose an extension of the Multi-Sense Skip-gram model called Incremental Multi-Sense Skip-gram (IMSSG) model which could learn the vectors of all senses per word incrementally. We evaluate all the systems on word similarity task and show that IMSSG is better than the other models.",
        "link": "http://dx.doi.org/10.31219/osf.io/k623t"
    },
    {
        "id": 1710,
        "title": "Comparing Dependency-based Compositional Models with Contextualized Word Embeddings",
        "authors": "Pablo Gamallo, Manuel Corral, Marcos Garcia",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010391812581265"
    },
    {
        "id": 1711,
        "title": "Constraining Weighted Word Co-occurrence Frequencies in Word Embeddings",
        "authors": "Paula Lauren",
        "published": "2021-12-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata52589.2021.9671892"
    },
    {
        "id": 1712,
        "title": "LSTM Easy-first Dependency Parsing with Pre-trained Word Embeddings and Character-level Word Embeddings in Vietnamese",
        "authors": "Binh Duc Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kse.2018.8573397"
    },
    {
        "id": 1713,
        "title": "Cultural Cartography with Word Embeddings",
        "authors": "Dustin S. Stoltz, Marshall A. Taylor",
        "published": "No Date",
        "citations": 0,
        "abstract": "Using the frequency of keywords is a classic approach in the formal analysis of text, but has the drawback of glossing over the relationality of word meanings. Word embedding models overcome this problem by constructing a standardized and continuous “meaning-space” where words are assigned a location based on relations of similarity to other words based on how they are used in natural language samples. We show how word embeddings are commensurate with prevailing theories of meaning in sociology and can be put to the task of interpretation via two kinds of navigation. First, one can hold terms constant and measure how the embedding space moves around them—much like astronomers measured the changing of celestial bodies with the seasons. Second, one can also hold the embedding space constant and see how documents or authors move relative to it—just as ships use the stars on a given night to determine their location. Using the empirical case of immigration discourse in the United States, we demonstrate the merits of these two broad strategies for advancing important topics in cultural theory, including social marking, media fields, echo chambers, and cultural diffusion and change more broadly.",
        "link": "http://dx.doi.org/10.31235/osf.io/5djcn"
    },
    {
        "id": 1714,
        "title": "Sociolinguistic Properties of Word Embeddings",
        "authors": "Alina Arseniev-Koehler, Jacob G. Foster",
        "published": "No Date",
        "citations": 1,
        "abstract": "This is a preprint for a book chapter published in Dehghani, Morteza, and Ryan L. Boyd, eds. Handbook of Language Analysis in Psychology. Guilford Publications, 2022. In this chapter, we describe the burgeoning empirical evidence showing how meanings captured in word embeddings correspond to human meanings. We then review the theoretical evidence illustrating how word embeddings correspond to — and diverge from — human cognitive strategies to represent and process meaning. In turn, these divergences illustrate challenges and future directions for research with word embeddings.",
        "link": "http://dx.doi.org/10.31235/osf.io/b8kud"
    },
    {
        "id": 1715,
        "title": "Learning Meta Word Embeddings by Unsupervised Weighted Concatenation of Source Embeddings",
        "authors": "Danushka Bollegala",
        "published": "2022-7",
        "citations": 0,
        "abstract": "Given multiple source word embeddings learnt using diverse algorithms and lexical resources, meta word embedding learning methods attempt to learn more accurate and wide-coverage word embeddings. \n\n\tPrior work on meta-embedding has repeatedly discovered that simple vector concatenation of the source embeddings to be a competitive baseline. \n\n\tHowever, it remains unclear as to why and when simple vector concatenation can produce accurate meta-embeddings. \n\n\tWe show that weighted concatenation can be seen as a spectrum matching operation between each source embedding and the meta-embedding, minimising the pairwise inner-product loss.\n\n\tFollowing this theoretical analysis, we propose two \\emph{unsupervised} methods to learn the optimal concatenation weights for creating meta-embeddings from a given set of source embeddings.\n\n\tExperimental results on multiple benchmark datasets show that the proposed weighted concatenated meta-embedding methods outperform previously proposed meta-embedding learning methods.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/563"
    },
    {
        "id": 1716,
        "title": "Word and Image Embeddings in Pill Recognition",
        "authors": "Richárd Rádli, Zsolt Vörösházi, László Czúni",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012460800003660"
    },
    {
        "id": 1717,
        "title": "When Word Embeddings Become Endangered",
        "authors": "Khalid Alnajjar",
        "published": "2021-3-23",
        "citations": 1,
        "abstract": "Big languages such as English and Finnish have many natural language processing (NLP) resources and models, but this is not the case for low-resourced and endangered languages as such resources are so scarce despite the great advantages they would provide for the language communities. The most common types of resources available for low-resourced and endangered languages are translation dictionaries and universal dependencies. In this paper, we present a method for constructing word embeddings for endangered languages using existing word embeddings of different resource-rich languages and the translation dictionaries of resource-poor languages. Thereafter, the embeddings are fine-tuned using the sentences in the universal dependencies and aligned to match the semantic spaces of the big languages; resulting in cross-lingual embeddings. The endangered languages we work with here are Erzya, Moksha, Komi-Zyrian and Skolt Sami. Furthermore, we build a universal sentiment analysis model for all the languages that are part of this study, whether endangered or not, by utilizing cross-lingual word embeddings. The evaluation conducted shows that our word embeddings for endangered languages are well-aligned with the resource-rich languages, and they are suitable for training task-specific models as demonstrated by our sentiment analysis models which achieved high accuracies. All our cross-lingual word embeddings and sentiment analysis models will be released openly via an easy-to-use Python library.",
        "link": "http://dx.doi.org/10.31885/9789515150257.24"
    },
    {
        "id": 1718,
        "title": "Unsupervised Word Sense Disambiguation Using Word Embeddings",
        "authors": "Behzad Moradi, Ebrahim Ansari, Zdenek Zabokrtsky",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/fruct48121.2019.8981526"
    },
    {
        "id": 1719,
        "title": "Competing Views of Word Meaning: Word Embeddings and Word Senses",
        "authors": "Gregory Grefenstette, Patrick Hanks",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "Abstract\nAt least since the invention of writing, people have been troubled by the problem of what a word means. Dictionaries have traditionally been written with numbered word senses, giving the impression that the different senses of a word are fixed abstract entities, which can be used to separate usages into neat piles according to their different meanings. Adam Kilgarriff’s daring 1997 article ‘I Don’t Believe in Word Senses’ challenged this traditional view of word meaning, presenting an account in which ‘the basic units are occurrences of the word in context’. Kilgarriff went on to develop the Sketch Engine, a statistical tool that enables lexicographers and NLP researchers, teachers, and students to explore the relationship between meanings and collocations (words and their contexts). In this review article, we compare the information provided by Kilgarriff’s Word Sketches with the recently developed Word Embedding techniques and with the results of Corpus Pattern Analysis.",
        "link": "http://dx.doi.org/10.1093/ijl/ecad005"
    },
    {
        "id": 1720,
        "title": "Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings",
        "authors": "Shintaro Ando, Nobuaki Minematsu, Daisuke Saito",
        "published": "2021-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-853"
    },
    {
        "id": 1721,
        "title": "A Hierarchical Book Representation of Word Embeddings for Effective Semantic Clustering and Search",
        "authors": "Avi Bleiweiss",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006192701540163"
    },
    {
        "id": 1722,
        "title": "An Introduction to Word Embeddings and Language Models",
        "authors": "Tammie Borders, Svitlana Volkova",
        "published": "2021-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1773690"
    },
    {
        "id": 1723,
        "title": "Context-aware Sentiment Analysis on Refined Word Embeddings Word2Vec Model",
        "authors": "Dr. Archana Sharma",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>This paper proposes a model for the refining of word vectors that does not require a labelled corpus and may be applied to any pre-trained word vectors. The suggested technique employs a sentiment intensity lexicon that may offer realvalued sentiment data to rank a collection of semantically and emotionally related nearest neighbors for each word. The ranking nearest neighbors is then utilized to control the direction and distance of the refinement technique, which iteratively enhances the word vector representation of each word. Experiments using SST demonstrate that the suggested strategy outperformed conventional word embeddings and sentiment embeddings for both coarse-grained and binary sentiment categorization. In addition, the performance of several models of deep neural networks has been enhanced. In future study, the suggested approach will be evaluated on more datasets. Additionally, additional experiments will be undertaken to offer a more comprehensive study. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20278077.v1"
    },
    {
        "id": 1724,
        "title": "Overcoming Poor Word Embeddings with Word Definitions",
        "authors": "Christopher Malon",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.starsem-1.27"
    },
    {
        "id": 1725,
        "title": "Semantic Properties of Cosine Based Bias Scores for Word Embeddings",
        "authors": "Sarah Schröder, Alexander Schulz, Fabian Hinder, Barbara Hammer",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012577200003654"
    },
    {
        "id": 1726,
        "title": "Context-aware Sentiment Analysis on Refined Word Embeddings Word2Vec Model",
        "authors": "Dr. Archana Sharma",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>This paper proposes a model for the refining of word vectors that does not require a labelled corpus and may be applied to any pre-trained word vectors. The suggested technique employs a sentiment intensity lexicon that may offer realvalued sentiment data to rank a collection of semantically and emotionally related nearest neighbors for each word. The ranking nearest neighbors is then utilized to control the direction and distance of the refinement technique, which iteratively enhances the word vector representation of each word. Experiments using SST demonstrate that the suggested strategy outperformed conventional word embeddings and sentiment embeddings for both coarse-grained and binary sentiment categorization. In addition, the performance of several models of deep neural networks has been enhanced. In future study, the suggested approach will be evaluated on more datasets. Additionally, additional experiments will be undertaken to offer a more comprehensive study. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20278077"
    },
    {
        "id": 1727,
        "title": "Dependency Based Bilingual word Embeddings without word alignment",
        "authors": "Taghreed Alqaisi, Alexandros Komninos, Simon O'Keefe",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206732"
    },
    {
        "id": 1728,
        "title": "Whole-Word Segmental Speech Recognition with Acoustic Word Embeddings",
        "authors": "Bowen Shi, Shane Settle, Karen Livescu",
        "published": "2021-1-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/slt48900.2021.9383578"
    },
    {
        "id": 1729,
        "title": "Biomedical Semantic Embeddings: Using hybrid sentences to construct biomedical word embeddings and its applications",
        "authors": "Arshad Shaik, Wei Jin",
        "published": "2019-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi.2019.8904533"
    },
    {
        "id": 1730,
        "title": "Bringing Order to Neural Word Embeddings with Embeddings Augmented by Random Permutations (",
        "authors": "Trevor Cohen, Dominic Widdows",
        "published": "2018",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/k18-1045"
    },
    {
        "id": 1731,
        "title": "En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects",
        "authors": "Taghreed Alqaisi, Simon O’Keefe",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-4611"
    },
    {
        "id": 1732,
        "title": "Reconstruction of Word Embeddings from Sub-Word Parameters",
        "authors": "Karl Stratos",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w17-4119"
    },
    {
        "id": 1733,
        "title": "Improved Learning of Word Embeddings with Word Definitions and Semantic Injection",
        "authors": "Yichi Zhang, Yinpei Dai, Zhijian Ou, Huixin Wang, Junlan Feng",
        "published": "2020-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-1702"
    },
    {
        "id": 1734,
        "title": "Computational scaling of political positions from textual data using word embeddings",
        "authors": "Patrick Schwabl",
        "published": "No Date",
        "citations": 0,
        "abstract": "Political positions can be scaled from textual data using word embeddings. This extended abstract proposes a method for automatically scaling political positions using word embeddings from political speeches. The method is based on the the idea of using association-based scores with two dictionaries, like in the word embedding association test (WEAT). I conducted computational experiments to show that the method works in principle and give ideas on how I want to improve the method in the future.",
        "link": "http://dx.doi.org/10.31235/osf.io/rb4sp"
    },
    {
        "id": 1735,
        "title": "l1 Regularization of Word Embeddings for Multi-Word Expression Identification",
        "authors": "Gábor Berend",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14232/actacyb.23.3.2018.5"
    },
    {
        "id": 1736,
        "title": "How to Use Word Embeddings for Natural Language Processing",
        "authors": "Barbara McGillivray",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781529609578"
    },
    {
        "id": 1737,
        "title": "Predicting Protein Binding Affinity With Word Embeddings and Recurrent Neural Networks",
        "authors": "Carlo Mazzaferro",
        "published": "No Date",
        "citations": 8,
        "abstract": "AbstractAt the core of our immunological system lies a group of proteins named Major Histocompatibility Complex (MHC), to which epitopes (also proteins sometimes named antigenic determinants), bind to eliciting a response. These responses are extremely varied and of widely different nature. For instance, Killer and Helper T cells are responsible for, respectively, counteracting viral pathogens and tumorous cells. Many other types exist, but their underlying structure can be very similar due to the fact that they all are proteins and bind to the MHC receptor in a similar fashion. With this framework in mind, being able to predict with precision the structure of a protein that will elicit a specific response in the human body represents a novel computational approach to drug discovery. Although many machine learning approaches have been used, no attempt to solve this problem using Recurrent Neural Networks (RNNs) exist. We extend the current efforts in the field by applying a variety of network architectures based on RNNs and word embeddings (WE). The code is freely available and under current development at https://github.com/carlomazzaferro/mhcPreds",
        "link": "http://dx.doi.org/10.1101/128223"
    },
    {
        "id": 1738,
        "title": "Dataset to accompany Clustering and Visualising Documents using Word Embeddings",
        "authors": "Jon Reades, Jennie Williams",
        "published": "2023-7-6",
        "citations": 1,
        "abstract": "Dataset to accompany Clustering and Visualising Documents using Word Embeddings, a lesson for the Programming Historian.",
        "link": "http://dx.doi.org/10.46430/phen0112"
    },
    {
        "id": 1739,
        "title": "Mudança semântica e word embeddings: estudos de caso na diacronia do português/ Semantic change and word embeddings: case studies on the diachrony of Portuguese",
        "authors": "Lucas Lage, Evandro Cunha",
        "published": "2022-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17851/2237-2083.30.4.2043-2086"
    },
    {
        "id": 1740,
        "title": "subs2vec: Word embeddings from subtitles in 55 languages",
        "authors": "Jeroen van Paridon, Bill Thompson",
        "published": "No Date",
        "citations": 2,
        "abstract": "This paper introduces a collection of vector-embeddings models of lexical semantics in 55 languages, trained on a large corpus of pseudo-conversational speech transcriptions from television shows and movies. The models were trained on the OpenSubtitles corpus using the fastText implementation of the skipgram algorithm. Performance comparable with (and in some cases exceeding) models trained on non-conversational (Wikipedia) text is reported on standard benchmark evaluation datasets. A novel evaluation method of particular relevance to psycholinguists is also introduced: prediction of experimental lexical norms in multiple languages. The models, as well as code for reproducing the models and all analyses reported in this paper (implemented as a user-friendly Python package), are freely available at: https://github.com/jvparidon/subs2vec/",
        "link": "http://dx.doi.org/10.31234/osf.io/fcrmy"
    },
    {
        "id": 1741,
        "title": "Using Word Embeddings in Turkish Part of Speech Tagging",
        "authors": "Şevket Can,  ",
        "published": "2021-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2021.11.5.1063"
    },
    {
        "id": 1742,
        "title": "Understanding Legal Meaning through Word Embeddings",
        "authors": "Douglas Rice",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3455747"
    },
    {
        "id": 1743,
        "title": "Word embeddings reveal an expanding moral circle",
        "authors": "Stefan Leach, Andrew Kitchin, Robbie M. Sutton",
        "published": "No Date",
        "citations": 0,
        "abstract": "The Enlightenment idea of historical moral progress asserts that civil societies become more moral over time. This progress is often depicted as an expanding moral circle, and is thought to be driven by language which fosters greater concern for the welfare of others. Our research supports this notion by examining historical trends in natural language use during the 19th and 20th centuries. We found that the associations between words denoting moral concern and words referring to people, animals, and the environment grew stronger over time. This suggests that moral progress is associated with changes in language that are consistent with an expanding moral circle.",
        "link": "http://dx.doi.org/10.31234/osf.io/9rpxj"
    },
    {
        "id": 1744,
        "title": "Word Embeddings for Morphologically Complex Languages",
        "authors": "Grzegorz Jurdzinski",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4467/20838476si.16.010.6191"
    },
    {
        "id": 1745,
        "title": "Word embeddings reveal how fundamental sentiments structure natural language",
        "authors": "Austin van Loon, Jeremy Freese",
        "published": "No Date",
        "citations": 2,
        "abstract": "Central to affect control theory are culturally shared meanings of concepts. That these sentiments overlap among members of a culture presumably reflects their roots in the language use that members observe.  Yet the degree to which the affective meaning of a concept is encoded in the way linguistic representations of that concept are used in everyday symbolic exchange has yet to be demonstrated.  The question has methodological as well as theoretical significance for affect control theory, as language may provide an unobtrusive, behavioral method of obtaining EPA ratings complementary to those heretofore obtained via questionnaires.  We pursue a series of studies that evaluate whether tools from machine learning and computational linguistics can capture the fundamental affective meaning of concepts from large text corpora. We develop an algorithm that uses word embeddings to predict EPA profiles available from a recent EPA dictionary derived from traditional questionnaires, as well as novel concepts collected using an open-source web app we have developed. Across both a held-out portion of the available data as well as the novel data, our best predictions correlate with survey-based measures of the E, P, and A ratings of concepts at a magnitude greater than 0.85, 0.8, and 0.75 respectively.",
        "link": "http://dx.doi.org/10.31235/osf.io/r7ewx"
    },
    {
        "id": 1746,
        "title": "Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction",
        "authors": "Silvia Severini, Viktor Hangya, Alexander Fraser, Hinrich Schütze",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.531"
    },
    {
        "id": 1747,
        "title": "A method for constructing word sense embeddings based on word sense induction",
        "authors": "Yujia Sun, Jan Platoš",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "AbstractPolysemy is an inherent characteristic of natural language. In order to make it easier to distinguish between different senses of polysemous words, we propose a method for encoding multiple different senses of polysemous words using a single vector. The method first uses a two-layer bidirectional long short-term memory neural network and a self-attention mechanism to extract the contextual information of polysemous words. Then, a K-means algorithm, which is improved by optimizing the density peaks clustering algorithm based on cosine similarity, is applied to perform word sense induction on the contextual information of polysemous words. Finally, the method constructs the corresponding word sense embedded representations of the polysemous words. The results of the experiments demonstrate that the proposed method produces better word sense induction than Euclidean distance, Pearson correlation, and KL-divergence and more accurate word sense embeddings than mean shift, DBSCAN, spectral clustering, and agglomerative clustering.",
        "link": "http://dx.doi.org/10.1038/s41598-023-40062-3"
    },
    {
        "id": 1748,
        "title": "New Word Pair Level Embeddings to Improve Word Pair Similarity",
        "authors": "Nazar Khan, Asma Shaukat",
        "published": "2017-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdar.2017.329"
    },
    {
        "id": 1749,
        "title": "Comparative Analysis of Word Embeddings for Capturing Word Similarities",
        "authors": "Martina Toshevska, Frosina Stojanovska, Jovan Kalajdjieski",
        "published": "2020-4-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/csit.2020.100402"
    },
    {
        "id": 1750,
        "title": "Improving accuracy of an existing semantic word labelling tool using word embeddings",
        "authors": "Hugo Sanjurjo-Gonzalez",
        "published": "2021-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/cisti52073.2021.9476323"
    },
    {
        "id": 1751,
        "title": "Peer Review #2 of \"Comparing general and specialized word embeddings for biomedical named entity recognition (v0.1)\"",
        "authors": "",
        "published": "2021-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.384v0.1/reviews/2"
    },
    {
        "id": 1752,
        "title": "Simplify: Automatic Arabic Sentence Simplification using Word Embeddings",
        "authors": "Yousef SalahEldin, Caroline Sabty",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.arabicnlp-1.35"
    },
    {
        "id": 1753,
        "title": "Lexical and Morpho-syntactic Features in Word Embeddings - A Case Study of Nouns in Swedish",
        "authors": "Ali Basirat, Marc Tang",
        "published": "2018",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006729606630674"
    },
    {
        "id": 1754,
        "title": "Peer Review #1 of \"Comparing general and specialized word embeddings for biomedical named entity recognition (v0.1)\"",
        "authors": "",
        "published": "2021-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.384v0.1/reviews/1"
    },
    {
        "id": 1755,
        "title": "Biomedical Knowledge Discovery From Unstructured Text Corpora Using Contextual Word Embeddings",
        "authors": "Sandip S. Panesar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground: Unsupervised extraction of knowledge from large, unstructured text corpora presents a challenge. Mathematical word embeddings taken from static language models such as Word2Vec have been utilized to discover \"latent knowledge\" within such domain-specific corpora. Here, semantic-similarity measures between representations of concepts or entities were used to predict relationships, which were later verified using domain-specific scientific techniques. Static language models have recently been surpassed at most downstream tasks by pre-trained, contextual language models like BERT. Some have postulated that contextualized embeddings potentially yield word representations superior to static ones for knowledge-discovery purposes. To address this question, two biomedically-trained BERT models (BioBERT and SciBERT) were used to encode n = 500, 1000 or 5000 sentences containing words of interest extracted from a biomedical corpus. The n representations for the words of interest were subsequently extracted and then aggregated to yield static-equivalent word representations for words belonging to vocabularies of biomedical intrinsic benchmarking tools for verbs and nouns. Using intrinsic benchmarking tasks, feasibility of using contextualized word representations for knowledge discovery tasks can be assessed: Word representations better encoding described reality are expected to demonstrate superior performance. \nResults:  The number of contextual examples used for aggregation had little effect on performance, however embeddings aggregated from shorter sequences outperformed those from longer ones. Performance also varied according to model used, with BioBERT demonstrating superior performance to static models for verbs, and SciBERT embeddings demonstrating superior performance to static embeddings for nouns. Neither model outperformed static models for both nouns and verbs. Moreover, performance varied according to model layer from which embeddings were extracted from, and depending upon whether a word was intrinsic to a particular model's vocabulary or required subword decomposition.\nConclusions: Based on these results, static-equivalent embeddings obtained from contextual models may be superior to those from static models. Moreover, as n has little effect on embedding performance, a computationally efficient method of sampling a corpus for contextual examples and leveraging BERT's architecture to obtain word embeddings suitable for knowledge discovery tasks is described.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2286334/v1"
    },
    {
        "id": 1756,
        "title": "Word Embeddings",
        "authors": "Emil Hvitfeldt, Julia Silge",
        "published": "2021-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003093459-6"
    },
    {
        "id": 1757,
        "title": "Enhancing Backchannel Prediction Using Word Embeddings",
        "authors": "Robin Ruede, Markus Müller, Sebastian Stüker, Alex Waibel",
        "published": "2017-8-20",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-1606"
    },
    {
        "id": 1758,
        "title": "Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings",
        "authors": "Myunghun Jung, Hoi Rin Kim",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10013"
    },
    {
        "id": 1759,
        "title": "Word Equations: Inherently Interpretable Sparse Word Embeddings through Sparse Coding",
        "authors": "Adly Templeton",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.blackboxnlp-1.12"
    },
    {
        "id": 1760,
        "title": "Leveraging Context for Perceptual Prediction using Word Embeddings",
        "authors": "Georgia-Ann Carter, Frank Keller, Paul Hoffman",
        "published": "No Date",
        "citations": 0,
        "abstract": "Pre-trained word embeddings have been used successfully in semantic NLP tasks to represent words. However, there is continued debate as to how well they encode useful information about the perceptual qualities of concepts. Previous research has shown mixed performance when embeddings are used to predict perceptual qualities. Here, we tested if we could improve performance by leveraging the ability of Transformer-based language models to represent word meaning in context. To this end, we conducted two experiments. Our first experiment focused on noun representations. We generated decontextualised (‘charcoal’) and contextualised (‘the brightness of charcoal’) Word2Vec and BERT embeddings for a large set of concepts and compared their ability to predict human ratings of the concepts’ brightness. We repeated this procedure to also probe for the shape of those concepts. In general, we found very good prediction performance for shape, and modest performance for brightness. The addition of context did not improve perceptual prediction performance. In Experiment 2, we investigated representations of adjective-noun pairs. Perceptual prediction performance was generally found to be good, with the non-additive nature of adjective brightness reflected in the word embeddings. We also found that the addition of context had a limited impact on how well perceptual features could be predicted. We frame these results against current work on the interpretability of language models and accounts of embodied cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/34tke"
    },
    {
        "id": 1761,
        "title": "Social biases in word embeddings and their relation to human cognition",
        "authors": "Aylin Caliskan, Molly Lewis",
        "published": "No Date",
        "citations": 10,
        "abstract": "We review recent methods for measuring social bias in word embeddings, and present evidence for their relationship to human cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/d84kg"
    },
    {
        "id": 1762,
        "title": "OPTIC: A Deep Neural Network Approach for Entity Linking using Word and Knowledge Embeddings",
        "authors": "Italo Oliveira, Diego Moussallem, Luís Garcia, Renato Fileto",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009351203150326"
    },
    {
        "id": 1763,
        "title": "Multilingual Jointly Trained Acoustic and Written Word Embeddings",
        "authors": "Yushi Hu, Shane Settle, Karen Livescu",
        "published": "2020-10-25",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2828"
    },
    {
        "id": 1764,
        "title": "Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction",
        "authors": "Adhiraj Banerjee, Vipul Arora",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-483"
    },
    {
        "id": 1765,
        "title": "Word associations and the distance properties of context-aware word embeddings",
        "authors": "Maria A. Rodriguez, Paola Merlo",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.conll-1.30"
    },
    {
        "id": 1766,
        "title": "A Comprehensive Analysis of Static Word Embeddings for Turkish",
        "authors": "Karahan Sarıtaş, Cahid  Arda Oz, Tunga Güngör",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4615519"
    },
    {
        "id": 1767,
        "title": "Word sense disambiguation: an evaluation study of semi-supervised approaches with word embeddings",
        "authors": "Samuel Sousa, Evangelos Milios, Lilian Berton",
        "published": "2020-7",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207225"
    },
    {
        "id": 1768,
        "title": "WEWD: A Combined Approach for Measuring Cross-lingual Semantic Word Similarity Based on Word Embeddings and Word Definitions",
        "authors": "Van-Tan Bui, Phuong-Thai Nguyen",
        "published": "2021-8-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rivf51545.2021.9642084"
    },
    {
        "id": 1769,
        "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
        "authors": "Zixiaofan Yang, Julia Hirschberg",
        "published": "2019-9-15",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-3119"
    },
    {
        "id": 1770,
        "title": "Peer Firm Identification Using Word Embeddings",
        "authors": "Taeyoung Kee",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9006438"
    },
    {
        "id": 1771,
        "title": "Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings",
        "authors": "Ewald van der Westhuizen, Thomas Niesler",
        "published": "2017-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-1437"
    },
    {
        "id": 1772,
        "title": "Generating Custom Word Embeddings for Geoscientific Corpi",
        "authors": "C.E. Birnie, M. Ravasi",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202032059"
    },
    {
        "id": 1773,
        "title": "Tamil News Clustering Using Word Embeddings",
        "authors": "M.S. Faathima Fayaza, Surangika Ranathunga",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mercon50084.2020.9185282"
    },
    {
        "id": 1774,
        "title": "Using Word Embeddings and Collocations for Modelling Word Associations",
        "authors": "Micha de Rijk, David Mareček",
        "published": "2020-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14712/00326585.002"
    },
    {
        "id": 1775,
        "title": "Evaluating Word String Embeddings and Loss Functions for CNN-Based Word Spotting",
        "authors": "Sebastian Sudholt, Gernot A. Fink",
        "published": "2017-11",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdar.2017.87"
    },
    {
        "id": 1776,
        "title": "Temporal Word Analogies: Identifying Lexical Replacement with\n            Diachronic Word Embeddings",
        "authors": "Terrence Szymanski",
        "published": "2017",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p17-2071"
    },
    {
        "id": 1777,
        "title": "Large Scale Intent Detection in Turkish Short Sentences with Contextual Word Embeddings",
        "authors": "Enes Dündar, Osman Kılıç, Tolga Çekiç, Yusufcan Manav, Onur Deniz",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010108301810186"
    },
    {
        "id": 1778,
        "title": "Prepositional Polysemy through the lens of contextualized word embeddings",
        "authors": "Lauren Fonteyn",
        "published": "2021-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4000/cognitextes.2014"
    },
    {
        "id": 1779,
        "title": "Word Embeddings",
        "authors": "Jan Žižka, František Dařena, Arnošt Svoboda",
        "published": "2019-10-31",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429469275-13"
    },
    {
        "id": 1780,
        "title": "Understanding and Improving Word Embeddings through a Neuroscientific Lens",
        "authors": "Sam Fereidooni, Viola Mocz, Dragomir Radev, Marvin Chun",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractDespite the success of models making use of word embeddings on many natural language tasks, these models often perform significantly worse than humans on several natural language understanding tasks. This difference in performance motivates us to ask: (1) if existing word vector representations have any basis in the brain’s representational structure for individual words, and (2) whether features from the brain can be used to improve word embedding model performance, defined as their correlation with human semantic judgements. To answer the first question, we compare the representational spaces of existing word embedding models with that of brain imaging data through representational similarity analysis. We answer the second question by using regression-based learning to constrain word vectors to the features of the brain imaging data, thereby determining if these modified word vectors exhibit increased performance over their unmodified counterparts. To collect semantic judgements as a measure of performance, we employed a novel multi-arrangement method. Our results show that there is variance in the representational space of the brain imaging data that remains uncaptured by word embedding models, and that brain imaging data can be used to increase their coherence with human performance.",
        "link": "http://dx.doi.org/10.1101/2020.09.18.304436"
    },
    {
        "id": 1781,
        "title": "SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings",
        "authors": "Jan Engler, Sandipan Sikdar, Marlene Lutz, Markus Strohmaier",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-emnlp.338"
    },
    {
        "id": 1782,
        "title": "Theoretical foundations and limits of word embeddings: What types of meaning can they capture?",
        "authors": "Alina Arseniev-Koehler",
        "published": "No Date",
        "citations": 5,
        "abstract": "This preprint is an earlier version of the published article: Arseniev-Koehler, Alina. \"Theoretical foundations and limits of word embeddings: what types of meaning can they capture?.\" Sociological Methods &amp; Research (2021): 00491241221140142.",
        "link": "http://dx.doi.org/10.31235/osf.io/vrwk3"
    },
    {
        "id": 1783,
        "title": "Word Embeddings-based Sentence-Level Sentiment Analysis considering Word Importance",
        "authors": "Toshitaka Hayashi, Hamido Fujita",
        "published": "2019",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12700/aph.16.7.2019.7.1"
    },
    {
        "id": 1784,
        "title": "WOVe:  Incorporating Word Order in GloVe Word Embeddings",
        "authors": "Mohammed Salah Ibrahim, Susan Gauch, Tyler Gerth, Brandon Cox",
        "published": "2022-6-17",
        "citations": 1,
        "abstract": "Word vector representations open up new opportunities to extract useful information from unstructured text. Defining a word as a vector made it easy for the machine learning algorithms to understand a text and extract information from. Word vector representations have been used in many applications such word synonyms, word analogy, syntactic parsing, and many others. GloVe, based on word contexts and matrix vectorization, is an effective vector-learning algorithm. It improves on previous vector-learning algorithms. However, the GloVe model fails to explicitly consider the order in which words appear within their contexts. In this paper, multiple methods of incorporating word order in GloVe word embeddings are proposed. Experimental results show that our Word Order Vector (WOVe) word embeddings approach outperforms unmodified GloVe on the natural language tasks of analogy completion and word similarity. WOVe with direct concatenation slightly outperformed GloVe on the word similarity task, increasing average rank by 2%.  However, it greatly improved on the GloVe baseline on a word analogy task, achieving an average 36.34% improvement in accuracy.",
        "link": "http://dx.doi.org/10.46328/ijonest.83"
    },
    {
        "id": 1785,
        "title": "Joining LDA and Word Embeddings for Covid-19 Topic Modeling on English and Arabic Data",
        "authors": "Amina Amara, Mohamed Ali Hadj Taieb, Mohamed Ben Aouicha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012320900003636"
    },
    {
        "id": 1786,
        "title": "Exploring Earth Science Applications using Word Embeddings",
        "authors": "Derek Koehl, Carson Davis, Rahul Ramachandran, Udaysankar Nair, Manil Maskey",
        "published": "No Date",
        "citations": 0,
        "abstract": "\n        &lt;p&gt;Word embedding are numeric representations of text which capture meanings and semantic relationships in text. Embeddings can be constructed using different methods such as One Hot encoding, Frequency-based or Prediction-based approaches. Prediction-based approaches such as&amp;#160; Word2Vec, can be used to generate word embeddings that can capture the underlying semantics and word relationships in a corpus. Word2Vec embeddings generated from domain specific corpus have been shown in studies to both predict relationships and augment word vectors to improve classifications. We describe results from two different experiments utilizing word embeddings for Earth science constructed from a corpus of over 20,000 journal papers using Word2Vec.&amp;#160;&lt;/p&gt;&lt;p&gt;The first experiment explores the analogy prediction performance of word embeddings built from the Earth science journal corpus and trained using domain-specific vocabulary. Our results demonstrate that the accuracy of domain-specific word embeddings in predicting Earth science analogy questions outperforms the ability of general corpus embedding to predict general analogy questions. While the results are as anticipated,&amp;#160; the substantial increase in accuracy, particularly in the lexicographical domain was encouraging. The results point to the need for developing a comprehensive Earth science analogy test set that covers the full breadth of lexicographical and encyclopedic categories for validating word embeddings.&lt;/p&gt;&lt;p&gt;The second experiment utilizes the word embeddings to augment metadata keyword classifications. Metadata describing NASA datasets have science keywords that are manually assigned which can lead to errors and inconsistencies. These science keywords are controlled vocabulary and are used to aid data discovery via faceted search and relevancy ranking. Given the small size of the number of metadata records with proper description and keywords, word embeddings were used for augmentation. A fully connected neural network was trained to suggest keywords given a description text. This approach provided the best accuracy at ~76% as compared to other methods tested.&lt;/p&gt;\n        ",
        "link": "http://dx.doi.org/10.5194/egusphere-egu2020-9966"
    },
    {
        "id": 1787,
        "title": "Computing Movie Script Similarity with Neural Word Embeddings",
        "authors": "Theodore Patsis",
        "published": "2021-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/urtc54388.2021.9701634"
    },
    {
        "id": 1788,
        "title": "Historical Changes in the Meaning of Work: Quantifying Smith, Marx and Weber with Word Embeddings",
        "authors": "Simon Walo",
        "published": "No Date",
        "citations": 0,
        "abstract": "The meaning of work is an important topic in various scientific disciplines. Until recently, however, historical changes in the meaning of work could not be analyzed quantitatively due to a lack of suitable data and methods. Using diachronic word embeddings based on a large historical corpus of English texts, this article is the first to do so for the period 1850-1990. It finds that 1) Adam Smith’s understanding of work as “toil and trouble” remains popular but slightly decreases over time, 2) work is increasingly considered to be a means to an end and therefore becomes increasingly alienated according to Marx, and 3) Weber’s work ethic fluctuates considerably and follows a similar pattern to income inequality during the observed period.",
        "link": "http://dx.doi.org/10.31235/osf.io/pxj7m"
    },
    {
        "id": 1789,
        "title": "Peer Review #3 of \"Comparing general and specialized word embeddings for biomedical named entity recognition (v0.1)\"",
        "authors": "M Xu",
        "published": "2021-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.384v0.1/reviews/3"
    },
    {
        "id": 1790,
        "title": "Concept Mover’s Distance:  Measuring Concept Engagement in Texts via Word  Embeddings",
        "authors": "Dustin S. Stoltz, Marshall A. Taylor",
        "published": "No Date",
        "citations": 0,
        "abstract": "We propose a method for measuring a text's engagement with a focal concept  using  distributional  representations  of  the  meaning  of  words.  More specically, this measure relies on Word Mover's Distance, which uses word embeddings to determine similarities between two documents.  In our approach, which we call Concept Mover's Distance, a document is measured by the minimum distance the words in the document need to travel to arrive at the position of a \"pseudo document\" consisting of only words denoting a focal concept.  This approach captures the prototypical structure of concepts,  is fairly robust to pruning sparse terms as well as variation in text lengths within a corpus, and when used with pre-trained embeddings, can be used even when terms denoting concepts are absent from corpora and can be applied to bag-of-words datasets.  We close by outlining some limitations of the proposed method as well as opportunities for future research.",
        "link": "http://dx.doi.org/10.31235/osf.io/5hc4z"
    },
    {
        "id": 1791,
        "title": "Improving Word Embeddings through Iterative Refinement of Word- and Character-level Models",
        "authors": "Phong Ha, Shanshan Zhang, Nemanja Djuric, Slobodan Vucetic",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.104"
    },
    {
        "id": 1792,
        "title": "Swarm optimization for Arabic word sense disambiguation based on English pre-trained word embeddings",
        "authors": "Bekhouche Abdelaali, Yamina Tlili-Guiassa",
        "published": "2022-11-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isia55826.2022.9993494"
    },
    {
        "id": 1793,
        "title": "The Ability of Word Embeddings to Capture Word Similarities",
        "authors": "Martina Toshevska, Frosina Stojanovska, Jovan Kalajdjieski",
        "published": "2020-6-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/ijnlc.2020.9302"
    },
    {
        "id": 1794,
        "title": "Large Scale Intent Detection in Turkish Short Sentences with Contextual Word Embeddings",
        "authors": "Enes Dündar, Osman Kılıç, Tolga Çekiç, Yusufcan Manav, Onur Deniz",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010108301870192"
    },
    {
        "id": 1795,
        "title": "Learning Word Embeddings from 10-K Filings Using PyTorch",
        "authors": "Saurabh Sehrawat",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3480902"
    },
    {
        "id": 1796,
        "title": "Analogy-based Assessment of Domain-specific Word Embeddings",
        "authors": "Derek Koehl, Carson Davis, Udaysankar Nair, Rahul Ramachandran",
        "published": "2020-3-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon44009.2020.9249736"
    },
    {
        "id": 1797,
        "title": "USING SEMANTIC RELATIONSHIPS TO ENHANCE NEURAL WORD EMBEDDINGS",
        "authors": "",
        "published": "2020-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33965/icwi2020_202012c020"
    },
    {
        "id": 1798,
        "title": "Contextualized vs. Static Word Embeddings for Word-based Analysis of Opposing Opinions",
        "authors": "Wassakorn Sarakul, Attapol T. Rutherford",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jcsse58229.2023.10202014"
    },
    {
        "id": 1799,
        "title": "Which Word Embeddings for Modeling Web Search Queries? Application to the Study of Search Strategies",
        "authors": "Claire Ibarboure, Ludovic Tanguy, Franck Amadieu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012177600003598"
    },
    {
        "id": 1800,
        "title": "A Supervised Multi-class Multi-label Word Embeddings Approach for Toxic Comment Classification",
        "authors": "Salvatore Carta, Andrea Corriga, Riccardo Mulas, Diego Recupero, Roberto Saia",
        "published": "2019",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008110901050112"
    }
]