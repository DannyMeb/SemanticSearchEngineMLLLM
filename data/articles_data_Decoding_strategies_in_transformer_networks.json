[
    {
        "id": 22705,
        "title": "Efficiency Optimization Strategies for Point Transformer Networks",
        "authors": "Jannis Unkrig, Markus Friedrich",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012325000003660"
    },
    {
        "id": 22706,
        "title": "Decoding the aging nexus: unravelling genetic networks and pharmacological\n            strategies for lifespan extension and the methuselah paradox",
        "authors": "Arunava Goswami,  ",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "We are all interested in knowing- whether genes and drugs can increase our\n            life-span. As per Bible, Methuselah's lifespan lasted for a total of 969 years. Recent\n            research has identified the Methuselah gene, a specific DNA segment that holds the\n            potential to promote robust and healthy aging. This discovery opens new avenues for the\n            development of pharmaceutical interventions aimed at extending human lifespan. Aging, a\n            complex process influenced by natural selection, has evolved over time, adapting to\n            factors such as cellular senescence and genetic instability. Research on aging has\n            extensively employed invertebrate models like cnidarians, worms, flies, and yeast.\n            Utilizing genetic methodologies with these organisms has resulted in the identification\n            of numerous aging genes. Remarkably, there is compelling evidence of evolutionary\n            conservation within longevity pathways across diverse species, including mammals. In\n            search of omic study, we would consider data from another set of experiments performed\n            on Cnidarians and show that there has a great advanced on the `biology of aging’ in an\n            indirect way. Cnidarians, like Turritopsis dohrnii, showcase \"ontogeny\n            reversal,\" reverting to earlier stages, thus achieving biological immortality\n            through repeated rejuvenation after reproduction. Alternatively, compounds like\n            resveratrol and rapamycin, have been identified as having the ability to decelerate\n            aging in model organisms. However, as of now, only rapamycin has demonstrated an impact\n            on longevity in experiments on mice. The opportunity to postpone human aging currently\n            exists, whether through established groups of tiny molecules or numerous emerging\n            alternatives. In this context, we explore the approaches to convert findings from\n            age-related research into pharmaceuticals.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55522/jmpas.v13i1.6243"
    },
    {
        "id": 22707,
        "title": "Transformer-Based Video Saliency Prediction with High Temporal Dimension Decoding",
        "authors": "Morteza Moradi, Simone Palazzo, Concetto Spampinato",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012422800003660"
    },
    {
        "id": 22708,
        "title": "On Transformer Autoregressive Decoding for Multivariate Time Series Forecasting",
        "authors": "Mohammed Aldosari, John Miller",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-171"
    },
    {
        "id": 22709,
        "title": "Generative Transformer with Knowledge-Guided Decoding for Academic Knowledge Graph Completion",
        "authors": "Xiangwen Liu, Shengyu Mao, Xiaohan Wang, Jiajun Bu",
        "published": "2023-2-21",
        "citations": 0,
        "abstract": "Academic knowledge graphs are essential resources and can be beneficial in widespread real-world applications. Most of the existing academic knowledge graphs are far from completion; thus, knowledge graph completion—the task of extending a knowledge graph with missing entities and relations—attracts many researchers. Most existing methods utilize low-dimensional embeddings to represent entities and relations and follow the discrimination paradigm for link prediction. However, discrimination approaches may suffer from the scaling issue during inference with large-scale academic knowledge graphs. In this paper, we propose a novel approach of a generative transformer with knowledge-guided decoding for academic knowledge graph completion. Specifically, we introduce generative academic knowledge graph pre-training with a transformer. Then, we propose knowledge-guided decoding, which leverages relevant knowledge in the training corpus as guidance for help. We conducted experiments on benchmark datasets for knowledge graph completion. The experimental results show that the proposed approach can achieve performance gains of 30 units of the MRR score over the baselines on the academic knowledge graph AIDA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11051073"
    },
    {
        "id": 22710,
        "title": "Multi-Branch Decoding Medical Image Segmentation Based on Transformer",
        "authors": "JinSong Zhang, AiGuo Chen, JunWei Zhang",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icis57766.2023.10210271"
    },
    {
        "id": 22711,
        "title": "Improving Text Recognition in Natural Images Using Contextual Modules and Transformer-based Decoding",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.08"
    },
    {
        "id": 22712,
        "title": "LLR Refinement Strategies for Iterative Detection and Decoding Schemes in Cell-Free Massive MIMO Networks",
        "authors": "Tonny Ssettumba, Zhichao Shao, Lukas T. N. Landau, Michelle S. P. Facina, Paulo Branco da Silva, Rodrigo C. de Lamare",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ieeeconf59524.2023.10476860"
    },
    {
        "id": 22713,
        "title": "GRU-Enhanced Decoding by Lightweight Transformer for Image Captioning",
        "authors": "Daksh Chaudhary, Kapil Sharma, Vikas Sharma, Prasuk Jain",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/confluence60223.2024.10463460"
    },
    {
        "id": 22714,
        "title": "Deep Decoding of Strategies - Frontiers in Quantitative Finance Seminar Slides",
        "authors": "Eric Benhamou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4393107"
    },
    {
        "id": 22715,
        "title": "Infrared and Visible Image Fusion Based on Res2Net-Transformer Automatic Encoding and Decoding",
        "authors": "Chunming Wu, Wukai Liu, Xin Ma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2024.048136"
    },
    {
        "id": 22716,
        "title": "Sequential Spatial Transformer Networks for Salient Object Classification",
        "authors": "David Dembinsky, Fatemeh Azimi, Federico Raue, Jörn Hees, Sebastian Palacio, Andreas Dengel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011667100003411"
    },
    {
        "id": 22717,
        "title": "EEG-based epileptic seizure pattern decoding using vision transformer",
        "authors": "Abdelhadi Hireche, Rafat Damseh, Parikshat Sirpal, Abdelkader Nasreddine Belkacem",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iit59782.2023.10366416"
    },
    {
        "id": 22718,
        "title": "Leveraging Transformer and Graph Neural Networks for Variable Misuse Detection",
        "authors": "Vitaly Romanov, Gcinizwe Dlamini, Aidar Valeev, Vladimir Ivanov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011997300003464"
    },
    {
        "id": 22719,
        "title": "Retracted: Regional Financial Data Processing Based on Distributed Decoding Technology",
        "authors": "Security and Communication Networks",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2024/9829230"
    },
    {
        "id": 22720,
        "title": "Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold",
        "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191125"
    },
    {
        "id": 22721,
        "title": "Self-Modularized Transformer: Learn to Modularize Networks for Systematic Generalization",
        "authors": "Yuichi Kamata, Moyuru Yamada, Takayuki Okatani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011682100003417"
    },
    {
        "id": 22722,
        "title": "Blockwise compression of transformer-based models without retraining",
        "authors": "Gaochen Dong, W. Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.001"
    },
    {
        "id": 22723,
        "title": "A Transformer Model with Spatiotemporal Input Embedding for fNIRS data-Driven Neural Decoding",
        "authors": "Hyunmin Lee, Taehun Kim, Jinung An",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bci60775.2024.10480484"
    },
    {
        "id": 22724,
        "title": "Multiband Task Related Components Enhance Rapid Cognition Decoding for Both Small and Similar Objects",
        "authors": "Yusong Zhou, Banghua Yang, Changyong Wang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106313"
    },
    {
        "id": 22725,
        "title": "Research on Particle Swarm Fusion Sliding Mode Tracking Decoding Technology for Rotary Transformer",
        "authors": "Yexin Du, Li Qing, Jie He",
        "published": "2023-2-1",
        "citations": 0,
        "abstract": "Abstract\nA sliding mode tracking fusion particle swarm optimization (PSO) was proposed to solve the soft decoding problem of the rotary transformer. Firstly, the principle of soft decoding of the rotary transformer was expounded and analyzed. Then the mathematical model of the sliding mode tracking and decoding system was established. Based on sliding mode tracking and decoding, the parameters in the model were selected iteratively by using a particle swarm optimization algorithm according to the fitness value function. Finally, the simulation environment was built by Simulink and S-function for simulation verification. The simulation results show that the decoding scheme based on particle swarm optimization and sliding mode tracking can track and decode the rotating transformers at different speeds, such as uniform speed, acceleration speed, and sines/cosines speed, without steady-state error and with certain anti-interference.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2428/1/012029"
    },
    {
        "id": 22726,
        "title": "Decoding the World of Computer Viruses: Types and Proactive Defense Strategies",
        "authors": "Viswajit Srinivasan, T. Navaneethakrishnan, C. Manoharan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4646424"
    },
    {
        "id": 22727,
        "title": "Understanding of Feature Representation in Convolutional Neural Networks and Vision Transformer",
        "authors": "Hiroaki Minoura, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011621300003417"
    },
    {
        "id": 22728,
        "title": "Two-Stream Spatial–Temporal Transformer Networks For Driver Drowsiness Detection",
        "authors": "Qianyi Jiang Qianyi Jiang, Huahu Xu Qianyi Jiang, Chen Cheng Huahu Xu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "\n                        <p>For driver drowsiness detection in the real world, the existing methods have good performance in general. However, when the face is blocked, the light is dim, and the driver’s head posture changes, the performance will deteriorate significantly. In this paper, a two-stream Spatial-Temporal transformer network intended to perform diver drowsiness detection task is proposed to solve the above problems. The spatial-temporal graph is extracted from the video and then the results are obtained from 2s-STTN. The model is a two-stream transformer network model. In our model, the Spatial Self-Attention module is used to learn the embedding of different facial landmarks, and the Temporal Self-Attention module is used to learn the correlation between the frames of facial feature points. Different activated facial landmarks are separated and recognized by class activation mapping technology. Each flow recognizes different activated facial features, extracts spatial or temporal features, and integrates the information about facial features, so as to improve the performance of the system. 2s-STTN can not only mine the long-term dependence of driver behavior from video, but also mine the driver drowsiness information provided by the unobstructed facial signs when the face is blocked. By conducting experiments and comparing our model with other models, it is demonstrated that the proposed model has good performance in driver drowsiness detection.</p>\n<p>&nbsp;</p>\n                    ",
        "keywords": "",
        "link": "http://dx.doi.org/10.53106/199115992023103405008"
    },
    {
        "id": 22729,
        "title": "Optimization Strategies for Low-Latency 5G NR LDPC Decoding on General Purpose Processor",
        "authors": "Mody Sy",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccc57789.2023.10165378"
    },
    {
        "id": 22730,
        "title": "MFTr_Locator: a Novel Transformer Model Decoding Multi-Label Protein Subcellular Locations in Multi-Field ImmunohistoChemistry Image",
        "authors": "Ziqian Wang, Kai Zou, Sihui Zhu, Peiqi Cai, Fan Yang",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csecs60003.2023.10428564"
    },
    {
        "id": 22731,
        "title": "Secrecy Performance of SWIPT Cognitive Radio Networks with Eavesdropper's Decoding Capability",
        "authors": "Soleha Kousar, Ajay Singh",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ncc60321.2024.10486035"
    },
    {
        "id": 22732,
        "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
        "authors": "Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, Emanuele Rodola",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.689"
    },
    {
        "id": 22733,
        "title": "Heterogeneous Graph Transformer for Advanced Persistent Threat Classification in Wireless Networks",
        "authors": "Kazeem Saheed, Shagufta Henna",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nfv-sdn59219.2023.10329745"
    },
    {
        "id": 22734,
        "title": "Improving collision resolution of superposed LoRa signals using a Slot-Free Decoding Scheme",
        "authors": "Weixuan Xiao, Nancy El Rachkidy, Alexandre Guitton",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.adhoc.2024.103442"
    },
    {
        "id": 22735,
        "title": "A Novel Technique for Differential Protection for Resistance Grounded Three Phase Transformer Using Wavelet Packet Transformer",
        "authors": "R. Gomathi, R. Arunkumar, S.T. Preethi, P.V. Arunprakash, G. Karkuzhali",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscna58489.2023.10370331"
    },
    {
        "id": 22736,
        "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks",
        "authors": "Sunit Bhattacharya, Ondřej Bojar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.9"
    },
    {
        "id": 22737,
        "title": "A reliable anchor regenerative-based transformer model for x-small and dense objects recognition",
        "authors": "Ponduri Vasanthi, Laavanya Mohan",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.020"
    },
    {
        "id": 22738,
        "title": "Decoding the Influence of Financial Constraints on Retail Product Strategies During Economic Cycles in the U.S.",
        "authors": "Juan Andres Espinosa-Torres",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4368509"
    },
    {
        "id": 22739,
        "title": "STTRE: A Spatio-Temporal Transformer with Relative Embeddings for multivariate time series forecasting",
        "authors": "Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.09.039"
    },
    {
        "id": 22740,
        "title": "Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks for Document Layout Analysis",
        "authors": "",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfer have been developed, it remains to be investigated to what degree they can effectively transfer knowledge. In this study, we aim to fill these gaps by conducting a comparative evaluation of state-of-the-art models in document layout analysis and investigating the potential of cross-lingual layout analysis by utilizing machine translation techniques.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.17"
    },
    {
        "id": 22741,
        "title": "Generating Synthetic Population Using Transformer Based Networks",
        "authors": "Phattranit Phattharajiranan, Veera Muangsin",
        "published": "2023-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsec59635.2023.10329722"
    },
    {
        "id": 22742,
        "title": "Joint estimation of pose, depth, and optical flow with a competition–cooperation transformer network",
        "authors": "Xiaochen Liu, Tao Zhang, Mingming Liu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.020"
    },
    {
        "id": 22743,
        "title": "Relaxed Attention for Transformer Models",
        "authors": "Timo Lohrenz, Björn Möller, Zhengyang Li, Tim Fingscheidt",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191643"
    },
    {
        "id": 22744,
        "title": "DCMSTRD: End-to-end Dense Captioning via Multi-Scale Transformer Decoding",
        "authors": "Zhuang Shao, Jungong Han, Kurt Debattista, Yanwei Pang",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmm.2024.3369863"
    },
    {
        "id": 22745,
        "title": "BrainGridNet: A two-branch depthwise CNN for decoding EEG-based multi-class motor imagery",
        "authors": "Xingfu Wang, Yu Wang, Wenxia Qi, Delin Kong, Wei Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.037"
    },
    {
        "id": 22746,
        "title": "Strategies For 5g Nr Networks Distribution In Rwanda",
        "authors": "",
        "published": "2023-2-27",
        "citations": 0,
        "abstract": "The fifth-generation mobile network has been developed and standardized with an intention of exploring the market beyond 2017. The question was to study the distribution strategies to fill out the needs of this strong network introduced for real-time feedback. Mobile operators proposed five alternative network architectures to 3GPP. The response to the question was to carry out the first distribution strategy which is Non-Standalone New Radio (NSA NR) in 2017 followed by a Standalone New Radio access network that was standardized in 2018. This study analyzes the distribution strategies of the 5G New Radio (NR) network, main technologies, benefits and drawbacks and compares NSA NR and SA NR distribution modes in terms of coverage, network capability, the inter-working which is between 4G and 5G, complexity and network distribution cost in Rwanda. This also highlights the potential challenges to implement 5G especially in Least Developed Countries. Micro Operators have been also used to accelerate the deployment for new entries and presented some challenges where the proposed solution for them has been the planification of a huge amount of economic resources as well as skilled manpower. The study concluded that the reliable and efficient distribution strategies of 5G NR Networks will be based on Non-Standalone (NSA), Standalone (SA) and the Micro Operators considerations to accelerate its distribution not only in global context but also in Rwanda",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jctcsr.02.01.03"
    },
    {
        "id": 22747,
        "title": "Ultrasonic Imaging and Flaw Detection with Optimized Convolutional Transformer Neural Networks",
        "authors": "Xin Zhang, Jafar Saniie",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ius51837.2023.10307997"
    },
    {
        "id": 22748,
        "title": "Decoding Corporate Financial Health: A Comprehensive Quantitative Analysis of Annual Accounts and Financing Strategies",
        "authors": " Cut Nadira Putri Kamal",
        "published": "2024-2-25",
        "citations": 0,
        "abstract": "Abstract\r\nThis study navigates the intricate landscape of corporate finance within publicly traded Indonesian companies, aiming to comprehensively grasp their financial health. A holistic approach merges quantitative analyses of annual accounts with qualitative insights from corporate finance, focusing on unraveling the nuanced relationship between financing techniques and financial performance indicators, considering diverse variables such as industry characteristics and company sizes. Employing robust methodologies including regression analysis, correlation analysis, and panel data analysis, the study uncovers significant trends. Noteworthy findings include a burgeoning reliance on debt financing, the emergence of hybrid financing structures, and industry-specific variations in financing preferences. The positive correlation between equity financing and profitability aligns seamlessly with agency theory, underlining its potential for aligning management interests with long-term value creation. Meanwhile, the nuanced impact of debt financing emphasizes the imperative for companies to delicately balance the benefits and risks associated with financial leverage. Implications derived from the research extend practical advice to industry practitioners, emphasizing the need for tailored financing strategies, strengthened corporate governance, and industry-specific considerations. Policymakers are encouraged to play a pivotal role in fostering equity market development, implementing responsible regulatory measures, and championing transparency in financial reporting. This research not only contributes to the existing financial literature by enriching academic discourse but also provides actionable insights for businesses navigating the intricate terrain of corporate finance in Indonesia.",
        "keywords": "",
        "link": "http://dx.doi.org/10.61292/birev.98"
    },
    {
        "id": 22749,
        "title": "Decoding E-Bulletins: A Thorough Examination of their Role in Contemporary Communication Strategies",
        "authors": "Krupal Joshi",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "An electronic bulletin, commonly known as an e-bulletin, is a dynamic and versatile digital publication or newsletter distributed through electronic means. These means predominantly include email delivery or hosting on dedicated websites. The overarching goal of e-bulletins is to facilitate the dissemination of information, updates, announcements, and various other relevant content to a specific audience. This electronic communication tool has gained widespread adoption across diverse sectors, including organizations, associations, academic institutions, and businesses. The digital landscape has ushered in a new era of communication, and e-bulletins stand as a testament to the adaptability of traditional communication methods to the evolving technological landscape.",
        "keywords": "",
        "link": "http://dx.doi.org/10.56450/jefi.2024.v2i01.07"
    },
    {
        "id": 22750,
        "title": "Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation",
        "authors": "Markus Freitag, Behrooz Ghorbani, Patrick Fernandes",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.617"
    },
    {
        "id": 22751,
        "title": "IoUformer: Pseudo-IoU prediction with transformer for visual tracking",
        "authors": "Huayue Cai, Long Lan, Jing Zhang, Xiang Zhang, Yibing Zhan, Zhigang Luo",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.055"
    },
    {
        "id": 22752,
        "title": "Backchannel Detection and Agreement Estimation from Video with Transformer Networks",
        "authors": "Ahmed Amer, Chirag Bhuvaneshwara, Gowtham K. Addluri, Mohammed M. Shaik, Vedant Bonde, Philipp Müller",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191640"
    },
    {
        "id": 22753,
        "title": "Error Detection Strategies for CRC-Concatenated Polar Codes under Successive Cancellation List Decoding",
        "authors": "Alexander Sauter, Balázs Matuz, Gianluigi Liva",
        "published": "2023-3-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089769"
    },
    {
        "id": 22754,
        "title": "Investigating the Use of Spatial Transformer Networks and Recurrent Neural Networks for Medical Image Segmentation",
        "authors": "Vineet Saxena, M N Nachappa, Ritu Shree",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470837"
    },
    {
        "id": 22755,
        "title": "Generalized image outpainting with U-transformer",
        "authors": "Penglei Gao, Xi Yang, Rui Zhang, John Y. Goulermas, Yujie Geng, Yuyao Yan, Kaizhu Huang",
        "published": "2023-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.02.021"
    },
    {
        "id": 22756,
        "title": "On Defining Smart Cities using Transformer Neural Networks",
        "authors": "Andrei Khurshudov",
        "published": "2024-1-28",
        "citations": 0,
        "abstract": "Cities worldwide are rapidly adopting “smart” technologies, transforming urban life. Despite this trend, a universally accepted definition of “smart city” remains elusive. Past efforts to define it haven’t yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new “compromise” definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing definitions of smart cities, generating a list of potential new composite definitions. Each of these new definitions was then tested against the pre-existing individual definitions we’ve gathered, using cosine similarity as our metric. This process identified smart city definitions with the highest average cosine similarity, semantically positioning them as the closest on average to all the 60 individual definitions selected.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24297/ijct.v24i.9579"
    },
    {
        "id": 22757,
        "title": "High Dynamic Range Imaging with Context-aware Transformer",
        "authors": "Fangfang Zhou, Zhengming Fu, Dan Zhang",
        "published": "2023-6-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191491"
    },
    {
        "id": 22758,
        "title": "Meta-Reinforcement Learning with Transformer Networks for Space Guidance Applications",
        "authors": "Lorenzo Federici, Roberto Furfaro",
        "published": "2024-1-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2061"
    },
    {
        "id": 22759,
        "title": "Decoding CIBIL: Understanding the Structure and Strategies for Improvement",
        "authors": " Mr. Rutik Vyawhare,  Dr. Rutuja Deshpande",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "This research paper delves into the intricate dynamics of credit scoring systems, particularly focusing on understanding the structure of CIBIL scores and delineating strategies for improving creditworthiness. Drawing upon theoretical frameworks from financial economics and regulatory economics, the study elucidates key factors influencing credit scores, including payment history, credit utilization, credit type, and duration. Through a mixed-methods approach encompassing secondary data analysis from the official websites of TransUnion CIBIL India and Bajaj Finance Limited, as well as qualitative exploration through thematic analysis, the research uncovers actionable insights for individuals seeking to enhance their credit profiles.By bridging theoretical insights with empirical findings, this study contributes to the literature on credit management and equips individuals with knowledge essential for navigating the complexities of credit assessment and improving financial resilience",
        "keywords": "",
        "link": "http://dx.doi.org/10.48175/ijarsct-17260"
    },
    {
        "id": 22760,
        "title": "Memory-efficient Transformer-based network model for Traveling Salesman Problem",
        "authors": "Hua Yang, Minghao Zhao, Lei Yuan, Yang Yu, Zhenhua Li, Ming Gu",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.02.014"
    },
    {
        "id": 22761,
        "title": "Retracted: Research on Pattern Recognition Method of Transformer Partial Discharge Based on Artificial Neural Network",
        "authors": "",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9783134"
    },
    {
        "id": 22762,
        "title": "On-FPGA Spiking Neural Networks for End-to-End Neural Decoding",
        "authors": "Gianluca Leone, Luigi Raffo, Paolo Meloni",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3269598"
    },
    {
        "id": 22763,
        "title": "Simple Peak Interference Cancellation (SPIC): Interference Cancellation Prior to Packet Decoding in LoRa Networks",
        "authors": "Jumana Bukhari, Zhenghao Zhang",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/latincom59467.2023.10361900"
    },
    {
        "id": 22764,
        "title": "Deforestation Detection in the Brazilian Amazon Using Transformer-based Networks",
        "authors": "Mariam Alshehri, Anes Ouadou, Grant Scott",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00130"
    },
    {
        "id": 22765,
        "title": "Towards infrared human pose estimation via Transformer",
        "authors": "Zhilei Zhu, Wanli Dong, Xiaoming Gao, Anjie Peng",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191173"
    },
    {
        "id": 22766,
        "title": "Complex-Valued Relative Positional Encodings for Transformer",
        "authors": "Gang Yang, Hongzhe Xu",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105716"
    },
    {
        "id": 22767,
        "title": "Improving the Performance of Human Part Segmentation Based on Swin Transformer",
        "authors": " Juan Du,  Tao Yang",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23020030"
    },
    {
        "id": 22768,
        "title": "CCGN: Centralized collaborative graphical transformer multi-agent reinforcement learning for multi-intersection signal free-corridor",
        "authors": "Hamza Mukhtar, Adil Afzal, Sultan Alahmari, Saud Yonbawi",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.027"
    },
    {
        "id": 22769,
        "title": "Decoding the Unique Price Behavior in the Japanese Stock Market with Convolutional Neural Networks",
        "authors": "Katsuhiko Okada, Yukinobu Hamuro, Moe Nakasuji",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4478013"
    },
    {
        "id": 22770,
        "title": "Decoding Convolutional Hadamard Codes and Turbo Hadamard Codes using Recurrent Neural Networks",
        "authors": "Sheng Jiang, Francis C. M. Lau",
        "published": "2024-2-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/icact60172.2024.10472004"
    },
    {
        "id": 22771,
        "title": "Spiking Neural Networks for Integrated Reach-to-Grasp Decoding on FPGAs",
        "authors": "Gianluca Leone, Luca Martis, Luigi Raffo, Paolo Meloni",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/biocas58349.2023.10389037"
    },
    {
        "id": 22772,
        "title": "Inrush-currents of series combination of transformer with in-phase regulation and phase shifting transformer at the interface between transmmission and distribution networks",
        "authors": "J. Bai, F. Shewarega, H. Vennegeerts, R. Lechner, G. Etz, M. Unterholzer-Moser",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.0951"
    },
    {
        "id": 22773,
        "title": "Explore Deep Learning Trends By Decoding Activated Networks",
        "authors": "K Gowri, MP Sunil, Rakesh Kumar Yadav",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiccit60255.2023.10466011"
    },
    {
        "id": 22774,
        "title": "Transformer-OPU: An FPGA-based Overlay Processor for Transformer Networks",
        "authors": "Yueyin Bai, Hao Zhou, Keqing Zhao, Jianli Chen, Jun Yu, Kun Wang",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/fccm57271.2023.00049"
    },
    {
        "id": 22775,
        "title": "Decoding self-motion from visual image sequence predicts distinctive features of reflexive motor responses to visual motion",
        "authors": "Daiki Nakamura, Hiroaki Gomi",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.020"
    },
    {
        "id": 22776,
        "title": "FITrans: Skin Lesion Segmentation Based on Feature Integration and Transformer",
        "authors": "Likun Zhang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105777"
    },
    {
        "id": 22777,
        "title": "Patch-wise Mixed-Precision Quantization of Vision Transformer",
        "authors": "Junrui Xiao, Zhikai Li, Lianwei Yang, Qingyi Gu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191205"
    },
    {
        "id": 22778,
        "title": "Spatial-Temporal Interaction Decoding Transformer for Unsupervised Multivariate Time Series Anomaly Detection",
        "authors": "Songlin Yang, Jing Li, Kuanzhi Shi, Yu Chen, Yunlong Zhu, Xudong He, Jinlong Wu, Chenling Pan",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448347"
    },
    {
        "id": 22779,
        "title": "Improving Seismic Fault Recognition with Self-Supervised Pre-Training: A Study of 3D Transformer-Based with Multi-Scale Decoding and Fusion",
        "authors": "Zeren Zhang, Ran Chen, Jinwen Ma",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "Seismic fault interpretation holds great significance in the fields of geophysics and geology. However, conventional methods of seismic fault recognition encounter various issues. For example, models trained on synthetic data often exhibit inadequate generalization when applied to field seismic data, and supervised learning is heavily dependent on the quantity and quality of annotated data, being susceptible to the subjectivity of interpreters. To address these challenges, we propose applying self-supervised pre-training methods to seismic fault recognition, exploring the transfer of 3D Transformer-based backbone networks and different pre-training methods on fault recognition tasks, thereby enabling the model to learn more powerful feature representations from extensive unlabeled datasets. Additionally, we propose an innovative pre-training strategy for the entire segmentation network based on the characteristics of seismic data and introduce a multi-scale decoding and fusion module that significantly improves recognition accuracy. Specifically, during the pre-training stage, we compare various self-supervision methods, like MAE, SimMIM, SimCLR, and a joint self-supervised learning approach. We adopt multi-scale decoding step-by-step fitting expansion targets during the fine-tuning stage. Ultimately merging features to refine fault edges, the model displays superior adaptability when handling narrow, elongated, and unevenly distributed fault annotations. Experiments demonstrate that our proposed method achieves state-of-the-art performance on Thebe, the currently largest publicly annotated dataset in this field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs16050922"
    },
    {
        "id": 22780,
        "title": "CT-Net: Asymmetric compound branch Transformer for medical image segmentation",
        "authors": "Ning Zhang, Long Yu, Dezhi Zhang, Weidong Wu, Shengwei Tian, Xiaojing Kang, Min Li",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.034"
    },
    {
        "id": 22781,
        "title": "Insights into Object Semantics: Leveraging Transformer Networks for Advanced Image Captioning",
        "authors": "Deema Abdal Hafeth, Stefanos Kollias",
        "published": "2024-3-11",
        "citations": 0,
        "abstract": "Image captioning is a technique used to generate descriptive captions for images. Typically, it involves employing a Convolutional Neural Network (CNN) as the encoder to extract visual features, and a decoder model, often based on Recurrent Neural Networks (RNNs), to generate the captions. Recently, the encoder–decoder architecture has witnessed the widespread adoption of the self-attention mechanism. However, this approach faces certain challenges that require further research. One such challenge is that the extracted visual features do not fully exploit the available image information, primarily due to the absence of semantic concepts. This limitation restricts the ability to fully comprehend the content depicted in the image. To address this issue, we present a new image-Transformer-based model boosted with image object semantic representation. Our model incorporates semantic representation in encoder attention, enhancing visual features by integrating instance-level concepts. Additionally, we employ Transformer as the decoder in the language generation module. By doing so, we achieve improved performance in generating accurate and diverse captions. We evaluated the performance of our model on the MS-COCO and novel MACE datasets. The results illustrate that our model aligns with state-of-the-art approaches in terms of caption generation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s24061796"
    },
    {
        "id": 22782,
        "title": "Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System",
        "authors": "Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.28"
    },
    {
        "id": 22783,
        "title": "Automatic bat call classification using transformer networks",
        "authors": "Frank Fundel, Daniel A. Braun, Sebastian Gottwald",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ecoinf.2023.102288"
    },
    {
        "id": 22784,
        "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
        "authors": "László Tóth, Amin Honarmandi Shandiz, Gábor Gosztolya, Tamás Gábor Csapó",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1607"
    },
    {
        "id": 22785,
        "title": "Faithfulness-Aware Decoding Strategies for Abstractive Summarization",
        "authors": "David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, Mohit Bansal",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.210"
    },
    {
        "id": 22786,
        "title": "Decoding Pragmatic Strategies in Ferdy Sambo's Plea: Seeking a Glimpse of Hope in the Claustrophobic Courtroom",
        "authors": "Rotua Elfrida, Jubilezer Sihite, Selfiyani Ketaren",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "The prevailing research inadequately explores the pragmatic strategies within courtroom discourse, particularly in non-Western legal systems such as Indonesia. This study embarks on bridging this gap by analyzing pragmatic strategies employed in Ferdy Sambo's plea within the Indonesian legal environment. Utilizing qualitative methods encompassing discourse and pragmatic analysis, this study scrutinizes the deployment of linguistic techniques and pragmatic strategies to meet communicative objectives. The investigation unveils a meticulous utilization of assertive and declarative illocutionary acts, demonstrating a narrative woven with assertiveness, seeking validation and empathy in the claustrophobic courtroom atmosphere.  Furthermore, it revealed the significance of understanding the intricate relationship between societal norms, values, and the narrative crafted within a plea, especially in culturally rich contexts like Indonesia. This endeavor enhances the comprehension of the pragmatic strategies in Ferdy Sambo's plea and delineates the broader societal influences that dictate the narrative dynamics within the Indonesian legal discourse. Through this lens, the study contributes substantially to pragmatics, discourse analysis, and legal communication, inviting further nuanced research.\r\n Keywords: Pragmatics, Ferdy Sambo, Claustrophobic Courtroom, Glimpse",
        "keywords": "",
        "link": "http://dx.doi.org/10.51278/bse.v4i1.982"
    },
    {
        "id": 22787,
        "title": "A Hierarchical Vision Transformer Using Overlapping Patch and Self-Supervised Learning",
        "authors": "Yaxin Ma, Ming Li, Jun Chang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191916"
    },
    {
        "id": 22788,
        "title": "Semantic Enrichment of Video Content using NLP Transformer Networks",
        "authors": "M. Hanumesh, K. Sankar Brahmachari, Anitha G, BALACHANDRA PATTANAIK",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4779344"
    },
    {
        "id": 22789,
        "title": "Decoding the Black Box through a Comparative Study on Clustering Features in Convolutional Neural Networks",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.061213"
    },
    {
        "id": 22790,
        "title": "Generative AI on a Budget: Processing Transformer- based Neural Networks at the Edge",
        "authors": "Y. Tanurhan, P. Paulin, T. Michiels",
        "published": "2023-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iedm45741.2023.10413675"
    },
    {
        "id": 22791,
        "title": "Decoding Student Success in Higher Education: A Comparative Study on Learning Strategies of Undergraduate and Graduate Students",
        "authors": "Ricardo Santos, Roberto Henriques",
        "published": "2024-4-2",
        "citations": 0,
        "abstract": "\r\n\r\nLearning management systems (LMS) provide a rich source of data about the engagement of students with courses and their materials that tends to be underutilized in practice. In this paper, we use data collected from the LMS to uncover learning strategies adopted by students and compare their effectiveness. Starting from a sample of over 11,000 enrollments at a Portuguese information management school, we extracted features indicative of self-regulated learning (SRL) behavior from the associated interactions. Then, we employed an unsupervised machine learning algorithm (k-means) to group students according to the similarity of their patterns of interaction. This process was conducted separately for undergraduate and graduate students. Our analysis uncovered five distinct learning strategy profiles at both the undergraduate and graduate levels: 1) active, prolonged and frequent engagement; 2) mildly frequent and task-focused engagement; 3) mildly frequent, mild activity in short sessions engagement; 4) likely procrastinators; and 5) inactive. Mapping strategies with the students' final grades, we found that students at both levels who accessed the LMS early and frequently had better outcomes. Conversely, students who exhibited procrastinating behavior had worse end-of-course grades. Interestingly, the relative effectiveness of the various learning strategies was consistent across instruction levels. Despite the LMS offering an incomplete and partial view of the learning processes students employ, these findings suggest potentially generalizable relationships between online student behaviors and learning outcomes. While further validation with new data is necessary, these connections between online behaviors and performance could guide the development of personalized, adaptive learning experiences.\r\n\r\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.5817/sp2023-3-3"
    },
    {
        "id": 22792,
        "title": "IC-CViT: Inverse-Consistent Convolutional Vision Transformer for Diffeomorphic Image Registration",
        "authors": "Tao Xu, Ting Jiang, Xiaoning Li",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191209"
    },
    {
        "id": 22793,
        "title": "Evaluation of Harmonic Distortion in Distribution Networks under Transformer N-1 Security Criterion",
        "authors": "Pablo Rodríguez-Pajarón, Araceli Hernández, Jovica V. Milanović",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202763"
    },
    {
        "id": 22794,
        "title": "EEG Source Imaging based on a Transformer Encoder Network",
        "authors": "Tongtong Zheng, Zijing Guan",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105793"
    },
    {
        "id": 22795,
        "title": "PointCMT: An MLP-Transformer Network for Contrastive Learning of Point Representation",
        "authors": "Chuyu Wang, Xianfeng Han, Guoqiang Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191174"
    },
    {
        "id": 22796,
        "title": "Optimal Data Decoding Strategies for Product-Coded Sequential Media Recording Via Latin Squares",
        "authors": "Suayb S. Arslan",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmag.2022.3218757"
    },
    {
        "id": 22797,
        "title": "Continuous Human Activity Classification with Radar Point Clouds and Point Transformer Networks",
        "authors": "Nicolas C. Kruse, Francesco Fioranelli, Alexander Yarovoy",
        "published": "2023-9-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eurad58043.2023.10289503"
    },
    {
        "id": 22798,
        "title": "SwinVI:3D Swin Transformer Model with U-net for Video Inpainting",
        "authors": "Wei Zhang, Yang Cao, Junhai Zhai",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192024"
    },
    {
        "id": 22799,
        "title": "Hierarchical vector transformer vehicle trajectories prediction with diffusion convolutional neural networks",
        "authors": "Yingjuan Tang, Hongwen He, Yong Wang",
        "published": "2024-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127526"
    },
    {
        "id": 22800,
        "title": "Restoring Degraded Old Films with Recursive Recurrent Transformer Networks",
        "authors": "Shan Lin, Edgar Simo-Serra",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00658"
    },
    {
        "id": 22801,
        "title": "Improving the Interpretability of fMRI Decoding Using Deep Neural Networks and Adversarial Robustness",
        "authors": "Patrick McClure, Dustin Moraczewski, Ka Chun Lam, Adam Thomas, Francisco Pereira",
        "published": "2023-8-7",
        "citations": 1,
        "abstract": "Deep neural networks (DNNs) are being increasingly used to make predictions from functional magnetic resonance imaging (fMRI) data. However, they are widely seen as uninterpretable “black boxes,” as it can be difficult to discover what input information is used by the DNN in the process, something important in both cognitive neuroscience and clinical applications. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, methods for creating maps often fail due to DNNs being sensitive to input noise or by focusing too much on the input and too little on the model. It is also challenging to evaluate how well saliency maps correspond to the truly relevant input information, as ground truth is not always available. In this paper, we review a variety of methods for producing gradient-based saliency maps and present an adversarial training method we developed to make DNNs robust to input noise, with the goal of improving the quality of DNN saliency maps. We introduce two quantitative evaluation procedures for saliency map methods in fMRI, applicable whenever a DNN or linear model is being trained to decode some information from imaging data. We evaluate the procedures using synthetic data sets, where the complex activation structure is known, and on fMRI data from the Human Connectome Project using saliency maps produced for linear models and DNNs doing task decoding in both settings. Our key finding is that saliency maps produced with different methods vary widely in quality in both synthetic and Human Connectome Project fMRI data, even when those methods have similar prediction performance. We found that training both linear and non-linear models using adversarial noise increased the quality of their saliency maps. We also found that, while some linear models generate good saliency maps in the highly controlled, synthetic data, non-linear DNN methods generate better saliency maps in real-world fMRI data. Finally, our experiments give evidence that combining adversarial training with a complex, non-linear model can improve saliency map quality when compared to several methods commonly used in the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52294/001c.85074"
    },
    {
        "id": 22802,
        "title": "STNILM: Switch Transformer based Non-Intrusive Load Monitoring for short and long duration appliances",
        "authors": "L.N. Sastry Varanasi, Sri Phani Krishna Karri",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101246"
    },
    {
        "id": 22803,
        "title": "A Dual-Decoding branch U-shaped semantic segmentation network combining Transformer attention with Decoder: DBUNet",
        "authors": "Yuefei Wang, Xi Yu, Xiaoyan Guo, Xilei Wang, Yuanhong Wei, Shijie Zeng",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jvcir.2023.103856"
    },
    {
        "id": 22804,
        "title": "A Modified Transformer Neural Network (MTNN) for Robust Intrusion Detection in IoT Networks",
        "authors": "Syed Wahaj Ahmed, Fabio Kientz, Rasha Kashef",
        "published": "2023-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itc-egypt58155.2023.10206134"
    },
    {
        "id": 22805,
        "title": "Islanding Detection Using Transformer Neural Networks",
        "authors": "Edson David, Jordan Rel Orillaza, Jhoanna Rhodette Pedrasa",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca60615.2024.10471069"
    },
    {
        "id": 22806,
        "title": "Decoding Malnutrition Trends in India: A Comprehensive Analysis Using National Family Health Survey Data for Informed Strategies and Interventions",
        "authors": "Bhautik Modi, Ankit Sheth, Rhythm Panchani",
        "published": "2023",
        "citations": 0,
        "abstract": "\nIntroduction:\nDespite the growth and development, in sectors, malnutrition continues to be a challenge that cannot be ignored. The current study is crafted with the primary objective of exploring the trends in malnutrition in India. It seeks to delve into both direct and indirect determinants, utilising data from the National Family Health Survey (NFHS) to gain valuable insights into the evolving landscape of nutrition in the country.\n\n\nMethodology:\nThe current study was conducted by reviewing the data of all NFHSs available on an online portal. The trend of malnutrition and its direct and indirect determinants were assessed.\n\n\nResults:\nThe percentage of children under 5 years who are underweight (weight-for-age) is in decreasing trend from 42.7% (NFHS-2) to 32.1% (NFHS-5). Similarly, the percentage of children under 5 years who are stunted (height-for-age) and/or wasted (weight-for-height) shows decreasing trends. Whereas, the percentage of children under 5 years who are severely wasted (weight-for-height) is increasing trend. Notably, direct and indirect determinants show an improving trend in NFHS-5.\n\n\nConclusion:\nAlthough many nutritional indicators such as underweight, stunting and wasting amongst children under 5 years of age show downward trends in India, the area of concern is the upward trend of indicators such as severely wasted (weight-for-height). Direct and indirect determinants for the nutritional indicators such as proper IYCF practices, routine immunisation and anaemia amongst women need to be strengthened, which can help in further improvement in the nutritional status of children below 5 years.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.4103/jnmo.jnmo_8_23"
    },
    {
        "id": 22807,
        "title": "VITAL: Vision Transformer Neural Networks for Accurate Smartphone Heterogeneity Resilient Indoor Localization",
        "authors": "Danish Gufran, Saideep Tiku, Sudeep Pasricha",
        "published": "2023-7-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247684"
    },
    {
        "id": 22808,
        "title": "Transformer-based Siamese and Triplet Networks for Facial Expression Intensity Estimation",
        "authors": "Motaz SABRI",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5057/ijae.ijae-d-22-00011"
    },
    {
        "id": 22809,
        "title": "Semantic Queries with Transformer for Referring Image Segmentation",
        "authors": "Yukun Zhai",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3605801.3605832"
    },
    {
        "id": 22810,
        "title": "Face Image Restoration Method Using Semantic and Transformer Splitting Networks",
        "authors": "Hyoungki Choi, Jinsol Choi, Heunseung Lim, Joonki Paik",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce59016.2024.10444243"
    },
    {
        "id": 22811,
        "title": "TrajTransGCN: Enhancing trajectory prediction by fusing transformer and graph neural networks",
        "authors": "Haojun Pan",
        "published": "2023-12-26",
        "citations": 0,
        "abstract": "This paper proposes a novel model named TrajTransGCN for taxi trajectory prediction, which leverages the power of both graph convolutional networks (GCNs) and Transformer. TrajTransGCN first passes the input through the GCN layer and then combines the GCN outputs with one-hot encoded categorical features as input to the transformer layer. This paper evaluates. TrajTransGCN uses real-world taxi trajectory datasets in Porto and compares it against several baselines. The experimental results show that TrajTransGCN outperforms all the other models in terms of both RMSE and MAPE. Specifically, the model achieves an RMSE of 0.0247 and a MAPE of 0.09%, which are significantly lower than those of the other models. The results demonstrate the effectiveness of the proposed model in predicting taxi trajectories, indicating the potential of leveraging both GCN and transformer layers in trajectory prediction tasks. In addition, this paper includes ablation experiments to demonstrate the effectiveness of using one-hot encodings of classification labels in complex real-time scenarios. In addition, a parameter study is carried out to examine how the TrajTransGCN's performance is impacted by the learning rate, the quantity of Transformer layers, and the size of the hidden dimension of the Transformer layer.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/29/20230785"
    },
    {
        "id": 22812,
        "title": "Intermittent demand forecasting with transformer neural networks",
        "authors": "G. Peter Zhang, Yusen Xia, Maohua Xie",
        "published": "2023-6-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10479-023-05447-7"
    },
    {
        "id": 22813,
        "title": "Faster Convergence for Transformer Fine-tuning with Line Search Methods",
        "authors": "Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192001"
    },
    {
        "id": 22814,
        "title": "Transformer Neural Networks, Information Biology, and Alzheimer’s Disease",
        "authors": "Melanie Swan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractBackgroundAlzheimer’s disease affects one in ten people older than 65 years, with the prevalence expected to triple by 2030, together with commensurate increases in cost. A comprehensive causal model of Alzheimer’s disease etiology is lacking despite more than three decades of study. Modern computational methods provide a new approach to multiscalar biosystems by modeling neuropathology in its native three‐dimensional character in topological biophysics and quantum computational models.MethodThis work describes how an emerging standard in machine learning, responsible for the AlphaFold (protein folding structure) and Gato generalist agent projects, transformer neural networks, may be applied to the study of Alzheimer’s disease per proven results in bot text chat, image recognition and creation, video generation, software programming, and autonomy in robotics and driving. The self‐attention and self‐learning mechanisms of transformer neural networks are discussed in application to multifaceted Alzheimer’s disease data sets involving neuroimaging scans, whole human genome data, and blood and CSF biomarker data.ResultA machine learning‐based topological biophysics model of Alzheimer’s disease neuropathology hypothesis‐testing (misfolded proteins, gene expression, diabetes type‐3, and peptide immune system stimulation) is described.ConclusionThere is an opportunity to apply state‐of‐the‐art transformer neural network machine learning techniques to the automated analysis of large accruing data stores of Alzheimer’s information (imaging, genomic, proteomic, and self‐reported data) towards the causal understanding of the neuropathology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/alz.071502"
    },
    {
        "id": 22815,
        "title": "Metro Ridership Forecasting using Inter-Station-Aware Transformer Networks",
        "authors": "Khaled Saleh, Adriana-Simona Mihaita, Yuming Ou",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421998"
    },
    {
        "id": 22816,
        "title": "MI-CAT: A transformer-based domain adaptation network for motor imagery classification",
        "authors": "Dongxue Zhang, Huiying Li, Jingmeng Xie",
        "published": "2023-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.005"
    },
    {
        "id": 22817,
        "title": "Are transformer-based models more robust than CNN-based models?",
        "authors": "Zhendong Liu, Shuwei Qian, Changhong Xia, Chongjun Wang",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.045"
    },
    {
        "id": 22818,
        "title": "Fine-tuning Strategies for Classifying Community-Engaged Research Studies Using Transformer-Based Models: Algorithm Development and Improvement Study",
        "authors": "Brian J Ferrell",
        "published": "2023-2-7",
        "citations": 2,
        "abstract": "\nBackground\nCommunity-engaged research (CEnR) involves institutions of higher education collaborating with organizations in their communities to exchange resources and knowledge to benefit a community’s well-being. While community engagement is a critical aspect of a university's mission, tracking and reporting CEnR metrics can be challenging, particularly in terms of external community relations and federally funded research programs. In this study, we aimed to develop a method for classifying CEnR studies that have been submitted to our university's institutional review board (IRB) to capture the level of community involvement in research studies. Tracking studies in which communities are “highly engaged” enables institutions to obtain a more comprehensive understanding of the prevalence of CEnR.\n\n\nObjective\nWe aimed to develop an updated experiment to classify CEnR and capture the distinct levels of involvement that a community partner has in the direction of a research study. To achieve this goal, we used a deep learning–based approach and evaluated the effectiveness of fine-tuning strategies on transformer-based models.\n\n\nMethods\nIn this study, we used fine-tuning techniques such as discriminative learning rates and freezing layers to train and test 135 slightly modified classification models based on 3 transformer-based architectures: BERT (Bidirectional Encoder Representations from Transformers), Bio+ClinicalBERT, and XLM-RoBERTa. For the discriminative learning rate technique, we applied different learning rates to different layers of the model, with the aim of providing higher learning rates to layers that are more specialized to the task at hand. For the freezing layers technique, we compared models with different levels of layer freezing, starting with all layers frozen and gradually unfreezing different layer groups. We evaluated the performance of the trained models using a holdout data set to assess their generalizability.\n\n\nResults\nOf the models evaluated, Bio+ClinicalBERT performed particularly well, achieving an accuracy of 73.08% and an F1-score of 62.94% on the holdout data set. All the models trained in this study outperformed our previous models by 10%-23% in terms of both F1-score and accuracy.\n\n\nConclusions\nOur findings suggest that transfer learning is a viable method for tracking CEnR studies and provide evidence that the use of fine-tuning strategies significantly improves transformer-based models. Our study also presents a tool for categorizing the type and volume of community engagement in research, which may be useful in addressing the challenges associated with reporting CEnR metrics.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/41137"
    },
    {
        "id": 22819,
        "title": "Guest Editorial: Special issue on encoding-decoding-based state estimation for neural networks",
        "authors": "Lifeng Ma, Lei Zou, Xiaojian Yi, Tingwen Huang",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.01.001"
    },
    {
        "id": 22820,
        "title": "SATCount: A scale-aware transformer-based class-agnostic counting framework",
        "authors": "Yutian Wang, Bin Yang, Xi Wang, Chao Liang, Jun Chen",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106126"
    },
    {
        "id": 22821,
        "title": "Transformer fault identification method based on improved oversampling and MSFO-SVM",
        "authors": "Guomin Xie, Shilin Li",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105767"
    },
    {
        "id": 22822,
        "title": "EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification",
        "authors": "Tien-Phat Nguyen, Trong-Thang Pham, Tri Nguyen, Hieu Le, Dung Nguyen, Hau Lam, Phong Nguyen, Jennifer Fowler, Minh-Triet Tran, Ngan Le",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00202"
    },
    {
        "id": 22823,
        "title": "EEG Based Brain Computer Interface System for Decoding Covert Speech using Deep Neural Networks",
        "authors": "Meenakshi Bisla, R.S Anand",
        "published": "2023-4-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csnt57126.2023.10134633"
    },
    {
        "id": 22824,
        "title": "Momentum Strategies using Networks",
        "authors": "Wang Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4602679"
    },
    {
        "id": 22825,
        "title": "Purinergic signaling: decoding its role in COVID-19 pathogenesis and promising treatment strategies",
        "authors": "Zahra Shafaghat, Amir-Hossein Khosrozadeh Ghomi, Hossein Khorramdelazad, Elaheh Safari",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10787-023-01344-4"
    },
    {
        "id": 22826,
        "title": "Decoding silent speech from high-density surface electromyographic data using transformer",
        "authors": "Rui Song, Xu Zhang, Xi Chen, Xiang Chen, Xun Chen, Shuang Yang, Erwei Yin",
        "published": "2023-2",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.bspc.2022.104298"
    },
    {
        "id": 22827,
        "title": "Multi-modal long document classification based on Hierarchical Prompt and Multi-modal Transformer",
        "authors": "Tengfei Liu, Yongli Hu, Junbin Gao, Jiapu Wang, Yanfeng Sun, Baocai Yin",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106322"
    },
    {
        "id": 22828,
        "title": "Decoding protein binding landscape on circular RNAs with base-resolution transformer models",
        "authors": "Hehe Wu, Xiaojian Liu, Yi Fang, Yang Yang, Yan Huang, Xiaoyong Pan, Hong-Bin Shen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compbiomed.2024.108175"
    },
    {
        "id": 22829,
        "title": "Cellular Traffic Forecasting Based on Inverted Transformer for Mobile Perception Dual-Level Base Station Sleep Control",
        "authors": "Jianwei Zhang, Chunchen Tan, Zengyu Cai, Liang Zhu, Yuan Feng, Shujun Liang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.adhoc.2024.103505"
    },
    {
        "id": 22830,
        "title": "Confirmation Bias-Aware Fake News Detection with Graph Transformer Networks",
        "authors": "Kayato Soga, Soh Yoshida, Mitsuji Muneyasu",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315635"
    },
    {
        "id": 22831,
        "title": "IDS-INT: Intrusion detection system using transformer-based transfer learning for imbalanced network traffic",
        "authors": "Farhan Ullah, Shamsher Ullah, Gautam Srivastava, Jerry Chun-Wei Lin",
        "published": "2024-2",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dcan.2023.03.008"
    },
    {
        "id": 22832,
        "title": "A transformer-based deep neural network model for SSVEP classification",
        "authors": "Jianbo Chen, Yangsong Zhang, Yudong Pan, Peng Xu, Cuntai Guan",
        "published": "2023-7",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.04.045"
    },
    {
        "id": 22833,
        "title": "Object Detection of Occlusion Point Cloud based on Transformer",
        "authors": "Jing Zhou, Jian Zhou, Teng Xing Lin, Zi Xin Gong",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191240"
    },
    {
        "id": 22834,
        "title": "LCDeT: LiDAR Curb Detection Network with Transformer",
        "authors": "Jian Gao, Haoxiang Jie, Bingqing Xu, Lifeng Liu, Jun Hu, Wei Liu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191580"
    },
    {
        "id": 22835,
        "title": "Leaf disease severity classification with explainable artificial intelligence using transformer networks",
        "authors": "",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.19101/ijatee.2022.10100136"
    },
    {
        "id": 22836,
        "title": "A Multi-scale Fusion Network with Transformer for Medical Image Segmentation",
        "authors": "Guidi Lin, Lingna Chen",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105758"
    },
    {
        "id": 22837,
        "title": "Salient Object Detection Based on Transformer and Multi-scale Feature Fusion",
        "authors": "Lijun Zhao, Han Wang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105709"
    },
    {
        "id": 22838,
        "title": "A New Hybrid Transformer Topology for Distribution Networks",
        "authors": "Rupert Power, Udaya Madawala, Craig Baguley, Bingkun Song",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iecon51785.2023.10312051"
    },
    {
        "id": 22839,
        "title": "Transformer Neural Networks, Information Biology, and Alzheimer’s Disease",
        "authors": "Melanie Swan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractBackgroundAlzheimer’s disease affects one in ten people older than 65 years, with the prevalence expected to triple by 2030, together with commensurate increases in cost. A comprehensive causal model of Alzheimer’s disease etiology is lacking despite more than three decades of study. Modern computational methods provide a new approach to multiscalar biosystems by modeling neuropathology in its native three‐dimensional character in topological biophysics and quantum computational models.MethodThis work describes how an emerging standard in machine learning, responsible for the AlphaFold (protein folding structure) and Gato generalist agent projects, transformer neural networks, may be applied to the study of Alzheimer’s disease per proven results in bot text chat, image recognition and creation, video generation, software programming, and autonomy in robotics and driving. The self‐attention and self‐learning mechanisms of transformer neural networks are discussed in application to multifaceted Alzheimer’s disease data sets involving neuroimaging scans, whole human genome data, and blood and CSF biomarker data.ResultA machine learning‐based topological biophysics model of Alzheimer’s disease neuropathology hypothesis‐testing (misfolded proteins, gene expression, diabetes type‐3, and peptide immune system stimulation) is described.ConclusionThere is an opportunity to apply state‐of‐the‐art transformer neural network machine learning techniques to the automated analysis of large accruing data stores of Alzheimer’s information (imaging, genomic, proteomic, and self‐reported data) towards the causal understanding of the neuropathology.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/alz.082050"
    },
    {
        "id": 22840,
        "title": "Leveraging Transformer Neural Networks for Enhanced Sentiment Analysis on Online Platform Comments",
        "authors": "Jiayi Tang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3650215.3650260"
    },
    {
        "id": 22841,
        "title": "A novel approach for oil-based transformer fault identification in electrical secondary distribution networks",
        "authors": "Hadija Mbembati, Hussein Bakiri",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e26336"
    },
    {
        "id": 22842,
        "title": "Hierarchical Transformer-based Siamese Network for Related Trading Detection in Financial Market",
        "authors": "Le Kang, Tai-Jiang Mu, Guoping Zhao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191382"
    },
    {
        "id": 22843,
        "title": "Transformer: Image Classification Based on Constitutional Neural Networks",
        "authors": "Yangrui Cheng, Fuqiang Xie, Yongzhou Li, Guangyi Zhao",
        "published": "2023-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135505"
    },
    {
        "id": 22844,
        "title": "Gait recognition using free-area transformer networks",
        "authors": "Guannan Chen, Shimin Wei",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00138-023-01467-2"
    },
    {
        "id": 22845,
        "title": "Rethinking Real Estate Pricing with Transformer Graph Neural Networks (T-GNN)",
        "authors": "Faraz Moghimi, Reid A. Johnson, Andy Krause",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00212"
    },
    {
        "id": 22846,
        "title": "Transformer training strategies for forecasting multiple load time series",
        "authors": "Matthias Hertel, Maximilian Beichter, Benedikt Heidrich, Oliver Neumann, Benjamin Schäfer, Ralf Mikut, Veit Hagenmeyer",
        "published": "2023-10-19",
        "citations": 1,
        "abstract": "AbstractIn the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that Transformers are effective for load forecasting when they are trained with the global training strategy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s42162-023-00278-z"
    },
    {
        "id": 22847,
        "title": "Low-complexity IRS beamforming based on sphere decoding and tabu search",
        "authors": "Seraphin Kimaryo, Kyungchun Lee",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/jcn.2023.000007"
    },
    {
        "id": 22848,
        "title": "Decoding Illusion Perception: A Comparative Analysis of Deep Neural Networks in the Müller-Lyer Illusion",
        "authors": "Hongtao Zhang, Shinichi Yoshida, Zhen Li",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394255"
    },
    {
        "id": 22849,
        "title": "A Novel End-to-End Transformer for Scene Graph Generation",
        "authors": "Chengkai Ren, Xiuhua Liu, Mengyuan Cao, Jian Zhang, Hongwei Wang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191798"
    },
    {
        "id": 22850,
        "title": "ViT-ReID: A Vehicle Re-identification Method Using Visual Transformer",
        "authors": "Linshan Du, Kuilin Huang, He Yan",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105738"
    },
    {
        "id": 22851,
        "title": "HINormer: Representation Learning On Heterogeneous Information Networks with Graph Transformer",
        "authors": "Qiheng Mao, Zemin Liu, Chenghao Liu, Jianling Sun",
        "published": "2023-4-30",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543507.3583493"
    },
    {
        "id": 22852,
        "title": "Opinion Mining from Audio Conversation using Machine Learning tools and BART Transformer",
        "authors": "Ann Mathews, Paras Nath Singh",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmnwc60182.2023.10435714"
    },
    {
        "id": 22853,
        "title": "TNN-IDS: Transformer neural network-based intrusion detection system for MQTT-enabled IoT Networks",
        "authors": "Safi Ullah, Jawad Ahmad, Muazzam A. Khan, Mohammed S. Alshehri, Wadii Boulila, Anis Koubaa, Sana Ullah Jan, M Munawwar Iqbal Ch",
        "published": "2023-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2023.110072"
    },
    {
        "id": 22854,
        "title": "TRMER: Transformer-Based End to End Printed Mathematical Expression Recognition",
        "authors": "Zhaokun Zhou, Shuaijian Ji, Yuqing Wang, Zhenyu Weng, Yuesheng Zhu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191139"
    },
    {
        "id": 22855,
        "title": "Pathogen Detection in Plant Leaves using Deep Neural Networks with Vision Transformer Technique",
        "authors": "",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.24214/jecet.b.12.4.16175"
    },
    {
        "id": 22856,
        "title": "UGTransformer: Unsupervised Graph Transformer Representation Learning",
        "authors": "Lixiang Xu, Haifeng Liu, Qingzhe Cui, Bin Luo, Ning Li, Yan Chen, Yuanyan Tang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192010"
    },
    {
        "id": 22857,
        "title": "Investigating Semantic Subspaces of Transformer Sentence Embeddings through Linear Structural Probing",
        "authors": "Dmitry Nikolaev, Sebastian Padó",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.11"
    },
    {
        "id": 22858,
        "title": "Extrapolation of affective norms using transformer-based neural networks and its application to experimental stimuli selection",
        "authors": "Hubert Plisiecki, Adam Sobieszek",
        "published": "2023-9-25",
        "citations": 1,
        "abstract": "AbstractData on the emotionality of words is important for the selection of experimental stimuli and sentiment analysis on large bodies of text. While norms for valence and arousal have been thoroughly collected in English, most languages do not have access to such large datasets. Moreover, theoretical developments lead to new dimensions being proposed, the norms for which are only partially available. In this paper, we propose a transformer-based neural network architecture for semantic and emotional norms extrapolation that predicts a whole ensemble of norms at once while achieving state-of-the-art correlations with human judgements on each. We improve on the previous approaches with regards to the correlations with human judgments by Δr = 0.1 on average. We precisely discuss the limitations of norm extrapolation as a whole, with a special focus on the introduced model. Further, we propose a unique practical application of our model by proposing a method of stimuli selection which performs unsupervised control by picking words that match in their semantic content. As the proposed model can easily be applied to different languages, we provide norm extrapolations for English, Polish, Dutch, German, French, and Spanish. To aid researchers, we also provide access to the extrapolation networks through an accessible web application.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13428-023-02212-3"
    },
    {
        "id": 22859,
        "title": "Self-Attention Vision Transformer with Transfer Learning for Efficient Crops and Weeds Classification",
        "authors": "Shubham Sharma, Manu Vardhan",
        "published": "2023-3-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112049"
    },
    {
        "id": 22860,
        "title": "Optimal Scheduling of Active Distribution Networks Considering Dynamic Transformer Rating Under High Penetration of Renewable Energies",
        "authors": "Nasrin Osali",
        "published": "2023-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictem56862.2023.10083734"
    },
    {
        "id": 22861,
        "title": "Application and Analysis of Convolutional Neural Networks and Vision Transformer Models in Fruit Recognition",
        "authors": "Shouzhe Liu",
        "published": "2023-1-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca56706.2023.10076218"
    },
    {
        "id": 22862,
        "title": "Transformer networks for univariate time series prediction in predictive process control",
        "authors": "Jan Mayer, Turgut Caglar, Roland Jochem",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ice/itmc58018.2023.10332374"
    },
    {
        "id": 22863,
        "title": "Engagement Recognition in Online Learning Based on an Improved Video Vision Transformer",
        "authors": "Zijian Guo, Zhuoyi Zhou, Jiahui Pan, Yan Liang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191579"
    },
    {
        "id": 22864,
        "title": "The Great Deception: A Comprehensive Study of Execution Strategies in Corporate Share Buy-Backs - Decoding the Enigma of Equity Repurchases and their Disproportionate Cost Structures",
        "authors": "Michael Seigne, Joerg Osterrieder",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4499366"
    },
    {
        "id": 22865,
        "title": "Multi-head second-order pooling for graph transformer networks",
        "authors": "Zhe Dong, Qilong Wang, Pengfei Zhu",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.01.017"
    },
    {
        "id": 22866,
        "title": "Classification of Attention Deficit Hyperactivity Disorder Using Hybrid SWIN Transformer and Graph Convolutional Neural Networks",
        "authors": "Kailash Shaw",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304922"
    },
    {
        "id": 22867,
        "title": "B-spline curve approximation with transformer neural networks",
        "authors": "Mathis Saillot, Dominique Michel, Ahmed Zidna",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.matcom.2024.04.010"
    },
    {
        "id": 22868,
        "title": "Decoding Acute Lymphoblastic Leukemia: Insights from Convolutional Neural Networks and Pretrained Model",
        "authors": "Jason Hendrawan, Jonathan Adrian, Verrel Juanto Lukmana, Felix Indra Kurniadi",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoris60118.2023.10352301"
    },
    {
        "id": 22869,
        "title": "Flexible and sustainable scheduling of electric power grids: A dynamic line and transformer rating based approach under uncertainty condition",
        "authors": "M. Akhlaghi, Z. Moravej, A. Bagheri",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101150"
    },
    {
        "id": 22870,
        "title": "Empirical Analysis of Training Strategies of Transformer-Based Japanese Chit-Chat Systems",
        "authors": "Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro",
        "published": "2023-1-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022973"
    },
    {
        "id": 22871,
        "title": "Control Strategies for Large Power Transformer HEMP/GMD Protection",
        "authors": "Timothy J. Donnelly, David G. Wilson, Rush D. Robinett, Wayne W. Weaver",
        "published": "2023-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/naps58826.2023.10318683"
    },
    {
        "id": 22872,
        "title": "AttentionMGT-DTA: A multi-modal drug-target affinity prediction using graph transformer and attention mechanism",
        "authors": "Hongjie Wu, Junkai Liu, Tengsheng Jiang, Quan Zou, Shujie Qi, Zhiming Cui, Prayag Tiwari, Yijie Ding",
        "published": "2024-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.018"
    },
    {
        "id": 22873,
        "title": "Decoding emotion with phase–amplitude fusion features of EEG functional connectivity network",
        "authors": "Liangliang Hu, Congming Tan, Jiayang Xu, Rui Qiao, Yilin Hu, Yin Tian",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106148"
    },
    {
        "id": 22874,
        "title": "Decoding Hydrogen Embrittlement in High Strength Coiled Tubing: Insights from Acid-Induced Failures, Field Data Analysis, and Corrosion Management Strategies",
        "authors": "G. McClelland, I. I. Galvan, G. L. Mallanao, B. Watson",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Abstract\nRecent reports have highlighted hydrogen embrittlement (HE) of high strength, quench-and-temper (Q&T) coiled tubing (CT) resulting from hydrochloric (HCl) acid usage in sour environments. HCl acid treatments expose CT surfaces to aggressive corrosion, often exacerbated by H2S from formation fluids or as a chemical reaction. Helping the CT industry recognize the morphologies of damage when the tube is retired and re-evaluating the CT grade selection and chemicals are vital for averting costly and dangerous CT failures.\nTo establish a comprehensive case history preceding the CT failure mode, pertinent field data must be collected and correlated, encompassing job frequency, acid and H2S exposure duration, concentration levels, downhole conditions, and inhibition procedures. Metallurgical analysis, including an exhaustive battery of tests, was conducted on the specimens: visual assessment, dimensional verification, fractography, metallographic analysis, mechanical integrity evaluation (comprising hardness and tensile testing), scanning electron microscopy (SEM), and energy-dispersive X-ray spectroscopy (EDS), along with sodium azide spot testing.\nA summary of field failures was evaluated from diverse operational environments and locations. Multiple factors contributed to premature CT retirement, particularly inadequate corrosion inhibition and sulfide scavenger programs. However, environmental conditions, operational stresses, microstructural differences, and susceptibilities of various high-grade materials (Q&T and conventional) were correlated and compared with industry research. Low pH fluids like hydrochloric acid or other acidic substances combined with H2S presence created a susceptibility for the high-grade CT materials consistent with other high strength oil and gas carbon steel materials. Material properties, specifically tensile strength and hardness showed a distinct susceptibility to HE with increasing tensile strength. Steels with tensile strengths below 140-ksi are relatively less vulnerable to HE, but susceptibility significantly escalates beyond this threshold. Typically, low cycle fatigue promoted complete through-wall crack propagation, with some cases demonstrating fatigue originating from the steel centerline, where hydrogen ions from acid tend to migrate and recombine as gas. Other initiation points include the OD/ID surfaces and the longitudinal weld. These initiation points demonstrated consistent hydrogen embrittlement intergranular failure mechanisms.\nRecent materials research in the Oil and Gas space related to HE and H2S exposure on materials similar to coiled tubing was evaluated for relevance. Two interesting areas of research are presented: fracture propagation theories with hydrogen presence related to fatigue environments, and the influence of various iron sulfide films resulting from the corrosion reaction of H2S and steel.\nSour immersion testing results on high strength coiled tubing are also presented to demonstrate the effectiveness of commercially available inhibitors compared to no inhibition, with good results on Q&T coiled tubing.\nThis study emphasizes the vital need to evaluate well conditions and working fluids compatibility (including inhibition) with CT materials to prolong CT operational life. Additionally, this study details the morphology of H2S-induced CT failures in acid stimulations, whether due to HE, Sulfide Stress Cracking (SSC), or Stress Corrosion Cracking (SCC), giving insight to future job planning. Prioritizing prevention planning with robust corrosion management is crucial for prolonging overall service life and minimizing operational disruptions in acidic environments using high strength Q&T CT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/218327-ms"
    },
    {
        "id": 22875,
        "title": "PT3: A Transformer-based Model for Sepsis Death Risk Prediction via Vital Signs Time Series",
        "authors": "Ruihong Luo, Minghui Gong, Chunping Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191261"
    },
    {
        "id": 22876,
        "title": "Prediction Based on Convolutional Neural Networks and Vision Transformer for GOES-XRS Solar Flare Time Series",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.060819"
    },
    {
        "id": 22877,
        "title": "A transformer-based model for event recognition and characterization in passive optical networks",
        "authors": "K. Abdelli, C. Tropschug, H. Griesser, S. Pachnicke",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/icp.2023.2379"
    },
    {
        "id": 22878,
        "title": "TPG-rayGAN: CT reconstruction based on transformer and generative adversarial networks",
        "authors": "Yufeng Wang, Qing Xia",
        "published": "2023-1-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2655901"
    },
    {
        "id": 22879,
        "title": "Modelling and Testing Transformer Differential Protection in Inverter-Based Generation Networks",
        "authors": "Krzysztof Solak, Waldemar Rebizant, Frank Mieske, Sebastian Schneider",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/epe58302.2023.10149224"
    },
    {
        "id": 22880,
        "title": "An Approch for Representation of Node Using Graph Transformer Networks",
        "authors": "Dev Vaghani",
        "published": "2023-1-31",
        "citations": 0,
        "abstract": "Abstract: In representation learning on graphs, graph neural networks (GNNs) have been widely employed and have attained cutting-edge performance in tasks like node categorization and link prediction. However, the majority of GNNs now in use are made to learn node representations on homogenous and fixed graphs. The limits are particularly significant when learning representations on a network that has been incorrectly described or one that is heterogeneous, or made up of different kinds of nodes and edges. This study proposes Graph Transformer Networks (GTNs), which may generate new network structures by finding valuable connections between disconnected nodes in the original graph and learning efficient node representation on the new graphs end-to-end. A basic layer of GTNs called the Graph Transformer layer learns a soft selection of edge types and composite relations to produce meaningful multi-hop connections known as meta-paths. This research demonstrates that GTNs can learn new graph structures from data and tasks without any prior domain expertise and that they can then use convolution on the new graphs to provide effective node representation. GTNs outperformed state-of-the-art approaches that need predefined meta-paths from domain knowledge in all three benchmark node classification tasks without the use of domain-specific graph pre-processing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22214/ijraset.2023.48485"
    },
    {
        "id": 22881,
        "title": "Decoding Therapeutic Applications of Quercetin: Recent Advancements in\nNanotechnological Strategies",
        "authors": "Sonam Grewal, Neha Tiwary, Gaurav Malik, Madhukar Garg, Geeta Deswal, Bhawna Chopra, Ajmer Singh Grewal, Ashwani K. Dhingra, Kumar Guarve",
        "published": "2024-1-17",
        "citations": 0,
        "abstract": "\nAbstract:\nFor centuries, people have used herbal medicine to treat a diversity of health complications\nand as a natural substance, they have a favourable effect on our health. Herbal ingredients can be utilized\nas lead molecules in the innovation and development of a new drug. Flavonoids are a class of\nchemical compounds with diverse phenolic structures, and they are found in a wide variety of foods,\nincluding fruits, vegetables, cereals, bark, roots, stems, flowers, tea, and wine. Quercetin is the most\nprevalent polyphenolic bioflavonoid or flavonoid. Quercetin is found in many food products and has\ndemonstrated a wide range of pharmacological activities, including the treatment of allergies, ocular\ndiseases, metabolic ailments, inflammatory illnesses, cardiovascular ailments and arthritis. Quercetin\nhas attracted interest as an emerging pharmacophore with the potential to significantly advance research\nand the development of novel therapeutic medicines for a variety of diseases. Despite having a\nhuge therapeutic potential, these flavonoids have unfavourable pharmacokinetic characteristics, low\nbioavailability, and poor solubility, limiting their application in therapeutics. The objective of the current\nstudy is to present a new update on the major therapeutic uses of quercetin and other types of\nnanocarriers that contain quercetin to treat various ailments.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2174/0122117385271262231114075737"
    },
    {
        "id": 22882,
        "title": "Image Alone Are Not Enough: A General Semantic-Augmented Transformer-Based Framework for Image Captioning",
        "authors": "Jiawei Liu, Xin Lin, Liang He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191656"
    },
    {
        "id": 22883,
        "title": "GenTAL: Generative Denoising Skip-gram Transformer for Unsupervised Binary Code Similarity Detection",
        "authors": "Li Tao Li, Steven H. H. Ding, Philippe Charland",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191550"
    },
    {
        "id": 22884,
        "title": "Tomato Disease Detection Using Vision Transformer with Residual L1-Norm Attention and Deep Neural Networks",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.57"
    },
    {
        "id": 22885,
        "title": "Transformer networks of human conceptual knowledge.",
        "authors": "Sudeep Bhatia, Russell Richie",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1037/rev0000319"
    },
    {
        "id": 22886,
        "title": "Segformer: Segment-Based Transformer with Decomposition for Long-Term Series Forecasting",
        "authors": "Jinhua Chen, Jin Fan, Zhen Liu, Jiaqian Xiang, Jia Wu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191412"
    },
    {
        "id": 22887,
        "title": "Chat Generative Pretrained Transformer (ChatGPT) for Data Analysis",
        "authors": "Niranjanamurthy M, Kantharaju V, Sirisha Satish, Nidhi Umashankar, Raviram V",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmnwc60182.2023.10435984"
    },
    {
        "id": 22888,
        "title": "LoGo Transformer: Hierarchy Lightweight Full Self-Attention Network for Corneal Endothelial Cell Segmentation",
        "authors": "Yinglin Zhang, Zichao Cai, Risa Higashita, Jiang Liu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191116"
    },
    {
        "id": 22889,
        "title": "Overview of various methods for decoding and constructing critical sets of polar codes",
        "authors": "Ilya Timokhin, Fedor Ivanov",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/jcn.2023.000049"
    },
    {
        "id": 22890,
        "title": "2S-DFN: Dual-semantic Decoding Fusion Networks for Fine-grained Image Recognition",
        "authors": "Pufen Zhang, Peng Shi, Song Zhang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00012"
    },
    {
        "id": 22891,
        "title": "Noise Recycling using GRAND for Improving the Decoding Performance",
        "authors": "Arslan Riaz, Amit Solomon, Furkan Ercan, Muriel Medard, Rabia Tugce Yazicigil, Ken R. Duffy",
        "published": "2023-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/comsnets56262.2023.10041405"
    },
    {
        "id": 22892,
        "title": "Data-Driven Strategies for Optimal Performance and Maintenance: Using Machine Learning for Improved Power Transformer Management",
        "authors": "M. Annas Albab Fauzi, I. Vanany, Herry Nugraha",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpee60001.2023.10453723"
    },
    {
        "id": 22893,
        "title": "Editorial: Decoding novel therapeutic targets, biomarkers, and drug development strategies against neurodegenerative disorders—A multi-omics approach",
        "authors": "E. Srinivasan, Siranjeevi Nagaraj, Sakthi Rajendran, R. Rajasekaran",
        "published": "2023-6-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fncom.2023.1227850"
    },
    {
        "id": 22894,
        "title": "GBT: Two-stage transformer framework for non-stationary time series forecasting",
        "authors": "Li Shen, Yuning Wei, Yangzhu Wang",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.044"
    },
    {
        "id": 22895,
        "title": "Decoding cancer insights: recent progress and strategies in proteomics for biomarker discovery",
        "authors": "Bangaru Naidu Thaddi, Vasu Babu Dabbada, Bhavani Ambati, Eswar Kumar Kilari",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42485-023-00121-9"
    },
    {
        "id": 22896,
        "title": "Transformer-based multi-attention hybrid networks for skin lesion segmentation",
        "authors": "Zhiwei Dong, Jinjiang Li, Zhen Hua",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.123016"
    },
    {
        "id": 22897,
        "title": "CrimeNet: Neural Structured Learning using Vision Transformer for violence detection",
        "authors": "Fernando J. Rendón-Segador, Juan A. Álvarez-García, Jose L. Salazar-González, Tatiana Tommasi",
        "published": "2023-4",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.048"
    },
    {
        "id": 22898,
        "title": "A bio-inspired positional embedding network for transformer-based models",
        "authors": "Xue-song Tang, Kuangrong Hao, Hui Wei",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.015"
    },
    {
        "id": 22899,
        "title": "Federated learning using game strategies: State-of-the-art and future trends",
        "authors": "Rajni Gupta, Juhi Gupta",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2023.109650"
    },
    {
        "id": 22900,
        "title": "Feasibility Study of Medical Image Segmentation Algorithm Based on Two-Branch Context-Aware Transformer Networks",
        "authors": "Jikuan Xu, Jiamin Zhang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsins60115.2023.10455629"
    },
    {
        "id": 22901,
        "title": "EVALUATION CRITERIA OF INSTALLED TRANSFORMER POWER AT DISTRIBUTION SUBSTATIONS OF ELECTRICAL NETWORKS",
        "authors": "O. Yandulskyi, V. Mossakovskyi",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "This work is devoted to approaches to determining and substantiating the optimal power value of transformers that installed at distribution substations. During the selection of the transformer under working the electrical load schedule of consumers of distribution substation in the power grid, it is necessary to ensure in full using of every unit of installed transformer capacity. For example, there are three transformers that belongs to the 10 kV, 35 kV and 110 kV voltage levels. It is shown the calculation of the optimal value of the transformer power and the selection of this power is performed, based on the nomenclature of transformers. It was carried out a technical and economic comparison of possible combinations in number and capacity of transformers to set the optimal power level.\r\nActive energy losses, profit from the transporting of active energy and the total discounted costs function are the estimation of the installed transformer capacity. The transformers chosen for installation at the substation of the power grid are estimated for the ability to work at the highest ambient temperature level of +40°С, based on the temperature of the hot-spot point on the surface of the insulation of the windings.\r\nThis work ends by conclusion in comparison of the results of two approaches to estimating the lifetime and residual operation life - when the hot-spot point is in stationary and when it moves during electric load changes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.20535/1813-5420.3.2022.270246"
    },
    {
        "id": 22902,
        "title": "Sea Ice Segmentation from SAR Data by Convolutional Transformer Networks",
        "authors": "Nicolae-Cătălin Ristea, Andrei Anghel, Mihai Datcu",
        "published": "2023-7-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10283427"
    },
    {
        "id": 22903,
        "title": "Why Bother with Geometry? On the Relevance of Linear Decompositions of Transformer Embeddings",
        "authors": "Timothee Mickus, Raúl Vázquez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.10"
    },
    {
        "id": 22904,
        "title": "ATCNet: A Novel Approach for Predicting Highway Visibility Using Attention-Enhanced Transformer–Capsule Networks",
        "authors": "Wen Li, Xuekun Yang, Guowu Yuan, Dan Xu",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "Meteorological disasters on highways can significantly reduce road traffic efficiency. Low visibility caused by dense fog is a severe meteorological disaster that greatly increases the incidence of traffic accidents on highways. Accurately predicting highway visibility and taking timely countermeasures can mitigate the impact of meteorological disasters and enhance traffic safety. This paper introduces the ATCNet model for highway visibility prediction. In ATCNet, we integrate Transformer, Capsule Networks (CapsNet), and self-attention mechanisms to leverage their respective complementary strengths. The Transformer component effectively captures the temporal characteristics of the data, while the Capsule Network efficiently decodes the spatial correlations and hierarchical structures among multidimensional meteorological elements. The self-attention mechanism, serving as the final decision-refining step, ensures that all key temporal and spatial hierarchical information is fully considered, significantly enhancing the accuracy and reliability of the predictions. This integrated approach is crucial in understanding highway visibility prediction tasks influenced by temporal variations and spatial complexities. Additionally, this study provides a self-collected publicly available dataset, WD13VIS, for meteorological research related to highway traffic in high-altitude mountain areas. This study evaluates the model’s performance in terms of Mean Squared Error (MSE) and Mean Absolute Error (MAE). Experimental results show that our ATCNet reduces the MSE and MAE by 1.21% and 3.7% on the WD13VIS dataset compared to the latest time series prediction model architecture. On the comparative dataset WDVigoVis, our ATCNet reduces the MSE and MAE by 2.05% and 5.4%, respectively. Our model’s predictions are accurate and effective, and our model shows significant progress compared to competing models, demonstrating strong universality. This model has been integrated into practical systems and has achieved positive results.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13050920"
    }
]