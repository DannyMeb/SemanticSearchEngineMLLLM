[
    {
        "id": 4705,
        "title": "Adaptive selection of local and non-local attention mechanisms for speech enhancement",
        "authors": "Xinmeng Xu, Weiping Tu, Yuhong Yang",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106236"
    },
    {
        "id": 4706,
        "title": "Integrating Multiple Visual Attention Mechanisms in Deep Neural Networks",
        "authors": "Fernando Martinez, Yijun Zhao",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00180"
    },
    {
        "id": 4707,
        "title": "Novel Hybrid Model Coupling WOA with BiLSTM Neural Networks and Attention Mechanisms",
        "authors": "Liuyu Wang, Huatai Pan, Zhengxian Cai",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10258008"
    },
    {
        "id": 4708,
        "title": "A study on the application of graph neural network in code clone detection",
        "authors": "Linfeng Dai",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3605801.3605834"
    },
    {
        "id": 4709,
        "title": "Dual-domain strip attention for image restoration",
        "authors": "Yuning Cui, Alois Knoll",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.003"
    },
    {
        "id": 4710,
        "title": "Co-attention enabled content-based image retrieval",
        "authors": "Zechao Hu, Adrian G. Bors",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.04.009"
    },
    {
        "id": 4711,
        "title": "Multimodal Biometric Recognition Neural Networks with Branch Attention Mechanisms",
        "authors": " LiuJun, Chen Mingjin,  GuoYong, Huang Zengxi",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/arace60380.2023.00021"
    },
    {
        "id": 4712,
        "title": "Combining external-latent attention for medical image segmentation",
        "authors": "Enmin Song, Bangcheng Zhan, Hong Liu",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.046"
    },
    {
        "id": 4713,
        "title": "Combining Multi-Head Attention and Sparse Multi-Head Attention Networks for Session-Based Recommendation",
        "authors": "Zhiwei Zhao, Xiaoye Wang, Yingyuan Xiao",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191924"
    },
    {
        "id": 4714,
        "title": "Meta attention for Off-Policy Actor-Critic",
        "authors": "Jiateng Huang, Wanrong Huang, Long Lan, Dan Wu",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.024"
    },
    {
        "id": 4715,
        "title": "Classification of DBS microelectrode recordings using a residual neural network with attention in the temporal domain",
        "authors": "K.A. Ciecierski, T. Mandat",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.021"
    },
    {
        "id": 4716,
        "title": "Multi-behavior Attention Mechanisms Graph Neural Networks based on Session Recommendation",
        "authors": "Xing Xing, Xuanming Zhang, Jianfu Cui, Jiale Chen, Zhichun Jia",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327142"
    },
    {
        "id": 4717,
        "title": "Meta-structure-based graph attention networks",
        "authors": "Jin Li, Qingyu Sun, Feng Zhang, Beining Yang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.025"
    },
    {
        "id": 4718,
        "title": "Spatial multi-attention conditional neural processes",
        "authors": "Li-Li Bao, Jiang-She Zhang, Chun-Xia Zhang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106201"
    },
    {
        "id": 4719,
        "title": "Knowledge-Preserving continual person re-identification using Graph Attention Network",
        "authors": "Zhaoshuo Liu, Chaolu Feng, Shuaizheng Chen, Jun Hu",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.033"
    },
    {
        "id": 4720,
        "title": "Spatial oblivion channel attention targeting intra-class diversity feature learning",
        "authors": "Honggui Han, Qiyu Zhang, Fangyu Li, Yongping Du",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.032"
    },
    {
        "id": 4721,
        "title": "Towards performance-maximizing neural network pruning via global channel attention",
        "authors": "Yingchun Wang, Song Guo, Jingcai Guo, Jie Zhang, Weizhan Zhang, Caixia Yan, Yuanhong Zhang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.065"
    },
    {
        "id": 4722,
        "title": "QCA-Net: Quantum-based Channel Attention for Deep Neural Networks",
        "authors": "Juntao Zhang, Peng Cheng, Zehan Li, Hao Wu, Wenbo An, Jun Zhou",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191662"
    },
    {
        "id": 4723,
        "title": "English-Afaan Oromo Machine Translation Using Deep Attention Neural Network",
        "authors": " Ebisa A. Gemechu, G. R. Kanagachidambaresan",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23030049"
    },
    {
        "id": 4724,
        "title": "Fourier feature decorrelation based sample attention for dense crowd localization",
        "authors": "Chao Wen, Hongqiang He, Yuhua Qian, Yu Xie, Wenjian Wang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106131"
    },
    {
        "id": 4725,
        "title": "STMMOT: Advancing multi-object tracking through spatiotemporal memory networks and multi-scale attention pyramids",
        "authors": "Hamza Mukhtar, Muhammad Usman Ghani Khan",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.09.047"
    },
    {
        "id": 4726,
        "title": "Self-attention learning network for face super-resolution",
        "authors": "Kangli Zeng, Zhongyuan Wang, Tao Lu, Jianyu Chen, Jiaming Wang, Zixiang Xiong",
        "published": "2023-3",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.006"
    },
    {
        "id": 4727,
        "title": "Few-shot link prediction for temporal knowledge graphs based on time-aware translation and attention mechanism",
        "authors": "Han Zhang, Luyi Bai",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.043"
    },
    {
        "id": 4728,
        "title": "RAFNet: Restricted attention fusion network for sleep apnea detection",
        "authors": "Ying Chen, Huijun Yue, Ruifeng Zou, Wenbin Lei, Wenjun Ma, Xiaomao Fan",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.019"
    },
    {
        "id": 4729,
        "title": "Transfer Learning with Graph Attention Networks for Team Recommendation",
        "authors": "Sagar Kaw, Ziad Kobti, Kalyani Selvarajah",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191717"
    },
    {
        "id": 4730,
        "title": "Multi-scale multi-reception attention network for bone age assessment in X-ray images",
        "authors": "Zhichao Yang, Cong Cong, Maurice Pagnucco, Yang Song",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.002"
    },
    {
        "id": 4731,
        "title": "Multi-Modal Co-Attention Capsule Network for Fake News Detection",
        "authors": " Chunyan Yin,  Yongheng Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x24010041"
    },
    {
        "id": 4732,
        "title": "Cell Attention Networks",
        "authors": "Lorenzo Giusti, Claudio Battiloro, Lucia Testa, Paolo Di Lorenzo, Stefania Sardellitti, Sergio Barbarossa",
        "published": "2023-6-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191530"
    },
    {
        "id": 4733,
        "title": "Graph-based social relation inference with multi-level conditional attention",
        "authors": "Xiaotian Yu, Hanling Yi, Qie Tang, Kun Huang, Wenze Hu, Shiliang Zhang, Xiaoyu Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106216"
    },
    {
        "id": 4734,
        "title": "COM: Contrastive Masked-attention model for incomplete multimodal learning",
        "authors": "Shuwei Qian, Chongjun Wang",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.003"
    },
    {
        "id": 4735,
        "title": "Residual Hybrid Attention Network for Single Image Dehazing",
        "authors": "Hui Ming Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191742"
    },
    {
        "id": 4736,
        "title": "Audioset classification with Graph Convolutional Attention model",
        "authors": "Xuliang Li, Junbin Gao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191960"
    },
    {
        "id": 4737,
        "title": "Unsupervised Bidirectional Contrastive Reconstruction and Adaptive Fine-Grained Channel Attention Networks for Image Dehazing",
        "authors": "Hang Sun, Yang Wen, Huijing Feng, Yuelin Zheng, Qi Mei, Dong Ren, Mei Yu",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106314"
    },
    {
        "id": 4738,
        "title": "Deep neural networks with attention mechanisms for Spodoptera frugiperda pupae sexing",
        "authors": "João Vitor de Andrade Porto, Fabio Prestes Cesar Rezende, Higor Henrique Picoli Nucci, Antonia Railda Roel, Gilberto Astolfi, Hemerson Pistori",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.atech.2023.100200"
    },
    {
        "id": 4739,
        "title": "Contextually enhanced ES-dRNN with dynamic attention for short-term load forecasting",
        "authors": "Slawek Smyl, Grzegorz Dudek, Paweł Pełka",
        "published": "2024-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.017"
    },
    {
        "id": 4740,
        "title": "Priors-assisted dehazing network with attention supervision and detail preservation",
        "authors": "Weichao Yi, Liquan Dong, Ming Liu, Mei Hui, Lingqin Kong, Yuejin Zhao",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106165"
    },
    {
        "id": 4741,
        "title": "Hierarchical Attention Master–Slave for heterogeneous multi-agent reinforcement learning",
        "authors": "Jiao Wang, Mingrui Yuan, Yun Li, Zihui Zhao",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.02.037"
    },
    {
        "id": 4742,
        "title": "SLAPP: Subgraph-level attention-based performance prediction for deep learning models",
        "authors": "Zhenyi Wang, Pengfei Yang, Linwei Hu, Bowen Zhang, Chengmin Lin, Wenkai Lv, Quan Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.043"
    },
    {
        "id": 4743,
        "title": "Characterizing functional brain networks via Spatio-Temporal Attention 4D Convolutional Neural Networks (STA-4DCNNs)",
        "authors": "Xi Jiang, Jiadong Yan, Yu Zhao, Mingxin Jiang, Yuzhong Chen, Jingchao Zhou, Zhenxiang Xiao, Zifan Wang, Rong Zhang, Benjamin Becker, Dajiang Zhu, Keith M. Kendrick, Tianming Liu",
        "published": "2023-1",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.004"
    },
    {
        "id": 4744,
        "title": "Convolutional Neural Network Modulation Recognition by Incorporating Attention Mechanisms",
        "authors": "Mingze Zuo, Yifan Liu",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc59986.2023.10421616"
    },
    {
        "id": 4745,
        "title": "EMAT: Efficient feature fusion network for visual tracking via optimized multi-head attention",
        "authors": "Jun Wang, Changwang Lai, Yuanyun Wang, Wenshuang Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106110"
    },
    {
        "id": 4746,
        "title": "ARPruning: An automatic channel pruning based on attention map ranking",
        "authors": "Tongtong Yuan, Zulin Li, Bo Liu, Yinan Tang, Yujia Liu",
        "published": "2024-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106220"
    },
    {
        "id": 4747,
        "title": "An end-to-end power knowledge extraction method based on convolutional neural networks and multi-headed attention mechanisms",
        "authors": "Bin Zhang, Yangjun Zhou, Liwen Qin, Shan Li",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005282"
    },
    {
        "id": 4748,
        "title": "Rectified Attention Gate Unit in Recurrent Neural Networks for Effective Attention Computation",
        "authors": "Manh-Hung Ha, Oscal Tzyh-Chiang Chen",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssp53291.2023.10207931"
    },
    {
        "id": 4749,
        "title": "Stacked attention hourglass network based robust facial landmark detection",
        "authors": "Ying Huang, He Huang",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.10.021"
    },
    {
        "id": 4750,
        "title": "Relaxed Attention for Transformer Models",
        "authors": "Timo Lohrenz, Björn Möller, Zhengyang Li, Tim Fingscheidt",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191643"
    },
    {
        "id": 4751,
        "title": "Double Graph Attention Networks for Visual Semantic Navigation",
        "authors": "Yunlian Lyu, Mohammad Sadegh Talebi",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11190-8"
    },
    {
        "id": 4752,
        "title": "Multihead Attention-based Audio Image Generation with Cross-Modal Shared Weight Classifier",
        "authors": "Yiming Xu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191393"
    },
    {
        "id": 4753,
        "title": "Adversarial Robustness Analysis of Attention Modules in Deep Neural Networks",
        "authors": "Guo Xingyi, Tang Chenhuan, Chen Jie, Jia Shijie",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.57237/j.se.2023.01.005"
    },
    {
        "id": 4754,
        "title": "Attention-Based Deep Spiking Neural Networks for Temporal Credit Assignment Problems",
        "authors": "Lang Qin, Ziming Wang, Rui Yan, Huajin Tang",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3240176"
    },
    {
        "id": 4755,
        "title": "Variational Disentangled Attention and Regularization for Visual Dialog",
        "authors": "Jen-Tzung Chien, Hsiu-Wei Tien",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191548"
    },
    {
        "id": 4756,
        "title": "Emotional Attention for Trained Convolutional Neural Networks",
        "authors": "Stefan Tsokov, Milena Lazarova, Adelina Aleksieva-Petrova",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/comsci59259.2023.10315883"
    },
    {
        "id": 4757,
        "title": "Best Fit Activation Functions for Attention Mechanism: Comparison and Enhancement",
        "authors": "Maan Alhazmi, Abdulrahman Altahhan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191885"
    },
    {
        "id": 4758,
        "title": "Application of Physics-Informed Attention-based Neural Networks for Anomaly Detection",
        "authors": "Mohammad F. Zahid",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-0783"
    },
    {
        "id": 4759,
        "title": "SAR-UNet: Small Attention Residual UNet for Explainable Nowcasting Tasks",
        "authors": "Mathieu Renault, Siamak Mehrkanoon",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191095"
    },
    {
        "id": 4760,
        "title": "Attention-enabled gated spiking neural P model for aspect-level sentiment classification",
        "authors": "Yanping Huang, Hong Peng, Qian Liu, Qian Yang, Jun Wang, David Orellana-Martín, Mario J. Pérez-Jiménez",
        "published": "2023-1",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.006"
    },
    {
        "id": 4761,
        "title": "Hierarchical attention-guided multiscale aggregation network for infrared small target detection",
        "authors": "Shunshun Zhong, Haibo Zhou, Zhongxu Zheng, Zhu Ma, Fan Zhang, Ji'an Duan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.036"
    },
    {
        "id": 4762,
        "title": "Attention-Based Real Image Restoration",
        "authors": "Saeed Anwar, Nick Barnes, Lars Petersson",
        "published": "2024",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3131739"
    },
    {
        "id": 4763,
        "title": "Spatial reconstructed local attention Res2Net with F0 subband for fake speech detection",
        "authors": "Cunhang Fan, Jun Xue, Jianhua Tao, Jiangyan Yi, Chenglong Wang, Chengshi Zheng, Zhao Lv",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106320"
    },
    {
        "id": 4764,
        "title": "AttentionMGT-DTA: A multi-modal drug-target affinity prediction using graph transformer and attention mechanism",
        "authors": "Hongjie Wu, Junkai Liu, Tengsheng Jiang, Quan Zou, Shujie Qi, Zhiming Cui, Prayag Tiwari, Yijie Ding",
        "published": "2024-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.018"
    },
    {
        "id": 4765,
        "title": "TCN-Attention-BIGRU: Building energy modelling based on attention mechanisms and temporal convolutional networks",
        "authors": "Yi Deng, Zhanpeng Yue, Ziyi Wu, Yitong Li, Yifei Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "<abstract>\n\n<p>Accurate and effective building energy consumption prediction is an important basis for carrying out energy-saving evaluation and the main basis for building energy-saving optimization design. However, due to the influence of environmental and human factors, energy consumption prediction is often inaccurate. Therefore, this paper presents a building energy consumption prediction model based on an attention mechanism, time convolutional neural (TCN) network fusion, and a bidirectional gated cycle unit (BIGRU). First, t-distributed stochastic neighbor embedding (T-SNE) was used to preprocess the data and extract the key features, and then a BIGRU was employed to acquire past and future data while capturing immediate connections. Then, to catch the long-term dependence, the dataset was partitioned into the TCN network, and the extended sequence was transformed into several short sequences. Consequently, the gradient explosion or vanishing problem is mitigated when the BIGRU handles lengthy sequences while reducing the spatial complexity. Second, the self-attention mechanism was introduced to enhance the model's capability to address data periodicity. The proposed model is superior to the other four models in accuracy, with an mean absolute error of 0.023, an mean-square error of 0.029, and an coefficient of determination of 0.979. Experimental results indicate that T-SNE can significantly improve the model performance, and the accuracy of predictions can be improved by the attention mechanism and the TCN network.</p>\n\n\t      </abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/era.2024098"
    },
    {
        "id": 4766,
        "title": "Masked Kinematic Continuity-aware Hierarchical Attention Network for pose estimation in videos",
        "authors": "Kyung-Min Jin, Gun-Hee Lee, Woo-Jeoung Nam, Tae-Kyung Kang, Hyun-Woo Kim, Seong-Whan Lee",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.038"
    },
    {
        "id": 4767,
        "title": "SSA-ICL: Multi-domain adaptive attention with intra-dataset continual learning for Facial expression recognition",
        "authors": "Hongxiang Gao, Min Wu, Zhenghua Chen, Yuwen Li, Xingyao Wang, Shan An, Jianqing Li, Chengyu Liu",
        "published": "2023-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.025"
    },
    {
        "id": 4768,
        "title": "Attention Based Graph Neural Networks",
        "authors": "MD Ibrahim Khalil, Muhammad Affan Abbas",
        "published": "2023-6-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/seai59139.2023.10217656"
    },
    {
        "id": 4769,
        "title": "Multi-Scale Non-Local Sparse Attention for Single Image Super-Resolution",
        "authors": "Xianwei Xiao, Baojiang Zhong",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191338"
    },
    {
        "id": 4770,
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "authors": "Yao Zhang, Yunpu Ma, Thomas Seidl, Volker Tresp",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191567"
    },
    {
        "id": 4771,
        "title": "HMM-GDAN: Hybrid multi-view and multi-scale graph duplex-attention networks for drug response prediction in cancer",
        "authors": "Youfa Liu, Shufan Tong, Yongyong Chen",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.036"
    },
    {
        "id": 4772,
        "title": "Path reliability-based graph attention networks",
        "authors": "Yayang Li, Shuqing Liang, Yuncheng Jiang",
        "published": "2023-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.021"
    },
    {
        "id": 4773,
        "title": "Frequency Principle of Attention-Based Convolutional Neural Networks for Single Image Super-Resolution",
        "authors": "Xian Zhang, Jian-Nan Su",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449125"
    },
    {
        "id": 4774,
        "title": "Transferable graph neural networks with deep alignment attention",
        "authors": "Ying Xie, Rongbin Xu, Yun Yang",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119232"
    },
    {
        "id": 4775,
        "title": "Stacked Attention-based Networks for Accurate and Interpretable Health Risk Prediction",
        "authors": "Yuxi Liu, Zhenhao Zhang, Campbell Thompson, Richard Leibbrandt, Shaowen Qin, Antonio Jimeno Yepes",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191099"
    },
    {
        "id": 4776,
        "title": "Predicting the Cascading Failure Propagation Path in Complex Networks Based On Attention-LSTM Neural Networks",
        "authors": "Donghong Li, Qin Wang, Xi Zhang, Xiujuan Fan",
        "published": "2023-5-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscas46773.2023.10181599"
    },
    {
        "id": 4777,
        "title": "Adversarial Attacks with Defense Mechanisms on Convolutional Neural Networks and Recurrent Neural Networks for Malware Classification",
        "authors": "Sharoug Alzaidy, Hamad Binsalleeh",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "In the field of behavioral detection, deep learning has been extensively utilized. For example, deep learning models have been utilized to detect and classify malware. Deep learning, however, has vulnerabilities that can be exploited with crafted inputs, resulting in malicious files being misclassified. Cyber-Physical Systems (CPS) may be compromised by malicious files, which can have catastrophic consequences. This paper presents a method for classifying Windows portable executables (PEs) using Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). To generate malware executable adversarial examples of PE, we conduct two white-box attacks, Jacobian-based Saliency Map Attack (JSMA) and Carlini and Wagner attack (C&W). An adversarial payload was injected into the DOS header, and a section was added to the file to preserve the PE functionality. The attacks successfully evaded the CNN model with a 91% evasion rate, whereas the RNN model evaded attacks at an 84.6% rate. Two defense mechanisms based on distillation and training techniques are examined in this study for overcoming adversarial example challenges. Distillation and training against JSMA resulted in the highest reductions in the evasion rates of 48.1% and 41.49%, respectively. Distillation and training against C&W resulted in the highest decrease in evasion rates, at 48.1% and 49.9%, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14041673"
    },
    {
        "id": 4778,
        "title": "LAC-GAN: Lesion attention conditional GAN for Ultra-widefield image synthesis",
        "authors": "Haijun Lei, Zhihui Tian, Hai Xie, Benjian Zhao, Xianlu Zeng, Jiuwen Cao, Weixin Liu, Jiantao Wang, Guoming Zhang, Shuqiang Wang, Baiying Lei",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.005"
    },
    {
        "id": 4779,
        "title": "Attention-based investigation and solution to the trade-off issue of adversarial training",
        "authors": "Changbin Shao, Wenbin Li, Jing Huo, Zhenhua Feng, Yang Gao",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106224"
    },
    {
        "id": 4780,
        "title": "CAKT: Coupling contrastive learning with attention networks for interpretable knowledge tracing",
        "authors": "Shuaishuai Zu, Li Li, Jun Shen",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191799"
    },
    {
        "id": 4781,
        "title": "Sparse Attention-based Graph Neural Networks for Traffic Forecasting",
        "authors": "Zubo Hu, Huiyuan Jiang",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248483"
    },
    {
        "id": 4782,
        "title": "Speech Emotion Recognition Using Convolutional Neural Networks with Attention Mechanism",
        "authors": "Konstantinos Mountzouris, Isidoros Perikos, Ioannis Hatzilygeroudis",
        "published": "2023-10-23",
        "citations": 1,
        "abstract": "Speech emotion recognition (SER) is an interesting and difficult problem to handle. In this paper, we deal with it through the implementation of deep learning networks. We have designed and implemented six different deep learning networks, a deep belief network (DBN), a simple deep neural network (SDNN), an LSTM network (LSTM), an LSTM network with the addition of an attention mechanism (LSTM-ATN), a convolutional neural network (CNN), and a convolutional neural network with the addition of an attention mechanism (CNN-ATN), having in mind, apart from solving the SER problem, to test the impact of the attention mechanism on the results. Dropout and batch normalization techniques are also used to improve the generalization ability (prevention of overfitting) of the models as well as to speed up the training process. The Surrey Audio–Visual Expressed Emotion (SAVEE) database and the Ryerson Audio–Visual Database (RAVDESS) were used for the training and evaluation of our models. The results showed that the networks with the addition of the attention mechanism did better than the others. Furthermore, they showed that the CNN-ATN was the best among the tested networks, achieving an accuracy of 74% for the SAVEE database and 77% for the RAVDESS, and exceeding existing state-of-the-art systems for the same datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12204376"
    },
    {
        "id": 4783,
        "title": "Cross-Modality Encoder Representations Based On External Attention Mechanism",
        "authors": "Yudong Zheng, Jun Lu",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105662"
    },
    {
        "id": 4784,
        "title": "Dense Attention: A Densely Connected Attention Mechanism for Vision Transformer",
        "authors": "Nannan Li, Yaran Chen, Dongbin Zhao",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191462"
    },
    {
        "id": 4785,
        "title": "DroneAttention: Sparse weighted temporal attention for drone-camera based activity recognition",
        "authors": "Santosh Kumar Yadav, Achleshwar Luthra, Esha Pahwa, Kamlesh Tiwari, Heena Rathore, Hari Mohan Pandey, Peter Corcoran",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.12.005"
    },
    {
        "id": 4786,
        "title": "Spacecraft anomaly detection with attention temporal convolution networks",
        "authors": "Liang Liu, Ling Tian, Zhao Kang, Tianqi Wan",
        "published": "2023-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08213-9"
    },
    {
        "id": 4787,
        "title": "MuLAN: Multi-level attention-enhanced matching network for few-shot knowledge graph completion",
        "authors": "Qianyu Li, Bozheng Feng, Xiaoli Tang, Han Yu, Hengjie Song",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106222"
    },
    {
        "id": 4788,
        "title": "SAlign: A Graph Neural Attention Framework for Aligning Structurally Heterogeneous Networks",
        "authors": "Shruti Saxena,  Joydeep Chandra",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Network alignment techniques that map the same entities across multiple networks assume that the mapping nodes in two different networks have similar attributes and neighborhood proximity. However, real-world networks often violate such assumptions, having diverse attributes and structural properties. Node mapping across such structurally heterogeneous networks remains a challenge. Although capturing the nodes’ entire neighborhood (in low-dimensional embeddings) may help deal with these characteristic differences, the issue of over-smoothing in the representations that come from higherorder learning still remains a major problem. To address the above concerns, we propose SAlign: a supervised graph neural attention framework for aligning structurally heterogeneous networks that learns the correlation of structural properties of mapping nodes using a set of labeled (mapped) anchor nodes. SAlign incorporates nodes’ graphlet information with a novel structure-aware cross-network attention mechanism that transfers the required higher-order structure information across networks. The information exchanged across networks helps in enhancing the expressivity of the graph neural network, thereby handling any potential over-smoothing problem. Extensive experiments on three real datasets demonstrate that SAlign consistently outperforms the state-of-the-art network alignment methods by at least 1.3-8% in terms of accuracy score. The code is available at https://github.com/shruti400/SAlign for reproducibility.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1613/jair.1.14427"
    },
    {
        "id": 4789,
        "title": "Underwater Image Enhancement with Phase Transfer and Attention",
        "authors": "MD Raqib Khan, Ashutosh Kulkarni, Shruti S. Phutke, Subrahmanyam Murala",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191620"
    },
    {
        "id": 4790,
        "title": "STCA-SNN: self-attention-based temporal-channel joint attention for spiking neural networks",
        "authors": "Xiyan Wu, Yong Song, Ya Zhou, Yurong Jiang, Yashuo Bai, Xinyi Li, Xin Yang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "Spiking Neural Networks (SNNs) have shown great promise in processing spatio-temporal information compared to Artificial Neural Networks (ANNs). However, there remains a performance gap between SNNs and ANNs, which impedes the practical application of SNNs. With intrinsic event-triggered property and temporal dynamics, SNNs have the potential to effectively extract spatio-temporal features from event streams. To leverage the temporal potential of SNNs, we propose a self-attention-based temporal-channel joint attention SNN (STCA-SNN) with end-to-end training, which infers attention weights along both temporal and channel dimensions concurrently. It models global temporal and channel information correlations with self-attention, enabling the network to learn ‘what’ and ‘when’ to attend simultaneously. Our experimental results show that STCA-SNNs achieve better performance on N-MNIST (99.67%), CIFAR10-DVS (81.6%), and N-Caltech 101 (80.88%) compared with the state-of-the-art SNNs. Meanwhile, our ablation study demonstrates that STCA-SNNs improve the accuracy of event stream classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1261543"
    },
    {
        "id": 4791,
        "title": "Attention-Capsule Network for Low-Light Image Recognition",
        "authors": "Shiqi Shen, Zetao Jiang, Xiaochun Lei, Xu Wu, Yuting He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191264"
    },
    {
        "id": 4792,
        "title": "SATNet: Upgraded LSTM Network For Mining Time Series Correlation Utilizing The Self-Attention Mechanism",
        "authors": "Xiangyu Dai, Quan Zou",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191490"
    },
    {
        "id": 4793,
        "title": "A<sup>2</sup>P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented Neural Networks",
        "authors": "Mohsen Ahmadzadeh, Mehdi Kamal, Ali Afzali-Kusha, Massoud Pedram",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3148818"
    },
    {
        "id": 4794,
        "title": "Hierarchical attention network with progressive feature fusion for facial expression recognition",
        "authors": "Huanjie Tao, Qianyue Duan",
        "published": "2024-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.033"
    },
    {
        "id": 4795,
        "title": "What Causes a Driver's Attention Shift? A Driver's Attention-Guided Driving Event Recognition Model",
        "authors": "Pengcheng Du, Tao Deng, Fei Yan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191126"
    },
    {
        "id": 4796,
        "title": "Attention Mechanisms in Convolutional Neural Networks for Nitrogen Treatment Detection in Tomato Leaves Using Hyperspectral Images",
        "authors": "Brahim Benmouna, Raziyeh Pourdarbani, Sajad Sabzi, Ruben Fernandez-Beltran, Ginés García-Mateos, José Miguel Molina-Martínez",
        "published": "2023-6-16",
        "citations": 2,
        "abstract": "Nitrogen is an essential macronutrient for the growth and development of tomatoes. However, excess nitrogen fertilization can affect the quality of tomato fruit, making it unattractive to consumers. Consequently, the aim of this study is to develop a method for the early detection of excessive nitrogen fertilizer use in Royal tomato by visible and near-infrared spectroscopy. Spectral reflectance values of tomato leaves were captured at wavelengths between 400 and 1100 nm, collected from several treatments after application of normal nitrogen and on the first, second, and third days after application of excess nitrogen. A new method based on convolutional neural networks (CNN) with an attention mechanism was proposed to perform the estimation of nitrogen overdose in tomato leaves. To verify the effectiveness of this method, the proposed attention mechanism-based CNN classifier was compared with an alternative CNN having the same architecture without integrating the attention mechanism, and with other CNN models, AlexNet and VGGNet. Experimental results showed that the CNN with an attention mechanism outperformed the alternative CNN, achieving a correct classification rate (CCR) of 97.33% for the treatment, compared with a CCR of 94.94% for the CNN alone. These findings will help in the development of a new tool for rapid and accurate detection of nitrogen fertilizer overuse in large areas.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12122706"
    },
    {
        "id": 4797,
        "title": "Human action recognition using multi-stream attention-based deep networks with heterogeneous data from overlapping sub-actions",
        "authors": "Rashmi M, Ram Mohana Reddy Guddeti",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09630-0"
    },
    {
        "id": 4798,
        "title": "Leveraging spatial residual attention and temporal Markov networks for video action understanding",
        "authors": "Yangyang Xu, Zengmao Wang, Xiaoping Zhang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.047"
    },
    {
        "id": 4799,
        "title": "CA-SSD: Channel-Independent Rare Class Attention for 3D Object Detection",
        "authors": "Jiahao Liu, Huixian Cheng, Guoqiang Xiao, Xianfeng Han",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191599"
    },
    {
        "id": 4800,
        "title": "Improved White Blood Cell Image Classification Using Deep Convolutional Neural Networks with Attention Mechanism",
        "authors": "Yuanfan Qian",
        "published": "2023-8-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isceic59030.2023.10271109"
    },
    {
        "id": 4801,
        "title": "Spatial Bias for attention-free non-local neural networks",
        "authors": "Junhyung Go, Jonngbin Ryu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122053"
    },
    {
        "id": 4802,
        "title": "Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks",
        "authors": "Mirazul Haque, Wei Yang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00163"
    },
    {
        "id": 4803,
        "title": "Attention in a Family of Boltzmann Machines Emerging From Modern Hopfield Networks",
        "authors": "Toshihiro Ota, Ryo Karakida",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Abstract\nHopfield networks and Boltzmann machines (BMs) are fundamental energy-based neural network models. Recent studies on modern Hopfield networks have broadened the class of energy functions and led to a unified perspective on general Hopfield networks, including an attention module. In this letter, we consider the BM counterparts of modern Hopfield networks using the associated energy functions and study their salient properties from a trainability perspective. In particular, the energy function corresponding to the attention module naturally introduces a novel BM, which we refer to as the attentional BM (AttnBM). We verify that AttnBM has a tractable likelihood function and gradient for certain special cases and is easy to train. Moreover, we reveal the hidden connections between AttnBM and some single-layer models, namely the gaussian–Bernoulli restricted BM and the denoising autoencoder with softmax units coming from denoising score matching. We also investigate BMs introduced by other energy functions and show that the energy function of dense associative memory models gives BMs belonging to exponential family harmoniums.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/neco_a_01597"
    },
    {
        "id": 4804,
        "title": "Automatic Food Recognition Using Deep Convolutional Neural Networks with Self-attention Mechanism",
        "authors": "Rahib Abiyev, Joseph Adepoju",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": "AbstractThe significance of food in human health and well-being cannot be overemphasized. Nowadays, in our dynamic life, people are increasingly concerned about their health due to increased nutritional ailments. For this reason, mobile food-tracking applications that require a reliable and robust food classification system are gaining popularity. To address this, we propose a robust food recognition model using deep convolutional neural networks with a self-attention mechanism (FRCNNSAM). By training multiple FRCNNSAM structures with varying parameters, we combine their predictions through averaging. To prevent over-fitting and under-fitting data augmentation to generate extra training data, regularization to avoid excessive model complexity was used. The FRCNNSAM model is tested on two novel datasets: Food-101 and MA Food-121. The model achieved an impressive accuracy of 96.40% on the Food-101 dataset and 95.11% on MA Food-121. Compared to baseline transfer learning models, the FRCNNSAM model surpasses performance by 8.12%. Furthermore, the evaluation on random internet images demonstrates the model's strong generalization ability, rendering it suitable for food image recognition and classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s44230-023-00057-9"
    },
    {
        "id": 4805,
        "title": "A Channel Attention based Large Kernel Convolutional Neural Network for Bearing Fault Diagnosis",
        "authors": "Fan Li, Liping Wang, Decheng Wang, Jun Wu, Ying Wang, Jiayu Sun",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105739"
    },
    {
        "id": 4806,
        "title": "Retracted: Channel-Wise Correlation Calibrates Attention Module for Convolutional Neural Networks",
        "authors": "",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9835050"
    },
    {
        "id": 4807,
        "title": "Image Caption Generator Using Attention Based Neural Networks",
        "authors": "Ashwini Mahendiran",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "Abstract: Image caption generation is a method used to create sentences that describe the scene depicted in a given image. The process includes identifying objects within the image, carrying out various operations, and identifying the most important features of the image. Once the system has identified this information, it generates the most relevant and concise description of the image, which is both grammatically and semantically correct. With the progress in deep-learning techniques, algorithms are able to generate text in the form of natural sentences that can effectively describe an image. However, replicating the natural human ability to comprehend image content and produce descriptive text is a difficult task for machines. The uses of image captioning are vast and of great significance, as it involves creating succinct captions utilizing a variety of techniques such as Natural Language Processing (NLP), Computer Vision(CV), and Deep Learning (DL) techniques. The current study presents a system that employs an attention mechanism, in addition to an encoder and a decoder, to generate captions. It utilizes a pretrained CNN, Inception V3, to extract features from the image and a RNN, GRU, to produce a relevant caption. The attention mechanism used in this model is Bahdanau attention, and the Flickr-8Kdataset is utilized for training the model. The results demonstrate the model's capability to understand images and generate text in a reasonable manner.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22214/ijraset.2023.53825"
    },
    {
        "id": 4808,
        "title": "Attention-Enhanced Graph Neural Networks With Global Context for Session-Based Recommendation",
        "authors": "Yingpei Chen, Yan Tang, Yuan Yuan",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3254897"
    },
    {
        "id": 4809,
        "title": "Re-parameterized Global Channel Interaction Attention Module for Convolutional Neural Networks",
        "authors": "Yincong Wang, Shoubiao Tan, Chunyu Peng",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cipcv58883.2023.00009"
    },
    {
        "id": 4810,
        "title": "Semantic Face Segmentation Using Convolutional Neural Networks With a Supervised Attention Module",
        "authors": "Akiyoshi Hizukuri, Yuto Hirata, Ryohei Nakayama",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3326420"
    },
    {
        "id": 4811,
        "title": "Enhancing Medical Image Segmentation with Attention-Based Recurrent Neural Networks",
        "authors": "Rakesh Kumar Dwivedi, Ananya Saha, Meenakshi Sharma",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470617"
    },
    {
        "id": 4812,
        "title": "Dense &amp; Attention Convolutional Neural Networks for Toe Walking Recognition",
        "authors": "Junde Chen, Rahul Soangra, Marybeth Grant-Beuttler, Y. A. Nanehkaran, Yuxin Wen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnsre.2023.3272362"
    },
    {
        "id": 4813,
        "title": "10X Faster Subgraph Matching: Dual Matching Networks with Interleaved Diffusion Attention",
        "authors": "Thanh Toan Nguyen, Quang Duc Nguyen, Zhao Ren, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191159"
    },
    {
        "id": 4814,
        "title": "Graph global attention network with memory: A deep learning approach for fake news detection",
        "authors": "Qian Chang, Xia Li, Zhao Duan",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106115"
    },
    {
        "id": 4815,
        "title": "CVANet: Cascaded visual attention network for single image super-resolution",
        "authors": "Weidong Zhang, Wenyi Zhao, Jia Li, Peixian Zhuang, Haihan Sun, Yibo Xu, Chongyi Li",
        "published": "2024-2",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.049"
    },
    {
        "id": 4816,
        "title": "DAttNet: monocular depth estimation network based on attention mechanisms",
        "authors": "Armando Astudillo, Alejandro Barrera, Carlos Guindel, Abdulla Al-Kaff, Fernando García",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09210-8"
    },
    {
        "id": 4817,
        "title": "Improving domain generalization by hybrid domain attention and localized maximum sensitivity",
        "authors": "Wing W.Y. Ng, Qin Zhang, Cankun Zhong, Jianjun Zhang",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.014"
    },
    {
        "id": 4818,
        "title": "Heterogeneous-graph Attention Reinforcement Learning for Football Matches",
        "authors": "Shiiie Wang, Yi Pan, Zhiqiang Pu, Jianqiang Yi, Yanyan Liang, Du Zhang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191648"
    },
    {
        "id": 4819,
        "title": "Dual-Encoder Attention Fusion Model for Aspect Sentiment Triplet Extraction",
        "authors": "Yunqi Zhang, Songda Li, Yuquan Lan, Hui Zhao, Gang Zhao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191487"
    },
    {
        "id": 4820,
        "title": "Full-Chip Voltage Prediction via Graph Attention Based Neural Networks",
        "authors": "Yuan Li, Pingqiang Zhou",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asicon58565.2023.10396114"
    },
    {
        "id": 4821,
        "title": "Weld Defects Detection by Neural Networks Based on Attention Mechanism",
        "authors": "Guang Yang, Zhifeng Liu, Jianjun Wang, Shuping Cui",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291954"
    },
    {
        "id": 4822,
        "title": "Lightweight graph convolutional networks combining multimodal fusion and spatio-temporal attention mechanisms",
        "authors": "Biao Guo, Jinyue Li, Xuetao Wang",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cipae60493.2023.00147"
    },
    {
        "id": 4823,
        "title": "Interpretable local flow attention for multi-step traffic flow prediction",
        "authors": "Xu Huang, Bowen Zhang, Shanshan Feng, Yunming Ye, Xutao Li",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.023"
    },
    {
        "id": 4824,
        "title": "Human Action Recognition Based on Residual Networks with Improved Attention Mechanisms",
        "authors": "Tao Li, Hao Zhu, Xuemei Sun, Lulu He",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291257"
    },
    {
        "id": 4825,
        "title": "HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN",
        "authors": "Md Shofiqul Islam, Khondokar Fida Hasan, Sunjida Sultana, Shahadat Uddin, Pietro Lio’, Julian M.W. Quinn, Mohammad Ali Moni",
        "published": "2023-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.004"
    },
    {
        "id": 4826,
        "title": "Gaze Estimation with Multi-scale Attention-based Convolutional Neural Networks",
        "authors": "Yuanyuan Zhang, Jing Li, Gaoxiang Ouyang",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/m2vip58386.2023.10413385"
    },
    {
        "id": 4827,
        "title": "Classification of Attention Deficit Hyperactivity Disorder Using Hybrid SWIN Transformer and Graph Convolutional Neural Networks",
        "authors": "Kailash Shaw",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ismsit58785.2023.10304922"
    },
    {
        "id": 4828,
        "title": "Sparse Attention-Based Neural Networks for Code Classification",
        "authors": "Ziyang Xiang, Zaixi Zhang, Qi Liu",
        "published": "2023-11-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dsins60115.2023.10455451"
    },
    {
        "id": 4829,
        "title": "Text sentiment analysis model based on RoBERTa-BiLSTM-Attention",
        "authors": "Jingjing Sun, Gongwu Chen",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3004930"
    },
    {
        "id": 4830,
        "title": "CSARUNet: an attention mechanism-based model for image tampering localization with ringed residual block",
        "authors": "Ying Guo, Shan Jiang, JinHong Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191388"
    },
    {
        "id": 4831,
        "title": "Enhancing Chinese Calligraphy Generation with Contour and Region-aware Attention",
        "authors": "Jinshan Zeng, Ling Tu, Jie Zhou, Yefei Wang, Jiguo Zeng",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192011"
    },
    {
        "id": 4832,
        "title": "Reconstruction-guided attention improves the object recognition robustness of neural networks",
        "authors": "Seoyoung Ahn, Hossein Adeli, Gregory Zelinsky",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1167/jov.23.9.5129"
    },
    {
        "id": 4833,
        "title": "Eff2CaNet:Convolutional Neural Networks Based on Coordinated Attention for COVID-19 Classification",
        "authors": "Yanzhang Li, Kaiqi Deng",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icfeict59519.2023.00025"
    },
    {
        "id": 4834,
        "title": "Attention map feature fusion network for Zero-Shot Sketch-based Image Retrieval",
        "authors": "Honggang Zhao, Mingyue Liu, Yinghua Lin, Mingyong Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191534"
    },
    {
        "id": 4835,
        "title": "A Crowd Counting Method Based on Multi-Scale Attention Network",
        "authors": "Yu Xu, Mu Liang, Ziting Gong",
        "published": "2023-2-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105713"
    },
    {
        "id": 4836,
        "title": "A Spatial–Channel–Temporal-Fused Attention for Spiking Neural Networks",
        "authors": "Wuque Cai, Hongze Sun, Rui Liu, Yan Cui, Jun Wang, Yang Xia, Dezhong Yao, Daqing Guo",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3278265"
    },
    {
        "id": 4837,
        "title": "TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks",
        "authors": "Rui-Jie Zhu, Malu Zhang, Qihang Zhao, Haoyu Deng, Yule Duan, Liang-Jian Deng",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3377717"
    },
    {
        "id": 4838,
        "title": "Common and distinct neural mechanisms of attention",
        "authors": "Ruobing Xia, Xiaomo Chen, Tatiana A. Engel, Tirin Moore",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.tics.2024.01.005"
    },
    {
        "id": 4839,
        "title": "An Attention-Based Convolutional Recurrent Neural Networks for Scene Text Recognition",
        "authors": "Adil Abdullah Abdulhussein Alshawi, Jafar Tanha, Mohammad Ali Balafar",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3352748"
    },
    {
        "id": 4840,
        "title": "Self-Supervised Signed Graph Attention Network for Social Recommendation",
        "authors": "Qin Zhao, Gang Liu, Fuli Yang, Ru Yang, Zuliang Kou, Dong Wang",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191310"
    },
    {
        "id": 4841,
        "title": "Adaptive Visual Field Multi-scale Generative Adversarial Networks Image Inpainting Base on Coordinate-Attention",
        "authors": "Gang Chen, Peipei Kang, Xingcai Wu, Zhenguo Yang, Wenyin Liu",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11233-0"
    },
    {
        "id": 4842,
        "title": "CyFormer: Accurate State-of-Health Prediction of Lithium-Ion Batteries via Cyclic Attention",
        "authors": "Zhiqiang Nie, Jiankun Zhao, Qicheng Li, Yong Qin",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191180"
    },
    {
        "id": 4843,
        "title": "A Robust Two-Dimensional DOA Estimation Approach Based on Convolutional Attention Network",
        "authors": "Rui Xiao, Ming–Yi You, Rui Zhou, Qingjiang Shi",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191726"
    },
    {
        "id": 4844,
        "title": "LoGo Transformer: Hierarchy Lightweight Full Self-Attention Network for Corneal Endothelial Cell Segmentation",
        "authors": "Yinglin Zhang, Zichao Cai, Risa Higashita, Jiang Liu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191116"
    },
    {
        "id": 4845,
        "title": "Tomato Disease Detection Using Vision Transformer with Residual L1-Norm Attention and Deep Neural Networks",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.57"
    },
    {
        "id": 4846,
        "title": "Refixation behavior in naturalistic viewing: Methods, mechanisms, and neural correlates",
        "authors": "Andrey R. Nikolaev, Radha Nila Meghanathan, Cees van Leeuwen",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "\nAbstract\nWhen freely viewing a scene, the eyes often return to previously visited locations. By tracking eye movements and coregistering eye movements and EEG, such refixations are shown to have multiple roles: repairing insufficient encoding from precursor fixations, supporting ongoing viewing by resampling relevant locations prioritized by precursor fixations, and aiding the construction of memory representations. All these functions of refixation behavior are understood to be underpinned by three oculomotor and cognitive systems and their associated brain structures. First, immediate saccade planning prior to refixations involves attentional selection of candidate locations to revisit. This process is likely supported by the dorsal attentional network. Second, visual working memory, involved in maintaining task-related information, is likely supported by the visual cortex. Third, higher-order relevance of scene locations, which depends on general knowledge and understanding of scene meaning, is likely supported by the hippocampal memory system. Working together, these structures bring about viewing behavior that balances exploring previously unvisited areas of a scene with exploiting visited areas through refixations.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13414-023-02836-9"
    },
    {
        "id": 4847,
        "title": "Microseismic Event Recognition and Transfer Learning Based on Convolutional Neural Network and Attention Mechanisms",
        "authors": "Shu Jin, Shichao Zhang, Ya Gao, Benli Yu, Shenglai Zhen",
        "published": "2024-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11770-024-1058-y"
    },
    {
        "id": 4848,
        "title": "Age Estimation Based on Graph Convolutional Networks and Multi-head Attention Mechanisms",
        "authors": "Miaomiao Yang, Changwei Yao, Shijin Yan",
        "published": "2023-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iciscae59047.2023.10393185"
    },
    {
        "id": 4849,
        "title": "Deep Learning-based Viewpoint Prediction Model and Influencing Attention Factors for Design Drawings",
        "authors": "Bai Liu",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105774"
    },
    {
        "id": 4850,
        "title": "A hybrid attention mechanism for multi-target entity relation extraction using graph neural networks",
        "authors": "Arshad Javeed",
        "published": "2023-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100444"
    },
    {
        "id": 4851,
        "title": "Automated vein verification using self-attention-based convolutional neural networks",
        "authors": "Mustafa Kocakulak, Adem Avcı, Nurettin Acır",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120550"
    },
    {
        "id": 4852,
        "title": "Blind Attention Geometric Restraint Neural Network for Single Image Dynamic/Defocus Deblurring",
        "authors": "Jie Zhang, Wanming Zhai",
        "published": "2023-11",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3151099"
    },
    {
        "id": 4853,
        "title": "Neural Networks Referees in 2022",
        "authors": "",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(22)00489-0"
    },
    {
        "id": 4854,
        "title": "Neural Networks Referees in 2023",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(23)00713-x"
    },
    {
        "id": 4855,
        "title": "LSTM-GATs: Wind field prediction based on graph attention deep neural network",
        "authors": "Shengjie Zhou",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3605801.3605841"
    },
    {
        "id": 4856,
        "title": "Attention-fused recurrent neural networks for DDoS attack detection based on blockchain technology",
        "authors": "shi zixin",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2682327"
    },
    {
        "id": 4857,
        "title": "Improving Camouflage Object Detection Using U-NET and VGG16 Deep Neural Networks and CBAM Attention Mechanism",
        "authors": "Erfan Akbarnezhad, Fatemeh Naserizadeh",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/qicar61538.2024.10496615"
    },
    {
        "id": 4858,
        "title": "Relation-Aware Graph Attention Network for Multi-Behavior Recommendation",
        "authors": "Ming Wu, Qiufen Ni, Jigang Wu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191140"
    },
    {
        "id": 4859,
        "title": "Optimizing Temperature Setting for Decomposition Furnace Based on Attention Mechanism and Neural Networks",
        "authors": "Shangkun Liu, Wei Shen, Chase Q. Wu, Xukang Lyu",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "The temperature setting for a decomposition furnace is of great importance for maintaining the normal operation of the furnace and other equipment in a cement plant and ensuring the output of high-quality cement products. Based on the principles of deep convolutional neural networks (CNNs), long short-term memory networks (LSTMs), and attention mechanisms, we propose a CNN-LSTM-A model to optimize the temperature settings for a decomposition furnace. The proposed model combines the features selected by Least Absolute Shrinkage and Selection Operator (Lasso) with others suggested by domain experts as inputs, and uses CNN to mine spatial features, LSTM to extract time series information, and an attention mechanism to optimize weights. We deploy sensors to collect production measurements at a real-life cement factory for experimentation and investigate the impact of hyperparameter changes on the performance of the proposed model. Experimental results show that CNN-LSTM-A achieves a superior performance in terms of prediction accuracy over existing models such as the basic LSTM model, deep-convolution-based LSTM model, and attention-mechanism-based LSTM model. The proposed model has potentials for wide deployment in cement plants to automate and optimize the operation of decomposition furnaces.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23249754"
    },
    {
        "id": 4860,
        "title": "Global Temperature Prediction by BiLSTM Model Based on Whale Optimization Algorithm and Attention Mechanism",
        "authors": "Bohao Li, Hewen Pan",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105678"
    },
    {
        "id": 4861,
        "title": "Single-Lead to Multi-Lead Electrocardiogram Reconstruction Using a Modified Attention U-Net Framework",
        "authors": "Akshit Garg, Vijay Vignesh Venkataramani, U. Deva Priyakumar",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191213"
    },
    {
        "id": 4862,
        "title": "SWTA: Sparse Weighted Temporal Attention for Drone-Based Activity Recognition",
        "authors": "Santosh Kumar Yadav, Esha Pahwa, Achleshwar Luthra, Kamlesh Tiwari, Hari Mohan Pandey",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191750"
    },
    {
        "id": 4863,
        "title": "Meta-MSGAT: Meta Multi-scale Fused Graph Attention Network",
        "authors": "Ting Chen, Jianming Wang, Yukuan Sun",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191411"
    },
    {
        "id": 4864,
        "title": "Performance Evaluation of Attention Mechanism and Spiking Neural Networks on sMRI Data for Suicide Ideation Assessment",
        "authors": "Corrine Francis, Abdulrazak Yahya Saleh Al-Hababi",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoco59262.2023.10397625"
    },
    {
        "id": 4865,
        "title": "Self-attention Convolutional Neural Network-based SLA Decomposition for Network Slicing",
        "authors": "Haidong Ji, Yuanyuan Wang, Lin Tian, Qian Sun, Zongshuai Zhang",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105804"
    },
    {
        "id": 4866,
        "title": "Hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot",
        "authors": "Yanbin Zhang, Mathias Thor, Nat Dilokthanakul, Zhendong Dai, Poramate Manoonpong",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.030"
    },
    {
        "id": 4867,
        "title": "Coexistence of Cyclic Sequential Pattern Recognition and Associative Memory in Neural Networks by Attractor Mechanisms",
        "authors": "Jingyang Huo, Jiali Yu, Min Wang, Zhang Yi, Jinsong Leng, Yong Liao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3368092"
    },
    {
        "id": 4868,
        "title": "A Study on the Classification of Cancers with Lung Cancer Pathological Images Using Deep Neural Networks and Self-Attention Structures",
        "authors": "",
        "published": "2023-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47750/jptcp.2023.30.06.042"
    },
    {
        "id": 4869,
        "title": "ECANodule: Accurate Pulmonary Nodule Detection and Segmentation with Efficient Channel Attention",
        "authors": "Deng Luo, Qingyuan He, Meng Ma, Kun Yan, Defeng Liu, Ping Wang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191210"
    },
    {
        "id": 4870,
        "title": "Improving Aspect Sentiment Triplet Extraction with Perturbed Masking and Edge-Enhanced Sentiment Graph Attention Network",
        "authors": "Songhua Yang, Tengxun Zhang, Hongfei Xu, Yuxiang Jia",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191551"
    },
    {
        "id": 4871,
        "title": "Application of graph frequency attention convolutional neural networks in depression treatment response",
        "authors": "Zihe Lu, Jialin Wang, Fengqin Wang, Zhoumin Wu",
        "published": "2023-11-17",
        "citations": 1,
        "abstract": "Depression, a prevalent global mental health disorder, necessitates precise treatment response prediction for the improvement of personalized care and patient prognosis. The Graph Convolutional Neural Networks (GCNs) have emerged as a promising technique for handling intricate signals and classification tasks owing to their end-to-end neural architecture and nonlinear processing capabilities. In this context, this article proposes a model named the Graph Frequency Attention Convolutional Neural Network (GFACNN). Primarily, the model transforms the EEG signals into graphs to depict the connections between electrodes and brain regions, while integrating a frequency attention module to accentuate brain rhythm information. The proposed approach delves into the application of graph neural networks in the classification of EEG data, aiming to evaluate the response to antidepressant treatment and discern between treatment-resistant and treatment-responsive cases. Experimental results obtained from an EEG dataset at Peking University People's Hospital demonstrate the notable performance of GFACNN in distinguishing treatment responses among depression patients, surpassing deep learning methodologies including CapsuleNet and GoogLeNet. This highlights the efficacy of graph neural networks in leveraging the connections within EEG signal data. Overall, GFACNN exhibits potential for the classification of depression EEG signals, thereby potentially aiding clinical diagnosis and treatment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpsyt.2023.1244208"
    },
    {
        "id": 4872,
        "title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model",
        "authors": "Yongchang Cao, Liang He, Zhen Wu, Xinyu Dai",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191265"
    },
    {
        "id": 4873,
        "title": "Spike Attention Coding for Spiking Neural Networks",
        "authors": "Jiawen Liu, Yifan Hu, Guoqi Li, Jing Pei, Lei Deng",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3310263"
    },
    {
        "id": 4874,
        "title": "Announcement of the Neural Networks Best Paper Award",
        "authors": "",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/s0893-6080(23)00714-1"
    },
    {
        "id": 4875,
        "title": "Multi-hop Relational Graph Attention Network for Text-to-SQL Parsing",
        "authors": "Hu Liu, Yuliang Shi, Jianlin Zhang, Xinjun Wang, Hui Li, Fanyu Kong",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191914"
    },
    {
        "id": 4876,
        "title": "FEAMNet: Light Field Depth Estimation Network Based On Feature Extraction and Attention Mechanism",
        "authors": "Yunming Liu, Yuxuan Pan, Kaiyue Luo, Yu Liu, Lin Zhang",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191959"
    },
    {
        "id": 4877,
        "title": "Optical and SAR remote sensing image ship detection based on attention mechanism",
        "authors": "Yu Zhao, Cong Zhang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005001"
    },
    {
        "id": 4878,
        "title": "Transformer with modified self-attention for flat-lattice Chinese NER",
        "authors": "Xiaojun Bi, Congcong Zhao, Weizheng Qiao",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005002"
    },
    {
        "id": 4879,
        "title": "Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks",
        "authors": "Ajwa Aslam, Allah Bux Sargano, Zulfiqar Habib",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.110494"
    },
    {
        "id": 4880,
        "title": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers (Student Abstract)",
        "authors": "Danilo Dordevic, Vukasin Bozic, Joseph Thommes, Daniele Coppola, Sidak Pal Singh",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these ”attentionless Transformers” to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward\nnetworks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30436"
    },
    {
        "id": 4881,
        "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
        "authors": "Xiaobing Sun, Jiaxi Li, Wei Lu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.650"
    },
    {
        "id": 4882,
        "title": "Q-SAT: Value Factorization with Self-Attention for Deep Multi-Agent Reinforcement Learning",
        "authors": "Xunhan Hu, Jian Zhao, Youpeng Zhao, Wengang Zhou, Houqiang Li",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191777"
    },
    {
        "id": 4883,
        "title": "Multi-Task Collaborative Attention Network for Pedestrian Attribute Recognition",
        "authors": "Junliang Cao, Hua Wei, Yongli Sun, Zhifeng Zhao, Wei Wang, Guangze Sun, Gang Wang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191574"
    },
    {
        "id": 4884,
        "title": "LRFAN: a multi-scale large receptive field attention neural network",
        "authors": "Ci Song, Zhaogong Zhang",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3603781.3603834"
    },
    {
        "id": 4885,
        "title": "A Neig-DoubleRipple recommendation model based on fused neighbourhood attention and aggregation",
        "authors": "HongWei Chen, Qigang Li",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3004865"
    },
    {
        "id": 4886,
        "title": "Moving Object Location Prediction Based on a Graph Neural Network with Temporal Attention",
        "authors": "Yubao Wu, Jun Qian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijsn.2023.10058288"
    },
    {
        "id": 4887,
        "title": "A Prediction Model of Drug-Drug Interactions Based on Graph Neural Networks and Subgraph Attention Mechanisms",
        "authors": "Lang Wang, Jiulei Jiang, Shengqing Li",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasc/picom/cbdcom/cy59711.2023.10361362"
    },
    {
        "id": 4888,
        "title": "RGDAN: A random graph diffusion attention network for traffic prediction",
        "authors": "Jin Fan, Wenchao Weng, Hao Tian, Huifeng Wu, Fu Zhu, Jia Wu",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.106093"
    },
    {
        "id": 4889,
        "title": "Meta Learning With Graph Attention Networks for Low-Data Drug Discovery",
        "authors": "Qiujie Lv, Guanxing Chen, Ziduo Yang, Weihe Zhong, Calvin Yu-Chian Chen",
        "published": "2024",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3250324"
    },
    {
        "id": 4890,
        "title": "Infrared spectra prediction using attention-based graph neural networks",
        "authors": "Naseem Saquer, Razib Iqbal, Joshua D. Ellis, Keiichi Yoshimatsu",
        "published": "2024",
        "citations": 0,
        "abstract": "In this work, we present attention-based graph neural networks to predict infrared (IR) spectra from chemical structures.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1039/d3dd00254c"
    },
    {
        "id": 4891,
        "title": "A Temporal Attention-based Model for Social Event Prediction",
        "authors": "Wang Yinsen, Zhang Xin, Pan Yan, Fu Zexin",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191427"
    },
    {
        "id": 4892,
        "title": "TransGlow: Attention-augmented Transduction model based on Graph Neural Networks for Water Flow Forecasting",
        "authors": "Naghmeh Shafiee Roudbari, Charalambos Poullis, Zachary Patterson, Ursula Eicker",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00092"
    },
    {
        "id": 4893,
        "title": "Multi-Head Attention Spatial-Temporal Graph Neural Networks for Traffic Forecasting",
        "authors": "Xiuwei Hu, Enlong Yu, Xiaoyu Zhao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4236/jcc.2024.123004"
    },
    {
        "id": 4894,
        "title": "Event-Guided Attention Network for Low Light Image Enhancement",
        "authors": "Qiaobin Wang, Haiyan Jin, Haonan Su, Zhaolin Xiao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191272"
    },
    {
        "id": 4895,
        "title": "Multi-hop Attention GNN with Answer-Evidence Contrastive Loss for Multi-hop QA",
        "authors": "Ni Yang, Meng Yang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191117"
    },
    {
        "id": 4896,
        "title": "TC-GAT: Graph Attention Network for Temporal Causality Discovery",
        "authors": "Xiaosong Yuan, Ke Chen, Wanli Zuo, Yijia Zhang",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191712"
    },
    {
        "id": 4897,
        "title": "Enhancing Power Efficiency Prediction in Small-Scale Data Centers using Attention-Based Deep Neural Networks",
        "authors": "Kumaravel Saranraj, Chuan-Ming Liu",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaceh59552.2023.10452493"
    },
    {
        "id": 4898,
        "title": "AdaptParse: Adaptive Contextual Aware Attention Network for Log Parsing via Word Classification",
        "authors": "Haitian Yang, Degang Sun, Yan Wang, Nan Zhao, Shixiang Zhang, Weiqing Huang",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191155"
    },
    {
        "id": 4899,
        "title": "Latent Attention Network With Position Perception for Visual Question Answering",
        "authors": "Jing Zhang, Xiaoqiang Liu, Zhe Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3377636"
    },
    {
        "id": 4900,
        "title": "Speech Emotion Recognition Using Dual Global Context Attention and Time-Frequency Features",
        "authors": "Peng Zhang, Xuheng Bai, Jing Zhao, Yan Liang, Fuqiang Wang, Xiaoming Wu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192031"
    },
    {
        "id": 4901,
        "title": "Efficient Method Using Attention Based Convolutional Neural Networks for Ceramic Tiles Defect Classification",
        "authors": "Souad Abbas, Hamouma Moumen, Fayçal Abbas",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18280/ria.370108"
    },
    {
        "id": 4902,
        "title": "Spatial-Temporal Self-Attention for Asynchronous Spiking Neural Networks",
        "authors": "Yuchen Wang, Kexin Shi, Chengzhuo Lu, Yuguo Liu, Malu Zhang, Hong Qu",
        "published": "2023-8",
        "citations": 1,
        "abstract": "The brain-inspired spiking neural networks (SNNs) are receiving increasing attention due to their asynchronous event-driven characteristics and low power consumption. As attention mechanisms recently become an indispensable part of sequence dependence modeling, the combination of SNNs and attention mechanisms holds great potential for energy-efficient and high-performance computing paradigms. However, the existing works cannot benefit from both temporal-wise attention and the asynchronous characteristic of SNNs. To fully leverage the advantages of both SNNs and attention mechanisms, we propose an SNNs-based spatial-temporal self-attention (STSA) mechanism, which calculates the feature dependence across the time and space domains without destroying the asynchronous transmission properties of SNNs. To further improve the performance, we also propose a spatial-temporal relative position bias (STRPB) for STSA to consider the spatiotemporal position of spikes. Based on the STSA and STRPB, we construct a spatial-temporal spiking Transformer framework, named STS-Transformer, which is powerful and enables SNNs to work in an asynchronous event-driven manner. Extensive experiments are conducted on popular neuromorphic datasets and speech datasets, including DVS128 Gesture, CIFAR10-DVS, and Google Speech Commands, and our experimental results can outperform other state-of-the-art models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/344"
    },
    {
        "id": 4903,
        "title": "Modeling the Wave Equation Using Physics-Informed Neural Networks Enhanced With Attention to Loss Weights",
        "authors": "Shaikhah Alkhadhr, Mohamed Almekkawy",
        "published": "2023-6-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096980"
    },
    {
        "id": 4904,
        "title": "Driver's visual fixation attention prediction in dynamic scenes using hybrid neural networks",
        "authors": "Chuan Xu, Han Liu, Qinghao Li, Yan Su",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dsp.2023.104217"
    }
]