[
    {
        "id": 1701,
        "title": "Adaptive selection of local and non-local attention mechanisms for speech enhancement",
        "authors": "Xinmeng Xu, Weiping Tu, Yuhong Yang",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106236"
    },
    {
        "id": 1702,
        "title": "Integrating Multiple Visual Attention Mechanisms in Deep Neural Networks",
        "authors": "Fernando Martinez, Yijun Zhao",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/compsac57700.2023.00180"
    },
    {
        "id": 1703,
        "title": "Novel Hybrid Model Coupling WOA with BiLSTM Neural Networks and Attention Mechanisms",
        "authors": "Liuyu Wang, Huatai Pan, Zhengxian Cai",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10258008"
    },
    {
        "id": 1704,
        "title": "A study on the application of graph neural network in code clone detection",
        "authors": "Linfeng Dai",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3605801.3605834"
    },
    {
        "id": 1705,
        "title": "Dual-domain strip attention for image restoration",
        "authors": "Yuning Cui, Alois Knoll",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.003"
    },
    {
        "id": 1706,
        "title": "Multimodal Biometric Recognition Neural Networks with Branch Attention Mechanisms",
        "authors": " LiuJun, Chen Mingjin,  GuoYong, Huang Zengxi",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/arace60380.2023.00021"
    },
    {
        "id": 1707,
        "title": "Co-attention enabled content-based image retrieval",
        "authors": "Zechao Hu, Adrian G. Bors",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.04.009"
    },
    {
        "id": 1708,
        "title": "Combining external-latent attention for medical image segmentation",
        "authors": "Enmin Song, Bangcheng Zhan, Hong Liu",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.046"
    },
    {
        "id": 1709,
        "title": "Combining Multi-Head Attention and Sparse Multi-Head Attention Networks for Session-Based Recommendation",
        "authors": "Zhiwei Zhao, Xiaoye Wang, Yingyuan Xiao",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191924"
    },
    {
        "id": 1710,
        "title": "Meta attention for Off-Policy Actor-Critic",
        "authors": "Jiateng Huang, Wanrong Huang, Long Lan, Dan Wu",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.024"
    },
    {
        "id": 1711,
        "title": "Classification of DBS microelectrode recordings using a residual neural network with attention in the temporal domain",
        "authors": "K.A. Ciecierski, T. Mandat",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.021"
    },
    {
        "id": 1712,
        "title": "Multi-behavior Attention Mechanisms Graph Neural Networks based on Session Recommendation",
        "authors": "Xing Xing, Xuanming Zhang, Jianfu Cui, Jiale Chen, Zhichun Jia",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327142"
    },
    {
        "id": 1713,
        "title": "Meta-structure-based graph attention networks",
        "authors": "Jin Li, Qingyu Sun, Feng Zhang, Beining Yang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.025"
    },
    {
        "id": 1714,
        "title": "Spatial multi-attention conditional neural processes",
        "authors": "Li-Li Bao, Jiang-She Zhang, Chun-Xia Zhang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106201"
    },
    {
        "id": 1715,
        "title": "Spatial oblivion channel attention targeting intra-class diversity feature learning",
        "authors": "Honggui Han, Qiyu Zhang, Fangyu Li, Yongping Du",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.07.032"
    },
    {
        "id": 1716,
        "title": "Knowledge-Preserving continual person re-identification using Graph Attention Network",
        "authors": "Zhaoshuo Liu, Chaolu Feng, Shuaizheng Chen, Jun Hu",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.033"
    },
    {
        "id": 1717,
        "title": "English-Afaan Oromo Machine Translation Using Deep Attention Neural Network",
        "authors": " Ebisa A. Gemechu, G. R. Kanagachidambaresan",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23030049"
    },
    {
        "id": 1718,
        "title": "Towards performance-maximizing neural network pruning via global channel attention",
        "authors": "Yingchun Wang, Song Guo, Jingcai Guo, Jie Zhang, Weizhan Zhang, Caixia Yan, Yuanhong Zhang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.065"
    },
    {
        "id": 1719,
        "title": "QCA-Net: Quantum-based Channel Attention for Deep Neural Networks",
        "authors": "Juntao Zhang, Peng Cheng, Zehan Li, Hao Wu, Wenbo An, Jun Zhou",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191662"
    },
    {
        "id": 1720,
        "title": "Fourier feature decorrelation based sample attention for dense crowd localization",
        "authors": "Chao Wen, Hongqiang He, Yuhua Qian, Yu Xie, Wenjian Wang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106131"
    },
    {
        "id": 1721,
        "title": "STMMOT: Advancing multi-object tracking through spatiotemporal memory networks and multi-scale attention pyramids",
        "authors": "Hamza Mukhtar, Muhammad Usman Ghani Khan",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.09.047"
    },
    {
        "id": 1722,
        "title": "Self-attention learning network for face super-resolution",
        "authors": "Kangli Zeng, Zhongyuan Wang, Tao Lu, Jianyu Chen, Jiaming Wang, Zixiang Xiong",
        "published": "2023-3",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.006"
    },
    {
        "id": 1723,
        "title": "Few-shot link prediction for temporal knowledge graphs based on time-aware translation and attention mechanism",
        "authors": "Han Zhang, Luyi Bai",
        "published": "2023-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.01.043"
    },
    {
        "id": 1724,
        "title": "RAFNet: Restricted attention fusion network for sleep apnea detection",
        "authors": "Ying Chen, Huijun Yue, Ruifeng Zou, Wenbin Lei, Wenjun Ma, Xiaomao Fan",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.019"
    },
    {
        "id": 1725,
        "title": "Transfer Learning with Graph Attention Networks for Team Recommendation",
        "authors": "Sagar Kaw, Ziad Kobti, Kalyani Selvarajah",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191717"
    },
    {
        "id": 1726,
        "title": "Multi-scale multi-reception attention network for bone age assessment in X-ray images",
        "authors": "Zhichao Yang, Cong Cong, Maurice Pagnucco, Yang Song",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.002"
    },
    {
        "id": 1727,
        "title": "Multi-Modal Co-Attention Capsule Network for Fake News Detection",
        "authors": " Chunyan Yin,  Yongheng Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x24010041"
    },
    {
        "id": 1728,
        "title": "Cell Attention Networks",
        "authors": "Lorenzo Giusti, Claudio Battiloro, Lucia Testa, Paolo Di Lorenzo, Stefania Sardellitti, Sergio Barbarossa",
        "published": "2023-6-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191530"
    },
    {
        "id": 1729,
        "title": "COM: Contrastive Masked-attention model for incomplete multimodal learning",
        "authors": "Shuwei Qian, Chongjun Wang",
        "published": "2023-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.03.003"
    },
    {
        "id": 1730,
        "title": "Graph-based social relation inference with multi-level conditional attention",
        "authors": "Xiaotian Yu, Hanling Yi, Qie Tang, Kun Huang, Wenze Hu, Shiliang Zhang, Xiaoyu Wang",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106216"
    },
    {
        "id": 1731,
        "title": "Residual Hybrid Attention Network for Single Image Dehazing",
        "authors": "Hui Ming Li",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191742"
    },
    {
        "id": 1732,
        "title": "Audioset classification with Graph Convolutional Attention model",
        "authors": "Xuliang Li, Junbin Gao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191960"
    },
    {
        "id": 1733,
        "title": "Deep neural networks with attention mechanisms for Spodoptera frugiperda pupae sexing",
        "authors": "João Vitor de Andrade Porto, Fabio Prestes Cesar Rezende, Higor Henrique Picoli Nucci, Antonia Railda Roel, Gilberto Astolfi, Hemerson Pistori",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.atech.2023.100200"
    },
    {
        "id": 1734,
        "title": "Contextually enhanced ES-dRNN with dynamic attention for short-term load forecasting",
        "authors": "Slawek Smyl, Grzegorz Dudek, Paweł Pełka",
        "published": "2024-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.017"
    },
    {
        "id": 1735,
        "title": "Priors-assisted dehazing network with attention supervision and detail preservation",
        "authors": "Weichao Yi, Liquan Dong, Ming Liu, Mei Hui, Lingqin Kong, Yuejin Zhao",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106165"
    },
    {
        "id": 1736,
        "title": "Hierarchical Attention Master–Slave for heterogeneous multi-agent reinforcement learning",
        "authors": "Jiao Wang, Mingrui Yuan, Yun Li, Zihui Zhao",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.02.037"
    },
    {
        "id": 1737,
        "title": "SLAPP: Subgraph-level attention-based performance prediction for deep learning models",
        "authors": "Zhenyi Wang, Pengfei Yang, Linwei Hu, Bowen Zhang, Chengmin Lin, Wenkai Lv, Quan Wang",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.043"
    },
    {
        "id": 1738,
        "title": "Characterizing functional brain networks via Spatio-Temporal Attention 4D Convolutional Neural Networks (STA-4DCNNs)",
        "authors": "Xi Jiang, Jiadong Yan, Yu Zhao, Mingxin Jiang, Yuzhong Chen, Jingchao Zhou, Zhenxiang Xiao, Zifan Wang, Rong Zhang, Benjamin Becker, Dajiang Zhu, Keith M. Kendrick, Tianming Liu",
        "published": "2023-1",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.004"
    },
    {
        "id": 1739,
        "title": "Convolutional Neural Network Modulation Recognition by Incorporating Attention Mechanisms",
        "authors": "Mingze Zuo, Yifan Liu",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc59986.2023.10421616"
    },
    {
        "id": 1740,
        "title": "EMAT: Efficient feature fusion network for visual tracking via optimized multi-head attention",
        "authors": "Jun Wang, Changwang Lai, Yuanyun Wang, Wenshuang Zhang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106110"
    },
    {
        "id": 1741,
        "title": "ARPruning: An automatic channel pruning based on attention map ranking",
        "authors": "Tongtong Yuan, Zulin Li, Bo Liu, Yinan Tang, Yujia Liu",
        "published": "2024-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106220"
    },
    {
        "id": 1742,
        "title": "An end-to-end power knowledge extraction method based on convolutional neural networks and multi-headed attention mechanisms",
        "authors": "Bin Zhang, Yangjun Zhou, Liwen Qin, Shan Li",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3005282"
    },
    {
        "id": 1743,
        "title": "Rectified Attention Gate Unit in Recurrent Neural Networks for Effective Attention Computation",
        "authors": "Manh-Hung Ha, Oscal Tzyh-Chiang Chen",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssp53291.2023.10207931"
    },
    {
        "id": 1744,
        "title": "Stacked attention hourglass network based robust facial landmark detection",
        "authors": "Ying Huang, He Huang",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.10.021"
    },
    {
        "id": 1745,
        "title": "Relaxed Attention for Transformer Models",
        "authors": "Timo Lohrenz, Björn Möller, Zhengyang Li, Tim Fingscheidt",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191643"
    },
    {
        "id": 1746,
        "title": "Double Graph Attention Networks for Visual Semantic Navigation",
        "authors": "Yunlian Lyu, Mohammad Sadegh Talebi",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11190-8"
    },
    {
        "id": 1747,
        "title": "Multihead Attention-based Audio Image Generation with Cross-Modal Shared Weight Classifier",
        "authors": "Yiming Xu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191393"
    },
    {
        "id": 1748,
        "title": "Attention-Based Deep Spiking Neural Networks for Temporal Credit Assignment Problems",
        "authors": "Lang Qin, Ziming Wang, Rui Yan, Huajin Tang",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3240176"
    },
    {
        "id": 1749,
        "title": "Adversarial Robustness Analysis of Attention Modules in Deep Neural Networks",
        "authors": "Guo Xingyi, Tang Chenhuan, Chen Jie, Jia Shijie",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.57237/j.se.2023.01.005"
    },
    {
        "id": 1750,
        "title": "Variational Disentangled Attention and Regularization for Visual Dialog",
        "authors": "Jen-Tzung Chien, Hsiu-Wei Tien",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191548"
    },
    {
        "id": 1751,
        "title": "Emotional Attention for Trained Convolutional Neural Networks",
        "authors": "Stefan Tsokov, Milena Lazarova, Adelina Aleksieva-Petrova",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/comsci59259.2023.10315883"
    },
    {
        "id": 1752,
        "title": "Best Fit Activation Functions for Attention Mechanism: Comparison and Enhancement",
        "authors": "Maan Alhazmi, Abdulrahman Altahhan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191885"
    },
    {
        "id": 1753,
        "title": "Application of Physics-Informed Attention-based Neural Networks for Anomaly Detection",
        "authors": "Mohammad F. Zahid",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-0783"
    },
    {
        "id": 1754,
        "title": "SAR-UNet: Small Attention Residual UNet for Explainable Nowcasting Tasks",
        "authors": "Mathieu Renault, Siamak Mehrkanoon",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191095"
    },
    {
        "id": 1755,
        "title": "Attention-enabled gated spiking neural P model for aspect-level sentiment classification",
        "authors": "Yanping Huang, Hong Peng, Qian Liu, Qian Yang, Jun Wang, David Orellana-Martín, Mario J. Pérez-Jiménez",
        "published": "2023-1",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.006"
    },
    {
        "id": 1756,
        "title": "Hierarchical attention-guided multiscale aggregation network for infrared small target detection",
        "authors": "Shunshun Zhong, Haibo Zhou, Zhongxu Zheng, Zhu Ma, Fan Zhang, Ji'an Duan",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.036"
    },
    {
        "id": 1757,
        "title": "Attention-Based Real Image Restoration",
        "authors": "Saeed Anwar, Nick Barnes, Lars Petersson",
        "published": "2024",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3131739"
    },
    {
        "id": 1758,
        "title": "TCN-Attention-BIGRU: Building energy modelling based on attention mechanisms and temporal convolutional networks",
        "authors": "Yi Deng, Zhanpeng Yue, Ziyi Wu, Yitong Li, Yifei Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "<abstract>\n\n<p>Accurate and effective building energy consumption prediction is an important basis for carrying out energy-saving evaluation and the main basis for building energy-saving optimization design. However, due to the influence of environmental and human factors, energy consumption prediction is often inaccurate. Therefore, this paper presents a building energy consumption prediction model based on an attention mechanism, time convolutional neural (TCN) network fusion, and a bidirectional gated cycle unit (BIGRU). First, t-distributed stochastic neighbor embedding (T-SNE) was used to preprocess the data and extract the key features, and then a BIGRU was employed to acquire past and future data while capturing immediate connections. Then, to catch the long-term dependence, the dataset was partitioned into the TCN network, and the extended sequence was transformed into several short sequences. Consequently, the gradient explosion or vanishing problem is mitigated when the BIGRU handles lengthy sequences while reducing the spatial complexity. Second, the self-attention mechanism was introduced to enhance the model's capability to address data periodicity. The proposed model is superior to the other four models in accuracy, with an mean absolute error of 0.023, an mean-square error of 0.029, and an coefficient of determination of 0.979. Experimental results indicate that T-SNE can significantly improve the model performance, and the accuracy of predictions can be improved by the attention mechanism and the TCN network.</p>\n\n\t      </abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/era.2024098"
    },
    {
        "id": 1759,
        "title": "AttentionMGT-DTA: A multi-modal drug-target affinity prediction using graph transformer and attention mechanism",
        "authors": "Hongjie Wu, Junkai Liu, Tengsheng Jiang, Quan Zou, Shujie Qi, Zhiming Cui, Prayag Tiwari, Yijie Ding",
        "published": "2024-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.018"
    },
    {
        "id": 1760,
        "title": "Masked Kinematic Continuity-aware Hierarchical Attention Network for pose estimation in videos",
        "authors": "Kyung-Min Jin, Gun-Hee Lee, Woo-Jeoung Nam, Tae-Kyung Kang, Hyun-Woo Kim, Seong-Whan Lee",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.038"
    },
    {
        "id": 1761,
        "title": "SSA-ICL: Multi-domain adaptive attention with intra-dataset continual learning for Facial expression recognition",
        "authors": "Hongxiang Gao, Min Wu, Zhenghua Chen, Yuwen Li, Xingyao Wang, Shan An, Jianqing Li, Chengyu Liu",
        "published": "2023-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.025"
    },
    {
        "id": 1762,
        "title": "Attention Based Graph Neural Networks",
        "authors": "MD Ibrahim Khalil, Muhammad Affan Abbas",
        "published": "2023-6-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/seai59139.2023.10217656"
    },
    {
        "id": 1763,
        "title": "Multi-Scale Non-Local Sparse Attention for Single Image Super-Resolution",
        "authors": "Xianwei Xiao, Baojiang Zhong",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191338"
    },
    {
        "id": 1764,
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "authors": "Yao Zhang, Yunpu Ma, Thomas Seidl, Volker Tresp",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191567"
    },
    {
        "id": 1765,
        "title": "HMM-GDAN: Hybrid multi-view and multi-scale graph duplex-attention networks for drug response prediction in cancer",
        "authors": "Youfa Liu, Shufan Tong, Yongyong Chen",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.036"
    },
    {
        "id": 1766,
        "title": "Path reliability-based graph attention networks",
        "authors": "Yayang Li, Shuqing Liang, Yuncheng Jiang",
        "published": "2023-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.021"
    },
    {
        "id": 1767,
        "title": "Transferable graph neural networks with deep alignment attention",
        "authors": "Ying Xie, Rongbin Xu, Yun Yang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119232"
    },
    {
        "id": 1768,
        "title": "Frequency Principle of Attention-Based Convolutional Neural Networks for Single Image Super-Resolution",
        "authors": "Xian Zhang, Jian-Nan Su",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449125"
    },
    {
        "id": 1769,
        "title": "Stacked Attention-based Networks for Accurate and Interpretable Health Risk Prediction",
        "authors": "Yuxi Liu, Zhenhao Zhang, Campbell Thompson, Richard Leibbrandt, Shaowen Qin, Antonio Jimeno Yepes",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191099"
    },
    {
        "id": 1770,
        "title": "Predicting the Cascading Failure Propagation Path in Complex Networks Based On Attention-LSTM Neural Networks",
        "authors": "Donghong Li, Qin Wang, Xi Zhang, Xiujuan Fan",
        "published": "2023-5-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscas46773.2023.10181599"
    },
    {
        "id": 1771,
        "title": "LAC-GAN: Lesion attention conditional GAN for Ultra-widefield image synthesis",
        "authors": "Haijun Lei, Zhihui Tian, Hai Xie, Benjian Zhao, Xianlu Zeng, Jiuwen Cao, Weixin Liu, Jiantao Wang, Guoming Zhang, Shuqiang Wang, Baiying Lei",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.005"
    },
    {
        "id": 1772,
        "title": "Attention-based investigation and solution to the trade-off issue of adversarial training",
        "authors": "Changbin Shao, Wenbin Li, Jing Huo, Zhenhua Feng, Yang Gao",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106224"
    },
    {
        "id": 1773,
        "title": "Adversarial Attacks with Defense Mechanisms on Convolutional Neural Networks and Recurrent Neural Networks for Malware Classification",
        "authors": "Sharoug Alzaidy, Hamad Binsalleeh",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "In the field of behavioral detection, deep learning has been extensively utilized. For example, deep learning models have been utilized to detect and classify malware. Deep learning, however, has vulnerabilities that can be exploited with crafted inputs, resulting in malicious files being misclassified. Cyber-Physical Systems (CPS) may be compromised by malicious files, which can have catastrophic consequences. This paper presents a method for classifying Windows portable executables (PEs) using Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). To generate malware executable adversarial examples of PE, we conduct two white-box attacks, Jacobian-based Saliency Map Attack (JSMA) and Carlini and Wagner attack (C&W). An adversarial payload was injected into the DOS header, and a section was added to the file to preserve the PE functionality. The attacks successfully evaded the CNN model with a 91% evasion rate, whereas the RNN model evaded attacks at an 84.6% rate. Two defense mechanisms based on distillation and training techniques are examined in this study for overcoming adversarial example challenges. Distillation and training against JSMA resulted in the highest reductions in the evasion rates of 48.1% and 41.49%, respectively. Distillation and training against C&W resulted in the highest decrease in evasion rates, at 48.1% and 49.9%, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14041673"
    },
    {
        "id": 1774,
        "title": "Sparse Attention-based Graph Neural Networks for Traffic Forecasting",
        "authors": "Zubo Hu, Huiyuan Jiang",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248483"
    },
    {
        "id": 1775,
        "title": "Speech Emotion Recognition Using Convolutional Neural Networks with Attention Mechanism",
        "authors": "Konstantinos Mountzouris, Isidoros Perikos, Ioannis Hatzilygeroudis",
        "published": "2023-10-23",
        "citations": 1,
        "abstract": "Speech emotion recognition (SER) is an interesting and difficult problem to handle. In this paper, we deal with it through the implementation of deep learning networks. We have designed and implemented six different deep learning networks, a deep belief network (DBN), a simple deep neural network (SDNN), an LSTM network (LSTM), an LSTM network with the addition of an attention mechanism (LSTM-ATN), a convolutional neural network (CNN), and a convolutional neural network with the addition of an attention mechanism (CNN-ATN), having in mind, apart from solving the SER problem, to test the impact of the attention mechanism on the results. Dropout and batch normalization techniques are also used to improve the generalization ability (prevention of overfitting) of the models as well as to speed up the training process. The Surrey Audio–Visual Expressed Emotion (SAVEE) database and the Ryerson Audio–Visual Database (RAVDESS) were used for the training and evaluation of our models. The results showed that the networks with the addition of the attention mechanism did better than the others. Furthermore, they showed that the CNN-ATN was the best among the tested networks, achieving an accuracy of 74% for the SAVEE database and 77% for the RAVDESS, and exceeding existing state-of-the-art systems for the same datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12204376"
    },
    {
        "id": 1776,
        "title": "CAKT: Coupling contrastive learning with attention networks for interpretable knowledge tracing",
        "authors": "Shuaishuai Zu, Li Li, Jun Shen",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191799"
    },
    {
        "id": 1777,
        "title": "Cross-Modality Encoder Representations Based On External Attention Mechanism",
        "authors": "Yudong Zheng, Jun Lu",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105662"
    },
    {
        "id": 1778,
        "title": "Dense Attention: A Densely Connected Attention Mechanism for Vision Transformer",
        "authors": "Nannan Li, Yaran Chen, Dongbin Zhao",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191462"
    },
    {
        "id": 1779,
        "title": "Spacecraft anomaly detection with attention temporal convolution networks",
        "authors": "Liang Liu, Ling Tian, Zhao Kang, Tianqi Wan",
        "published": "2023-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08213-9"
    },
    {
        "id": 1780,
        "title": "DroneAttention: Sparse weighted temporal attention for drone-camera based activity recognition",
        "authors": "Santosh Kumar Yadav, Achleshwar Luthra, Esha Pahwa, Kamlesh Tiwari, Heena Rathore, Hari Mohan Pandey, Peter Corcoran",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.12.005"
    },
    {
        "id": 1781,
        "title": "MuLAN: Multi-level attention-enhanced matching network for few-shot knowledge graph completion",
        "authors": "Qianyu Li, Bozheng Feng, Xiaoli Tang, Han Yu, Hengjie Song",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106222"
    },
    {
        "id": 1782,
        "title": "SAlign: A Graph Neural Attention Framework for Aligning Structurally Heterogeneous Networks",
        "authors": "Shruti Saxena,  Joydeep Chandra",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Network alignment techniques that map the same entities across multiple networks assume that the mapping nodes in two different networks have similar attributes and neighborhood proximity. However, real-world networks often violate such assumptions, having diverse attributes and structural properties. Node mapping across such structurally heterogeneous networks remains a challenge. Although capturing the nodes’ entire neighborhood (in low-dimensional embeddings) may help deal with these characteristic differences, the issue of over-smoothing in the representations that come from higherorder learning still remains a major problem. To address the above concerns, we propose SAlign: a supervised graph neural attention framework for aligning structurally heterogeneous networks that learns the correlation of structural properties of mapping nodes using a set of labeled (mapped) anchor nodes. SAlign incorporates nodes’ graphlet information with a novel structure-aware cross-network attention mechanism that transfers the required higher-order structure information across networks. The information exchanged across networks helps in enhancing the expressivity of the graph neural network, thereby handling any potential over-smoothing problem. Extensive experiments on three real datasets demonstrate that SAlign consistently outperforms the state-of-the-art network alignment methods by at least 1.3-8% in terms of accuracy score. The code is available at https://github.com/shruti400/SAlign for reproducibility.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1613/jair.1.14427"
    },
    {
        "id": 1783,
        "title": "Underwater Image Enhancement with Phase Transfer and Attention",
        "authors": "MD Raqib Khan, Ashutosh Kulkarni, Shruti S. Phutke, Subrahmanyam Murala",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191620"
    },
    {
        "id": 1784,
        "title": "STCA-SNN: self-attention-based temporal-channel joint attention for spiking neural networks",
        "authors": "Xiyan Wu, Yong Song, Ya Zhou, Yurong Jiang, Yashuo Bai, Xinyi Li, Xin Yang",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "Spiking Neural Networks (SNNs) have shown great promise in processing spatio-temporal information compared to Artificial Neural Networks (ANNs). However, there remains a performance gap between SNNs and ANNs, which impedes the practical application of SNNs. With intrinsic event-triggered property and temporal dynamics, SNNs have the potential to effectively extract spatio-temporal features from event streams. To leverage the temporal potential of SNNs, we propose a self-attention-based temporal-channel joint attention SNN (STCA-SNN) with end-to-end training, which infers attention weights along both temporal and channel dimensions concurrently. It models global temporal and channel information correlations with self-attention, enabling the network to learn ‘what’ and ‘when’ to attend simultaneously. Our experimental results show that STCA-SNNs achieve better performance on N-MNIST (99.67%), CIFAR10-DVS (81.6%), and N-Caltech 101 (80.88%) compared with the state-of-the-art SNNs. Meanwhile, our ablation study demonstrates that STCA-SNNs improve the accuracy of event stream classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1261543"
    },
    {
        "id": 1785,
        "title": "Attention-Capsule Network for Low-Light Image Recognition",
        "authors": "Shiqi Shen, Zetao Jiang, Xiaochun Lei, Xu Wu, Yuting He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191264"
    },
    {
        "id": 1786,
        "title": "SATNet: Upgraded LSTM Network For Mining Time Series Correlation Utilizing The Self-Attention Mechanism",
        "authors": "Xiangyu Dai, Quan Zou",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191490"
    },
    {
        "id": 1787,
        "title": "A<sup>2</sup>P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented Neural Networks",
        "authors": "Mohsen Ahmadzadeh, Mehdi Kamal, Ali Afzali-Kusha, Massoud Pedram",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3148818"
    },
    {
        "id": 1788,
        "title": "What Causes a Driver's Attention Shift? A Driver's Attention-Guided Driving Event Recognition Model",
        "authors": "Pengcheng Du, Tao Deng, Fei Yan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191126"
    },
    {
        "id": 1789,
        "title": "Hierarchical attention network with progressive feature fusion for facial expression recognition",
        "authors": "Huanjie Tao, Qianyue Duan",
        "published": "2024-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.11.033"
    },
    {
        "id": 1790,
        "title": "Attention Mechanisms in Convolutional Neural Networks for Nitrogen Treatment Detection in Tomato Leaves Using Hyperspectral Images",
        "authors": "Brahim Benmouna, Raziyeh Pourdarbani, Sajad Sabzi, Ruben Fernandez-Beltran, Ginés García-Mateos, José Miguel Molina-Martínez",
        "published": "2023-6-16",
        "citations": 2,
        "abstract": "Nitrogen is an essential macronutrient for the growth and development of tomatoes. However, excess nitrogen fertilization can affect the quality of tomato fruit, making it unattractive to consumers. Consequently, the aim of this study is to develop a method for the early detection of excessive nitrogen fertilizer use in Royal tomato by visible and near-infrared spectroscopy. Spectral reflectance values of tomato leaves were captured at wavelengths between 400 and 1100 nm, collected from several treatments after application of normal nitrogen and on the first, second, and third days after application of excess nitrogen. A new method based on convolutional neural networks (CNN) with an attention mechanism was proposed to perform the estimation of nitrogen overdose in tomato leaves. To verify the effectiveness of this method, the proposed attention mechanism-based CNN classifier was compared with an alternative CNN having the same architecture without integrating the attention mechanism, and with other CNN models, AlexNet and VGGNet. Experimental results showed that the CNN with an attention mechanism outperformed the alternative CNN, achieving a correct classification rate (CCR) of 97.33% for the treatment, compared with a CCR of 94.94% for the CNN alone. These findings will help in the development of a new tool for rapid and accurate detection of nitrogen fertilizer overuse in large areas.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12122706"
    },
    {
        "id": 1791,
        "title": "Human action recognition using multi-stream attention-based deep networks with heterogeneous data from overlapping sub-actions",
        "authors": "Rashmi M, Ram Mohana Reddy Guddeti",
        "published": "2024-3-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-024-09630-0"
    },
    {
        "id": 1792,
        "title": "Leveraging spatial residual attention and temporal Markov networks for video action understanding",
        "authors": "Yangyang Xu, Zengmao Wang, Xiaoping Zhang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.10.047"
    },
    {
        "id": 1793,
        "title": "CA-SSD: Channel-Independent Rare Class Attention for 3D Object Detection",
        "authors": "Jiahao Liu, Huixian Cheng, Guoqiang Xiao, Xianfeng Han",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191599"
    },
    {
        "id": 1794,
        "title": "Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks",
        "authors": "Mirazul Haque, Wei Yang",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00163"
    },
    {
        "id": 1795,
        "title": "Improved White Blood Cell Image Classification Using Deep Convolutional Neural Networks with Attention Mechanism",
        "authors": "Yuanfan Qian",
        "published": "2023-8-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isceic59030.2023.10271109"
    },
    {
        "id": 1796,
        "title": "Spatial Bias for attention-free non-local neural networks",
        "authors": "Junhyung Go, Jonngbin Ryu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122053"
    },
    {
        "id": 1797,
        "title": "Re-parameterized Global Channel Interaction Attention Module for Convolutional Neural Networks",
        "authors": "Yincong Wang, Shoubiao Tan, Chunyu Peng",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cipcv58883.2023.00009"
    },
    {
        "id": 1798,
        "title": "Enhancing Medical Image Segmentation with Attention-Based Recurrent Neural Networks",
        "authors": "Rakesh Kumar Dwivedi, Ananya Saha, Meenakshi Sharma",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470617"
    },
    {
        "id": 1799,
        "title": "Semantic Face Segmentation Using Convolutional Neural Networks With a Supervised Attention Module",
        "authors": "Akiyoshi Hizukuri, Yuto Hirata, Ryohei Nakayama",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3326420"
    },
    {
        "id": 1800,
        "title": "Attention in a Family of Boltzmann Machines Emerging From Modern Hopfield Networks",
        "authors": "Toshihiro Ota, Ryo Karakida",
        "published": "2023-7-12",
        "citations": 0,
        "abstract": "Abstract\nHopfield networks and Boltzmann machines (BMs) are fundamental energy-based neural network models. Recent studies on modern Hopfield networks have broadened the class of energy functions and led to a unified perspective on general Hopfield networks, including an attention module. In this letter, we consider the BM counterparts of modern Hopfield networks using the associated energy functions and study their salient properties from a trainability perspective. In particular, the energy function corresponding to the attention module naturally introduces a novel BM, which we refer to as the attentional BM (AttnBM). We verify that AttnBM has a tractable likelihood function and gradient for certain special cases and is easy to train. Moreover, we reveal the hidden connections between AttnBM and some single-layer models, namely the gaussian–Bernoulli restricted BM and the denoising autoencoder with softmax units coming from denoising score matching. We also investigate BMs introduced by other energy functions and show that the energy function of dense associative memory models gives BMs belonging to exponential family harmoniums.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/neco_a_01597"
    }
]