[
    {
        "id": 20105,
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": "Jia Lim, Hady Lauw",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.534"
    },
    {
        "id": 20106,
        "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
        "authors": "Michael Hanna, Yonatan Belinkov, Sandro Pezzelle",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.744"
    },
    {
        "id": 20107,
        "title": "Transformer-Based Language Models as Psycholinguistic Subjects: Focusing on Understanding Metaphor",
        "authors": "Wonil Chung,  ",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "Metaphor is a fundamental aspect of human language and cognition, playing a crucial role in communication, comprehension, and creative expression. In light of the recent advancements demonstrated by prominent language models, a pivotal question arises: Can these expansive language models effectively discern metaphorical knowledge? The primary objective involves comparing the surprisal values estimated from neural network language models like autoregressive and bidirectional language models to the reaction times of human when exposed to both metaphorical and literal sentences. Our secondary objective involves assessing the AI's comprehension of metaphors by utilizing the sensicality ratings generated by sophisticated ChatGPT. To achieve this, we used psycholinguistic methods, and adopted the experimental materials from Lai, Currana, and Menna (2009). We found the surprisal values estimated from the autoregressive language model demonstrate metaphor processing that closely resembles that of native speakers. Furthermore, ChatGPT's processing of conventional metaphorical sentences closely resembles its approach to literal sentences, mirroring the convergence observed in native speakers' ERP response to conventional metaphorical sentences and their alignment with that of literal sentences.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14342/smog.2023.119.87"
    },
    {
        "id": 20108,
        "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
        "authors": "Luke Gessler, Nathan Schneider",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.17"
    },
    {
        "id": 20109,
        "title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models",
        "authors": "Sebastian Koch, Stefan Pasch",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386765"
    },
    {
        "id": 20110,
        "title": "Attacking a Transformer-Based Models for Arabic Language as Low Resources Language (LRL) Using Word-Substitution Methods",
        "authors": "Hanin Alshalan, Banafsheh Rekabdar",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00025"
    },
    {
        "id": 20111,
        "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
        "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.195"
    },
    {
        "id": 20112,
        "title": "Enhancing Text Summarization: Evaluating Transformer-Based Models and the Role of Large Language Models like ChatGPT",
        "authors": "Pınar Savcı, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391040"
    },
    {
        "id": 20113,
        "title": "Classical Machine Learning and Transformer Models for Offensive and Abusive Language Classification on Dziri Language",
        "authors": "Mohammed Mehdi Bouchene, Kheireddine Abainia",
        "published": "2023-9-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasa59624.2023.10286654"
    },
    {
        "id": 20114,
        "title": "Accurate, interpretable predictions of materials properties within transformer language models",
        "authors": "Vadim Korolev, Pavel Protsenko",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patter.2023.100803"
    },
    {
        "id": 20115,
        "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
        "authors": "Lina Conti, Guillaume Wisniewski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.641"
    },
    {
        "id": 20116,
        "title": "20.5 C-Transformer: A 2.6-18.1μJ/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models",
        "authors": "Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, Hoi-Jun Yoo",
        "published": "2024-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isscc49657.2024.10454330"
    },
    {
        "id": 20117,
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": "Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.459"
    },
    {
        "id": 20118,
        "title": "On the effect of dropping layers of pre-trained transformer models",
        "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov",
        "published": "2023-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csl.2022.101429"
    },
    {
        "id": 20119,
        "title": "Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models",
        "authors": "Behrad TaghiBeyglou, Frank Rudzicz",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100046"
    },
    {
        "id": 20120,
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": "Soo Ryu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.27"
    },
    {
        "id": 20121,
        "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
        "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.276"
    },
    {
        "id": 20122,
        "title": "Neural network architecture «transformer»: Artificial Intelligence and its role in Natural Language Processing",
        "authors": "A.M Evstropov, E.A Tarlakovskaya, I.A Sidorov",
        "published": "2023",
        "citations": 0,
        "abstract": "The transformer neural architecture has become a cornerstone of natural language processing (NLP) models. It is a powerful tool for language understanding and generation, enabling machines to process human language that is close to human understanding. NLP has significantly improved in recent years due to pro-gress in artificial intelligence (AI). One of the key developments that has enabled these improvements is the transformer neural network architecture. In this paper, I will explore the transformer architecture, its main concepts, and its application in NLP.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18411/trnio-05-2023-633"
    },
    {
        "id": 20123,
        "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
        "authors": "Chong Yu, Tao Chen, Zhongxue Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.15"
    },
    {
        "id": 20124,
        "title": "Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation",
        "authors": "Yuliang Cai, Jesse Thomason, Mohammad Rostami",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.466"
    },
    {
        "id": 20125,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 20126,
        "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
        "authors": "Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.3"
    },
    {
        "id": 20127,
        "title": "AI-Generated Text Detector for Arabic Language Using Encoder-Based Transformer Architecture",
        "authors": "Hamed Alshammari, Ahmed El-Sayed, Khaled Elleithy",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "The effectiveness of existing AI detectors is notably hampered when processing Arabic texts. This study introduces a novel AI text classifier designed specifically for Arabic, tackling the distinct challenges inherent in processing this language. A particular focus is placed on accurately recognizing human-written texts (HWTs), an area where existing AI detectors have demonstrated significant limitations. To achieve this goal, this paper utilized and fine-tuned two Transformer-based models, AraELECTRA and XLM-R, by training them on two distinct datasets: a large dataset comprising 43,958 examples and a custom dataset with 3078 examples that contain HWT and AI-generated texts (AIGTs) from various sources, including ChatGPT 3.5, ChatGPT-4, and BARD. The proposed architecture is adaptable to any language, but this work evaluates these models’ efficiency in recognizing HWTs versus AIGTs in Arabic as an example of Semitic languages. The performance of the proposed models has been compared against the two prominent existing AI detectors, GPTZero and OpenAI Text Classifier, particularly on the AIRABIC benchmark dataset. The results reveal that the proposed classifiers outperform both GPTZero and OpenAI Text Classifier with 81% accuracy compared to 63% and 50% for GPTZero and OpenAI Text Classifier, respectively. Furthermore, integrating a Dediacritization Layer prior to the classification model demonstrated a significant enhancement in the detection accuracy of both HWTs and AIGTs. This Dediacritization step markedly improved the classification accuracy, elevating it from 81% to as high as 99% and, in some instances, even achieving 100%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bdcc8030032"
    },
    {
        "id": 20128,
        "title": "How Are Idioms Processed Inside Transformer Language Models?",
        "authors": "Ye Tian, Isobel James, Hye Son",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.starsem-1.16"
    },
    {
        "id": 20129,
        "title": "A Survey of Transformer-Based Natural Language Processing Models",
        "authors": "鸣姝 赖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/airr.2023.123025"
    },
    {
        "id": 20130,
        "title": "Transformer-Based Language Models for Bulgarian",
        "authors": "Iva Marinova,  , Kiril Simov, Petya Osenova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_077"
    },
    {
        "id": 20131,
        "title": "A transformer-based architecture for the automatic detection of clickbait for Arabic headlines",
        "authors": "Jihad R’Baiti, Rdouan Faizi, Youssef Hmamouche, Amal El Fallah Seghrouchni",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnlp58431.2023.00052"
    },
    {
        "id": 20132,
        "title": "Computing Architecture for Large-Language Models (LLMs) and Large Multimodal Models (LMMs)",
        "authors": "Bor-Sung Liang",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3626184.3639692"
    },
    {
        "id": 20133,
        "title": "Leveraging Transformer-based Language Models for Enhanced Service Insight in Tourism",
        "authors": "Aleyna Er, Şuayb Talha Özçelik, Meltem Turhan Yöndem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391041"
    },
    {
        "id": 20134,
        "title": "Transformer-Based Conditional Language Models to Generate Filipino News Articles",
        "authors": "Kenrick Lance T. Buñag, Rosanna A. Esquivel",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.46254/an13.20230595"
    },
    {
        "id": 20135,
        "title": "HealthMavericks@MEDIQA-Chat 2023: Benchmarking different Transformer based models for Clinical Dialogue Summarization",
        "authors": "Kunal Suri, Saumajit Saha, Atul Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.50"
    },
    {
        "id": 20136,
        "title": "Zelda Rose: a tool for hassle-free training of transformer models",
        "authors": "Loïc Grobol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.6"
    },
    {
        "id": 20137,
        "title": "Analyzing Human and ChatGPT Responses: A Comparative Study of Transformer Models in Natural Language Processing",
        "authors": "Tunahan Gökçimen, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391042"
    },
    {
        "id": 20138,
        "title": "How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?",
        "authors": "Laura Zanella, Yannick Toussaint",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.12"
    },
    {
        "id": 20139,
        "title": "The evolution of transformer models from unidirectional to  bidirectional in Natural Language Processing",
        "authors": "Yihang Sun",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Transformer models have revolutionized Natural Language Processing (NLP), transitioning from traditional sequential models to innovative architectures based on attention mechanisms. The shift from unidirectional to bidirectional models has been a remarkable development in NLP. This paper mainly focuses on the evolution of NLP caused by Transformer models, with the transition from unidirectional to bidirectional modeling. This paper explores how the transformer model has revolutionized NLP, and the evolution from traditional sequential models to innovative attention-driven architectures. In this paper, it mainly discusses the limitations of traditional NLP models like RNNs, LSTMs and CNN when handling lengthy text sequences and complex dependencies, highlighting how transformer models, employing self-attention mechanisms and bidirectional modeling (e.g., BERT and GPT), have significantly improved NLP tasks. It provides a thorough review of the shift from unidirectional to bidirectional transformer models, offering insights into their utilization and development. Finally, this paper concludes with a summary and outlook for the entire study.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230794"
    },
    {
        "id": 20140,
        "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
        "authors": "Tharindu Madusanka, Iqra Zahid, Hao Li, Ian Pratt-Hartmann, Riza Batista-Navarro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.536"
    },
    {
        "id": 20141,
        "title": "Compact Transformer-based Language Models for the Moroccan Darija",
        "authors": "Mohamed Aghzal, Mohamed Amine El Bouni, Saad Driouech, Asmaa Mourhir",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cist56084.2023.10409912"
    },
    {
        "id": 20142,
        "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks",
        "authors": "Sunit Bhattacharya, Ondřej Bojar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.9"
    },
    {
        "id": 20143,
        "title": "Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models",
        "authors": "Alejo Lopez-Avila, Víctor Suárez-Paniagua",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.124"
    },
    {
        "id": 20144,
        "title": "Large Language Models (LLMs): Representation Matters, Low-Resource Languages and Multi-Modal Architecture",
        "authors": "Ganesh Mani, Galane Basha Namomsa",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/africon55910.2023.10293675"
    },
    {
        "id": 20145,
        "title": "Using of Transformer-Based Language Models to Separate Traffic Packets of Different Protocols",
        "authors": "Zalina Rusinova, Yury Chernyshov",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/redundancy59964.2023.10330184"
    },
    {
        "id": 20146,
        "title": "Leveraging Transformer Models in the Cyberbullying Text Classification System for the Low-resource Bengali Language",
        "authors": "Md. Nesarul Hoque, Md. Hanif Seddiqui",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441412"
    },
    {
        "id": 20147,
        "title": "The Generalization and Robustness of Transformer-Based Language Models on Commonsense Reasoning",
        "authors": "Ke Shen",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The advent of powerful transformer-based discriminative language models and, more recently, generative GPT-family models, has led to notable advancements in natural language processing (NLP), particularly in commonsense reasoning tasks. One such task is commonsense reasoning, where performance is usually evaluated through multiple-choice question-answering benchmarks. Till date, many such benchmarks have been proposed and `leaderboards' tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy on the task to an in-depth, context-sensitive probing of LLMs' generalization and robustness. To gain deeper insight into diagnosing these models' performance in commonsense reasoning scenarios, this thesis addresses three main studies: the generalization ability of transformer-based language models on commonsense reasoning, the trend in confidence distribution of these language models confronted with ambiguous inference tasks, and a proposed risk-centric evaluation framework for both discriminative and generative language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30410"
    },
    {
        "id": 20148,
        "title": "Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation",
        "authors": "Premanand Ghadekar, Neel Malwatkar, Nikhil Sontakke, Nirvisha Soni",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asiancon58793.2023.10270018"
    },
    {
        "id": 20149,
        "title": "Dynamic Low-rank Estimation for Transformer-based Language Models",
        "authors": "Ting Hua, Xiao Li, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, Hongxia Jin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.621"
    },
    {
        "id": 20150,
        "title": "Online Aggression Identification Using Ensembled Transformer-based Language Models",
        "authors": "Sneha Chinivar, Roopa M S, Arunalatha J S, Venugopal K R",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307921"
    },
    {
        "id": 20151,
        "title": "Cognitive decline assessment using semantic linguistic content and transformer deep learning architecture",
        "authors": "Rini PL, Gayathri KS",
        "published": "2023-11-16",
        "citations": 1,
        "abstract": "AbstractBackgroundDementia is a cognitive decline that leads to the progressive deterioration of an individual's ability to perform daily activities independently. As a result, a considerable amount of time and resources are spent on caretaking. Early detection of dementia can significantly reduce the effort and resources needed for caretaking.AimsThis research proposes an approach for assessing cognitive decline by analysing speech data, specifically focusing on speech relevance as a crucial indicator for memory recall.Methods & ProceduresThis is a cross‐sectional, online, self‐administered. The proposed method used deep learning architecture based on transformers, with BERT (Bidirectional Encoder Representations from Transformers) and Sentence‐Transformer to derive encoded representations of speech transcripts. These representations provide contextually descriptive information that is used to analyse the relevance of sentences in their respective contexts. The encoded information is then compared using cosine similarity metrics to measure the relevance of uttered sequences of sentences. The study uses the Pitt Corpus Dementia dataset for experimentation, which consists of speech data from individuals with and without dementia. The accuracy of the proposed multi‐QA‐MPNet (Multi‐Query Maximum Inner Product Search Pretraining) model is compared with other pretrained transformer models of Sentence‐Transformer.Outcomes & ResultsThe results show that the proposed approach outperforms the other models in capturing context level information, particularly semantic memory. Additionally, the study explores the suitability of different similarity measures to evaluate the relevance of uttered sequences of sentences. The experimentation reveals that cosine similarity is the most appropriate measure for this task.Conclusions & ImplicationsThis finding has significant implications for the early warning signs of dementia, as it suggests that cosine similarity metrics can effectively capture the semantic relevance of spoken language. The persistent cognitive decline over time acts as one of the indicators for prevalence of dementia. Additionally early dementia could be recognised by analysis on other modalities like speech and brain images.WHAT THIS PAPER ADDSWhat is already known on this subject\nIt is already known that speech‐ and language‐based detection methods can be useful for dementia diagnosis, as language difficulties are often early signs of the disease. Additionally, deep learning algorithms have shown promise in detecting and diagnosing dementia through analysing large datasets, particularly in speech‐ and language‐based detection methods. However, further research is needed to validate the performance of these algorithms on larger and more diverse datasets and to address potential biases and limitations.What this paper adds to existing knowledge\nThis study presents a unique and effective approach for cognitive decline assessment through analysing speech data. The study provides valuable insights into the importance of context and semantic memory in accurately detecting the potential in dementia and demonstrates the applicability of deep learning models for this purpose. The findings of this study have important clinical implications and can inform future research and development in the field of dementia detection and care.What are the potential or actual clinical implications of this work?\nThe proposed approach for cognitive decline assessment using speech data and deep learning models has significant clinical implications. It has the potential to improve the accuracy and efficiency of dementia diagnosis, leading to earlier detection and more effective treatments, which can improve patient outcomes and quality of life.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/1460-6984.12973"
    },
    {
        "id": 20152,
        "title": "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models",
        "authors": "Jaewan Choi, Jaehyun Park, Kwanhee Kyung, Nam Sung Kim, Jung Ho Ahn",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/hpca57654.2024.00052"
    },
    {
        "id": 20153,
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": "Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.931"
    },
    {
        "id": 20154,
        "title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models",
        "authors": "James O’Neill, Sourav Dutta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.114"
    },
    {
        "id": 20155,
        "title": "Transformer-based language models for mental health issues: A survey",
        "authors": "Candida M. Greco, Andrea Simeri, Andrea Tagarelli, Ester Zumpano",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.02.016"
    },
    {
        "id": 20156,
        "title": "Transformer Models for Recognizing Abusive Language An investigation and review on Tweeteval and SOLID dataset",
        "authors": "Fabeela Ali Rawther, Geevarghese Titus",
        "published": "2023-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceeict56924.2023.10157848"
    },
    {
        "id": 20157,
        "title": "BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bangla",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.17"
    },
    {
        "id": 20158,
        "title": "Time Series Forecasting of Air Pollutant PM2.5 Using Transformer Architecture",
        "authors": "K. Azhahudurai, V. Veeramanikandan",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21275/sr231125192357"
    },
    {
        "id": 20159,
        "title": "An Attention-Based Backend Allowing Efficient Fine-Tuning of Transformer Models for Speaker Verification",
        "authors": "Junyi Peng, Oldrich Plchot, Themos Stafylakis, Ladislav Mosner, Lukas Burget, Jan Cernocky",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022775"
    },
    {
        "id": 20160,
        "title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.34"
    },
    {
        "id": 20161,
        "title": "Automatic text summarization using transformer-based language models",
        "authors": "Ritika Rao, Sourabh Sharma, Nitin Malik",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13198-024-02280-4"
    },
    {
        "id": 20162,
        "title": "Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models",
        "authors": "Mohammadreza Banaei, Klaudia Bałazy, Artur Kasymov, Rémi Lebret, Jacek Tabor, Karl Aberer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.133"
    },
    {
        "id": 20163,
        "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
        "authors": "Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14722/ndss.2024.24115"
    },
    {
        "id": 20164,
        "title": "Transforming Text Generation in NLP: Deep Learning with GPT Models and 2023 Twitter Corpus Using Transformer Architecture",
        "authors": "Et al. Ghaith Alomari",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "This paper presents the design, implementation, and evaluation of a Transformer-based Generative Pre-trained Transformer (GPT) model tailored for character-level text generation. Leveraging the robust architecture of the Transformer, the model has been trained on a corpus sourced from social media text data, with the aim of exploring the intricacies of language patterns within a condensed and informal text setting. Key aspects of the model include a multi-head self-attention mechanism with a custom head configuration, positional embeddings, and layer normalization to promote stability in learning. It operates with a defined set of hyperparameters: a batch size of 32, a block size of 128, 200 iterations, a learning rate of 3e-4, and employs 4 attention heads across 4 layers with an embedding dimension of 384. The model has been optimized using the AdamW optimizer and includes regularization through dropout to prevent overfitting.Through a series of training iterations, the model demonstrates a converging behavior in loss metrics, indicating effective learning, and showcases the capacity to generate coherent text sequences post-training. Training and validation losses have been reported, revealing the nuances in model performance and generalization capabilities. The generated text samples postulate the model's potential in capturing the contextual flow of the dataset. This study further plots the loss curves, visually representing the training dynamics and convergence patterns. The final model, encapsulated within a PyTorch framework, presents a step forward in the realm of neural text generation, contributing to the ongoing advancements in language modeling and its applications in understanding and generating human-like text.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i9.9463"
    },
    {
        "id": 20165,
        "title": "Leveraging Large Language Models for Auto-remediation in Microservices Architecture",
        "authors": "Komal Sarda",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acsos-c58168.2023.00025"
    },
    {
        "id": 20166,
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": "Byung-Doh Oh, William Schuler",
        "published": "2023-3-27",
        "citations": 14,
        "abstract": "AbstractThis work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/tacl_a_00548"
    },
    {
        "id": 20167,
        "title": "Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)",
        "authors": "Wesley Morris, Scott Crossley, Langdon Holmes, Anne Trumbore",
        "published": "2023-3-13",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3576050.3576098"
    },
    {
        "id": 20168,
        "title": "Transformer-Based Composite Language Models for Text Evaluation and Classification",
        "authors": "Mihailo Škorić, Miloš Utvić, Ranka Stanković",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "Parallel natural language processing systems were previously successfully tested on the tasks of part-of-speech tagging and authorship attribution through mini-language modeling, for which they achieved significantly better results than independent methods in the cases of seven European languages. The aim of this paper is to present the advantages of using composite language models in the processing and evaluation of texts written in arbitrary highly inflective and morphology-rich natural language, particularly Serbian. A perplexity-based dataset, the main asset for the methodology assessment, was created using a series of generative pre-trained transformers trained on different representations of the Serbian language corpus and a set of sentences classified into three groups (expert translations, corrupted translations, and machine translations). The paper describes a comparative analysis of calculated perplexities in order to measure the classification capability of different models on two binary classification tasks. In the course of the experiment, we tested three standalone language models (baseline) and two composite language models (which are based on perplexities outputted by all three standalone models). The presented results single out a complex stacked classifier using a multitude of features extracted from perplexity vectors as the optimal architecture of composite language models for both tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11224660"
    },
    {
        "id": 20169,
        "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
        "authors": "Candida M. Greco, Andrea Tagarelli",
        "published": "2023-11-20",
        "citations": 1,
        "abstract": "AbstractTransformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10506-023-09374-7"
    },
    {
        "id": 20170,
        "title": "Sign Language-to-Text Dictionary with Lightweight Transformer Models",
        "authors": "Jérôme Fink, Pierre Poitier, Maxime André, Loup Meurice, Benoît Frénay, Anthony Cleve, Bruno Dumas, Laurence Meurant",
        "published": "2023-8",
        "citations": 0,
        "abstract": "The recent advances in deep learning have been beneficial to automatic sign language recognition (SLR). However, free-to-access, usable, and accessible tools are still not widely available to the deaf community. The need for a sign language-to-text dictionary was raised by a bilingual deaf school in Belgium and linguist experts in sign languages (SL) in order to improve the autonomy of students. To meet that need, an efficient SLR system was built based on a specific transformer model. The proposed system is able to recognize 700 different signs, with a top-10 accuracy of 83%. Those results are competitive with other systems in the literature while using 10 times less parameters than existing solutions. The integration of this model into a usable and accessible web application for the dictionary is also introduced. A user-centered human-computer interaction (HCI) methodology was followed to design and implement the user interface. To the best of our knowledge, this is the first publicly released sign language-to-text dictionary using video captured by a standard camera.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/662"
    },
    {
        "id": 20171,
        "title": "Analysis of transformer-based models for time series data, natural language processing, and computer vision",
        "authors": "Huanzhang Chen, Yunfan Hou, Tianhao Miao, Jiayu Xue",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The birth of the Transformer revolutionarily signalled the start of a new epic chapter in the deep learning era. Through an encoder-decoder architecture, including residual connection, multi-head self-attention, etc., it completely reformed the deep models and unified the models used in traditional computer vision (CV) and natural language processing (NLP) problems. In recent years, many papers published have adapted the original Transformer model to better complete tasks in time series analysis, CV, and NLP. In the area of natural language processing, Bidirectional Encoder Representations from Transformers (BERT) employs a two-way transformer structure to learn context-based language representation, whereas Generative Pre-trained Transformer (GPT) employs a one-way transformer but enhances corpus training to enhance the model effect. The Vision Transformer model is the cornerstone of computer vision. It separates the input image into various patches, projects each patch into vectorized features, and then passes the them to Transformer. Based on the idea of the Vision Transformer, Swin Transformer and Biformer further optimized the Transformer and achieved better results. Time series combines the ideas embodied in CV and NLP, and in doing so, improves the specificity and various difficulties of time series problems to lower algorithm complexity and increase prediction accuracy. This article summarizes the uses and improvements of the Transformer in NLP, CV and time series, explores the development history and ideas on algorithm optimization, and predicts the potential developments of Transformer in these three fields.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231084"
    },
    {
        "id": 20172,
        "title": "Formal Software Architecture Rule Learning: A Comparative Investigation between Large Language Models and Inductive Techniques",
        "authors": "Christian Schindler, Andreas Rausch",
        "published": "2024-2-20",
        "citations": 0,
        "abstract": "This paper explores the application of inferring software architecture rules from examples using Machine Learning (ML). We investigate different methods from Inductive Rule Learning and utilize Large Language Models (LLMs). Traditional manual rule specification approaches are time-consuming and error-prone, motivating the need for automated rule discovery. Leveraging a dataset of software architecture instances and a meta-model capturing implementation facts, we used inductive learning algorithms and LLMs to extract meaningful rules. The induced rules are evaluated against a predefined hypothesis and their generalizability across different system subsets is investigated. The research highlights the capabilities and limitations of ML-based rule learning in the area of software architecture, aiming to inspire further innovation in data-driven rule discovery for more intelligent software architecture practices.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics13050816"
    },
    {
        "id": 20173,
        "title": "Graph-Enhanced Transformer Architecture with Novel Use of CEFR Vocabulary Profile and Filled Pauses in Automated Speaking Assessment",
        "authors": "Jiun Ting Li, Tien-Hong Lo, Bi-Cheng Yan, Yung-Chang Hsu, Berlin Chen",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/slate.2023-21"
    },
    {
        "id": 20174,
        "title": "Defect transformer: An efficient hybrid transformer architecture for surface defect detection",
        "authors": "Junpu Wang, Guili Xu, Fuju Yan, Jinjin Wang, Zhengsheng Wang",
        "published": "2023-4",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.measurement.2023.112614"
    },
    {
        "id": 20175,
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "authors": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, Sumit Sanghai",
        "published": "2023",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.298"
    },
    {
        "id": 20176,
        "title": "An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models",
        "authors": "Sang-Soo Park, KyungSoo Kim, Jinin So, Jin Jung, Jonggeon Lee, Kyoungwan Woo, Nayeon Kim, Younghyun Lee, Hyungyo Kim, Yongsuk Kwon, Jinhyun Kim, Jieun Lee, YeonGon Cho, Yongmin Tai, Jeonghyeon Cho, Hoyoung Song, Jung Ho Ahn, Nam Sung Kim",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/hpca57654.2024.00078"
    },
    {
        "id": 20177,
        "title": "How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models",
        "authors": "Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "Abstract\nDespite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., kitchen chair: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/coli_a_00495"
    },
    {
        "id": 20178,
        "title": "Adapting transformer-based language models for heart disease detection and risk factors extraction",
        "authors": "Essam H. Houssein, Rehab E. Mohamed, Gang Hu, Abdelmgeid A. Ali",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "AbstractEfficiently treating cardiac patients before the onset of a heart attack relies on the precise prediction of heart disease. Identifying and detecting the risk factors for heart disease such as diabetes mellitus, Coronary Artery Disease (CAD), hyperlipidemia, hypertension, smoking, familial CAD history, obesity, and medications is critical for developing effective preventative and management measures. Although Electronic Health Records (EHRs) have emerged as valuable resources for identifying these risk factors, their unstructured format poses challenges for cardiologists in retrieving relevant information. This research proposed employing transfer learning techniques to automatically extract heart disease risk factors from EHRs. Leveraging transfer learning, a deep learning technique has demonstrated a significant performance in various clinical natural language processing (NLP) applications, particularly in heart disease risk prediction. This study explored the application of transformer-based language models, specifically utilizing pre-trained architectures like BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, BioClinicalBERT, XLNet, and BioBERT for heart disease detection and extraction of related risk factors from clinical notes, using the i2b2 dataset. These transformer models are pre-trained on an extensive corpus of medical literature and clinical records to gain a deep understanding of contextualized language representations. Adapted models are then fine-tuned using annotated datasets specific to heart disease, such as the i2b2 dataset, enabling them to learn patterns and relationships within the domain. These models have demonstrated superior performance in extracting semantic information from EHRs, automating high-performance heart disease risk factor identification, and performing downstream NLP tasks within the clinical domain. This study proposed fine-tuned five widely used transformer-based models, namely BERT, RoBERTa, BioClinicalBERT, XLNet, and BioBERT, using the 2014 i2b2 clinical NLP challenge dataset. The fine-tuned models surpass conventional approaches in predicting the presence of heart disease risk factors with impressive accuracy. The RoBERTa model has achieved the highest performance, with micro F1-scores of 94.27%, while the BERT, BioClinicalBERT, XLNet, and BioBERT models have provided competitive performances with micro F1-scores of 93.73%, 94.03%, 93.97%, and 93.99%, respectively. Finally, a simple ensemble of the five transformer-based models has been proposed, which outperformed the most existing methods in heart disease risk fan, achieving a micro F1-Score of 94.26%. This study demonstrated the efficacy of transfer learning using transformer-based models in enhancing risk prediction and facilitating early intervention for heart disease prevention.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40537-024-00903-y"
    },
    {
        "id": 20179,
        "title": "A Comparative Analysis of Transformer-based Protein Language Models for Remote Homology Prediction",
        "authors": "Anowarul Kabir, Asher Moldwin, Amarda Shehu",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3584371.3612942"
    },
    {
        "id": 20180,
        "title": "Semantics Squad at BLP-2023 Task 2: Sentiment Analysis of Bangla Text with Fine Tuned Transformer Based Models",
        "authors": "Krishno Dey, Md. Arid Hasan, Prerona Tarannum, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.41"
    },
    {
        "id": 20181,
        "title": "Modal Fusion Sign Language Recognition Based on Swin Transformer Architecture",
        "authors": "Yujie Gao, Yadong Li, Bin Liu, Yifan Tian, Minfeng Zhu, Jian Luo",
        "published": "2023-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icivc58118.2023.10270259"
    },
    {
        "id": 20182,
        "title": "Semantics Squad at BLP-2023 Task 1: Violence Inciting Bangla Text Detection with Fine-Tuned Transformer-Based Models",
        "authors": "Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.28"
    },
    {
        "id": 20183,
        "title": "Tolstoy’s Genius Explored by Deep Learning using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "Artificial Narrow Intelligence is in the phase of moving towards the AGN, which will attempt to decide as a human being. We are getting closer to it by each day, but AI actually is indefinite to many, although it is no different than any other set of mathematically defined computer operations in its core. Generating new data from a pre-trained model introduces new challenges to science & technology. In this work, the design of such an architecture from scratch, solving problems, and introducing alternative approaches are what has been conducted. Using a deep thinker, Tolstoy, as an object of study is a source of motivation for the entire research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.132306"
    },
    {
        "id": 20184,
        "title": "Assessment of bidirectional transformer encoder model and attention based bidirectional LSTM language models for fake news detection",
        "authors": "Anshika Choudhary, Anuja Arora",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jretconser.2023.103545"
    },
    {
        "id": 20185,
        "title": "Analysis of the evolution of advanced transformer-based language models: experiments on opinion mining",
        "authors": "Nour Eddine Zekaouiu, Siham Yousfi, Maryem Rhanoui, Mounia Mikram",
        "published": "2023-12-1",
        "citations": 1,
        "abstract": "<p>Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.</p>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i4.pp1995-2010"
    },
    {
        "id": 20186,
        "title": "Investigating the Effect of Discourse Connectives on Transformer Surprisal: Language Models Understand Connectives, Even So They Are Surprised",
        "authors": "Yan Cong, Emmanuele Chersoni, Yu-Yin Hsu, Philippe Blache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.17"
    },
    {
        "id": 20187,
        "title": "Transformer-based Models for Language Identification: A Comparative Study",
        "authors": "Bharathi Mohan G, R Prasanna Kumar, Elakkiya R, Venkatakrishnan R, Harrieni Shankar, Y Sree Harshitha, Harini K, M Nikhil Reddy",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscan58655.2023.10394757"
    },
    {
        "id": 20188,
        "title": "Tolstoy’s Genius Explored by Deep Learning Using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4670984"
    },
    {
        "id": 20189,
        "title": "Movie Caption Generation with Vision Transformer and Transformer-based Language Model",
        "authors": "Sorato Nakamura, Hidekazu Yanagimoto, Kiyota Hashimoto",
        "published": "2023-7-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iiai-aai59060.2023.00027"
    },
    {
        "id": 20190,
        "title": "Cyberbullying Text Identification based on Deep Learning and Transformer-based Language Models",
        "authors": "Khalid Saifullah, Muhammad Ibrahim Khan, Suhaima Jamal, Iqbal H. Sarker",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "In the contemporary digital age, social media platforms like Facebook, Twitter, and YouTube serve as vital channels for individuals to express ideas and connect with others. Despite fostering increased connectivity, these platforms have inadvertently given rise to negative behaviors, particularly cyberbullying. While extensive research has been conducted on high-resource languages such as English, there is a notable scarcity of resources for low-resource languages like Bengali, Arabic, Tamil, etc., particularly in terms of language modeling. This study addresses this gap by developing a cyberbullying text identification system called BullyFilterNeT tailored for social media texts, considering Bengali as a test case. The intelligent BullyFilterNeT system devised overcomes Out-of-Vocabulary (OOV) challenges associated with non-contextual embeddings and addresses the limitations of context-aware feature representations. To facilitate a comprehensive understanding, three non-contextual embedding models GloVe, FastText, and Word2Vec are developed for feature extraction in Bengali. These embedding models are utilized in the classification models, employing three statistical models (SVM, SGD, Libsvm), and four deep learning models (CNN, VDCNN, LSTM, GRU). Additionally, the study employs six transformer-based language models: mBERT, bELECTRA, IndicBERT, XML-RoBERTa, DistilBERT, and BanglaBERT, respectively to overcome the limitations of earlier models. Remarkably, BanglaBERT-based BullyFilterNeT achieves the highest accuracy of 88.04% in our test set, underscoring its effectiveness in cyberbullying text identification in the Bengali language.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4108/eetinis.v11i1.4703"
    },
    {
        "id": 20191,
        "title": "Detecting Tweets Containing Cannabidiol-Related COVID-19 Misinformation Using Transformer Language Models and Warning Letters From Food and Drug Administration: Content Analysis and Identification",
        "authors": "Jason Turner, Mehmed Kantardzic, Rachel Vickers-Smith, Andrew G Brown",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "\nBackground\nThe COVID-19 has introduced yet another opportunity to web-based sellers of loosely regulated substances, such as cannabidiol (CBD), to promote sales under false pretenses of curing the disease. Therefore, it has become necessary to innovate ways to identify such instances of misinformation.\n\n\nObjective\nWe sought to identify COVID-19 misinformation as it relates to the sales or promotion of CBD and used transformer-based language models to identify tweets semantically similar to quotes taken from known instances of misinformation. In this case, the known misinformation was the publicly available Warning Letters from Food and Drug Administration (FDA).\n\n\nMethods\nWe collected tweets using CBD- and COVID-19–related terms. Using a previously trained model, we extracted the tweets indicating commercialization and sales of CBD and annotated those containing COVID-19 misinformation according to the FDA definitions. We encoded the collection of tweets and misinformation quotes into sentence vectors and then calculated the cosine similarity between each quote and each tweet. This allowed us to establish a threshold to identify tweets that were making false claims regarding CBD and COVID-19 while minimizing the instances of false positives.\n\n\nResults\nWe demonstrated that by using quotes taken from Warning Letters issued by FDA to perpetrators of similar misinformation, we can identify semantically similar tweets that also contain misinformation. This was accomplished by identifying a cosine distance threshold between the sentence vectors of the Warning Letters and tweets.\n\n\nConclusions\nThis research shows that commercial CBD or COVID-19 misinformation can potentially be identified and curbed using transformer-based language models and known prior instances of misinformation. Our approach functions without the need for labeled data, potentially reducing the time at which misinformation can be identified. Our approach shows promise in that it is easily adapted to identify other forms of misinformation related to loosely regulated substances.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2196/38390"
    },
    {
        "id": 20192,
        "title": "Transformer la cité de transit de Beutre : pour une conception ouverte en architecture",
        "authors": "Marion Howa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56698/metropolitiques.1943"
    },
    {
        "id": 20193,
        "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
        "authors": "Katerina Papadimitriou, Gerasimos Potamianos",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-2198"
    },
    {
        "id": 20194,
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
        "authors": "Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.26"
    },
    {
        "id": 20195,
        "title": "Improving Natural Language Inference in Arabic Using Transformer Models and Linguistically Informed Pre-Training",
        "authors": "Mohammad Majd Saad Al Deen, Maren Pielka, Jörn Hees, Bouthaina Soulef Abdou, Rafet Sifa",
        "published": "2023-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371891"
    },
    {
        "id": 20196,
        "title": "How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery",
        "authors": "S. B. Atallah, N. R. Banda, A. Banda, N. A. Roeck",
        "published": "2023-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10151-023-02837-8"
    },
    {
        "id": 20197,
        "title": "Research and Application of Large Language Models in HealthcareCurrent Development of Large Language Models in the Healthcare FieldA Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare: A Framework for Applying Large Language Models and the Opportunities and Challenges of Large Language Models in Healthcare",
        "authors": "Chunfang Zhou, Qingyue Gong, Jinyang Zhu, Huidan Luan",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3644116.3644226"
    },
    {
        "id": 20198,
        "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
        "authors": "Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, Greg Ver Steeg",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.539"
    },
    {
        "id": 20199,
        "title": "Using Large Language Models in the Companion Cognitive Architecture: A Case Study and Future Prospects",
        "authors": "Constantine Nakos, Kenneth D. Forbus",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "The goal of the Companion cognitive architecture is to understand how to create human-like software social organisms.  Thus natural language capabilities, both for reading and conversation, are essential.  Recently we have begun experimenting with large language models as a component in the Companion architecture.  This paper summarizes a case study indicating why we are currently using BERT with our symbolic natural language understanding system.  It also describes some additional ways we are contemplating using large language models with Companions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaaiss.v2i1.27700"
    },
    {
        "id": 20200,
        "title": "BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language Generation",
        "authors": "Masaki Asada, Makoto Miwa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.34"
    },
    {
        "id": 20201,
        "title": "Captioning Remote Sensing Images Using Transformer Architecture",
        "authors": "Wrucha Nanal, Mohammadreza Hajiarbabi",
        "published": "2023-2-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiic57133.2023.10067039"
    },
    {
        "id": 20202,
        "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
        "authors": "Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, Peter Ramadge",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.102"
    },
    {
        "id": 20203,
        "title": "MSR92 Can Artificial Intelligence (AI) Large Language Models (LLMS) Such as Generative Pre-Trained Transformer (GPT) Be Used to Automate Literature Reviews?",
        "authors": "I. Guerra, J. Gallinaro, K. Rtveladze, A. Lambova, E. Asenova",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jval.2023.09.2151"
    },
    {
        "id": 20204,
        "title": "Exploring Transformer Model in Longitudinal Pharmacokinetic/Pharmacodynamic Analyses and Comparing with Alternative Natural Language Processing Models",
        "authors": "Yiming Cheng, Hongxiang Hu, Xin Dong, Xiaoran Hao, Yan Li",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.xphs.2024.02.008"
    },
    {
        "id": 20205,
        "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
        "authors": "Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song",
        "published": "2024-3-31",
        "citations": 24,
        "abstract": "Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3617680"
    },
    {
        "id": 20206,
        "title": "Molecular language models: RNNs or transformer?",
        "authors": "Yangyang Chen, Zixu Wang, Xiangxiang Zeng, Yayang Li, Pengyong Li, Xiucai Ye, Tetsuya Sakurai",
        "published": "2023-7-17",
        "citations": 1,
        "abstract": "Abstract\nLanguage models have shown the capacity to learn complex molecular distributions. In the field of molecular generation, they are designed to explore the distribution of molecules, and previous studies have demonstrated their ability to learn molecule sequences. In the early times, recurrent neural networks (RNNs) were widely used for feature extraction from sequence data and have been used for various molecule generation tasks. In recent years, the attention mechanism for sequence data has become popular. It captures the underlying relationships between words and is widely applied to language models. The Transformer-Layer, a model based on a self-attentive mechanism, also shines the same as the RNN-based model. In this research, we investigated the difference between RNNs and the Transformer-Layer to learn a more complex distribution of molecules. For this purpose, we experimented with three different generative tasks: the distributions of molecules with elevated scores of penalized LogP, multimodal distributions of molecules and the largest molecules in PubChem. We evaluated the models on molecular properties, basic metrics, Tanimoto similarity, etc. In addition, we applied two different representations of the molecule, SMILES and SELFIES. The results show that the two language models can learn complex molecular distributions and SMILES-based representation has better performance than SELFIES. The choice between RNNs and the Transformer-Layer needs to be based on the characteristics of dataset. RNNs work better on data focus on local features and decreases with multidistribution data, while the Transformer-Layer is more suitable when meeting molecular with larger weights and focusing on global features.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bfgp/elad012"
    },
    {
        "id": 20207,
        "title": "Am I hurt?: Evaluating Psychological Pain Detection in Hindi Text using Transformer-based Models",
        "authors": "Ravleen Kaur, M. P. S. Bhatia, Akshi Kumar",
        "published": "2024-3-5",
        "citations": 0,
        "abstract": "\n            The automated evaluation of pain is critical for developing effective pain management approaches that seek to alleviate while preserving patients’ functioning. Transformer-based models can aid in detecting pain from Hindi text data gathered from social media by leveraging their ability to capture complex language patterns and contextual information. By understanding the nuances and context of Hindi text, transformer models can effectively identify linguistic cues, sentiment and expressions associated with pain enabling the detection and analysis of pain-related content present in social media posts. The purpose of this research is to analyse the feasibility of utilizing NLP techniques to automatically identify pain within Hindi textual data, providing a valuable tool for pain assessment in Hindi-speaking populations. The research showcases the HindiPainNet model, a deep neural network that employs the IndicBERT model, classifying the dataset into two class labels {pain, no_pain} for detecting pain in Hindi textual data. The model is trained and tested using a novel dataset, दर्द-ए-शायरी (pronounced as\n            Dard-e-Shayari\n            ) curated using posts from social media platforms. The results demonstrate the model's effectiveness, achieving an accuracy of 70.5%. This pioneer research highlights the potential of utilizing textual data from diverse sources to identify and understand pain experiences based on psychosocial factors. This research could pave the path for the development of automated pain assessment tools that help medical professionals comprehend and treat pain in Hindi speaking populations. Additionally, it opens avenues to conduct further NLP-based multilingual pain detection research, addressing the needs of diverse language communities.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3650206"
    },
    {
        "id": 20208,
        "title": "Augmenting human innovation teams with artificial intelligence: Exploring transformer‐based language models",
        "authors": "Sebastian G. Bouschery, Vera Blazevic, Frank T. Piller",
        "published": "2023-3",
        "citations": 59,
        "abstract": "AbstractThe use of transformer‐based language models in artificial intelligence (AI) has increased adoption in various industries and led to significant productivity advancements in business operations. This article explores how these models can be used to augment human innovation teams in the new product development process, allowing for larger problem and solution spaces to be explored and ultimately leading to higher innovation performance. The article proposes the use of the AI‐augmented double diamond framework to structure the exploration of how these models can assist in new product development (NPD) tasks, such as text summarization, sentiment analysis, and idea generation. It also discusses the limitations of the technology and the potential impact of AI on established practices in NPD. The article establishes a research agenda for exploring the use of language models in this area and the role of humans in hybrid innovation teams. (Note: Following the idea of this article, GPT‐3 alone generated this abstract. Only minor formatting edits were performed by humans.)",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/jpim.12656"
    },
    {
        "id": 20209,
        "title": "Language Models for Everyone—Responsible and Transparent Development of Open Large Language Models",
        "authors": "Daniel Gillblad",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/cmsf2023008051"
    },
    {
        "id": 20210,
        "title": "Lightening-Transformer: A Dynamically-Operated Optically-Interconnected Photonic Transformer Accelerator",
        "authors": "Hanqing Zhu, Jiaqi Gu, Hanrui Wang, Zixuan Jiang, Zhekai Zhang, Rongxing Tang, Chenghao Feng, Song Han, Ray T. Chen, David Z. Pan",
        "published": "2024-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/hpca57654.2024.00059"
    },
    {
        "id": 20211,
        "title": "EmptyMind at BLP-2023 Task 2: Sentiment Analysis of Bangla Social Media Posts using Transformer-Based Models",
        "authors": "Karnis Fatema, Udoy Das, Md Ayon Mia, Md Sajidul Mowla, Mahshar Yahan, Md Fayez Ullah, Arpita Sarker, Hasan Murad",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.39"
    },
    {
        "id": 20212,
        "title": "Probabilistic generative transformer language models for generative design of molecules",
        "authors": "Lai Wei, Nihang Fu, Yuqi Song, Qian Wang, Jianjun Hu",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "AbstractSelf-supervised neural language models have recently found wide applications in the generative design of organic molecules and protein sequences as well as representation learning for downstream structure classification and functional prediction. However, most of the existing deep learning models for molecule design usually require a big dataset and have a black-box architecture, which makes it difficult to interpret their design logic. Here we propose the Generative Molecular Transformer (GMTransformer), a probabilistic neural network model for generative design of molecules. Our model is built on the blank filling language model originally developed for text processing, which has demonstrated unique advantages in learning the “molecules grammars” with high-quality generation, interpretability, and data efficiency. Benchmarked on the MOSES datasets, our models achieve high novelty and Scaf compared to other baselines. The probabilistic generation steps have the potential in tinkering with molecule design due to their capability of recommending how to modify existing molecules with explanation, guided by the learned implicit molecule chemistry. The source code and datasets can be accessed freely at https://github.com/usccolumbia/GMTransformer",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s13321-023-00759-z"
    },
    {
        "id": 20213,
        "title": "DTT: An Example-Driven Tabular Transformer for Joinability by Leveraging Large Language Models",
        "authors": "Arash Dargahi Nobari, Davood Rafiei",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Many organizations rely on data from government and third-party sources, and those sources rarely follow the same data formatting. This introduces challenges in integrating data from multiple sources or aligning external sources with internal databases. Commercial database systems do not offer adequate support for integrating data from heterogeneous sources, and manual integration is both time-consuming and inefficient. State-of-the-art data integration approaches that rely on similarity functions and textual transformations often fail to handle challenging cases where multiple mappings are required, or the mappings go beyond simple textual transformations.\nIn this paper, we study the potentials of deep neural models for transforming tables for joinability. In particular, we cast the problem as a prediction task and develop a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation. Our framework can efficiently learn the patterns for mapping a source formatting into an expected target using just a few examples, which can then be used for tasks such as table joining, filling in missing values, and error detection. Compared to state-of-the-art mapping and joining approaches, our framework delivers noticeably more accurate and scalable performance on both real-world and synthetic datasets. Our experimental evaluation also shows that the performance of the proposed framework using our fine-tuned model is at par or better than large language models such as GPT-3, despite the significant difference in size, and that using large language models within our framework improves their performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3639279"
    },
    {
        "id": 20214,
        "title": "Style Augmented Transformer Architecture for Automatic Essay Assessment",
        "authors": "Tirthankar Dasgupta, Gaurav K. Singh, Lipika Dey",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icalt58122.2023.00105"
    },
    {
        "id": 20215,
        "title": "Grammatical versus Spelling Error Correction: An Investigation into the Responsiveness of Transformer-Based Language Models Using BART and MarianMT",
        "authors": "Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, KS Suriya",
        "published": "2024-3-21",
        "citations": 0,
        "abstract": " Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilised to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text. This project aims at analysing different kinds of errors that occur in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%). ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219649224500370"
    },
    {
        "id": 20216,
        "title": "Low-light image enhancement based on Transformer and CNN architecture",
        "authors": "Keyuan Chen, Bin Chen, Shiqian Wu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326484"
    },
    {
        "id": 20217,
        "title": "Evolutionary neural architecture search combining multi-branch ConvNet and improved transformer",
        "authors": "Yang Xu, Yongjie Ma",
        "published": "2023-9-22",
        "citations": 4,
        "abstract": "AbstractDeep convolutional neural networks (CNNs) have achieved promising performance in the field of deep learning, but the manual design turns out to be very difficult due to the increasingly complex topologies of CNNs. Recently, neural architecture search (NAS) methods have been proposed to automatically design network architectures, which are superior to handcrafted counterparts. Unfortunately, most current NAS methods suffer from either highly computational complexity of generated architectures or limitations in the flexibility of architecture design. To address above issues, this article proposes an evolutionary neural architecture search (ENAS) method based on improved Transformer and multi-branch ConvNet. The multi-branch block enriches the feature space and enhances the representational capacity of a network by combining paths with different complexities. Since convolution is inherently a local operation, a simple yet powerful “batch-free normalization Transformer Block” (BFNTBlock) is proposed to leverage both local information and long-range feature dependencies. In particular, the design of batch-free normalization (BFN) and batch normalization (BN) mixed in the BFNTBlock blocks the accumulation of estimation shift ascribe to the stack of BN, which has favorable effects for performance improvement. The proposed method achieves remarkable accuracies, 97.24 $$\\%$$\n%\n and 80.06 $$\\%$$\n%\n on CIFAR10 and CIFAR100, respectively, with high computational efficiency, i.e. only 1.46 and 1.53 GPU days. To validate the universality of our method in application scenarios, the proposed algorithm is verified on two real-world applications, including the GTSRB and NEU-CLS dataset, and achieves a better performance than common methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-42931-3"
    },
    {
        "id": 20218,
        "title": "Efficient Structured Prediction with Transformer Encoders",
        "authors": "Ali Basirat",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "Finetuning is a useful method for adapting Transformer-based text encoders to new tasks but can be computationally expensive for structured prediction tasks that require tuning at the token level. Furthermore, finetuning is inherently inefficient in updating all base model parameters, which prevents parameter sharing across tasks. To address these issues, we propose a method for efficient task adaptation of frozen Transformer encoders based on the local contribution of their intermediate layers to token representations. Our adapter uses a novel attention mechanism to aggregate intermediate layers and tailor the resulting representations to a target task. Experiments on several structured prediction tasks demonstrate that our method outperforms previous approaches, retaining over 99% of the finetuning performance at a fraction of the training cost. Our proposed method offers an efficient solution for adapting frozen Transformer encoders to new tasks, improving performance and enabling parameter sharing across different tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3384/nejlt.2000-1533.2024.4932"
    },
    {
        "id": 20219,
        "title": "Applications of transformer-based language models in bioinformatics: a survey",
        "authors": "Shuang Zhang, Rui Fan, Yuti Liu, Shuang Chen, Qiao Liu, Wanwen Zeng",
        "published": "2023-1-5",
        "citations": 24,
        "abstract": "AbstractSummaryThe transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods.Supplementary informationSupplementary data are available at Bioinformatics Advances online.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioadv/vbad001"
    },
    {
        "id": 20220,
        "title": "Effectiveness of Data Augmentation and Ensembling Using Transformer-Based Models for Sentiment Analysis: Software Engineering Perspective",
        "authors": "Zubair Tusar, Sadat Sharfuddin, Muhtasim Abid, Md. Haque, Md. Mostafa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012092500003538"
    },
    {
        "id": 20221,
        "title": "DGA domain detection using pretrained character based transformer models",
        "authors": "Bronjon Gogoi, Tasiruddin Ahmed",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183602"
    },
    {
        "id": 20222,
        "title": "A literature review on multimodal deep learning models for detecting mental disorders in conversational data: Pre-transformer and transformer-based approaches",
        "authors": "Zilei Shao",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "This paper provides a comprehensive review of multimodal deep learning models that utilize conversational data to detect mental health disorders. In addition to discussing models based on the Transformer, such as BERT (Bidirectional Encoder Representations from Transformers), this paper addresses models that existed prior to the Transformer, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The paper covers the application of these models in the construction of multimodal deep learning systems to detect mental disorders. In addition, the difficulties encountered by multimodal deep learning systems are brought up. Furthermore, the paper proposes research directions for enhancing the performance and robustness of these models in mental health applications. By shedding light on the potential of multimodal deep learning in mental health care, this paper aims to foster further research and development in this critical domain.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/18/20230993"
    },
    {
        "id": 20223,
        "title": "Building Blocks for a Complex-Valued Transformer Architecture",
        "authors": "Florian Eilers, Xiaoyi Jiang",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095349"
    },
    {
        "id": 20224,
        "title": "Exploring Federated Learning to Trace Depression in Social Media with Language Models",
        "authors": "Arthur B. Vasconcelos, Lúcia Maria de A. Drummond, Rafaela C. Brum, Aline Paes",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sbac-padw60351.2023.00014"
    },
    {
        "id": 20225,
        "title": "Analyzing Software Architecture Documentation Models According to Agile Characteristics",
        "authors": "Leonardo Barreto, Tayana Conte",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011852300003467"
    },
    {
        "id": 20226,
        "title": "Untied Positional Encodings for Efficient Transformer-Based Speech Recognition",
        "authors": "Lahiru Samarakoon, Ivan Fung",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10023097"
    },
    {
        "id": 20227,
        "title": "Multimodal Very Short-Term Solar Irradiance Forecasting Using Sky Image-Numerical Fusion: Advanced Fusion Method Using the Gate Architecture and the Transformer Architecture",
        "authors": "Liwenbo Zhang, Robin Wilson, Mark Sumner, Yupeng Wu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4371894"
    },
    {
        "id": 20228,
        "title": "A Recommender Plug-in for Enterprise Architecture Models",
        "authors": "Sashikanth Raavikanti, Simon Hacks, Sotirios Katsikeas",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011709000003467"
    },
    {
        "id": 20229,
        "title": "Ertim at SemEval-2023 Task 2: Fine-tuning of Transformer Language Models and External Knowledge Leveraging for NER in Farsi, English, French and Chinese",
        "authors": "Kevin Deturck, Pierre Magistry, Bénédicte Diot-Parvaz Ahmad, Ilaine Wang, Damien Nouvel, Hugo Lafayette",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.semeval-1.306"
    },
    {
        "id": 20230,
        "title": "Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks for Document Layout Analysis",
        "authors": "",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "Document AI aims to automatically analyze documents by leveraging natural language processing and computer vision techniques. One of the major tasks of Document AI is document layout analysis, which structures document pages by interpreting the content and spatial relationships of layout, image, and text. This task can be image-centric, wherein the aim is to identify and label various regions such as authors and paragraphs, or text-centric, where the focus is on classifying individual words in a document. Although there are increasingly sophisticated methods for improving layout analysis, doubts remain about the extent to which their findings can be generalized to a broader context. Specifically, prior work developed systems based on very different architectures, such as transformer-based, graph-based, and CNNs. However, no work has mentioned the effectiveness of these models in a comparative analysis. Moreover, while language-independent Document AI models capable of knowledge transfer have been developed, it remains to be investigated to what degree they can effectively transfer knowledge. In this study, we aim to fill these gaps by conducting a comparative evaluation of state-of-the-art models in document layout analysis and investigating the potential of cross-lingual layout analysis by utilizing machine translation techniques.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.17"
    },
    {
        "id": 20231,
        "title": "CrudeBERT: Applying Economic Theory Towards Fine-Tuning Transformer-Based Sentiment Analysis Models to the Crude Oil Market",
        "authors": "Himmet Kaplan, Ralf-Peter Mundani, Heiko Rölke, Albert Weichselbraun",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011749600003467"
    },
    {
        "id": 20232,
        "title": "Impact of Transformer-Based Models and User Clustering in Early Fake News Detection in Social Media",
        "authors": "Sakshi Kalra, Yashvardhan Sharma, Mehul Agrawal, Sai Mantri, Gajendra Chauhan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011684000003411"
    },
    {
        "id": 20233,
        "title": "Towards supporting malleable architecture models",
        "authors": "Robbert Jongeling, Federico Ciccozzi",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsa-c57050.2023.00064"
    },
    {
        "id": 20234,
        "title": "Open Vocabulary Keyword Spotting with Small-Footprint ASR-based Architecture and Language Models",
        "authors": "Mikołaj Pudo, Mateusz Wosik, Artur Janicki",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15439/2023f8594"
    },
    {
        "id": 20235,
        "title": "An investigation into improving El Niño-Southern Oscillation prediction based on temporal transformer architecture",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.28919/cmbn/8371"
    },
    {
        "id": 20236,
        "title": "Microservices Architecture Language for Describing Service View",
        "authors": "Luka Lelovic, Michael Mathews, Amr Abdelfattah, Tomas Cerny",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011850200003488"
    },
    {
        "id": 20237,
        "title": "Paraphrase Generation Model Using Transformer Based Architecture",
        "authors": "Mosima Anna Masethe, Hlaudi Daniel Masethe, Sunday Olusegun Ojo, Pius A Owolawi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4683780"
    },
    {
        "id": 20238,
        "title": "Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation",
        "authors": "Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, Peter Ramadge",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.397"
    },
    {
        "id": 20239,
        "title": "Relaxed Attention for Transformer Models",
        "authors": "Timo Lohrenz, Björn Möller, Zhengyang Li, Tim Fingscheidt",
        "published": "2023-6-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191643"
    },
    {
        "id": 20240,
        "title": "Foundations of ANNs: Tolstoy’s Genius Explored using Transformer Architecture",
        "authors": "Shahriyar Guliyev",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "Artificial Narrow Intelligence is in the phase of moving towards the AGN, which will attempt to decide as a human being. We are getting closer to it by each day, but AI actually is indefinite to many, although it is no different than any other set of mathematically defined computer operations in its core. Generating new data from a pre-trained model introduces new challenges to science & technology. In this work, the design of such an architecture from scratch, solving problems, and introducing alternative approaches are what has been conducted. Using a deep thinker, Tolstoy, as an object of study is a source of motivation for the entire research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijaia.2024.15105"
    },
    {
        "id": 20241,
        "title": "Do transformer models do phonology like a linguist?",
        "authors": "Saliha Muradoglu, Mans Hulden",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.541"
    },
    {
        "id": 20242,
        "title": "Data Augmentation for Automated Essay Scoring using Transformer Models",
        "authors": "Kshitij Gupta",
        "published": "2023-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aisc56616.2023.10085523"
    },
    {
        "id": 20243,
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": "Liwen Hou, David Smith",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.230"
    },
    {
        "id": 20244,
        "title": "Detecting Fine-Grained Emotions from COVID-19 Tweets Using Transformer-Based Architecture",
        "authors": "Rida Javed Kutty, Nazura Javed, Rahul Mallya",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itis59651.2023.10420305"
    },
    {
        "id": 20245,
        "title": "Automatic assessment of divergent thinking in Chinese language with TransDis: A transformer-based language model approach",
        "authors": "Tianchen Yang, Qifan Zhang, Zhaoyang Sun, Yubo Hou",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3758/s13428-023-02313-z"
    },
    {
        "id": 20246,
        "title": "Bidirectional Transformer Reranker for Grammatical Error Correction",
        "authors": "Ying Zhang, Hidetaka Kamigaito, Manabu Okumura",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.31.3"
    },
    {
        "id": 20247,
        "title": "Generalization of finetuned transformer language models to new clinical contexts",
        "authors": "Kevin Xie, Samuel W Terman, Ryan S Gallagher, Chloe E Hill, Kathryn A Davis, Brian Litt, Dan Roth, Colin A Ellis",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "Abstract\n\nObjective\nWe have previously developed a natural language processing pipeline using clinical notes written by epilepsy specialists to extract seizure freedom, seizure frequency text, and date of last seizure text for patients with epilepsy. It is important to understand how our methods generalize to new care contexts.\n\n\nMaterials and methods\nWe evaluated our pipeline on unseen notes from nonepilepsy-specialist neurologists and non-neurologists without any additional algorithm training. We tested the pipeline out-of-institution using epilepsy specialist notes from an outside medical center with only minor preprocessing adaptations. We examined reasons for discrepancies in performance in new contexts by measuring physical and semantic similarities between documents.\n\n\nResults\nOur ability to classify patient seizure freedom decreased by at least 0.12 agreement when moving from epilepsy specialists to nonspecialists or other institutions. On notes from our institution, textual overlap between the extracted outcomes and the gold standard annotations attained from manual chart review decreased by at least 0.11 F1 when an answer existed but did not change when no answer existed; here our models generalized on notes from the outside institution, losing at most 0.02 agreement. We analyzed textual differences and found that syntactic and semantic differences in both clinically relevant sentences and surrounding contexts significantly influenced model performance.\n\n\nDiscussion and conclusion\nModel generalization performance decreased on notes from nonspecialists; out-of-institution generalization on epilepsy specialist notes required small changes to preprocessing but was especially good for seizure frequency text and date of last seizure text, opening opportunities for multicenter collaborations using these outcomes.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/jamiaopen/ooad070"
    },
    {
        "id": 20248,
        "title": "Blending Dependency Parsers With Language Models",
        "authors": "Nicos Isaak",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011781800003393"
    },
    {
        "id": 20249,
        "title": "Detecting Hate Speech Utilizing Deep Convolutional Network and Transformer Models",
        "authors": "Utkarsh Mittal",
        "published": "2023-8-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/elexcom58812.2023.10370502"
    },
    {
        "id": 20250,
        "title": "Explainable Large Language Models &amp; iContracts",
        "authors": "Georgios Stathis",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012607400003636"
    },
    {
        "id": 20251,
        "title": "Investigating Lightweight Transformer Models for Defect Detection",
        "authors": "Hanyun Wang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "In industrial production, product defect detection is vital for quality control. Traditional manual inspection is inefficient and error-prone. Deep learning, particularly in image processing, has enabled computer-based automated defect detection. This paper proposes a Visual Transformer-based model to overcome limitations in industrial anomaly detection. Leveraging pretrained Vision Transformer and Point Transformer models, it extracts features from RGB images and point cloud data. Multimodal feature fusion enhances anomaly perception, with residual connections mitigating feature loss. On the MVTec AD dataset, it achieves 96.3% AU PRO for anomaly detection and 99.3% Pixel ROCAUC for anomaly segmentation. To enable deployment on devices like Raspberry Pi, the paper introduces a lightweight model via post-training quantization and pruning. This results in a 28.52% inference speedup with only a 1.08% average detection accuracy drop, facilitating practical industrial applications on compact devices.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/ajst.v7i3.12694"
    },
    {
        "id": 20252,
        "title": "Time Series Forecasting with Transformer Models and Application to Asset Management",
        "authors": "Edmond Lezmi, Jiali Xu",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4375798"
    },
    {
        "id": 20253,
        "title": "Design and Experimental Analysis of a Modular Smart Transformer Architecture",
        "authors": "Levy F. Costa, Youngjong Ko, Marco Liserre",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ecce53617.2023.10362128"
    },
    {
        "id": 20254,
        "title": "Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer",
        "authors": " Mohanad Sameer,  Ahmed Talib,  Alla Hussein",
        "published": "2023-3-21",
        "citations": 2,
        "abstract": "Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when utilizing the Common Voice dataset from Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 hours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition systems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training requires a very large amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.51173/jt.v5i1.749"
    },
    {
        "id": 20255,
        "title": "Language models in automated essay scoring: Insights for the Turkish language",
        "authors": "Tahereh FİROOZİ, Okan BULUT, Mark GİERL",
        "published": "2023-12-27",
        "citations": 0,
        "abstract": "The proliferation of large language models represents a paradigm shift in the landscape of automated essay scoring (AES) systems, fundamentally elevating their accuracy and efficacy. This study presents an extensive examination of large language models, with a particular emphasis on the transformative influence of transformer-based models, such as BERT, mBERT, LaBSE, and GPT, in augmenting the accuracy of multilingual AES systems. The exploration of these advancements within the context of the Turkish language serves as a compelling illustration of the potential for harnessing large language models to elevate AES performance in in low-resource linguistic environments. Our study provides valuable insights for the ongoing discourse on the intersection of artificial intelligence and educational assessment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.21449/ijate.1394194"
    },
    {
        "id": 20256,
        "title": "UsingWikidata for Enhancing Compositionality in Pre-trained Language Models",
        "authors": "Meriem Beloucif,  , Mihir Bansal, Chris Biemann,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_019"
    },
    {
        "id": 20257,
        "title": "Load Autoformer: A Transformer architecture for short-term load forecasting",
        "authors": "Yuzhe Huang, Fan Mo, Zitong Zhang, Chenghan Li, Kan Li",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ispec58282.2023.10402999"
    },
    {
        "id": 20258,
        "title": "A Transformer-based Neural Architecture Search Method",
        "authors": "Shang Wang, Huanrong Tang, Jianquan Ouyang",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583133.3590735"
    },
    {
        "id": 20259,
        "title": "Adaptation of Multilingual T5 Transformer for Indonesian Language",
        "authors": "Mukhlish Fuadi, Adhi Dharma Wibawa, Surya Sumpeno",
        "published": "2023-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itis59651.2023.10420049"
    },
    {
        "id": 20260,
        "title": "Semanformer: Semantics-aware Embedding Dimensionality Reduction Using Transformer-Based Models",
        "authors": "Mallika Boyapati, Ramazan Aygun",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc59802.2024.00027"
    },
    {
        "id": 20261,
        "title": "Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations",
        "authors": "Patrícia Pereira, Rui Ribeiro, Helena Moniz, Luisa Coheur, Joao Paulo Carvalho",
        "published": "2023-8-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/fuzz52849.2023.10309719"
    },
    {
        "id": 20262,
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
        "authors": "Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.225"
    },
    {
        "id": 20263,
        "title": "Facilitating the Learning Engineering Process for Educational Conversational Modules Using Transformer-Based Language Models",
        "authors": "Behzad Mirzababaei, Viktoria Pammer-Schindler",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tlt.2024.3367738"
    },
    {
        "id": 20264,
        "title": "Blockwise compression of transformer-based models without retraining",
        "authors": "Gaochen Dong, W. Chen",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.001"
    },
    {
        "id": 20265,
        "title": "Insights into the inner workings of transformer models for protein function prediction",
        "authors": "Markus Wenzel, Erik Grüner, Nils Strodthoff",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "Abstract\n\nMotivation\nWe explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too.\n\n\nResults\nThe approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins.\n\n\nAvailability and Implementation\nSource code can be accessed at https://github.com/markuswenzel/xai-proteins.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioinformatics/btae031"
    },
    {
        "id": 20266,
        "title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
        "authors": "Aleksa Cvetanović, Predrag Tadić",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/telfor59449.2023.10372792"
    },
    {
        "id": 20267,
        "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
        "authors": "Junyi Peng, Oldřich Plchot, Themos Stafylakis, Ladislav Mosner, Lukáš Burget, Jan \"Honza\" Černocký",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-453"
    },
    {
        "id": 20268,
        "title": "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models",
        "authors": "Jaewan Choi, Jaehyun Park, Kwanhee Kyung, Nam Sung Kim, Jung Ho Ahn",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lca.2023.3305386"
    },
    {
        "id": 20269,
        "title": "An Efficient ConvNet for Learned Image Compression with Transformer-Style Architecture",
        "authors": "Haihang Ruan, Feng Wang, Yan Wang",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/vcip59821.2023.10402780"
    },
    {
        "id": 20270,
        "title": "Searching Efficient Neural Architecture with Multi-resolution Fusion Transformer for Appearance-based Gaze Estimation",
        "authors": "Vikrant Nagpure, Kenji Okuma",
        "published": "2023-1",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00095"
    },
    {
        "id": 20271,
        "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
        "authors": "Kangjie Cao, Ting Zhang, Jueqiao Huang",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "AbstractIn the field of engineering systems—particularly in underground drilling and green stormwater management—real-time predictions are vital for enhancing operational performance, ensuring safety, and increasing efficiency. Addressing this niche, our study introduces a novel LSTM-transformer hybrid architecture, uniquely specialized for multi-task real-time predictions. Building on advancements in attention mechanisms and sequence modeling, our model integrates the core strengths of LSTM and Transformer architectures, offering a superior alternative to traditional predictive models. Further enriched with online learning, our architecture dynamically adapts to variable operational conditions and continuously incorporates new field data. Utilizing knowledge distillation techniques, we efficiently transfer insights from larger, pretrained networks, thereby achieving high predictive accuracy without sacrificing computational resources. Rigorous experiments on sector-specific engineering datasets validate the robustness and effectiveness of our approach. Notably, our model exhibits clear advantages over existing methods in terms of predictive accuracy, real-time adaptability, and computational efficiency. This work contributes a pioneering predictive framework for targeted engineering applications, offering actionable insights into.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-024-55483-x"
    },
    {
        "id": 20272,
        "title": "Enhancing Endometrial Tumor Detection: Early Diagnosis with Advanced Vision Transformer Architecture",
        "authors": "Bhawna Swarnkar, Nilay Khare, Manasi Gyanchandani",
        "published": "2024-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sceecs61402.2024.10482329"
    },
    {
        "id": 20273,
        "title": "Hierarchical Large Language Models in Cloud-Edge-End Architecture for Heterogeneous Robot Cluster Control",
        "authors": "Zhirong Luan, Yujun Lai, Rundong Huang, Yan Yan, Jingwei Wang, Jizhou Lu, Badong Chen",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640771.3643717"
    },
    {
        "id": 20274,
        "title": "Scenario-based models of architecture objects for main streets",
        "authors": "Serhii Buravchenko",
        "published": "2023-4-14",
        "citations": 0,
        "abstract": "The formulation of the problem - the design of main streets is problematic because their construction and reconstruction is carried out by separate objects, often without taking into account the overall composition of the street, without the need to organize orientation when moving in transport or create an aesthetic sequence.\r\nPublications related to the composition of main and main streets mainly cover the period of their active design as integral objects and are limited to purely theoretical questions regarding visual perception. At the same time, the development of visualization makes it possible to present the scenario of opening the street in real time.\r\nThe purpose of this publication is to define and justify the scenario models of the organization of the main street based on the interpretation of art forms unfolding in time, as well as additional methods of specifying the requirements for background and accent buildings of the main streets.\r\nThe novelty of the publication lies in the interpretation of the main street as an object of scenario organization, the classification of scenarios based on prototypes, and the specification of requirements for characteristic types of second-order architectural objects - individual buildings and complexes.\r\nResearch methods are based on the analysis of the perception of the composition of the street as a whole, and individual parts, which at the same time can be objects of other spaces or \"nested\" urban structures.\r\nThe result of the study is the systematization of second-order architectural objects and the requirements for them related to their role in the composition of the main street.\r\nMethodical approaches and models of architectural objects are revealed, depending on the requirements for their visual perception.\r\nIn further research, the author intends to conduct an analysis of the combination in certain spaces of requirements for the perception of traffic and pedestrians, the principles of differentiation of spaces for different conditions of use.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32347/2077-3455.2023.66.14-31"
    },
    {
        "id": 20275,
        "title": "An ensemble novel architecture for Bangla Mathematical Entity Recognition (MER) using transformer based learning",
        "authors": "Tanjim Taharat Aurpa, Md Shoaib Ahmed",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2024.e25467"
    },
    {
        "id": 20276,
        "title": "Multimodal Transformer for Risk Classification: Analyzing the Impact of Different Data Modalities",
        "authors": "Niklas Holtz, Jorge Marx Gomez",
        "published": "2023-5-20",
        "citations": 1,
        "abstract": "Risk classification plays a critical role in domains such as finance, insurance, and healthcare. However, identifying risks can be a challenging task when dealing with different types of data. In this paper, we present a novel approach using the Multimodal Transformer for risk classification, and we investigate the use of data augmentation for risk data through automated retrieval of news articles. We achieved this through keyword extraction based on the title and descriptions of risks and using various selection metrics. We evaluate our approach using a real-world dataset containing numerical, categorical, and textual data. Our results demonstrate that the use of the Multimodal Transformer for risk classification outperforms other models that only utilize textual data. We show that the inclusion of numerical and categorical data improves the performance of the model, particularly for risks that are difficult to classify based on textual data alone. Additionally, our research indicates that the utilization of data augmentation techniques yields enhanced performance outcomes in models. This methodology presents a promising avenue for enterprises to effectively mitigate risks and make well-informed decisions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130803"
    },
    {
        "id": 20277,
        "title": "Can Language Models Solve Complex Subsurface Data Integrations: Building Subsurface Copilots with Large Language Models (LLMs)",
        "authors": "T.B. Grant, J. Goldwater, E. Knudsen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202439022"
    },
    {
        "id": 20278,
        "title": "Transformer-based text similarity and second language proficiency: A case of written production by learners of Korean",
        "authors": "Gyu-Ho Shin, Boo Kyung Jung, Seongmin Mun",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2024.100060"
    },
    {
        "id": 20279,
        "title": "\"TRANSFORMER LANGUAGE MODEL-BASED MOBILE LEARNING SOLUTION FOR HIGHER EDUCATION\"",
        "authors": "",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33965/es_ml2023_202302l048"
    },
    {
        "id": 20280,
        "title": "Deriving Language Models from Masked Language Models",
        "authors": "Lucas Torroba Hennigen, Yoon Kim",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.99"
    },
    {
        "id": 20281,
        "title": "Smart Home Notifications in Croatian Language: A Transformer-Based Approach",
        "authors": "Magdalena Simunec, Renato Soic",
        "published": "2023-7-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/contel58387.2023.10198978"
    },
    {
        "id": 20282,
        "title": "Arabic sign language letters recognition using Vision Transformer",
        "authors": "Aya F. Alnabih, Ashraf Y. Maghari",
        "published": "2024-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18681-3"
    },
    {
        "id": 20283,
        "title": "Genealogical Relationship Extraction from Unstructured Text Using Fine-Tuned Transformer Models",
        "authors": "Carloangello Parrolivelli, Lubomir Stanchev",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsc56153.2023.00035"
    },
    {
        "id": 20284,
        "title": "Persian Ezafeh Recognition using Transformer-Based Models",
        "authors": "Ali Ansari, Zahra Ebrahimian, Ramin Toosi, Mohammad Ali Akhaee",
        "published": "2023-5-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icwr57742.2023.10139204"
    },
    {
        "id": 20285,
        "title": "\"TRANSFORMER LANGUAGE MODEL-BASED MOBILE LEARNING SOLUTION FOR HIGHER EDUCATION\"",
        "authors": "",
        "published": "2023-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33965/es_ml2023_202301l048"
    },
    {
        "id": 20286,
        "title": "An enhancement of transformer-based architecture with randomized regularization for wind speed prediction",
        "authors": "Tham Vo",
        "published": "2023-1-30",
        "citations": 0,
        "abstract": "The wind power is considered as a potential renewable energy resource which requires less management cost and effort than the others like as tidal, geothermal, etc. However, the natural randomization and volatility aspects of wind in different regions have brought several challenges for efficiently as well as reliably operating the wind-based power supply grid. Thus, it is necessary to have centralized monitoring centers for managing as well as optimizing the performance of wind power farms. Among different management task, wind speed prediction is considered as an important task which directly support for further wind-based power supply resource planning/optimization, hence towards power shortage risk and operating cost reductions. Normally, considering as traditional time-series based prediction problem, most of previous deep learning-based models have demonstrated significant improvement in accuracy performance of wind speed prediction problem. However, most of recurrent neural network (RNN) as well as sequential auto-encoding (AE) based architectures still suffered several limitations related to the capability of sufficient preserving the spatiotemporal and long-range time dependent information of complex time-series based wind datasets. Moreover, previous RNN-based wind speed predictive models also perform poor prediction results within high-complex/noised time-series based wind speed datasets. Thus, in order to overcome these limitations, in this paper we proposed a novel integrated convolutional neural network (CNN)-based spatiotemporal randomization mechanism with transformer-based architecture for wind speed prediction problem, called as: RTrans-WP. Within our RTrans-WP model, we integrated the deep neural encoding component with a randomized CNN learning mechanism to softy align temporal feature within the long-range time-dependent learning context. The utilization of randomized CNN component at the data encoding part also enables to reduce noises and time-series based observation uncertainties which are occurred during the data representation learning and wind speed prediction-driven fine-tuning processes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/jifs-222446"
    },
    {
        "id": 20287,
        "title": "Hybrid UNet transformer architecture for ischemic stoke segmentation with MRI and CT datasets",
        "authors": "Wei Kwek Soh, Jagath C. Rajapakse",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "A hybrid UNet and Transformer (HUT) network is introduced to combine the merits of the UNet and Transformer architectures, improving brain lesion segmentation from MRI and CT scans. The HUT overcomes the limitations of conventional approaches by utilizing two parallel stages: one based on UNet and the other on Transformers. The Transformer-based stage captures global dependencies and long-range correlations. It uses intermediate feature vectors from the UNet decoder and improves segmentation accuracy by enhancing the attention and relationship modeling between voxel patches derived from the 3D brain volumes. In addition, HUT incorporates self-supervised learning on the transformer network. This allows the transformer network to learn by maintaining consistency between the classification layers of the different resolutions of patches and augmentations. There is an improvement in the rate of convergence of the training and the overall capability of segmentation. Experimental results on benchmark datasets, including ATLAS and ISLES2018, demonstrate HUT's advantage over the state-of-the-art methods. HUT achieves higher Dice scores and reduced Hausdorff Distance scores in single-modality and multi-modality lesion segmentation. HUT outperforms the state-the-art network SPiN in the single-modality MRI segmentation on Anatomical Tracings of lesion After Stroke (ATLAS) dataset by 4.84% of Dice score and a large margin of 40.7% in the Hausdorff Distance score. HUT also performed well on CT perfusion brain scans in the Ischemic Stroke Lesion Segmentation (ISLES2018) dataset and demonstrated an improvement over the recent state-of-the-art network USSLNet by 3.3% in the Dice score and 12.5% in the Hausdorff Distance score. With the analysis of both single and multi-modalities datasets (ATLASR12 and ISLES2018), we show that HUT can perform and generalize well on different datasets.Code is available at: https://github.com/vicsohntu/HUT_CT.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnins.2023.1298514"
    },
    {
        "id": 20288,
        "title": "Towards Lightweight Transformer Architecture: an Analysis on Semantic Segmentation",
        "authors": "Bohdan Ivaniuk-Skulskyi, Nadiya Shvai, Arcadi Llanza, Amir Nakib",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467926"
    },
    {
        "id": 20289,
        "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
        "authors": "Aitor Ormazabal, Mikel Artetxe, Eneko Agirre",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.180"
    },
    {
        "id": 20290,
        "title": "CardioBERTpt: Transformer-based Models for Cardiology Language Representation in Portuguese",
        "authors": "Elisa Terumi Rubel Schneider, Yohan Bonescki Gumiel, João Vitor Andrioli de Souza, Lilian Mie Mukai, Lucas Emanuel Silva e Oliveira, Marina de Sa Rebelo, Marco Antonio Gutierrez, Jose Eduardo Krieger, Douglas Teodoro, Claudia Moro, Emerson Cabrera Paraiso",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cbms58004.2023.00247"
    },
    {
        "id": 20291,
        "title": "Stellenwert von Natural Language Processing und chatbasierten Generative Language Models",
        "authors": "Markus Haar, Michael Sonntagbauer, Stefan Kluge",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00063-023-01098-5"
    },
    {
        "id": 20292,
        "title": "Localizing in-domain adaptation of transformer-based biomedical language models",
        "authors": "Tommaso Mario Buonocore, Claudio Crema, Alberto Redolfi, Riccardo Bellazzi, Enea Parimbelli",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jbi.2023.104431"
    },
    {
        "id": 20293,
        "title": "Pipeline Signed Japanese Translation Using PBSMT and Transformer in a Low-Resource Setting",
        "authors": "Ken Yano, Akira Utsumi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.30.30"
    },
    {
        "id": 20294,
        "title": "Evaluating Unsupervised Hierarchical Topic Models Using a Labeled Dataset",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_091"
    },
    {
        "id": 20295,
        "title": "Embodied human language models vs. Large Language Models, or why Artificial Intelligence cannot explain the modal be able to",
        "authors": "Sergio Torres-Martínez",
        "published": "2024-2-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12304-024-09553-2"
    },
    {
        "id": 20296,
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": "Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-910"
    },
    {
        "id": 20297,
        "title": "Transformer-Based Lip-Reading with Regularized Dropout and Relaxed Attention",
        "authors": "Zhengyang Li, Timo Lohrenz, Matthias Dunkelberg, Tim Fingscheidt",
        "published": "2023-1-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10023442"
    },
    {
        "id": 20298,
        "title": "QGFORMER: Quantum-Classical Hybrid Transformer Architecture for Gravitational Wave Detection",
        "authors": "Hu Jiaxiang, Liu Jiale",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccwamtip60502.2023.10387052"
    },
    {
        "id": 20299,
        "title": "Word Syllabification for Indonesian Language using Transformer",
        "authors": "Muhammad Haykal Kamil, Suyanto Suyanto, Mochammad Arif Bijaksana",
        "published": "2023-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isitia59021.2023.10221089"
    },
    {
        "id": 20300,
        "title": "Sparse Universal Transformer",
        "authors": "Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.12"
    },
    {
        "id": 20301,
        "title": "Identifying and Classifying Fake COVID-19 Tweets using Transformer Models",
        "authors": "Yasmine Eid Mahmoud, Farid Ali Mousa, Ayat Mahmoud",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/imsa58542.2023.10217786"
    },
    {
        "id": 20302,
        "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
        "authors": "Jan Lehečka, Jan Švec, Josef V. Psutka, Pavel Ircing",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-872"
    },
    {
        "id": 20303,
        "title": "OGT Transformer-Based Bengali Language Model for Text Generation with 100M Parameters",
        "authors": "",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets47913"
    },
    {
        "id": 20304,
        "title": "Prompt Engineering or Fine-Tuning? A Case Study on Phishing Detection with Large Language Models",
        "authors": "Fouad Trad, Ali Chehab",
        "published": "2024-2-6",
        "citations": 2,
        "abstract": "Large Language Models (LLMs) are reshaping the landscape of Machine Learning (ML) application development. The emergence of versatile LLMs capable of undertaking a wide array of tasks has reduced the necessity for intensive human involvement in training and maintaining ML models. Despite these advancements, a pivotal question emerges: can these generalized models negate the need for task-specific models? This study addresses this question by comparing the effectiveness of LLMs in detecting phishing URLs when utilized with prompt-engineering techniques versus when fine-tuned. Notably, we explore multiple prompt-engineering strategies for phishing URL detection and apply them to two chat models, GPT-3.5-turbo and Claude 2. In this context, the maximum result achieved was an F1-score of 92.74% by using a test set of 1000 samples. Following this, we fine-tune a range of base LLMs, including GPT-2, Bloom, Baby LLaMA, and DistilGPT-2—all primarily developed for text generation—exclusively for phishing URL detection. The fine-tuning approach culminated in a peak performance, achieving an F1-score of 97.29% and an AUC of 99.56% on the same test set, thereby outperforming existing state-of-the-art methods. These results highlight that while LLMs harnessed through prompt engineering can expedite application development processes, achieving a decent performance, they are not as effective as dedicated, task-specific LLMs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/make6010018"
    }
]