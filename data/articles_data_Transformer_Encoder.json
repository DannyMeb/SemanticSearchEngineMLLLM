[
    {
        "id": 10060,
        "title": "Regional Flood Inundation Nowcast Using Double-Encoder Transformer",
        "authors": "Shao-Kun Shiu, Li-Chiu Chang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the context of rapid global population growth and extensive economic development, urbanization is expanding rapidly. The expansion of urbanization brings about increasingly complex challenges for cities, and flooding is one of the disasters faced. Climate change has led to a significant increase in extreme hydrological events, particularly a sharp rise in rainfall intensity, further elevating the risk of flooding in low-lying urban areas. The study area is located in Taipei City, characterized by low-lying terrain surrounded by mountains, and is influenced by subtropical climate. The frequent occurrence of heavy rainfall during the monsoon season and typhoons contributes to frequent flooding events, with the additional impact of climate change increasing the risk of intense rainfall. Therefore, the real-time prediction of regional flooding and its application in urban management becomes an imperative task, aiding in early warning, effective flood risk response, and ensuring sustainable urban development.\nThis study utilizes the Double-Encoder Transformer model for real-time flood forecasting leveraging dual-encoder architecture to process and analyze diverse data types relevant predicting floods. One encoder could be dedicated to interpreting meteorological data, such as rainfall spatial distribution. This encoder focuses on extracting and understanding the complex patterns in weather-related data, which are crucial for predicting the likelihood of flooding. The second encoder, on the other hand, could handle geographical and environmental data, including terrain topology, and land use patterns. This encoder is adept at understanding how environmental factors contribute to flood risk in specific areas. By concurrently processing these two streams of information, the Double-Encoder Transformer can create a more comprehensive prediction model. It can identify correlations between meteorological conditions and environmental responses, leading to more accurate and timely flood forecasts. This approach enhances the model's ability to predict not only when and where floods might occur but also their potential severity, aiding in disaster preparedness and resource allocation.\nOverall, the application of the Double-Encoder Transformer in flood forecasting represents a significant advancement in disaster management, leveraging AI's power to integrate and analyze complex, multi-faceted data for better, more informed decision-making in critical situations.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-16762"
    },
    {
        "id": 10061,
        "title": "Neutral Group Prediction with Bidirectional Encoder Representations from Transformer",
        "authors": "Tazizur Rahman, Yang Sok Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4382826"
    },
    {
        "id": 10062,
        "title": "A cross-attention transformer encoder for paired sequence data",
        "authors": "Ceder Dens, Kris Laukens, Pieter Meysman, Wout Bittremieux",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractTransformer-based sequence encoding architectures are often limited to a single-sequence input while some tasks require a multi-sequence input. For example, the peptide–MHCII binding prediction task where the input consists of two protein sequences. Current workarounds to solve this input-type mismatch lack resemblance with the biological mechanisms behind the task. As a solution, we propose a novel cross-attention transformer encoder that creates a cross-attended embedding of both input sequences. We compare its classification performance on the peptide–MHCII binding prediction task to a baseline logistic regression model and a default transformer encoder. Finally, we make visualizations of the attention layers to show how the different models learn different patterns.",
        "link": "http://dx.doi.org/10.1101/2023.12.11.571066"
    },
    {
        "id": 10063,
        "title": "Tee4ehr: Transformer Event Encoder for Better Representation Learning in Electronic Health Records",
        "authors": "Hojjat Karami, anisoara ionescu, David Atienza",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4756035"
    },
    {
        "id": 10064,
        "title": "Fast-Fnet: Accelerating Transformer Encoder Models Via Efficient Fourier Layers",
        "authors": "Nurullah Sevim, Ege  Ozan Ozyedek, Furkan Sahinuc, Aykut Koç",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566618"
    },
    {
        "id": 10065,
        "title": "GPA-TUNet: Transformer and GPA Attention Co-Encoder for Medical Image Segmentation",
        "authors": "Chaoqun Li, Liejun Wang, Yongming Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nU-Net has become baseline standard in the medical image segmentation tasks, but it has limitations in explicitly modeling long-term dependencies. Transformer has the ability to capture long-term relevance through its internal self-attention. However, Transformer is committed to modeling the correlation of all elements, but its awareness of local foreground information is not significant. Since medical images are often presented as regional blocks, local information is equally important. In this paper, we propose the GPA-TUNet by considering local and global information synthetically. Specifically, we propose a new attention mechanism to highlight local foreground information, called group parallel axial attention (GPA). Furthermore, we effectively combine GPA with Transformer in encoder part of model. It can not only highlight the foreground information of sample, but also reduce the negative influence of background information on the segmentation results. Meanwhile, we introduce the sMLP block to improve the global modeling capability of network. Sparse connectivity and weight sharing are well achieved by applying it. Extensive experiments on public datasets confirm the excellent performance of our proposed GPA-TUNet. In particular, on Synapse and ACDC datasets, mean DSC reached 80.37% and 90.37% respectively, mean HD95 reached 20.55% and 1.23% respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1607219/v1"
    },
    {
        "id": 10066,
        "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
        "authors": "Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-359"
    },
    {
        "id": 10067,
        "title": "HaloAE: A Local Transformer Auto-Encoder for Anomaly Detection and Localization Based on HaloNet",
        "authors": "Emilie Mathian, Huidong Liu, Lynnette Fernandez-Cuesta, Dimitris Samaras, Matthieu Foll, Liming Chen",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011865900003417"
    },
    {
        "id": 10068,
        "title": "Violation detection based on a modified transformer encoder network with Bi-LSTM",
        "authors": "Yuyu Zeng, Yu Zhang, XinSheng Zhu, Lan Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Due to the rapid growth of China’s security market, the early detection\nof accounting violations by listed companies has become increasingly\nurgent. However, the current lack of regulatory efficiency means that\nsuch problems are often not detected and solved. To solve these issues,\nresearchers have proposed traditional methods and artificial\nintelligence methods in recent decades. However, some existing methods\nhave high requirements for setting indicators and parameters, and their\ndetection accuracy could be improved. To take advantage of the concept\nof the self-attention mechanism, we built a modified transformer encoder\nnetwork with Bi-LSTM to analyze accounting data and identify whether\nthere are violations by listed companies. This model attempts to enhance\nthe long-term feature capture capability for processing structural data,\nsuch as accounting data, through Bi-LSTM and stacking multiple\ntransformer encoder networks while reducing the speed of computation due\nto the parallel power of the self-attention mechanism. Several\nexperiments were conducted on the accounting dataset released by the\nopen-source China Stock Market & Accounting Research (CSMAR). The\nresults show that the proposed model, by the composition of Bi-LSTM as\nthe input layer and three layers of transformer encoder network, has the\nbest detection performance in comparison to the deep neural network\n(DNN), random forest models, etc., with a testing accuracy of 0.9758 and\nan AUC of 0.98. Furthermore, we embedded the pretrained model in a\nWeChat miniprogram to provide a better interactive experience for users\nduring the preliminary detection of a single file with the required\naccounting data.",
        "link": "http://dx.doi.org/10.22541/au.167059997.74750948/v1"
    },
    {
        "id": 10069,
        "title": "Transformer-based Hierarchical Encoder for Document Classification",
        "authors": "Harsh Sakhrani, Saloni Parekh, Pratik Ratadiya",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw53433.2021.00109"
    },
    {
        "id": 10070,
        "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
        "authors": "Jian Luo, Jianzong Wang, Ning Cheng, Jing Xiao",
        "published": "2021-8-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-1066"
    },
    {
        "id": 10071,
        "title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition",
        "authors": "Xinyuan Zhou, Emre Yılmaz, Yanhua Long, Yijie Li, Haizhou Li",
        "published": "2020-10-25",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2488"
    },
    {
        "id": 10072,
        "title": "A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies",
        "authors": "shivacharan oruganti, Deepa Karuppaiah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4317584"
    },
    {
        "id": 10073,
        "title": "A Hybrid Encoder Transformer Network for Video Inpainting",
        "authors": "Hongshan Gan, Yi Wan",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790116"
    },
    {
        "id": 10074,
        "title": "Dual Transformer Encoder Model for Medical Image Classification",
        "authors": "Fangyuan Yan, Bin Yan, Mingtao Pei",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222303"
    },
    {
        "id": 10075,
        "title": "Fully Transformer Detector with Multiscale Encoder and Dynamic Decoder",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/wcse.2023.06.016"
    },
    {
        "id": 10076,
        "title": "End-to-End Heart Failure Detection Based on Multipath Transformer Encoder",
        "authors": "Nan Jiang, Siyu Yang, Shudong Xia, Yanyun Dai, Jijun Tong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4639512"
    },
    {
        "id": 10077,
        "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
        "authors": "Guan-Lin Chao, Ian Lane",
        "published": "2019-9-15",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-1355"
    },
    {
        "id": 10078,
        "title": "Transformer Encoder Based Self-Supervised Learning for Hvac Fault Detection with Unlabeled Data",
        "authors": "Mohammad Abdollah Fadel Abdollah, Rossano Scoccia, Marcello Aprile",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4715186"
    },
    {
        "id": 10079,
        "title": "Multimodal Neural Machine Translation Using CNN and Transformer Encoder",
        "authors": "Hiroki Takushima, Akihiro Tamura, Takashi Ninomiya, Hideki Nakayama",
        "published": "2019-4-2",
        "citations": 2,
        "abstract": "Multimodal machine translation uses images related to source language sentences as inputs to improve translation quality. Previous multimodal Neural Machine Translation (NMT) models, which incorporate visual features of each image region into an encoder for source language sentences or an attention mechanism between an encoder and a decoder, cannot catch the relation between visual features from each image region. This paper proposes a new multimodal NMT model, which encodes an input image using a Convolutional Neural Network (CNN) and a Transformer encoder. In particular, the proposed image encoder first extracts visual features from each image region using a CNN, and then encodes an input image on the basis of the extracted visual features using a Transformer encoder, where the relation between visual features from each image region are captured by a self-attention mechanism of the Transformer encoder. The experiments on the English-German translation task using the Multi30k data set show that the proposed model achieves 0.96 BLEU points improvement against a baseline Transformer NMT model without image inputs and 0.47 BLEU points improvement against a baseline multimodal Transformer NMT model without a Transformer encoder for images.",
        "link": "http://dx.doi.org/10.29007/hxhn"
    },
    {
        "id": 10080,
        "title": "An Efficient Skeleton-Based Fall Detection Algorithm Using Temporal Convolutional Networks with Transformer Encoder",
        "authors": "Xiaoqun Yu, Chenfeng Wang, Wenyu Wu, Shuping Xiong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4750350"
    },
    {
        "id": 10081,
        "title": "Detect Turn-takings in Subtitle Streams with Semantic Recall Transformer Encoder",
        "authors": "Yuhai Liang, Qiang Zhou",
        "published": "2020-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp51396.2020.9310512"
    },
    {
        "id": 10082,
        "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
        "authors": "Timo Lohrenz, Zhengyang Li, Tim Fingscheidt",
        "published": "2021-8-30",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2021-555"
    },
    {
        "id": 10083,
        "title": "Learn Dynamic Facial Motion Representations Using Transformer Encoder",
        "authors": "Zheng Sun, Andrew Sumsion, Shad Torrie, Dah-Jye Lee",
        "published": "2022-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ietc54973.2022.9796917"
    },
    {
        "id": 10084,
        "title": "Syntax-aware Transformer Encoder for Neural Machine Translation",
        "authors": "Sufeng Duan, Hai Zhao, Junru Zhou, Rui Wang",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp48816.2019.9037672"
    },
    {
        "id": 10085,
        "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
        "authors": "Yu Suzuki, Tsuneo Kato, Akihiro Tamura",
        "published": "2022-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-10344"
    },
    {
        "id": 10086,
        "title": "Multi-encoder Transformer Network for Automatic Post-Editing",
        "authors": "Jaehun Shin, Jong-Hyeok Lee",
        "published": "2018",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-6470"
    },
    {
        "id": 10087,
        "title": "Multi-Scale Deformable Transformer Encoder Based Single-Stage Pedestrian Detection",
        "authors": "Jing Yuan, Panagiotis Barmpoutis, Tania Stathaki",
        "published": "2022-10-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897361"
    },
    {
        "id": 10088,
        "title": "ECONOMICAL IMPLEMENTATION OF ATTENTION IN THE TRANSFORMER ENCODER FOR SPEECH RECOGNITION",
        "authors": "V. Y. Chuchupal",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58633/2305-8129_2022_1_119"
    },
    {
        "id": 10089,
        "title": "Encoder/Decoder Transformer-Based Framework to Detect Hate Speech from Tweets",
        "authors": " Usman, S. M. K. Quadri",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003371380-19"
    },
    {
        "id": 10090,
        "title": "PiTE: TCR-epitope Binding Affinity Prediction Pipeline using Transformer-based Sequence Encoder",
        "authors": "Pengfei Zhang, Seojin Bang, Heewook Lee",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811270611_0032"
    },
    {
        "id": 10091,
        "title": "EEG Source Imaging based on a Transformer Encoder Network",
        "authors": "Tongtong Zheng, Zijing Guan",
        "published": "2023-2-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nnice58320.2023.10105793"
    },
    {
        "id": 10092,
        "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Yves Scherrer, Jörg Tiedemann",
        "published": "2020",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.49"
    },
    {
        "id": 10093,
        "title": "Knowledge Transfer Between Tasks and Languages in the Multi-task Encoder-agnostic Transformer-based Models",
        "authors": "Dmitry Karpov,  , Vasily Konovalov",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "We explore the knowledge transfer in the simple multi-task encoder-agnostic transformer-based models on five dialog tasks: emotion classification, sentiment classification, toxicity classification, intent classification, and topic classification. We show that these mo dels’ accuracy differs from the analogous single-task models by ∼0.9%. These results hold for the multiple transformer backbones. At the same time, these models have the same backbone for all tasks, which allows them to have about 0.1% more parameters than any analogous single-task model and to support multiple tasks simultaneously. We also found that if we decrease the dataset size to a certain extent, multi-task models outperform singletask ones, especially on the smallest datasets. We also show that while training multilingual models on the Russian data, adding the English data from the same task to the training sample can improve model performance for the multi-task and single-task settings. The improvement can reach 4-5% if the Russian data are scarce enough. We have integrated these models to the DeepPavlov library and to the DREAM dialogue platform.",
        "link": "http://dx.doi.org/10.28995/2075-7182-2023-22-200-214"
    },
    {
        "id": 10094,
        "title": "B-Cell Linear Epitope Prediction Using Transformer Encoder",
        "authors": "Youjin Kim, Jinhee Park, Junseok Kwon",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55196.2022.9952973"
    },
    {
        "id": 10095,
        "title": "BiVaSE: A bilingual variational sentence encoder with randomly initialized Transformer layers",
        "authors": "Bence Nyéki",
        "published": "2022-12-12",
        "citations": 0,
        "abstract": "AbstractTransformer-based NLP models have achieved state-of-the-art results in many NLP tasks including text classification and text generation. However, the layers of these models do not output any explicit representations for texts units larger than tokens (e.g. sentences), although such representations are required to perform text classification. Sentence encodings are usually obtained by applying a pooling technique during fine-tuning on a specific task. In this paper, a new sentence encoder is introduced. Relying on an autoencoder architecture, it was trained to learn sentence representations from the very beginning of its training. The model was trained on bilingual data with variational Bayesian inference. Sentence representations were evaluated in downstream and linguistic probing tasks. Although the newly introduced encoder generally performs worse than well-known Transformer-based encoders, the experiments show that it was able to learn to incorporate linguistic information in the sentence representations.",
        "link": "http://dx.doi.org/10.1556/2062.2022.00584"
    },
    {
        "id": 10096,
        "title": "Fractional Fourier Transform Meets Transformer Encoder",
        "authors": "Furkan Sahinuc, Aykut Koc",
        "published": "2022",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsp.2022.3217975"
    },
    {
        "id": 10097,
        "title": "CNN-Based Encoder and Transformer-Based Decoder for Efficient Semantic Segmentation",
        "authors": "Seunghun Moon, Suk-ju Kang",
        "published": "2024-1-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceic61013.2024.10457284"
    },
    {
        "id": 10098,
        "title": "Document-Level Relation Extraction with Structure Enhanced Transformer Encoder",
        "authors": "Wanlong Liu, Li Zhou, Dingyi Zeng, Hong Qu",
        "published": "2022-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892647"
    },
    {
        "id": 10099,
        "title": "Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer",
        "authors": "Joosung Lee",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.inlg-1.25"
    },
    {
        "id": 10100,
        "title": "Glass Objects Detection Based on Transformer Encoder-Decoder",
        "authors": "Xiaonan Hou, Minghao Zhan, Chunlei Wang, Chunhui Fan",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacr55854.2022.9935562"
    },
    {
        "id": 10101,
        "title": "Transformer-Based Bidirectional Encoder Representations for Emotion Detection from Text",
        "authors": "Ashok Kumar J, Erik Cambria, Tina Esther Trueman",
        "published": "2021-12-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660152"
    },
    {
        "id": 10102,
        "title": "Named Entity Recognition Based on Transformer Encoder in the Medical Field",
        "authors": "Xiyv Mou, Xindong Zhang",
        "published": "2022-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwecai55315.2022.00111"
    },
    {
        "id": 10103,
        "title": "Question Classification Using Universal Sentence Encoder and Deep Contextualized Transformer",
        "authors": "Najam Arif, Seemab Latif, Rabia Latif",
        "published": "2021-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dese54285.2021.9719473"
    },
    {
        "id": 10104,
        "title": "A Personalized Paper Recommendation Method Based on Knowledge Graph and Transformer Encoder with a Self-Attention Mechanism",
        "authors": "Li Gao, Yu Lan, Qingkui Chen, Yi Liu",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4329702"
    },
    {
        "id": 10105,
        "title": "Extracting Syntactic Trees from Transformer Encoder Self-Attentions",
        "authors": "David Mareček, Rudolf Rosa",
        "published": "2018",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5444"
    },
    {
        "id": 10106,
        "title": "Transformer-Based Encoder-Encoder Architecture for Spoken Term Detection",
        "authors": "Jan Švec, Luboš Šmídl, Jan Lehečka",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47665-5_28"
    },
    {
        "id": 10107,
        "title": "Bi-Encoder Transformer Network for Mandarin-English Code-Switching Speech Recognition Using Mixture of Experts",
        "authors": "Yizhou Lu, Mingkun Huang, Hao Li, Jiaqi Guo, Yanmin Qian",
        "published": "2020-10-25",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2485"
    },
    {
        "id": 10108,
        "title": "Multi-Encoder Transformer for Korean Abstractive Text Summarization",
        "authors": "Youhyun Shin",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3277754"
    },
    {
        "id": 10109,
        "title": "Transformer with Multi-block Encoder for Multi-turn Dialogue Translation",
        "authors": "Shih-Wen Ke, Yu-Cyuan Lin",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieem58616.2023.10406480"
    },
    {
        "id": 10110,
        "title": "Transformer Encoder Based Self-Supervised Learning for Hvac Fault Detection and Diagnostics with Unlabeled Data: A Case Study",
        "authors": "Mohammad Abdollah Fadel Abdollah, Rossano Scoccia, Marcello Aprile",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4662964"
    },
    {
        "id": 10111,
        "title": "More than Encoder: Introducing Transformer Decoder to Upsample",
        "authors": "Yijiang Li, Wentian Cai, Ying Gao, Chengming Li, Xiping Hu",
        "published": "2022-12-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995378"
    },
    {
        "id": 10112,
        "title": "Bi-directional Encoder Representation of Transformer model for Sequential Music Recommender System",
        "authors": "Naina Yadav, Anil Kumar Singh",
        "published": "2020-12-16",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3441501.3441503"
    },
    {
        "id": 10113,
        "title": "TEAM: Transformer Encoder Attention Module for Video Classification",
        "authors": "Hae Sung Park, Yong Suk Choi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/csse.2023.043245"
    },
    {
        "id": 10114,
        "title": "Attention Generative Adversarial Network with Transformer Encoder for Missing Sensor Time Series",
        "authors": "Zeng-Song Xu, Song Ma, Tao Sun, Xi-Ming Sun",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450504"
    },
    {
        "id": 10115,
        "title": "Enhancing Real-Time Strategy Games via Transformer Encoder with Patch Embedding",
        "authors": "Shaobo Hu, Wei Liu",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csis-iac60628.2023.10363820"
    },
    {
        "id": 10116,
        "title": "Classification of Short-Interfering RNA Through Transformer Encoder Model",
        "authors": "Rolando Pula, Maria Rejane Nepacina, Lorena Ilagan",
        "published": "2024-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/restcon60981.2024.10463552"
    },
    {
        "id": 10117,
        "title": "Swin Transformer Combined with Convolutional Encoder For Cephalometric Landmarks Detection",
        "authors": "Ao Yueyuan, Wu Hong",
        "published": "2021-12-17",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccwamtip53232.2021.9674147"
    },
    {
        "id": 10118,
        "title": "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
        "authors": "Tarun Gupta, Tuan Duc Truong, Tran The Anh, Eng Siong Chng",
        "published": "2022-9-18",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2022-567"
    },
    {
        "id": 10119,
        "title": "RUITE: Refining UI Layout Aesthetics Using Transformer Encoder",
        "authors": "Soliha Rahman, Vinoth Pandian Sermuga Pandian, Matthias Jarke",
        "published": "2021-4-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3397482.3450716"
    },
    {
        "id": 10120,
        "title": "Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer",
        "authors": " Mohanad Sameer,  Ahmed Talib,  Alla Hussein",
        "published": "2023-3-21",
        "citations": 2,
        "abstract": "Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when utilizing the Common Voice dataset from Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 hours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition systems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training requires a very large amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language model.",
        "link": "http://dx.doi.org/10.51173/jt.v5i1.749"
    },
    {
        "id": 10121,
        "title": "Transformer Encoder for Efficient CAPTCHA Recognize",
        "authors": "Yaoting Li, Haixia Pan, Huolong Ye, Jiayu Zheng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cbase60015.2023.10439128"
    },
    {
        "id": 10122,
        "title": "Prediction of Remaining Useful Life of Rolling Bearings based on Transformer Encoder",
        "authors": "Huan Zhan, Hongsheng Li, Xinfa Xiao",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291391"
    },
    {
        "id": 10123,
        "title": "A Cross-Scenes Ancient Character Recognition Model Based on Normalized Generator and Transformer Encoder in Computational Archaeology",
        "authors": "JiaYing Gao, Fausto Giunchiglia, Tongyu Zhao, Chuntao Li, Hao Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4741242"
    },
    {
        "id": 10124,
        "title": "Porous Lattice Transformer Encoder for Chinese NER",
        "authors": "Xue Mengge, Bowen Yu, Tingwen Liu, Yue Zhang, Erli Meng, Bin Wang",
        "published": "2020",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.340"
    },
    {
        "id": 10125,
        "title": "TransVAT: Transformer Encoder with Variational Attention for Few-Shot Fault Diagnosis",
        "authors": "Yifan Zhan, Rui Yang",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/safeprocess58597.2023.10295900"
    },
    {
        "id": 10126,
        "title": "Comparative Analysis of Pretrained Encoder-Decoder Transformer Models for Extreme Text Summarization",
        "authors": "Tamma RajyaLakshmi, K.S. Kuppusamy",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacic59454.2023.10435363"
    },
    {
        "id": 10127,
        "title": "Basic Ensemble Learning of Encoder Representations from Transformer for Disaster-mentioning Tweets Classification",
        "authors": "Yizhou Yang",
        "published": "2021-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csaiee54046.2021.9543322"
    },
    {
        "id": 10128,
        "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
        "authors": "Alessandro Raganato, Jörg Tiedemann",
        "published": "2018",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5431"
    },
    {
        "id": 10129,
        "title": "Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder",
        "authors": "John Pougué Biyong, Bo Wang, Terry Lyons, Alejo Nevado-Holgado",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.clinicalnlp-1.5"
    },
    {
        "id": 10130,
        "title": "Transformer-Encoder Detector Module: Using Context to Improve Robustness to Adversarial Attacks on Object Detection",
        "authors": "Faisal Alamri, Sinan Kalkan, Nicolas Pugeault",
        "published": "2021-1-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9413344"
    },
    {
        "id": 10131,
        "title": "Transformer with Enhanced Encoder and Monotonic Decoder for Automatic Speech Recognition",
        "authors": "Priyabrata Karmakar, Shyh Wei Teng, Guojun Lu",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dicta56598.2022.10034576"
    },
    {
        "id": 10132,
        "title": "Transformer encoder with multiscale deep learning for pain classification using physiological signals",
        "authors": "Zhenyuan Lu, Burcu Ozek, Sagar Kamarthi",
        "published": "2023-12-6",
        "citations": 1,
        "abstract": "Pain, a pervasive global health concern, affects a large segment of population worldwide. Accurate pain assessment remains a challenge due to the limitations of conventional self-report scales, which often yield inconsistent results and are susceptible to bias. Recognizing this gap, our study introduces PainAttnNet, a novel deep-learning model designed for precise pain intensity classification using physiological signals. We investigate whether PainAttnNet would outperform existing models in capturing temporal dependencies. The model integrates multiscale convolutional networks, squeeze-and-excitation residual networks, and a transformer encoder block. This integration is pivotal for extracting robust features across multiple time windows, emphasizing feature interdependencies, and enhancing temporal dependency analysis. Evaluation of PainAttnNet on the BioVid heat pain dataset confirm the model’s superior performance over the existing models. The results establish PainAttnNet as a promising tool for automating and refining pain assessments. Our research not only introduces a novel computational approach but also sets the stage for more individualized and accurate pain assessment and management in the future.",
        "link": "http://dx.doi.org/10.3389/fphys.2023.1294577"
    },
    {
        "id": 10133,
        "title": "Acoustic Word Embedding Model with Transformer Encoder and Multivariate Joint Loss",
        "authors": "Yunyun Gao, Qiang Zhang, Lasheng Zhao",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwoc57905.2023.10199703"
    },
    {
        "id": 10134,
        "title": "Distance Restricted Transformer Encoder for Multi-Label Classification",
        "authors": "Xiaomei Wang, Yaqian Li, Tong Luo, Yandong Guo, Yanwei Fu, Xiangyang Xue",
        "published": "2021-7-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme51207.2021.9428164"
    },
    {
        "id": 10135,
        "title": "Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks",
        "authors": "Barry Ridge, Rok Pahic, Ales Ude, Jun Morimoto",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794253"
    },
    {
        "id": 10136,
        "title": "METrans: Multi‐encoder transformer for ischemic stroke segmentation",
        "authors": "Jing Wang, Shuyu Wang, Wei Liang",
        "published": "2022-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/ell2.12444"
    },
    {
        "id": 10137,
        "title": "A New Transformer Fault Diagnosis Method Based on Classified Deep Auto-encoder Network",
        "authors": "Tie CHEN, Qi-de TAN",
        "published": "2017-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/dtcse/cece2017/14583"
    },
    {
        "id": 10138,
        "title": "EMET: Embeddings from Multilingual-Encoder Transformer for Fake News Detection",
        "authors": "Stephane Schwarz, Antonio Theophilo, Anderson Rocha",
        "published": "2020-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9054673"
    },
    {
        "id": 10139,
        "title": "A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder",
        "authors": "Xipeng Qiu, Hengzhi Pei, Hang Yan, Xuanjing Huang",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.260"
    },
    {
        "id": 10140,
        "title": "Classification and Generation of Arabic News Titles from Raw Text Based on an Encoder-Decoder Transformer Model (mT5)",
        "authors": "‪Ayedh Abdulaziz Mohsen‬‏, Marwah Yahya Al-Nahari, Akram Alsubari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMultilingual Transformer 5 (MT5) is a versatile architecture in natural language processing (NLP) that demonstrates proficiency across various languages. This study aimed to improve the performance of the MT5 model in two key tasks: topic classification and headline generation. The datasets used were 183K and 294K samples. The classification task involved categorizing news articles, while the news generation task aimed to create coherent and contextually relevant Arabic news content. Through careful fine-tuning and rigorous evaluation, the MT5 model significantly advances its ability to address complex challenges in Arabic NLP. This study provides practical insights into real-world applications in processing Arab news. The performance of the MT5 model was evaluated using various online platforms. The mT5small model achieved an accuracy of 0.7858 and an F1 score of 0.7858, while the mT5base model achieved an accuracy of 0.8230 and an F1 score of 0.8230. The generative approach for headline generation yielded Rouge-1, Rouge-2, and Rouge-L scores under the task \"Generative of Headlines.\" These outcomes demonstrate the effectiveness of the fine-tuned MT5 model across various evaluation metrics and tasks, confirming its potential for practical applications in Arabic NLP.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3982909/v1"
    },
    {
        "id": 10141,
        "title": "Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation",
        "authors": "Raman Goel, Seba Susan, Sachin Vashisht, Armaan Dhanda",
        "published": "2021-9-28",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aciiw52867.2021.9666315"
    },
    {
        "id": 10142,
        "title": "A Comparison of Transformer and LSTM Encoder Decoder Models for ASR",
        "authors": "Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schluter, Hermann Ney",
        "published": "2019-12",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru46091.2019.9004025"
    },
    {
        "id": 10143,
        "title": "A Multi-layer Bidirectional Transformer Encoder for Pre-trained Word Embedding: A Survey of BERT",
        "authors": "Rohit Kumar Kaliyar",
        "published": "2020-1",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/confluence47617.2020.9058044"
    },
    {
        "id": 10144,
        "title": "Movie genre prediction based on the bidirectional encoder representations from transformer",
        "authors": "Lu Li, Jin Lin, Tongyu Li",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "The rapid expansion of digital media has underscored the growing significance of predicting genres to target audiences effectively and to enhance filmmakers' understanding of viewer preferences. In this study, we introduced a novel method for forecasting movie genres, leveraging the power of the Bidirectional Encoder Representations from Transformer (BERT) deep learning model. The research team employed a dataset sourced from Douban's website, which featured 5,000 movies, complete with cover images, titles, and genre information. This undertaking tackled several key challenges, including the extraction of features from textual data, the categorization of movie genres, and the incorporation of cultural nuances. BERT's bidirectional representation, especially its variant tailored for Chinese language tasks, 'bert-base-chinese', was employed to extract textual features. Meanwhile, the visual features from movie covers were processed using the Wide ResNet-50-2 architecture. The combined features underwent classification, and the resulting model achieved an accuracy of 34.67%, a recall rate of 74.5%, and an F1 score of 0.4755. The study validates the potential of the BERT model in predicting movie genres and offers significant insights for future research in multimedia content classification.",
        "link": "http://dx.doi.org/10.54254/2755-2721/47/20241383"
    },
    {
        "id": 10145,
        "title": "EMG-based 3D hand gesture prediction using transformer–encoder classification",
        "authors": "Tahira Mahboob, Min Young Chung, Kae Won Choi",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.icte.2023.04.005"
    },
    {
        "id": 10146,
        "title": "T-EnFP: An Efficient Transformer Encoder-Based System for Driving Behavior Classification",
        "authors": "Bin Guo, John H.L. Hansen",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448392"
    },
    {
        "id": 10147,
        "title": "Classification of URDU headline news using Bidirectional Encoder Representation from Transformer and Traditional Machine learning Algorithm",
        "authors": "Kainat Mujahid, Sania Bhatti, Mohsin Memon",
        "published": "2021-11-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/imtic53841.2021.9719828"
    },
    {
        "id": 10148,
        "title": "Stacked encoder–decoder transformer with boundary smoothing for action segmentation",
        "authors": "Gyeong‐hyeon Kim, Eunwoo Kim",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/ell2.12678"
    },
    {
        "id": 10149,
        "title": "Lattice-Based Transformer Encoder for Neural Machine Translation",
        "authors": "Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, Kehai Chen",
        "published": "2019",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1298"
    },
    {
        "id": 10150,
        "title": "Optimized vision transformer encoder with cnn for automatic psoriasis disease detection",
        "authors": "Gagan Vishwakarma, Amit Kumar Nandanwar, Ghanshyam Singh Thakur",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-023-16871-z"
    },
    {
        "id": 10151,
        "title": "KGETCDA: an efficient representation learning framework based on knowledge graph encoder from transformer for predicting circRNA-disease associations",
        "authors": "Jinyang Wu, Zhiwei Ning, Yidong Ding, Ying Wang, Qinke Peng, Laiyi Fu",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTRecent studies have demonstrated the significant role that circRNA plays in the progression of human diseases. Identifying circRNA-disease associations (CDA) in an efficient manner can offer crucial insights into disease diagnosis. While traditional biological experiments can be time-consuming and labor-intensive, computational methods have emerged as a viable alternative in recent years. However, these methods are often limited by data sparsity and their inability to explore high-order information. In this paper, we introduce a novel method named Knowledge Graph Encoder from Transformer for predicting CDA (KGETCDA). Specifically, KGETCDA first integrates more than 10 databases to construct a large heterogeneous non-coding RNA dataset, which contains multiple relationships between circRNA, miRNA, lncRNA and disease. Then, a biological knowledge graph is created based on this dataset and Transformer-based knowledge representation learning and attentive propagation layers are applied to obtain high-quality embeddings with accurately captured high-order interaction information. Finally, multilayer perceptron is utilized to predict the matching scores of CDA based on their embeddings. Our empirical results demonstrate that KGETCDA significantly outperforms other state-of-the-art models. To enhance user experience, we have developed an interactive web-based platform named HNRBase that allows users to visualize, download data and make predictions using KGETCDA with ease.",
        "link": "http://dx.doi.org/10.1101/2023.03.28.534642"
    },
    {
        "id": 10152,
        "title": "An FFT-based CNN-Transformer Encoder for Semantic Segmentation of Radar Sounder Signal",
        "authors": "Raktim Ghosh, Francesca Bovolo",
        "published": "2022-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2636693"
    },
    {
        "id": 10153,
        "title": "Survey of BERT (Bidirectional Encoder Representation Transformer) types",
        "authors": "Athar Hussein Mohammed, Ali H. Ali",
        "published": "2021-7-1",
        "citations": 4,
        "abstract": "AbstractThere are many algorithms used in Natural Language Processing( NLP) to achieve good results, such as Machine Learning (ML), Deep Learning(DL) and many other algorithms. In Natural Language Processing,the first challenges is to convert text to numbers for using by any algorithm that a researcher choose. So how can convert text to numbers? This is happen by using Word Embedding algorithms such as skip gram,bags of words,BERT and etc. Representing words as numerical vectors by relying on the contents has become one of the effective methods for analyzing texts in machine learning, so that each word is represented by a vector to determine its meaning or to know how close or distant this word from the rest of the other word. BERT(Bidirectional Encoder Representation Transformer) is one of the embedding methods. It is designed to pre-trained form left and right in all layer deep training. It is a deep language model that is used for various tasks in natural language processing. In this paper we will review the different versions and types of BERT.",
        "link": "http://dx.doi.org/10.1088/1742-6596/1963/1/012173"
    },
    {
        "id": 10154,
        "title": "AI-Generated Text Detector for Arabic Language Using Encoder-Based Transformer Architecture",
        "authors": "Hamed Alshammari, Ahmed El-Sayed, Khaled Elleithy",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "The effectiveness of existing AI detectors is notably hampered when processing Arabic texts. This study introduces a novel AI text classifier designed specifically for Arabic, tackling the distinct challenges inherent in processing this language. A particular focus is placed on accurately recognizing human-written texts (HWTs), an area where existing AI detectors have demonstrated significant limitations. To achieve this goal, this paper utilized and fine-tuned two Transformer-based models, AraELECTRA and XLM-R, by training them on two distinct datasets: a large dataset comprising 43,958 examples and a custom dataset with 3078 examples that contain HWT and AI-generated texts (AIGTs) from various sources, including ChatGPT 3.5, ChatGPT-4, and BARD. The proposed architecture is adaptable to any language, but this work evaluates these models’ efficiency in recognizing HWTs versus AIGTs in Arabic as an example of Semitic languages. The performance of the proposed models has been compared against the two prominent existing AI detectors, GPTZero and OpenAI Text Classifier, particularly on the AIRABIC benchmark dataset. The results reveal that the proposed classifiers outperform both GPTZero and OpenAI Text Classifier with 81% accuracy compared to 63% and 50% for GPTZero and OpenAI Text Classifier, respectively. Furthermore, integrating a Dediacritization Layer prior to the classification model demonstrated a significant enhancement in the detection accuracy of both HWTs and AIGTs. This Dediacritization step markedly improved the classification accuracy, elevating it from 81% to as high as 99% and, in some instances, even achieving 100%.",
        "link": "http://dx.doi.org/10.3390/bdcc8030032"
    },
    {
        "id": 10155,
        "title": "Accurate Human Mesh Reconstruction from a Video with Transformer Based Encoder",
        "authors": "Hui Liu, Jianming Wang",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3532213.3532310"
    },
    {
        "id": 10156,
        "title": "MSEDTNet: Multi-Scale Encoder and Decoder with Transformer for Bladder Tumor Segmentation",
        "authors": "Yixing Wang, Xiufen Ye",
        "published": "2022-10-17",
        "citations": 1,
        "abstract": "The precise segmentation of bladder tumors from MRI is essential for bladder cancer diagnosis and personalized therapy selection. Limited by the properties of tumor morphology, achieving precise segmentation from MRI images remains challenging. In recent years, deep convolutional neural networks have provided a promising solution for bladder tumor segmentation from MRI. However, deep-learning-based methods still face two weakness: (1) multi-scale feature extraction and utilization are inadequate, being limited by the learning approach. (2) The establishment of explicit long-distance dependence is difficult due to the limited receptive field of convolution kernels. These limitations raise challenges in the learning of global semantic information, which is critical for bladder cancer segmentation. To tackle the problem, a newly auxiliary segmentation algorithm integrating a multi-scale encoder and decoder with a transformer is proposed, which is called MSEDTNet. Specifically, the designed encoder with multi-scale pyramidal convolution (MSPC) is utilized to generate compact feature maps which capture the richly detailed local features of the image. Furthermore, the transformer bottleneck is then leveraged to model the long-distance dependency between high-level tumor semantics from a global space. Finally, a decoder with a spatial context fusion module (SCFM) is adopted to fuse the context information and gradually produce high-resolution segmentation results. The experimental results of T2-weighted MRI scans from 86 patients show that MSEDTNet achieves an overall Jaccard index of 83.46%, a Dice similarity coefficient of 92.35%, and a complexity less than that of other, similar models. This suggests that the method proposed in this article can be used as an efficient tool for clinical bladder cancer segmentation.",
        "link": "http://dx.doi.org/10.3390/electronics11203347"
    },
    {
        "id": 10157,
        "title": "Video Anomaly Detection Using Encoder-Decoder Networks with Video Vision Transformer and Channel Attention Blocks",
        "authors": "Shimpei Kobayashi, Akiyoshi Hizukuri, Ryohei Nakayama",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mva57639.2023.10215921"
    },
    {
        "id": 10158,
        "title": "MarSan at SemEval-2022 Task 11: Multilingual complex named entity recognition using T5 and transformer encoder",
        "authors": "Ehsan Tavan, Maryam Najafi",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.semeval-1.226"
    },
    {
        "id": 10159,
        "title": "Fine-Grained Image Recognition by Integrating Transformer Encoder Blocks in a Robust Single Stage Object Detector",
        "authors": "Usman Ali, Seungmin Oh, Tai-Won Um, Minsoo Hann, Jinsul Kim",
        "published": "No Date",
        "citations": 1,
        "abstract": "Fine-grained image classification remains an ongoing challenge in the computer vision field, which is particularly intended to identify objects within sub-categories. It is a difficult task since there is a minimal and substantial intra-class variance. The current methods address the issue by first locating selective regions with Region Proposal Networks (RPN), object localization, or part localization, followed by implementing a CNN Network or SVM classifier to those selective regions. This approach, however, makes the process simple by implementing a single-stage end-to-end feature encoding with a localization method, which leads to improved feature representations of individual tokens/regions by integrating the transformer encoder blocks into the Yolov5 backbone structure. These Transformer Encoder Blocks, with their self-attention mechanism, effectively captured the global dependencies and enabled the model to learn relationships between distant regions. This improved the model ability to understand context and captured long-range spatial relationships in the image. We also replaced the Yolov5 detection heads with three transformer heads at the output for object recognition using the discriminative and informative features maps from transformer encoder blocks. We established the potential of the single stage detector for the fine-grained image recognition task, by achieving state of the art 93.4% accuracy, as well as outperforming the existing Yolov5 model. The effectiveness of our approach is assessed using the Stanford car dataset, which includes 16,185 images of 196 different classes of vehicles with significantly identical visual appearances.",
        "link": "http://dx.doi.org/10.20944/preprints202306.0152.v1"
    }
]